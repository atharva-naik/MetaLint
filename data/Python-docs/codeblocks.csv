description,title,code_block,python_version
"Writing Context Managers

Under the hood, the ‘with‘ statement is fairly complicated. Most
people will only use ‘with‘ in company with existing objects and
don’t need to know these details, so you can skip the rest of this section if
you like.  Authors of new objects will need to understand the details of the
underlying implementation and should keep reading.
A high-level explanation of the context management protocol is:

The expression is evaluated and should result in an object called a “context
manager”.  The context manager must have __enter__() and __exit__()
methods.
The context manager’s __enter__() method is called.  The value returned
is assigned to VAR.  If no as VAR clause is present, the value is simply
discarded.
The code in BLOCK is executed.
If BLOCK raises an exception, the context manager’s __exit__() method
is called with three arguments, the exception details (type, value, traceback,
the same values returned by sys.exc_info(), which can also be None
if no exception occurred).  The method’s return value controls whether an exception
is re-raised: any false value re-raises the exception, and True will result
in suppressing it.  You’ll only rarely want to suppress the exception, because
if you do the author of the code containing the ‘with‘ statement will
never realize anything went wrong.
If BLOCK didn’t raise an exception,  the __exit__() method is still
called, but type, value, and traceback are all None.

Let’s think through an example.  I won’t present detailed code but will only
sketch the methods necessary for a database that supports transactions.
(For people unfamiliar with database terminology: a set of changes to the
database are grouped into a transaction.  Transactions can be either committed,
meaning that all the changes are written into the database, or rolled back,
meaning that the changes are all discarded and the database is unchanged.  See
any database textbook for more information.)
Let’s assume there’s an object representing a database connection. Our goal will
be to let the user write code like this:
db_connection = DatabaseConnection()
with db_connection as cursor:
    cursor.execute('insert into ...')
    cursor.execute('delete from ...')
    # ... more operations ...


The transaction should be committed if the code in the block runs flawlessly or
rolled back if there’s an exception. Here’s the basic interface for
DatabaseConnection that I’ll assume:
class DatabaseConnection:
    # Database interface
    def cursor(self):
        ""Returns a cursor object and starts a new transaction""
    def commit(self):
        ""Commits current transaction""
    def rollback(self):
        ""Rolls back current transaction""


The __enter__() method is pretty easy, having only to start a new
transaction.  For this application the resulting cursor object would be a useful
result, so the method will return it.  The user can then add as cursor to
their ‘with‘ statement to bind the cursor to a variable name.
class DatabaseConnection:
    ...
    def __enter__(self):
        # Code to start a new transaction
        cursor = self.cursor()
        return cursor


The __exit__() method is the most complicated because it’s where most of
the work has to be done.  The method has to check if an exception occurred.  If
there was no exception, the transaction is committed.  The transaction is rolled
back if there was an exception.
In the code below, execution will just fall off the end of the function,
returning the default value of None.  None is false, so the exception
will be re-raised automatically.  If you wished, you could be more explicit and
add a return statement at the marked location.
class DatabaseConnection:
    ...
    def __exit__(self, type, value, tb):
        if tb is None:
            # No exception, so commit
            self.commit()
        else:
            # Exception occurred, so rollback.
            self.rollback()
            # return False",pep-343-the-with-statement - writing-context-managers,"db_connection = DatabaseConnection()
with db_connection as cursor:
    cursor.execute('insert into ...')
    cursor.execute('delete from ...')
    # ... more operations ...
",2.6
,pep-343-the-with-statement - writing-context-managers,"class DatabaseConnection:
    # Database interface
    def cursor(self):
        ""Returns a cursor object and starts a new transaction""
    def commit(self):
        ""Commits current transaction""
    def rollback(self):
        ""Rolls back current transaction""
",2.6
,pep-343-the-with-statement - writing-context-managers,"class DatabaseConnection:
    ...
    def __enter__(self):
        # Code to start a new transaction
        cursor = self.cursor()
        return cursor
",2.6
,pep-343-the-with-statement - writing-context-managers,"class DatabaseConnection:
    ...
    def __exit__(self, type, value, tb):
        if tb is None:
            # No exception, so commit
            self.commit()
        else:
            # Exception occurred, so rollback.
            self.rollback()
            # return False
",2.6
"The contextlib module

The contextlib module provides some functions and a decorator that
are useful when writing objects for use with the ‘with‘ statement.
The decorator is called contextmanager(), and lets you write a single
generator function instead of defining a new class.  The generator should yield
exactly one value.  The code up to the yield will be executed as the
__enter__() method, and the value yielded will be the method’s return
value that will get bound to the variable in the ‘with‘ statement’s
as clause, if any.  The code after the yield will be
executed in the __exit__() method.  Any exception raised in the block will
be raised by the yield statement.
Using this decorator, our database example from the previous section
could be written as:
from contextlib import contextmanager

@contextmanager
def db_transaction(connection):
    cursor = connection.cursor()
    try:
        yield cursor
    except:
        connection.rollback()
        raise
    else:
        connection.commit()

db = DatabaseConnection()
with db_transaction(db) as cursor:
    ...


The contextlib module also has a nested(mgr1, mgr2, ...) function
that combines a number of context managers so you don’t need to write nested
‘with‘ statements.  In this example, the single ‘with‘
statement both starts a database transaction and acquires a thread lock:
lock = threading.Lock()
with nested (db_transaction(db), lock) as (cursor, locked):
    ...


Finally, the closing() function returns its argument so that it can be
bound to a variable, and calls the argument’s .close() method at the end
of the block.
import urllib, sys
from contextlib import closing

with closing(urllib.urlopen('http://www.yahoo.com')) as f:
    for line in f:
        sys.stdout.write(line)



See also

PEP 343 - The “with” statement
PEP written by Guido van Rossum and Nick Coghlan; implemented by Mike Bland,
Guido van Rossum, and Neal Norwitz.  The PEP shows the code generated for a
‘with‘ statement, which can be helpful in learning how the statement
works.

The documentation  for the contextlib module.",pep-343-the-with-statement - the-contextlib-module,"from contextlib import contextmanager

@contextmanager
def db_transaction(connection):
    cursor = connection.cursor()
    try:
        yield cursor
    except:
        connection.rollback()
        raise
    else:
        connection.commit()

db = DatabaseConnection()
with db_transaction(db) as cursor:
    ...
",2.6
,pep-343-the-with-statement - the-contextlib-module,"lock = threading.Lock()
with nested (db_transaction(db), lock) as (cursor, locked):
    ...
",2.6
,pep-343-the-with-statement - the-contextlib-module,"import urllib, sys
from contextlib import closing

with closing(urllib.urlopen('http://www.yahoo.com')) as f:
    for line in f:
        sys.stdout.write(line)
",2.6
"PEP 371: The multiprocessing Package

The new multiprocessing package lets Python programs create new
processes that will perform a computation and return a result to the
parent.  The parent and child processes can communicate using queues
and pipes, synchronize their operations using locks and semaphores,
and can share simple arrays of data.
The multiprocessing module started out as an exact emulation of
the threading module using processes instead of threads.  That
goal was discarded along the path to Python 2.6, but the general
approach of the module is still similar.  The fundamental class
is the Process, which is passed a callable object and
a collection of arguments.  The start() method
sets the callable running in a subprocess, after which you can call
the is_alive() method to check whether the subprocess is still running
and the join() method to wait for the process to exit.
Here’s a simple example where the subprocess will calculate a
factorial.  The function doing the calculation is written strangely so
that it takes significantly longer when the input argument is a
multiple of 4.
import time
from multiprocessing import Process, Queue


def factorial(queue, N):
    ""Compute a factorial.""
    # If N is a multiple of 4, this function will take much longer.
    if (N % 4) == 0:
        time.sleep(.05 * N/4)

    # Calculate the result
    fact = 1L
    for i in range(1, N+1):
        fact = fact * i

    # Put the result on the queue
    queue.put(fact)

if __name__ == '__main__':
    queue = Queue()

    N = 5

    p = Process(target=factorial, args=(queue, N))
    p.start()
    p.join()

    result = queue.get()
    print 'Factorial', N, '=', result


A Queue is used to communicate the input parameter N and
the result.  The Queue object is stored in a global variable.
The child process will use the value of the variable when the child
was created; because it’s a Queue, parent and child can use
the object to communicate.  (If the parent were to change the value of
the global variable, the child’s value would be unaffected, and vice
versa.)
Two other classes, Pool and Manager, provide
higher-level interfaces.  Pool will create a fixed number of
worker processes, and requests can then be distributed to the workers
by calling apply() or apply_async() to add a single request,
and map() or map_async() to add a number of
requests.  The following code uses a Pool to spread requests
across 5 worker processes and retrieve a list of results:
from multiprocessing import Pool

def factorial(N, dictionary):
    ""Compute a factorial.""
    ...
p = Pool(5)
result = p.map(factorial, range(1, 1000, 10))
for v in result:
    print v


This produces the following output:
1
39916800
51090942171709440000
8222838654177922817725562880000000
33452526613163807108170062053440751665152000000000
...


The other high-level interface, the Manager class, creates a
separate server process that can hold master copies of Python data
structures.  Other processes can then access and modify these data
structures using proxy objects.  The following example creates a
shared dictionary by calling the dict() method; the worker
processes then insert values into the dictionary.  (Locking is not
done for you automatically, which doesn’t matter in this example.
Manager‘s methods also include Lock(), RLock(),
and Semaphore() to create shared locks.)
import time
from multiprocessing import Pool, Manager

def factorial(N, dictionary):
    ""Compute a factorial.""
    # Calculate the result
    fact = 1L
    for i in range(1, N+1):
        fact = fact * i

    # Store result in dictionary
    dictionary[N] = fact

if __name__ == '__main__':
    p = Pool(5)
    mgr = Manager()
    d = mgr.dict()         # Create shared dictionary

    # Run tasks using the pool
    for N in range(1, 1000, 10):
        p.apply_async(factorial, (N, d))

    # Mark pool as closed -- no more tasks can be added.
    p.close()

    # Wait for tasks to exit
    p.join()

    # Output results
    for k, v in sorted(d.items()):
        print k, v


This will produce the output:
1 1
11 39916800
21 51090942171709440000
31 8222838654177922817725562880000000
41 33452526613163807108170062053440751665152000000000
51 15511187532873822802242430164693032110632597200169861120000...


See also
The documentation for the multiprocessing module.

PEP 371 - Addition of the multiprocessing package
PEP written by Jesse Noller and Richard Oudkerk;
implemented by Richard Oudkerk and Jesse Noller.",pep-371-the-multiprocessing-package,"import time
from multiprocessing import Process, Queue


def factorial(queue, N):
    ""Compute a factorial.""
    # If N is a multiple of 4, this function will take much longer.
    if (N % 4) == 0:
        time.sleep(.05 * N/4)

    # Calculate the result
    fact = 1L
    for i in range(1, N+1):
        fact = fact * i

    # Put the result on the queue
    queue.put(fact)

if __name__ == '__main__':
    queue = Queue()

    N = 5

    p = Process(target=factorial, args=(queue, N))
    p.start()
    p.join()

    result = queue.get()
    print 'Factorial', N, '=', result
",2.6
,pep-371-the-multiprocessing-package,"from multiprocessing import Pool

def factorial(N, dictionary):
    ""Compute a factorial.""
    ...
p = Pool(5)
result = p.map(factorial, range(1, 1000, 10))
for v in result:
    print v
",2.6
,pep-371-the-multiprocessing-package,"1
39916800
51090942171709440000
8222838654177922817725562880000000
33452526613163807108170062053440751665152000000000
...
",2.6
,pep-371-the-multiprocessing-package,"import time
from multiprocessing import Pool, Manager

def factorial(N, dictionary):
    ""Compute a factorial.""
    # Calculate the result
    fact = 1L
    for i in range(1, N+1):
        fact = fact * i

    # Store result in dictionary
    dictionary[N] = fact

if __name__ == '__main__':
    p = Pool(5)
    mgr = Manager()
    d = mgr.dict()         # Create shared dictionary

    # Run tasks using the pool
    for N in range(1, 1000, 10):
        p.apply_async(factorial, (N, d))

    # Mark pool as closed -- no more tasks can be added.
    p.close()

    # Wait for tasks to exit
    p.join()

    # Output results
    for k, v in sorted(d.items()):
        print k, v
",2.6
"PEP 3101: Advanced String Formatting

In Python 3.0, the % operator is supplemented by a more powerful string
formatting method, format().  Support for the str.format() method
has been backported to Python 2.6.
In 2.6, both 8-bit and Unicode strings have a .format() method that
treats the string as a template and takes the arguments to be formatted.
The formatting template uses curly brackets ({, }) as special characters:
>>> # Substitute positional argument 0 into the string.
>>> ""User ID: {0}"".format(""root"")
'User ID: root'
>>> # Use the named keyword arguments
>>> ""User ID: {uid}   Last seen: {last_login}"".format(
...    uid=""root"",
...    last_login = ""5 Mar 2008 07:20"")
'User ID: root   Last seen: 5 Mar 2008 07:20'


Curly brackets can be escaped by doubling them:
>>> ""Empty dict: {{}}"".format()
""Empty dict: {}""


Field names can be integers indicating positional arguments, such as
{0}, {1}, etc. or names of keyword arguments.  You can also
supply compound field names that read attributes or access dictionary keys:
>>> import sys
>>> print 'Platform: {0.platform}\nPython version: {0.version}'.format(sys)
Platform: darwin
Python version: 2.6a1+ (trunk:61261M, Mar  5 2008, 20:29:41)
[GCC 4.0.1 (Apple Computer, Inc. build 5367)]'

>>> import mimetypes
>>> 'Content-type: {0[.mp4]}'.format(mimetypes.types_map)
'Content-type: video/mp4'


Note that when using dictionary-style notation such as [.mp4], you
don’t need to put any quotation marks around the string; it will look
up the value using .mp4 as the key.  Strings beginning with a
number will be converted to an integer.  You can’t write more
complicated expressions inside a format string.
So far we’ve shown how to specify which field to substitute into the
resulting string.  The precise formatting used is also controllable by
adding a colon followed by a format specifier.  For example:
>>> # Field 0: left justify, pad to 15 characters
>>> # Field 1: right justify, pad to 6 characters
>>> fmt = '{0:15} ${1:>6}'
>>> fmt.format('Registration', 35)
'Registration    $    35'
>>> fmt.format('Tutorial', 50)
'Tutorial        $    50'
>>> fmt.format('Banquet', 125)
'Banquet         $   125'


Format specifiers can reference other fields through nesting:
>>> fmt = '{0:{1}}'
>>> width = 15
>>> fmt.format('Invoice #1234', width)
'Invoice #1234  '
>>> width = 35
>>> fmt.format('Invoice #1234', width)
'Invoice #1234                      '


The alignment of a field within the desired width can be specified:






Character
Effect



< (default)
Left-align

>
Right-align

^
Center

=
(For numeric types only) Pad after the sign.



Format specifiers can also include a presentation type, which
controls how the value is formatted.  For example, floating-point numbers
can be formatted as a general number or in exponential notation:
>>> '{0:g}'.format(3.75)
'3.75'
>>> '{0:e}'.format(3.75)
'3.750000e+00'


A variety of presentation types are available.  Consult the 2.6
documentation for a complete list; here’s a sample:






b
Binary. Outputs the number in base 2.

c
Character. Converts the integer to the corresponding Unicode character
before printing.

d
Decimal Integer. Outputs the number in base 10.

o
Octal format. Outputs the number in base 8.

x
Hex format. Outputs the number in base 16, using lower-case letters for
the digits above 9.

e
Exponent notation. Prints the number in scientific notation using the
letter ‘e’ to indicate the exponent.

g
General format. This prints the number as a fixed-point number, unless
the number is too large, in which case it switches to ‘e’ exponent
notation.

n
Number. This is the same as ‘g’ (for floats) or ‘d’ (for integers),
except that it uses the current locale setting to insert the appropriate
number separator characters.

%
Percentage. Multiplies the number by 100 and displays in fixed (‘f’)
format, followed by a percent sign.



Classes and types can define a __format__() method to control how they’re
formatted.  It receives a single argument, the format specifier:
def __format__(self, format_spec):
    if isinstance(format_spec, unicode):
        return unicode(str(self))
    else:
        return str(self)


There’s also a format() builtin that will format a single
value.  It calls the type’s __format__() method with the
provided specifier:
>>> format(75.6564, '.2f')
'75.66'



See also

Format String Syntax
The reference documentation for format fields.
PEP 3101 - Advanced String Formatting
PEP written by Talin. Implemented by Eric Smith.",pep-3101-advanced-string-formatting,">>> # Substitute positional argument 0 into the string.
>>> ""User ID: {0}"".format(""root"")
'User ID: root'
>>> # Use the named keyword arguments
>>> ""User ID: {uid}   Last seen: {last_login}"".format(
...    uid=""root"",
...    last_login = ""5 Mar 2008 07:20"")
'User ID: root   Last seen: 5 Mar 2008 07:20'
",2.6
,pep-3101-advanced-string-formatting,">>> ""Empty dict: {{}}"".format()
""Empty dict: {}""
",2.6
,pep-3101-advanced-string-formatting,">>> import sys
>>> print 'Platform: {0.platform}\nPython version: {0.version}'.format(sys)
Platform: darwin
Python version: 2.6a1+ (trunk:61261M, Mar  5 2008, 20:29:41)
[GCC 4.0.1 (Apple Computer, Inc. build 5367)]'

>>> import mimetypes
>>> 'Content-type: {0[.mp4]}'.format(mimetypes.types_map)
'Content-type: video/mp4'
",2.6
,pep-3101-advanced-string-formatting,">>> # Field 0: left justify, pad to 15 characters
>>> # Field 1: right justify, pad to 6 characters
>>> fmt = '{0:15} ${1:>6}'
>>> fmt.format('Registration', 35)
'Registration    $    35'
>>> fmt.format('Tutorial', 50)
'Tutorial        $    50'
>>> fmt.format('Banquet', 125)
'Banquet         $   125'
",2.6
,pep-3101-advanced-string-formatting,">>> fmt = '{0:{1}}'
>>> width = 15
>>> fmt.format('Invoice #1234', width)
'Invoice #1234  '
>>> width = 35
>>> fmt.format('Invoice #1234', width)
'Invoice #1234                      '
",2.6
,pep-3101-advanced-string-formatting,">>> '{0:g}'.format(3.75)
'3.75'
>>> '{0:e}'.format(3.75)
'3.750000e+00'
",2.6
,pep-3101-advanced-string-formatting,"def __format__(self, format_spec):
    if isinstance(format_spec, unicode):
        return unicode(str(self))
    else:
        return str(self)
",2.6
,pep-3101-advanced-string-formatting,">>> format(75.6564, '.2f')
'75.66'
",2.6
"PEP 3105: print As a Function

The print statement becomes the print() function in Python 3.0.
Making print() a function makes it possible to replace the function
by doing def print(...) or importing a new function from somewhere else.
Python 2.6 has a __future__ import that removes print as language
syntax, letting you use the functional form instead.  For example:
>>> from __future__ import print_function
>>> print('# of entries', len(dictionary), file=sys.stderr)


The signature of the new function is:
def print(*args, sep=' ', end='\n', file=None)

The parameters are:


args: positional arguments whose values will be printed out.
sep: the separator, which will be printed between arguments.
end: the ending text, which will be printed after all of the
arguments have been output.
file: the file object to which the output will be sent.



See also

PEP 3105 - Make print a function
PEP written by Georg Brandl.",pep-3105-print-as-a-function,">>> from __future__ import print_function
>>> print('# of entries', len(dictionary), file=sys.stderr)
",2.6
"PEP 3110: Exception-Handling Changes

One error that Python programmers occasionally make
is writing the following code:
try:
    ...
except TypeError, ValueError:  # Wrong!
    ...


The author is probably trying to catch both TypeError and
ValueError exceptions, but this code actually does something
different: it will catch TypeError and bind the resulting
exception object to the local name ""ValueError"".  The
ValueError exception will not be caught at all.  The correct
code specifies a tuple of exceptions:
try:
    ...
except (TypeError, ValueError):
    ...


This error happens because the use of the comma here is ambiguous:
does it indicate two different nodes in the parse tree, or a single
node that’s a tuple?
Python 3.0 makes this unambiguous by replacing the comma with the word
“as”.  To catch an exception and store the exception object in the
variable exc, you must write:
try:
    ...
except TypeError as exc:
    ...


Python 3.0 will only support the use of “as”, and therefore interprets
the first example as catching two different exceptions.  Python 2.6
supports both the comma and “as”, so existing code will continue to
work.  We therefore suggest using “as” when writing new Python code
that will only be executed with 2.6.

See also

PEP 3110 - Catching Exceptions in Python 3000
PEP written and implemented by Collin Winter.",pep-3110-exception-handling-changes,"try:
    ...
except TypeError, ValueError:  # Wrong!
    ...
",2.6
,pep-3110-exception-handling-changes,"try:
    ...
except (TypeError, ValueError):
    ...
",2.6
,pep-3110-exception-handling-changes,"try:
    ...
except TypeError as exc:
    ...
",2.6
"PEP 3112: Byte Literals

Python 3.0 adopts Unicode as the language’s fundamental string type and
denotes 8-bit literals differently, either as b'string'
or using a bytes constructor.  For future compatibility,
Python 2.6 adds bytes as a synonym for the str type,
and it also supports the b'' notation.
The 2.6 str differs from 3.0’s bytes type in various
ways; most notably, the constructor is completely different.  In 3.0,
bytes([65, 66, 67]) is 3 elements long, containing the bytes
representing ABC; in 2.6, bytes([65, 66, 67]) returns the
12-byte string representing the str() of the list.
The primary use of bytes in 2.6 will be to write tests of
object type such as isinstance(x, bytes).  This will help the 2to3
converter, which can’t tell whether 2.x code intends strings to
contain either characters or 8-bit bytes; you can now
use either bytes or str to represent your intention
exactly, and the resulting code will also be correct in Python 3.0.
There’s also a __future__ import that causes all string literals
to become Unicode strings.  This means that \u escape sequences
can be used to include Unicode characters:
from __future__ import unicode_literals

s = ('\u751f\u3080\u304e\u3000\u751f\u3054'
     '\u3081\u3000\u751f\u305f\u307e\u3054')

print len(s)               # 12 Unicode characters


At the C level, Python 3.0 will rename the existing 8-bit
string type, called PyStringObject in Python 2.x,
to PyBytesObject.  Python 2.6 uses #define
to support using the names PyBytesObject(),
PyBytes_Check(), PyBytes_FromStringAndSize(),
and all the other functions and macros used with strings.
Instances of the bytes type are immutable just
as strings are.  A new bytearray type stores a mutable
sequence of bytes:
>>> bytearray([65, 66, 67])
bytearray(b'ABC')
>>> b = bytearray(u'\u21ef\u3244', 'utf-8')
>>> b
bytearray(b'\xe2\x87\xaf\xe3\x89\x84')
>>> b[0] = '\xe3'
>>> b
bytearray(b'\xe3\x87\xaf\xe3\x89\x84')
>>> unicode(str(b), 'utf-8')
u'\u31ef \u3244'


Byte arrays support most of the methods of string types, such as
startswith()/endswith(), find()/rfind(),
and some of the methods of lists, such as append(),
pop(),  and reverse().
>>> b = bytearray('ABC')
>>> b.append('d')
>>> b.append(ord('e'))
>>> b
bytearray(b'ABCde')


There’s also a corresponding C API, with
PyByteArray_FromObject(),
PyByteArray_FromStringAndSize(),
and various other functions.

See also

PEP 3112 - Bytes literals in Python 3000
PEP written by Jason Orendorff; backported to 2.6 by Christian Heimes.",pep-3112-byte-literals,"from __future__ import unicode_literals

s = ('\u751f\u3080\u304e\u3000\u751f\u3054'
     '\u3081\u3000\u751f\u305f\u307e\u3054')

print len(s)               # 12 Unicode characters
",2.6
,pep-3112-byte-literals,">>> bytearray([65, 66, 67])
bytearray(b'ABC')
>>> b = bytearray(u'\u21ef\u3244', 'utf-8')
>>> b
bytearray(b'\xe2\x87\xaf\xe3\x89\x84')
>>> b[0] = '\xe3'
>>> b
bytearray(b'\xe3\x87\xaf\xe3\x89\x84')
>>> unicode(str(b), 'utf-8')
u'\u31ef \u3244'
",2.6
,pep-3112-byte-literals,">>> b = bytearray('ABC')
>>> b.append('d')
>>> b.append(ord('e'))
>>> b
bytearray(b'ABCde')
",2.6
"PEP 3119: Abstract Base Classes

Some object-oriented languages such as Java support interfaces,
declaring that a class has a given set of methods or supports a given
access protocol.  Abstract Base Classes (or ABCs) are an equivalent
feature for Python. The ABC support consists of an abc module
containing a metaclass called ABCMeta, special handling of
this metaclass by the isinstance() and issubclass()
builtins, and a collection of basic ABCs that the Python developers
think will be widely useful.  Future versions of Python will probably
add more ABCs.
Let’s say you have a particular class and wish to know whether it supports
dictionary-style access.  The phrase “dictionary-style” is vague, however.
It probably means that accessing items with obj[1] works.
Does it imply that setting items with obj[2] = value works?
Or that the object will have keys(), values(), and items()
methods?  What about the iterative variants  such as iterkeys()?  copy()
and update()?  Iterating over the object with iter()?
The Python 2.6 collections module includes a number of
different ABCs that represent these distinctions.  Iterable
indicates that a class defines __iter__(), and
Container means the class defines a __contains__()
method and therefore supports x in y expressions.  The basic
dictionary interface of getting items, setting items, and
keys(), values(), and items(), is defined by the
MutableMapping ABC.
You can derive your own classes from a particular ABC
to indicate they support that ABC’s interface:
import collections

class Storage(collections.MutableMapping):
    ...


Alternatively, you could write the class without deriving from
the desired ABC and instead register the class by
calling the ABC’s register() method:
import collections

class Storage:
    ...

collections.MutableMapping.register(Storage)


For classes that you write, deriving from the ABC is probably clearer.
The register()  method is useful when you’ve written a new
ABC that can describe an existing type or class, or if you want
to declare that some third-party class implements an ABC.
For example, if you defined a PrintableType ABC,
it’s legal to do:
# Register Python's types
PrintableType.register(int)
PrintableType.register(float)
PrintableType.register(str)


Classes should obey the semantics specified by an ABC, but
Python can’t check this; it’s up to the class author to
understand the ABC’s requirements and to implement the code accordingly.
To check whether an object supports a particular interface, you can
now write:
def func(d):
    if not isinstance(d, collections.MutableMapping):
        raise ValueError(""Mapping object expected, not %r"" % d)


Don’t feel that you must now begin writing lots of checks as in the
above example.  Python has a strong tradition of duck-typing, where
explicit type-checking is never done and code simply calls methods on
an object, trusting that those methods will be there and raising an
exception if they aren’t.  Be judicious in checking for ABCs and only
do it where it’s absolutely necessary.
You can write your own ABCs by using abc.ABCMeta as the
metaclass in a class definition:
from abc import ABCMeta, abstractmethod

class Drawable():
    __metaclass__ = ABCMeta

    @abstractmethod
    def draw(self, x, y, scale=1.0):
        pass

    def draw_doubled(self, x, y):
        self.draw(x, y, scale=2.0)


class Square(Drawable):
    def draw(self, x, y, scale):
        ...


In the Drawable ABC above, the draw_doubled() method
renders the object at twice its size and can be implemented in terms
of other methods described in Drawable.  Classes implementing
this ABC therefore don’t need to provide their own implementation
of draw_doubled(), though they can do so.  An implementation
of draw() is necessary, though; the ABC can’t provide
a useful generic implementation.
You can apply the @abstractmethod decorator to methods such as
draw() that must be implemented; Python will then raise an
exception for classes that don’t define the method.
Note that the exception is only raised when you actually
try to create an instance of a subclass lacking the method:
>>> class Circle(Drawable):
...     pass
...
>>> c = Circle()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: Can't instantiate abstract class Circle with abstract methods draw
>>>


Abstract data attributes can be declared using the
@abstractproperty decorator:
from abc import abstractproperty
...

@abstractproperty
def readonly(self):
   return self._x


Subclasses must then define a readonly() property.

See also

PEP 3119 - Introducing Abstract Base Classes
PEP written by Guido van Rossum and Talin.
Implemented by Guido van Rossum.
Backported to 2.6 by Benjamin Aranguren, with Alex Martelli.",pep-3119-abstract-base-classes,"import collections

class Storage(collections.MutableMapping):
    ...
",2.6
,pep-3119-abstract-base-classes,"import collections

class Storage:
    ...

collections.MutableMapping.register(Storage)
",2.6
,pep-3119-abstract-base-classes,"# Register Python's types
PrintableType.register(int)
PrintableType.register(float)
PrintableType.register(str)
",2.6
,pep-3119-abstract-base-classes,"def func(d):
    if not isinstance(d, collections.MutableMapping):
        raise ValueError(""Mapping object expected, not %r"" % d)
",2.6
,pep-3119-abstract-base-classes,"from abc import ABCMeta, abstractmethod

class Drawable():
    __metaclass__ = ABCMeta

    @abstractmethod
    def draw(self, x, y, scale=1.0):
        pass

    def draw_doubled(self, x, y):
        self.draw(x, y, scale=2.0)


class Square(Drawable):
    def draw(self, x, y, scale):
        ...
",2.6
,pep-3119-abstract-base-classes,">>> class Circle(Drawable):
...     pass
...
>>> c = Circle()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: Can't instantiate abstract class Circle with abstract methods draw
>>>
",2.6
,pep-3119-abstract-base-classes,"from abc import abstractproperty
...

@abstractproperty
def readonly(self):
   return self._x
",2.6
"PEP 3127: Integer Literal Support and Syntax

Python 3.0 changes the syntax for octal (base-8) integer literals,
prefixing them with “0o” or “0O” instead of a leading zero, and adds
support for binary (base-2) integer literals, signalled by a “0b” or
“0B” prefix.
Python 2.6 doesn’t drop support for a leading 0 signalling
an octal number, but it does add support for “0o” and “0b”:
>>> 0o21, 2*8 + 1
(17, 17)
>>> 0b101111
47


The oct() builtin still returns numbers
prefixed with a leading zero, and a new bin()
builtin returns the binary representation for a number:
>>> oct(42)
'052'
>>> future_builtins.oct(42)
'0o52'
>>> bin(173)
'0b10101101'


The int() and long() builtins will now accept the “0o”
and “0b” prefixes when base-8 or base-2 are requested, or when the
base argument is zero (signalling that the base used should be
determined from the string):
>>> int ('0o52', 0)
42
>>> int('1101', 2)
13
>>> int('0b1101', 2)
13
>>> int('0b1101', 0)
13



See also

PEP 3127 - Integer Literal Support and Syntax
PEP written by Patrick Maupin; backported to 2.6 by
Eric Smith.",pep-3127-integer-literal-support-and-syntax,">>> 0o21, 2*8 + 1
(17, 17)
>>> 0b101111
47
",2.6
,pep-3127-integer-literal-support-and-syntax,">>> oct(42)
'052'
>>> future_builtins.oct(42)
'0o52'
>>> bin(173)
'0b10101101'
",2.6
,pep-3127-integer-literal-support-and-syntax,">>> int ('0o52', 0)
42
>>> int('1101', 2)
13
>>> int('0b1101', 2)
13
>>> int('0b1101', 0)
13
",2.6
"PEP 3129: Class Decorators

Decorators have been extended from functions to classes.  It’s now legal to
write:
@foo
@bar
class A:
  pass


This is equivalent to:
class A:
  pass

A = foo(bar(A))



See also

PEP 3129 - Class Decorators
PEP written by Collin Winter.",pep-3129-class-decorators,"@foo
@bar
class A:
  pass
",2.6
,pep-3129-class-decorators,"class A:
  pass

A = foo(bar(A))
",2.6
"The fractions Module

To fill out the hierarchy of numeric types, the fractions
module provides a rational-number class.  Rational numbers store their
values as a numerator and denominator forming a fraction, and can
exactly represent numbers such as 2/3 that floating-point numbers
can only approximate.
The Fraction constructor takes two Integral values
that will be the numerator and denominator of the resulting fraction.
>>> from fractions import Fraction
>>> a = Fraction(2, 3)
>>> b = Fraction(2, 5)
>>> float(a), float(b)
(0.66666666666666663, 0.40000000000000002)
>>> a+b
Fraction(16, 15)
>>> a/b
Fraction(5, 3)


For converting floating-point numbers to rationals,
the float type now has an as_integer_ratio() method that returns
the numerator and denominator for a fraction that evaluates to the same
floating-point value:
>>> (2.5) .as_integer_ratio()
(5, 2)
>>> (3.1415) .as_integer_ratio()
(7074029114692207L, 2251799813685248L)
>>> (1./3) .as_integer_ratio()
(6004799503160661L, 18014398509481984L)


Note that values that can only be approximated by floating-point
numbers, such as 1./3, are not simplified to the number being
approximated; the fraction attempts to match the floating-point value
exactly.
The fractions module is based upon an implementation by Sjoerd
Mullender that was in Python’s Demo/classes/ directory for a
long time.  This implementation was significantly updated by Jeffrey
Yasskin.",pep-3141-a-type-hierarchy-for-numbers - the-fractions-module,">>> from fractions import Fraction
>>> a = Fraction(2, 3)
>>> b = Fraction(2, 5)
>>> float(a), float(b)
(0.66666666666666663, 0.40000000000000002)
>>> a+b
Fraction(16, 15)
>>> a/b
Fraction(5, 3)
",2.6
,pep-3141-a-type-hierarchy-for-numbers - the-fractions-module,">>> (2.5) .as_integer_ratio()
(5, 2)
>>> (3.1415) .as_integer_ratio()
(7074029114692207L, 2251799813685248L)
>>> (1./3) .as_integer_ratio()
(6004799503160661L, 18014398509481984L)
",2.6
"The ast module

The ast module provides an Abstract Syntax Tree
representation of Python code, and Armin Ronacher
contributed a set of helper functions that perform a variety of
common tasks.  These will be useful for HTML templating
packages, code analyzers, and similar tools that process
Python code.
The parse() function takes an expression and returns an AST.
The dump() function outputs a representation of a tree, suitable
for debugging:
import ast

t = ast.parse(""""""
d = {}
for i in 'abcdefghijklm':
    d[i + i] = ord(i) - ord('a') + 1
print d
"""""")
print ast.dump(t)


This outputs a deeply nested tree:
Module(body=[
  Assign(targets=[
    Name(id='d', ctx=Store())
   ], value=Dict(keys=[], values=[]))
  For(target=Name(id='i', ctx=Store()),
      iter=Str(s='abcdefghijklm'), body=[
    Assign(targets=[
      Subscript(value=
        Name(id='d', ctx=Load()),
          slice=
          Index(value=
            BinOp(left=Name(id='i', ctx=Load()), op=Add(),
             right=Name(id='i', ctx=Load()))), ctx=Store())
     ], value=
     BinOp(left=
      BinOp(left=
       Call(func=
        Name(id='ord', ctx=Load()), args=[
          Name(id='i', ctx=Load())
         ], keywords=[], starargs=None, kwargs=None),
       op=Sub(), right=Call(func=
        Name(id='ord', ctx=Load()), args=[
          Str(s='a')
         ], keywords=[], starargs=None, kwargs=None)),
       op=Add(), right=Num(n=1)))
    ], orelse=[])
   Print(dest=None, values=[
     Name(id='d', ctx=Load())
   ], nl=True)
 ])

The literal_eval() method takes a string or an AST
representing a literal expression, parses and evaluates it, and
returns the resulting value.  A literal expression is a Python
expression containing only strings, numbers, dictionaries,
etc. but no statements or function calls.  If you need to
evaluate an expression but cannot accept the security risk of using an
eval() call, literal_eval() will handle it safely:
>>> literal = '(""a"", ""b"", {2:4, 3:8, 1:2})'
>>> print ast.literal_eval(literal)
('a', 'b', {1: 2, 2: 4, 3: 8})
>>> print ast.literal_eval('""a"" + ""b""')
Traceback (most recent call last):
  ...
ValueError: malformed string


The module also includes NodeVisitor and
NodeTransformer classes for traversing and modifying an AST,
and functions for common transformations such as changing line
numbers.",new-and-improved-modules - the-ast-module,"import ast

t = ast.parse(""""""
d = {}
for i in 'abcdefghijklm':
    d[i + i] = ord(i) - ord('a') + 1
print d
"""""")
print ast.dump(t)
",2.6
,new-and-improved-modules - the-ast-module,">>> literal = '(""a"", ""b"", {2:4, 3:8, 1:2})'
>>> print ast.literal_eval(literal)
('a', 'b', {1: 2, 2: 4, 3: 8})
>>> print ast.literal_eval('""a"" + ""b""')
Traceback (most recent call last):
  ...
ValueError: malformed string
",2.6
"The json module: JavaScript Object Notation

The new json module supports the encoding and decoding of Python types in
JSON (Javascript Object Notation). JSON is a lightweight interchange format
often used in web applications. For more information about JSON, see
http://www.json.org.
json comes with support for decoding and encoding most built-in Python
types. The following example encodes and decodes a dictionary:
>>> import json
>>> data = {""spam"" : ""foo"", ""parrot"" : 42}
>>> in_json = json.dumps(data) # Encode the data
>>> in_json
'{""parrot"": 42, ""spam"": ""foo""}'
>>> json.loads(in_json) # Decode into a Python object
{""spam"" : ""foo"", ""parrot"" : 42}


It’s also possible to write your own decoders and encoders to support
more types. Pretty-printing of the JSON strings is also supported.
json (originally called simplejson) was written by Bob
Ippolito.",new-and-improved-modules - the-json-module-javascript-object-notation,">>> import json
>>> data = {""spam"" : ""foo"", ""parrot"" : 42}
>>> in_json = json.dumps(data) # Encode the data
>>> in_json
'{""parrot"": 42, ""spam"": ""foo""}'
>>> json.loads(in_json) # Decode into a Python object
{""spam"" : ""foo"", ""parrot"" : 42}
",2.6
"The plistlib module: A Property-List Parser

The .plist format is commonly used on Mac OS X to
store basic data types (numbers, strings, lists,
and dictionaries) by serializing them into an XML-based format.
It resembles the XML-RPC serialization of data types.
Despite being primarily used on Mac OS X, the format
has nothing Mac-specific about it and the Python implementation works
on any platform that Python supports, so the plistlib module
has been promoted to the standard library.
Using the module is simple:
import sys
import plistlib
import datetime

# Create data structure
data_struct = dict(lastAccessed=datetime.datetime.now(),
                   version=1,
                   categories=('Personal','Shared','Private'))

# Create string containing XML.
plist_str = plistlib.writePlistToString(data_struct)
new_struct = plistlib.readPlistFromString(plist_str)
print data_struct
print new_struct

# Write data structure to a file and read it back.
plistlib.writePlist(data_struct, '/tmp/customizations.plist')
new_struct = plistlib.readPlist('/tmp/customizations.plist')

# read/writePlist accepts file-like objects as well as paths.
plistlib.writePlist(data_struct, sys.stdout)",new-and-improved-modules - the-plistlib-module-a-property-list-parser,"import sys
import plistlib
import datetime

# Create data structure
data_struct = dict(lastAccessed=datetime.datetime.now(),
                   version=1,
                   categories=('Personal','Shared','Private'))

# Create string containing XML.
plist_str = plistlib.writePlistToString(data_struct)
new_struct = plistlib.readPlistFromString(plist_str)
print data_struct
print new_struct

# Write data structure to a file and read it back.
plistlib.writePlist(data_struct, '/tmp/customizations.plist')
new_struct = plistlib.readPlist('/tmp/customizations.plist')

# read/writePlist accepts file-like objects as well as paths.
plistlib.writePlist(data_struct, sys.stdout)
",2.6
"PEP 372: Adding an Ordered Dictionary to collections

Regular Python dictionaries iterate over key/value pairs in arbitrary order.
Over the years, a number of authors have written alternative implementations
that remember the order that the keys were originally inserted.  Based on
the experiences from those implementations, 2.7 introduces a new
OrderedDict class in the collections module.
The OrderedDict API provides the same interface as regular
dictionaries but iterates over keys and values in a guaranteed order
depending on when a key was first inserted:
>>> from collections import OrderedDict
>>> d = OrderedDict([('first', 1),
...                  ('second', 2),
...                  ('third', 3)])
>>> d.items()
[('first', 1), ('second', 2), ('third', 3)]


If a new entry overwrites an existing entry, the original insertion
position is left unchanged:
>>> d['second'] = 4
>>> d.items()
[('first', 1), ('second', 4), ('third', 3)]


Deleting an entry and reinserting it will move it to the end:
>>> del d['second']
>>> d['second'] = 5
>>> d.items()
[('first', 1), ('third', 3), ('second', 5)]


The popitem() method has an optional last
argument that defaults to True.  If last is true, the most recently
added key is returned and removed; if it’s false, the
oldest key is selected:
>>> od = OrderedDict([(x,0) for x in range(20)])
>>> od.popitem()
(19, 0)
>>> od.popitem()
(18, 0)
>>> od.popitem(last=False)
(0, 0)
>>> od.popitem(last=False)
(1, 0)


Comparing two ordered dictionaries checks both the keys and values,
and requires that the insertion order was the same:
>>> od1 = OrderedDict([('first', 1),
...                    ('second', 2),
...                    ('third', 3)])
>>> od2 = OrderedDict([('third', 3),
...                    ('first', 1),
...                    ('second', 2)])
>>> od1 == od2
False
>>> # Move 'third' key to the end
>>> del od2['third']; od2['third'] = 3
>>> od1 == od2
True


Comparing an OrderedDict with a regular dictionary
ignores the insertion order and just compares the keys and values.
How does the OrderedDict work?  It maintains a
doubly-linked list of keys, appending new keys to the list as they’re inserted.
A secondary dictionary maps keys to their corresponding list node, so
deletion doesn’t have to traverse the entire linked list and therefore
remains O(1).
The standard library now supports use of ordered dictionaries in several
modules.

The ConfigParser module uses them by default, meaning that
configuration files can now be read, modified, and then written back
in their original order.
The _asdict() method for
collections.namedtuple() now returns an ordered dictionary with the
values appearing in the same order as the underlying tuple indices.
The json module’s JSONDecoder class
constructor was extended with an object_pairs_hook parameter to
allow OrderedDict instances to be built by the decoder.
Support was also added for third-party tools like
PyYAML.


See also

PEP 372 - Adding an ordered dictionary to collectionsPEP written by Armin Ronacher and Raymond Hettinger;
implemented by Raymond Hettinger.",pep-372-adding-an-ordered-dictionary-to-collections,">>> from collections import OrderedDict
>>> d = OrderedDict([('first', 1),
...                  ('second', 2),
...                  ('third', 3)])
>>> d.items()
[('first', 1), ('second', 2), ('third', 3)]
",2.7
,pep-372-adding-an-ordered-dictionary-to-collections,">>> d['second'] = 4
>>> d.items()
[('first', 1), ('second', 4), ('third', 3)]
",2.7
,pep-372-adding-an-ordered-dictionary-to-collections,">>> del d['second']
>>> d['second'] = 5
>>> d.items()
[('first', 1), ('third', 3), ('second', 5)]
",2.7
,pep-372-adding-an-ordered-dictionary-to-collections,">>> od = OrderedDict([(x,0) for x in range(20)])
>>> od.popitem()
(19, 0)
>>> od.popitem()
(18, 0)
>>> od.popitem(last=False)
(0, 0)
>>> od.popitem(last=False)
(1, 0)
",2.7
,pep-372-adding-an-ordered-dictionary-to-collections,">>> od1 = OrderedDict([('first', 1),
...                    ('second', 2),
...                    ('third', 3)])
>>> od2 = OrderedDict([('third', 3),
...                    ('first', 1),
...                    ('second', 2)])
>>> od1 == od2
False
>>> # Move 'third' key to the end
>>> del od2['third']; od2['third'] = 3
>>> od1 == od2
True
",2.7
"PEP 378: Format Specifier for Thousands Separator

To make program output more readable, it can be useful to add
separators to large numbers, rendering them as
18,446,744,073,709,551,616 instead of 18446744073709551616.
The fully general solution for doing this is the locale module,
which can use different separators (“,” in North America, “.” in
Europe) and different grouping sizes, but locale is complicated
to use and unsuitable for multi-threaded applications where different
threads are producing output for different locales.
Therefore, a simple comma-grouping mechanism has been added to the
mini-language used by the str.format() method.  When
formatting a floating-point number, simply include a comma between the
width and the precision:
>>> '{:20,.2f}'.format(18446744073709551616.0)
'18,446,744,073,709,551,616.00'


When formatting an integer, include the comma after the width:
>>> '{:20,d}'.format(18446744073709551616)
'18,446,744,073,709,551,616'


This mechanism is not adaptable at all; commas are always used as the
separator and the grouping is always into three-digit groups.  The
comma-formatting mechanism isn’t as general as the locale
module, but it’s easier to use.

See also

PEP 378 - Format Specifier for Thousands SeparatorPEP written by Raymond Hettinger; implemented by Eric Smith.",pep-378-format-specifier-for-thousands-separator,">>> '{:20,.2f}'.format(18446744073709551616.0)
'18,446,744,073,709,551,616.00'
",2.7
,pep-378-format-specifier-for-thousands-separator,">>> '{:20,d}'.format(18446744073709551616)
'18,446,744,073,709,551,616'
",2.7
"PEP 389: The argparse Module for Parsing Command Lines

The argparse module for parsing command-line arguments was
added as a more powerful replacement for the
optparse module.
This means Python now supports three different modules for parsing
command-line arguments: getopt, optparse, and
argparse.  The getopt module closely resembles the C
library’s getopt() function, so it remains useful if you’re writing a
Python prototype that will eventually be rewritten in C.
optparse becomes redundant, but there are no plans to remove it
because there are many scripts still using it, and there’s no
automated way to update these scripts.  (Making the argparse
API consistent with optparse’s interface was discussed but
rejected as too messy and difficult.)
In short, if you’re writing a new script and don’t need to worry
about compatibility with earlier versions of Python, use
argparse instead of optparse.
Here’s an example:
import argparse

parser = argparse.ArgumentParser(description='Command-line example.')

# Add optional switches
parser.add_argument('-v', action='store_true', dest='is_verbose',
                    help='produce verbose output')
parser.add_argument('-o', action='store', dest='output',
                    metavar='FILE',
                    help='direct output to FILE instead of stdout')
parser.add_argument('-C', action='store', type=int, dest='context',
                    metavar='NUM', default=0,
                    help='display NUM lines of added context')

# Allow any number of additional arguments.
parser.add_argument(nargs='*', action='store', dest='inputs',
                    help='input filenames (default is stdin)')

args = parser.parse_args()
print args.__dict__


Unless you override it, -h and --help switches
are automatically added, and produce neatly formatted output:
-> ./python.exe argparse-example.py --help
usage: argparse-example.py [-h] [-v] [-o FILE] [-C NUM] [inputs [inputs ...]]

Command-line example.

positional arguments:
  inputs      input filenames (default is stdin)

optional arguments:
  -h, --help  show this help message and exit
  -v          produce verbose output
  -o FILE     direct output to FILE instead of stdout
  -C NUM      display NUM lines of added context


As with optparse, the command-line switches and arguments
are returned as an object with attributes named by the dest parameters:
-> ./python.exe argparse-example.py -v
{'output': None,
 'is_verbose': True,
 'context': 0,
 'inputs': []}

-> ./python.exe argparse-example.py -v -o /tmp/output -C 4 file1 file2
{'output': '/tmp/output',
 'is_verbose': True,
 'context': 4,
 'inputs': ['file1', 'file2']}


argparse has much fancier validation than optparse; you
can specify an exact number of arguments as an integer, 0 or more
arguments by passing '*', 1 or more by passing '+', or an
optional argument with '?'.  A top-level parser can contain
sub-parsers to define subcommands that have different sets of
switches, as in svn commit, svn checkout, etc.  You can
specify an argument’s type as FileType, which will
automatically open files for you and understands that '-' means
standard input or output.

See also

argparse documentationThe documentation page of the argparse module.

Upgrading optparse codePart of the Python documentation, describing how to convert
code that uses optparse.

PEP 389 - argparse - New Command Line Parsing ModulePEP written and implemented by Steven Bethard.",pep-389-the-argparse-module-for-parsing-command-lines,"import argparse

parser = argparse.ArgumentParser(description='Command-line example.')

# Add optional switches
parser.add_argument('-v', action='store_true', dest='is_verbose',
                    help='produce verbose output')
parser.add_argument('-o', action='store', dest='output',
                    metavar='FILE',
                    help='direct output to FILE instead of stdout')
parser.add_argument('-C', action='store', type=int, dest='context',
                    metavar='NUM', default=0,
                    help='display NUM lines of added context')

# Allow any number of additional arguments.
parser.add_argument(nargs='*', action='store', dest='inputs',
                    help='input filenames (default is stdin)')

args = parser.parse_args()
print args.__dict__
",2.7
,pep-389-the-argparse-module-for-parsing-command-lines,"-> ./python.exe argparse-example.py --help
usage: argparse-example.py [-h] [-v] [-o FILE] [-C NUM] [inputs [inputs ...]]

Command-line example.

positional arguments:
  inputs      input filenames (default is stdin)

optional arguments:
  -h, --help  show this help message and exit
  -v          produce verbose output
  -o FILE     direct output to FILE instead of stdout
  -C NUM      display NUM lines of added context
",2.7
,pep-389-the-argparse-module-for-parsing-command-lines,"-> ./python.exe argparse-example.py -v
{'output': None,
 'is_verbose': True,
 'context': 0,
 'inputs': []}

-> ./python.exe argparse-example.py -v -o /tmp/output -C 4 file1 file2
{'output': '/tmp/output',
 'is_verbose': True,
 'context': 4,
 'inputs': ['file1', 'file2']}
",2.7
"PEP 391: Dictionary-Based Configuration For Logging

The logging module is very flexible; applications can define
a tree of logging subsystems, and each logger in this tree can filter
out certain messages, format them differently, and direct messages to
a varying number of handlers.
All this flexibility can require a lot of configuration.  You can
write Python statements to create objects and set their properties,
but a complex set-up requires verbose but boring code.
logging also supports a fileConfig()
function that parses a file, but the file format doesn’t support
configuring filters, and it’s messier to generate programmatically.
Python 2.7 adds a dictConfig() function that
uses a dictionary to configure logging.  There are many ways to
produce a dictionary from different sources: construct one with code;
parse a file containing JSON; or use a YAML parsing library if one is
installed.  For more information see Configuration functions.
The following example configures two loggers, the root logger and a
logger named “network”.  Messages sent to the root logger will be
sent to the system log using the syslog protocol, and messages
to the “network” logger will be written to a network.log file
that will be rotated once the log reaches 1MB.
import logging
import logging.config

configdict = {
 'version': 1,    # Configuration schema in use; must be 1 for now
 'formatters': {
     'standard': {
         'format': ('%(asctime)s %(name)-15s '
                    '%(levelname)-8s %(message)s')}},

 'handlers': {'netlog': {'backupCount': 10,
                     'class': 'logging.handlers.RotatingFileHandler',
                     'filename': '/logs/network.log',
                     'formatter': 'standard',
                     'level': 'INFO',
                     'maxBytes': 1000000},
              'syslog': {'class': 'logging.handlers.SysLogHandler',
                         'formatter': 'standard',
                         'level': 'ERROR'}},

 # Specify all the subordinate loggers
 'loggers': {
             'network': {
                         'handlers': ['netlog']
             }
 },
 # Specify properties of the root logger
 'root': {
          'handlers': ['syslog']
 },
}

# Set up configuration
logging.config.dictConfig(configdict)

# As an example, log two error messages
logger = logging.getLogger('/')
logger.error('Database not found')

netlogger = logging.getLogger('network')
netlogger.error('Connection failed')


Three smaller enhancements to the logging module, all
implemented by Vinay Sajip, are:

The SysLogHandler class now supports
syslogging over TCP.  The constructor has a socktype parameter
giving the type of socket to use, either socket.SOCK_DGRAM
for UDP or socket.SOCK_STREAM for TCP.  The default
protocol remains UDP.
Logger instances gained a getChild()
method that retrieves a descendant logger using a relative path.
For example, once you retrieve a logger by doing log = getLogger('app'),
calling log.getChild('network.listen') is equivalent to
getLogger('app.network.listen').
The LoggerAdapter class gained an
isEnabledFor() method that takes a
level and returns whether the underlying logger would
process a message of that level of importance.


See also

PEP 391 - Dictionary-Based Configuration For LoggingPEP written and implemented by Vinay Sajip.",pep-391-dictionary-based-configuration-for-logging,"import logging
import logging.config

configdict = {
 'version': 1,    # Configuration schema in use; must be 1 for now
 'formatters': {
     'standard': {
         'format': ('%(asctime)s %(name)-15s '
                    '%(levelname)-8s %(message)s')}},

 'handlers': {'netlog': {'backupCount': 10,
                     'class': 'logging.handlers.RotatingFileHandler',
                     'filename': '/logs/network.log',
                     'formatter': 'standard',
                     'level': 'INFO',
                     'maxBytes': 1000000},
              'syslog': {'class': 'logging.handlers.SysLogHandler',
                         'formatter': 'standard',
                         'level': 'ERROR'}},

 # Specify all the subordinate loggers
 'loggers': {
             'network': {
                         'handlers': ['netlog']
             }
 },
 # Specify properties of the root logger
 'root': {
          'handlers': ['syslog']
 },
}

# Set up configuration
logging.config.dictConfig(configdict)

# As an example, log two error messages
logger = logging.getLogger('/')
logger.error('Database not found')

netlogger = logging.getLogger('network')
netlogger.error('Connection failed')
",2.7
"PEP 3106: Dictionary Views

The dictionary methods keys(), values(), and
items() are different in Python 3.x.  They return an object
called a view instead of a fully materialized list.
It’s not possible to change the return values of keys(),
values(), and items() in Python 2.7 because
too much code would break.  Instead the 3.x versions were added
under the new names viewkeys(), viewvalues(),
and viewitems().
>>> d = dict((i*10, chr(65+i)) for i in range(26))
>>> d
{0: 'A', 130: 'N', 10: 'B', 140: 'O', 20: ..., 250: 'Z'}
>>> d.viewkeys()
dict_keys([0, 130, 10, 140, 20, 150, 30, ..., 250])


Views can be iterated over, but the key and item views also behave
like sets.  The & operator performs intersection, and |
performs a union:
>>> d1 = dict((i*10, chr(65+i)) for i in range(26))
>>> d2 = dict((i**.5, i) for i in range(1000))
>>> d1.viewkeys() & d2.viewkeys()
set([0.0, 10.0, 20.0, 30.0])
>>> d1.viewkeys() | range(0, 30)
set([0, 1, 130, 3, 4, 5, 6, ..., 120, 250])


The view keeps track of the dictionary and its contents change as the
dictionary is modified:
>>> vk = d.viewkeys()
>>> vk
dict_keys([0, 130, 10, ..., 250])
>>> d[260] = '&'
>>> vk
dict_keys([0, 130, 260, 10, ..., 250])


However, note that you can’t add or remove keys while you’re iterating
over the view:
>>> for k in vk:
...     d[k*2] = k
...
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
RuntimeError: dictionary changed size during iteration


You can use the view methods in Python 2.x code, and the 2to3
converter will change them to the standard keys(),
values(), and items() methods.

See also

PEP 3106 - Revamping dict.keys(), .values() and .items()PEP written by Guido van Rossum.
Backported to 2.7 by Alexandre Vassalotti; bpo-1967.",pep-3106-dictionary-views,">>> d = dict((i*10, chr(65+i)) for i in range(26))
>>> d
{0: 'A', 130: 'N', 10: 'B', 140: 'O', 20: ..., 250: 'Z'}
>>> d.viewkeys()
dict_keys([0, 130, 10, 140, 20, 150, 30, ..., 250])
",2.7
,pep-3106-dictionary-views,">>> d1 = dict((i*10, chr(65+i)) for i in range(26))
>>> d2 = dict((i**.5, i) for i in range(1000))
>>> d1.viewkeys() & d2.viewkeys()
set([0.0, 10.0, 20.0, 30.0])
>>> d1.viewkeys() | range(0, 30)
set([0, 1, 130, 3, 4, 5, 6, ..., 120, 250])
",2.7
,pep-3106-dictionary-views,">>> vk = d.viewkeys()
>>> vk
dict_keys([0, 130, 10, ..., 250])
>>> d[260] = '&'
>>> vk
dict_keys([0, 130, 260, 10, ..., 250])
",2.7
,pep-3106-dictionary-views,">>> for k in vk:
...     d[k*2] = k
...
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
RuntimeError: dictionary changed size during iteration
",2.7
"PEP 3137: The memoryview Object

The memoryview object provides a view of another object’s
memory content that matches the bytes type’s interface.
>>> import string
>>> m = memoryview(string.letters)
>>> m
<memory at 0x37f850>
>>> len(m)           # Returns length of underlying object
52
>>> m[0], m[25], m[26]   # Indexing returns one byte
('a', 'z', 'A')
>>> m2 = m[0:26]         # Slicing returns another memoryview
>>> m2
<memory at 0x37f080>


The content of the view can be converted to a string of bytes or
a list of integers:
>>> m2.tobytes()
'abcdefghijklmnopqrstuvwxyz'
>>> m2.tolist()
[97, 98, 99, 100, 101, 102, 103, ... 121, 122]
>>>


memoryview objects allow modifying the underlying object if
it’s a mutable object.
>>> m2[0] = 75
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: cannot modify read-only memory
>>> b = bytearray(string.letters)  # Creating a mutable object
>>> b
bytearray(b'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ')
>>> mb = memoryview(b)
>>> mb[0] = '*'         # Assign to view, changing the bytearray.
>>> b[0:5]              # The bytearray has been changed.
bytearray(b'*bcde')
>>>



See also

PEP 3137 - Immutable Bytes and Mutable BufferPEP written by Guido van Rossum.
Implemented by Travis Oliphant, Antoine Pitrou and others.
Backported to 2.7 by Antoine Pitrou; bpo-2396.",pep-3137-the-memoryview-object,">>> import string
>>> m = memoryview(string.letters)
>>> m
<memory at 0x37f850>
>>> len(m)           # Returns length of underlying object
52
>>> m[0], m[25], m[26]   # Indexing returns one byte
('a', 'z', 'A')
>>> m2 = m[0:26]         # Slicing returns another memoryview
>>> m2
<memory at 0x37f080>
",2.7
,pep-3137-the-memoryview-object,">>> m2.tobytes()
'abcdefghijklmnopqrstuvwxyz'
>>> m2.tolist()
[97, 98, 99, 100, 101, 102, 103, ... 121, 122]
>>>
",2.7
,pep-3137-the-memoryview-object,">>> m2[0] = 75
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: cannot modify read-only memory
>>> b = bytearray(string.letters)  # Creating a mutable object
>>> b
bytearray(b'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ')
>>> mb = memoryview(b)
>>> mb[0] = '*'         # Assign to view, changing the bytearray.
>>> b[0:5]              # The bytearray has been changed.
bytearray(b'*bcde')
>>>
",2.7
"Interpreter Changes

A new environment variable, PYTHONWARNINGS,
allows controlling warnings.  It should be set to a string
containing warning settings, equivalent to those
used with the -W switch, separated by commas.
(Contributed by Brian Curtin; bpo-7301.)
For example, the following setting will print warnings every time
they occur, but turn warnings from the Cookie module into an
error.  (The exact syntax for setting an environment variable varies
across operating systems and shells.)
export PYTHONWARNINGS=all,error:::Cookie:0",other-language-changes - interpreter-changes,"export PYTHONWARNINGS=all,error:::Cookie:0
",2.7
"Optimizations

Several performance enhancements have been added:

A new opcode was added to perform the initial setup for
with statements, looking up the __enter__() and
__exit__() methods.  (Contributed by Benjamin Peterson.)
The garbage collector now performs better for one common usage
pattern: when many objects are being allocated without deallocating
any of them.  This would previously take quadratic
time for garbage collection, but now the number of full garbage collections
is reduced as the number of objects on the heap grows.
The new logic only performs a full garbage collection pass when
the middle generation has been collected 10 times and when the
number of survivor objects from the middle generation exceeds 10% of
the number of objects in the oldest generation.  (Suggested by Martin
von Löwis and implemented by Antoine Pitrou; bpo-4074.)
The garbage collector tries to avoid tracking simple containers
which can’t be part of a cycle. In Python 2.7, this is now true for
tuples and dicts containing atomic types (such as ints, strings,
etc.). Transitively, a dict containing tuples of atomic types won’t
be tracked either. This helps reduce the cost of each
garbage collection by decreasing the number of objects to be
considered and traversed by the collector.
(Contributed by Antoine Pitrou; bpo-4688.)
Long integers are now stored internally either in base 2**15 or in base
2**30, the base being determined at build time.  Previously, they
were always stored in base 2**15.  Using base 2**30 gives
significant performance improvements on 64-bit machines, but
benchmark results on 32-bit machines have been mixed.  Therefore,
the default is to use base 2**30 on 64-bit machines and base 2**15
on 32-bit machines; on Unix, there’s a new configure option
--enable-big-digits that can be used to override this default.
Apart from the performance improvements this change should be
invisible to end users, with one exception: for testing and
debugging purposes there’s a new structseq sys.long_info that
provides information about the internal format, giving the number of
bits per digit and the size in bytes of the C type used to store
each digit:
>>> import sys
>>> sys.long_info
sys.long_info(bits_per_digit=30, sizeof_digit=4)


(Contributed by Mark Dickinson; bpo-4258.)
Another set of changes made long objects a few bytes smaller: 2 bytes
smaller on 32-bit systems and 6 bytes on 64-bit.
(Contributed by Mark Dickinson; bpo-5260.)

The division algorithm for long integers has been made faster
by tightening the inner loop, doing shifts instead of multiplications,
and fixing an unnecessary extra iteration.
Various benchmarks show speedups of between 50% and 150% for long
integer divisions and modulo operations.
(Contributed by Mark Dickinson; bpo-5512.)
Bitwise operations are also significantly faster (initial patch by
Gregory Smith; bpo-1087418).
The implementation of % checks for the left-side operand being
a Python string and special-cases it; this results in a 1–3%
performance increase for applications that frequently use %
with strings, such as templating libraries.
(Implemented by Collin Winter; bpo-5176.)
List comprehensions with an if condition are compiled into
faster bytecode.  (Patch by Antoine Pitrou, back-ported to 2.7
by Jeffrey Yasskin; bpo-4715.)
Converting an integer or long integer to a decimal string was made
faster by special-casing base 10 instead of using a generalized
conversion function that supports arbitrary bases.
(Patch by Gawain Bolton; bpo-6713.)
The split(), replace(), rindex(),
rpartition(), and rsplit() methods of string-like types
(strings, Unicode strings, and bytearray objects) now use a
fast reverse-search algorithm instead of a character-by-character
scan.  This is sometimes faster by a factor of 10.  (Added by
Florent Xicluna; bpo-7462 and bpo-7622.)
The pickle and cPickle modules now automatically
intern the strings used for attribute names, reducing memory usage
of the objects resulting from unpickling.  (Contributed by Jake
McGuire; bpo-5084.)
The cPickle module now special-cases dictionaries,
nearly halving the time required to pickle them.
(Contributed by Collin Winter; bpo-5670.)",other-language-changes - optimizations,">>> import sys
>>> sys.long_info
sys.long_info(bits_per_digit=30, sizeof_digit=4)
",2.7
"New module: importlib

Python 3.1 includes the importlib package, a re-implementation
of the logic underlying Python’s import statement.
importlib is useful for implementors of Python interpreters and
to users who wish to write new importers that can participate in the
import process.  Python 2.7 doesn’t contain the complete
importlib package, but instead has a tiny subset that contains
a single function, import_module().
import_module(name, package=None) imports a module.  name is
a string containing the module or package’s name.  It’s possible to do
relative imports by providing a string that begins with a .
character, such as ..utils.errors.  For relative imports, the
package argument must be provided and is the name of the package that
will be used as the anchor for
the relative import.  import_module() both inserts the imported
module into sys.modules and returns the module object.
Here are some examples:
>>> from importlib import import_module
>>> anydbm = import_module('anydbm')  # Standard absolute import
>>> anydbm
<module 'anydbm' from '/p/python/Lib/anydbm.py'>
>>> # Relative import
>>> file_util = import_module('..file_util', 'distutils.command')
>>> file_util
<module 'distutils.file_util' from '/python/Lib/distutils/file_util.pyc'>


importlib was implemented by Brett Cannon and introduced in
Python 3.1.",new-and-improved-modules - new-module-importlib,">>> from importlib import import_module
>>> anydbm = import_module('anydbm')  # Standard absolute import
>>> anydbm
<module 'anydbm' from '/p/python/Lib/anydbm.py'>
>>> # Relative import
>>> file_util = import_module('..file_util', 'distutils.command')
>>> file_util
<module 'distutils.file_util' from '/python/Lib/distutils/file_util.pyc'>
",2.7
"Updated module: unittest

The unittest module was greatly enhanced; many
new features were added.  Most of these features were implemented
by Michael Foord, unless otherwise noted.  The enhanced version of
the module is downloadable separately for use with Python versions 2.4 to 2.6,
packaged as the unittest2 package, from
https://pypi.org/project/unittest2.
When used from the command line, the module can automatically discover
tests.  It’s not as fancy as py.test or
nose, but provides a
simple way to run tests kept within a set of package directories.  For example,
the following command will search the test/ subdirectory for
any importable test files named test*.py:
python -m unittest discover -s test


Consult the unittest module documentation for more details.
(Developed in bpo-6001.)
The main() function supports some other new options:

-b or --buffer will buffer the standard output
and standard error streams during each test.  If the test passes,
any resulting output will be discarded; on failure, the buffered
output will be displayed.
-c or --catch will cause the control-C interrupt
to be handled more gracefully.  Instead of interrupting the test
process immediately, the currently running test will be completed
and then the partial results up to the interruption will be reported.
If you’re impatient, a second press of control-C will cause an immediate
interruption.
This control-C handler tries to avoid causing problems when the code
being tested or the tests being run have defined a signal handler of
their own, by noticing that a signal handler was already set and
calling it.  If this doesn’t work for you, there’s a
removeHandler() decorator that can be used to mark tests that
should have the control-C handling disabled.

-f or --failfast makes
test execution stop immediately when a test fails instead of
continuing to execute further tests.  (Suggested by Cliff Dyer and
implemented by Michael Foord; bpo-8074.)

The progress messages now show ‘x’ for expected failures
and ‘u’ for unexpected successes when run in verbose mode.
(Contributed by Benjamin Peterson.)
Test cases can raise the SkipTest exception to skip a
test (bpo-1034053).
The error messages for assertEqual(),
assertTrue(), and assertFalse()
failures now provide more information.  If you set the
longMessage attribute of your TestCase classes to
true, both the standard error message and any additional message you
provide will be printed for failures.  (Added by Michael Foord; bpo-5663.)
The assertRaises() method now
returns a context handler when called without providing a callable
object to run.  For example, you can write this:
with self.assertRaises(KeyError):
    {}['foo']


(Implemented by Antoine Pitrou; bpo-4444.)
Module- and class-level setup and teardown fixtures are now supported.
Modules can contain setUpModule() and tearDownModule()
functions.  Classes can have setUpClass() and
tearDownClass() methods that must be defined as class methods
(using @classmethod or equivalent).  These functions and
methods are invoked when the test runner switches to a test case in a
different module or class.
The methods addCleanup() and
doCleanups() were added.
addCleanup() lets you add cleanup functions that
will be called unconditionally (after setUp() if
setUp() fails, otherwise after tearDown()). This allows
for much simpler resource allocation and deallocation during tests
(bpo-5679).
A number of new methods were added that provide more specialized
tests.  Many of these methods were written by Google engineers
for use in their test suites; Gregory P. Smith, Michael Foord, and
GvR worked on merging them into Python’s version of unittest.

assertIsNone() and assertIsNotNone() take one
expression and verify that the result is or is not None.
assertIs() and assertIsNot()
take two values and check whether the two values evaluate to the same object or not.
(Added by Michael Foord; bpo-2578.)
assertIsInstance() and
assertNotIsInstance() check whether
the resulting object is an instance of a particular class, or of
one of a tuple of classes.  (Added by Georg Brandl; bpo-7031.)
assertGreater(), assertGreaterEqual(),
assertLess(), and assertLessEqual() compare
two quantities.
assertMultiLineEqual() compares two strings, and if they’re
not equal, displays a helpful comparison that highlights the
differences in the two strings.  This comparison is now used by
default when Unicode strings are compared with assertEqual().
assertRegexpMatches() and
assertNotRegexpMatches() checks whether the
first argument is a string matching or not matching the regular
expression provided as the second argument (bpo-8038).
assertRaisesRegexp() checks whether a particular exception
is raised, and then also checks that the string representation of
the exception matches the provided regular expression.
assertIn() and assertNotIn()
tests whether first is or is not in  second.
assertItemsEqual() tests whether two provided sequences
contain the same elements.
assertSetEqual() compares whether two sets are equal, and
only reports the differences between the sets in case of error.
Similarly, assertListEqual() and assertTupleEqual()
compare the specified types and explain any differences without necessarily
printing their full values; these methods are now used by default
when comparing lists and tuples using assertEqual().
More generally, assertSequenceEqual() compares two sequences
and can optionally check whether both sequences are of a
particular type.
assertDictEqual() compares two dictionaries and reports the
differences; it’s now used by default when you compare two dictionaries
using assertEqual().  assertDictContainsSubset() checks whether
all of the key/value pairs in first are found in second.
assertAlmostEqual() and assertNotAlmostEqual() test
whether first and second are approximately equal.  This method
can either round their difference to an optionally-specified number
of places (the default is 7) and compare it to zero, or require
the difference to be smaller than a supplied delta value.
loadTestsFromName() properly honors the
suiteClass attribute of
the TestLoader. (Fixed by Mark Roddy; bpo-6866.)
A new hook lets you extend the assertEqual() method to handle
new data types.  The addTypeEqualityFunc() method takes a type
object and a function. The function will be used when both of the
objects being compared are of the specified type.  This function
should compare the two objects and raise an exception if they don’t
match; it’s a good idea for the function to provide additional
information about why the two objects aren’t matching, much as the new
sequence comparison methods do.

unittest.main() now takes an optional exit argument.  If
false, main() doesn’t call sys.exit(), allowing
main() to be used from the interactive interpreter.
(Contributed by J. Pablo Fernández; bpo-3379.)
TestResult has new startTestRun() and
stopTestRun() methods that are called immediately before
and after a test run.  (Contributed by Robert Collins; bpo-5728.)
With all these changes, the unittest.py was becoming awkwardly
large, so the module was turned into a package and the code split into
several files (by Benjamin Peterson).  This doesn’t affect how the
module is imported or used.

See also

http://www.voidspace.org.uk/python/articles/unittest2.shtmlDescribes the new features, how to use them, and the
rationale for various design decisions.  (By Michael Foord.)",new-and-improved-modules - updated-module-unittest,"python -m unittest discover -s test
",2.7
,new-and-improved-modules - updated-module-unittest,"with self.assertRaises(KeyError):
    {}['foo']
",2.7
"Updated module: ElementTree 1.3

The version of the ElementTree library included with Python was updated to
version 1.3.  Some of the new features are:

The various parsing functions now take a parser keyword argument
giving an XMLParser instance that will
be used.  This makes it possible to override the file’s internal encoding:
p = ET.XMLParser(encoding='utf-8')
t = ET.XML(""""""<root/>"""""", parser=p)


Errors in parsing XML now raise a ParseError exception, whose
instances have a position attribute
containing a (line, column) tuple giving the location of the problem.

ElementTree’s code for converting trees to a string has been
significantly reworked, making it roughly twice as fast in many
cases.  The ElementTree.write()
and Element.write() methods now have a method parameter that can be
“xml” (the default), “html”, or “text”.  HTML mode will output empty
elements as <empty></empty> instead of <empty/>, and text
mode will skip over elements and only output the text chunks.  If
you set the tag attribute of an element to None but
leave its children in place, the element will be omitted when the
tree is written out, so you don’t need to do more extensive rearrangement
to remove a single element.
Namespace handling has also been improved.  All xmlns:<whatever>
declarations are now output on the root element, not scattered throughout
the resulting XML.  You can set the default namespace for a tree
by setting the default_namespace attribute and can
register new prefixes with register_namespace().  In XML mode,
you can use the true/false xml_declaration parameter to suppress the
XML declaration.

New Element method:
extend() appends the items from a
sequence to the element’s children.  Elements themselves behave like
sequences, so it’s easy to move children from one element to
another:
from xml.etree import ElementTree as ET

t = ET.XML(""""""<list>
  <item>1</item> <item>2</item>  <item>3</item>
</list>"""""")
new = ET.XML('<root/>')
new.extend(t)

# Outputs <root><item>1</item>...</root>
print ET.tostring(new)



New Element method:
iter() yields the children of the
element as a generator.  It’s also possible to write for child in
elem: to loop over an element’s children.  The existing method
getiterator() is now deprecated, as is getchildren()
which constructs and returns a list of children.
New Element method:
itertext() yields all chunks of
text that are descendants of the element.  For example:
t = ET.XML(""""""<list>
  <item>1</item> <item>2</item>  <item>3</item>
</list>"""""")

# Outputs ['\n  ', '1', ' ', '2', '  ', '3', '\n']
print list(t.itertext())



Deprecated: using an element as a Boolean (i.e., if elem:) would
return true if the element had any children, or false if there were
no children.  This behaviour is confusing – None is false, but
so is a childless element? – so it will now trigger a
FutureWarning.  In your code, you should be explicit: write
len(elem) != 0 if you’re interested in the number of children,
or elem is not None.

Fredrik Lundh develops ElementTree and produced the 1.3 version;
you can read his article describing 1.3 at
http://effbot.org/zone/elementtree-13-intro.htm.
Florent Xicluna updated the version included with
Python, after discussions on python-dev and in bpo-6472.)",new-and-improved-modules - updated-module-elementtree-1-3,"p = ET.XMLParser(encoding='utf-8')
t = ET.XML(""""""<root/>"""""", parser=p)
",2.7
,new-and-improved-modules - updated-module-elementtree-1-3,"from xml.etree import ElementTree as ET

t = ET.XML(""""""<list>
  <item>1</item> <item>2</item>  <item>3</item>
</list>"""""")
new = ET.XML('<root/>')
new.extend(t)

# Outputs <root><item>1</item>...</root>
print ET.tostring(new)
",2.7
,new-and-improved-modules - updated-module-elementtree-1-3,"t = ET.XML(""""""<list>
  <item>1</item> <item>2</item>  <item>3</item>
</list>"""""")

# Outputs ['\n  ', '1', ' ', '2', '  ', '3', '\n']
print list(t.itertext())
",2.7
"Capsules

Python 3.1 adds a new C datatype, PyCapsule, for providing a
C API to an extension module.  A capsule is essentially the holder of
a C void * pointer, and is made available as a module attribute; for
example, the socket module’s API is exposed as socket.CAPI,
and unicodedata exposes ucnhash_CAPI.  Other extensions
can import the module, access its dictionary to get the capsule
object, and then get the void * pointer, which will usually point
to an array of pointers to the module’s various API functions.
There is an existing data type already used for this,
PyCObject, but it doesn’t provide type safety.  Evil code
written in pure Python could cause a segmentation fault by taking a
PyCObject from module A and somehow substituting it for the
PyCObject in module B.   Capsules know their own name,
and getting the pointer requires providing the name:
void *vtable;

if (!PyCapsule_IsValid(capsule, ""mymodule.CAPI"") {
        PyErr_SetString(PyExc_ValueError, ""argument type invalid"");
        return NULL;
}

vtable = PyCapsule_GetPointer(capsule, ""mymodule.CAPI"");


You are assured that vtable points to whatever you’re expecting.
If a different capsule was passed in, PyCapsule_IsValid() would
detect the mismatched name and return false.  Refer to
Providing a C API for an Extension Module for more information on using these objects.
Python 2.7 now uses capsules internally to provide various
extension-module APIs, but the PyCObject_AsVoidPtr() was
modified to handle capsules, preserving compile-time compatibility
with the CObject interface.  Use of
PyCObject_AsVoidPtr() will signal a
PendingDeprecationWarning, which is silent by default.
Implemented in Python 3.1 and backported to 2.7 by Larry Hastings;
discussed in bpo-5630.",build-and-c-api-changes - capsules,"void *vtable;

if (!PyCapsule_IsValid(capsule, ""mymodule.CAPI"") {
        PyErr_SetString(PyExc_ValueError, ""argument type invalid"");
        return NULL;
}

vtable = PyCapsule_GetPointer(capsule, ""mymodule.CAPI"");
",2.7
"Porting to Python 2.7

This section lists previously described changes and other bugfixes
that may require changes to your code:

The range() function processes its arguments more
consistently; it will now call __int__() on non-float,
non-integer arguments that are supplied to it.  (Fixed by Alexander
Belopolsky; bpo-1533.)
The string format() method changed the default precision used
for floating-point and complex numbers from 6 decimal
places to 12, which matches the precision used by str().
(Changed by Eric Smith; bpo-5920.)
Because of an optimization for the with statement, the special
methods __enter__() and __exit__() must belong to the object’s
type, and cannot be directly attached to the object’s instance.  This
affects new-style classes (derived from object) and C extension
types.  (bpo-6101.)
Due to a bug in Python 2.6, the exc_value parameter to
__exit__() methods was often the string representation of the
exception, not an instance.  This was fixed in 2.7, so exc_value
will be an instance as expected.  (Fixed by Florent Xicluna;
bpo-7853.)
When a restricted set of attributes were set using __slots__,
deleting an unset attribute would not raise AttributeError
as you would expect.  Fixed by Benjamin Peterson; bpo-7604.)

In the standard library:

Operations with datetime instances that resulted in a year
falling outside the supported range didn’t always raise
OverflowError.  Such errors are now checked more carefully
and will now raise the exception. (Reported by Mark Leander, patch
by Anand B. Pillai and Alexander Belopolsky; bpo-7150.)
When using Decimal instances with a string’s
format() method, the default alignment was previously
left-alignment.  This has been changed to right-alignment, which might
change the output of your programs.
(Changed by Mark Dickinson; bpo-6857.)
Comparisons involving a signaling NaN value (or sNAN) now signal
InvalidOperation instead of silently returning a true or
false value depending on the comparison operator.  Quiet NaN values
(or NaN) are now hashable.  (Fixed by Mark Dickinson;
bpo-7279.)

The ElementTree library, xml.etree, no longer escapes
ampersands and angle brackets when outputting an XML processing
instruction (which looks like <?xml-stylesheet href=”#style1”?>)
or comment (which looks like <!– comment –>).
(Patch by Neil Muller; bpo-2746.)
The readline() method of StringIO objects now does
nothing when a negative length is requested, as other file-like
objects do.  (bpo-7348).
The syslog module will now use the value of sys.argv[0] as the
identifier instead of the previous default value of 'python'.
(Changed by Sean Reifschneider; bpo-8451.)
The tarfile module’s default error handling has changed, to
no longer suppress fatal errors.  The default error level was previously 0,
which meant that errors would only result in a message being written to the
debug log, but because the debug log is not activated by default,
these errors go unnoticed.  The default error level is now 1,
which raises an exception if there’s an error.
(Changed by Lars Gustäbel; bpo-7357.)
The urlparse module’s urlsplit() now handles
unknown URL schemes in a fashion compliant with RFC 3986: if the
URL is of the form ""<something>://..."", the text before the
:// is treated as the scheme, even if it’s a made-up scheme that
the module doesn’t know about.  This change may break code that
worked around the old behaviour.  For example, Python 2.6.4 or 2.5
will return the following:
>>> import urlparse
>>> urlparse.urlsplit('invented://host/filename?query')
('invented', '', '//host/filename?query', '', '')


Python 2.7 (and Python 2.6.5) will return:
>>> import urlparse
>>> urlparse.urlsplit('invented://host/filename?query')
('invented', 'host', '/filename?query', '', '')


(Python 2.7 actually produces slightly different output, since it
returns a named tuple instead of a standard tuple.)


For C extensions:

C extensions that use integer format codes with the PyArg_Parse*
family of functions will now raise a TypeError exception
instead of triggering a DeprecationWarning (bpo-5080).
Use the new PyOS_string_to_double() function instead of the old
PyOS_ascii_strtod() and PyOS_ascii_atof() functions,
which are now deprecated.

For applications that embed Python:

The PySys_SetArgvEx() function was added, letting
applications close a security hole when the existing
PySys_SetArgv() function was used.  Check whether you’re
calling PySys_SetArgv() and carefully consider whether the
application should be using PySys_SetArgvEx() with
updatepath set to false.",porting-to-python-2-7,">>> import urlparse
>>> urlparse.urlsplit('invented://host/filename?query')
('invented', '', '//host/filename?query', '', '')
",2.7
,porting-to-python-2-7,">>> import urlparse
>>> urlparse.urlsplit('invented://host/filename?query')
('invented', 'host', '/filename?query', '', '')
",2.7
"PEP 476: Enabling certificate verification by default for stdlib http clients

PEP 476 updated httplib and modules which use it, such as
urllib2 and xmlrpclib, to now verify that the server
presents a certificate which is signed by a Certificate Authority in the
platform trust store and whose hostname matches the hostname being requested
by default, significantly improving security for many applications. This
change was made in the Python 2.7.9 release.
For applications which require the old previous behavior, they can pass an
alternate context:
import urllib2
import ssl

# This disables all verification
context = ssl._create_unverified_context()

# This allows using a specific certificate for the host, which doesn't need
# to be in the trust store
context = ssl.create_default_context(cafile=""/path/to/file.crt"")

urllib2.urlopen(""https://invalid-cert"", context=context)",new-features-added-to-python-2-7-maintenance-releases - pep-476-enabling-certificate-verification-by-default-for-stdlib-http-clients,"import urllib2
import ssl

# This disables all verification
context = ssl._create_unverified_context()

# This allows using a specific certificate for the host, which doesn't need
# to be in the trust store
context = ssl.create_default_context(cafile=""/path/to/file.crt"")

urllib2.urlopen(""https://invalid-cert"", context=context)
",2.7
"Print Is A Function

The print statement has been replaced with a print()
function, with keyword arguments to replace most of the special syntax
of the old print statement (PEP 3105).  Examples:
Old: print ""The answer is"", 2*2
New: print(""The answer is"", 2*2)

Old: print x,           # Trailing comma suppresses newline
New: print(x, end="" "")  # Appends a space instead of a newline

Old: print              # Prints a newline
New: print()            # You must call the function!

Old: print >>sys.stderr, ""fatal error""
New: print(""fatal error"", file=sys.stderr)

Old: print (x, y)       # prints repr((x, y))
New: print((x, y))      # Not the same as print(x, y)!


You can also customize the separator between items, e.g.:
print(""There are <"", 2**32, ""> possibilities!"", sep="""")


which produces:
There are <4294967296> possibilities!

Note:

The print() function doesn’t support the “softspace” feature of
the old print statement.  For example, in Python 2.x,
print ""A\n"", ""B"" would write ""A\nB\n""; but in Python 3.0,
print(""A\n"", ""B"") writes ""A\n B\n"".
Initially, you’ll be finding yourself typing the old print x
a lot in interactive mode.  Time to retrain your fingers to type
print(x) instead!
When using the 2to3 source-to-source conversion tool, all
print statements are automatically converted to
print() function calls, so this is mostly a non-issue for
larger projects.",common-stumbling-blocks - print-is-a-function,"Old: print ""The answer is"", 2*2
New: print(""The answer is"", 2*2)

Old: print x,           # Trailing comma suppresses newline
New: print(x, end="" "")  # Appends a space instead of a newline

Old: print              # Prints a newline
New: print()            # You must call the function!

Old: print >>sys.stderr, ""fatal error""
New: print(""fatal error"", file=sys.stderr)

Old: print (x, y)       # prints repr((x, y))
New: print((x, y))      # Not the same as print(x, y)!
",3.0
,common-stumbling-blocks - print-is-a-function,"print(""There are <"", 2**32, ""> possibilities!"", sep="""")
",3.0
"New Syntax


PEP 3107: Function argument and return value annotations.  This
provides a standardized way of annotating a function’s parameters
and return value.  There are no semantics attached to such
annotations except that they can be introspected at runtime using
the __annotations__ attribute.  The intent is to encourage
experimentation through metaclasses, decorators or frameworks.

PEP 3102: Keyword-only arguments.  Named parameters occurring
after *args in the parameter list must be specified using
keyword syntax in the call.  You can also use a bare * in the
parameter list to indicate that you don’t accept a variable-length
argument list, but you do have keyword-only arguments.

Keyword arguments are allowed after the list of base classes in a
class definition.  This is used by the new convention for specifying
a metaclass (see next section), but can be used for other purposes
as well, as long as the metaclass supports it.

PEP 3104: nonlocal statement.  Using nonlocal x
you can now assign directly to a variable in an outer (but
non-global) scope.  nonlocal is a new reserved word.

PEP 3132: Extended Iterable Unpacking.  You can now write things
like a, b, *rest = some_sequence.  And even *rest, a =
stuff.  The rest object is always a (possibly empty) list; the
right-hand side may be any iterable.  Example:
(a, *rest, b) = range(5)


This sets a to 0, b to 4, and rest to [1, 2, 3].

Dictionary comprehensions: {k: v for k, v in stuff} means the
same thing as dict(stuff) but is more flexible.  (This is
PEP 0274 vindicated. :-)

Set literals, e.g. {1, 2}.  Note that {} is an empty
dictionary; use set() for an empty set.  Set comprehensions are
also supported; e.g., {x for x in stuff} means the same thing as
set(stuff) but is more flexible.

New octal literals, e.g. 0o720 (already in 2.6).  The old octal
literals (0720) are gone.

New binary literals, e.g. 0b1010 (already in 2.6), and
there is a new corresponding builtin function, bin().

Bytes literals are introduced with a leading b or B, and
there is a new corresponding builtin function, bytes().",overview-of-syntax-changes - new-syntax,"(a, *rest, b) = range(5)
",3.0
"Changed Syntax


PEP 3109 and PEP 3134: new raise statement syntax:
raise [expr [from expr]].  See below.

as and with are now reserved words.  (Since
2.6, actually.)

True, False, and None are reserved
words.  (2.6 partially enforced the restrictions on None
already.)

Change from except exc, var to
except exc as var.  See PEP 3110.

PEP 3115: New Metaclass Syntax.  Instead of:
class C:
    __metaclass__ = M
    ...

you must now use:
class C(metaclass=M):
    ...


The module-global __metaclass__ variable is no longer
supported.  (It was a crutch to make it easier to default to
new-style classes without deriving every class from
object.)

List comprehensions no longer support the syntactic form
[... for var in item1, item2, ...].  Use
[... for var in (item1, item2, ...)] instead.
Also note that list comprehensions have different semantics: they
are closer to syntactic sugar for a generator expression inside a
list() constructor, and in particular the loop control
variables are no longer leaked into the surrounding scope.

The ellipsis (...) can be used as an atomic expression
anywhere.  (Previously it was only allowed in slices.)  Also, it
must now be spelled as ....  (Previously it could also be
spelled as . . ., by a mere accident of the grammar.)",overview-of-syntax-changes - changed-syntax,"class C(metaclass=M):
    ...
",3.0
"Changes To Exceptions

The APIs for raising and catching exception have been cleaned up and
new powerful features added:

PEP 0352: All exceptions must be derived (directly or indirectly)
from BaseException.  This is the root of the exception
hierarchy.  This is not new as a recommendation, but the
requirement to inherit from BaseException is new.  (Python
2.6 still allowed classic classes to be raised, and placed no
restriction on what you can catch.)  As a consequence, string
exceptions are finally truly and utterly dead.

Almost all exceptions should actually derive from Exception;
BaseException should only be used as a base class for
exceptions that should only be handled at the top level, such as
SystemExit or KeyboardInterrupt.  The recommended
idiom for handling all exceptions except for this latter category is
to use except Exception.

StandardError was removed (in 2.6 already).

Exceptions no longer behave as sequences.  Use the args
attribute instead.

PEP 3109: Raising exceptions.  You must now use raise
Exception(args) instead of raise Exception, args.
Additionally, you can no longer explicitly specify a traceback;
instead, if you have to do this, you can assign directly to the
__traceback__ attribute (see below).

PEP 3110: Catching exceptions.  You must now use
except SomeException as variable instead
of except SomeException, variable.  Moreover, the
variable is explicitly deleted when the except block
is left.

PEP 3134: Exception chaining.  There are two cases: implicit
chaining and explicit chaining.  Implicit chaining happens when an
exception is raised in an except or finally
handler block.  This usually happens due to a bug in the handler
block; we call this a secondary exception.  In this case, the
original exception (that was being handled) is saved as the
__context__ attribute of the secondary exception.
Explicit chaining is invoked with this syntax:
raise SecondaryException() from primary_exception


(where primary_exception is any expression that produces an
exception object, probably an exception that was previously caught).
In this case, the primary exception is stored on the
__cause__ attribute of the secondary exception.  The
traceback printed when an unhandled exception occurs walks the chain
of __cause__ and __context__ attributes and prints a
separate traceback for each component of the chain, with the primary
exception at the top.  (Java users may recognize this behavior.)

PEP 3134: Exception objects now store their traceback as the
__traceback__ attribute.  This means that an exception
object now contains all the information pertaining to an exception,
and there are fewer reasons to use sys.exc_info() (though the
latter is not removed).

A few exception messages are improved when Windows fails to load an
extension module.  For example, error code 193 is now %1 is
not a valid Win32 application.  Strings now deal with non-English
locales.",changes-to-exceptions,"raise SecondaryException() from primary_exception
",3.0
"PEP 372: Ordered Dictionaries

Regular Python dictionaries iterate over key/value pairs in arbitrary order.
Over the years, a number of authors have written alternative implementations
that remember the order that the keys were originally inserted.  Based on
the experiences from those implementations, a new
collections.OrderedDict class has been introduced.
The OrderedDict API is substantially the same as regular dictionaries
but will iterate over keys and values in a guaranteed order depending on
when a key was first inserted.  If a new entry overwrites an existing entry,
the original insertion position is left unchanged.  Deleting an entry and
reinserting it will move it to the end.
The standard library now supports use of ordered dictionaries in several
modules.  The configparser module uses them by default.  This lets
configuration files be read, modified, and then written back in their original
order.  The _asdict() method for collections.namedtuple() now
returns an ordered dictionary with the values appearing in the same order as
the underlying tuple indicies.  The json module is being built-out with
an object_pairs_hook to allow OrderedDicts to be built by the decoder.
Support was also added for third-party tools like PyYAML.

See also

PEP 372 - Ordered Dictionaries
PEP written by Armin Ronacher and Raymond Hettinger.  Implementation
written by Raymond Hettinger.


Since an ordered dictionary remembers its insertion order, it can be used
in conjuction with sorting to make a sorted dictionary:
>>> # regular unsorted dictionary
>>> d = {'banana': 3, 'apple':4, 'pear': 1, 'orange': 2}

>>> # dictionary sorted by key
>>> OrderedDict(sorted(d.items(), key=lambda t: t[0]))
OrderedDict([('apple', 4), ('banana', 3), ('orange', 2), ('pear', 1)])

>>> # dictionary sorted by value
>>> OrderedDict(sorted(d.items(), key=lambda t: t[1]))
OrderedDict([('pear', 1), ('orange', 2), ('banana', 3), ('apple', 4)])

>>> # dictionary sorted by length of the key string
>>> OrderedDict(sorted(d.items(), key=lambda t: len(t[0])))
OrderedDict([('pear', 1), ('apple', 4), ('orange', 2), ('banana', 3)])


The new sorted dictionaries maintain their sort order when entries
are deleted.  But when new keys are added, the keys are appended
to the end and the sort is not maintained.",pep-372-ordered-dictionaries,">>> # regular unsorted dictionary
>>> d = {'banana': 3, 'apple':4, 'pear': 1, 'orange': 2}

>>> # dictionary sorted by key
>>> OrderedDict(sorted(d.items(), key=lambda t: t[0]))
OrderedDict([('apple', 4), ('banana', 3), ('orange', 2), ('pear', 1)])

>>> # dictionary sorted by value
>>> OrderedDict(sorted(d.items(), key=lambda t: t[1]))
OrderedDict([('pear', 1), ('orange', 2), ('banana', 3), ('apple', 4)])

>>> # dictionary sorted by length of the key string
>>> OrderedDict(sorted(d.items(), key=lambda t: len(t[0])))
OrderedDict([('pear', 1), ('apple', 4), ('orange', 2), ('banana', 3)])
",3.1
"PEP 378: Format Specifier for Thousands Separator

The builtin format() function and the str.format() method use
a mini-language that now includes a simple, non-locale aware way to format
a number with a thousands separator.  That provides a way to humanize a
program’s output, improving its professional appearance and readability:
>>> format(1234567, ',d')
'1,234,567'
>>> format(1234567.89, ',.2f')
'1,234,567.89'
>>> format(12345.6 + 8901234.12j, ',f')
'12,345.600000+8,901,234.120000j'
>>> format(Decimal('1234567.89'), ',f')
'1,234,567.89'


The supported types are int, float, complex
and decimal.Decimal.
Discussions are underway about how to specify alternative separators
like dots, spaces, apostrophes, or underscores.  Locale-aware applications
should use the existing n format specifier which already has some support
for thousands separators.

See also

PEP 378 - Format Specifier for Thousands Separator
PEP written by Raymond Hettinger and implemented by Eric Smith and
Mark Dickinson.",pep-378-format-specifier-for-thousands-separator,">>> format(1234567, ',d')
'1,234,567'
>>> format(1234567.89, ',.2f')
'1,234,567.89'
>>> format(12345.6 + 8901234.12j, ',f')
'12,345.600000+8,901,234.120000j'
>>> format(Decimal('1234567.89'), ',f')
'1,234,567.89'
",3.1
"Other Language Changes

Some smaller changes made to the core Python language are:

Directories and zip archives containing a __main__.py
file can now be executed directly by passing their name to the
interpreter. The directory/zipfile is automatically inserted as the
first entry in sys.path.  (Suggestion and initial patch by Andy Chu;
revised patch by Phillip J. Eby and Nick Coghlan; issue 1739468.)

The int() type gained a bit_length method that returns the
number of bits necessary to represent its argument in binary:
>>> n = 37
>>> bin(37)
'0b100101'
>>> n.bit_length()
6
>>> n = 2**123-1
>>> n.bit_length()
123
>>> (n+1).bit_length()
124


(Contributed by Fredrik Johansson, Victor Stinner, Raymond Hettinger,
and Mark Dickinson; issue 3439.)

The fields in format() strings can now be automatically
numbered:
>>> 'Sir {} of {}'.format('Gallahad', 'Camelot')
'Sir Gallahad of Camelot'


Formerly, the string would have required numbered fields such as:
'Sir {0} of {1}'.
(Contributed by Eric Smith; issue 5237.)

The string.maketrans() function is deprecated and is replaced by new
static methods, bytes.maketrans() and bytearray.maketrans().
This change solves the confusion around which types were supported by the
string module. Now, str, bytes, and
bytearray each have their own maketrans and translate
methods with intermediate translation tables of the appropriate type.
(Contributed by Georg Brandl; issue 5675.)

The syntax of the with statement now allows multiple context
managers in a single statement:
>>> with open('mylog.txt') as infile, open('a.out', 'w') as outfile:
...     for line in infile:
...         if '<critical>' in line:
...             outfile.write(line)


With the new syntax, the contextlib.nested() function is no longer
needed and is now deprecated.
(Contributed by Georg Brandl and Mattias Brändström;
appspot issue 53094.)

round(x, n) now returns an integer if x is an integer.
Previously it returned a float:
>>> round(1123, -2)
1100


(Contributed by Mark Dickinson; issue 4707.)

Python now uses David Gay’s algorithm for finding the shortest floating
point representation that doesn’t change its value.  This should help
mitigate some of the confusion surrounding binary floating point
numbers.
The significance is easily seen with a number like 1.1 which does not
have an exact equivalent in binary floating point.  Since there is no exact
equivalent, an expression like float('1.1') evaluates to the nearest
representable value which is 0x1.199999999999ap+0 in hex or
1.100000000000000088817841970012523233890533447265625 in decimal. That
nearest value was and still is used in subsequent floating point
calculations.
What is new is how the number gets displayed.  Formerly, Python used a
simple approach.  The value of repr(1.1) was computed as format(1.1,
'.17g') which evaluated to '1.1000000000000001'. The advantage of
using 17 digits was that it relied on IEEE-754 guarantees to assure that
eval(repr(1.1)) would round-trip exactly to its original value.  The
disadvantage is that many people found the output to be confusing (mistaking
intrinsic limitations of binary floating point representation as being a
problem with Python itself).
The new algorithm for repr(1.1) is smarter and returns '1.1'.
Effectively, it searches all equivalent string representations (ones that
get stored with the same underlying float value) and returns the shortest
representation.
The new algorithm tends to emit cleaner representations when possible, but
it does not change the underlying values.  So, it is still the case that
1.1 + 2.2 != 3.3 even though the representations may suggest otherwise.
The new algorithm depends on certain features in the underlying floating
point implementation.  If the required features are not found, the old
algorithm will continue to be used.  Also, the text pickle protocols
assure cross-platform portability by using the old algorithm.
(Contributed by Eric Smith and Mark Dickinson; issue 1580)",other-language-changes,">>> n = 37
>>> bin(37)
'0b100101'
>>> n.bit_length()
6
>>> n = 2**123-1
>>> n.bit_length()
123
>>> (n+1).bit_length()
124
",3.1
,other-language-changes,">>> 'Sir {} of {}'.format('Gallahad', 'Camelot')
'Sir Gallahad of Camelot'
",3.1
,other-language-changes,">>> with open('mylog.txt') as infile, open('a.out', 'w') as outfile:
...     for line in infile:
...         if '<critical>' in line:
...             outfile.write(line)
",3.1
,other-language-changes,">>> round(1123, -2)
1100
",3.1
"New, Improved, and Deprecated Modules


Added a collections.Counter class to support convenient
counting of unique items in a sequence or iterable:
>>> Counter(['red', 'blue', 'red', 'green', 'blue', 'blue'])
Counter({'blue': 3, 'red': 2, 'green': 1})


(Contributed by Raymond Hettinger; issue 1696199.)

Added a new module, tkinter.ttk for access to the Tk themed widget set.
The basic idea of ttk is to separate, to the extent possible, the code
implementing a widget’s behavior from the code implementing its appearance.
(Contributed by Guilherme Polo; issue 2983.)

The gzip.GzipFile and bz2.BZ2File classes now support
the context manager protocol:
>>> # Automatically close file after writing
>>> with gzip.GzipFile(filename, ""wb"") as f:
...     f.write(b""xxx"")


(Contributed by Antoine Pitrou.)

The decimal module now supports methods for creating a
decimal object from a binary float.  The conversion is
exact but can sometimes be surprising:
>>> Decimal.from_float(1.1)
Decimal('1.100000000000000088817841970012523233890533447265625')


The long decimal result shows the actual binary fraction being
stored for 1.1.  The fraction has many digits because 1.1 cannot
be exactly represented in binary.
(Contributed by Raymond Hettinger and Mark Dickinson.)

The itertools module grew two new functions.  The
itertools.combinations_with_replacement() function is one of
four for generating combinatorics including permutations and Cartesian
products.  The itertools.compress() function mimics its namesake
from APL.  Also, the existing itertools.count() function now has
an optional step argument and can accept any type of counting
sequence including fractions.Fraction and
decimal.Decimal:
>>> [p+q for p,q in combinations_with_replacement('LOVE', 2)]
['LL', 'LO', 'LV', 'LE', 'OO', 'OV', 'OE', 'VV', 'VE', 'EE']

>>> list(compress(data=range(10), selectors=[0,0,1,1,0,1,0,1,0,0]))
[2, 3, 5, 7]

>>> c = count(start=Fraction(1,2), step=Fraction(1,6))
>>> [next(c), next(c), next(c), next(c)]
[Fraction(1, 2), Fraction(2, 3), Fraction(5, 6), Fraction(1, 1)]


(Contributed by Raymond Hettinger.)

collections.namedtuple() now supports a keyword argument
rename which lets invalid fieldnames be automatically converted to
positional names in the form _0, _1, etc.  This is useful when
the field names are being created by an external source such as a
CSV header, SQL field list, or user input:
>>> query = input()
SELECT region, dept, count(*) FROM main GROUPBY region, dept

>>> cursor.execute(query)
>>> query_fields = [desc[0] for desc in cursor.description]
>>> UserQuery = namedtuple('UserQuery', query_fields, rename=True)
>>> pprint.pprint([UserQuery(*row) for row in cursor])
[UserQuery(region='South', dept='Shipping', _2=185),
 UserQuery(region='North', dept='Accounting', _2=37),
 UserQuery(region='West', dept='Sales', _2=419)]


(Contributed by Raymond Hettinger; issue 1818.)

The re.sub(), re.subn() and re.split() functions now
accept a flags parameter.
(Contributed by Gregory Smith.)

The logging module now implements a simple logging.NullHandler
class for applications that are not using logging but are calling
library code that does.  Setting-up a null handler will suppress
spurious warnings such as “No handlers could be found for logger foo”:
>>> h = logging.NullHandler()
>>> logging.getLogger(""foo"").addHandler(h)


(Contributed by Vinay Sajip; issue 4384).

The runpy module which supports the -m command line switch
now supports the execution of packages by looking for and executing
a __main__ submodule when a package name is supplied.
(Contributed by Andi Vajda; issue 4195.)

The pdb module can now access and display source code loaded via
zipimport (or any other conformant PEP 302 loader).
(Contributed by Alexander Belopolsky; issue 4201.)

functools.partial objects can now be pickled.



(Suggested by Antoine Pitrou and Jesse Noller.  Implemented by
Jack Diederich; issue 5228.)

Add pydoc help topics for symbols so that help('@')
works as expected in the interactive environment.
(Contributed by David Laban; issue 4739.)

The unittest module now supports skipping individual tests or classes
of tests. And it supports marking a test as a expected failure, a test that
is known to be broken, but shouldn’t be counted as a failure on a
TestResult:
class TestGizmo(unittest.TestCase):

    @unittest.skipUnless(sys.platform.startswith(""win""), ""requires Windows"")
    def test_gizmo_on_windows(self):
        ...

    @unittest.expectedFailure
    def test_gimzo_without_required_library(self):
        ...


Also, tests for exceptions have been builtout to work with context managers
using the with statement:
def test_division_by_zero(self):
    with self.assertRaises(ZeroDivisionError):
        x / 0


In addition, several new assertion methods were added including
assertSetEqual(), assertDictEqual(),
assertDictContainsSubset(), assertListEqual(),
assertTupleEqual(), assertSequenceEqual(),
assertRaisesRegexp(), assertIsNone(),
and assertIsNotNone().
(Contributed by Benjamin Peterson and Antoine Pitrou.)

The io module has three new constants for the seek()
method SEEK_SET, SEEK_CUR, and SEEK_END.

The sys.version_info tuple is now a named tuple:
>>> sys.version_info
sys.version_info(major=3, minor=1, micro=0, releaselevel='alpha', serial=2)


(Contributed by Ross Light; issue 4285.)

The nntplib and imaplib modules now support IPv6.
(Contributed by Derek Morr; issue 1655 and issue 1664.)

The pickle module has been adapted for better interoperability with
Python 2.x when used with protocol 2 or lower.  The reorganization of the
standard library changed the formal reference for many objects.  For
example, __builtin__.set in Python 2 is called builtins.set in Python
3. This change confounded efforts to share data between different versions of
Python.  But now when protocol 2 or lower is selected, the pickler will
automatically use the old Python 2 names for both loading and dumping. This
remapping is turned-on by default but can be disabled with the fix_imports
option:
>>> s = {1, 2, 3}
>>> pickle.dumps(s, protocol=0)
b'c__builtin__\nset\np0\n((lp1\nL1L\naL2L\naL3L\natp2\nRp3\n.'
>>> pickle.dumps(s, protocol=0, fix_imports=False)
b'cbuiltins\nset\np0\n((lp1\nL1L\naL2L\naL3L\natp2\nRp3\n.'


An unfortunate but unavoidable side-effect of this change is that protocol 2
pickles produced by Python 3.1 won’t be readable with Python 3.0. The latest
pickle protocol, protocol 3, should be used when migrating data between
Python 3.x implementations, as it doesn’t attempt to remain compatible with
Python 2.x.
(Contributed by Alexandre Vassalotti and Antoine Pitrou, issue 6137.)

A new module, importlib was added.  It provides a complete, portable,
pure Python reference implementation of the import statement and its
counterpart, the __import__() function.  It represents a substantial
step forward in documenting and defining the actions that take place during
imports.
(Contributed by Brett Cannon.)",new-improved-and-deprecated-modules,">>> Counter(['red', 'blue', 'red', 'green', 'blue', 'blue'])
Counter({'blue': 3, 'red': 2, 'green': 1})
",3.1
,new-improved-and-deprecated-modules,">>> # Automatically close file after writing
>>> with gzip.GzipFile(filename, ""wb"") as f:
...     f.write(b""xxx"")
",3.1
,new-improved-and-deprecated-modules,">>> Decimal.from_float(1.1)
Decimal('1.100000000000000088817841970012523233890533447265625')
",3.1
,new-improved-and-deprecated-modules,">>> [p+q for p,q in combinations_with_replacement('LOVE', 2)]
['LL', 'LO', 'LV', 'LE', 'OO', 'OV', 'OE', 'VV', 'VE', 'EE']

>>> list(compress(data=range(10), selectors=[0,0,1,1,0,1,0,1,0,0]))
[2, 3, 5, 7]

>>> c = count(start=Fraction(1,2), step=Fraction(1,6))
>>> [next(c), next(c), next(c), next(c)]
[Fraction(1, 2), Fraction(2, 3), Fraction(5, 6), Fraction(1, 1)]
",3.1
,new-improved-and-deprecated-modules,">>> query = input()
SELECT region, dept, count(*) FROM main GROUPBY region, dept

>>> cursor.execute(query)
>>> query_fields = [desc[0] for desc in cursor.description]
>>> UserQuery = namedtuple('UserQuery', query_fields, rename=True)
>>> pprint.pprint([UserQuery(*row) for row in cursor])
[UserQuery(region='South', dept='Shipping', _2=185),
 UserQuery(region='North', dept='Accounting', _2=37),
 UserQuery(region='West', dept='Sales', _2=419)]
",3.1
,new-improved-and-deprecated-modules,">>> h = logging.NullHandler()
>>> logging.getLogger(""foo"").addHandler(h)
",3.1
,new-improved-and-deprecated-modules,"class TestGizmo(unittest.TestCase):

    @unittest.skipUnless(sys.platform.startswith(""win""), ""requires Windows"")
    def test_gizmo_on_windows(self):
        ...

    @unittest.expectedFailure
    def test_gimzo_without_required_library(self):
        ...
",3.1
,new-improved-and-deprecated-modules,"def test_division_by_zero(self):
    with self.assertRaises(ZeroDivisionError):
        x / 0
",3.1
,new-improved-and-deprecated-modules,">>> sys.version_info
sys.version_info(major=3, minor=1, micro=0, releaselevel='alpha', serial=2)
",3.1
,new-improved-and-deprecated-modules,">>> s = {1, 2, 3}
>>> pickle.dumps(s, protocol=0)
b'c__builtin__\nset\np0\n((lp1\nL1L\naL2L\naL3L\natp2\nRp3\n.'
>>> pickle.dumps(s, protocol=0, fix_imports=False)
b'cbuiltins\nset\np0\n((lp1\nL1L\naL2L\naL3L\natp2\nRp3\n.'
",3.1
"Build and C API Changes

Changes to Python’s build process and to the C API include:

Integers are now stored internally either in base 2**15 or in base
2**30, the base being determined at build time.  Previously, they
were always stored in base 2**15.  Using base 2**30 gives
significant performance improvements on 64-bit machines, but
benchmark results on 32-bit machines have been mixed.  Therefore,
the default is to use base 2**30 on 64-bit machines and base 2**15
on 32-bit machines; on Unix, there’s a new configure option
--enable-big-digits that can be used to override this default.
Apart from the performance improvements this change should be invisible to
end users, with one exception: for testing and debugging purposes there’s a
new sys.int_info that provides information about the
internal format, giving the number of bits per digit and the size in bytes
of the C type used to store each digit:
>>> import sys
>>> sys.int_info
sys.int_info(bits_per_digit=30, sizeof_digit=4)


(Contributed by Mark Dickinson; issue 4258.)

The PyLong_AsUnsignedLongLong() function now handles a negative
pylong by raising OverflowError instead of TypeError.
(Contributed by Mark Dickinson and Lisandro Dalcrin; issue 5175.)

Deprecated PyNumber_Int().  Use PyNumber_Long() instead.
(Contributed by Mark Dickinson; issue 4910.)

Added a new PyOS_string_to_double() function to replace the
deprecated functions PyOS_ascii_strtod() and PyOS_ascii_atof().
(Contributed by Mark Dickinson; issue 5914.)

Added PyCapsule as a replacement for the PyCObject API.
The principal difference is that the new type has a well defined interface
for passing typing safety information and a less complicated signature
for calling a destructor.  The old type had a problematic API and is now
deprecated.
(Contributed by Larry Hastings; issue 5630.)",build-and-c-api-changes,">>> import sys
>>> sys.int_info
sys.int_info(bits_per_digit=30, sizeof_digit=4)
",3.1
"Porting to Python 3.1

This section lists previously described changes and other bugfixes
that may require changes to your code:

The new floating point string representations can break existing doctests.
For example:
def e():
    '''Compute the base of natural logarithms.

    >>> e()
    2.7182818284590451

    '''
    return sum(1/math.factorial(x) for x in reversed(range(30)))

doctest.testmod()

**********************************************************************
Failed example:
    e()
Expected:
    2.7182818284590451
Got:
    2.718281828459045
**********************************************************************



The automatic name remapping in the pickle module for protocol 2 or lower can
make Python 3.1 pickles unreadable in Python 3.0.  One solution is to use
protocol 3.  Another solution is to set the fix_imports option to False.
See the discussion above for more details.",porting-to-python-3-1,"def e():
    '''Compute the base of natural logarithms.

    >>> e()
    2.7182818284590451

    '''
    return sum(1/math.factorial(x) for x in reversed(range(30)))

doctest.testmod()

**********************************************************************
Failed example:
    e()
Expected:
    2.7182818284590451
Got:
    2.718281828459045
**********************************************************************
",3.1
"PEP 389: Argparse Command Line Parsing Module

A new module for command line parsing, argparse, was introduced to
overcome the limitations of optparse which did not provide support for
positional arguments (not just options), subcommands, required options and other
common patterns of specifying and validating options.
This module has already had widespread success in the community as a
third-party module.  Being more fully featured than its predecessor, the
argparse module is now the preferred module for command-line processing.
The older module is still being kept available because of the substantial amount
of legacy code that depends on it.
Here’s an annotated example parser showing features like limiting results to a
set of choices, specifying a metavar in the help screen, validating that one
or more positional arguments is present, and making a required option:
import argparse
parser = argparse.ArgumentParser(
            description = 'Manage servers',         # main description for help
            epilog = 'Tested on Solaris and Linux') # displayed after help
parser.add_argument('action',                       # argument name
            choices = ['deploy', 'start', 'stop'],  # three allowed values
            help = 'action on each target')         # help msg
parser.add_argument('targets',
            metavar = 'HOSTNAME',                   # var name used in help msg
            nargs = '+',                            # require one or more targets
            help = 'url for target machines')       # help msg explanation
parser.add_argument('-u', '--user',                 # -u or --user option
            required = True,                        # make it a required argument
            help = 'login as user')


Example of calling the parser on a command string:
>>> cmd  = 'deploy sneezy.example.com sleepy.example.com -u skycaptain'
>>> result = parser.parse_args(cmd.split())
>>> result.action
'deploy'
>>> result.targets
['sneezy.example.com', 'sleepy.example.com']
>>> result.user
'skycaptain'


Example of the parser’s automatically generated help:
>>> parser.parse_args('-h'.split())

usage: manage_cloud.py [-h] -u USER
                       {deploy,start,stop} HOSTNAME [HOSTNAME ...]

Manage servers

positional arguments:
  {deploy,start,stop}   action on each target
  HOSTNAME              url for target machines

optional arguments:
  -h, --help            show this help message and exit
  -u USER, --user USER  login as user

Tested on Solaris and Linux


An especially nice argparse feature is the ability to define subparsers,
each with their own argument patterns and help displays:
import argparse
parser = argparse.ArgumentParser(prog='HELM')
subparsers = parser.add_subparsers()

parser_l = subparsers.add_parser('launch', help='Launch Control')   # first subgroup
parser_l.add_argument('-m', '--missiles', action='store_true')
parser_l.add_argument('-t', '--torpedos', action='store_true')

parser_m = subparsers.add_parser('move', help='Move Vessel',        # second subgroup
                                 aliases=('steer', 'turn'))         # equivalent names
parser_m.add_argument('-c', '--course', type=int, required=True)
parser_m.add_argument('-s', '--speed', type=int, default=0)

$ ./helm.py --help                         # top level help (launch and move)
$ ./helm.py launch --help                  # help for launch options
$ ./helm.py launch --missiles              # set missiles=True and torpedos=False
$ ./helm.py steer --course 180 --speed 5   # set movement parameters


See also

PEP 389 - New Command Line Parsing Module
PEP written by Steven Bethard.

Upgrading optparse code for details on the differences from optparse.",pep-389-argparse-command-line-parsing-module,"import argparse
parser = argparse.ArgumentParser(
            description = 'Manage servers',         # main description for help
            epilog = 'Tested on Solaris and Linux') # displayed after help
parser.add_argument('action',                       # argument name
            choices = ['deploy', 'start', 'stop'],  # three allowed values
            help = 'action on each target')         # help msg
parser.add_argument('targets',
            metavar = 'HOSTNAME',                   # var name used in help msg
            nargs = '+',                            # require one or more targets
            help = 'url for target machines')       # help msg explanation
parser.add_argument('-u', '--user',                 # -u or --user option
            required = True,                        # make it a required argument
            help = 'login as user')
",3.2
,pep-389-argparse-command-line-parsing-module,">>> cmd  = 'deploy sneezy.example.com sleepy.example.com -u skycaptain'
>>> result = parser.parse_args(cmd.split())
>>> result.action
'deploy'
>>> result.targets
['sneezy.example.com', 'sleepy.example.com']
>>> result.user
'skycaptain'
",3.2
,pep-389-argparse-command-line-parsing-module,">>> parser.parse_args('-h'.split())

usage: manage_cloud.py [-h] -u USER
                       {deploy,start,stop} HOSTNAME [HOSTNAME ...]

Manage servers

positional arguments:
  {deploy,start,stop}   action on each target
  HOSTNAME              url for target machines

optional arguments:
  -h, --help            show this help message and exit
  -u USER, --user USER  login as user

Tested on Solaris and Linux
",3.2
"PEP 391:  Dictionary Based Configuration for Logging

The logging module provided two kinds of configuration, one style with
function calls for each option or another style driven by an external file saved
in a ConfigParser format.  Those options did not provide the flexibility
to create configurations from JSON or YAML files, nor did they support
incremental configuration, which is needed for specifying logger options from a
command line.
To support a more flexible style, the module now offers
logging.config.dictConfig() for specifying logging configuration with
plain Python dictionaries.  The configuration options include formatters,
handlers, filters, and loggers.  Here’s a working example of a configuration
dictionary:
{""version"": 1,
 ""formatters"": {""brief"": {""format"": ""%(levelname)-8s: %(name)-15s: %(message)s""},
                ""full"": {""format"": ""%(asctime)s %(name)-15s %(levelname)-8s %(message)s""}
                },
 ""handlers"": {""console"": {
                   ""class"": ""logging.StreamHandler"",
                   ""formatter"": ""brief"",
                   ""level"": ""INFO"",
                   ""stream"": ""ext://sys.stdout""},
              ""console_priority"": {
                   ""class"": ""logging.StreamHandler"",
                   ""formatter"": ""full"",
                   ""level"": ""ERROR"",
                   ""stream"": ""ext://sys.stderr""}
              },
 ""root"": {""level"": ""DEBUG"", ""handlers"": [""console"", ""console_priority""]}}


If that dictionary is stored in a file called conf.json, it can be
loaded and called with code like this:
>>> import json, logging.config
>>> with open('conf.json') as f:
        conf = json.load(f)
>>> logging.config.dictConfig(conf)
>>> logging.info(""Transaction completed normally"")
INFO    : root           : Transaction completed normally
>>> logging.critical(""Abnormal termination"")
2011-02-17 11:14:36,694 root            CRITICAL Abnormal termination



See also

PEP 391 - Dictionary Based Configuration for Logging
PEP written by Vinay Sajip.",pep-391-dictionary-based-configuration-for-logging,"{""version"": 1,
 ""formatters"": {""brief"": {""format"": ""%(levelname)-8s: %(name)-15s: %(message)s""},
                ""full"": {""format"": ""%(asctime)s %(name)-15s %(levelname)-8s %(message)s""}
                },
 ""handlers"": {""console"": {
                   ""class"": ""logging.StreamHandler"",
                   ""formatter"": ""brief"",
                   ""level"": ""INFO"",
                   ""stream"": ""ext://sys.stdout""},
              ""console_priority"": {
                   ""class"": ""logging.StreamHandler"",
                   ""formatter"": ""full"",
                   ""level"": ""ERROR"",
                   ""stream"": ""ext://sys.stderr""}
              },
 ""root"": {""level"": ""DEBUG"", ""handlers"": [""console"", ""console_priority""]}}
",3.2
,pep-391-dictionary-based-configuration-for-logging,">>> import json, logging.config
>>> with open('conf.json') as f:
        conf = json.load(f)
>>> logging.config.dictConfig(conf)
>>> logging.info(""Transaction completed normally"")
INFO    : root           : Transaction completed normally
>>> logging.critical(""Abnormal termination"")
2011-02-17 11:14:36,694 root            CRITICAL Abnormal termination
",3.2
"PEP 3148:  The concurrent.futures module

Code for creating and managing concurrency is being collected in a new top-level
namespace, concurrent.  Its first member is a futures package which provides
a uniform high-level interface for managing threads and processes.
The design for concurrent.futures was inspired by the
java.util.concurrent package.  In that model, a running call and its result
are represented by a Future object that abstracts
features common to threads, processes, and remote procedure calls.  That object
supports status checks (running or done), timeouts, cancellations, adding
callbacks, and access to results or exceptions.
The primary offering of the new module is a pair of executor classes for
launching and managing calls.  The goal of the executors is to make it easier to
use existing tools for making parallel calls. They save the effort needed to
setup a pool of resources, launch the calls, create a results queue, add
time-out handling, and limit the total number of threads, processes, or remote
procedure calls.
Ideally, each application should share a single executor across multiple
components so that process and thread limits can be centrally managed.  This
solves the design challenge that arises when each component has its own
competing strategy for resource management.
Both classes share a common interface with three methods:
submit() for scheduling a callable and
returning a Future object;
map() for scheduling many asynchronous calls
at a time, and shutdown() for freeing
resources.  The class is a context manager and can be used in a
with statement to assure that resources are automatically released
when currently pending futures are done executing.
A simple of example of ThreadPoolExecutor is a
launch of four parallel threads for copying files:
import concurrent.futures, shutil
with concurrent.futures.ThreadPoolExecutor(max_workers=4) as e:
    e.submit(shutil.copy, 'src1.txt', 'dest1.txt')
    e.submit(shutil.copy, 'src2.txt', 'dest2.txt')
    e.submit(shutil.copy, 'src3.txt', 'dest3.txt')
    e.submit(shutil.copy, 'src4.txt', 'dest4.txt')



See also

PEP 3148 - Futures – Execute Computations Asynchronously
PEP written by Brian Quinlan.

Code for Threaded Parallel URL reads, an
example using threads to fetch multiple web pages in parallel.
Code for computing prime numbers in
parallel, an example demonstrating
ProcessPoolExecutor.",pep-3148-the-concurrent-futures-module,"import concurrent.futures, shutil
with concurrent.futures.ThreadPoolExecutor(max_workers=4) as e:
    e.submit(shutil.copy, 'src1.txt', 'dest1.txt')
    e.submit(shutil.copy, 'src2.txt', 'dest2.txt')
    e.submit(shutil.copy, 'src3.txt', 'dest3.txt')
    e.submit(shutil.copy, 'src4.txt', 'dest4.txt')
",3.2
"PEP 3147:  PYC Repository Directories

Python’s scheme for caching bytecode in .pyc files did not work well in
environments with multiple Python interpreters.  If one interpreter encountered
a cached file created by another interpreter, it would recompile the source and
overwrite the cached file, thus losing the benefits of caching.
The issue of “pyc fights” has become more pronounced as it has become
commonplace for Linux distributions to ship with multiple versions of Python.
These conflicts also arise with CPython alternatives such as Unladen Swallow.
To solve this problem, Python’s import machinery has been extended to use
distinct filenames for each interpreter.  Instead of Python 3.2 and Python 3.3 and
Unladen Swallow each competing for a file called “mymodule.pyc”, they will now
look for “mymodule.cpython-32.pyc”, “mymodule.cpython-33.pyc”, and
“mymodule.unladen10.pyc”.  And to prevent all of these new files from
cluttering source directories, the pyc files are now collected in a
“__pycache__” directory stored under the package directory.
Aside from the filenames and target directories, the new scheme has a few
aspects that are visible to the programmer:

Imported modules now have a __cached__ attribute which stores the name
of the actual file that was imported:
>>> import collections
>>> collections.__cached__
'c:/py32/lib/__pycache__/collections.cpython-32.pyc'



The tag that is unique to each interpreter is accessible from the imp
module:
>>> import imp
>>> imp.get_tag()
'cpython-32'



Scripts that try to deduce source filename from the imported file now need to
be smarter.  It is no longer sufficient to simply strip the “c” from a ”.pyc”
filename.  Instead, use the new functions in the imp module:
>>> imp.source_from_cache('c:/py32/lib/__pycache__/collections.cpython-32.pyc')
'c:/py32/lib/collections.py'
>>> imp.cache_from_source('c:/py32/lib/collections.py')
'c:/py32/lib/__pycache__/collections.cpython-32.pyc'



The py_compile and compileall modules have been updated to
reflect the new naming convention and target directory.  The command-line
invocation of compileall has new options: -i for
specifying a list of files and directories to compile and -b which causes
bytecode files to be written to their legacy location rather than
__pycache__.

The importlib.abc module has been updated with new abstract base
classes for loading bytecode files.  The obsolete
ABCs, PyLoader and
PyPycLoader, have been deprecated (instructions on how
to stay Python 3.1 compatible are included with the documentation).



See also

PEP 3147 - PYC Repository Directories
PEP written by Barry Warsaw.",pep-3147-pyc-repository-directories,">>> import collections
>>> collections.__cached__
'c:/py32/lib/__pycache__/collections.cpython-32.pyc'
",3.2
,pep-3147-pyc-repository-directories,">>> import imp
>>> imp.get_tag()
'cpython-32'
",3.2
,pep-3147-pyc-repository-directories,">>> imp.source_from_cache('c:/py32/lib/__pycache__/collections.cpython-32.pyc')
'c:/py32/lib/collections.py'
>>> imp.cache_from_source('c:/py32/lib/collections.py')
'c:/py32/lib/__pycache__/collections.cpython-32.pyc'
",3.2
"PEP 3149: ABI Version Tagged .so Files

The PYC repository directory allows multiple bytecode cache files to be
co-located.  This PEP implements a similar mechanism for shared object files by
giving them a common directory and distinct names for each version.
The common directory is “pyshared” and the file names are made distinct by
identifying the Python implementation (such as CPython, PyPy, Jython, etc.), the
major and minor version numbers, and optional build flags (such as “d” for
debug, “m” for pymalloc, “u” for wide-unicode).  For an arbitrary package “foo”,
you may see these files when the distribution package is installed:
/usr/share/pyshared/foo.cpython-32m.so
/usr/share/pyshared/foo.cpython-33md.so


In Python itself, the tags are accessible from functions in the sysconfig
module:
>>> import sysconfig
>>> sysconfig.get_config_var('SOABI')       # find the version tag
'cpython-32mu'
>>> sysconfig.get_config_var('EXT_SUFFIX')  # find the full filename extension
'.cpython-32mu.so'



See also

PEP 3149 - ABI Version Tagged .so Files
PEP written by Barry Warsaw.",pep-3149-abi-version-tagged-so-files,"/usr/share/pyshared/foo.cpython-32m.so
/usr/share/pyshared/foo.cpython-33md.so
",3.2
,pep-3149-abi-version-tagged-so-files,">>> import sysconfig
>>> sysconfig.get_config_var('SOABI')       # find the version tag
'cpython-32mu'
>>> sysconfig.get_config_var('EXT_SUFFIX')  # find the full filename extension
'.cpython-32mu.so'
",3.2
"Other Language Changes

Some smaller changes made to the core Python language are:

String formatting for format() and str.format() gained new
capabilities for the format character #.  Previously, for integers in
binary, octal, or hexadecimal, it caused the output to be prefixed with ‘0b’,
‘0o’, or ‘0x’ respectively.  Now it can also handle floats, complex, and
Decimal, causing the output to always have a decimal point even when no digits
follow it.
>>> format(20, '#o')
'0o24'
>>> format(12.34, '#5.0f')
'  12.'


(Suggested by Mark Dickinson and implemented by Eric Smith in issue 7094.)

There is also a new str.format_map() method that extends the
capabilities of the existing str.format() method by accepting arbitrary
mapping objects.  This new method makes it possible to use string
formatting with any of Python’s many dictionary-like objects such as
defaultdict, Shelf,
ConfigParser, or dbm.  It is also useful with
custom dict subclasses that normalize keys before look-up or that
supply a __missing__() method for unknown keys:
>>> import shelve
>>> d = shelve.open('tmp.shl')
>>> 'The {project_name} status is {status} as of {date}'.format_map(d)
'The testing project status is green as of February 15, 2011'

>>> class LowerCasedDict(dict):
        def __getitem__(self, key):
            return dict.__getitem__(self, key.lower())
>>> lcd = LowerCasedDict(part='widgets', quantity=10)
>>> 'There are {QUANTITY} {Part} in stock'.format_map(lcd)
'There are 10 widgets in stock'

>>> class PlaceholderDict(dict):
        def __missing__(self, key):
            return '<{}>'.format(key)
>>> 'Hello {name}, welcome to {location}'.format_map(PlaceholderDict())
'Hello <name>, welcome to <location>'





(Suggested by Raymond Hettinger and implemented by Eric Smith in
issue 6081.)

The interpreter can now be started with a quiet option, -q, to prevent
the copyright and version information from being displayed in the interactive
mode.  The option can be introspected using the sys.flags attribute:
$ python -q
>>> sys.flags
sys.flags(debug=0, division_warning=0, inspect=0, interactive=0,
optimize=0, dont_write_bytecode=0, no_user_site=0, no_site=0,
ignore_environment=0, verbose=0, bytes_warning=0, quiet=1)

(Contributed by Marcin Wojdyr in issue 1772833).

The hasattr() function works by calling getattr() and detecting
whether an exception is raised.  This technique allows it to detect methods
created dynamically by __getattr__() or __getattribute__() which
would otherwise be absent from the class dictionary.  Formerly, hasattr
would catch any exception, possibly masking genuine errors.  Now, hasattr
has been tightened to only catch AttributeError and let other
exceptions pass through:
>>> class A:
        @property
        def f(self):
            return 1 // 0

>>> a = A()
>>> hasattr(a, 'f')
Traceback (most recent call last):
  ...
ZeroDivisionError: integer division or modulo by zero


(Discovered by Yury Selivanov and fixed by Benjamin Peterson; issue 9666.)

The str() of a float or complex number is now the same as its
repr(). Previously, the str() form was shorter but that just
caused confusion and is no longer needed now that the shortest possible
repr() is displayed by default:
>>> import math
>>> repr(math.pi)
'3.141592653589793'
>>> str(math.pi)
'3.141592653589793'


(Proposed and implemented by Mark Dickinson; issue 9337.)

memoryview objects now have a release() method
and they also now support the context manager protocol.  This allows timely
release of any resources that were acquired when requesting a buffer from the
original object.
>>> with memoryview(b'abcdefgh') as v:
        print(v.tolist())
[97, 98, 99, 100, 101, 102, 103, 104]


(Added by Antoine Pitrou; issue 9757.)

Previously it was illegal to delete a name from the local namespace if it
occurs as a free variable in a nested block:
def outer(x):
    def inner():
       return x
    inner()
    del x


This is now allowed.  Remember that the target of an except clause
is cleared, so this code which used to work with Python 2.6, raised a
SyntaxError with Python 3.1 and now works again:
def f():
    def print_error():
       print(e)
    try:
       something
    except Exception as e:
       print_error()
       # implicit ""del e"" here


(See issue 4617.)

The internal structsequence tool now creates subclasses of tuple.
This means that C structures like those returned by os.stat(),
time.gmtime(), and sys.version_info now work like a
named tuple and now work with functions and methods that
expect a tuple as an argument.  This is a big step forward in making the C
structures as flexible as their pure Python counterparts:
>>> isinstance(sys.version_info, tuple)
True
>>> 'Version %d.%d.%d %s(%d)' % sys.version_info
'Version 3.2.0 final(0)'


(Suggested by Arfrever Frehtes Taifersar Arahesis and implemented
by Benjamin Peterson in issue 8413.)

Warnings are now easier to control using the PYTHONWARNINGS
environment variable as an alternative to using -W at the command line:
$ export PYTHONWARNINGS='ignore::RuntimeWarning::,once::UnicodeWarning::'

(Suggested by Barry Warsaw and implemented by Philip Jenvey in issue 7301.)

A new warning category, ResourceWarning, has been added.  It is
emitted when potential issues with resource consumption or cleanup
are detected.  It is silenced by default in normal release builds but
can be enabled through the means provided by the warnings
module, or on the command line.
A ResourceWarning is issued at interpreter shutdown if the
gc.garbage list isn’t empty, and if gc.DEBUG_UNCOLLECTABLE is
set, all uncollectable objects are printed.  This is meant to make the
programmer aware that their code contains object finalization issues.
A ResourceWarning is also issued when a file object is destroyed
without having been explicitly closed.  While the deallocator for such
object ensures it closes the underlying operating system resource
(usually, a file descriptor), the delay in deallocating the object could
produce various issues, especially under Windows.  Here is an example
of enabling the warning from the command line:
$ python -q -Wdefault
>>> f = open(""foo"", ""wb"")
>>> del f
__main__:1: ResourceWarning: unclosed file <_io.BufferedWriter name='foo'>

(Added by Antoine Pitrou and Georg Brandl in issue 10093 and issue 477863.)

range objects now support index and count methods. This is part
of an effort to make more objects fully implement the
collections.Sequence abstract base class.  As a result, the
language will have a more uniform API.  In addition, range objects
now support slicing and negative indices, even with values larger than
sys.maxsize.  This makes range more interoperable with lists:
>>> range(0, 100, 2).count(10)
1
>>> range(0, 100, 2).index(10)
5
>>> range(0, 100, 2)[5]
10
>>> range(0, 100, 2)[0:5]
range(0, 10, 2)


(Contributed by Daniel Stutzbach in issue 9213, by Alexander Belopolsky
in issue 2690, and by Nick Coghlan in issue 10889.)

The callable() builtin function from Py2.x was resurrected.  It provides
a concise, readable alternative to using an abstract base class in an
expression like isinstance(x, collections.Callable):
>>> callable(max)
True
>>> callable(20)
False


(See issue 10518.)

Python’s import mechanism can now load modules installed in directories with
non-ASCII characters in the path name.  This solved an aggravating problem
with home directories for users with non-ASCII characters in their usernames.



(Required extensive work by Victor Stinner in issue 9425.)",other-language-changes,">>> format(20, '#o')
'0o24'
>>> format(12.34, '#5.0f')
'  12.'
",3.2
,other-language-changes,">>> import shelve
>>> d = shelve.open('tmp.shl')
>>> 'The {project_name} status is {status} as of {date}'.format_map(d)
'The testing project status is green as of February 15, 2011'

>>> class LowerCasedDict(dict):
        def __getitem__(self, key):
            return dict.__getitem__(self, key.lower())
>>> lcd = LowerCasedDict(part='widgets', quantity=10)
>>> 'There are {QUANTITY} {Part} in stock'.format_map(lcd)
'There are 10 widgets in stock'

>>> class PlaceholderDict(dict):
        def __missing__(self, key):
            return '<{}>'.format(key)
>>> 'Hello {name}, welcome to {location}'.format_map(PlaceholderDict())
'Hello <name>, welcome to <location>'
",3.2
,other-language-changes,">>> class A:
        @property
        def f(self):
            return 1 // 0

>>> a = A()
>>> hasattr(a, 'f')
Traceback (most recent call last):
  ...
ZeroDivisionError: integer division or modulo by zero
",3.2
,other-language-changes,">>> import math
>>> repr(math.pi)
'3.141592653589793'
>>> str(math.pi)
'3.141592653589793'
",3.2
,other-language-changes,">>> with memoryview(b'abcdefgh') as v:
        print(v.tolist())
[97, 98, 99, 100, 101, 102, 103, 104]
",3.2
,other-language-changes,"def outer(x):
    def inner():
       return x
    inner()
    del x
",3.2
,other-language-changes,"def f():
    def print_error():
       print(e)
    try:
       something
    except Exception as e:
       print_error()
       # implicit ""del e"" here
",3.2
,other-language-changes,">>> isinstance(sys.version_info, tuple)
True
>>> 'Version %d.%d.%d %s(%d)' % sys.version_info
'Version 3.2.0 final(0)'
",3.2
,other-language-changes,">>> range(0, 100, 2).count(10)
1
>>> range(0, 100, 2).index(10)
5
>>> range(0, 100, 2)[5]
10
>>> range(0, 100, 2)[0:5]
range(0, 10, 2)
",3.2
,other-language-changes,">>> callable(max)
True
>>> callable(20)
False
",3.2
"functools


The functools module includes a new decorator for caching function
calls.  functools.lru_cache() can save repeated queries to an external
resource whenever the results are expected to be the same.
For example, adding a caching decorator to a database query function can save
database accesses for popular searches:
>>> import functools
>>> @functools.lru_cache(maxsize=300)
>>> def get_phone_number(name):
        c = conn.cursor()
        c.execute('SELECT phonenumber FROM phonelist WHERE name=?', (name,))
        return c.fetchone()[0]


>>> for name in user_requests:
        get_phone_number(name)        # cached lookup


To help with choosing an effective cache size, the wrapped function is
instrumented for tracking cache statistics:
>>> get_phone_number.cache_info()
CacheInfo(hits=4805, misses=980, maxsize=300, currsize=300)


If the phonelist table gets updated, the outdated contents of the cache can be
cleared with:
>>> get_phone_number.cache_clear()


(Contributed by Raymond Hettinger and incorporating design ideas from Jim
Baker, Miki Tebeka, and Nick Coghlan; see recipe 498245, recipe 577479, issue 10586, and
issue 10593.)

The functools.wraps() decorator now adds a __wrapped__ attribute
pointing to the original callable function.  This allows wrapped functions to
be introspected.  It also copies __annotations__ if defined.  And now
it also gracefully skips over missing attributes such as __doc__ which
might not be defined for the wrapped callable.
In the above example, the cache can be removed by recovering the original
function:
>>> get_phone_number = get_phone_number.__wrapped__    # uncached function


(By Nick Coghlan and Terrence Cole; issue 9567, issue 3445, and
issue 8814.)

To help write classes with rich comparison methods, a new decorator
functools.total_ordering() will use a existing equality and inequality
methods to fill in the remaining methods.
For example, supplying __eq__ and __lt__ will enable
total_ordering() to fill-in __le__, __gt__ and __ge__:
@total_ordering
class Student:
    def __eq__(self, other):
        return ((self.lastname.lower(), self.firstname.lower()) ==
                (other.lastname.lower(), other.firstname.lower()))
    def __lt__(self, other):
        return ((self.lastname.lower(), self.firstname.lower()) <
                (other.lastname.lower(), other.firstname.lower()))


With the total_ordering decorator, the remaining comparison methods
are filled in automatically.
(Contributed by Raymond Hettinger.)

To aid in porting programs from Python 2, the functools.cmp_to_key()
function converts an old-style comparison function to
modern key function:
>>> # locale-aware sort order
>>> sorted(iterable, key=cmp_to_key(locale.strcoll))


For sorting examples and a brief sorting tutorial, see the Sorting HowTo tutorial.
(Contributed by Raymond Hettinger.)",new-improved-and-deprecated-modules - functools,">>> import functools
>>> @functools.lru_cache(maxsize=300)
>>> def get_phone_number(name):
        c = conn.cursor()
        c.execute('SELECT phonenumber FROM phonelist WHERE name=?', (name,))
        return c.fetchone()[0]
",3.2
,new-improved-and-deprecated-modules - functools,">>> for name in user_requests:
        get_phone_number(name)        # cached lookup
",3.2
,new-improved-and-deprecated-modules - functools,">>> get_phone_number.cache_info()
CacheInfo(hits=4805, misses=980, maxsize=300, currsize=300)
",3.2
,new-improved-and-deprecated-modules - functools,">>> get_phone_number.cache_clear()
",3.2
,new-improved-and-deprecated-modules - functools,">>> get_phone_number = get_phone_number.__wrapped__    # uncached function
",3.2
,new-improved-and-deprecated-modules - functools,"@total_ordering
class Student:
    def __eq__(self, other):
        return ((self.lastname.lower(), self.firstname.lower()) ==
                (other.lastname.lower(), other.firstname.lower()))
    def __lt__(self, other):
        return ((self.lastname.lower(), self.firstname.lower()) <
                (other.lastname.lower(), other.firstname.lower()))
",3.2
,new-improved-and-deprecated-modules - functools,">>> # locale-aware sort order
>>> sorted(iterable, key=cmp_to_key(locale.strcoll))
",3.2
"itertools


The itertools module has a new accumulate() function
modeled on APL’s scan operator and Numpy’s accumulate function:
>>> from itertools import accumulate
>>> list(accumulate([8, 2, 50]))
[8, 10, 60]


>>> prob_dist = [0.1, 0.4, 0.2, 0.3]
>>> list(accumulate(prob_dist))      # cumulative probability distribution
[0.1, 0.5, 0.7, 1.0]


For an example using accumulate(), see the examples for
the random module.
(Contributed by Raymond Hettinger and incorporating design suggestions
from Mark Dickinson.)",new-improved-and-deprecated-modules - itertools,">>> from itertools import accumulate
>>> list(accumulate([8, 2, 50]))
[8, 10, 60]
",3.2
,new-improved-and-deprecated-modules - itertools,">>> prob_dist = [0.1, 0.4, 0.2, 0.3]
>>> list(accumulate(prob_dist))      # cumulative probability distribution
[0.1, 0.5, 0.7, 1.0]
",3.2
"collections


The collections.Counter class now has two forms of in-place
subtraction, the existing -= operator for saturating subtraction and the new
subtract() method for regular subtraction.  The
former is suitable for multisets
which only have positive counts, and the latter is more suitable for use cases
that allow negative counts:
>>> tally = Counter(dogs=5, cat=3)
>>> tally -= Counter(dogs=2, cats=8)    # saturating subtraction
>>> tally
Counter({'dogs': 3})


>>> tally = Counter(dogs=5, cats=3)
>>> tally.subtract(dogs=2, cats=8)      # regular subtraction
>>> tally
Counter({'dogs': 3, 'cats': -5})


(Contributed by Raymond Hettinger.)

The collections.OrderedDict class has a new method
move_to_end() which takes an existing key and
moves it to either the first or last position in the ordered sequence.
The default is to move an item to the last position.  This is equivalent of
renewing an entry with od[k] = od.pop(k).
A fast move-to-end operation is useful for resequencing entries.  For example,
an ordered dictionary can be used to track order of access by aging entries
from the oldest to the most recently accessed.
>>> d = OrderedDict.fromkeys(['a', 'b', 'X', 'd', 'e'])
>>> list(d)
['a', 'b', 'X', 'd', 'e']
>>> d.move_to_end('X')
>>> list(d)
['a', 'b', 'd', 'e', 'X']


(Contributed by Raymond Hettinger.)

The collections.deque class grew two new methods
count() and reverse() that
make them more substitutable for list objects:
>>> d = deque('simsalabim')
>>> d.count('s')
2
>>> d.reverse()
>>> d
deque(['m', 'i', 'b', 'a', 'l', 'a', 's', 'm', 'i', 's'])


(Contributed by Raymond Hettinger.)",new-improved-and-deprecated-modules - collections,">>> tally = Counter(dogs=5, cat=3)
>>> tally -= Counter(dogs=2, cats=8)    # saturating subtraction
>>> tally
Counter({'dogs': 3})
",3.2
,new-improved-and-deprecated-modules - collections,">>> tally = Counter(dogs=5, cats=3)
>>> tally.subtract(dogs=2, cats=8)      # regular subtraction
>>> tally
Counter({'dogs': 3, 'cats': -5})
",3.2
,new-improved-and-deprecated-modules - collections,">>> d = OrderedDict.fromkeys(['a', 'b', 'X', 'd', 'e'])
>>> list(d)
['a', 'b', 'X', 'd', 'e']
>>> d.move_to_end('X')
>>> list(d)
['a', 'b', 'd', 'e', 'X']
",3.2
,new-improved-and-deprecated-modules - collections,">>> d = deque('simsalabim')
>>> d.count('s')
2
>>> d.reverse()
>>> d
deque(['m', 'i', 'b', 'a', 'l', 'a', 's', 'm', 'i', 's'])
",3.2
"threading

The threading module has a new Barrier
synchronization class for making multiple threads wait until all of them have
reached a common barrier point.  Barriers are useful for making sure that a task
with multiple preconditions does not run until all of the predecessor tasks are
complete.
Barriers can work with an arbitrary number of threads.  This is a generalization
of a Rendezvous which
is defined for only two threads.
Implemented as a two-phase cyclic barrier, Barrier objects
are suitable for use in loops.  The separate filling and draining phases
assure that all threads get released (drained) before any one of them can loop
back and re-enter the barrier.  The barrier fully resets after each cycle.
Example of using barriers:
from threading import Barrier, Thread

def get_votes(site):
    ballots = conduct_election(site)
    all_polls_closed.wait()        # do not count until all polls are closed
    totals = summarize(ballots)
    publish(site, totals)

all_polls_closed = Barrier(len(sites))
for site in sites:
    Thread(target=get_votes, args=(site,)).start()


In this example, the barrier enforces a rule that votes cannot be counted at any
polling site until all polls are closed.  Notice how a solution with a barrier
is similar to one with threading.Thread.join(), but the threads stay alive
and continue to do work (summarizing ballots) after the barrier point is
crossed.
If any of the predecessor tasks can hang or be delayed, a barrier can be created
with an optional timeout parameter.  Then if the timeout period elapses before
all the predecessor tasks reach the barrier point, all waiting threads are
released and a BrokenBarrierError exception is raised:
def get_votes(site):
    ballots = conduct_election(site)
    try:
        all_polls_closed.wait(timeout = midnight - time.now())
    except BrokenBarrierError:
        lockbox = seal_ballots(ballots)
        queue.put(lockbox)
    else:
        totals = summarize(ballots)
        publish(site, totals)


In this example, the barrier enforces a more robust rule.  If some election
sites do not finish before midnight, the barrier times-out and the ballots are
sealed and deposited in a queue for later handling.
See Barrier Synchronization Patterns for
more examples of how barriers can be used in parallel computing.  Also, there is
a simple but thorough explanation of barriers in The Little Book of Semaphores, section 3.6.
(Contributed by Kristján Valur Jónsson with an API review by Jeffrey Yasskin in
issue 8777.)",new-improved-and-deprecated-modules - threading,"from threading import Barrier, Thread

def get_votes(site):
    ballots = conduct_election(site)
    all_polls_closed.wait()        # do not count until all polls are closed
    totals = summarize(ballots)
    publish(site, totals)

all_polls_closed = Barrier(len(sites))
for site in sites:
    Thread(target=get_votes, args=(site,)).start()
",3.2
,new-improved-and-deprecated-modules - threading,"def get_votes(site):
    ballots = conduct_election(site)
    try:
        all_polls_closed.wait(timeout = midnight - time.now())
    except BrokenBarrierError:
        lockbox = seal_ballots(ballots)
        queue.put(lockbox)
    else:
        totals = summarize(ballots)
        publish(site, totals)
",3.2
"datetime and time


The datetime module has a new type timezone that
implements the tzinfo interface by returning a fixed UTC
offset and timezone name. This makes it easier to create timezone-aware
datetime objects:
>>> from datetime import datetime, timezone

>>> datetime.now(timezone.utc)
datetime.datetime(2010, 12, 8, 21, 4, 2, 923754, tzinfo=datetime.timezone.utc)

>>> datetime.strptime(""01/01/2000 12:00 +0000"", ""%m/%d/%Y %H:%M %z"")
datetime.datetime(2000, 1, 1, 12, 0, tzinfo=datetime.timezone.utc)



Also, timedelta objects can now be multiplied by
float and divided by float and int objects.
And timedelta objects can now divide one another.

The datetime.date.strftime() method is no longer restricted to years
after 1900.  The new supported year range is from 1000 to 9999 inclusive.

Whenever a two-digit year is used in a time tuple, the interpretation has been
governed by time.accept2dyear.  The default is True which means that
for a two-digit year, the century is guessed according to the POSIX rules
governing the %y strptime format.
Starting with Py3.2, use of the century guessing heuristic will emit a
DeprecationWarning.  Instead, it is recommended that
time.accept2dyear be set to False so that large date ranges
can be used without guesswork:
>>> import time, warnings
>>> warnings.resetwarnings()      # remove the default warning filters

>>> time.accept2dyear = True      # guess whether 11 means 11 or 2011
>>> time.asctime((11, 1, 1, 12, 34, 56, 4, 1, 0))
Warning (from warnings module):
  ...
DeprecationWarning: Century info guessed for a 2-digit year.
'Fri Jan  1 12:34:56 2011'

>>> time.accept2dyear = False     # use the full range of allowable dates
>>> time.asctime((11, 1, 1, 12, 34, 56, 4, 1, 0))
'Fri Jan  1 12:34:56 11'


Several functions now have significantly expanded date ranges.  When
time.accept2dyear is false, the time.asctime() function will
accept any year that fits in a C int, while the time.mktime() and
time.strftime() functions will accept the full range supported by the
corresponding operating system functions.


(Contributed by Alexander Belopolsky and Victor Stinner in issue 1289118,
issue 5094, issue 6641, issue 2706, issue 1777412, issue 8013,
and issue 10827.)",new-improved-and-deprecated-modules - datetime-and-time,">>> from datetime import datetime, timezone

>>> datetime.now(timezone.utc)
datetime.datetime(2010, 12, 8, 21, 4, 2, 923754, tzinfo=datetime.timezone.utc)

>>> datetime.strptime(""01/01/2000 12:00 +0000"", ""%m/%d/%Y %H:%M %z"")
datetime.datetime(2000, 1, 1, 12, 0, tzinfo=datetime.timezone.utc)
",3.2
,new-improved-and-deprecated-modules - datetime-and-time,">>> import time, warnings
>>> warnings.resetwarnings()      # remove the default warning filters

>>> time.accept2dyear = True      # guess whether 11 means 11 or 2011
>>> time.asctime((11, 1, 1, 12, 34, 56, 4, 1, 0))
Warning (from warnings module):
  ...
DeprecationWarning: Century info guessed for a 2-digit year.
'Fri Jan  1 12:34:56 2011'

>>> time.accept2dyear = False     # use the full range of allowable dates
>>> time.asctime((11, 1, 1, 12, 34, 56, 4, 1, 0))
'Fri Jan  1 12:34:56 11'
",3.2
"math

The math module has been updated with six new functions inspired by the
C99 standard.
The isfinite() function provides a reliable and fast way to detect
special values.  It returns True for regular numbers and False for Nan or
Infinity:
>>> [isfinite(x) for x in (123, 4.56, float('Nan'), float('Inf'))]
[True, True, False, False]


The expm1() function computes e**x-1 for small values of x
without incurring the loss of precision that usually accompanies the subtraction
of nearly equal quantities:
>>> expm1(0.013671875)   # more accurate way to compute e**x-1 for a small x
0.013765762467652909


The erf() function computes a probability integral or Gaussian
error function.  The
complementary error function, erfc(), is 1 - erf(x):
>>> erf(1.0/sqrt(2.0))   # portion of normal distribution within 1 standard deviation
0.682689492137086
>>> erfc(1.0/sqrt(2.0))  # portion of normal distribution outside 1 standard deviation
0.31731050786291404
>>> erf(1.0/sqrt(2.0)) + erfc(1.0/sqrt(2.0))
1.0


The gamma() function is a continuous extension of the factorial
function.  See http://en.wikipedia.org/wiki/Gamma_function for details.  Because
the function is related to factorials, it grows large even for small values of
x, so there is also a lgamma() function for computing the natural
logarithm of the gamma function:
>>> gamma(7.0)           # six factorial
720.0
>>> lgamma(801.0)        # log(800 factorial)
4551.950730698041


(Contributed by Mark Dickinson.)",new-improved-and-deprecated-modules - math,">>> [isfinite(x) for x in (123, 4.56, float('Nan'), float('Inf'))]
[True, True, False, False]
",3.2
,new-improved-and-deprecated-modules - math,">>> expm1(0.013671875)   # more accurate way to compute e**x-1 for a small x
0.013765762467652909
",3.2
,new-improved-and-deprecated-modules - math,">>> erf(1.0/sqrt(2.0))   # portion of normal distribution within 1 standard deviation
0.682689492137086
>>> erfc(1.0/sqrt(2.0))  # portion of normal distribution outside 1 standard deviation
0.31731050786291404
>>> erf(1.0/sqrt(2.0)) + erfc(1.0/sqrt(2.0))
1.0
",3.2
,new-improved-and-deprecated-modules - math,">>> gamma(7.0)           # six factorial
720.0
>>> lgamma(801.0)        # log(800 factorial)
4551.950730698041
",3.2
"abc

The abc module now supports abstractclassmethod() and
abstractstaticmethod().
These tools make it possible to define an abstract base class that
requires a particular classmethod() or staticmethod() to be
implemented:
class Temperature(metaclass=abc.ABCMeta):
    @abc.abstractclassmethod
    def from_fahrenheit(cls, t):
        ...
    @abc.abstractclassmethod
    def from_celsius(cls, t):
        ...


(Patch submitted by Daniel Urban; issue 5867.)",new-improved-and-deprecated-modules - abc,"class Temperature(metaclass=abc.ABCMeta):
    @abc.abstractclassmethod
    def from_fahrenheit(cls, t):
        ...
    @abc.abstractclassmethod
    def from_celsius(cls, t):
        ...
",3.2
"io

The io.BytesIO has a new method, getbuffer(), which
provides functionality similar to memoryview().  It creates an editable
view of the data without making a copy.  The buffer’s random access and support
for slice notation are well-suited to in-place editing:
>>> REC_LEN, LOC_START, LOC_LEN = 34, 7, 11

>>> def change_location(buffer, record_number, location):
        start = record_number * REC_LEN + LOC_START
        buffer[start: start+LOC_LEN] = location

>>> import io

>>> byte_stream = io.BytesIO(
    b'G3805  storeroom  Main chassis    '
    b'X7899  shipping   Reserve cog     '
    b'L6988  receiving  Primary sprocket'
)
>>> buffer = byte_stream.getbuffer()
>>> change_location(buffer, 1, b'warehouse  ')
>>> change_location(buffer, 0, b'showroom   ')
>>> print(byte_stream.getvalue())
b'G3805  showroom   Main chassis    '
b'X7899  warehouse  Reserve cog     '
b'L6988  receiving  Primary sprocket'


(Contributed by Antoine Pitrou in issue 5506.)",new-improved-and-deprecated-modules - io,">>> REC_LEN, LOC_START, LOC_LEN = 34, 7, 11

>>> def change_location(buffer, record_number, location):
        start = record_number * REC_LEN + LOC_START
        buffer[start: start+LOC_LEN] = location

>>> import io

>>> byte_stream = io.BytesIO(
    b'G3805  storeroom  Main chassis    '
    b'X7899  shipping   Reserve cog     '
    b'L6988  receiving  Primary sprocket'
)
>>> buffer = byte_stream.getbuffer()
>>> change_location(buffer, 1, b'warehouse  ')
>>> change_location(buffer, 0, b'showroom   ')
>>> print(byte_stream.getvalue())
b'G3805  showroom   Main chassis    '
b'X7899  warehouse  Reserve cog     '
b'L6988  receiving  Primary sprocket'
",3.2
"reprlib

When writing a __repr__() method for a custom container, it is easy to
forget to handle the case where a member refers back to the container itself.
Python’s builtin objects such as list and set handle
self-reference by displaying ”...” in the recursive part of the representation
string.
To help write such __repr__() methods, the reprlib module has a new
decorator, recursive_repr(), for detecting recursive calls to
__repr__() and substituting a placeholder string instead:
>>> class MyList(list):
        @recursive_repr()
        def __repr__(self):
            return '<' + '|'.join(map(repr, self)) + '>'

>>> m = MyList('abc')
>>> m.append(m)
>>> m.append('x')
>>> print(m)
<'a'|'b'|'c'|...|'x'>


(Contributed by Raymond Hettinger in issue 9826 and issue 9840.)",new-improved-and-deprecated-modules - reprlib,">>> class MyList(list):
        @recursive_repr()
        def __repr__(self):
            return '<' + '|'.join(map(repr, self)) + '>'

>>> m = MyList('abc')
>>> m.append(m)
>>> m.append('x')
>>> print(m)
<'a'|'b'|'c'|...|'x'>
",3.2
"logging

In addition to dictionary-based configuration described above, the
logging package has many other improvements.
The logging documentation has been augmented by a basic tutorial, an advanced tutorial, and a cookbook of
logging recipes.  These documents are the fastest way to learn about logging.
The logging.basicConfig() set-up function gained a style argument to
support three different types of string formatting.  It defaults to “%” for
traditional %-formatting, can be set to “{” for the new str.format() style, or
can be set to “$” for the shell-style formatting provided by
string.Template.  The following three configurations are equivalent:
>>> from logging import basicConfig
>>> basicConfig(style='%', format=""%(name)s -> %(levelname)s: %(message)s"")
>>> basicConfig(style='{', format=""{name} -> {levelname} {message}"")
>>> basicConfig(style='$', format=""$name -> $levelname: $message"")


If no configuration is set-up before a logging event occurs, there is now a
default configuration using a StreamHandler directed to
sys.stderr for events of WARNING level or higher.  Formerly, an
event occurring before a configuration was set-up would either raise an
exception or silently drop the event depending on the value of
logging.raiseExceptions.  The new default handler is stored in
logging.lastResort.
The use of filters has been simplified.  Instead of creating a
Filter object, the predicate can be any Python callable that
returns True or False.
There were a number of other improvements that add flexibility and simplify
configuration.  See the module documentation for a full listing of changes in
Python 3.2.",new-improved-and-deprecated-modules - logging,">>> from logging import basicConfig
>>> basicConfig(style='%', format=""%(name)s -> %(levelname)s: %(message)s"")
>>> basicConfig(style='{', format=""{name} -> {levelname} {message}"")
>>> basicConfig(style='$', format=""$name -> $levelname: $message"")
",3.2
"csv

The csv module now supports a new dialect, unix_dialect,
which applies quoting for all fields and a traditional Unix style with '\n' as
the line terminator.  The registered dialect name is unix.
The csv.DictWriter has a new method,
writeheader() for writing-out an initial row to document
the field names:
>>> import csv, sys
>>> w = csv.DictWriter(sys.stdout, ['name', 'dept'], dialect='unix')
>>> w.writeheader()
""name"",""dept""
>>> w.writerows([
        {'name': 'tom', 'dept': 'accounting'},
        {'name': 'susan', 'dept': 'Salesl'}])
""tom"",""accounting""
""susan"",""sales""


(New dialect suggested by Jay Talbot in issue 5975, and the new method
suggested by Ed Abraham in issue 1537721.)",new-improved-and-deprecated-modules - csv,">>> import csv, sys
>>> w = csv.DictWriter(sys.stdout, ['name', 'dept'], dialect='unix')
>>> w.writeheader()
""name"",""dept""
>>> w.writerows([
        {'name': 'tom', 'dept': 'accounting'},
        {'name': 'susan', 'dept': 'Salesl'}])
""tom"",""accounting""
""susan"",""sales""
",3.2
"contextlib

There is a new and slightly mind-blowing tool
ContextDecorator that is helpful for creating a
context manager that does double duty as a function decorator.
As a convenience, this new functionality is used by
contextmanager() so that no extra effort is needed to support
both roles.
The basic idea is that both context managers and function decorators can be used
for pre-action and post-action wrappers.  Context managers wrap a group of
statements using a with statement, and function decorators wrap a
group of statements enclosed in a function.  So, occasionally there is a need to
write a pre-action or post-action wrapper that can be used in either role.
For example, it is sometimes useful to wrap functions or groups of statements
with a logger that can track the time of entry and time of exit.  Rather than
writing both a function decorator and a context manager for the task, the
contextmanager() provides both capabilities in a single
definition:
from contextlib import contextmanager
import logging

logging.basicConfig(level=logging.INFO)

@contextmanager
def track_entry_and_exit(name):
    logging.info('Entering: {}'.format(name))
    yield
    logging.info('Exiting: {}'.format(name))


Formerly, this would have only been usable as a context manager:
with track_entry_and_exit('widget loader'):
    print('Some time consuming activity goes here')
    load_widget()


Now, it can be used as a decorator as well:
@track_entry_and_exit('widget loader')
def activity():
    print('Some time consuming activity goes here')
    load_widget()


Trying to fulfill two roles at once places some limitations on the technique.
Context managers normally have the flexibility to return an argument usable by
a with statement, but there is no parallel for function decorators.
In the above example, there is not a clean way for the track_entry_and_exit
context manager to return a logging instance for use in the body of enclosed
statements.
(Contributed by Michael Foord in issue 9110.)",new-improved-and-deprecated-modules - contextlib,"from contextlib import contextmanager
import logging

logging.basicConfig(level=logging.INFO)

@contextmanager
def track_entry_and_exit(name):
    logging.info('Entering: {}'.format(name))
    yield
    logging.info('Exiting: {}'.format(name))
",3.2
,new-improved-and-deprecated-modules - contextlib,"with track_entry_and_exit('widget loader'):
    print('Some time consuming activity goes here')
    load_widget()
",3.2
,new-improved-and-deprecated-modules - contextlib,"@track_entry_and_exit('widget loader')
def activity():
    print('Some time consuming activity goes here')
    load_widget()
",3.2
"decimal and fractions

Mark Dickinson crafted an elegant and efficient scheme for assuring that
different numeric datatypes will have the same hash value whenever their actual
values are equal (issue 8188):
assert hash(Fraction(3, 2)) == hash(1.5) == \
       hash(Decimal(""1.5"")) == hash(complex(1.5, 0))


Some of the hashing details are exposed through a new attribute,
sys.hash_info, which describes the bit width of the hash value, the
prime modulus, the hash values for infinity and nan, and the multiplier
used for the imaginary part of a number:
>>> sys.hash_info
sys.hash_info(width=64, modulus=2305843009213693951, inf=314159, nan=0, imag=1000003)


An early decision to limit the inter-operability of various numeric types has
been relaxed.  It is still unsupported (and ill-advised) to have implicit
mixing in arithmetic expressions such as Decimal('1.1') + float('1.1')
because the latter loses information in the process of constructing the binary
float.  However, since existing floating point value can be converted losslessly
to either a decimal or rational representation, it makes sense to add them to
the constructor and to support mixed-type comparisons.

The decimal.Decimal constructor now accepts float objects
directly so there in no longer a need to use the from_float()
method (issue 8257).
Mixed type comparisons are now fully supported so that
Decimal objects can be directly compared with float
and fractions.Fraction (issue 2531 and issue 8188).

Similar changes were made to fractions.Fraction so that the
from_float() and from_decimal()
methods are no longer needed (issue 8294):
>>> Decimal(1.1)
Decimal('1.100000000000000088817841970012523233890533447265625')
>>> Fraction(1.1)
Fraction(2476979795053773, 2251799813685248)


Another useful change for the decimal module is that the
Context.clamp attribute is now public.  This is useful in creating
contexts that correspond to the decimal interchange formats specified in IEEE
754 (see issue 8540).
(Contributed by Mark Dickinson and Raymond Hettinger.)",new-improved-and-deprecated-modules - decimal-and-fractions,"assert hash(Fraction(3, 2)) == hash(1.5) == \
       hash(Decimal(""1.5"")) == hash(complex(1.5, 0))
",3.2
,new-improved-and-deprecated-modules - decimal-and-fractions,">>> sys.hash_info
sys.hash_info(width=64, modulus=2305843009213693951, inf=314159, nan=0, imag=1000003)
",3.2
,new-improved-and-deprecated-modules - decimal-and-fractions,">>> Decimal(1.1)
Decimal('1.100000000000000088817841970012523233890533447265625')
>>> Fraction(1.1)
Fraction(2476979795053773, 2251799813685248)
",3.2
"ftp

The ftplib.FTP class now supports the context manager protocol to
unconditionally consume socket.error exceptions and to close the FTP
connection when done:
>>> from ftplib import FTP
>>> with FTP(""ftp1.at.proftpd.org"") as ftp:
        ftp.login()
        ftp.dir()

'230 Anonymous login ok, restrictions apply.'
dr-xr-xr-x   9 ftp      ftp           154 May  6 10:43 .
dr-xr-xr-x   9 ftp      ftp           154 May  6 10:43 ..
dr-xr-xr-x   5 ftp      ftp          4096 May  6 10:43 CentOS
dr-xr-xr-x   3 ftp      ftp            18 Jul 10  2008 Fedora


Other file-like objects such as mmap.mmap and fileinput.input()
also grew auto-closing context managers:
with fileinput.input(files=('log1.txt', 'log2.txt')) as f:
    for line in f:
        process(line)


(Contributed by Tarek Ziadé and Giampaolo Rodolà in issue 4972, and
by Georg Brandl in issue 8046 and issue 1286.)
The FTP_TLS class now accepts a context parameter, which is a
ssl.SSLContext object allowing bundling SSL configuration options,
certificates and private keys into a single (potentially long-lived) structure.
(Contributed by Giampaolo Rodolà; issue 8806.)",new-improved-and-deprecated-modules - ftp,">>> from ftplib import FTP
>>> with FTP(""ftp1.at.proftpd.org"") as ftp:
        ftp.login()
        ftp.dir()

'230 Anonymous login ok, restrictions apply.'
dr-xr-xr-x   9 ftp      ftp           154 May  6 10:43 .
dr-xr-xr-x   9 ftp      ftp           154 May  6 10:43 ..
dr-xr-xr-x   5 ftp      ftp          4096 May  6 10:43 CentOS
dr-xr-xr-x   3 ftp      ftp            18 Jul 10  2008 Fedora
",3.2
,new-improved-and-deprecated-modules - ftp,"with fileinput.input(files=('log1.txt', 'log2.txt')) as f:
    for line in f:
        process(line)
",3.2
"select

The select module now exposes a new, constant attribute,
PIPE_BUF, which gives the minimum number of bytes which are
guaranteed not to block when select.select() says a pipe is ready
for writing.
>>> import select
>>> select.PIPE_BUF
512


(Available on Unix systems. Patch by Sébastien Sablé in issue 9862)",new-improved-and-deprecated-modules - select,">>> import select
>>> select.PIPE_BUF
512
",3.2
"gzip and zipfile

gzip.GzipFile now implements the io.BufferedIOBase
abstract base class (except for truncate()).  It also has a
peek() method and supports unseekable as well as
zero-padded file objects.
The gzip module also gains the compress() and
decompress() functions for easier in-memory compression and
decompression.  Keep in mind that text needs to be encoded as bytes
before compressing and decompressing:
>>> s = 'Three shall be the number thou shalt count, '
>>> s += 'and the number of the counting shall be three'
>>> b = s.encode()                        # convert to utf-8
>>> len(b)
89
>>> c = gzip.compress(b)
>>> len(c)
77
>>> gzip.decompress(c).decode()[:42]      # decompress and convert to text
'Three shall be the number thou shalt count,'


(Contributed by Anand B. Pillai in issue 3488; and by Antoine Pitrou, Nir
Aides and Brian Curtin in issue 9962, issue 1675951, issue 7471 and
issue 2846.)
Also, the zipfile.ZipExtFile class was reworked internally to represent
files stored inside an archive.  The new implementation is significantly faster
and can be wrapped in a io.BufferedReader object for more speedups.  It
also solves an issue where interleaved calls to read and readline gave the
wrong results.
(Patch submitted by Nir Aides in issue 7610.)",new-improved-and-deprecated-modules - gzip-and-zipfile,">>> s = 'Three shall be the number thou shalt count, '
>>> s += 'and the number of the counting shall be three'
>>> b = s.encode()                        # convert to utf-8
>>> len(b)
89
>>> c = gzip.compress(b)
>>> len(c)
77
>>> gzip.decompress(c).decode()[:42]      # decompress and convert to text
'Three shall be the number thou shalt count,'
",3.2
"tarfile

The TarFile class can now be used as a context manager.  In
addition, its add() method has a new option, filter,
that controls which files are added to the archive and allows the file metadata
to be edited.
The new filter option replaces the older, less flexible exclude parameter
which is now deprecated.  If specified, the optional filter parameter needs to
be a keyword argument.  The user-supplied filter function accepts a
TarInfo object and returns an updated
TarInfo object, or if it wants the file to be excluded, the
function can return None:
>>> import tarfile, glob

>>> def myfilter(tarinfo):
       if tarinfo.isfile():             # only save real files
            tarinfo.uname = 'monty'     # redact the user name
            return tarinfo

>>> with tarfile.open(name='myarchive.tar.gz', mode='w:gz') as tf:
        for filename in glob.glob('*.txt'):
            tf.add(filename, filter=myfilter)
        tf.list()
-rw-r--r-- monty/501        902 2011-01-26 17:59:11 annotations.txt
-rw-r--r-- monty/501        123 2011-01-26 17:59:11 general_questions.txt
-rw-r--r-- monty/501       3514 2011-01-26 17:59:11 prion.txt
-rw-r--r-- monty/501        124 2011-01-26 17:59:11 py_todo.txt
-rw-r--r-- monty/501       1399 2011-01-26 17:59:11 semaphore_notes.txt


(Proposed by Tarek Ziadé and implemented by Lars Gustäbel in issue 6856.)",new-improved-and-deprecated-modules - tarfile,">>> import tarfile, glob

>>> def myfilter(tarinfo):
       if tarinfo.isfile():             # only save real files
            tarinfo.uname = 'monty'     # redact the user name
            return tarinfo

>>> with tarfile.open(name='myarchive.tar.gz', mode='w:gz') as tf:
        for filename in glob.glob('*.txt'):
            tf.add(filename, filter=myfilter)
        tf.list()
-rw-r--r-- monty/501        902 2011-01-26 17:59:11 annotations.txt
-rw-r--r-- monty/501        123 2011-01-26 17:59:11 general_questions.txt
-rw-r--r-- monty/501       3514 2011-01-26 17:59:11 prion.txt
-rw-r--r-- monty/501        124 2011-01-26 17:59:11 py_todo.txt
-rw-r--r-- monty/501       1399 2011-01-26 17:59:11 semaphore_notes.txt
",3.2
"hashlib

The hashlib module has two new constant attributes listing the hashing
algorithms guaranteed to be present in all implementations and those available
on the current implementation:
>>> import hashlib

>>> hashlib.algorithms_guaranteed
{'sha1', 'sha224', 'sha384', 'sha256', 'sha512', 'md5'}

>>> hashlib.algorithms_available
{'md2', 'SHA256', 'SHA512', 'dsaWithSHA', 'mdc2', 'SHA224', 'MD4', 'sha256',
'sha512', 'ripemd160', 'SHA1', 'MDC2', 'SHA', 'SHA384', 'MD2',
'ecdsa-with-SHA1','md4', 'md5', 'sha1', 'DSA-SHA', 'sha224',
'dsaEncryption', 'DSA', 'RIPEMD160', 'sha', 'MD5', 'sha384'}


(Suggested by Carl Chenet in issue 7418.)",new-improved-and-deprecated-modules - hashlib,">>> import hashlib

>>> hashlib.algorithms_guaranteed
{'sha1', 'sha224', 'sha384', 'sha256', 'sha512', 'md5'}

>>> hashlib.algorithms_available
{'md2', 'SHA256', 'SHA512', 'dsaWithSHA', 'mdc2', 'SHA224', 'MD4', 'sha256',
'sha512', 'ripemd160', 'SHA1', 'MDC2', 'SHA', 'SHA384', 'MD2',
'ecdsa-with-SHA1','md4', 'md5', 'sha1', 'DSA-SHA', 'sha224',
'dsaEncryption', 'DSA', 'RIPEMD160', 'sha', 'MD5', 'sha384'}
",3.2
"ast

The ast module has a wonderful a general-purpose tool for safely
evaluating expression strings using the Python literal
syntax.  The ast.literal_eval() function serves as a secure alternative to
the builtin eval() function which is easily abused.  Python 3.2 adds
bytes and set literals to the list of supported types:
strings, bytes, numbers, tuples, lists, dicts, sets, booleans, and None.
>>> from ast import literal_eval

>>> request = ""{'req': 3, 'func': 'pow', 'args': (2, 0.5)}""
>>> literal_eval(request)
{'args': (2, 0.5), 'req': 3, 'func': 'pow'}

>>> request = ""os.system('do something harmful')""
>>> literal_eval(request)
Traceback (most recent call last):
  ...
ValueError: malformed node or string: <_ast.Call object at 0x101739a10>


(Implemented by Benjamin Peterson and Georg Brandl.)",new-improved-and-deprecated-modules - ast,">>> from ast import literal_eval

>>> request = ""{'req': 3, 'func': 'pow', 'args': (2, 0.5)}""
>>> literal_eval(request)
{'args': (2, 0.5), 'req': 3, 'func': 'pow'}

>>> request = ""os.system('do something harmful')""
>>> literal_eval(request)
Traceback (most recent call last):
  ...
ValueError: malformed node or string: <_ast.Call object at 0x101739a10>
",3.2
"os

Different operating systems use various encodings for filenames and environment
variables.  The os module provides two new functions,
fsencode() and fsdecode(), for encoding and decoding
filenames:
>>> filename = 'Sehenswürdigkeiten'
>>> os.fsencode(filename)
b'Sehensw\xc3\xbcrdigkeiten'


Some operating systems allow direct access to encoded bytes in the
environment.  If so, the os.supports_bytes_environ constant will be
true.
For direct access to encoded environment variables (if available),
use the new os.getenvb() function or use os.environb
which is a bytes version of os.environ.
(Contributed by Victor Stinner.)",new-improved-and-deprecated-modules - os,">>> filename = 'Sehenswürdigkeiten'
>>> os.fsencode(filename)
b'Sehensw\xc3\xbcrdigkeiten'
",3.2
"shutil

The shutil.copytree() function has two new options:

ignore_dangling_symlinks: when symlinks=False so that the function
copies a file pointed to by a symlink, not the symlink itself. This option
will silence the error raised if the file doesn’t exist.
copy_function: is a callable that will be used to copy files.
shutil.copy2() is used by default.

(Contributed by Tarek Ziadé.)
In addition, the shutil module now supports archiving operations for zipfiles, uncompressed tarfiles, gzipped tarfiles,
and bzipped tarfiles.  And there are functions for registering additional
archiving file formats (such as xz compressed tarfiles or custom formats).
The principal functions are make_archive() and
unpack_archive().  By default, both operate on the current
directory (which can be set by os.chdir()) and on any sub-directories.
The archive filename needs to be specified with a full pathname.  The archiving
step is non-destructive (the original files are left unchanged).
>>> import shutil, pprint

>>> os.chdir('mydata')                               # change to the source directory
>>> f = shutil.make_archive('/var/backup/mydata',
                            'zip')                   # archive the current directory
>>> f                                                # show the name of archive
'/var/backup/mydata.zip'
>>> os.chdir('tmp')                                  # change to an unpacking
>>> shutil.unpack_archive('/var/backup/mydata.zip')  # recover the data

>>> pprint.pprint(shutil.get_archive_formats())      # display known formats
[('bztar', ""bzip2'ed tar-file""),
 ('gztar', ""gzip'ed tar-file""),
 ('tar', 'uncompressed tar file'),
 ('zip', 'ZIP file')]

>>> shutil.register_archive_format(                  # register a new archive format
        name = 'xz',
        function = xz.compress,                      # callable archiving function
        extra_args = [('level', 8)],                 # arguments to the function
        description = 'xz compression'
)


(Contributed by Tarek Ziadé.)",new-improved-and-deprecated-modules - shutil,">>> import shutil, pprint

>>> os.chdir('mydata')                               # change to the source directory
>>> f = shutil.make_archive('/var/backup/mydata',
                            'zip')                   # archive the current directory
>>> f                                                # show the name of archive
'/var/backup/mydata.zip'
>>> os.chdir('tmp')                                  # change to an unpacking
>>> shutil.unpack_archive('/var/backup/mydata.zip')  # recover the data

>>> pprint.pprint(shutil.get_archive_formats())      # display known formats
[('bztar', ""bzip2'ed tar-file""),
 ('gztar', ""gzip'ed tar-file""),
 ('tar', 'uncompressed tar file'),
 ('zip', 'ZIP file')]

>>> shutil.register_archive_format(                  # register a new archive format
        name = 'xz',
        function = xz.compress,                      # callable archiving function
        extra_args = [('level', 8)],                 # arguments to the function
        description = 'xz compression'
)
",3.2
"html

A new html module was introduced with only a single function,
escape(), which is used for escaping reserved characters from HTML
markup:
>>> import html
>>> html.escape('x > 2 && x < 7')
'x &gt; 2 &amp;&amp; x &lt; 7'",new-improved-and-deprecated-modules - html,">>> import html
>>> html.escape('x > 2 && x < 7')
'x &gt; 2 &amp;&amp; x &lt; 7'
",3.2
"unittest

The unittest module has a number of improvements supporting test discovery for
packages, easier experimentation at the interactive prompt, new testcase
methods, improved diagnostic messages for test failures, and better method
names.

The command-line call python -m unittest can now accept file paths
instead of module names for running specific tests (issue 10620).  The new
test discovery can find tests within packages, locating any test importable
from the top-level directory.  The top-level directory can be specified with
the -t option, a pattern for matching files with -p, and a directory to
start discovery with -s:
$ python -m unittest discover -s my_proj_dir -p _test.py

(Contributed by Michael Foord.)

Experimentation at the interactive prompt is now easier because the
unittest.case.TestCase class can now be instantiated without
arguments:
>>> TestCase().assertEqual(pow(2, 3), 8)


(Contributed by Michael Foord.)

The unittest module has two new methods,
assertWarns() and
assertWarnsRegex() to verify that a given warning type
is triggered by the code under test:
with self.assertWarns(DeprecationWarning):
    legacy_function('XYZ')


(Contributed by Antoine Pitrou, issue 9754.)
Another new method, assertCountEqual() is used to
compare two iterables to determine if their element counts are equal (whether
the same elements are present with the same number of occurrences regardless
of order):
def test_anagram(self):
    self.assertCountEqual('algorithm', 'logarithm')


(Contributed by Raymond Hettinger.)

A principal feature of the unittest module is an effort to produce meaningful
diagnostics when a test fails.  When possible, the failure is recorded along
with a diff of the output.  This is especially helpful for analyzing log files
of failed test runs. However, since diffs can sometime be voluminous, there is
a new maxDiff attribute that sets maximum length of
diffs displayed.

In addition, the method names in the module have undergone a number of clean-ups.
For example, assertRegex() is the new name for
assertRegexpMatches() which was misnamed because the
test uses re.search(), not re.match().  Other methods using
regular expressions are now named using short form “Regex” in preference to
“Regexp” – this matches the names used in other unittest implementations,
matches Python’s old name for the re module, and it has unambiguous
camel-casing.
(Contributed by Raymond Hettinger and implemented by Ezio Melotti.)

To improve consistency, some long-standing method aliases are being
deprecated in favor of the preferred names:







Old Name

Preferred Name




assert_()

assertTrue()


assertEquals()

assertEqual()


assertNotEquals()

assertNotEqual()


assertAlmostEquals()

assertAlmostEqual()


assertNotAlmostEquals()

assertNotAlmostEqual()





Likewise, the TestCase.fail* methods deprecated in Python 3.1 are expected
to be removed in Python 3.3.  Also see the Deprecated aliases section in
the unittest documentation.
(Contributed by Ezio Melotti; issue 9424.)

The assertDictContainsSubset() method was deprecated
because it was misimplemented with the arguments in the wrong order.  This
created hard-to-debug optical illusions where tests like
TestCase().assertDictContainsSubset({'a':1, 'b':2}, {'a':1}) would fail.
(Contributed by Raymond Hettinger.)",new-improved-and-deprecated-modules - unittest,">>> TestCase().assertEqual(pow(2, 3), 8)
",3.2
,new-improved-and-deprecated-modules - unittest,"with self.assertWarns(DeprecationWarning):
    legacy_function('XYZ')
",3.2
,new-improved-and-deprecated-modules - unittest,"def test_anagram(self):
    self.assertCountEqual('algorithm', 'logarithm')
",3.2
"tempfile

The tempfile module has a new context manager,
TemporaryDirectory which provides easy deterministic
cleanup of temporary directories:
with tempfile.TemporaryDirectory() as tmpdirname:
    print('created temporary dir:', tmpdirname)


(Contributed by Neil Schemenauer and Nick Coghlan; issue 5178.)",new-improved-and-deprecated-modules - tempfile,"with tempfile.TemporaryDirectory() as tmpdirname:
    print('created temporary dir:', tmpdirname)
",3.2
"inspect


The inspect module has a new function
getgeneratorstate() to easily identify the current state of a
generator-iterator:
>>> from inspect import getgeneratorstate
>>> def gen():
        yield 'demo'
>>> g = gen()
>>> getgeneratorstate(g)
'GEN_CREATED'
>>> next(g)
'demo'
>>> getgeneratorstate(g)
'GEN_SUSPENDED'
>>> next(g, None)
>>> getgeneratorstate(g)
'GEN_CLOSED'


(Contributed by Rodolpho Eckhardt and Nick Coghlan, issue 10220.)

To support lookups without the possibility of activating a dynamic attribute,
the inspect module has a new function, getattr_static().
Unlike hasattr(), this is a true read-only search, guaranteed not to
change state while it is searching:
>>> class A:
        @property
        def f(self):
            print('Running')
            return 10

>>> a = A()
>>> getattr(a, 'f')
Running
10
>>> inspect.getattr_static(a, 'f')
<property object at 0x1022bd788>





(Contributed by Michael Foord.)",new-improved-and-deprecated-modules - inspect,">>> from inspect import getgeneratorstate
>>> def gen():
        yield 'demo'
>>> g = gen()
>>> getgeneratorstate(g)
'GEN_CREATED'
>>> next(g)
'demo'
>>> getgeneratorstate(g)
'GEN_SUSPENDED'
>>> next(g, None)
>>> getgeneratorstate(g)
'GEN_CLOSED'
",3.2
,new-improved-and-deprecated-modules - inspect,">>> class A:
        @property
        def f(self):
            print('Running')
            return 10

>>> a = A()
>>> getattr(a, 'f')
Running
10
>>> inspect.getattr_static(a, 'f')
<property object at 0x1022bd788>
",3.2
"dis

The dis module gained two new functions for inspecting code,
code_info() and show_code().  Both provide detailed code
object information for the supplied function, method, source code string or code
object.  The former returns a string and the latter prints it:
>>> import dis, random
>>> dis.show_code(random.choice)
Name:              choice
Filename:          /Library/Frameworks/Python.framework/Versions/3.2/lib/python3.2/random.py
Argument count:    2
Kw-only arguments: 0
Number of locals:  3
Stack size:        11
Flags:             OPTIMIZED, NEWLOCALS, NOFREE
Constants:
   0: 'Choose a random element from a non-empty sequence.'
   1: 'Cannot choose from an empty sequence'
Names:
   0: _randbelow
   1: len
   2: ValueError
   3: IndexError
Variable names:
   0: self
   1: seq
   2: i


In addition, the dis() function now accepts string arguments
so that the common idiom dis(compile(s, '', 'eval')) can be shortened
to dis(s):
>>> dis('3*x+1 if x%2==1 else x//2')
  1           0 LOAD_NAME                0 (x)
              3 LOAD_CONST               0 (2)
              6 BINARY_MODULO
              7 LOAD_CONST               1 (1)
             10 COMPARE_OP               2 (==)
             13 POP_JUMP_IF_FALSE       28
             16 LOAD_CONST               2 (3)
             19 LOAD_NAME                0 (x)
             22 BINARY_MULTIPLY
             23 LOAD_CONST               1 (1)
             26 BINARY_ADD
             27 RETURN_VALUE
        >>   28 LOAD_NAME                0 (x)
             31 LOAD_CONST               0 (2)
             34 BINARY_FLOOR_DIVIDE
             35 RETURN_VALUE


Taken together, these improvements make it easier to explore how CPython is
implemented and to see for yourself what the language syntax does
under-the-hood.
(Contributed by Nick Coghlan in issue 9147.)",new-improved-and-deprecated-modules - dis,">>> import dis, random
>>> dis.show_code(random.choice)
Name:              choice
Filename:          /Library/Frameworks/Python.framework/Versions/3.2/lib/python3.2/random.py
Argument count:    2
Kw-only arguments: 0
Number of locals:  3
Stack size:        11
Flags:             OPTIMIZED, NEWLOCALS, NOFREE
Constants:
   0: 'Choose a random element from a non-empty sequence.'
   1: 'Cannot choose from an empty sequence'
Names:
   0: _randbelow
   1: len
   2: ValueError
   3: IndexError
Variable names:
   0: self
   1: seq
   2: i
",3.2
,new-improved-and-deprecated-modules - dis,">>> dis('3*x+1 if x%2==1 else x//2')
  1           0 LOAD_NAME                0 (x)
              3 LOAD_CONST               0 (2)
              6 BINARY_MODULO
              7 LOAD_CONST               1 (1)
             10 COMPARE_OP               2 (==)
             13 POP_JUMP_IF_FALSE       28
             16 LOAD_CONST               2 (3)
             19 LOAD_NAME                0 (x)
             22 BINARY_MULTIPLY
             23 LOAD_CONST               1 (1)
             26 BINARY_ADD
             27 RETURN_VALUE
        >>   28 LOAD_NAME                0 (x)
             31 LOAD_CONST               0 (2)
             34 BINARY_FLOOR_DIVIDE
             35 RETURN_VALUE
",3.2
"site

The site module has three new functions useful for reporting on the
details of a given Python installation.

getsitepackages() lists all global site-packages directories.
getuserbase() reports on the user’s base directory where data can
be stored.
getusersitepackages() reveals the user-specific site-packages
directory path.

>>> import site
>>> site.getsitepackages()
['/Library/Frameworks/Python.framework/Versions/3.2/lib/python3.2/site-packages',
 '/Library/Frameworks/Python.framework/Versions/3.2/lib/site-python',
 '/Library/Python/3.2/site-packages']
>>> site.getuserbase()
'/Users/raymondhettinger/Library/Python/3.2'
>>> site.getusersitepackages()
'/Users/raymondhettinger/Library/Python/3.2/lib/python/site-packages'


Conveniently, some of site’s functionality is accessible directly from the
command-line:
$ python -m site --user-base
/Users/raymondhettinger/.local
$ python -m site --user-site
/Users/raymondhettinger/.local/lib/python3.2/site-packages

(Contributed by Tarek Ziadé in issue 6693.)",new-improved-and-deprecated-modules - site,">>> import site
>>> site.getsitepackages()
['/Library/Frameworks/Python.framework/Versions/3.2/lib/python3.2/site-packages',
 '/Library/Frameworks/Python.framework/Versions/3.2/lib/site-python',
 '/Library/Python/3.2/site-packages']
>>> site.getuserbase()
'/Users/raymondhettinger/Library/Python/3.2'
>>> site.getusersitepackages()
'/Users/raymondhettinger/Library/Python/3.2/lib/python/site-packages'
",3.2
"sysconfig

The new sysconfig module makes it straightforward to discover
installation paths and configuration variables that vary across platforms and
installations.
The module offers access simple access functions for platform and version
information:

get_platform() returning values like linux-i586 or
macosx-10.6-ppc.
get_python_version() returns a Python version string
such as “3.2”.

It also provides access to the paths and variables corresponding to one of
seven named schemes used by distutils.  Those include posix_prefix,
posix_home, posix_user, nt, nt_user, os2, os2_home:

get_paths() makes a dictionary containing installation paths
for the current installation scheme.
get_config_vars() returns a dictionary of platform specific
variables.

There is also a convenient command-line interface:
C:\Python32>python -m sysconfig
Platform: ""win32""
Python version: ""3.2""
Current installation scheme: ""nt""

Paths:
        data = ""C:\Python32""
        include = ""C:\Python32\Include""
        platinclude = ""C:\Python32\Include""
        platlib = ""C:\Python32\Lib\site-packages""
        platstdlib = ""C:\Python32\Lib""
        purelib = ""C:\Python32\Lib\site-packages""
        scripts = ""C:\Python32\Scripts""
        stdlib = ""C:\Python32\Lib""

Variables:
        BINDIR = ""C:\Python32""
        BINLIBDEST = ""C:\Python32\Lib""
        EXE = "".exe""
        INCLUDEPY = ""C:\Python32\Include""
        LIBDEST = ""C:\Python32\Lib""
        SO = "".pyd""
        VERSION = ""32""
        abiflags = """"
        base = ""C:\Python32""
        exec_prefix = ""C:\Python32""
        platbase = ""C:\Python32""
        prefix = ""C:\Python32""
        projectbase = ""C:\Python32""
        py_version = ""3.2""
        py_version_nodot = ""32""
        py_version_short = ""3.2""
        srcdir = ""C:\Python32""
        userbase = ""C:\Documents and Settings\Raymond\Application Data\Python""


(Moved out of Distutils by Tarek Ziadé.)",new-improved-and-deprecated-modules - sysconfig,"C:\Python32>python -m sysconfig
Platform: ""win32""
Python version: ""3.2""
Current installation scheme: ""nt""

Paths:
        data = ""C:\Python32""
        include = ""C:\Python32\Include""
        platinclude = ""C:\Python32\Include""
        platlib = ""C:\Python32\Lib\site-packages""
        platstdlib = ""C:\Python32\Lib""
        purelib = ""C:\Python32\Lib\site-packages""
        scripts = ""C:\Python32\Scripts""
        stdlib = ""C:\Python32\Lib""

Variables:
        BINDIR = ""C:\Python32""
        BINLIBDEST = ""C:\Python32\Lib""
        EXE = "".exe""
        INCLUDEPY = ""C:\Python32\Include""
        LIBDEST = ""C:\Python32\Lib""
        SO = "".pyd""
        VERSION = ""32""
        abiflags = """"
        base = ""C:\Python32""
        exec_prefix = ""C:\Python32""
        platbase = ""C:\Python32""
        prefix = ""C:\Python32""
        projectbase = ""C:\Python32""
        py_version = ""3.2""
        py_version_nodot = ""32""
        py_version_short = ""3.2""
        srcdir = ""C:\Python32""
        userbase = ""C:\Documents and Settings\Raymond\Application Data\Python""
",3.2
"configparser

The configparser module was modified to improve usability and
predictability of the default parser and its supported INI syntax.  The old
ConfigParser class was removed in favor of SafeConfigParser
which has in turn been renamed to ConfigParser. Support
for inline comments is now turned off by default and section or option
duplicates are not allowed in a single configuration source.
Config parsers gained a new API based on the mapping protocol:
>>> parser = ConfigParser()
>>> parser.read_string(""""""
[DEFAULT]
location = upper left
visible = yes
editable = no
color = blue

[main]
title = Main Menu
color = green

[options]
title = Options
"""""")
>>> parser['main']['color']
'green'
>>> parser['main']['editable']
'no'
>>> section = parser['options']
>>> section['title']
'Options'
>>> section['title'] = 'Options (editable: %(editable)s)'
>>> section['title']
'Options (editable: no)'


The new API is implemented on top of the classical API, so custom parser
subclasses should be able to use it without modifications.
The INI file structure accepted by config parsers can now be customized. Users
can specify alternative option/value delimiters and comment prefixes, change the
name of the DEFAULT section or switch the interpolation syntax.
There is support for pluggable interpolation including an additional interpolation
handler ExtendedInterpolation:
>>> parser = ConfigParser(interpolation=ExtendedInterpolation())
>>> parser.read_dict({'buildout': {'directory': '/home/ambv/zope9'},
                      'custom': {'prefix': '/usr/local'}})
>>> parser.read_string(""""""
    [buildout]
    parts =
      zope9
      instance
    find-links =
      ${buildout:directory}/downloads/dist

    [zope9]
    recipe = plone.recipe.zope9install
    location = /opt/zope

    [instance]
    recipe = plone.recipe.zope9instance
    zope9-location = ${zope9:location}
    zope-conf = ${custom:prefix}/etc/zope.conf
    """""")
>>> parser['buildout']['find-links']
'\n/home/ambv/zope9/downloads/dist'
>>> parser['instance']['zope-conf']
'/usr/local/etc/zope.conf'
>>> instance = parser['instance']
>>> instance['zope-conf']
'/usr/local/etc/zope.conf'
>>> instance['zope9-location']
'/opt/zope'


A number of smaller features were also introduced, like support for specifying
encoding in read operations, specifying fallback values for get-functions, or
reading directly from dictionaries and strings.
(All changes contributed by Łukasz Langa.)",new-improved-and-deprecated-modules - configparser,">>> parser = ConfigParser()
>>> parser.read_string(""""""
[DEFAULT]
location = upper left
visible = yes
editable = no
color = blue

[main]
title = Main Menu
color = green

[options]
title = Options
"""""")
>>> parser['main']['color']
'green'
>>> parser['main']['editable']
'no'
>>> section = parser['options']
>>> section['title']
'Options'
>>> section['title'] = 'Options (editable: %(editable)s)'
>>> section['title']
'Options (editable: no)'
",3.2
,new-improved-and-deprecated-modules - configparser,">>> parser = ConfigParser(interpolation=ExtendedInterpolation())
>>> parser.read_dict({'buildout': {'directory': '/home/ambv/zope9'},
                      'custom': {'prefix': '/usr/local'}})
>>> parser.read_string(""""""
    [buildout]
    parts =
      zope9
      instance
    find-links =
      ${buildout:directory}/downloads/dist

    [zope9]
    recipe = plone.recipe.zope9install
    location = /opt/zope

    [instance]
    recipe = plone.recipe.zope9instance
    zope9-location = ${zope9:location}
    zope-conf = ${custom:prefix}/etc/zope.conf
    """""")
>>> parser['buildout']['find-links']
'\n/home/ambv/zope9/downloads/dist'
>>> parser['instance']['zope-conf']
'/usr/local/etc/zope.conf'
>>> instance = parser['instance']
>>> instance['zope-conf']
'/usr/local/etc/zope.conf'
>>> instance['zope9-location']
'/opt/zope'
",3.2
"urllib.parse

A number of usability improvements were made for the urllib.parse module.
The urlparse() function now supports IPv6 addresses as described in RFC 2732:
>>> import urllib.parse
>>> urllib.parse.urlparse('http://[dead:beef:cafe:5417:affe:8FA3:deaf:feed]/foo/')
ParseResult(scheme='http',
            netloc='[dead:beef:cafe:5417:affe:8FA3:deaf:feed]',
            path='/foo/',
            params='',
            query='',
            fragment='')


The urldefrag() function now returns a named tuple:
>>> r = urllib.parse.urldefrag('http://python.org/about/#target')
>>> r
DefragResult(url='http://python.org/about/', fragment='target')
>>> r[0]
'http://python.org/about/'
>>> r.fragment
'target'


And, the urlencode() function is now much more flexible,
accepting either a string or bytes type for the query argument.  If it is a
string, then the safe, encoding, and error parameters are sent to
quote_plus() for encoding:
>>> urllib.parse.urlencode([
         ('type', 'telenovela'),
         ('name', '¿Dónde Está Elisa?')],
         encoding='latin-1')
'type=telenovela&name=%BFD%F3nde+Est%E1+Elisa%3F'


As detailed in Parsing ASCII Encoded Bytes, all the urllib.parse
functions now accept ASCII-encoded byte strings as input, so long as they are
not mixed with regular strings.  If ASCII-encoded byte strings are given as
parameters, the return types will also be an ASCII-encoded byte strings:
>>> urllib.parse.urlparse(b'http://www.python.org:80/about/')
ParseResultBytes(scheme=b'http', netloc=b'www.python.org:80',
                 path=b'/about/', params=b'', query=b'', fragment=b'')


(Work by Nick Coghlan, Dan Mahn, and Senthil Kumaran in issue 2987,
issue 5468, and issue 9873.)",new-improved-and-deprecated-modules - urllib-parse,">>> import urllib.parse
>>> urllib.parse.urlparse('http://[dead:beef:cafe:5417:affe:8FA3:deaf:feed]/foo/')
ParseResult(scheme='http',
            netloc='[dead:beef:cafe:5417:affe:8FA3:deaf:feed]',
            path='/foo/',
            params='',
            query='',
            fragment='')
",3.2
,new-improved-and-deprecated-modules - urllib-parse,">>> r = urllib.parse.urldefrag('http://python.org/about/#target')
>>> r
DefragResult(url='http://python.org/about/', fragment='target')
>>> r[0]
'http://python.org/about/'
>>> r.fragment
'target'
",3.2
,new-improved-and-deprecated-modules - urllib-parse,">>> urllib.parse.urlencode([
         ('type', 'telenovela'),
         ('name', '¿Dónde Está Elisa?')],
         encoding='latin-1')
'type=telenovela&name=%BFD%F3nde+Est%E1+Elisa%3F'
",3.2
,new-improved-and-deprecated-modules - urllib-parse,">>> urllib.parse.urlparse(b'http://www.python.org:80/about/')
ParseResultBytes(scheme=b'http', netloc=b'www.python.org:80',
                 path=b'/about/', params=b'', query=b'', fragment=b'')
",3.2
"Optimizations

A number of small performance enhancements have been added:

Python’s peephole optimizer now recognizes patterns such x in {1, 2, 3} as
being a test for membership in a set of constants.  The optimizer recasts the
set as a frozenset and stores the pre-built constant.
Now that the speed penalty is gone, it is practical to start writing
membership tests using set-notation.  This style is both semantically clear
and operationally fast:
extension = name.rpartition('.')[2]
if extension in {'xml', 'html', 'xhtml', 'css'}:
    handle(name)


(Patch and additional tests contributed by Dave Malcolm; issue 6690).

Serializing and unserializing data using the pickle module is now
several times faster.
(Contributed by Alexandre Vassalotti, Antoine Pitrou
and the Unladen Swallow team in issue 9410 and issue 3873.)

The Timsort algorithm used in
list.sort() and sorted() now runs faster and uses less memory
when called with a key function.  Previously, every element of
a list was wrapped with a temporary object that remembered the key value
associated with each element.  Now, two arrays of keys and values are
sorted in parallel.  This saves the memory consumed by the sort wrappers,
and it saves time lost to delegating comparisons.
(Patch by Daniel Stutzbach in issue 9915.)

JSON decoding performance is improved and memory consumption is reduced
whenever the same string is repeated for multiple keys.  Also, JSON encoding
now uses the C speedups when the sort_keys argument is true.
(Contributed by Antoine Pitrou in issue 7451 and by Raymond Hettinger and
Antoine Pitrou in issue 10314.)

Recursive locks (created with the threading.RLock() API) now benefit
from a C implementation which makes them as fast as regular locks, and between
10x and 15x faster than their previous pure Python implementation.
(Contributed by Antoine Pitrou; issue 3001.)

The fast-search algorithm in stringlib is now used by the split(),
splitlines() and replace() methods on
bytes, bytearray and str objects. Likewise, the
algorithm is also used by rfind(), rindex(), rsplit() and
rpartition().
(Patch by Florent Xicluna in issue 7622 and issue 7462.)

Integer to string conversions now work two “digits” at a time, reducing the
number of division and modulo operations.
(issue 6713 by Gawain Bolton, Mark Dickinson, and Victor Stinner.)


There were several other minor optimizations. Set differencing now runs faster
when one operand is much larger than the other (patch by Andress Bennetts in
issue 8685).  The array.repeat() method has a faster implementation
(issue 1569291 by Alexander Belopolsky). The BaseHTTPRequestHandler
has more efficient buffering (issue 3709 by Andrew Schaaf).  The
operator.attrgetter() function has been sped-up (issue 10160 by
Christos Georgiou).  And ConfigParser loads multi-line arguments a bit
faster (issue 7113 by Łukasz Langa).",optimizations,"extension = name.rpartition('.')[2]
if extension in {'xml', 'html', 'xhtml', 'css'}:
    handle(name)
",3.2
"Porting to Python 3.2

This section lists previously described changes and other bugfixes that may
require changes to your code:

The configparser module has a number of clean-ups.  The major change is
to replace the old ConfigParser class with long-standing preferred
alternative SafeConfigParser.  In addition there are a number of
smaller incompatibilities:

The interpolation syntax is now validated on
get() and
set() operations. In the default
interpolation scheme, only two tokens with percent signs are valid: %(name)s
and %%, the latter being an escaped percent sign.
The set() and
add_section() methods now verify that
values are actual strings.  Formerly, unsupported types could be introduced
unintentionally.
Duplicate sections or options from a single source now raise either
DuplicateSectionError or
DuplicateOptionError.  Formerly, duplicates would
silently overwrite a previous entry.
Inline comments are now disabled by default so now the ; character
can be safely used in values.
Comments now can be indented.  Consequently, for ; or # to appear at
the start of a line in multiline values, it has to be interpolated.  This
keeps comment prefix characters in values from being mistaken as comments.
"""" is now a valid value and is no longer automatically converted to an
empty string. For empty strings, use ""option ="" in a line.


The nntplib module was reworked extensively, meaning that its APIs
are often incompatible with the 3.1 APIs.

bytearray objects can no longer be used as filenames; instead,
they should be converted to bytes.

The array.tostring() and array.fromstring() have been renamed to
array.tobytes() and array.frombytes() for clarity.  The old names
have been deprecated. (See issue 8990.)

PyArg_Parse*() functions:

“t#” format has been removed: use “s#” or “s*” instead
“w” and “w#” formats has been removed: use “w*” instead


The PyCObject type, deprecated in 3.1, has been removed.  To wrap
opaque C pointers in Python objects, the PyCapsule API should be used
instead; the new type has a well-defined interface for passing typing safety
information and a less complicated signature for calling a destructor.

The sys.setfilesystemencoding() function was removed because
it had a flawed design.

The random.seed() function and method now salt string seeds with an
sha512 hash function.  To access the previous version of seed in order to
reproduce Python 3.1 sequences, set the version argument to 1,
random.seed(s, version=1).

The previously deprecated string.maketrans() function has been removed
in favor of the static methods bytes.maketrans() and
bytearray.maketrans().  This change solves the confusion around which
types were supported by the string module.  Now, str,
bytes, and bytearray each have their own maketrans and
translate methods with intermediate translation tables of the appropriate
type.
(Contributed by Georg Brandl; issue 5675.)

The previously deprecated contextlib.nested() function has been removed
in favor of a plain with statement which can accept multiple
context managers.  The latter technique is faster (because it is built-in),
and it does a better job finalizing multiple context managers when one of them
raises an exception:
with open('mylog.txt') as infile, open('a.out', 'w') as outfile:
    for line in infile:
        if '<critical>' in line:
            outfile.write(line)


(Contributed by Georg Brandl and Mattias Brändström;
appspot issue 53094.)

struct.pack() now only allows bytes for the s string pack code.
Formerly, it would accept text arguments and implicitly encode them to bytes
using UTF-8.  This was problematic because it made assumptions about the
correct encoding and because a variable-length encoding can fail when writing
to fixed length segment of a structure.
Code such as struct.pack('<6sHHBBB', 'GIF87a', x, y) should be rewritten
with to use bytes instead of text, struct.pack('<6sHHBBB', b'GIF87a', x, y).
(Discovered by David Beazley and fixed by Victor Stinner; issue 10783.)

The xml.etree.ElementTree class now raises an
xml.etree.ElementTree.ParseError when a parse fails. Previously it
raised a xml.parsers.expat.ExpatError.

The new, longer str() value on floats may break doctests which rely on
the old output format.

In subprocess.Popen, the default value for close_fds is now
True under Unix; under Windows, it is True if the three standard
streams are set to None, False otherwise.  Previously, close_fds
was always False by default, which produced difficult to solve bugs
or race conditions when open file descriptors would leak into the child
process.

Support for legacy HTTP 0.9 has been removed from urllib.request
and http.client.  Such support is still present on the server side
(in http.server).
(Contributed by Antoine Pitrou, issue 10711.)

SSL sockets in timeout mode now raise socket.timeout when a timeout
occurs, rather than a generic SSLError.
(Contributed by Antoine Pitrou, issue 10272.)

The misleading functions PyEval_AcquireLock() and
PyEval_ReleaseLock() have been officially deprecated.  The
thread-state aware APIs (such as PyEval_SaveThread()
and PyEval_RestoreThread()) should be used instead.

Due to security risks, asyncore.handle_accept() has been deprecated, and
a new function, asyncore.handle_accepted(), was added to replace it.
(Contributed by Giampaolo Rodola in issue 6706.)

Due to the new GIL implementation, PyEval_InitThreads()
cannot be called before Py_Initialize() anymore.",porting-to-python-3-2,"with open('mylog.txt') as infile, open('a.out', 'w') as outfile:
    for line in infile:
        if '<critical>' in line:
            outfile.write(line)
",3.2
"PEP 3151: Reworking the OS and IO exception hierarchy

The hierarchy of exceptions raised by operating system errors is now both
simplified and finer-grained.
You don’t have to worry anymore about choosing the appropriate exception
type between OSError, IOError, EnvironmentError,
WindowsError, mmap.error, socket.error or
select.error.  All these exception types are now only one:
OSError.  The other names are kept as aliases for compatibility
reasons.
Also, it is now easier to catch a specific error condition.  Instead of
inspecting the errno attribute (or args[0]) for a particular
constant from the errno module, you can catch the adequate
OSError subclass.  The available subclasses are the following:

BlockingIOError
ChildProcessError
ConnectionError
FileExistsError
FileNotFoundError
InterruptedError
IsADirectoryError
NotADirectoryError
PermissionError
ProcessLookupError
TimeoutError

And the ConnectionError itself has finer-grained subclasses:

BrokenPipeError
ConnectionAbortedError
ConnectionRefusedError
ConnectionResetError

Thanks to the new exceptions, common usages of the errno can now be
avoided.  For example, the following code written for Python 3.2:
from errno import ENOENT, EACCES, EPERM

try:
    with open(""document.txt"") as f:
        content = f.read()
except IOError as err:
    if err.errno == ENOENT:
        print(""document.txt file is missing"")
    elif err.errno in (EACCES, EPERM):
        print(""You are not allowed to read document.txt"")
    else:
        raise


can now be written without the errno import and without manual
inspection of exception attributes:
try:
    with open(""document.txt"") as f:
        content = f.read()
except FileNotFoundError:
    print(""document.txt file is missing"")
except PermissionError:
    print(""You are not allowed to read document.txt"")



See also

PEP 3151 - Reworking the OS and IO Exception Hierarchy
PEP written and implemented by Antoine Pitrou",pep-3151-reworking-the-os-and-io-exception-hierarchy,"from errno import ENOENT, EACCES, EPERM

try:
    with open(""document.txt"") as f:
        content = f.read()
except IOError as err:
    if err.errno == ENOENT:
        print(""document.txt file is missing"")
    elif err.errno in (EACCES, EPERM):
        print(""You are not allowed to read document.txt"")
    else:
        raise
",3.3
,pep-3151-reworking-the-os-and-io-exception-hierarchy,"try:
    with open(""document.txt"") as f:
        content = f.read()
except FileNotFoundError:
    print(""document.txt file is missing"")
except PermissionError:
    print(""You are not allowed to read document.txt"")
",3.3
"PEP 380: Syntax for Delegating to a Subgenerator

PEP 380 adds the yield from expression, allowing a generator to
delegate
part of its operations to another generator. This allows a section of code
containing yield to be factored out and placed in another generator.
Additionally, the subgenerator is allowed to return with a value, and the
value is made available to the delegating generator.
While designed primarily for use in delegating to a subgenerator, the yield
from expression actually allows delegation to arbitrary subiterators.
For simple iterators, yield from iterable is essentially just a shortened
form of for item in iterable: yield item:
>>> def g(x):
...     yield from range(x, 0, -1)
...     yield from range(x)
...
>>> list(g(5))
[5, 4, 3, 2, 1, 0, 1, 2, 3, 4]


However, unlike an ordinary loop, yield from allows subgenerators to
receive sent and thrown values directly from the calling scope, and
return a final value to the outer generator:
>>> def accumulate():
...     tally = 0
...     while 1:
...         next = yield
...         if next is None:
...             return tally
...         tally += next
...
>>> def gather_tallies(tallies):
...     while 1:
...         tally = yield from accumulate()
...         tallies.append(tally)
...
>>> tallies = []
>>> acc = gather_tallies(tallies)
>>> next(acc) # Ensure the accumulator is ready to accept values
>>> for i in range(4):
...     acc.send(i)
...
>>> acc.send(None) # Finish the first tally
>>> for i in range(5):
...     acc.send(i)
...
>>> acc.send(None) # Finish the second tally
>>> tallies
[6, 10]


The main principle driving this change is to allow even generators that are
designed to be used with the send and throw methods to be split into
multiple subgenerators as easily as a single large function can be split into
multiple subfunctions.

See also

PEP 380 - Syntax for Delegating to a Subgenerator
PEP written by Greg Ewing; implementation by Greg Ewing, integrated into
3.3 by Renaud Blanch, Ryan Kelly and Nick Coghlan; documentation by
Zbigniew Jędrzejewski-Szmek and Nick Coghlan",pep-380-syntax-for-delegating-to-a-subgenerator,">>> def g(x):
...     yield from range(x, 0, -1)
...     yield from range(x)
...
>>> list(g(5))
[5, 4, 3, 2, 1, 0, 1, 2, 3, 4]
",3.3
,pep-380-syntax-for-delegating-to-a-subgenerator,">>> def accumulate():
...     tally = 0
...     while 1:
...         next = yield
...         if next is None:
...             return tally
...         tally += next
...
>>> def gather_tallies(tallies):
...     while 1:
...         tally = yield from accumulate()
...         tallies.append(tally)
...
>>> tallies = []
>>> acc = gather_tallies(tallies)
>>> next(acc) # Ensure the accumulator is ready to accept values
>>> for i in range(4):
...     acc.send(i)
...
>>> acc.send(None) # Finish the first tally
>>> for i in range(5):
...     acc.send(i)
...
>>> acc.send(None) # Finish the second tally
>>> tallies
[6, 10]
",3.3
"PEP 409: Suppressing exception context

PEP 409 introduces new syntax that allows the display of the chained
exception context to be disabled. This allows cleaner error messages in
applications that convert between exception types:
>>> class D:
...     def __init__(self, extra):
...         self._extra_attributes = extra
...     def __getattr__(self, attr):
...         try:
...             return self._extra_attributes[attr]
...         except KeyError:
...             raise AttributeError(attr) from None
...
>>> D({}).x
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<stdin>"", line 8, in __getattr__
AttributeError: x


Without the from None suffix to suppress the cause, the original
exception would be displayed by default:
>>> class C:
...     def __init__(self, extra):
...         self._extra_attributes = extra
...     def __getattr__(self, attr):
...         try:
...             return self._extra_attributes[attr]
...         except KeyError:
...             raise AttributeError(attr)
...
>>> C({}).x
Traceback (most recent call last):
  File ""<stdin>"", line 6, in __getattr__
KeyError: 'x'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<stdin>"", line 8, in __getattr__
AttributeError: x


No debugging capability is lost, as the original exception context remains
available if needed (for example, if an intervening library has incorrectly
suppressed valuable underlying details):
>>> try:
...     D({}).x
... except AttributeError as exc:
...     print(repr(exc.__context__))
...
KeyError('x',)



See also

PEP 409 - Suppressing exception context
PEP written by Ethan Furman; implemented by Ethan Furman and Nick
Coghlan.",pep-409-suppressing-exception-context,">>> class D:
...     def __init__(self, extra):
...         self._extra_attributes = extra
...     def __getattr__(self, attr):
...         try:
...             return self._extra_attributes[attr]
...         except KeyError:
...             raise AttributeError(attr) from None
...
>>> D({}).x
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<stdin>"", line 8, in __getattr__
AttributeError: x
",3.3
,pep-409-suppressing-exception-context,">>> class C:
...     def __init__(self, extra):
...         self._extra_attributes = extra
...     def __getattr__(self, attr):
...         try:
...             return self._extra_attributes[attr]
...         except KeyError:
...             raise AttributeError(attr)
...
>>> C({}).x
Traceback (most recent call last):
  File ""<stdin>"", line 6, in __getattr__
KeyError: 'x'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<stdin>"", line 8, in __getattr__
AttributeError: x
",3.3
,pep-409-suppressing-exception-context,">>> try:
...     D({}).x
... except AttributeError as exc:
...     print(repr(exc.__context__))
...
KeyError('x',)
",3.3
"PEP 3155: Qualified name for classes and functions

Functions and class objects have a new __qualname__ attribute representing
the “path” from the module top-level to their definition.  For global functions
and classes, this is the same as __name__.  For other functions and classes,
it provides better information about where they were actually defined, and
how they might be accessible from the global scope.
Example with (non-bound) methods:
>>> class C:
...     def meth(self):
...         pass
>>> C.meth.__name__
'meth'
>>> C.meth.__qualname__
'C.meth'


Example with nested classes:
>>> class C:
...     class D:
...         def meth(self):
...             pass
...
>>> C.D.__name__
'D'
>>> C.D.__qualname__
'C.D'
>>> C.D.meth.__name__
'meth'
>>> C.D.meth.__qualname__
'C.D.meth'


Example with nested functions:
>>> def outer():
...     def inner():
...         pass
...     return inner
...
>>> outer().__name__
'inner'
>>> outer().__qualname__
'outer.<locals>.inner'


The string representation of those objects is also changed to include the
new, more precise information:
>>> str(C.D)
""<class '__main__.C.D'>""
>>> str(C.D.meth)
'<function C.D.meth at 0x7f46b9fe31e0>'



See also

PEP 3155 - Qualified name for classes and functions
PEP written and implemented by Antoine Pitrou.",pep-3155-qualified-name-for-classes-and-functions,">>> class C:
...     def meth(self):
...         pass
>>> C.meth.__name__
'meth'
>>> C.meth.__qualname__
'C.meth'
",3.3
,pep-3155-qualified-name-for-classes-and-functions,">>> class C:
...     class D:
...         def meth(self):
...             pass
...
>>> C.D.__name__
'D'
>>> C.D.__qualname__
'C.D'
>>> C.D.meth.__name__
'meth'
>>> C.D.meth.__qualname__
'C.D.meth'
",3.3
,pep-3155-qualified-name-for-classes-and-functions,">>> def outer():
...     def inner():
...         pass
...     return inner
...
>>> outer().__name__
'inner'
>>> outer().__qualname__
'outer.<locals>.inner'
",3.3
,pep-3155-qualified-name-for-classes-and-functions,">>> str(C.D)
""<class '__main__.C.D'>""
>>> str(C.D.meth)
'<function C.D.meth at 0x7f46b9fe31e0>'
",3.3
"email


Policy Framework

The email package now has a policy framework.  A
Policy is an object with several methods and properties
that control how the email package behaves.  The primary policy for Python 3.3
is the Compat32 policy, which provides backward
compatibility with the email package in Python 3.2.  A policy can be
specified when an email message is parsed by a parser, or when a
Message object is created, or when an email is
serialized using a generator.  Unless overridden, a policy passed
to a parser is inherited by all the Message object and sub-objects
created by the parser.  By default a generator will use the policy of
the Message object it is serializing.  The default policy is
compat32.
The minimum set of controls implemented by all policy objects are:







max_line_length
The maximum length, excluding the linesep character(s),
individual lines may have when a Message is
serialized.  Defaults to 78.

linesep
The character used to separate individual lines when a
Message is serialized.  Defaults to \n.

cte_type
7bit or 8bit.  8bit applies only to a
Bytes generator, and means that non-ASCII may
be used where allowed by the protocol (or where it
exists in the original input).

raise_on_defect
Causes a parser to raise error when defects are
encountered instead of adding them to the Message
object’s defects list.




A new policy instance, with new settings, is created using the
clone() method of policy objects.  clone takes
any of the above controls as keyword arguments.  Any control not specified in
the call retains its default value.  Thus you can create a policy that uses
\r\n linesep characters like this:
mypolicy = compat32.clone(linesep='\r\n')


Policies can be used to make the generation of messages in the format needed by
your application simpler.  Instead of having to remember to specify
linesep='\r\n' in all the places you call a generator, you can specify
it once, when you set the policy used by the parser or the Message,
whichever your program uses to create Message objects.  On the other hand,
if you need to generate messages in multiple forms, you can still specify the
parameters in the appropriate generator call.  Or you can have custom
policy instances for your different cases, and pass those in when you create
the generator.


Provisional Policy with New Header API

While the policy framework is worthwhile all by itself, the main motivation for
introducing it is to allow the creation of new policies that implement new
features for the email package in a way that maintains backward compatibility
for those who do not use the new policies.  Because the new policies introduce a
new API, we are releasing them in Python 3.3 as a provisional policy.  Backwards incompatible changes (up to and including
removal of the code) may occur if deemed necessary by the core developers.
The new policies are instances of EmailPolicy,
and add the following additional controls:







refold_source
Controls whether or not headers parsed by a
parser are refolded by the
generator.  It can be none, long,
or all.  The default is long, which means that
source headers with a line longer than
max_line_length get refolded.  none means no
line get refolded, and all means that all lines
get refolded.

header_factory
A callable that take a name and value and
produces a custom header object.




The header_factory is the key to the new features provided by the new
policies.  When one of the new policies is used, any header retrieved from
a Message object is an object produced by the header_factory, and any
time you set a header on a Message it becomes an object produced by
header_factory.  All such header objects have a name attribute equal
to the header name.  Address and Date headers have additional attributes
that give you access to the parsed data of the header.  This means you can now
do things like this:
>>> m = Message(policy=SMTP)
>>> m['To'] = 'Éric <foo@example.com>'
>>> m['to']
'Éric <foo@example.com>'
>>> m['to'].addresses
(Address(display_name='Éric', username='foo', domain='example.com'),)
>>> m['to'].addresses[0].username
'foo'
>>> m['to'].addresses[0].display_name
'Éric'
>>> m['Date'] = email.utils.localtime()
>>> m['Date'].datetime
datetime.datetime(2012, 5, 25, 21, 39, 24, 465484, tzinfo=datetime.timezone(datetime.timedelta(-1, 72000), 'EDT'))
>>> m['Date']
'Fri, 25 May 2012 21:44:27 -0400'
>>> print(m)
To: =?utf-8?q?=C3=89ric?= <foo@example.com>
Date: Fri, 25 May 2012 21:44:27 -0400


You will note that the unicode display name is automatically encoded as
utf-8 when the message is serialized, but that when the header is accessed
directly, you get the unicode version.  This eliminates any need to deal with
the email.header decode_header() or
make_header() functions.
You can also create addresses from parts:
>>> m['cc'] = [Group('pals', [Address('Bob', 'bob', 'example.com'),
...                           Address('Sally', 'sally', 'example.com')]),
...            Address('Bonzo', addr_spec='bonz@laugh.com')]
>>> print(m)
To: =?utf-8?q?=C3=89ric?= <foo@example.com>
Date: Fri, 25 May 2012 21:44:27 -0400
cc: pals: Bob <bob@example.com>, Sally <sally@example.com>;, Bonzo <bonz@laugh.com>


Decoding to unicode is done automatically:
>>> m2 = message_from_string(str(m))
>>> m2['to']
'Éric <foo@example.com>'


When you parse a message, you can use the addresses and groups
attributes of the header objects to access the groups and individual
addresses:
>>> m2['cc'].addresses
(Address(display_name='Bob', username='bob', domain='example.com'), Address(display_name='Sally', username='sally', domain='example.com'), Address(display_name='Bonzo', username='bonz', domain='laugh.com'))
>>> m2['cc'].groups
(Group(display_name='pals', addresses=(Address(display_name='Bob', username='bob', domain='example.com'), Address(display_name='Sally', username='sally', domain='example.com')), Group(display_name=None, addresses=(Address(display_name='Bonzo', username='bonz', domain='laugh.com'),))


In summary, if you use one of the new policies, header manipulation works the
way it ought to:  your application works with unicode strings, and the email
package transparently encodes and decodes the unicode to and from the RFC
standard Content Transfer Encodings.


Other API Changes

New BytesHeaderParser, added to the parser
module to complement HeaderParser and complete the Bytes
API.
New utility functions:


format_datetime(): given a datetime,
produce a string formatted for use in an email header.
parsedate_to_datetime(): given a date string from
an email header, convert it into an aware datetime,
or a naive datetime if the offset is -0000.
localtime(): With no argument, returns the
current local time as an aware datetime using the local
timezone.  Given an aware datetime,
converts it into an aware datetime using the
local timezone.",improved-modules - email,"mypolicy = compat32.clone(linesep='\r\n')
",3.3
,improved-modules - email,">>> m = Message(policy=SMTP)
>>> m['To'] = 'Éric <foo@example.com>'
>>> m['to']
'Éric <foo@example.com>'
>>> m['to'].addresses
(Address(display_name='Éric', username='foo', domain='example.com'),)
>>> m['to'].addresses[0].username
'foo'
>>> m['to'].addresses[0].display_name
'Éric'
>>> m['Date'] = email.utils.localtime()
>>> m['Date'].datetime
datetime.datetime(2012, 5, 25, 21, 39, 24, 465484, tzinfo=datetime.timezone(datetime.timedelta(-1, 72000), 'EDT'))
>>> m['Date']
'Fri, 25 May 2012 21:44:27 -0400'
>>> print(m)
To: =?utf-8?q?=C3=89ric?= <foo@example.com>
Date: Fri, 25 May 2012 21:44:27 -0400
",3.3
,improved-modules - email,">>> m['cc'] = [Group('pals', [Address('Bob', 'bob', 'example.com'),
...                           Address('Sally', 'sally', 'example.com')]),
...            Address('Bonzo', addr_spec='bonz@laugh.com')]
>>> print(m)
To: =?utf-8?q?=C3=89ric?= <foo@example.com>
Date: Fri, 25 May 2012 21:44:27 -0400
cc: pals: Bob <bob@example.com>, Sally <sally@example.com>;, Bonzo <bonz@laugh.com>
",3.3
,improved-modules - email,">>> m2 = message_from_string(str(m))
>>> m2['to']
'Éric <foo@example.com>'
",3.3
,improved-modules - email,">>> m2['cc'].addresses
(Address(display_name='Bob', username='bob', domain='example.com'), Address(display_name='Sally', username='sally', domain='example.com'), Address(display_name='Bonzo', username='bonz', domain='laugh.com'))
>>> m2['cc'].groups
(Group(display_name='pals', addresses=(Address(display_name='Bob', username='bob', domain='example.com'), Address(display_name='Sally', username='sally', domain='example.com')), Group(display_name=None, addresses=(Address(display_name='Bonzo', username='bonz', domain='laugh.com'),))
",3.3
"nntplib

The nntplib.NNTP class now supports the context manager protocol to
unconditionally consume socket.error exceptions and to close the NNTP
connection when done:
>>> from nntplib import NNTP
>>> with NNTP('news.gmane.org') as n:
...     n.group('gmane.comp.python.committers')
...
('211 1755 1 1755 gmane.comp.python.committers', 1755, 1, 1755, 'gmane.comp.python.committers')
>>>


(Contributed by Giampaolo Rodolà in issue 9795)",improved-modules - nntplib,">>> from nntplib import NNTP
>>> with NNTP('news.gmane.org') as n:
...     n.group('gmane.comp.python.committers')
...
('211 1755 1 1755 gmane.comp.python.committers', 1755, 1, 1755, 'gmane.comp.python.committers')
>>>
",3.3
"urllib

The Request class, now accepts a method argument
used by get_method() to determine what HTTP method
should be used.  For example, this will send a 'HEAD' request:
>>> urlopen(Request('http://www.python.org', method='HEAD'))


(issue 1673007)",improved-modules - urllib,">>> urlopen(Request('http://www.python.org', method='HEAD'))
",3.3
"Improvements to Codec Handling

Since it was first introduced, the codecs module has always been
intended to operate as a type-neutral dynamic encoding and decoding
system. However, its close coupling with the Python text model, especially
the type restricted convenience methods on the builtin str,
bytes and bytearray types, has historically obscured that
fact.
As a key step in clarifying the situation, the codecs.encode() and
codecs.decode() convenience functions are now properly documented in
Python 2.7, 3.3 and 3.4. These functions have existed in the codecs
module (and have been covered by the regression test suite) since Python 2.4,
but were previously only discoverable through runtime introspection.
Unlike the convenience methods on str, bytes and
bytearray, the codecs convenience functions support arbitrary
codecs in both Python 2 and Python 3, rather than being limited to Unicode text
encodings (in Python 3) or basestring <-> basestring conversions (in
Python 2).
In Python 3.4, the interpreter is able to identify the known non-text
encodings provided in the standard library and direct users towards these
general purpose convenience functions when appropriate:
>>> b""abcdef"".decode(""hex"")
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
LookupError: 'hex' is not a text encoding; use codecs.decode() to handle arbitrary codecs

>>> ""hello"".encode(""rot13"")
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
LookupError: 'rot13' is not a text encoding; use codecs.encode() to handle arbitrary codecs

>>> open(""foo.txt"", encoding=""hex"")
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
LookupError: 'hex' is not a text encoding; use codecs.open() to handle arbitrary codecs


In a related change, whenever it is feasible without breaking backwards
compatibility, exceptions raised during encoding and decoding operations
are wrapped in a chained exception of the same type that mentions the
name of the codec responsible for producing the error:
>>> import codecs

>>> codecs.decode(b""abcdefgh"", ""hex"")
Traceback (most recent call last):
  File ""/usr/lib/python3.4/encodings/hex_codec.py"", line 20, in hex_decode
    return (binascii.a2b_hex(input), len(input))
binascii.Error: Non-hexadecimal digit found

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
binascii.Error: decoding with 'hex' codec failed (Error: Non-hexadecimal digit found)

>>> codecs.encode(""hello"", ""bz2"")
Traceback (most recent call last):
  File ""/usr/lib/python3.4/encodings/bz2_codec.py"", line 17, in bz2_encode
    return (bz2.compress(input), len(input))
  File ""/usr/lib/python3.4/bz2.py"", line 498, in compress
    return comp.compress(data) + comp.flush()
TypeError: 'str' does not support the buffer interface

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: encoding with 'bz2' codec failed (TypeError: 'str' does not support the buffer interface)


Finally, as the examples above show, these improvements have permitted
the restoration of the convenience aliases for the non-Unicode codecs that
were themselves restored in Python 3.2. This means that encoding binary data
to and from its hexadecimal representation (for example) can now be written
as:
>>> from codecs import encode, decode
>>> encode(b""hello"", ""hex"")
b'68656c6c6f'
>>> decode(b""68656c6c6f"", ""hex"")
b'hello'


The binary and text transforms provided in the standard library are detailed
in Binary Transforms and Text Transforms.
(Contributed by Nick Coghlan in issue 7475, issue 17827,
issue 17828 and issue 19619.)",new-features - improvements-to-codec-handling,">>> b""abcdef"".decode(""hex"")
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
LookupError: 'hex' is not a text encoding; use codecs.decode() to handle arbitrary codecs

>>> ""hello"".encode(""rot13"")
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
LookupError: 'rot13' is not a text encoding; use codecs.encode() to handle arbitrary codecs

>>> open(""foo.txt"", encoding=""hex"")
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
LookupError: 'hex' is not a text encoding; use codecs.open() to handle arbitrary codecs
",3.4
,new-features - improvements-to-codec-handling,">>> import codecs

>>> codecs.decode(b""abcdefgh"", ""hex"")
Traceback (most recent call last):
  File ""/usr/lib/python3.4/encodings/hex_codec.py"", line 20, in hex_decode
    return (binascii.a2b_hex(input), len(input))
binascii.Error: Non-hexadecimal digit found

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
binascii.Error: decoding with 'hex' codec failed (Error: Non-hexadecimal digit found)

>>> codecs.encode(""hello"", ""bz2"")
Traceback (most recent call last):
  File ""/usr/lib/python3.4/encodings/bz2_codec.py"", line 17, in bz2_encode
    return (bz2.compress(input), len(input))
  File ""/usr/lib/python3.4/bz2.py"", line 498, in compress
    return comp.compress(data) + comp.flush()
TypeError: 'str' does not support the buffer interface

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: encoding with 'bz2' codec failed (TypeError: 'str' does not support the buffer interface)
",3.4
,new-features - improvements-to-codec-handling,">>> from codecs import encode, decode
>>> encode(b""hello"", ""hex"")
b'68656c6c6f'
>>> decode(b""68656c6c6f"", ""hex"")
b'hello'
",3.4
"dis

Functions show_code(), dis(), distb(), and
disassemble() now accept a keyword-only file argument that
controls where they write their output.
The dis module is now built around an Instruction class
that provides object oriented access to the details of each individual bytecode
operation.
A new method, get_instructions(), provides an iterator that emits
the Instruction stream for a given piece of Python code.  Thus it is now
possible to write a program that inspects and manipulates a bytecode
object in ways different from those provided by the dis module
itself.  For example:
>>> import dis
>>> for instr in dis.get_instructions(lambda x: x + 1):
...     print(instr.opname)
LOAD_FAST
LOAD_CONST
BINARY_ADD
RETURN_VALUE


The various display tools in the dis module have been rewritten to use
these new components.
In addition, a new application-friendly class Bytecode provides
an object-oriented API for inspecting bytecode in both in human-readable form
and for iterating over instructions.  The Bytecode constructor
takes the same arguments that get_instruction() does (plus an
optional current_offset), and the resulting object can be iterated to produce
Instruction objects.  But it also has a dis
method, equivalent to calling dis on the constructor argument, but
returned as a multi-line string:
>>> bytecode = dis.Bytecode(lambda x: x +1, current_offset=3)
>>> for instr in bytecode:
...     print('{} ({})'.format(instr.opname, instr.opcode))
LOAD_FAST (124)
LOAD_CONST (100)
BINARY_ADD (23)
RETURN_VALUE (83)
>>> bytecode.dis().splitlines()       
['  1           0 LOAD_FAST                0 (x)',
 '      -->     3 LOAD_CONST               1 (1)',
 '              6 BINARY_ADD',
 '              7 RETURN_VALUE']


Bytecode also has a class method,
from_traceback(), that provides the ability to manipulate a
traceback (that is, print(Bytecode.from_traceback(tb).dis()) is equivalent
to distb(tb)).
(Contributed by Nick Coghlan, Ryan Kelly and Thomas Kluyver in issue 11816
and Claudiu Popa in issue 17916.)
New function stack_effect() computes the effect on the Python stack
of a given opcode and argument, information that is not otherwise available.
(Contributed by Larry Hastings in issue 19722.)",improved-modules - dis,">>> import dis
>>> for instr in dis.get_instructions(lambda x: x + 1):
...     print(instr.opname)
LOAD_FAST
LOAD_CONST
BINARY_ADD
RETURN_VALUE
",3.4
,improved-modules - dis,">>> bytecode = dis.Bytecode(lambda x: x +1, current_offset=3)
>>> for instr in bytecode:
...     print('{} ({})'.format(instr.opname, instr.opcode))
LOAD_FAST (124)
LOAD_CONST (100)
BINARY_ADD (23)
RETURN_VALUE (83)
>>> bytecode.dis().splitlines()       
['  1           0 LOAD_FAST                0 (x)',
 '      -->     3 LOAD_CONST               1 (1)',
 '              6 BINARY_ADD',
 '              7 RETURN_VALUE']
",3.4
"unittest

The TestCase class has a new method,
subTest(), that produces a context manager whose
with block becomes a “sub-test”.  This context manager allows a test
method to dynamically generate subtests  by, say, calling the subTest
context manager inside a loop.  A single test method can thereby produce an
indefinite number of separately-identified and separately-counted tests, all of
which will run even if one or more of them fail.  For example:
class NumbersTest(unittest.TestCase):
    def test_even(self):
        for i in range(6):
            with self.subTest(i=i):
                self.assertEqual(i % 2, 0)


will result in six subtests, each identified in the unittest verbose output
with a label consisting of the variable name i and a particular value for
that variable (i=0, i=1, etc).  See Distinguishing test iterations using subtests for the full
version of this example.  (Contributed by Antoine Pitrou in issue 16997.)
unittest.main() now accepts an iterable of test names for
defaultTest, where previously it only accepted a single test name as a
string.  (Contributed by Jyrki Pulliainen in issue 15132.)
If SkipTest is raised during test discovery (that is, at the
module level in the test file), it is now reported as a skip instead of an
error.  (Contributed by Zach Ware in issue 16935.)
discover() now sorts the discovered files to provide
consistent test ordering.  (Contributed by Martin Melin and Jeff Ramnani in
issue 16709.)
TestSuite now drops references to tests as soon as the test
has been run, if the test is successful.  On Python interpreters that do
garbage collection, this allows the tests to be garbage collected if nothing
else is holding a reference to the test.  It is possible to override this
behavior by creating a TestSuite subclass that defines a
custom _removeTestAtIndex method.  (Contributed by Tom Wardill, Matt
McClure, and Andrew Svetlov in issue 11798.)
A new test assertion context-manager, assertLogs(),
will ensure that a given block of code emits a log message using the
logging module.  By default the message can come from any logger and
have a priority of INFO or higher, but both the logger name and an
alternative minimum logging level may be specified.  The object returned by the
context manager can be queried for the LogRecords and/or
formatted messages that were logged.  (Contributed by Antoine Pitrou in
issue 18937.)
Test discovery now works with namespace packages (Contributed by Claudiu Popa
in issue 17457.)
unittest.mock objects now inspect their specification signatures when
matching calls, which means an argument can now be matched by either position
or name, instead of only by position.  (Contributed by Antoine Pitrou in
issue 17015.)
mock_open() objects now have readline and readlines
methods.  (Contributed by Toshio Kuratomi in issue 17467.)",improved-modules - unittest,"class NumbersTest(unittest.TestCase):
    def test_even(self):
        for i in range(6):
            with self.subTest(i=i):
                self.assertEqual(i % 2, 0)
",3.4
"PEP 476: Enabling certificate verification by default for stdlib http clients

http.client and modules which use it, such as urllib.request and
xmlrpc.client, will now verify that the server presents a certificate
which is signed by a CA in the platform trust store and whose hostname matches
the hostname being requested by default, significantly improving security for
many applications.
For applications which require the old previous behavior, they can pass an
alternate context:
import urllib.request
import ssl

# This disables all verification
context = ssl._create_unverified_context()

# This allows using a specific certificate for the host, which doesn't need
# to be in the trust store
context = ssl.create_default_context(cafile=""/path/to/file.crt"")

urllib.request.urlopen(""https://invalid-cert"", context=context)",changed-in-3-4-3 - pep-476-enabling-certificate-verification-by-default-for-stdlib-http-clients,"import urllib.request
import ssl

# This disables all verification
context = ssl._create_unverified_context()

# This allows using a specific certificate for the host, which doesn't need
# to be in the trust store
context = ssl.create_default_context(cafile=""/path/to/file.crt"")

urllib.request.urlopen(""https://invalid-cert"", context=context)
",3.4
"PEP 492 - Coroutines with async and await syntax

PEP 492 greatly improves support for asynchronous programming in Python
by adding awaitable objects,
coroutine functions,
asynchronous iteration,
and asynchronous context managers.
Coroutine functions are declared using the new async def syntax:
>>> async def coro():
...     return 'spam'


Inside a coroutine function, the new await expression can be used
to suspend coroutine execution until the result is available.  Any object
can be awaited, as long as it implements the awaitable protocol by
defining the __await__() method.
PEP 492 also adds async for statement for convenient iteration
over asynchronous iterables.
An example of a rudimentary HTTP client written using the new syntax:
import asyncio

async def http_get(domain):
    reader, writer = await asyncio.open_connection(domain, 80)

    writer.write(b'\r\n'.join([
        b'GET / HTTP/1.1',
        b'Host: %b' % domain.encode('latin-1'),
        b'Connection: close',
        b'', b''
    ]))

    async for line in reader:
        print('>>>', line)

    writer.close()

loop = asyncio.get_event_loop()
try:
    loop.run_until_complete(http_get('example.com'))
finally:
    loop.close()


Similarly to asynchronous iteration, there is a new syntax for asynchronous
context managers.  The following script:
import asyncio

async def coro(name, lock):
    print('coro {}: waiting for lock'.format(name))
    async with lock:
        print('coro {}: holding the lock'.format(name))
        await asyncio.sleep(1)
        print('coro {}: releasing the lock'.format(name))

loop = asyncio.get_event_loop()
lock = asyncio.Lock()
coros = asyncio.gather(coro(1, lock), coro(2, lock))
try:
    loop.run_until_complete(coros)
finally:
    loop.close()


will output:
coro 2: waiting for lock
coro 2: holding the lock
coro 1: waiting for lock
coro 2: releasing the lock
coro 1: holding the lock
coro 1: releasing the lock


Note that both async for and async with can only
be used inside a coroutine function declared with async def.
Coroutine functions are intended to be run inside a compatible event loop,
such as the asyncio loop.

Note

Changed in version 3.5.2: Starting with CPython 3.5.2, __aiter__ can directly return
asynchronous iterators.  Returning
an awaitable object will result in a
PendingDeprecationWarning.
See more details in the Asynchronous Iterators documentation
section.



See also

PEP 492 – Coroutines with async and await syntax
PEP written and implemented by Yury Selivanov.",new-features - pep-492-coroutines-with-async-and-await-syntax,">>> async def coro():
...     return 'spam'
",3.5
,new-features - pep-492-coroutines-with-async-and-await-syntax,"import asyncio

async def http_get(domain):
    reader, writer = await asyncio.open_connection(domain, 80)

    writer.write(b'\r\n'.join([
        b'GET / HTTP/1.1',
        b'Host: %b' % domain.encode('latin-1'),
        b'Connection: close',
        b'', b''
    ]))

    async for line in reader:
        print('>>>', line)

    writer.close()

loop = asyncio.get_event_loop()
try:
    loop.run_until_complete(http_get('example.com'))
finally:
    loop.close()
",3.5
,new-features - pep-492-coroutines-with-async-and-await-syntax,"import asyncio

async def coro(name, lock):
    print('coro {}: waiting for lock'.format(name))
    async with lock:
        print('coro {}: holding the lock'.format(name))
        await asyncio.sleep(1)
        print('coro {}: releasing the lock'.format(name))

loop = asyncio.get_event_loop()
lock = asyncio.Lock()
coros = asyncio.gather(coro(1, lock), coro(2, lock))
try:
    loop.run_until_complete(coros)
finally:
    loop.close()
",3.5
,new-features - pep-492-coroutines-with-async-and-await-syntax,"coro 2: waiting for lock
coro 2: holding the lock
coro 1: waiting for lock
coro 2: releasing the lock
coro 1: holding the lock
coro 1: releasing the lock
",3.5
"PEP 465 - A dedicated infix operator for matrix multiplication

PEP 465 adds the @ infix operator for matrix multiplication.
Currently, no builtin Python types implement the new operator, however, it
can be implemented by defining __matmul__(), __rmatmul__(),
and __imatmul__() for regular, reflected, and in-place matrix
multiplication.  The semantics of these methods is similar to that of
methods defining other infix arithmetic operators.
Matrix multiplication is a notably common operation in many fields of
mathematics, science, engineering, and the addition of @ allows writing
cleaner code:
S = (H @ beta - r).T @ inv(H @ V @ H.T) @ (H @ beta - r)


instead of:
S = dot((dot(H, beta) - r).T,
        dot(inv(dot(dot(H, V), H.T)), dot(H, beta) - r))


NumPy 1.10 has support for the new operator:
>>> import numpy

>>> x = numpy.ones(3)
>>> x
array([ 1., 1., 1.])

>>> m = numpy.eye(3)
>>> m
array([[ 1., 0., 0.],
       [ 0., 1., 0.],
       [ 0., 0., 1.]])

>>> x @ m
array([ 1., 1., 1.])



See also

PEP 465 – A dedicated infix operator for matrix multiplication
PEP written by Nathaniel J. Smith; implemented by Benjamin Peterson.",new-features - pep-465-a-dedicated-infix-operator-for-matrix-multiplication,"S = (H @ beta - r).T @ inv(H @ V @ H.T) @ (H @ beta - r)
",3.5
,new-features - pep-465-a-dedicated-infix-operator-for-matrix-multiplication,"S = dot((dot(H, beta) - r).T,
        dot(inv(dot(dot(H, V), H.T)), dot(H, beta) - r))
",3.5
,new-features - pep-465-a-dedicated-infix-operator-for-matrix-multiplication,">>> import numpy

>>> x = numpy.ones(3)
>>> x
array([ 1., 1., 1.])

>>> m = numpy.eye(3)
>>> m
array([[ 1., 0., 0.],
       [ 0., 1., 0.],
       [ 0., 0., 1.]])

>>> x @ m
array([ 1., 1., 1.])
",3.5
"PEP 448 - Additional Unpacking Generalizations

PEP 448 extends the allowed uses of the * iterable unpacking
operator and ** dictionary unpacking operator.  It is now possible
to use an arbitrary number of unpackings in function calls:
>>> print(*[1], *[2], 3, *[4, 5])
1 2 3 4 5

>>> def fn(a, b, c, d):
...     print(a, b, c, d)
...

>>> fn(**{'a': 1, 'c': 3}, **{'b': 2, 'd': 4})
1 2 3 4


Similarly, tuple, list, set, and dictionary displays allow multiple
unpackings (see Expression lists and Dictionary displays):
>>> *range(4), 4
(0, 1, 2, 3, 4)

>>> [*range(4), 4]
[0, 1, 2, 3, 4]

>>> {*range(4), 4, *(5, 6, 7)}
{0, 1, 2, 3, 4, 5, 6, 7}

>>> {'x': 1, **{'y': 2}}
{'x': 1, 'y': 2}



See also

PEP 448 – Additional Unpacking Generalizations
PEP written by Joshua Landau; implemented by Neil Girdhar,
Thomas Wouters, and Joshua Landau.",new-features - pep-448-additional-unpacking-generalizations,">>> print(*[1], *[2], 3, *[4, 5])
1 2 3 4 5

>>> def fn(a, b, c, d):
...     print(a, b, c, d)
...

>>> fn(**{'a': 1, 'c': 3}, **{'b': 2, 'd': 4})
1 2 3 4
",3.5
,new-features - pep-448-additional-unpacking-generalizations,">>> *range(4), 4
(0, 1, 2, 3, 4)

>>> [*range(4), 4]
[0, 1, 2, 3, 4]

>>> {*range(4), 4, *(5, 6, 7)}
{0, 1, 2, 3, 4, 5, 6, 7}

>>> {'x': 1, **{'y': 2}}
{'x': 1, 'y': 2}
",3.5
"PEP 461 - percent formatting support for bytes and bytearray

PEP 461 adds support for the %
interpolation operator to bytes
and bytearray.
While interpolation is usually thought of as a string operation, there are
cases where interpolation on bytes or bytearrays makes sense, and the
work needed to make up for this missing functionality detracts from the
overall readability of the code.  This issue is particularly important when
dealing with wire format protocols, which are often a mixture of binary
and ASCII compatible text.
Examples:
>>> b'Hello %b!' % b'World'
b'Hello World!'

>>> b'x=%i y=%f' % (1, 2.5)
b'x=1 y=2.500000'


Unicode is not allowed for %b, but it is accepted by %a (equivalent of
repr(obj).encode('ascii', 'backslashreplace')):
>>> b'Hello %b!' % 'World'
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: %b requires bytes, or an object that implements __bytes__, not 'str'

>>> b'price: %a' % '10€'
b""price: '10\\u20ac'""


Note that %s and %r conversion types, although supported, should
only be used in codebases that need compatibility with Python 2.

See also

PEP 461 – Adding % formatting to bytes and bytearray
PEP written by Ethan Furman; implemented by Neil Schemenauer and
Ethan Furman.",new-features - pep-461-percent-formatting-support-for-bytes-and-bytearray,">>> b'Hello %b!' % b'World'
b'Hello World!'

>>> b'x=%i y=%f' % (1, 2.5)
b'x=1 y=2.500000'
",3.5
,new-features - pep-461-percent-formatting-support-for-bytes-and-bytearray,">>> b'Hello %b!' % 'World'
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: %b requires bytes, or an object that implements __bytes__, not 'str'

>>> b'price: %a' % '10€'
b""price: '10\\u20ac'""
",3.5
"PEP 484 - Type Hints

Function annotation syntax has been a Python feature since version 3.0
(PEP 3107), however the semantics of annotations has been left undefined.
Experience has shown that the majority of function annotation
uses were to provide type hints to function parameters and return values.  It
became evident that it would be beneficial for Python users, if the
standard library included the base definitions and tools for type annotations.
PEP 484 introduces a provisional module to
provide these standard definitions and tools, along with some conventions
for situations where annotations are not available.
For example, here is a simple function whose argument and return type
are declared in the annotations:
def greeting(name: str) -> str:
    return 'Hello ' + name


While these annotations are available at runtime through the usual
__annotations__ attribute, no automatic type checking happens at
runtime.  Instead, it is assumed that a separate off-line type checker
(e.g. mypy) will be used for on-demand
source code analysis.
The type system supports unions, generic types, and a special type
named Any which is consistent with (i.e. assignable to
and from) all types.

See also

typing module documentation

PEP 484 – Type Hints
PEP written by Guido van Rossum, Jukka Lehtosalo, and Łukasz Langa;
implemented by Guido van Rossum.



PEP 483 – The Theory of Type Hints
PEP written by Guido van Rossum",new-features - pep-484-type-hints,"def greeting(name: str) -> str:
    return 'Hello ' + name
",3.5
"PEP 471 - os.scandir() function – a better and faster directory iterator

PEP 471 adds a new directory iteration function, os.scandir(),
to the standard library.  Additionally, os.walk() is now
implemented using scandir, which makes it 3 to 5 times faster
on POSIX systems and 7 to 20 times faster on Windows systems.  This is
largely achieved by greatly reducing the number of calls to os.stat()
required to walk a directory tree.
Additionally, scandir returns an iterator, as opposed to returning
a list of file names, which improves memory efficiency when iterating
over very large directories.
The following example shows a simple use of os.scandir() to display all
the files (excluding directories) in the given path that don’t start with
'.'. The entry.is_file() call will generally
not make an additional system call:
for entry in os.scandir(path):
    if not entry.name.startswith('.') and entry.is_file():
        print(entry.name)



See also

PEP 471 – os.scandir() function – a better and faster directory iterator
PEP written and implemented by Ben Hoyt with the help of Victor Stinner.",new-features - pep-471-os-scandir-function-a-better-and-faster-directory-iterator,"for entry in os.scandir(path):
    if not entry.name.startswith('.') and entry.is_file():
        print(entry.name)
",3.5
"PEP 475: Retry system calls failing with EINTR

An errno.EINTR error code is returned whenever a system call, that
is waiting for I/O, is interrupted by a signal.  Previously, Python would
raise InterruptedError in such cases.  This meant that, when writing a
Python application, the developer had two choices:

Ignore the InterruptedError.
Handle the InterruptedError and attempt to restart the interrupted
system call at every call site.

The first option makes an application fail intermittently.
The second option adds a large amount of boilerplate that makes the
code nearly unreadable.  Compare:
print(""Hello World"")


and:
while True:
    try:
        print(""Hello World"")
        break
    except InterruptedError:
        continue


PEP 475 implements automatic retry of system calls on
EINTR.  This removes the burden of dealing with EINTR
or InterruptedError in user code in most situations and makes
Python programs, including the standard library, more robust.  Note that
the system call is only retried if the signal handler does not raise an
exception.
Below is a list of functions which are now retried when interrupted
by a signal:

open() and io.open();
functions of the faulthandler module;
os functions: fchdir(), fchmod(),
fchown(), fdatasync(), fstat(),
fstatvfs(), fsync(), ftruncate(),
mkfifo(), mknod(), open(),
posix_fadvise(), posix_fallocate(), pread(),
pwrite(), read(), readv(), sendfile(),
wait3(), wait4(), wait(),
waitid(), waitpid(), write(),
writev();
special cases: os.close() and os.dup2() now ignore
EINTR errors; the syscall is not retried (see the PEP
for the rationale);
select functions: devpoll.poll(),
epoll.poll(),
kqueue.control(),
poll.poll(), select();
methods of the socket class: accept(),
connect() (except for non-blocking sockets),
recv(), recvfrom(),
recvmsg(), send(),
sendall(), sendmsg(),
sendto();
signal.sigtimedwait() and signal.sigwaitinfo();
time.sleep().


See also

PEP 475 – Retry system calls failing with EINTR
PEP and implementation written by Charles-François Natali and
Victor Stinner, with the help of Antoine Pitrou (the French connection).",new-features - pep-475-retry-system-calls-failing-with-eintr,"print(""Hello World"")
",3.5
,new-features - pep-475-retry-system-calls-failing-with-eintr,"while True:
    try:
        print(""Hello World"")
        break
    except InterruptedError:
        continue
",3.5
"PEP 479: Change StopIteration handling inside generators

The interaction of generators and StopIteration in Python 3.4 and
earlier was sometimes surprising, and could conceal obscure bugs.  Previously,
StopIteration raised accidentally inside a generator function was
interpreted as the end of the iteration by the loop construct driving the
generator.
PEP 479 changes the behavior of generators: when a StopIteration
exception is raised inside a generator, it is replaced with a
RuntimeError before it exits the generator frame.  The main goal of
this change is to ease debugging in the situation where an unguarded
next() call raises StopIteration and causes the iteration controlled
by the generator to terminate silently. This is particularly pernicious in
combination with the yield from construct.
This is a backwards incompatible change, so to enable the new behavior,
a __future__ import is necessary:
>>> from __future__ import generator_stop

>>> def gen():
...     next(iter([]))
...     yield
...
>>> next(gen())
Traceback (most recent call last):
  File ""<stdin>"", line 2, in gen
StopIteration

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
RuntimeError: generator raised StopIteration


Without a __future__ import, a PendingDeprecationWarning will be
raised whenever a StopIteration exception is raised inside a generator.

See also

PEP 479 – Change StopIteration handling inside generators
PEP written by Chris Angelico and Guido van Rossum. Implemented by
Chris Angelico, Yury Selivanov and Nick Coghlan.",new-features - pep-479-change-stopiteration-handling-inside-generators,">>> from __future__ import generator_stop

>>> def gen():
...     next(iter([]))
...     yield
...
>>> next(gen())
Traceback (most recent call last):
  File ""<stdin>"", line 2, in gen
StopIteration

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
RuntimeError: generator raised StopIteration
",3.5
"PEP 485: A function for testing approximate equality

PEP 485 adds the math.isclose() and cmath.isclose()
functions which tell whether two values are approximately equal or
“close” to each other.  Whether or not two values are considered
close is determined according to given absolute and relative tolerances.
Relative tolerance is the maximum allowed difference between isclose
arguments, relative to the larger absolute value:
>>> import math
>>> a = 5.0
>>> b = 4.99998
>>> math.isclose(a, b, rel_tol=1e-5)
True
>>> math.isclose(a, b, rel_tol=1e-6)
False


It is also possible to compare two values using absolute tolerance, which
must be a non-negative value:
>>> import math
>>> a = 5.0
>>> b = 4.99998
>>> math.isclose(a, b, abs_tol=0.00003)
True
>>> math.isclose(a, b, abs_tol=0.00001)
False



See also

PEP 485 – A function for testing approximate equality
PEP written by Christopher Barker; implemented by Chris Barker and
Tal Einat.",new-features - pep-485-a-function-for-testing-approximate-equality,">>> import math
>>> a = 5.0
>>> b = 4.99998
>>> math.isclose(a, b, rel_tol=1e-5)
True
>>> math.isclose(a, b, rel_tol=1e-6)
False
",3.5
,new-features - pep-485-a-function-for-testing-approximate-equality,">>> import math
>>> a = 5.0
>>> b = 4.99998
>>> math.isclose(a, b, abs_tol=0.00003)
True
>>> math.isclose(a, b, abs_tol=0.00001)
False
",3.5
"zipapp

The new zipapp module (specified in PEP 441) provides an API and
command line tool for creating executable Python Zip Applications, which
were introduced in Python 2.6 in bpo-1739468, but which were not well
publicized, either at the time or since.
With the new module, bundling your application is as simple as putting all
the files, including a __main__.py file, into a directory myapp
and running:
$ python -m zipapp myapp
$ python myapp.pyz


The module implementation has been contributed by Paul Moore in
bpo-23491.

See also
PEP 441 – Improving Python ZIP Application Support",new-modules - zipapp,"$ python -m zipapp myapp
$ python myapp.pyz
",3.5
"collections

The OrderedDict class is now implemented in C, which
makes it 4 to 100 times faster.  (Contributed by Eric Snow in bpo-16991.)
OrderedDict.items(),
OrderedDict.keys(),
OrderedDict.values() views now support
reversed() iteration.
(Contributed by Serhiy Storchaka in bpo-19505.)
The deque class now defines
index(), insert(), and
copy(), and supports the + and * operators.
This allows deques to be recognized as a MutableSequence
and improves their substitutability for lists.
(Contributed by Raymond Hettinger in bpo-23704.)
Docstrings produced by namedtuple() can now be updated:
Point = namedtuple('Point', ['x', 'y'])
Point.__doc__ += ': Cartesian coodinate'
Point.x.__doc__ = 'abscissa'
Point.y.__doc__ = 'ordinate'


(Contributed by Berker Peksag in bpo-24064.)
The UserString class now implements the
__getnewargs__(), __rmod__(), casefold(),
format_map(), isprintable(), and maketrans()
methods to match the corresponding methods of str.
(Contributed by Joe Jevnik in bpo-22189.)",improved-modules - collections,"Point = namedtuple('Point', ['x', 'y'])
Point.__doc__ += ': Cartesian coodinate'
Point.x.__doc__ = 'abscissa'
Point.y.__doc__ = 'ordinate'
",3.5
"configparser

configparser now provides a way to customize the conversion
of values by specifying a dictionary of converters in the
ConfigParser constructor, or by defining them
as methods in ConfigParser subclasses.  Converters defined in
a parser instance are inherited by its section proxies.
Example:
>>> import configparser
>>> conv = {}
>>> conv['list'] = lambda v: [e.strip() for e in v.split() if e.strip()]
>>> cfg = configparser.ConfigParser(converters=conv)
>>> cfg.read_string(""""""
... [s]
... list = a b c d e f g
... """""")
>>> cfg.get('s', 'list')
'a b c d e f g'
>>> cfg.getlist('s', 'list')
['a', 'b', 'c', 'd', 'e', 'f', 'g']
>>> section = cfg['s']
>>> section.getlist('list')
['a', 'b', 'c', 'd', 'e', 'f', 'g']


(Contributed by Łukasz Langa in bpo-18159.)",improved-modules - configparser,">>> import configparser
>>> conv = {}
>>> conv['list'] = lambda v: [e.strip() for e in v.split() if e.strip()]
>>> cfg = configparser.ConfigParser(converters=conv)
>>> cfg.read_string(""""""
... [s]
... list = a b c d e f g
... """""")
>>> cfg.get('s', 'list')
'a b c d e f g'
>>> cfg.getlist('s', 'list')
['a', 'b', 'c', 'd', 'e', 'f', 'g']
>>> section = cfg['s']
>>> section.getlist('list')
['a', 'b', 'c', 'd', 'e', 'f', 'g']
",3.5
"contextlib

The new redirect_stderr() context manager (similar to
redirect_stdout()) makes it easier for utility scripts to
handle inflexible APIs that write their output to sys.stderr and
don’t provide any options to redirect it:
>>> import contextlib, io, logging
>>> f = io.StringIO()
>>> with contextlib.redirect_stderr(f):
...     logging.warning('warning')
...
>>> f.getvalue()
'WARNING:root:warning\n'


(Contributed by Berker Peksag in bpo-22389.)",improved-modules - contextlib,">>> import contextlib, io, logging
>>> f = io.StringIO()
>>> with contextlib.redirect_stderr(f):
...     logging.warning('warning')
...
>>> f.getvalue()
'WARNING:root:warning\n'
",3.5
"enum

The Enum callable has a new parameter start to
specify the initial number of enum values if only names are provided:
>>> Animal = enum.Enum('Animal', 'cat dog', start=10)
>>> Animal.cat
<Animal.cat: 10>
>>> Animal.dog
<Animal.dog: 11>


(Contributed by Ethan Furman in bpo-21706.)",improved-modules - enum,">>> Animal = enum.Enum('Animal', 'cat dog', start=10)
>>> Animal.cat
<Animal.cat: 10>
>>> Animal.dog
<Animal.dog: 11>
",3.5
"heapq

Element comparison in merge() can now be customized by
passing a key function in a new optional key keyword argument,
and a new optional reverse keyword argument can be used to reverse element
comparison:
>>> import heapq
>>> a = ['9', '777', '55555']
>>> b = ['88', '6666']
>>> list(heapq.merge(a, b, key=len))
['9', '88', '777', '6666', '55555']
>>> list(heapq.merge(reversed(a), reversed(b), key=len, reverse=True))
['55555', '6666', '777', '88', '9']


(Contributed by Raymond Hettinger in bpo-13742.)",improved-modules - heapq,">>> import heapq
>>> a = ['9', '777', '55555']
>>> b = ['88', '6666']
>>> list(heapq.merge(a, b, key=len))
['9', '88', '777', '6666', '55555']
>>> list(heapq.merge(reversed(a), reversed(b), key=len, reverse=True))
['55555', '6666', '777', '88', '9']
",3.5
"http.client

HTTPConnection.getresponse()
now raises a RemoteDisconnected exception when a
remote server connection is closed unexpectedly.  Additionally, if a
ConnectionError (of which RemoteDisconnected
is a subclass) is raised, the client socket is now closed automatically,
and will reconnect on the next request:
import http.client
conn = http.client.HTTPConnection('www.python.org')
for retries in range(3):
    try:
        conn.request('GET', '/')
        resp = conn.getresponse()
    except http.client.RemoteDisconnected:
        pass


(Contributed by Martin Panter in bpo-3566.)",improved-modules - http-client,"import http.client
conn = http.client.HTTPConnection('www.python.org')
for retries in range(3):
    try:
        conn.request('GET', '/')
        resp = conn.getresponse()
    except http.client.RemoteDisconnected:
        pass
",3.5
"inspect

Both the Signature and Parameter classes are
now picklable and hashable.  (Contributed by Yury Selivanov in bpo-20726
and bpo-20334.)
A new
BoundArguments.apply_defaults()
method provides a way to set default values for missing arguments:
>>> def foo(a, b='ham', *args): pass
>>> ba = inspect.signature(foo).bind('spam')
>>> ba.apply_defaults()
>>> ba.arguments
OrderedDict([('a', 'spam'), ('b', 'ham'), ('args', ())])


(Contributed by Yury Selivanov in bpo-24190.)
A new class method
Signature.from_callable() makes
subclassing of Signature easier.  (Contributed
by Yury Selivanov and Eric Snow in bpo-17373.)
The signature() function now accepts a follow_wrapped
optional keyword argument, which, when set to False, disables automatic
following of __wrapped__ links.
(Contributed by Yury Selivanov in bpo-20691.)
A set of new functions to inspect
coroutine functions and
coroutine objects has been added:
iscoroutine(), iscoroutinefunction(),
isawaitable(), getcoroutinelocals(),
and getcoroutinestate().
(Contributed by Yury Selivanov in bpo-24017 and bpo-24400.)
The stack(), trace(),
getouterframes(), and getinnerframes()
functions now return a list of named tuples.
(Contributed by Daniel Shahaf in bpo-16808.)",improved-modules - inspect,">>> def foo(a, b='ham', *args): pass
>>> ba = inspect.signature(foo).bind('spam')
>>> ba.apply_defaults()
>>> ba.arguments
OrderedDict([('a', 'spam'), ('b', 'ham'), ('args', ())])
",3.5
"ipaddress

Both the IPv4Network and IPv6Network classes
now accept an (address, netmask) tuple argument, so as to easily construct
network objects from existing addresses:
>>> import ipaddress
>>> ipaddress.IPv4Network(('127.0.0.0', 8))
IPv4Network('127.0.0.0/8')
>>> ipaddress.IPv4Network(('127.0.0.0', '255.0.0.0'))
IPv4Network('127.0.0.0/8')


(Contributed by Peter Moody and Antoine Pitrou in bpo-16531.)
A new reverse_pointer attribute for the
IPv4Network and IPv6Network classes
returns the name of the reverse DNS PTR record:
>>> import ipaddress
>>> addr = ipaddress.IPv4Address('127.0.0.1')
>>> addr.reverse_pointer
'1.0.0.127.in-addr.arpa'
>>> addr6 = ipaddress.IPv6Address('::1')
>>> addr6.reverse_pointer
'1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa'


(Contributed by Leon Weber in bpo-20480.)",improved-modules - ipaddress,">>> import ipaddress
>>> ipaddress.IPv4Network(('127.0.0.0', 8))
IPv4Network('127.0.0.0/8')
>>> ipaddress.IPv4Network(('127.0.0.0', '255.0.0.0'))
IPv4Network('127.0.0.0/8')
",3.5
,improved-modules - ipaddress,">>> import ipaddress
>>> addr = ipaddress.IPv4Address('127.0.0.1')
>>> addr.reverse_pointer
'1.0.0.127.in-addr.arpa'
>>> addr6 = ipaddress.IPv6Address('::1')
>>> addr6.reverse_pointer
'1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa'
",3.5
"locale

A new delocalize() function can be used to convert a string into
a normalized number string, taking the LC_NUMERIC settings into account:
>>> import locale
>>> locale.setlocale(locale.LC_NUMERIC, 'de_DE.UTF-8')
'de_DE.UTF-8'
>>> locale.delocalize('1.234,56')
'1234.56'
>>> locale.setlocale(locale.LC_NUMERIC, 'en_US.UTF-8')
'en_US.UTF-8'
>>> locale.delocalize('1,234.56')
'1234.56'


(Contributed by Cédric Krier in bpo-13918.)",improved-modules - locale,">>> import locale
>>> locale.setlocale(locale.LC_NUMERIC, 'de_DE.UTF-8')
'de_DE.UTF-8'
>>> locale.delocalize('1.234,56')
'1234.56'
>>> locale.setlocale(locale.LC_NUMERIC, 'en_US.UTF-8')
'en_US.UTF-8'
>>> locale.delocalize('1,234.56')
'1234.56'
",3.5
"logging

All logging methods (Logger log(),
exception(), critical(),
debug(), etc.), now accept exception instances
as an exc_info argument, in addition to boolean values and exception
tuples:
>>> import logging
>>> try:
...     1/0
... except ZeroDivisionError as ex:
...     logging.error('exception', exc_info=ex)
ERROR:root:exception


(Contributed by Yury Selivanov in bpo-20537.)
The handlers.HTTPHandler class now
accepts an optional ssl.SSLContext instance to configure SSL
settings used in an HTTP connection.
(Contributed by Alex Gaynor in bpo-22788.)
The handlers.QueueListener class now
takes a respect_handler_level keyword argument which, if set to True,
will pass messages to handlers taking handler levels into account.
(Contributed by Vinay Sajip.)",improved-modules - logging,">>> import logging
>>> try:
...     1/0
... except ZeroDivisionError as ex:
...     logging.error('exception', exc_info=ex)
ERROR:root:exception
",3.5
"os

The new scandir() function returning an iterator of
DirEntry objects has been added.  If possible, scandir()
extracts file attributes while scanning a directory, removing the need to
perform subsequent system calls to determine file type or attributes, which may
significantly improve performance.  (Contributed by Ben Hoyt with the help
of Victor Stinner in bpo-22524.)
On Windows, a new
stat_result.st_file_attributes
attribute is now available.  It corresponds to the dwFileAttributes member
of the BY_HANDLE_FILE_INFORMATION structure returned by
GetFileInformationByHandle().  (Contributed by Ben Hoyt in bpo-21719.)
The urandom() function now uses the getrandom() syscall on Linux 3.17
or newer, and getentropy() on OpenBSD 5.6 and newer, removing the need to
use /dev/urandom and avoiding failures due to potential file descriptor
exhaustion.  (Contributed by Victor Stinner in bpo-22181.)
New get_blocking() and set_blocking() functions allow
getting and setting a file descriptor’s blocking mode (O_NONBLOCK.)
(Contributed by Victor Stinner in bpo-22054.)
The truncate() and ftruncate() functions are now supported
on Windows.  (Contributed by Steve Dower in bpo-23668.)
There is a new os.path.commonpath() function returning the longest
common sub-path of each passed pathname.  Unlike the
os.path.commonprefix() function, it always returns a valid
path:
>>> os.path.commonprefix(['/usr/lib', '/usr/local/lib'])
'/usr/l'

>>> os.path.commonpath(['/usr/lib', '/usr/local/lib'])
'/usr'


(Contributed by Rafik Draoui and Serhiy Storchaka in bpo-10395.)",improved-modules - os,">>> os.path.commonprefix(['/usr/lib', '/usr/local/lib'])
'/usr/l'

>>> os.path.commonpath(['/usr/lib', '/usr/local/lib'])
'/usr'
",3.5
"pathlib

The new Path.samefile() method can be used
to check whether the path points to the same file as another path, which can
be either another Path object, or a string:
>>> import pathlib
>>> p1 = pathlib.Path('/etc/hosts')
>>> p2 = pathlib.Path('/etc/../etc/hosts')
>>> p1.samefile(p2)
True


(Contributed by Vajrasky Kok and Antoine Pitrou in bpo-19775.)
The Path.mkdir() method now accepts a new optional
exist_ok argument to match mkdir -p and os.makedirs()
functionality.  (Contributed by Berker Peksag in bpo-21539.)
There is a new Path.expanduser() method to
expand ~ and ~user prefixes.  (Contributed by Serhiy Storchaka and
Claudiu Popa in bpo-19776.)
A new Path.home() class method can be used to get
a Path instance representing the user’s home
directory.
(Contributed by Victor Salgado and Mayank Tripathi in bpo-19777.)
New Path.write_text(),
Path.read_text(),
Path.write_bytes(),
Path.read_bytes() methods to simplify
read/write operations on files.
The following code snippet will create or rewrite existing file
~/spam42:
>>> import pathlib
>>> p = pathlib.Path('~/spam42')
>>> p.expanduser().write_text('ham')
3


(Contributed by Christopher Welborn in bpo-20218.)",improved-modules - pathlib,">>> import pathlib
>>> p1 = pathlib.Path('/etc/hosts')
>>> p2 = pathlib.Path('/etc/../etc/hosts')
>>> p1.samefile(p2)
True
",3.5
,improved-modules - pathlib,">>> import pathlib
>>> p = pathlib.Path('~/spam42')
>>> p.expanduser().write_text('ham')
3
",3.5
"re

References and conditional references to groups with fixed length are now
allowed in lookbehind assertions:
>>> import re
>>> pat = re.compile(r'(a|b).(?<=\1)c')
>>> pat.match('aac')
<_sre.SRE_Match object; span=(0, 3), match='aac'>
>>> pat.match('bbc')
<_sre.SRE_Match object; span=(0, 3), match='bbc'>


(Contributed by Serhiy Storchaka in bpo-9179.)
The number of capturing groups in regular expressions is no longer limited to
100.  (Contributed by Serhiy Storchaka in bpo-22437.)
The sub() and subn() functions now replace unmatched
groups with empty strings instead of raising an exception.
(Contributed by Serhiy Storchaka in bpo-1519638.)
The re.error exceptions have new attributes,
msg, pattern,
pos, lineno,
and colno, that provide better context
information about the error:
>>> re.compile(""""""
...     (?x)
...     .++
... """""")
Traceback (most recent call last):
   ...
sre_constants.error: multiple repeat at position 16 (line 3, column 7)


(Contributed by Serhiy Storchaka in bpo-22578.)",improved-modules - re,">>> import re
>>> pat = re.compile(r'(a|b).(?<=\1)c')
>>> pat.match('aac')
<_sre.SRE_Match object; span=(0, 3), match='aac'>
>>> pat.match('bbc')
<_sre.SRE_Match object; span=(0, 3), match='bbc'>
",3.5
,improved-modules - re,">>> re.compile(""""""
...     (?x)
...     .++
... """""")
Traceback (most recent call last):
   ...
sre_constants.error: multiple repeat at position 16 (line 3, column 7)
",3.5
"subprocess

The new run() function has been added.
It runs the specified command and returns a
CompletedProcess object, which describes a finished
process.  The new API is more consistent and is the recommended approach
to invoking subprocesses in Python code that does not need to maintain
compatibility with earlier Python versions.
(Contributed by Thomas Kluyver in bpo-23342.)
Examples:
>>> subprocess.run([""ls"", ""-l""])  # doesn't capture output
CompletedProcess(args=['ls', '-l'], returncode=0)

>>> subprocess.run(""exit 1"", shell=True, check=True)
Traceback (most recent call last):
  ...
subprocess.CalledProcessError: Command 'exit 1' returned non-zero exit status 1

>>> subprocess.run([""ls"", ""-l"", ""/dev/null""], stdout=subprocess.PIPE)
CompletedProcess(args=['ls', '-l', '/dev/null'], returncode=0,
stdout=b'crw-rw-rw- 1 root root 1, 3 Jan 23 16:23 /dev/null\n')",improved-modules - subprocess,">>> subprocess.run([""ls"", ""-l""])  # doesn't capture output
CompletedProcess(args=['ls', '-l'], returncode=0)

>>> subprocess.run(""exit 1"", shell=True, check=True)
Traceback (most recent call last):
  ...
subprocess.CalledProcessError: Command 'exit 1' returned non-zero exit status 1

>>> subprocess.run([""ls"", ""-l"", ""/dev/null""], stdout=subprocess.PIPE)
CompletedProcess(args=['ls', '-l', '/dev/null'], returncode=0,
stdout=b'crw-rw-rw- 1 root root 1, 3 Jan 23 16:23 /dev/null\n')
",3.5
"Changes in Python behavior


Due to an oversight, earlier Python versions erroneously accepted the
following syntax:
f(1 for x in [1], *args)
f(1 for x in [1], **kwargs)


Python 3.5 now correctly raises a SyntaxError, as generator
expressions must be put in parentheses if not a sole argument to a function.",porting-to-python-3-5 - changes-in-python-behavior,"f(1 for x in [1], *args)
f(1 for x in [1], **kwargs)
",3.5
"PEP 498: Formatted string literals

PEP 498 introduces a new kind of string literals: f-strings, or
formatted string literals.
Formatted string literals are prefixed with 'f' and are similar to
the format strings accepted by str.format().  They contain replacement
fields surrounded by curly braces.  The replacement fields are expressions,
which are evaluated at run time, and then formatted using the
format() protocol:
>>> name = ""Fred""
>>> f""He said his name is {name}.""
'He said his name is Fred.'
>>> width = 10
>>> precision = 4
>>> value = decimal.Decimal(""12.34567"")
>>> f""result: {value:{width}.{precision}}""  # nested fields
'result:      12.35'



See also

PEP 498 – Literal String Interpolation.PEP written and implemented by Eric V. Smith.


Feature documentation.",new-features - pep-498-formatted-string-literals,">>> name = ""Fred""
>>> f""He said his name is {name}.""
'He said his name is Fred.'
>>> width = 10
>>> precision = 4
>>> value = decimal.Decimal(""12.34567"")
>>> f""result: {value:{width}.{precision}}""  # nested fields
'result:      12.35'
",3.6
"PEP 526: Syntax for variable annotations

PEP 484 introduced the standard for type annotations of function
parameters, a.k.a. type hints. This PEP adds syntax to Python for annotating
the types of variables including class variables and instance variables:
primes: List[int] = []

captain: str  # Note: no initial value!

class Starship:
    stats: Dict[str, int] = {}


Just as for function annotations, the Python interpreter does not attach any
particular meaning to variable annotations and only stores them in the
__annotations__ attribute of a class or module.
In contrast to variable declarations in statically typed languages,
the goal of annotation syntax is to provide an easy way to specify structured
type metadata for third party tools and libraries via the abstract syntax tree
and the __annotations__ attribute.

See also

PEP 526 – Syntax for variable annotations.PEP written by Ryan Gonzalez, Philip House, Ivan Levkivskyi, Lisa Roach,
and Guido van Rossum. Implemented by Ivan Levkivskyi.


Tools that use or will use the new syntax:
mypy,
pytype, PyCharm, etc.",new-features - pep-526-syntax-for-variable-annotations,"primes: List[int] = []

captain: str  # Note: no initial value!

class Starship:
    stats: Dict[str, int] = {}
",3.6
"PEP 515: Underscores in Numeric Literals

PEP 515 adds the ability to use underscores in numeric literals for
improved readability.  For example:
>>> 1_000_000_000_000_000
1000000000000000
>>> 0x_FF_FF_FF_FF
4294967295


Single underscores are allowed between digits and after any base
specifier.  Leading, trailing, or multiple underscores in a row are not
allowed.
The string formatting language also now has support
for the '_' option to signal the use of an underscore for a thousands
separator for floating point presentation types and for integer
presentation type 'd'.  For integer presentation types 'b',
'o', 'x', and 'X', underscores will be inserted every 4
digits:
>>> '{:_}'.format(1000000)
'1_000_000'
>>> '{:_x}'.format(0xFFFFFFFF)
'ffff_ffff'



See also

PEP 515 – Underscores in Numeric LiteralsPEP written by Georg Brandl and Serhiy Storchaka.",new-features - pep-515-underscores-in-numeric-literals,">>> 1_000_000_000_000_000
1000000000000000
>>> 0x_FF_FF_FF_FF
4294967295
",3.6
,new-features - pep-515-underscores-in-numeric-literals,">>> '{:_}'.format(1000000)
'1_000_000'
>>> '{:_x}'.format(0xFFFFFFFF)
'ffff_ffff'
",3.6
"PEP 525: Asynchronous Generators

PEP 492 introduced support for native coroutines and async / await
syntax to Python 3.5.  A notable limitation of the Python 3.5 implementation
is that it was not possible to use await and yield in the same
function body.  In Python 3.6 this restriction has been lifted, making it
possible to define asynchronous generators:
async def ticker(delay, to):
    """"""Yield numbers from 0 to *to* every *delay* seconds.""""""
    for i in range(to):
        yield i
        await asyncio.sleep(delay)


The new syntax allows for faster and more concise code.

See also

PEP 525 – Asynchronous GeneratorsPEP written and implemented by Yury Selivanov.",new-features - pep-525-asynchronous-generators,"async def ticker(delay, to):
    """"""Yield numbers from 0 to *to* every *delay* seconds.""""""
    for i in range(to):
        yield i
        await asyncio.sleep(delay)
",3.6
"PEP 530: Asynchronous Comprehensions

PEP 530 adds support for using async for in list, set, dict
comprehensions and generator expressions:
result = [i async for i in aiter() if i % 2]


Additionally, await expressions are supported in all kinds
of comprehensions:
result = [await fun() for fun in funcs if await condition()]



See also

PEP 530 – Asynchronous ComprehensionsPEP written and implemented by Yury Selivanov.",new-features - pep-530-asynchronous-comprehensions,"result = [i async for i in aiter() if i % 2]
",3.6
,new-features - pep-530-asynchronous-comprehensions,"result = [await fun() for fun in funcs if await condition()]
",3.6
"PEP 487: Simpler customization of class creation

It is now possible to customize subclass creation without using a metaclass.
The new __init_subclass__ classmethod will be called on the base class
whenever a new subclass is created:
class PluginBase:
    subclasses = []

    def __init_subclass__(cls, **kwargs):
        super().__init_subclass__(**kwargs)
        cls.subclasses.append(cls)

class Plugin1(PluginBase):
    pass

class Plugin2(PluginBase):
    pass


In order to allow zero-argument super() calls to work correctly from
__init_subclass__() implementations, custom metaclasses must
ensure that the new __classcell__ namespace entry is propagated to
type.__new__ (as described in Creating the class object).

See also

PEP 487 – Simpler customization of class creationPEP written and implemented by Martin Teichmann.


Feature documentation",new-features - pep-487-simpler-customization-of-class-creation,"class PluginBase:
    subclasses = []

    def __init_subclass__(cls, **kwargs):
        super().__init_subclass__(**kwargs)
        cls.subclasses.append(cls)

class Plugin1(PluginBase):
    pass

class Plugin2(PluginBase):
    pass
",3.6
"PEP 487: Descriptor Protocol Enhancements

PEP 487 extends the descriptor protocol to include the new optional
__set_name__() method.  Whenever a new class is defined, the new
method will be called on all descriptors included in the definition, providing
them with a reference to the class being defined and the name given to the
descriptor within the class namespace.  In other words, instances of
descriptors can now know the attribute name of the descriptor in the
owner class:
class IntField:
    def __get__(self, instance, owner):
        return instance.__dict__[self.name]

    def __set__(self, instance, value):
        if not isinstance(value, int):
            raise ValueError(f'expecting integer in {self.name}')
        instance.__dict__[self.name] = value

    # this is the new initializer:
    def __set_name__(self, owner, name):
        self.name = name

class Model:
    int_field = IntField()



See also

PEP 487 – Simpler customization of class creationPEP written and implemented by Martin Teichmann.


Feature documentation",new-features - pep-487-descriptor-protocol-enhancements,"class IntField:
    def __get__(self, instance, owner):
        return instance.__dict__[self.name]

    def __set__(self, instance, value):
        if not isinstance(value, int):
            raise ValueError(f'expecting integer in {self.name}')
        instance.__dict__[self.name] = value

    # this is the new initializer:
    def __set_name__(self, owner, name):
        self.name = name

class Model:
    int_field = IntField()
",3.6
"PEP 519: Adding a file system path protocol

File system paths have historically been represented as str
or bytes objects. This has led to people who write code which
operate on file system paths to assume that such objects are only one
of those two types (an int representing a file descriptor
does not count as that is not a file path). Unfortunately that
assumption prevents alternative object representations of file system
paths like pathlib from working with pre-existing code,
including Python’s standard library.
To fix this situation, a new interface represented by
os.PathLike has been defined. By implementing the
__fspath__() method, an object signals that it
represents a path. An object can then provide a low-level
representation of a file system path as a str or
bytes object. This means an object is considered
path-like if it implements
os.PathLike or is a str or bytes object
which represents a file system path. Code can use os.fspath(),
os.fsdecode(), or os.fsencode() to explicitly get a
str and/or bytes representation of a path-like
object.
The built-in open() function has been updated to accept
os.PathLike objects, as have all relevant functions in the
os and os.path modules, and most other functions and
classes in the standard library.  The os.DirEntry class
and relevant classes in pathlib have also been updated to
implement os.PathLike.
The hope is that updating the fundamental functions for operating
on file system paths will lead to third-party code to implicitly
support all path-like objects without any
code changes, or at least very minimal ones (e.g. calling
os.fspath() at the beginning of code before operating on a
path-like object).
Here are some examples of how the new interface allows for
pathlib.Path to be used more easily and transparently with
pre-existing code:
>>> import pathlib
>>> with open(pathlib.Path(""README"")) as f:
...     contents = f.read()
...
>>> import os.path
>>> os.path.splitext(pathlib.Path(""some_file.txt""))
('some_file', '.txt')
>>> os.path.join(""/a/b"", pathlib.Path(""c""))
'/a/b/c'
>>> import os
>>> os.fspath(pathlib.Path(""some_file.txt""))
'some_file.txt'


(Implemented by Brett Cannon, Ethan Furman, Dusty Phillips, and Jelle Zijlstra.)

See also

PEP 519 – Adding a file system path protocolPEP written by Brett Cannon and Koos Zevenhoven.",new-features - pep-519-adding-a-file-system-path-protocol,">>> import pathlib
>>> with open(pathlib.Path(""README"")) as f:
...     contents = f.read()
...
>>> import os.path
>>> os.path.splitext(pathlib.Path(""some_file.txt""))
('some_file', '.txt')
>>> os.path.join(""/a/b"", pathlib.Path(""c""))
'/a/b/c'
>>> import os
>>> os.fspath(pathlib.Path(""some_file.txt""))
'some_file.txt'
",3.6
"PEP 495: Local Time Disambiguation

In most world locations, there have been and will be times when local clocks
are moved back.  In those times, intervals are introduced in which local
clocks show the same time twice in the same day. In these situations, the
information displayed on a local clock (or stored in a Python datetime
instance) is insufficient to identify a particular moment in time.
PEP 495 adds the new fold attribute to instances of
datetime.datetime and datetime.time classes to differentiate
between two moments in time for which local times are the same:
>>> u0 = datetime(2016, 11, 6, 4, tzinfo=timezone.utc)
>>> for i in range(4):
...     u = u0 + i*HOUR
...     t = u.astimezone(Eastern)
...     print(u.time(), 'UTC =', t.time(), t.tzname(), t.fold)
...
04:00:00 UTC = 00:00:00 EDT 0
05:00:00 UTC = 01:00:00 EDT 0
06:00:00 UTC = 01:00:00 EST 1
07:00:00 UTC = 02:00:00 EST 0


The values of the fold attribute have the
value 0 for all instances except those that represent the second
(chronologically) moment in time in an ambiguous case.

See also

PEP 495 – Local Time DisambiguationPEP written by Alexander Belopolsky and Tim Peters, implementation
by Alexander Belopolsky.",new-features - pep-495-local-time-disambiguation,">>> u0 = datetime(2016, 11, 6, 4, tzinfo=timezone.utc)
>>> for i in range(4):
...     u = u0 + i*HOUR
...     t = u.astimezone(Eastern)
...     print(u.time(), 'UTC =', t.time(), t.tzname(), t.fold)
...
04:00:00 UTC = 00:00:00 EDT 0
05:00:00 UTC = 01:00:00 EDT 0
06:00:00 UTC = 01:00:00 EST 1
07:00:00 UTC = 02:00:00 EST 0
",3.6
"PYTHONMALLOC environment variable

The new PYTHONMALLOC environment variable allows setting the Python
memory allocators and installing debug hooks.
It is now possible to install debug hooks on Python memory allocators on Python
compiled in release mode using PYTHONMALLOC=debug. Effects of debug hooks:

Newly allocated memory is filled with the byte 0xCB
Freed memory is filled with the byte 0xDB
Detect violations of the Python memory allocator API. For example,
PyObject_Free() called on a memory block allocated by
PyMem_Malloc().
Detect writes before the start of a buffer (buffer underflows)
Detect writes after the end of a buffer (buffer overflows)
Check that the GIL is held when allocator
functions of PYMEM_DOMAIN_OBJ (ex: PyObject_Malloc()) and
PYMEM_DOMAIN_MEM (ex: PyMem_Malloc()) domains are called.

Checking if the GIL is held is also a new feature of Python 3.6.
See the PyMem_SetupDebugHooks() function for debug hooks on Python
memory allocators.
It is now also possible to force the usage of the malloc() allocator of
the C library for all Python memory allocations using PYTHONMALLOC=malloc.
This is helpful when using external memory debuggers like Valgrind on
a Python compiled in release mode.
On error, the debug hooks on Python memory allocators now use the
tracemalloc module to get the traceback where a memory block was
allocated.
Example of fatal error on buffer overflow using
python3.6 -X tracemalloc=5 (store 5 frames in traces):
Debug memory block at address p=0x7fbcd41666f8: API 'o'
    4 bytes originally requested
    The 7 pad bytes at p-7 are FORBIDDENBYTE, as expected.
    The 8 pad bytes at tail=0x7fbcd41666fc are not all FORBIDDENBYTE (0xfb):
        at tail+0: 0x02 *** OUCH
        at tail+1: 0xfb
        at tail+2: 0xfb
        at tail+3: 0xfb
        at tail+4: 0xfb
        at tail+5: 0xfb
        at tail+6: 0xfb
        at tail+7: 0xfb
    The block was made by call #1233329 to debug malloc/realloc.
    Data at p: 1a 2b 30 00

Memory block allocated at (most recent call first):
  File ""test/test_bytes.py"", line 323
  File ""unittest/case.py"", line 600
  File ""unittest/case.py"", line 648
  File ""unittest/suite.py"", line 122
  File ""unittest/suite.py"", line 84

Fatal Python error: bad trailing pad byte

Current thread 0x00007fbcdbd32700 (most recent call first):
  File ""test/test_bytes.py"", line 323 in test_hex
  File ""unittest/case.py"", line 600 in run
  File ""unittest/case.py"", line 648 in __call__
  File ""unittest/suite.py"", line 122 in run
  File ""unittest/suite.py"", line 84 in __call__
  File ""unittest/suite.py"", line 122 in run
  File ""unittest/suite.py"", line 84 in __call__
  ...


(Contributed by Victor Stinner in bpo-26516 and bpo-26564.)",new-features - pythonmalloc-environment-variable,"Debug memory block at address p=0x7fbcd41666f8: API 'o'
    4 bytes originally requested
    The 7 pad bytes at p-7 are FORBIDDENBYTE, as expected.
    The 8 pad bytes at tail=0x7fbcd41666fc are not all FORBIDDENBYTE (0xfb):
        at tail+0: 0x02 *** OUCH
        at tail+1: 0xfb
        at tail+2: 0xfb
        at tail+3: 0xfb
        at tail+4: 0xfb
        at tail+5: 0xfb
        at tail+6: 0xfb
        at tail+7: 0xfb
    The block was made by call #1233329 to debug malloc/realloc.
    Data at p: 1a 2b 30 00

Memory block allocated at (most recent call first):
  File ""test/test_bytes.py"", line 323
  File ""unittest/case.py"", line 600
  File ""unittest/case.py"", line 648
  File ""unittest/suite.py"", line 122
  File ""unittest/suite.py"", line 84

Fatal Python error: bad trailing pad byte

Current thread 0x00007fbcdbd32700 (most recent call first):
  File ""test/test_bytes.py"", line 323 in test_hex
  File ""unittest/case.py"", line 600 in run
  File ""unittest/case.py"", line 648 in __call__
  File ""unittest/suite.py"", line 122 in run
  File ""unittest/suite.py"", line 84 in __call__
  File ""unittest/suite.py"", line 122 in run
  File ""unittest/suite.py"", line 84 in __call__
  ...
",3.6
"decimal

New Decimal.as_integer_ratio()
method that returns a pair (n, d) of integers that represent the given
Decimal instance as a fraction, in lowest terms and
with a positive denominator:
>>> Decimal('-3.14').as_integer_ratio()
(-157, 50)


(Contributed by Stefan Krah amd Mark Dickinson in bpo-25928.)",improved-modules - decimal,">>> Decimal('-3.14').as_integer_ratio()
(-157, 50)
",3.6
"enum

Two new enumeration base classes have been added to the enum module:
Flag and IntFlags.  Both are used to define
constants that can be combined using the bitwise operators.
(Contributed by Ethan Furman in bpo-23591.)
Many standard library modules have been updated to use the
IntFlags class for their constants.
The new enum.auto value can be used to assign values to enum
members automatically:
>>> from enum import Enum, auto
>>> class Color(Enum):
...     red = auto()
...     blue = auto()
...     green = auto()
...
>>> list(Color)
[<Color.red: 1>, <Color.blue: 2>, <Color.green: 3>]",improved-modules - enum,">>> from enum import Enum, auto
>>> class Color(Enum):
...     red = auto()
...     blue = auto()
...     green = auto()
...
>>> list(Color)
[<Color.red: 1>, <Color.blue: 2>, <Color.green: 3>]
",3.6
"traceback

Both the traceback module and the interpreter’s builtin exception display now
abbreviate long sequences of repeated lines in tracebacks as shown in the
following example:
>>> def f(): f()
...
>>> f()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<stdin>"", line 1, in f
  File ""<stdin>"", line 1, in f
  File ""<stdin>"", line 1, in f
  [Previous line repeated 995 more times]
RecursionError: maximum recursion depth exceeded


(Contributed by Emanuel Barry in bpo-26823.)",improved-modules - traceback,">>> def f(): f()
...
>>> f()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<stdin>"", line 1, in f
  File ""<stdin>"", line 1, in f
  File ""<stdin>"", line 1, in f
  [Previous line repeated 995 more times]
RecursionError: maximum recursion depth exceeded
",3.6
"typing

Since the typing module is provisional,
all changes introduced in Python 3.6 have also been
backported to Python 3.5.x.
The typing module has a much improved support for generic type
aliases.  For example Dict[str, Tuple[S, T]] is now a valid
type annotation.
(Contributed by Guido van Rossum in Github #195.)
The typing.ContextManager class has been added for
representing contextlib.AbstractContextManager.
(Contributed by Brett Cannon in bpo-25609.)
The typing.Collection class has been added for
representing collections.abc.Collection.
(Contributed by Ivan Levkivskyi in bpo-27598.)
The typing.ClassVar type construct has been added to
mark class variables.  As introduced in PEP 526, a variable annotation
wrapped in ClassVar indicates that a given attribute is intended to be used as
a class variable and should not be set on instances of that class.
(Contributed by Ivan Levkivskyi in Github #280.)
A new TYPE_CHECKING constant that is assumed to be
True by the static type chekers, but is False at runtime.
(Contributed by Guido van Rossum in Github #230.)
A new NewType() helper function has been added to create
lightweight distinct types for annotations:
from typing import NewType

UserId = NewType('UserId', int)
some_id = UserId(524313)


The static type checker will treat the new type as if it were a subclass
of the original type.  (Contributed by Ivan Levkivskyi in Github #189.)",improved-modules - typing,"from typing import NewType

UserId = NewType('UserId', int)
some_id = UserId(524313)
",3.6
"warnings

A new optional source parameter has been added to the
warnings.warn_explicit() function: the destroyed object which emitted a
ResourceWarning. A source attribute has also been added to
warnings.WarningMessage (contributed by Victor Stinner in
bpo-26568 and bpo-26567).
When a ResourceWarning warning is logged, the tracemalloc module is now
used to try to retrieve the traceback where the destroyed object was allocated.
Example with the script example.py:
import warnings

def func():
    return open(__file__)

f = func()
f = None


Output of the command python3.6 -Wd -X tracemalloc=5 example.py:
example.py:7: ResourceWarning: unclosed file <_io.TextIOWrapper name='example.py' mode='r' encoding='UTF-8'>
  f = None
Object allocated at (most recent call first):
  File ""example.py"", lineno 4
    return open(__file__)
  File ""example.py"", lineno 6
    f = func()


The “Object allocated at” traceback is new and is only displayed if
tracemalloc is tracing Python memory allocations and if the
warnings module was already imported.",improved-modules - warnings,"import warnings

def func():
    return open(__file__)

f = func()
f = None
",3.6
,improved-modules - warnings,"example.py:7: ResourceWarning: unclosed file <_io.TextIOWrapper name='example.py' mode='r' encoding='UTF-8'>
  f = None
Object allocated at (most recent call first):
  File ""example.py"", lineno 4
    return open(__file__)
  File ""example.py"", lineno 6
    f = func()
",3.6
"Other Improvements


When --version (short form: -V) is supplied twice,
Python prints sys.version for detailed information.
$ ./python -VV
Python 3.6.0b4+ (3.6:223967b49e49+, Nov 21 2016, 20:55:04)
[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)]",other-improvements,"$ ./python -VV
Python 3.6.0b4+ (3.6:223967b49e49+, Nov 21 2016, 20:55:04)
[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)]
",3.6
"Changes in the Python API


open() will no longer allow combining the 'U' mode flag
with '+'.
(Contributed by Jeff Balogh and John O’Connor in bpo-2091.)
sqlite3 no longer implicitly commits an open transaction before DDL
statements.
On Linux, os.urandom() now blocks until the system urandom entropy pool
is initialized to increase the security.
When importlib.abc.Loader.exec_module() is defined,
importlib.abc.Loader.create_module() must also be defined.
PyErr_SetImportError() now sets TypeError when its msg
argument is not set. Previously only NULL was returned.
The format of the co_lnotab attribute of code objects changed to support
a negative line number delta. By default, Python does not emit bytecode with
a negative line number delta. Functions using frame.f_lineno,
PyFrame_GetLineNumber() or PyCode_Addr2Line() are not affected.
Functions directly decoding co_lnotab should be updated to use a signed
8-bit integer type for the line number delta, but this is only required to
support applications using a negative line number delta. See
Objects/lnotab_notes.txt for the co_lnotab format and how to decode
it, and see the PEP 511 for the rationale.
The functions in the compileall module now return booleans instead
of 1 or 0 to represent success or failure, respectively. Thanks to
booleans being a subclass of integers, this should only be an issue if you
were doing identity checks for 1 or 0. See bpo-25768.
Reading the port attribute of
urllib.parse.urlsplit() and urlparse() results
now raises ValueError for out-of-range values, rather than
returning None.  See bpo-20059.
The imp module now raises a DeprecationWarning instead of
PendingDeprecationWarning.
The following modules have had missing APIs added to their __all__
attributes to match the documented APIs:
calendar, cgi, csv,
ElementTree, enum,
fileinput, ftplib, logging, mailbox,
mimetypes, optparse, plistlib, smtpd,
subprocess, tarfile, threading and
wave.  This means they will export new symbols when import *
is used.
(Contributed by Joel Taddei and Jacek Kołodziej in bpo-23883.)
When performing a relative import, if __package__ does not compare equal
to __spec__.parent then ImportWarning is raised.
(Contributed by Brett Cannon in bpo-25791.)
When a relative import is performed and no parent package is known, then
ImportError will be raised. Previously, SystemError could be
raised. (Contributed by Brett Cannon in bpo-18018.)
Servers based on the socketserver module, including those
defined in http.server, xmlrpc.server and
wsgiref.simple_server, now only catch exceptions derived
from Exception. Therefore if a request handler raises
an exception like SystemExit or KeyboardInterrupt,
handle_error() is no longer called, and
the exception will stop a single-threaded server. (Contributed by
Martin Panter in bpo-23430.)
spwd.getspnam() now raises a PermissionError instead of
KeyError if the user doesn’t have privileges.
The socket.socket.close() method now raises an exception if
an error (e.g. EBADF) was reported by the underlying system call.
(Contributed by Martin Panter in bpo-26685.)
The decode_data argument for the smtpd.SMTPChannel and
smtpd.SMTPServer constructors is now False by default.
This means that the argument passed to
process_message() is now a bytes object by
default, and process_message() will be passed keyword arguments.
Code that has already been updated in accordance with the deprecation
warning generated by 3.5 will not be affected.
All optional arguments of the dump(), dumps(),
load() and loads() functions and
JSONEncoder and JSONDecoder class
constructors in the json module are now keyword-only.
(Contributed by Serhiy Storchaka in bpo-18726.)
Subclasses of type which don’t override type.__new__ may no
longer use the one-argument form to get the type of an object.
As part of PEP 487, the handling of keyword arguments passed to
type (other than the metaclass hint, metaclass) is now
consistently delegated to object.__init_subclass__(). This means that
type.__new__() and type.__init__() both now accept arbitrary
keyword arguments, but object.__init_subclass__() (which is called from
type.__new__()) will reject them by default. Custom metaclasses
accepting additional keyword arguments will need to adjust their calls to
type.__new__() (whether direct or via super) accordingly.
In distutils.command.sdist.sdist, the default_format
attribute has been removed and is no longer honored. Instead, the
gzipped tarfile format is the default on all platforms and no
platform-specific selection is made.
In environments where distributions are
built on Windows and zip distributions are required, configure
the project with a setup.cfg file containing the following:
[sdist]
formats=zip


This behavior has also been backported to earlier Python versions
by Setuptools 26.0.0.

In the urllib.request module and the
http.client.HTTPConnection.request() method, if no Content-Length
header field has been specified and the request body is a file object,
it is now sent with HTTP 1.1 chunked encoding. If a file object has to
be sent to a HTTP 1.0 server, the Content-Length value now has to be
specified by the caller.
(Contributed by Demian Brecht and Rolf Krahl with tweaks from
Martin Panter in bpo-12319.)
The DictReader now returns rows of type
OrderedDict.
(Contributed by Steve Holden in bpo-27842.)
The crypt.METHOD_CRYPT will no longer be added to crypt.methods
if unsupported by the platform.
(Contributed by Victor Stinner in bpo-25287.)
The verbose and rename arguments for
namedtuple() are now keyword-only.
(Contributed by Raymond Hettinger in bpo-25628.)
On Linux, ctypes.util.find_library() now looks in
LD_LIBRARY_PATH for shared libraries.
(Contributed by Vinay Sajip in bpo-9998.)
The imaplib.IMAP4 class now handles flags containing the
']' character in messages sent from the server to improve
real-world compatibility.
(Contributed by Lita Cho in bpo-21815.)
The mmap.write() function now returns the number
of bytes written like other write methods.
(Contributed by Jakub Stasiak in bpo-26335.)
The pkgutil.iter_modules() and pkgutil.walk_packages()
functions now return ModuleInfo named tuples.
(Contributed by Ramchandra Apte in bpo-17211.)
re.sub() now raises an error for invalid numerical group
references in replacement templates even if the pattern is not
found in the string.  The error message for invalid group references
now includes the group index and the position of the reference.
(Contributed by SilentGhost, Serhiy Storchaka in bpo-25953.)
zipfile.ZipFile will now raise NotImplementedError for
unrecognized compression values.  Previously a plain RuntimeError
was raised.  Additionally, calling ZipFile methods
on a closed ZipFile or calling the write() method
on a ZipFile created with mode 'r' will raise a ValueError.
Previously, a RuntimeError was raised in those scenarios.
when custom metaclasses are combined with zero-argument super() or
direct references from methods to the implicit __class__ closure
variable, the implicit __classcell__ namespace entry must now be passed
up to type.__new__ for initialisation. Failing to do so will result in
a DeprecationWarning in Python 3.6 and a RuntimeError in
Python 3.8.
With the introduction of ModuleNotFoundError, import system consumers
may start expecting import system replacements to raise that more specific
exception when appropriate, rather than the less-specific ImportError.
To provide future compatibility with such consumers, implementors of
alternative import systems that completely replace __import__() will
need to update their implementations to raise the new subclass when a module
can’t be found at all. Implementors of compliant plugins to the default
import system shouldn’t need to make any changes, as the default import
system will raise the new subclass when appropriate.",porting-to-python-3-6 - changes-in-the-python-api,"[sdist]
formats=zip
",3.6
"PEP 563: Postponed Evaluation of Annotations

The advent of type hints in Python uncovered two glaring usability issues
with the functionality of annotations added in PEP 3107 and refined
further in PEP 526:

annotations could only use names which were already available in the
current scope, in other words they didn’t support forward references
of any kind; and
annotating source code had adverse effects on startup time of Python
programs.

Both of these issues are fixed by postponing the evaluation of
annotations.  Instead of compiling code which executes expressions in
annotations at their definition time, the compiler stores the annotation
in a string form equivalent to the AST of the expression in question.
If needed, annotations can be resolved at runtime using
typing.get_type_hints().  In the common case where this is not
required, the annotations are cheaper to store (since short strings
are interned by the interpreter) and make startup time faster.
Usability-wise, annotations now support forward references, making the
following syntax valid:
class C:
    @classmethod
    def from_string(cls, source: str) -> C:
        ...

    def validate_b(self, obj: B) -> bool:
        ...

class B:
    ...


Since this change breaks compatibility, the new behavior needs to be enabled
on a per-module basis in Python 3.7 using a __future__ import:
from __future__ import annotations


It will become the default in Python 3.10.

See also

PEP 563 – Postponed evaluation of annotationsPEP written and implemented by Łukasz Langa.",new-features - pep-563-postponed-evaluation-of-annotations,"class C:
    @classmethod
    def from_string(cls, source: str) -> C:
        ...

    def validate_b(self, obj: B) -> bool:
        ...

class B:
    ...
",3.7
,new-features - pep-563-postponed-evaluation-of-annotations,"from __future__ import annotations
",3.7
"dataclasses

The new dataclass() decorator provides a way to declare
data classes.  A data class describes its attributes using class variable
annotations.  Its constructor and other magic methods, such as
__repr__(), __eq__(), and
__hash__() are generated automatically.
Example:
@dataclass
class Point:
    x: float
    y: float
    z: float = 0.0

p = Point(1.5, 2.5)
print(p)   # produces ""Point(x=1.5, y=2.5, z=0.0)""



See also

PEP 557 – Data ClassesPEP written and implemented by Eric V. Smith",new-modules - dataclasses,"@dataclass
class Point:
    x: float
    y: float
    z: float = 0.0

p = Point(1.5, 2.5)
print(p)   # produces ""Point(x=1.5, y=2.5, z=0.0)""
",3.7
"asyncio

The asyncio module has received many new features, usability and
performance improvements.  Notable changes
include:

The new provisional asyncio.run() function can
be used to run a coroutine from synchronous code by automatically creating and
destroying the event loop.
(Contributed by Yury Selivanov in bpo-32314.)
asyncio gained support for contextvars.
loop.call_soon(),
loop.call_soon_threadsafe(),
loop.call_later(),
loop.call_at(), and
Future.add_done_callback()
have a new optional keyword-only context parameter.
Tasks now track their context automatically.
See PEP 567 for more details.
(Contributed by Yury Selivanov in bpo-32436.)
The new asyncio.create_task() function has been added as a shortcut
to asyncio.get_event_loop().create_task().
(Contributed by Andrew Svetlov in bpo-32311.)
The new loop.start_tls()
method can be used to upgrade an existing connection to TLS.
(Contributed by Yury Selivanov in bpo-23749.)
The new loop.sock_recv_into()
method allows reading data from a socket directly into a provided buffer making
it possible to reduce data copies.
(Contributed by Antoine Pitrou in bpo-31819.)
The new asyncio.current_task() function returns the currently running
Task instance, and the new asyncio.all_tasks()
function returns a set of all existing Task instances in a given loop.
The Task.current_task() and
Task.all_tasks() methods have been deprecated.
(Contributed by Andrew Svetlov in bpo-32250.)
The new provisional BufferedProtocol class allows
implementing streaming protocols with manual control over the receive buffer.
(Contributed by Yury Selivanov in bpo-32251.)
The new asyncio.get_running_loop() function returns the currently
running loop, and raises a RuntimeError if no loop is running.
This is in contrast with asyncio.get_event_loop(), which will create
a new event loop if none is running.
(Contributed by Yury Selivanov in bpo-32269.)
The new StreamWriter.wait_closed()
coroutine method allows waiting until the stream writer is closed.  The new
StreamWriter.is_closing() method
can be used to determine if the writer is closing.
(Contributed by Andrew Svetlov in bpo-32391.)
The new loop.sock_sendfile()
coroutine method allows sending files using os.sendfile when possible.
(Contributed by Andrew Svetlov in bpo-32410.)
The new Future.get_loop() and
Task.get_loop() methods return the instance of the loop on which a task or
a future were created.
Server.get_loop() allows doing the same for
asyncio.Server objects.
(Contributed by Yury Selivanov in bpo-32415 and
Srinivas Reddy Thatiparthy in bpo-32418.)
It is now possible to control how instances of asyncio.Server begin
serving.  Previously, the server would start serving immediately when created.
The new start_serving keyword argument to
loop.create_server() and
loop.create_unix_server(),
as well as Server.start_serving(), and
Server.serve_forever()
can be used to decouple server instantiation and serving.  The new
Server.is_serving() method returns True
if the server is serving.  Server objects are now
asynchronous context managers:
srv = await loop.create_server(...)

async with srv:
    # some code

# At this point, srv is closed and no longer accepts new connections.


(Contributed by Yury Selivanov in bpo-32662.)

Callback objects returned by
loop.call_later()
gained the new when() method which
returns an absolute scheduled callback timestamp.
(Contributed by Andrew Svetlov in bpo-32741.)
The loop.create_datagram_endpoint()  method
gained support for Unix sockets.
(Contributed by Quentin Dawans in bpo-31245.)
The asyncio.open_connection(), asyncio.start_server() functions,
loop.create_connection(),
loop.create_server(),
loop.create_accepted_socket()
methods and their corresponding UNIX socket variants now accept the
ssl_handshake_timeout keyword argument.
(Contributed by Neil Aspinall in bpo-29970.)
The new Handle.cancelled() method returns
True if the callback was cancelled.
(Contributed by Marat Sharafutdinov in bpo-31943.)
The asyncio source has been converted to use the
async/await syntax.
(Contributed by Andrew Svetlov in bpo-32193.)
The new ReadTransport.is_reading()
method can be used to determine the reading state of the transport.
Additionally, calls to
ReadTransport.resume_reading()
and ReadTransport.pause_reading()
are now idempotent.
(Contributed by Yury Selivanov in bpo-32356.)
Loop methods which accept socket paths now support passing
path-like objects.
(Contributed by Yury Selivanov in bpo-32066.)
In asyncio TCP sockets on Linux are now created with TCP_NODELAY
flag set by default.
(Contributed by Yury Selivanov and Victor Stinner in bpo-27456.)
Exceptions occurring in cancelled tasks are no longer logged.
(Contributed by Yury Selivanov in bpo-30508.)
New WindowsSelectorEventLoopPolicy and
WindowsProactorEventLoopPolicy classes.
(Contributed by Yury Selivanov in bpo-33792.)

Several asyncio APIs have been
deprecated.",improved-modules - asyncio,"srv = await loop.create_server(...)

async with srv:
    # some code

# At this point, srv is closed and no longer accepts new connections.
",3.7
"Changes in Python Behavior


async and await names are now reserved keywords.
Code using these names as identifiers will now raise a SyntaxError.
(Contributed by Jelle Zijlstra in bpo-30406.)
PEP 479 is enabled for all code in Python 3.7, meaning that
StopIteration exceptions raised directly or indirectly in
coroutines and generators are transformed into RuntimeError
exceptions.
(Contributed by Yury Selivanov in bpo-32670.)
object.__aiter__() methods can no longer be declared as
asynchronous.  (Contributed by Yury Selivanov in bpo-31709.)
Due to an oversight, earlier Python versions erroneously accepted the
following syntax:
f(1 for x in [1],)

class C(1 for x in [1]):
    pass


Python 3.7 now correctly raises a SyntaxError, as a generator
expression always needs to be directly inside a set of parentheses
and cannot have a comma on either side, and the duplication of the
parentheses can be omitted only on calls.
(Contributed by Serhiy Storchaka in bpo-32012 and bpo-32023.)

When using the -m switch, the initial working directory is now added
to sys.path, rather than an empty string (which dynamically denoted
the current working directory at the time of each import). Any programs that
are checking for the empty string, or otherwise relying on the previous
behaviour, will need to be updated accordingly (e.g. by also checking for
os.getcwd() or os.path.dirname(__main__.__file__), depending on why
the code was checking for the empty string in the first place).",porting-to-python-3-7 - changes-in-python-behavior,"f(1 for x in [1],)

class C(1 for x in [1]):
    pass
",3.7
"Assignment expressions

There is new syntax := that assigns values to variables as part of a larger
expression. It is affectionately known as “the walrus operator” due to
its resemblance to the eyes and tusks of a walrus.
In this example, the assignment expression helps avoid calling
len() twice:
if (n := len(a)) > 10:
    print(f""List is too long ({n} elements, expected <= 10)"")


A similar benefit arises during regular expression matching where
match objects are needed twice, once to test whether a match
occurred and another to extract a subgroup:
discount = 0.0
if (mo := re.search(r'(\d+)% discount', advertisement)):
    discount = float(mo.group(1)) / 100.0


The operator is also useful with while-loops that compute
a value to test loop termination and then need that same
value again in the body of the loop:
# Loop over fixed length blocks
while (block := f.read(256)) != '':
    process(block)


Another motivating use case arises in list comprehensions where
a value computed in a filtering condition is also needed in
the expression body:
[clean_name.title() for name in names
 if (clean_name := normalize('NFC', name)) in allowed_names]


Try to limit use of the walrus operator to clean cases that reduce
complexity and improve readability.
See PEP 572 for a full description.
(Contributed by Emily Morehouse in bpo-35224.)",new-features - assignment-expressions,"if (n := len(a)) > 10:
    print(f""List is too long ({n} elements, expected <= 10)"")
",3.8
,new-features - assignment-expressions,"discount = 0.0
if (mo := re.search(r'(\d+)% discount', advertisement)):
    discount = float(mo.group(1)) / 100.0
",3.8
,new-features - assignment-expressions,"# Loop over fixed length blocks
while (block := f.read(256)) != '':
    process(block)
",3.8
,new-features - assignment-expressions,"[clean_name.title() for name in names
 if (clean_name := normalize('NFC', name)) in allowed_names]
",3.8
"Positional-only parameters

There is a new function parameter syntax / to indicate that some
function parameters must be specified positionally and cannot be used as
keyword arguments.  This is the same notation shown by help() for C
functions annotated with Larry Hastings’ Argument Clinic tool.
In the following example, parameters a and b are positional-only,
while c or d can be positional or keyword, and e or f are
required to be keywords:
def f(a, b, /, c, d, *, e, f):
    print(a, b, c, d, e, f)


The following is a valid call:
f(10, 20, 30, d=40, e=50, f=60)


However, these are invalid calls:
f(10, b=20, c=30, d=40, e=50, f=60)   # b cannot be a keyword argument
f(10, 20, 30, 40, 50, f=60)           # e must be a keyword argument


One use case for this notation is that it allows pure Python functions
to fully emulate behaviors of existing C coded functions.  For example,
the built-in divmod() function does not accept keyword arguments:
def divmod(a, b, /):
    ""Emulate the built in divmod() function""
    return (a // b, a % b)


Another use case is to preclude keyword arguments when the parameter
name is not helpful.  For example, the builtin len() function has
the signature len(obj, /).  This precludes awkward calls such as:
len(obj='hello')  # The ""obj"" keyword argument impairs readability


A further benefit of marking a parameter as positional-only is that it
allows the parameter name to be changed in the future without risk of
breaking client code.  For example, in the statistics module, the
parameter name dist may be changed in the future.  This was made
possible with the following function specification:
def quantiles(dist, /, *, n=4, method='exclusive')
    ...


Since the parameters to the left of / are not exposed as possible
keywords, the parameters names remain available for use in **kwargs:
>>> def f(a, b, /, **kwargs):
...     print(a, b, kwargs)
...
>>> f(10, 20, a=1, b=2, c=3)         # a and b are used in two ways
10 20 {'a': 1, 'b': 2, 'c': 3}


This greatly simplifies the implementation of functions and methods
that need to accept arbitrary keyword arguments.  For example, here
is an excerpt from code in the collections module:
class Counter(dict):

    def __init__(self, iterable=None, /, **kwds):
        # Note ""iterable"" is a possible keyword argument


See PEP 570 for a full description.
(Contributed by Pablo Galindo in bpo-36540.)",new-features - positional-only-parameters,"def f(a, b, /, c, d, *, e, f):
    print(a, b, c, d, e, f)
",3.8
,new-features - positional-only-parameters,"f(10, 20, 30, d=40, e=50, f=60)
",3.8
,new-features - positional-only-parameters,"f(10, b=20, c=30, d=40, e=50, f=60)   # b cannot be a keyword argument
f(10, 20, 30, 40, 50, f=60)           # e must be a keyword argument
",3.8
,new-features - positional-only-parameters,"def divmod(a, b, /):
    ""Emulate the built in divmod() function""
    return (a // b, a % b)
",3.8
,new-features - positional-only-parameters,"len(obj='hello')  # The ""obj"" keyword argument impairs readability
",3.8
,new-features - positional-only-parameters,"def quantiles(dist, /, *, n=4, method='exclusive')
    ...
",3.8
,new-features - positional-only-parameters,">>> def f(a, b, /, **kwargs):
...     print(a, b, kwargs)
...
>>> f(10, 20, a=1, b=2, c=3)         # a and b are used in two ways
10 20 {'a': 1, 'b': 2, 'c': 3}
",3.8
,new-features - positional-only-parameters,"class Counter(dict):

    def __init__(self, iterable=None, /, **kwds):
        # Note ""iterable"" is a possible keyword argument
",3.8
"f-strings support = for self-documenting expressions and debugging

Added an = specifier to f-strings. An f-string such as
f'{expr=}' will expand to the text of the expression, an equal sign,
then the representation of the evaluated expression.  For example:
>>> user = 'eric_idle'
>>> member_since = date(1975, 7, 31)
>>> f'{user=} {member_since=}'
""user='eric_idle' member_since=datetime.date(1975, 7, 31)""


The usual f-string format specifiers allow more
control over how the result of the expression is displayed:
>>> delta = date.today() - member_since
>>> f'{user=!s}  {delta.days=:,d}'
'user=eric_idle  delta.days=16,075'


The = specifier will display the whole expression so that
calculations can be shown:
>>> print(f'{theta=}  {cos(radians(theta))=:.3f}')
theta=30  cos(radians(theta))=0.866


(Contributed by Eric V. Smith and Larry Hastings in bpo-36817.)",new-features - f-strings-support-for-self-documenting-expressions-and-debugging,">>> user = 'eric_idle'
>>> member_since = date(1975, 7, 31)
>>> f'{user=} {member_since=}'
""user='eric_idle' member_since=datetime.date(1975, 7, 31)""
",3.8
,new-features - f-strings-support-for-self-documenting-expressions-and-debugging,">>> delta = date.today() - member_since
>>> f'{user=!s}  {delta.days=:,d}'
'user=eric_idle  delta.days=16,075'
",3.8
,new-features - f-strings-support-for-self-documenting-expressions-and-debugging,">>> print(f'{theta=}  {cos(radians(theta))=:.3f}')
theta=30  cos(radians(theta))=0.866
",3.8
"Other Language Changes


A continue statement was illegal in the finally clause
due to a problem with the implementation.  In Python 3.8 this restriction
was lifted.
(Contributed by Serhiy Storchaka in bpo-32489.)
The bool, int, and fractions.Fraction types
now have an as_integer_ratio() method like that found in
float and decimal.Decimal.  This minor API extension
makes it possible to write numerator, denominator =
x.as_integer_ratio() and have it work across multiple numeric types.
(Contributed by Lisa Roach in bpo-33073 and Raymond Hettinger in
bpo-37819.)
Constructors of int, float and complex will now
use the __index__() special method, if available and the
corresponding method __int__(), __float__()
or __complex__() is not available.
(Contributed by Serhiy Storchaka in bpo-20092.)
Added support of \N{name} escapes in regular expressions:
>>> notice = 'Copyright © 2019'
>>> copyright_year_pattern = re.compile(r'\N{copyright sign}\s*(\d{4})')
>>> int(copyright_year_pattern.search(notice).group(1))
2019


(Contributed by Jonathan Eunice and Serhiy Storchaka in bpo-30688.)

Dict and dictviews are now iterable in reversed insertion order using
reversed(). (Contributed by Rémi Lapeyre in bpo-33462.)
The syntax allowed for keyword names in function calls was further
restricted. In particular, f((keyword)=arg) is no longer allowed. It was
never intended to permit more than a bare name on the left-hand side of a
keyword argument assignment term.
(Contributed by Benjamin Peterson in bpo-34641.)
Generalized iterable unpacking in yield and
return statements no longer requires enclosing parentheses.
This brings the yield and return syntax into better agreement with
normal assignment syntax:
>>> def parse(family):
        lastname, *members = family.split()
        return lastname.upper(), *members

>>> parse('simpsons homer marge bart lisa sally')
('SIMPSONS', 'homer', 'marge', 'bart', 'lisa', 'sally')


(Contributed by David Cuthbert and Jordan Chapman in bpo-32117.)

When a comma is missed in code such as [(10, 20) (30, 40)], the
compiler displays a SyntaxWarning with a helpful suggestion.
This improves on just having a TypeError indicating that the
first tuple was not callable.  (Contributed by Serhiy Storchaka in
bpo-15248.)
Arithmetic operations between subclasses of datetime.date or
datetime.datetime and datetime.timedelta objects now return
an instance of the subclass, rather than the base class. This also affects
the return type of operations whose implementation (directly or indirectly)
uses datetime.timedelta arithmetic, such as
astimezone().
(Contributed by Paul Ganssle in bpo-32417.)
When the Python interpreter is interrupted by Ctrl-C (SIGINT) and the
resulting KeyboardInterrupt exception is not caught, the Python process
now exits via a SIGINT signal or with the correct exit code such that the
calling process can detect that it died due to a Ctrl-C.  Shells on POSIX
and Windows use this to properly terminate scripts in interactive sessions.
(Contributed by Google via Gregory P. Smith in bpo-1054041.)
Some advanced styles of programming require updating the
types.CodeType object for an existing function.  Since code
objects are immutable, a new code object needs to be created, one
that is modeled on the existing code object.  With 19 parameters,
this was somewhat tedious.  Now, the new replace() method makes
it possible to create a clone with a few altered parameters.
Here’s an example that alters the statistics.mean() function to
prevent the data parameter from being used as a keyword argument:
>>> from statistics import mean
>>> mean(data=[10, 20, 90])
40
>>> mean.__code__ = mean.__code__.replace(co_posonlyargcount=1)
>>> mean(data=[10, 20, 90])
Traceback (most recent call last):
  ...
TypeError: mean() got some positional-only arguments passed as keyword arguments: 'data'


(Contributed by Victor Stinner in bpo-37032.)

For integers, the three-argument form of the pow() function now
permits the exponent to be negative in the case where the base is
relatively prime to the modulus. It then computes a modular inverse to
the base when the exponent is -1, and a suitable power of that
inverse for other negative exponents.  For example, to compute the
modular multiplicative inverse of 38
modulo 137, write:
>>> pow(38, -1, 137)
119
>>> 119 * 38 % 137
1


Modular inverses arise in the solution of linear Diophantine
equations.
For example, to find integer solutions for 4258𝑥 + 147𝑦 = 369,
first rewrite as 4258𝑥 ≡ 369 (mod 147) then solve:
>>> x = 369 * pow(4258, -1, 147) % 147
>>> y = (4258 * x - 369) // -147
>>> 4258 * x + 147 * y
369


(Contributed by Mark Dickinson in bpo-36027.)

Dict comprehensions have been synced-up with dict literals so that the
key is computed first and the value second:
>>> # Dict comprehension
>>> cast = {input('role? '): input('actor? ') for i in range(2)}
role? King Arthur
actor? Chapman
role? Black Knight
actor? Cleese

>>> # Dict literal
>>> cast = {input('role? '): input('actor? ')}
role? Sir Robin
actor? Eric Idle


The guaranteed execution order is helpful with assignment expressions
because variables assigned in the key expression will be available in
the value expression:
>>> names = ['Martin von Löwis', 'Łukasz Langa', 'Walter Dörwald']
>>> {(n := normalize('NFC', name)).casefold() : n for name in names}
{'martin von löwis': 'Martin von Löwis',
 'łukasz langa': 'Łukasz Langa',
 'walter dörwald': 'Walter Dörwald'}


(Contributed by Jörn Heissler in bpo-35224.)

The object.__reduce__() method can now return a tuple from two to
six elements long. Formerly, five was the limit.  The new, optional sixth
element is a callable with a (obj, state) signature.  This allows the
direct control over the state-updating behavior of a specific object.  If
not None, this callable will have priority over the object’s
__setstate__() method.
(Contributed by Pierre Glaser and Olivier Grisel in bpo-35900.)",other-language-changes,">>> notice = 'Copyright © 2019'
>>> copyright_year_pattern = re.compile(r'\N{copyright sign}\s*(\d{4})')
>>> int(copyright_year_pattern.search(notice).group(1))
2019
",3.8
,other-language-changes,">>> def parse(family):
        lastname, *members = family.split()
        return lastname.upper(), *members

>>> parse('simpsons homer marge bart lisa sally')
('SIMPSONS', 'homer', 'marge', 'bart', 'lisa', 'sally')
",3.8
,other-language-changes,">>> from statistics import mean
>>> mean(data=[10, 20, 90])
40
>>> mean.__code__ = mean.__code__.replace(co_posonlyargcount=1)
>>> mean(data=[10, 20, 90])
Traceback (most recent call last):
  ...
TypeError: mean() got some positional-only arguments passed as keyword arguments: 'data'
",3.8
,other-language-changes,">>> pow(38, -1, 137)
119
>>> 119 * 38 % 137
1
",3.8
,other-language-changes,">>> x = 369 * pow(4258, -1, 147) % 147
>>> y = (4258 * x - 369) // -147
>>> 4258 * x + 147 * y
369
",3.8
,other-language-changes,">>> # Dict comprehension
>>> cast = {input('role? '): input('actor? ') for i in range(2)}
role? King Arthur
actor? Chapman
role? Black Knight
actor? Cleese

>>> # Dict literal
>>> cast = {input('role? '): input('actor? ')}
role? Sir Robin
actor? Eric Idle
",3.8
,other-language-changes,">>> names = ['Martin von Löwis', 'Łukasz Langa', 'Walter Dörwald']
>>> {(n := normalize('NFC', name)).casefold() : n for name in names}
{'martin von löwis': 'Martin von Löwis',
 'łukasz langa': 'Łukasz Langa',
 'walter dörwald': 'Walter Dörwald'}
",3.8
"New Modules


The new importlib.metadata module provides (provisional) support for
reading metadata from third-party packages.  For example, it can extract an
installed package’s version number, list of entry points, and more:
>>> # Note following example requires that the popular ""requests""
>>> # package has been installed.
>>>
>>> from importlib.metadata import version, requires, files
>>> version('requests')
'2.22.0'
>>> list(requires('requests'))
['chardet (<3.1.0,>=3.0.2)']
>>> list(files('requests'))[:5]
[PackagePath('requests-2.22.0.dist-info/INSTALLER'),
 PackagePath('requests-2.22.0.dist-info/LICENSE'),
 PackagePath('requests-2.22.0.dist-info/METADATA'),
 PackagePath('requests-2.22.0.dist-info/RECORD'),
 PackagePath('requests-2.22.0.dist-info/WHEEL')]


(Contributed by Barry Warsaw and Jason R. Coombs in bpo-34632.)",new-modules,">>> # Note following example requires that the popular ""requests""
>>> # package has been installed.
>>>
>>> from importlib.metadata import version, requires, files
>>> version('requests')
'2.22.0'
>>> list(requires('requests'))
['chardet (<3.1.0,>=3.0.2)']
>>> list(files('requests'))[:5]
[PackagePath('requests-2.22.0.dist-info/INSTALLER'),
 PackagePath('requests-2.22.0.dist-info/LICENSE'),
 PackagePath('requests-2.22.0.dist-info/METADATA'),
 PackagePath('requests-2.22.0.dist-info/RECORD'),
 PackagePath('requests-2.22.0.dist-info/WHEEL')]
",3.8
"asyncio

asyncio.run() has graduated from the provisional to stable API. This
function can be used to execute a coroutine and return the result while
automatically managing the event loop. For example:
import asyncio

async def main():
    await asyncio.sleep(0)
    return 42

asyncio.run(main())


This is roughly equivalent to:
import asyncio

async def main():
    await asyncio.sleep(0)
    return 42

loop = asyncio.new_event_loop()
asyncio.set_event_loop(loop)
try:
    loop.run_until_complete(main())
finally:
    asyncio.set_event_loop(None)
    loop.close()


The actual implementation is significantly more complex. Thus,
asyncio.run() should be the preferred way of running asyncio programs.
(Contributed by Yury Selivanov in bpo-32314.)
Running python -m asyncio launches a natively async REPL.  This allows rapid
experimentation with code that has a top-level await.  There is no
longer a need to directly call asyncio.run() which would spawn a new event
loop on every invocation:
$ python -m asyncio
asyncio REPL 3.8.0
Use ""await"" directly instead of ""asyncio.run()"".
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import asyncio
>>> await asyncio.sleep(10, result='hello')
hello


(Contributed by Yury Selivanov in bpo-37028.)
The exception asyncio.CancelledError now inherits from
BaseException rather than Exception and no longer inherits
from concurrent.futures.CancelledError.
(Contributed by Yury Selivanov in bpo-32528.)
On Windows, the default event loop is now ProactorEventLoop.
(Contributed by Victor Stinner in bpo-34687.)
ProactorEventLoop now also supports UDP.
(Contributed by Adam Meily and Andrew Svetlov in bpo-29883.)
ProactorEventLoop can now be interrupted by
KeyboardInterrupt (“CTRL+C”).
(Contributed by Vladimir Matveev in bpo-23057.)
Added asyncio.Task.get_coro() for getting the wrapped coroutine
within an asyncio.Task.
(Contributed by Alex Grönholm in bpo-36999.)
Asyncio tasks can now be named, either by passing the name keyword
argument to asyncio.create_task() or
the create_task() event loop method, or by
calling the set_name() method on the task object. The
task name is visible in the repr() output of asyncio.Task and
can also be retrieved using the get_name() method.
(Contributed by Alex Grönholm in bpo-34270.)
Added support for
Happy Eyeballs to
asyncio.loop.create_connection(). To specify the behavior, two new
parameters have been added: happy_eyeballs_delay and interleave. The Happy
Eyeballs algorithm improves responsiveness in applications that support IPv4
and IPv6 by attempting to simultaneously connect using both.
(Contributed by twisteroid ambassador in bpo-33530.)",improved-modules - asyncio,"import asyncio

async def main():
    await asyncio.sleep(0)
    return 42

asyncio.run(main())
",3.8
,improved-modules - asyncio,"import asyncio

async def main():
    await asyncio.sleep(0)
    return 42

loop = asyncio.new_event_loop()
asyncio.set_event_loop(loop)
try:
    loop.run_until_complete(main())
finally:
    asyncio.set_event_loop(None)
    loop.close()
",3.8
,improved-modules - asyncio,"$ python -m asyncio
asyncio REPL 3.8.0
Use ""await"" directly instead of ""asyncio.run()"".
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import asyncio
>>> await asyncio.sleep(10, result='hello')
hello
",3.8
"cProfile

The cProfile.Profile class can now be used as a context manager.
Profile a block of code by running:
import cProfile

with cProfile.Profile() as profiler:
      # code to be profiled
      ...


(Contributed by Scott Sanderson in bpo-29235.)",improved-modules - cprofile,"import cProfile

with cProfile.Profile() as profiler:
      # code to be profiled
      ...
",3.8
"functools

functools.lru_cache() can now be used as a straight decorator rather
than as a function returning a decorator.  So both of these are now supported:
@lru_cache
def f(x):
    ...

@lru_cache(maxsize=256)
def f(x):
    ...


(Contributed by Raymond Hettinger in bpo-36772.)
Added a new functools.cached_property() decorator, for computed properties
cached for the life of the instance.
import functools
import statistics

class Dataset:
   def __init__(self, sequence_of_numbers):
      self.data = sequence_of_numbers

   @functools.cached_property
   def variance(self):
      return statistics.variance(self.data)


(Contributed by Carl Meyer in bpo-21145)
Added a new functools.singledispatchmethod() decorator that converts
methods into generic functions using
single dispatch:
from functools import singledispatchmethod
from contextlib import suppress

class TaskManager:

    def __init__(self, tasks):
        self.tasks = list(tasks)

    @singledispatchmethod
    def discard(self, value):
        with suppress(ValueError):
            self.tasks.remove(value)

    @discard.register(list)
    def _(self, tasks):
        targets = set(tasks)
        self.tasks = [x for x in self.tasks if x not in targets]


(Contributed by Ethan Smith in bpo-32380)",improved-modules - functools,"@lru_cache
def f(x):
    ...

@lru_cache(maxsize=256)
def f(x):
    ...
",3.8
,improved-modules - functools,"import functools
import statistics

class Dataset:
   def __init__(self, sequence_of_numbers):
      self.data = sequence_of_numbers

   @functools.cached_property
   def variance(self):
      return statistics.variance(self.data)
",3.8
,improved-modules - functools,"from functools import singledispatchmethod
from contextlib import suppress

class TaskManager:

    def __init__(self, tasks):
        self.tasks = list(tasks)

    @singledispatchmethod
    def discard(self, value):
        with suppress(ValueError):
            self.tasks.remove(value)

    @discard.register(list)
    def _(self, tasks):
        targets = set(tasks)
        self.tasks = [x for x in self.tasks if x not in targets]
",3.8
"inspect

The inspect.getdoc() function can now find docstrings for __slots__
if that attribute is a dict where the values are docstrings.
This provides documentation options similar to what we already have
for property(), classmethod(), and staticmethod():
class AudioClip:
    __slots__ = {'bit_rate': 'expressed in kilohertz to one decimal place',
                 'duration': 'in seconds, rounded up to an integer'}
    def __init__(self, bit_rate, duration):
        self.bit_rate = round(bit_rate / 1000.0, 1)
        self.duration = ceil(duration)


(Contributed by Raymond Hettinger in bpo-36326.)",improved-modules - inspect,"class AudioClip:
    __slots__ = {'bit_rate': 'expressed in kilohertz to one decimal place',
                 'duration': 'in seconds, rounded up to an integer'}
    def __init__(self, bit_rate, duration):
        self.bit_rate = round(bit_rate / 1000.0, 1)
        self.duration = ceil(duration)
",3.8
"itertools

The itertools.accumulate() function added an option initial keyword
argument to specify an initial value:
>>> from itertools import accumulate
>>> list(accumulate([10, 5, 30, 15], initial=1000))
[1000, 1010, 1015, 1045, 1060]


(Contributed by Lisa Roach in bpo-34659.)",improved-modules - itertools,">>> from itertools import accumulate
>>> list(accumulate([10, 5, 30, 15], initial=1000))
[1000, 1010, 1015, 1045, 1060]
",3.8
"math

Added new function math.dist() for computing Euclidean distance
between two points.  (Contributed by Raymond Hettinger in bpo-33089.)
Expanded the math.hypot() function to handle multiple dimensions.
Formerly, it only supported the 2-D case.
(Contributed by Raymond Hettinger in bpo-33089.)
Added new function, math.prod(), as analogous function to sum()
that returns the product of a ‘start’ value (default: 1) times an iterable of
numbers:
>>> prior = 0.8
>>> likelihoods = [0.625, 0.84, 0.30]
>>> math.prod(likelihoods, start=prior)
0.126


(Contributed by Pablo Galindo in bpo-35606.)
Added two new combinatoric functions math.perm() and math.comb():
>>> math.perm(10, 3)    # Permutations of 10 things taken 3 at a time
720
>>> math.comb(10, 3)    # Combinations of 10 things taken 3 at a time
120


(Contributed by Yash Aggarwal, Keller Fuchs, Serhiy Storchaka, and Raymond
Hettinger in bpo-37128, bpo-37178, and bpo-35431.)
Added a new function math.isqrt() for computing accurate integer square
roots without conversion to floating point.  The new function supports
arbitrarily large integers.  It is faster than floor(sqrt(n)) but slower
than math.sqrt():
>>> r = 650320427
>>> s = r ** 2
>>> isqrt(s - 1)         # correct
650320426
>>> floor(sqrt(s - 1))   # incorrect
650320427


(Contributed by Mark Dickinson in bpo-36887.)
The function math.factorial() no longer accepts arguments that are not
int-like. (Contributed by Pablo Galindo in bpo-33083.)",improved-modules - math,">>> prior = 0.8
>>> likelihoods = [0.625, 0.84, 0.30]
>>> math.prod(likelihoods, start=prior)
0.126
",3.8
,improved-modules - math,">>> math.perm(10, 3)    # Permutations of 10 things taken 3 at a time
720
>>> math.comb(10, 3)    # Combinations of 10 things taken 3 at a time
120
",3.8
,improved-modules - math,">>> r = 650320427
>>> s = r ** 2
>>> isqrt(s - 1)         # correct
650320426
>>> floor(sqrt(s - 1))   # incorrect
650320427
",3.8
"pprint

The pprint module added a sort_dicts parameter to several functions.
By default, those functions continue to sort dictionaries before rendering or
printing.  However, if sort_dicts is set to false, the dictionaries retain
the order that keys were inserted.  This can be useful for comparison to JSON
inputs during debugging.
In addition, there is a convenience new function, pprint.pp() that is
like pprint.pprint() but with sort_dicts defaulting to False:
>>> from pprint import pprint, pp
>>> d = dict(source='input.txt', operation='filter', destination='output.txt')
>>> pp(d, width=40)                  # Original order
{'source': 'input.txt',
 'operation': 'filter',
 'destination': 'output.txt'}
>>> pprint(d, width=40)              # Keys sorted alphabetically
{'destination': 'output.txt',
 'operation': 'filter',
 'source': 'input.txt'}


(Contributed by Rémi Lapeyre in bpo-30670.)",improved-modules - pprint,">>> from pprint import pprint, pp
>>> d = dict(source='input.txt', operation='filter', destination='output.txt')
>>> pp(d, width=40)                  # Original order
{'source': 'input.txt',
 'operation': 'filter',
 'destination': 'output.txt'}
>>> pprint(d, width=40)              # Keys sorted alphabetically
{'destination': 'output.txt',
 'operation': 'filter',
 'source': 'input.txt'}
",3.8
"statistics

Added statistics.fmean() as a faster, floating point variant of
statistics.mean().  (Contributed by Raymond Hettinger and
Steven D’Aprano in bpo-35904.)
Added statistics.geometric_mean()
(Contributed by Raymond Hettinger in bpo-27181.)
Added statistics.multimode() that returns a list of the most
common values. (Contributed by Raymond Hettinger in bpo-35892.)
Added statistics.quantiles() that divides data or a distribution
in to equiprobable intervals (e.g. quartiles, deciles, or percentiles).
(Contributed by Raymond Hettinger in bpo-36546.)
Added statistics.NormalDist, a tool for creating
and manipulating normal distributions of a random variable.
(Contributed by Raymond Hettinger in bpo-36018.)
>>> temperature_feb = NormalDist.from_samples([4, 12, -3, 2, 7, 14])
>>> temperature_feb.mean
6.0
>>> temperature_feb.stdev
6.356099432828281

>>> temperature_feb.cdf(3)            # Chance of being under 3 degrees
0.3184678262814532
>>> # Relative chance of being 7 degrees versus 10 degrees
>>> temperature_feb.pdf(7) / temperature_feb.pdf(10)
1.2039930378537762

>>> el_niño = NormalDist(4, 2.5)
>>> temperature_feb += el_niño        # Add in a climate effect
>>> temperature_feb
NormalDist(mu=10.0, sigma=6.830080526611674)

>>> temperature_feb * (9/5) + 32      # Convert to Fahrenheit
NormalDist(mu=50.0, sigma=12.294144947901014)
>>> temperature_feb.samples(3)        # Generate random samples
[7.672102882379219, 12.000027119750287, 4.647488369766392]",improved-modules - statistics,">>> temperature_feb = NormalDist.from_samples([4, 12, -3, 2, 7, 14])
>>> temperature_feb.mean
6.0
>>> temperature_feb.stdev
6.356099432828281

>>> temperature_feb.cdf(3)            # Chance of being under 3 degrees
0.3184678262814532
>>> # Relative chance of being 7 degrees versus 10 degrees
>>> temperature_feb.pdf(7) / temperature_feb.pdf(10)
1.2039930378537762

>>> el_niño = NormalDist(4, 2.5)
>>> temperature_feb += el_niño        # Add in a climate effect
>>> temperature_feb
NormalDist(mu=10.0, sigma=6.830080526611674)

>>> temperature_feb * (9/5) + 32      # Convert to Fahrenheit
NormalDist(mu=50.0, sigma=12.294144947901014)
>>> temperature_feb.samples(3)        # Generate random samples
[7.672102882379219, 12.000027119750287, 4.647488369766392]
",3.8
"typing

The typing module incorporates several new features:

A dictionary type with per-key types.  See PEP 589 and
typing.TypedDict.
TypedDict uses only string keys.  By default, every key is required
to be present. Specify “total=False” to allow keys to be optional:
class Location(TypedDict, total=False):
    lat_long: tuple
    grid_square: str
    xy_coordinate: tuple



Literal types.  See PEP 586 and typing.Literal.
Literal types indicate that a parameter or return value
is constrained to one or more specific literal values:
def get_status(port: int) -> Literal['connected', 'disconnected']:
    ...



“Final” variables, functions, methods and classes.  See PEP 591,
typing.Final and typing.final().
The final qualifier instructs a static type checker to restrict
subclassing, overriding, or reassignment:
pi: Final[float] = 3.1415926536



Protocol definitions.  See PEP 544, typing.Protocol and
typing.runtime_checkable().  Simple ABCs like
typing.SupportsInt are now Protocol subclasses.
New protocol class typing.SupportsIndex.
New functions typing.get_origin() and typing.get_args().",improved-modules - typing,"class Location(TypedDict, total=False):
    lat_long: tuple
    grid_square: str
    xy_coordinate: tuple
",3.8
,improved-modules - typing,"def get_status(port: int) -> Literal['connected', 'disconnected']:
    ...
",3.8
,improved-modules - typing,"pi: Final[float] = 3.1415926536
",3.8
"unittest

Added AsyncMock to support an asynchronous version of
Mock.  Appropriate new assert functions for testing
have been added as well.
(Contributed by Lisa Roach in bpo-26467).
Added addModuleCleanup() and
addClassCleanup() to unittest to support
cleanups for setUpModule() and
setUpClass().
(Contributed by Lisa Roach in bpo-24412.)
Several mock assert functions now also print a list of actual calls upon
failure. (Contributed by Petter Strandmark in bpo-35047.)
unittest module gained support for coroutines to be used as test cases
with unittest.IsolatedAsyncioTestCase.
(Contributed by Andrew Svetlov in bpo-32972.)
Example:
import unittest


class TestRequest(unittest.IsolatedAsyncioTestCase):

    async def asyncSetUp(self):
        self.connection = await AsyncConnection()

    async def test_get(self):
        response = await self.connection.get(""https://example.com"")
        self.assertEqual(response.status_code, 200)

    async def asyncTearDown(self):
        await self.connection.close()


if __name__ == ""__main__"":
    unittest.main()",improved-modules - unittest,"import unittest


class TestRequest(unittest.IsolatedAsyncioTestCase):

    async def asyncSetUp(self):
        self.connection = await AsyncConnection()

    async def test_get(self):
        response = await self.connection.get(""https://example.com"")
        self.assertEqual(response.status_code, 200)

    async def asyncTearDown(self):
        await self.connection.close()


if __name__ == ""__main__"":
    unittest.main()
",3.8
"Changes in the C API


The PyCompilerFlags structure got a new cf_feature_version
field. It should be initialized to PY_MINOR_VERSION. The field is ignored
by default, and is used if and only if PyCF_ONLY_AST flag is set in
cf_flags.
(Contributed by Guido van Rossum in bpo-35766.)
The PyEval_ReInitThreads() function has been removed from the C API.
It should not be called explicitly: use PyOS_AfterFork_Child()
instead.
(Contributed by Victor Stinner in bpo-36728.)
On Unix, C extensions are no longer linked to libpython except on Android
and Cygwin. When Python is embedded, libpython must not be loaded with
RTLD_LOCAL, but RTLD_GLOBAL instead. Previously, using
RTLD_LOCAL, it was already not possible to load C extensions which
were not linked to libpython, like C extensions of the standard
library built by the *shared* section of Modules/Setup.
(Contributed by Victor Stinner in bpo-21536.)
Use of # variants of formats in parsing or building value (e.g.
PyArg_ParseTuple(), Py_BuildValue(), PyObject_CallFunction(),
etc.) without PY_SSIZE_T_CLEAN defined raises DeprecationWarning now.
It will be removed in 3.10 or 4.0.  Read Parsing arguments and building values for detail.
(Contributed by Inada Naoki in bpo-36381.)
Instances of heap-allocated types (such as those created with
PyType_FromSpec()) hold a reference to their type object.
Increasing the reference count of these type objects has been moved from
PyType_GenericAlloc() to the more low-level functions,
PyObject_Init() and PyObject_INIT().
This makes types created through PyType_FromSpec() behave like
other classes in managed code.
Statically allocated types are not affected.
For the vast majority of cases, there should be no side effect.
However, types that manually increase the reference count after allocating
an instance (perhaps to work around the bug) may now become immortal.
To avoid this, these classes need to call Py_DECREF on the type object
during instance deallocation.
To correctly port these types into 3.8, please apply the following
changes:

Remove Py_INCREF on the type object after allocating an
instance - if any.
This may happen after calling PyObject_New(),
PyObject_NewVar(), PyObject_GC_New(),
PyObject_GC_NewVar(), or any other custom allocator that uses
PyObject_Init() or PyObject_INIT().
Example:
static foo_struct *
foo_new(PyObject *type) {
    foo_struct *foo = PyObject_GC_New(foo_struct, (PyTypeObject *) type);
    if (foo == NULL)
        return NULL;
#if PY_VERSION_HEX < 0x03080000
    // Workaround for Python issue 35810; no longer necessary in Python 3.8
    PY_INCREF(type)
#endif
    return foo;
}



Ensure that all custom tp_dealloc functions of heap-allocated types
decrease the type’s reference count.
Example:
static void
foo_dealloc(foo_struct *instance) {
    PyObject *type = Py_TYPE(instance);
    PyObject_GC_Del(instance);
#if PY_VERSION_HEX >= 0x03080000
    // This was not needed before Python 3.8 (Python issue 35810)
    Py_DECREF(type);
#endif
}




(Contributed by Eddie Elizondo in bpo-35810.)

The Py_DEPRECATED() macro has been implemented for MSVC.
The macro now must be placed before the symbol name.
Example:
Py_DEPRECATED(3.8) PyAPI_FUNC(int) Py_OldFunction(void);


(Contributed by Zackery Spytz in bpo-33407.)

The interpreter does not pretend to support binary compatibility of
extension types across feature releases, anymore.  A PyTypeObject
exported by a third-party extension module is supposed to have all the
slots expected in the current Python version, including
tp_finalize (Py_TPFLAGS_HAVE_FINALIZE
is not checked anymore before reading tp_finalize).
(Contributed by Antoine Pitrou in bpo-32388.)

The functions PyNode_AddChild() and PyParser_AddToken() now accept
two additional int arguments end_lineno and end_col_offset.
The libpython38.a file to allow MinGW tools to link directly against
python38.dll is no longer included in the regular Windows distribution.
If you require this file, it may be generated with the gendef and
dlltool tools, which are part of the MinGW binutils package:
gendef - python38.dll > tmp.def
dlltool --dllname python38.dll --def tmp.def --output-lib libpython38.a


The location of an installed pythonXY.dll will depend on the
installation options and the version and language of Windows. See
Using Python on Windows for more information. The resulting library should be
placed in the same directory as pythonXY.lib, which is generally the
libs directory under your Python installation.
(Contributed by Steve Dower in bpo-37351.)",porting-to-python-3-8 - changes-in-the-c-api,"static foo_struct *
foo_new(PyObject *type) {
    foo_struct *foo = PyObject_GC_New(foo_struct, (PyTypeObject *) type);
    if (foo == NULL)
        return NULL;
#if PY_VERSION_HEX < 0x03080000
    // Workaround for Python issue 35810; no longer necessary in Python 3.8
    PY_INCREF(type)
#endif
    return foo;
}
",3.8
,porting-to-python-3-8 - changes-in-the-c-api,"static void
foo_dealloc(foo_struct *instance) {
    PyObject *type = Py_TYPE(instance);
    PyObject_GC_Del(instance);
#if PY_VERSION_HEX >= 0x03080000
    // This was not needed before Python 3.8 (Python issue 35810)
    Py_DECREF(type);
#endif
}
",3.8
,porting-to-python-3-8 - changes-in-the-c-api,"Py_DEPRECATED(3.8) PyAPI_FUNC(int) Py_OldFunction(void);
",3.8
,porting-to-python-3-8 - changes-in-the-c-api,"gendef - python38.dll > tmp.def
dlltool --dllname python38.dll --def tmp.def --output-lib libpython38.a
",3.8
"Demos and Tools

Added a benchmark script for timing various ways to access variables:
Tools/scripts/var_access_benchmark.py.
(Contributed by Raymond Hettinger in bpo-35884.)
Here’s a summary of performance improvements since Python 3.3:
Python version                       3.3     3.4     3.5     3.6     3.7     3.8
--------------                       ---     ---     ---     ---     ---     ---

Variable and attribute read access:
    read_local                       4.0     7.1     7.1     5.4     5.1     3.9
    read_nonlocal                    5.3     7.1     8.1     5.8     5.4     4.4
    read_global                     13.3    15.5    19.0    14.3    13.6     7.6
    read_builtin                    20.0    21.1    21.6    18.5    19.0     7.5
    read_classvar_from_class        20.5    25.6    26.5    20.7    19.5    18.4
    read_classvar_from_instance     18.5    22.8    23.5    18.8    17.1    16.4
    read_instancevar                26.8    32.4    33.1    28.0    26.3    25.4
    read_instancevar_slots          23.7    27.8    31.3    20.8    20.8    20.2
    read_namedtuple                 68.5    73.8    57.5    45.0    46.8    18.4
    read_boundmethod                29.8    37.6    37.9    29.6    26.9    27.7

Variable and attribute write access:
    write_local                      4.6     8.7     9.3     5.5     5.3     4.3
    write_nonlocal                   7.3    10.5    11.1     5.6     5.5     4.7
    write_global                    15.9    19.7    21.2    18.0    18.0    15.8
    write_classvar                  81.9    92.9    96.0   104.6   102.1    39.2
    write_instancevar               36.4    44.6    45.8    40.0    38.9    35.5
    write_instancevar_slots         28.7    35.6    36.1    27.3    26.6    25.7

Data structure read access:
    read_list                       19.2    24.2    24.5    20.8    20.8    19.0
    read_deque                      19.9    24.7    25.5    20.2    20.6    19.8
    read_dict                       19.7    24.3    25.7    22.3    23.0    21.0
    read_strdict                    17.9    22.6    24.3    19.5    21.2    18.9

Data structure write access:
    write_list                      21.2    27.1    28.5    22.5    21.6    20.0
    write_deque                     23.8    28.7    30.1    22.7    21.8    23.5
    write_dict                      25.9    31.4    33.3    29.3    29.2    24.7
    write_strdict                   22.9    28.4    29.9    27.5    25.2    23.1

Stack (or queue) operations:
    list_append_pop                144.2    93.4   112.7    75.4    74.2    50.8
    deque_append_pop                30.4    43.5    57.0    49.4    49.2    42.5
    deque_append_popleft            30.8    43.7    57.3    49.7    49.7    42.8

Timing loop:
    loop_overhead                    0.3     0.5     0.6     0.4     0.3     0.3


The benchmarks were measured on an
Intel® Core™ i7-4960HQ processor
running the macOS 64-bit builds found at
python.org.
The benchmark script displays timings in nanoseconds.",porting-to-python-3-8 - demos-and-tools,"Python version                       3.3     3.4     3.5     3.6     3.7     3.8
--------------                       ---     ---     ---     ---     ---     ---

Variable and attribute read access:
    read_local                       4.0     7.1     7.1     5.4     5.1     3.9
    read_nonlocal                    5.3     7.1     8.1     5.8     5.4     4.4
    read_global                     13.3    15.5    19.0    14.3    13.6     7.6
    read_builtin                    20.0    21.1    21.6    18.5    19.0     7.5
    read_classvar_from_class        20.5    25.6    26.5    20.7    19.5    18.4
    read_classvar_from_instance     18.5    22.8    23.5    18.8    17.1    16.4
    read_instancevar                26.8    32.4    33.1    28.0    26.3    25.4
    read_instancevar_slots          23.7    27.8    31.3    20.8    20.8    20.2
    read_namedtuple                 68.5    73.8    57.5    45.0    46.8    18.4
    read_boundmethod                29.8    37.6    37.9    29.6    26.9    27.7

Variable and attribute write access:
    write_local                      4.6     8.7     9.3     5.5     5.3     4.3
    write_nonlocal                   7.3    10.5    11.1     5.6     5.5     4.7
    write_global                    15.9    19.7    21.2    18.0    18.0    15.8
    write_classvar                  81.9    92.9    96.0   104.6   102.1    39.2
    write_instancevar               36.4    44.6    45.8    40.0    38.9    35.5
    write_instancevar_slots         28.7    35.6    36.1    27.3    26.6    25.7

Data structure read access:
    read_list                       19.2    24.2    24.5    20.8    20.8    19.0
    read_deque                      19.9    24.7    25.5    20.2    20.6    19.8
    read_dict                       19.7    24.3    25.7    22.3    23.0    21.0
    read_strdict                    17.9    22.6    24.3    19.5    21.2    18.9

Data structure write access:
    write_list                      21.2    27.1    28.5    22.5    21.6    20.0
    write_deque                     23.8    28.7    30.1    22.7    21.8    23.5
    write_dict                      25.9    31.4    33.3    29.3    29.2    24.7
    write_strdict                   22.9    28.4    29.9    27.5    25.2    23.1

Stack (or queue) operations:
    list_append_pop                144.2    93.4   112.7    75.4    74.2    50.8
    deque_append_pop                30.4    43.5    57.0    49.4    49.2    42.5
    deque_append_popleft            30.8    43.7    57.3    49.7    49.7    42.8

Timing loop:
    loop_overhead                    0.3     0.5     0.6     0.4     0.3     0.3
",3.8
"Dictionary Merge & Update Operators

Merge (|) and update (|=) operators have been added to the built-in
dict class. Those complement the existing dict.update and
{**d1, **d2} methods of merging dictionaries.
Example:
>>> x = {""key1"": ""value1 from x"", ""key2"": ""value2 from x""}
>>> y = {""key2"": ""value2 from y"", ""key3"": ""value3 from y""}
>>> x | y
{'key1': 'value1 from x', 'key2': 'value2 from y', 'key3': 'value3 from y'}
>>> y | x
{'key2': 'value2 from x', 'key3': 'value3 from y', 'key1': 'value1 from x'}


See PEP 584 for a full description.
(Contributed by Brandt Bucher in bpo-36144.)",new-features - dictionary-merge-update-operators,">>> x = {""key1"": ""value1 from x"", ""key2"": ""value2 from x""}
>>> y = {""key2"": ""value2 from y"", ""key3"": ""value3 from y""}
>>> x | y
{'key1': 'value1 from x', 'key2': 'value2 from y', 'key3': 'value3 from y'}
>>> y | x
{'key2': 'value2 from x', 'key3': 'value3 from y', 'key1': 'value1 from x'}
",3.9
"Type Hinting Generics in Standard Collections

In type annotations you can now use built-in collection types such as
list and dict as generic types instead of importing the
corresponding capitalized types (e.g. List or Dict) from
typing.  Some other types in the standard library are also now generic,
for example queue.Queue.
Example:
def greet_all(names: list[str]) -> None:
    for name in names:
        print(""Hello"", name)


See PEP 585 for more details.  (Contributed by Guido van Rossum,
Ethan Smith, and Batuhan Taşkaya in bpo-39481.)",new-features - type-hinting-generics-in-standard-collections,"def greet_all(names: list[str]) -> None:
    for name in names:
        print(""Hello"", name)
",3.9
"zoneinfo

The zoneinfo module brings support for the IANA time zone database to
the standard library. It adds zoneinfo.ZoneInfo, a concrete
datetime.tzinfo implementation backed by the system’s time zone data.
Example:
>>> from zoneinfo import ZoneInfo
>>> from datetime import datetime, timedelta

>>> # Daylight saving time
>>> dt = datetime(2020, 10, 31, 12, tzinfo=ZoneInfo(""America/Los_Angeles""))
>>> print(dt)
2020-10-31 12:00:00-07:00
>>> dt.tzname()
'PDT'

>>> # Standard time
>>> dt += timedelta(days=7)
>>> print(dt)
2020-11-07 12:00:00-08:00
>>> print(dt.tzname())
PST


As a fall-back source of data for platforms that don’t ship the IANA database,
the tzdata module was released as a first-party package – distributed via
PyPI and maintained by the CPython core team.

See also

PEP 615 – Support for the IANA Time Zone Database in the Standard LibraryPEP written and implemented by Paul Ganssle",new-modules - zoneinfo,">>> from zoneinfo import ZoneInfo
>>> from datetime import datetime, timedelta

>>> # Daylight saving time
>>> dt = datetime(2020, 10, 31, 12, tzinfo=ZoneInfo(""America/Los_Angeles""))
>>> print(dt)
2020-10-31 12:00:00-07:00
>>> dt.tzname()
'PDT'

>>> # Standard time
>>> dt += timedelta(days=7)
>>> print(dt)
2020-11-07 12:00:00-08:00
>>> print(dt.tzname())
PST
",3.9
"Optimizations


Optimized the idiom for assignment a temporary variable in comprehensions.
Now for y in [expr] in comprehensions is as fast as a simple assignment
y = expr.  For example:

sums = [s for s in [0] for x in data for s in [s + x]]

Unlike the := operator this idiom does not leak a variable to the
outer scope.
(Contributed by Serhiy Storchaka in bpo-32856.)

Optimized signal handling in multithreaded applications. If a thread different
than the main thread gets a signal, the bytecode evaluation loop is no longer
interrupted at each bytecode instruction to check for pending signals which
cannot be handled. Only the main thread of the main interpreter can handle
signals.
Previously, the bytecode evaluation loop was interrupted at each instruction
until the main thread handles signals.
(Contributed by Victor Stinner in bpo-40010.)

Optimized the subprocess module on FreeBSD using closefrom().
(Contributed by Ed Maste, Conrad Meyer, Kyle Evans, Kubilay Kocak and Victor
Stinner in bpo-38061.)
PyLong_FromDouble() is now up to 1.87x faster for values that
fit into long.
(Contributed by Sergey Fedoseev in bpo-37986.)
A number of Python builtins (range, tuple, set,
frozenset, list, dict) are now sped up by using
PEP 590 vectorcall protocol.
(Contributed by Dong-hee Na, Mark Shannon, Jeroen Demeyer and Petr Viktorin in bpo-37207.)
Optimized difference_update() for the case when the other set
is much larger than the base set.
(Suggested by Evgeny Kapun with code contributed by Michele Orrù in bpo-8425.)
Python’s small object allocator (obmalloc.c) now allows (no more than)
one empty arena to remain available for immediate reuse, without returning
it to the OS.  This prevents thrashing in simple loops where an arena could
be created and destroyed anew on each iteration.
(Contributed by Tim Peters in bpo-37257.)
floor division of float operation now has a better performance. Also
the message of ZeroDivisionError for this operation is updated.
(Contributed by Dong-hee Na in bpo-39434.)
Decoding short ASCII strings with UTF-8 and ascii codecs is now about
15% faster.  (Contributed by Inada Naoki in bpo-37348.)

Here’s a summary of performance improvements from Python 3.4 through Python 3.9:
Python version                       3.4     3.5     3.6     3.7     3.8    3.9
--------------                       ---     ---     ---     ---     ---    ---

Variable and attribute read access:
    read_local                       7.1     7.1     5.4     5.1     3.9    3.9
    read_nonlocal                    7.1     8.1     5.8     5.4     4.4    4.5
    read_global                     15.5    19.0    14.3    13.6     7.6    7.8
    read_builtin                    21.1    21.6    18.5    19.0     7.5    7.8
    read_classvar_from_class        25.6    26.5    20.7    19.5    18.4   17.9
    read_classvar_from_instance     22.8    23.5    18.8    17.1    16.4   16.9
    read_instancevar                32.4    33.1    28.0    26.3    25.4   25.3
    read_instancevar_slots          27.8    31.3    20.8    20.8    20.2   20.5
    read_namedtuple                 73.8    57.5    45.0    46.8    18.4   18.7
    read_boundmethod                37.6    37.9    29.6    26.9    27.7   41.1

Variable and attribute write access:
    write_local                      8.7     9.3     5.5     5.3     4.3    4.3
    write_nonlocal                  10.5    11.1     5.6     5.5     4.7    4.8
    write_global                    19.7    21.2    18.0    18.0    15.8   16.7
    write_classvar                  92.9    96.0   104.6   102.1    39.2   39.8
    write_instancevar               44.6    45.8    40.0    38.9    35.5   37.4
    write_instancevar_slots         35.6    36.1    27.3    26.6    25.7   25.8

Data structure read access:
    read_list                       24.2    24.5    20.8    20.8    19.0   19.5
    read_deque                      24.7    25.5    20.2    20.6    19.8   20.2
    read_dict                       24.3    25.7    22.3    23.0    21.0   22.4
    read_strdict                    22.6    24.3    19.5    21.2    18.9   21.5

Data structure write access:
    write_list                      27.1    28.5    22.5    21.6    20.0   20.0
    write_deque                     28.7    30.1    22.7    21.8    23.5   21.7
    write_dict                      31.4    33.3    29.3    29.2    24.7   25.4
    write_strdict                   28.4    29.9    27.5    25.2    23.1   24.5

Stack (or queue) operations:
    list_append_pop                 93.4   112.7    75.4    74.2    50.8   50.6
    deque_append_pop                43.5    57.0    49.4    49.2    42.5   44.2
    deque_append_popleft            43.7    57.3    49.7    49.7    42.8   46.4

Timing loop:
    loop_overhead                    0.5     0.6     0.4     0.3     0.3    0.3


These results were generated from the variable access benchmark script at:
Tools/scripts/var_access_benchmark.py. The benchmark script displays timings
in nanoseconds.  The benchmarks were measured on an
Intel® Core™ i7-4960HQ processor
running the macOS 64-bit builds found at
python.org.",optimizations,"Python version                       3.4     3.5     3.6     3.7     3.8    3.9
--------------                       ---     ---     ---     ---     ---    ---

Variable and attribute read access:
    read_local                       7.1     7.1     5.4     5.1     3.9    3.9
    read_nonlocal                    7.1     8.1     5.8     5.4     4.4    4.5
    read_global                     15.5    19.0    14.3    13.6     7.6    7.8
    read_builtin                    21.1    21.6    18.5    19.0     7.5    7.8
    read_classvar_from_class        25.6    26.5    20.7    19.5    18.4   17.9
    read_classvar_from_instance     22.8    23.5    18.8    17.1    16.4   16.9
    read_instancevar                32.4    33.1    28.0    26.3    25.4   25.3
    read_instancevar_slots          27.8    31.3    20.8    20.8    20.2   20.5
    read_namedtuple                 73.8    57.5    45.0    46.8    18.4   18.7
    read_boundmethod                37.6    37.9    29.6    26.9    27.7   41.1

Variable and attribute write access:
    write_local                      8.7     9.3     5.5     5.3     4.3    4.3
    write_nonlocal                  10.5    11.1     5.6     5.5     4.7    4.8
    write_global                    19.7    21.2    18.0    18.0    15.8   16.7
    write_classvar                  92.9    96.0   104.6   102.1    39.2   39.8
    write_instancevar               44.6    45.8    40.0    38.9    35.5   37.4
    write_instancevar_slots         35.6    36.1    27.3    26.6    25.7   25.8

Data structure read access:
    read_list                       24.2    24.5    20.8    20.8    19.0   19.5
    read_deque                      24.7    25.5    20.2    20.6    19.8   20.2
    read_dict                       24.3    25.7    22.3    23.0    21.0   22.4
    read_strdict                    22.6    24.3    19.5    21.2    18.9   21.5

Data structure write access:
    write_list                      27.1    28.5    22.5    21.6    20.0   20.0
    write_deque                     28.7    30.1    22.7    21.8    23.5   21.7
    write_dict                      31.4    33.3    29.3    29.2    24.7   25.4
    write_strdict                   28.4    29.9    27.5    25.2    23.1   24.5

Stack (or queue) operations:
    list_append_pop                 93.4   112.7    75.4    74.2    50.8   50.6
    deque_append_pop                43.5    57.0    49.4    49.2    42.5   44.2
    deque_append_popleft            43.7    57.3    49.7    49.7    42.8   46.4

Timing loop:
    loop_overhead                    0.5     0.6     0.4     0.3     0.3    0.3
",3.9
"Changes in the C API


Instances of heap-allocated types (such as those created with
PyType_FromSpec() and similar APIs) hold a reference to their type
object since Python 3.8. As indicated in the “Changes in the C API” of Python
3.8, for the vast majority of cases, there should be no side effect but for
types that have a custom tp_traverse function,
ensure that all custom tp_traverse functions of heap-allocated types
visit the object’s type.

Example:
int
foo_traverse(foo_struct *self, visitproc visit, void *arg) {
// Rest of the traverse function
#if PY_VERSION_HEX >= 0x03090000
    // This was not needed before Python 3.9 (Python issue 35810 and 40217)
    Py_VISIT(Py_TYPE(self));
#endif
}



If your traverse function delegates to tp_traverse of its base class
(or another type), ensure that Py_TYPE(self) is visited only once.
Note that only heap types are expected to visit the type in tp_traverse.

For example, if your tp_traverse function includes:
base->tp_traverse(self, visit, arg)


then add:
#if PY_VERSION_HEX >= 0x03090000
    // This was not needed before Python 3.9 (Python issue 35810 and 40217)
    if (base->tp_flags & Py_TPFLAGS_HEAPTYPE) {
        // a heap type's tp_traverse already visited Py_TYPE(self)
    } else {
        Py_VISIT(Py_TYPE(self));
    }
#else



(See bpo-35810 and bpo-40217 for more information.)

The functions PyEval_CallObject, PyEval_CallFunction,
PyEval_CallMethod and PyEval_CallObjectWithKeywords are deprecated.
Use PyObject_Call() and its variants instead.
(See more details in bpo-29548.)",porting-to-python-3-9 - changes-in-the-c-api,"int
foo_traverse(foo_struct *self, visitproc visit, void *arg) {
// Rest of the traverse function
#if PY_VERSION_HEX >= 0x03090000
    // This was not needed before Python 3.9 (Python issue 35810 and 40217)
    Py_VISIT(Py_TYPE(self));
#endif
}
",3.9
,porting-to-python-3-9 - changes-in-the-c-api,"base->tp_traverse(self, visit, arg)
",3.9
,porting-to-python-3-9 - changes-in-the-c-api,"#if PY_VERSION_HEX >= 0x03090000
    // This was not needed before Python 3.9 (Python issue 35810 and 40217)
    if (base->tp_flags & Py_TPFLAGS_HEAPTYPE) {
        // a heap type's tp_traverse already visited Py_TYPE(self)
    } else {
        Py_VISIT(Py_TYPE(self));
    }
#else
",3.9
"typing

The behavior of typing.Literal was changed to conform with PEP 586
and to match the behavior of static type checkers specified in the PEP.

Literal now de-duplicates parameters.
Equality comparisons between Literal objects are now order independent.
Literal comparisons now respect types.  For example,
Literal[0] == Literal[False] previously evaluated to True.  It is
now False.  To support this change, the internally used type cache now
supports differentiating types.
Literal objects will now raise a TypeError exception during
equality comparisons if any of their parameters are not hashable.
Note that declaring Literal with mutable parameters will not throw
an error:
>>> from typing import Literal
>>> Literal[{0}]
>>> Literal[{0}] == Literal[{False}]
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: unhashable type: 'set'




(Contributed by Yurii Karabas in bpo-42345.)",notable-changes-in-python-3-9-1 - id4,">>> from typing import Literal
>>> Literal[{0}]
>>> Literal[{0}] == Literal[{False}]
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: unhashable type: 'set'
",3.9
"Parenthesized context managers

Using enclosing parentheses for continuation across multiple lines
in context managers is now supported. This allows formatting a long
collection of context managers in multiple lines in a similar way
as it was previously possible with import statements. For instance,
all these examples are now valid:
with (CtxManager() as example):
    ...

with (
    CtxManager1(),
    CtxManager2()
):
    ...

with (CtxManager1() as example,
      CtxManager2()):
    ...

with (CtxManager1(),
      CtxManager2() as example):
    ...

with (
    CtxManager1() as example1,
    CtxManager2() as example2
):
    ...


it is also possible to use a trailing comma at the end of the
enclosed group:
with (
    CtxManager1() as example1,
    CtxManager2() as example2,
    CtxManager3() as example3,
):
    ...


This new syntax uses the non LL(1) capacities of the new parser.
Check PEP 617 for more details.
(Contributed by Guido van Rossum, Pablo Galindo and Lysandros Nikolaou
in bpo-12782 and bpo-40334.)",new-features - parenthesized-context-managers,"with (CtxManager() as example):
    ...

with (
    CtxManager1(),
    CtxManager2()
):
    ...

with (CtxManager1() as example,
      CtxManager2()):
    ...

with (CtxManager1(),
      CtxManager2() as example):
    ...

with (
    CtxManager1() as example1,
    CtxManager2() as example2
):
    ...
",3.10
,new-features - parenthesized-context-managers,"with (
    CtxManager1() as example1,
    CtxManager2() as example2,
    CtxManager3() as example3,
):
    ...
",3.10
"Better error messages


SyntaxErrors

When parsing code that contains unclosed parentheses or brackets the interpreter
now includes the location of the unclosed bracket of parentheses instead of displaying
SyntaxError: unexpected EOF while parsing or pointing to some incorrect location.
For instance, consider the following code (notice the unclosed ‘{‘):
expected = {9: 1, 18: 2, 19: 2, 27: 3, 28: 3, 29: 3, 36: 4, 37: 4,
            38: 4, 39: 4, 45: 5, 46: 5, 47: 5, 48: 5, 49: 5, 54: 6,
some_other_code = foo()


Previous versions of the interpreter reported confusing places as the location of
the syntax error:
File ""example.py"", line 3
    some_other_code = foo()
                    ^
SyntaxError: invalid syntax


but in Python 3.10 a more informative error is emitted:
File ""example.py"", line 1
    expected = {9: 1, 18: 2, 19: 2, 27: 3, 28: 3, 29: 3, 36: 4, 37: 4,
               ^
SyntaxError: '{' was never closed


In a similar way, errors involving unclosed string literals (single and triple
quoted) now point to the start of the string instead of reporting EOF/EOL.
These improvements are inspired by previous work in the PyPy interpreter.
(Contributed by Pablo Galindo in bpo-42864 and Batuhan Taskaya in
bpo-40176.)
SyntaxError exceptions raised by the interpreter will now highlight the
full error range of the expression that constitutes the syntax error itself,
instead of just where the problem is detected. In this way, instead of displaying
(before Python 3.10):
>>> foo(x, z for z in range(10), t, w)
  File ""<stdin>"", line 1
    foo(x, z for z in range(10), t, w)
           ^
SyntaxError: Generator expression must be parenthesized


now Python 3.10 will display the exception as:
>>> foo(x, z for z in range(10), t, w)
  File ""<stdin>"", line 1
    foo(x, z for z in range(10), t, w)
           ^^^^^^^^^^^^^^^^^^^^
SyntaxError: Generator expression must be parenthesized


This improvement was contributed by Pablo Galindo in bpo-43914.
A considerable amount of new specialized messages for SyntaxError exceptions
have been incorporated. Some of the most notable ones are as follows:

Missing : before blocks:

>>> if rocket.position > event_horizon
  File ""<stdin>"", line 1
    if rocket.position > event_horizon
                                      ^
SyntaxError: expected ':'


(Contributed by Pablo Galindo in bpo-42997.)


Unparenthesised tuples in comprehensions targets:

>>> {x,y for x,y in zip('abcd', '1234')}
  File ""<stdin>"", line 1
    {x,y for x,y in zip('abcd', '1234')}
     ^
SyntaxError: did you forget parentheses around the comprehension target?


(Contributed by Pablo Galindo in bpo-43017.)


Missing commas in collection literals and between expressions:

>>> items = {
... x: 1,
... y: 2
... z: 3,
  File ""<stdin>"", line 3
    y: 2
       ^
SyntaxError: invalid syntax. Perhaps you forgot a comma?


(Contributed by Pablo Galindo in bpo-43822.)


Multiple Exception types without parentheses:

>>> try:
...     build_dyson_sphere()
... except NotEnoughScienceError, NotEnoughResourcesError:
  File ""<stdin>"", line 3
    except NotEnoughScienceError, NotEnoughResourcesError:
           ^
SyntaxError: multiple exception types must be parenthesized


(Contributed by Pablo Galindo in bpo-43149.)


Missing : and values in dictionary literals:

>>> values = {
... x: 1,
... y: 2,
... z:
... }
  File ""<stdin>"", line 4
    z:
     ^
SyntaxError: expression expected after dictionary key and ':'

>>> values = {x:1, y:2, z w:3}
  File ""<stdin>"", line 1
    values = {x:1, y:2, z w:3}
                        ^
SyntaxError: ':' expected after dictionary key


(Contributed by Pablo Galindo in bpo-43823.)


try blocks without except or finally blocks:

>>> try:
...     x = 2
... something = 3
  File ""<stdin>"", line 3
    something  = 3
    ^^^^^^^^^
SyntaxError: expected 'except' or 'finally' block


(Contributed by Pablo Galindo in bpo-44305.)


Usage of = instead of == in comparisons:

>>> if rocket.position = event_horizon:
  File ""<stdin>"", line 1
    if rocket.position = event_horizon:
                       ^
SyntaxError: cannot assign to attribute here. Maybe you meant '==' instead of '='?


(Contributed by Pablo Galindo in bpo-43797.)


Usage of * in f-strings:

>>> f""Black holes {*all_black_holes} and revelations""
  File ""<stdin>"", line 1
    (*all_black_holes)
     ^
SyntaxError: f-string: cannot use starred expression here


(Contributed by Pablo Galindo in bpo-41064.)





IndentationErrors

Many IndentationError exceptions now have more context regarding what kind of block
was expecting an indentation, including the location of the statement:
>>> def foo():
...    if lel:
...    x = 2
  File ""<stdin>"", line 3
    x = 2
    ^
IndentationError: expected an indented block after 'if' statement in line 2




AttributeErrors

When printing AttributeError, PyErr_Display() will offer
suggestions of similar attribute names in the object that the exception was
raised from:
>>> collections.namedtoplo
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: module 'collections' has no attribute 'namedtoplo'. Did you mean: namedtuple?


(Contributed by Pablo Galindo in bpo-38530.)


Warning
Notice this won’t work if PyErr_Display() is not called to display the error
which can happen if some other custom error display function is used. This is a common
scenario in some REPLs like IPython.




NameErrors

When printing NameError raised by the interpreter, PyErr_Display()
will offer suggestions of similar variable names in the function that the exception
was raised from:
>>> schwarzschild_black_hole = None
>>> schwarschild_black_hole
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
NameError: name 'schwarschild_black_hole' is not defined. Did you mean: schwarzschild_black_hole?


(Contributed by Pablo Galindo in bpo-38530.)


Warning
Notice this won’t work if PyErr_Display() is not called to display the error,
which can happen if some other custom error display function is used. This is a common
scenario in some REPLs like IPython.",new-features - better-error-messages,"expected = {9: 1, 18: 2, 19: 2, 27: 3, 28: 3, 29: 3, 36: 4, 37: 4,
            38: 4, 39: 4, 45: 5, 46: 5, 47: 5, 48: 5, 49: 5, 54: 6,
some_other_code = foo()
",3.10
,new-features - better-error-messages,"File ""example.py"", line 3
    some_other_code = foo()
                    ^
SyntaxError: invalid syntax
",3.10
,new-features - better-error-messages,"File ""example.py"", line 1
    expected = {9: 1, 18: 2, 19: 2, 27: 3, 28: 3, 29: 3, 36: 4, 37: 4,
               ^
SyntaxError: '{' was never closed
",3.10
,new-features - better-error-messages,">>> foo(x, z for z in range(10), t, w)
  File ""<stdin>"", line 1
    foo(x, z for z in range(10), t, w)
           ^
SyntaxError: Generator expression must be parenthesized
",3.10
,new-features - better-error-messages,">>> foo(x, z for z in range(10), t, w)
  File ""<stdin>"", line 1
    foo(x, z for z in range(10), t, w)
           ^^^^^^^^^^^^^^^^^^^^
SyntaxError: Generator expression must be parenthesized
",3.10
,new-features - better-error-messages,">>> if rocket.position > event_horizon
  File ""<stdin>"", line 1
    if rocket.position > event_horizon
                                      ^
SyntaxError: expected ':'
",3.10
,new-features - better-error-messages,">>> {x,y for x,y in zip('abcd', '1234')}
  File ""<stdin>"", line 1
    {x,y for x,y in zip('abcd', '1234')}
     ^
SyntaxError: did you forget parentheses around the comprehension target?
",3.10
,new-features - better-error-messages,">>> items = {
... x: 1,
... y: 2
... z: 3,
  File ""<stdin>"", line 3
    y: 2
       ^
SyntaxError: invalid syntax. Perhaps you forgot a comma?
",3.10
,new-features - better-error-messages,">>> try:
...     build_dyson_sphere()
... except NotEnoughScienceError, NotEnoughResourcesError:
  File ""<stdin>"", line 3
    except NotEnoughScienceError, NotEnoughResourcesError:
           ^
SyntaxError: multiple exception types must be parenthesized
",3.10
,new-features - better-error-messages,">>> values = {
... x: 1,
... y: 2,
... z:
... }
  File ""<stdin>"", line 4
    z:
     ^
SyntaxError: expression expected after dictionary key and ':'

>>> values = {x:1, y:2, z w:3}
  File ""<stdin>"", line 1
    values = {x:1, y:2, z w:3}
                        ^
SyntaxError: ':' expected after dictionary key
",3.10
,new-features - better-error-messages,">>> try:
...     x = 2
... something = 3
  File ""<stdin>"", line 3
    something  = 3
    ^^^^^^^^^
SyntaxError: expected 'except' or 'finally' block
",3.10
,new-features - better-error-messages,">>> if rocket.position = event_horizon:
  File ""<stdin>"", line 1
    if rocket.position = event_horizon:
                       ^
SyntaxError: cannot assign to attribute here. Maybe you meant '==' instead of '='?
",3.10
,new-features - better-error-messages,">>> f""Black holes {*all_black_holes} and revelations""
  File ""<stdin>"", line 1
    (*all_black_holes)
     ^
SyntaxError: f-string: cannot use starred expression here
",3.10
,new-features - better-error-messages,">>> def foo():
...    if lel:
...    x = 2
  File ""<stdin>"", line 3
    x = 2
    ^
IndentationError: expected an indented block after 'if' statement in line 2
",3.10
,new-features - better-error-messages,">>> collections.namedtoplo
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: module 'collections' has no attribute 'namedtoplo'. Did you mean: namedtuple?
",3.10
,new-features - better-error-messages,">>> schwarzschild_black_hole = None
>>> schwarschild_black_hole
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
NameError: name 'schwarschild_black_hole' is not defined. Did you mean: schwarzschild_black_hole?
",3.10
"PEP 634: Structural Pattern Matching

Structural pattern matching has been added in the form of a match statement
and case statements of patterns with associated actions. Patterns
consist of sequences, mappings, primitive data types as well as class instances.
Pattern matching enables programs to extract information from complex data types,
branch on the structure of data, and apply specific actions based on different
forms of data.

Syntax and operations

The generic syntax of pattern matching is:
match subject:
    case <pattern_1>:
        <action_1>
    case <pattern_2>:
        <action_2>
    case <pattern_3>:
        <action_3>
    case _:
        <action_wildcard>


A match statement takes an expression and compares its value to successive
patterns given as one or more case blocks.  Specifically, pattern matching
operates by:


using data with type and shape (the subject)
evaluating the subject in the match statement
comparing the subject with each pattern in a case statement
from top to bottom until a match is confirmed.
executing the action associated with the pattern of the confirmed
match
If an exact match is not confirmed, the last case, a wildcard _,
if provided, will be used as the matching case. If an exact match is
not confirmed and a wildcard case does not exist, the entire match
block is a no-op.




Declarative approach

Readers may be aware of pattern matching through the simple example of matching
a subject (data object) to a literal (pattern) with the switch statement found
in C, Java or JavaScript (and many other languages). Often the switch statement
is used for comparison of an object/expression with case statements containing
literals.
More powerful examples of pattern matching can be found in languages such as
Scala and Elixir. With structural pattern matching, the approach is “declarative” and
explicitly states the conditions (the patterns) for data to match.
While an “imperative” series of instructions using nested “if” statements
could be used to accomplish something similar to structural pattern matching,
it is less clear than the “declarative” approach. Instead the “declarative”
approach states the conditions to meet for a match and is more readable through
its explicit patterns. While structural pattern matching can be used in its
simplest form comparing a variable to a literal in a case statement, its
true value for Python lies in its handling of the subject’s type and shape.


Simple pattern: match to a literal

Let’s look at this example as pattern matching in its simplest form: a value,
the subject, being matched to several literals, the patterns. In the example
below, status is the subject of the match statement. The patterns are
each of the case statements, where literals represent request status codes.
The associated action to the case is executed after a match:
def http_error(status):
    match status:
        case 400:
            return ""Bad request""
        case 404:
            return ""Not found""
        case 418:
            return ""I'm a teapot""
        case _:
            return ""Something's wrong with the internet""


If the above function is passed a status of 418, “I’m a teapot” is returned.
If the above function is passed a status of 500, the case statement with
_ will match as a wildcard, and “Something’s wrong with the internet” is
returned.
Note the last block: the variable name, _, acts as a wildcard and insures
the subject will always match. The use of _ is optional.
You can combine several literals in a single pattern using | (“or”):
case 401 | 403 | 404:
    return ""Not allowed""



Behavior without the wildcard

If we modify the above example by removing the last case block, the example
becomes:
def http_error(status):
    match status:
        case 400:
            return ""Bad request""
        case 404:
            return ""Not found""
        case 418:
            return ""I'm a teapot""


Without the use of _ in a case statement, a match may not exist. If no
match exists, the behavior is a no-op. For example, if status of 500 is
passed, a no-op occurs.



Patterns with a literal and variable

Patterns can look like unpacking assignments, and a pattern may be used to bind
variables. In this example, a data point can be unpacked to its x-coordinate
and y-coordinate:
# point is an (x, y) tuple
match point:
    case (0, 0):
        print(""Origin"")
    case (0, y):
        print(f""Y={y}"")
    case (x, 0):
        print(f""X={x}"")
    case (x, y):
        print(f""X={x}, Y={y}"")
    case _:
        raise ValueError(""Not a point"")


The first pattern has two literals, (0, 0), and may be thought of as an
extension of the literal pattern shown above. The next two patterns combine a
literal and a variable, and the variable binds a value from the subject
(point).  The fourth pattern captures two values, which makes it
conceptually similar to the unpacking assignment (x, y) = point.


Patterns and classes

If you are using classes to structure your data, you can use as a pattern
the class name followed by an argument list resembling a constructor. This
pattern has the ability to capture class attributes into variables:
class Point:
    x: int
    y: int

def location(point):
    match point:
        case Point(x=0, y=0):
            print(""Origin is the point's location."")
        case Point(x=0, y=y):
            print(f""Y={y} and the point is on the y-axis."")
        case Point(x=x, y=0):
            print(f""X={x} and the point is on the x-axis."")
        case Point():
            print(""The point is located somewhere else on the plane."")
        case _:
            print(""Not a point"")



Patterns with positional parameters

You can use positional parameters with some builtin classes that provide an
ordering for their attributes (e.g. dataclasses). You can also define a specific
position for attributes in patterns by setting the __match_args__ special
attribute in your classes. If it’s set to (“x”, “y”), the following patterns
are all equivalent (and all bind the y attribute to the var variable):
Point(1, var)
Point(1, y=var)
Point(x=1, y=var)
Point(y=var, x=1)





Nested patterns

Patterns can be arbitrarily nested.  For example, if our data is a short
list of points, it could be matched like this:
match points:
    case []:
        print(""No points in the list."")
    case [Point(0, 0)]:
        print(""The origin is the only point in the list."")
    case [Point(x, y)]:
        print(f""A single point {x}, {y} is in the list."")
    case [Point(0, y1), Point(0, y2)]:
        print(f""Two points on the Y axis at {y1}, {y2} are in the list."")
    case _:
        print(""Something else is found in the list."")




Complex patterns and the wildcard

To this point, the examples have used _ alone in the last case statement.
A wildcard can be used in more complex patterns, such as ('error', code, _).
For example:
match test_variable:
    case ('warning', code, 40):
        print(""A warning has been received."")
    case ('error', code, _):
        print(f""An error {code} occurred."")


In the above case, test_variable will match for (‘error’, code, 100) and
(‘error’, code, 800).


Guard

We can add an if clause to a pattern, known as a “guard”.  If the
guard is false, match goes on to try the next case block.  Note
that value capture happens before the guard is evaluated:
match point:
    case Point(x, y) if x == y:
        print(f""The point is located on the diagonal Y=X at {x}."")
    case Point(x, y):
        print(f""Point is not on the diagonal."")




Other Key Features

Several other key features:

Like unpacking assignments, tuple and list patterns have exactly the
same meaning and actually match arbitrary sequences. Technically,
the subject must be a sequence.
Therefore, an important exception is that patterns don’t match iterators.
Also, to prevent a common mistake, sequence patterns don’t match strings.
Sequence patterns support wildcards: [x, y, *rest] and (x, y,
*rest) work similar to wildcards in unpacking assignments.  The
name after * may also be _, so (x, y, *_) matches a sequence
of at least two items without binding the remaining items.
Mapping patterns: {""bandwidth"": b, ""latency"": l} captures the
""bandwidth"" and ""latency"" values from a dict.  Unlike sequence
patterns, extra keys are ignored.  A wildcard **rest is also
supported.  (But **_ would be redundant, so is not allowed.)
Subpatterns may be captured using the as keyword:
case (Point(x1, y1), Point(x2, y2) as p2): ...


This binds x1, y1, x2, y2 like you would expect without the as clause,
and p2 to the entire second item of the subject.

Most literals are compared by equality. However, the singletons True,
False and None are compared by identity.
Named constants may be used in patterns.  These named constants must be
dotted names to prevent the constant from being interpreted as a capture
variable:
from enum import Enum
class Color(Enum):
    RED = 0
    GREEN = 1
    BLUE = 2

match color:
    case Color.RED:
        print(""I see red!"")
    case Color.GREEN:
        print(""Grass is green"")
    case Color.BLUE:
        print(""I'm feeling the blues :("")




For the full specification see PEP 634.  Motivation and rationale
are in PEP 635, and a longer tutorial is in PEP 636.",new-features - pep-634-structural-pattern-matching,"match subject:
    case <pattern_1>:
        <action_1>
    case <pattern_2>:
        <action_2>
    case <pattern_3>:
        <action_3>
    case _:
        <action_wildcard>
",3.10
,new-features - pep-634-structural-pattern-matching,"def http_error(status):
    match status:
        case 400:
            return ""Bad request""
        case 404:
            return ""Not found""
        case 418:
            return ""I'm a teapot""
        case _:
            return ""Something's wrong with the internet""
",3.10
,new-features - pep-634-structural-pattern-matching,"case 401 | 403 | 404:
    return ""Not allowed""
",3.10
,new-features - pep-634-structural-pattern-matching,"def http_error(status):
    match status:
        case 400:
            return ""Bad request""
        case 404:
            return ""Not found""
        case 418:
            return ""I'm a teapot""
",3.10
,new-features - pep-634-structural-pattern-matching,"# point is an (x, y) tuple
match point:
    case (0, 0):
        print(""Origin"")
    case (0, y):
        print(f""Y={y}"")
    case (x, 0):
        print(f""X={x}"")
    case (x, y):
        print(f""X={x}, Y={y}"")
    case _:
        raise ValueError(""Not a point"")
",3.10
,new-features - pep-634-structural-pattern-matching,"class Point:
    x: int
    y: int

def location(point):
    match point:
        case Point(x=0, y=0):
            print(""Origin is the point's location."")
        case Point(x=0, y=y):
            print(f""Y={y} and the point is on the y-axis."")
        case Point(x=x, y=0):
            print(f""X={x} and the point is on the x-axis."")
        case Point():
            print(""The point is located somewhere else on the plane."")
        case _:
            print(""Not a point"")
",3.10
,new-features - pep-634-structural-pattern-matching,"Point(1, var)
Point(1, y=var)
Point(x=1, y=var)
Point(y=var, x=1)
",3.10
,new-features - pep-634-structural-pattern-matching,"match points:
    case []:
        print(""No points in the list."")
    case [Point(0, 0)]:
        print(""The origin is the only point in the list."")
    case [Point(x, y)]:
        print(f""A single point {x}, {y} is in the list."")
    case [Point(0, y1), Point(0, y2)]:
        print(f""Two points on the Y axis at {y1}, {y2} are in the list."")
    case _:
        print(""Something else is found in the list."")
",3.10
,new-features - pep-634-structural-pattern-matching,"match test_variable:
    case ('warning', code, 40):
        print(""A warning has been received."")
    case ('error', code, _):
        print(f""An error {code} occurred."")
",3.10
,new-features - pep-634-structural-pattern-matching,"match point:
    case Point(x, y) if x == y:
        print(f""The point is located on the diagonal Y=X at {x}."")
    case Point(x, y):
        print(f""Point is not on the diagonal."")
",3.10
,new-features - pep-634-structural-pattern-matching,"case (Point(x1, y1), Point(x2, y2) as p2): ...
",3.10
,new-features - pep-634-structural-pattern-matching,"from enum import Enum
class Color(Enum):
    RED = 0
    GREEN = 1
    BLUE = 2

match color:
    case Color.RED:
        print(""I see red!"")
    case Color.GREEN:
        print(""Grass is green"")
    case Color.BLUE:
        print(""I'm feeling the blues :("")
",3.10
"Optional EncodingWarning and encoding=""locale"" option

The default encoding of TextIOWrapper and open() is
platform and locale dependent. Since UTF-8 is used on most Unix
platforms, omitting encoding option when opening UTF-8 files
(e.g. JSON, YAML, TOML, Markdown) is a very common bug. For example:
# BUG: ""rb"" mode or encoding=""utf-8"" should be used.
with open(""data.json"") as f:
    data = json.load(f)


To find this type of bug, an optional EncodingWarning is added.
It is emitted when sys.flags.warn_default_encoding
is true and locale-specific default encoding is used.
-X warn_default_encoding option and PYTHONWARNDEFAULTENCODING
are added to enable the warning.
See Text Encoding for more information.",new-features - optional-encodingwarning-and-encoding-locale-option,"# BUG: ""rb"" mode or encoding=""utf-8"" should be used.
with open(""data.json"") as f:
    data = json.load(f)
",3.10
"PEP 604: New Type Union Operator

A new type union operator was introduced which enables the syntax X | Y.
This provides a cleaner way of expressing ‘either type X or type Y’ instead of
using typing.Union, especially in type hints.
In previous versions of Python, to apply a type hint for functions accepting
arguments of multiple types, typing.Union was used:
def square(number: Union[int, float]) -> Union[int, float]:
    return number ** 2


Type hints can now be written in a more succinct manner:
def square(number: int | float) -> int | float:
    return number ** 2


This new syntax is also accepted as the second argument to isinstance()
and issubclass():
>>> isinstance(1, int | str)
True


See Union Type and PEP 604 for more details.
(Contributed by Maggie Moss and Philippe Prados in bpo-41428,
with additions by Yurii Karabas and Serhiy Storchaka in bpo-44490.)",new-features-related-to-type-hints - pep-604-new-type-union-operator,"def square(number: Union[int, float]) -> Union[int, float]:
    return number ** 2
",3.10
,new-features-related-to-type-hints - pep-604-new-type-union-operator,"def square(number: int | float) -> int | float:
    return number ** 2
",3.10
,new-features-related-to-type-hints - pep-604-new-type-union-operator,">>> isinstance(1, int | str)
True
",3.10
"PEP 613: TypeAlias

PEP 484 introduced the concept of type aliases, only requiring them to be
top-level unannotated assignments. This simplicity sometimes made it difficult
for type checkers to distinguish between type aliases and ordinary assignments,
especially when forward references or invalid types were involved. Compare:
StrCache = 'Cache[str]'  # a type alias
LOG_PREFIX = 'LOG[DEBUG]'  # a module constant


Now the typing module has a special value TypeAlias
which lets you declare type aliases more explicitly:
StrCache: TypeAlias = 'Cache[str]'  # a type alias
LOG_PREFIX = 'LOG[DEBUG]'  # a module constant


See PEP 613 for more details.
(Contributed by Mikhail Golubev in bpo-41923.)",new-features-related-to-type-hints - pep-613-typealias,"StrCache = 'Cache[str]'  # a type alias
LOG_PREFIX = 'LOG[DEBUG]'  # a module constant
",3.10
,new-features-related-to-type-hints - pep-613-typealias,"StrCache: TypeAlias = 'Cache[str]'  # a type alias
LOG_PREFIX = 'LOG[DEBUG]'  # a module constant
",3.10
"dataclasses


__slots__

Added slots parameter in dataclasses.dataclass() decorator.
(Contributed by Yurii Karabas in bpo-42269)


Keyword-only fields

dataclasses now supports fields that are keyword-only in the
generated __init__ method.  There are a number of ways of specifying
keyword-only fields.
You can say that every field is keyword-only:
from dataclasses import dataclass

@dataclass(kw_only=True)
class Birthday:
    name: str
    birthday: datetime.date


Both name and birthday are keyword-only parameters to the
generated __init__ method.
You can specify keyword-only on a per-field basis:
from dataclasses import dataclass

@dataclass
class Birthday:
    name: str
    birthday: datetime.date = field(kw_only=True)


Here only birthday is keyword-only.  If you set kw_only on
individual fields, be aware that there are rules about re-ordering
fields due to keyword-only fields needing to follow non-keyword-only
fields.  See the full dataclasses documentation for details.
You can also specify that all fields following a KW_ONLY marker are
keyword-only.  This will probably be the most common usage:
from dataclasses import dataclass, KW_ONLY

@dataclass
class Point:
    x: float
    y: float
    _: KW_ONLY
    z: float = 0.0
    t: float = 0.0


Here, z and t are keyword-only parameters, while x and
y are not.
(Contributed by Eric V. Smith in bpo-43532.)",improved-modules - dataclasses,"from dataclasses import dataclass

@dataclass(kw_only=True)
class Birthday:
    name: str
    birthday: datetime.date
",3.10
,improved-modules - dataclasses,"from dataclasses import dataclass

@dataclass
class Birthday:
    name: str
    birthday: datetime.date = field(kw_only=True)
",3.10
,improved-modules - dataclasses,"from dataclasses import dataclass, KW_ONLY

@dataclass
class Point:
    x: float
    y: float
    _: KW_ONLY
    z: float = 0.0
    t: float = 0.0
",3.10
"typing

For major changes, see New Features Related to Type Hints.
The behavior of typing.Literal was changed to conform with PEP 586
and to match the behavior of static type checkers specified in the PEP.

Literal now de-duplicates parameters.
Equality comparisons between Literal objects are now order independent.
Literal comparisons now respect types.  For example,
Literal[0] == Literal[False] previously evaluated to True.  It is
now False.  To support this change, the internally used type cache now
supports differentiating types.
Literal objects will now raise a TypeError exception during
equality comparisons if any of their parameters are not hashable.
Note that declaring Literal with unhashable parameters will not throw
an error:
>>> from typing import Literal
>>> Literal[{0}]
>>> Literal[{0}] == Literal[{False}]
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: unhashable type: 'set'




(Contributed by Yurii Karabas in bpo-42345.)
Add new function typing.is_typeddict() to introspect if an annotation
is a typing.TypedDict.
(Contributed by Patrick Reader in bpo-41792.)
Subclasses of typing.Protocol which only have data variables declared
will now raise a TypeError when checked with isinstance unless they
are decorated with runtime_checkable().  Previously, these checks
passed silently.  Users should decorate their
subclasses with the runtime_checkable() decorator
if they want runtime protocols.
(Contributed by Yurii Karabas in bpo-38908.)
Importing from the typing.io and typing.re submodules will now emit
DeprecationWarning.  These submodules have been deprecated since
Python 3.8 and will be removed in a future version of Python.  Anything
belonging to those submodules should be imported directly from
typing instead.
(Contributed by Sebastian Rittau in bpo-38291.)",improved-modules - typing,">>> from typing import Literal
>>> Literal[{0}]
>>> Literal[{0}] == Literal[{False}]
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: unhashable type: 'set'
",3.10
"Changes in the Python API


The etype parameters of the format_exception(),
format_exception_only(), and
print_exception() functions in the traceback module
have been renamed to exc.
(Contributed by Zackery Spytz and Matthias Bussonnier in bpo-26389.)
atexit: At Python exit, if a callback registered with
atexit.register() fails, its exception is now logged. Previously, only
some exceptions were logged, and the last exception was always silently
ignored.
(Contributed by Victor Stinner in bpo-42639.)
collections.abc.Callable generic now flattens type parameters, similar
to what typing.Callable currently does.  This means that
collections.abc.Callable[[int, str], str] will have __args__ of
(int, str, str); previously this was ([int, str], str).  Code which
accesses the arguments via typing.get_args() or __args__ need to account
for this change.  Furthermore, TypeError may be raised for invalid forms
of parameterizing collections.abc.Callable which may have passed
silently in Python 3.9.
(Contributed by Ken Jin in bpo-42195.)
socket.htons() and socket.ntohs() now raise OverflowError
instead of DeprecationWarning if the given parameter will not fit in
a 16-bit unsigned integer.
(Contributed by Erlend E. Aasland in bpo-42393.)
The loop parameter has been removed from most of asyncio‘s
high-level API following deprecation
in Python 3.8.
A coroutine that currently looks like this:
async def foo(loop):
    await asyncio.sleep(1, loop=loop)


Should be replaced with this:
async def foo():
    await asyncio.sleep(1)


If foo() was specifically designed not to run in the current thread’s
running event loop (e.g. running in another thread’s event loop), consider
using asyncio.run_coroutine_threadsafe() instead.
(Contributed by Yurii Karabas, Andrew Svetlov, Yury Selivanov and Kyle Stanley
in bpo-42392.)

The types.FunctionType constructor now inherits the current builtins
if the globals dictionary has no ""__builtins__"" key, rather than using
{""None"": None} as builtins: same behavior as eval() and
exec() functions.  Defining a function with def function(...): ...
in Python is not affected, globals cannot be overridden with this syntax: it
also inherits the current builtins.
(Contributed by Victor Stinner in bpo-42990.)",porting-to-python-3-10 - changes-in-the-python-api,"async def foo(loop):
    await asyncio.sleep(1, loop=loop)
",3.10
,porting-to-python-3-10 - changes-in-the-python-api,"async def foo():
    await asyncio.sleep(1)
",3.10
"Changes in the C API


The C API functions PyParser_SimpleParseStringFlags,
PyParser_SimpleParseStringFlagsFilename,
PyParser_SimpleParseFileFlags, PyNode_Compile and the type
used by these functions, struct _node, were removed due to the switch
to the new PEG parser.
Source should be now be compiled directly to a code object using, for
example, Py_CompileString(). The resulting code object can then be
evaluated using, for example, PyEval_EvalCode().
Specifically:

A call to PyParser_SimpleParseStringFlags followed by
PyNode_Compile can be replaced by calling Py_CompileString().
There is no direct replacement for PyParser_SimpleParseFileFlags.
To compile code from a FILE * argument, you will need to read
the file in C and pass the resulting buffer to Py_CompileString().
To compile a file given a char * filename, explicitly open the file, read
it and compile the result. One way to do this is using the io
module with PyImport_ImportModule(), PyObject_CallMethod(),
PyBytes_AsString() and Py_CompileString(),
as sketched below. (Declarations and error handling are omitted.)
io_module = Import_ImportModule(""io"");
fileobject = PyObject_CallMethod(io_module, ""open"", ""ss"", filename, ""rb"");
source_bytes_object = PyObject_CallMethod(fileobject, ""read"", """");
result = PyObject_CallMethod(fileobject, ""close"", """");
source_buf = PyBytes_AsString(source_bytes_object);
code = Py_CompileString(source_buf, filename, Py_file_input);



For FrameObject objects, the f_lasti member now represents a wordcode
offset instead of a simple offset into the bytecode string. This means that this
number needs to be multiplied by 2 to be used with APIs that expect a byte offset
instead (like PyCode_Addr2Line() for example). Notice as well that the
f_lasti member of FrameObject objects is not considered stable: please
use PyFrame_GetLineNumber() instead.",porting-to-python-3-10 - changes-in-the-c-api,"io_module = Import_ImportModule(""io"");
fileobject = PyObject_CallMethod(io_module, ""open"", ""ss"", filename, ""rb"");
source_bytes_object = PyObject_CallMethod(fileobject, ""read"", """");
result = PyObject_CallMethod(fileobject, ""close"", """");
source_buf = PyBytes_AsString(source_bytes_object);
code = Py_CompileString(source_buf, filename, Py_file_input);
",3.10
"Porting to Python 3.10


The PY_SSIZE_T_CLEAN macro must now be defined to use
PyArg_ParseTuple() and Py_BuildValue() formats which use
#: es#, et#, s#, u#, y#, z#, U# and Z#.
See Parsing arguments and building values and PEP 353.
(Contributed by Victor Stinner in bpo-40943.)
Since Py_REFCNT() is changed to the inline static function,
Py_REFCNT(obj) = new_refcnt must be replaced with Py_SET_REFCNT(obj, new_refcnt):
see Py_SET_REFCNT() (available since Python 3.9). For backward
compatibility, this macro can be used:
#if PY_VERSION_HEX < 0x030900A4
#  define Py_SET_REFCNT(obj, refcnt) ((Py_REFCNT(obj) = (refcnt)), (void)0)
#endif


(Contributed by Victor Stinner in bpo-39573.)

Calling PyDict_GetItem() without GIL held had been allowed
for historical reason. It is no longer allowed.
(Contributed by Victor Stinner in bpo-40839.)
PyUnicode_FromUnicode(NULL, size) and PyUnicode_FromStringAndSize(NULL, size)
raise DeprecationWarning now.  Use PyUnicode_New() to allocate
Unicode object without initial data.
(Contributed by Inada Naoki in bpo-36346.)
The private _PyUnicode_Name_CAPI structure of the PyCapsule API
unicodedata.ucnhash_CAPI has been moved to the internal C API.
(Contributed by Victor Stinner in bpo-42157.)
Py_GetPath(), Py_GetPrefix(), Py_GetExecPrefix(),
Py_GetProgramFullPath(), Py_GetPythonHome() and
Py_GetProgramName() functions now return NULL if called before
Py_Initialize() (before Python is initialized). Use the new
Python Initialization Configuration API to get the Python Path Configuration.
(Contributed by Victor Stinner in bpo-42260.)
PyList_SET_ITEM(), PyTuple_SET_ITEM() and
PyCell_SET() macros can no longer be used as l-value or r-value.
For example, x = PyList_SET_ITEM(a, b, c) and
PyList_SET_ITEM(a, b, c) = x now fail with a compiler error. It prevents
bugs like if (PyList_SET_ITEM (a, b, c) < 0) ... test.
(Contributed by Zackery Spytz and Victor Stinner in bpo-30459.)
The non-limited API files odictobject.h, parser_interface.h,
picklebufobject.h, pyarena.h, pyctype.h, pydebug.h,
pyfpe.h, and pytime.h have been moved to the Include/cpython
directory. These files must not be included directly, as they are already
included in Python.h; see Include Files. If they have
been included directly, consider including Python.h instead.
(Contributed by Nicholas Sim in bpo-35134.)
Use the Py_TPFLAGS_IMMUTABLETYPE type flag to create immutable type
objects. Do not rely on Py_TPFLAGS_HEAPTYPE to decide if a type
object is mutable or not; check if Py_TPFLAGS_IMMUTABLETYPE is set
instead.
(Contributed by Victor Stinner and Erlend E. Aasland in bpo-43908.)
The undocumented function Py_FrozenMain has been removed from the
limited API. The function is mainly useful for custom builds of Python.
(Contributed by Petr Viktorin in bpo-26241.)",c-api-changes - id2,"#if PY_VERSION_HEX < 0x030900A4
#  define Py_SET_REFCNT(obj, refcnt) ((Py_REFCNT(obj) = (refcnt)), (void)0)
#endif
",3.10
"PEP 657: Fine-grained error locations in tracebacks

When printing tracebacks, the interpreter will now point to the exact expression
that caused the error, instead of just the line. For example:
Traceback (most recent call last):
  File ""distance.py"", line 11, in <module>
    print(manhattan_distance(p1, p2))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""distance.py"", line 6, in manhattan_distance
    return abs(point_1.x - point_2.x) + abs(point_1.y - point_2.y)
                           ^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'x'


Previous versions of the interpreter would point to just the line, making it
ambiguous which object was None. These enhanced errors can also be helpful
when dealing with deeply nested dict objects and multiple function calls:
Traceback (most recent call last):
  File ""query.py"", line 37, in <module>
    magic_arithmetic('foo')
  File ""query.py"", line 18, in magic_arithmetic
    return add_counts(x) / 25
           ^^^^^^^^^^^^^
  File ""query.py"", line 24, in add_counts
    return 25 + query_user(user1) + query_user(user2)
                ^^^^^^^^^^^^^^^^^
  File ""query.py"", line 32, in query_user
    return 1 + query_count(db, response['a']['b']['c']['user'], retry=True)
                               ~~~~~~~~~~~~~~~~~~^^^^^
TypeError: 'NoneType' object is not subscriptable


As well as complex arithmetic expressions:
Traceback (most recent call last):
  File ""calculation.py"", line 54, in <module>
    result = (x / y / z) * (a / b / c)
              ~~~~~~^~~
ZeroDivisionError: division by zero


Additionally, the information used by the enhanced traceback feature
is made available via a general API, that can be used to correlate
bytecode instructions with source code location.
This information can be retrieved using:

The codeobject.co_positions() method in Python.
The PyCode_Addr2Location() function in the C API.

See PEP 657 for more details. (Contributed by Pablo Galindo, Batuhan Taskaya
and Ammar Askar in bpo-43950.)

Note
This feature requires storing column positions in Code Objects,
which may result in a small increase in interpreter memory usage
and disk usage for compiled Python files.
To avoid storing the extra information
and deactivate printing the extra traceback information,
use the -X no_debug_ranges command line option
or the PYTHONNODEBUGRANGES environment variable.",new-features - pep-657-fine-grained-error-locations-in-tracebacks,"Traceback (most recent call last):
  File ""distance.py"", line 11, in <module>
    print(manhattan_distance(p1, p2))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""distance.py"", line 6, in manhattan_distance
    return abs(point_1.x - point_2.x) + abs(point_1.y - point_2.y)
                           ^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'x'
",3.11
,new-features - pep-657-fine-grained-error-locations-in-tracebacks,"Traceback (most recent call last):
  File ""query.py"", line 37, in <module>
    magic_arithmetic('foo')
  File ""query.py"", line 18, in magic_arithmetic
    return add_counts(x) / 25
           ^^^^^^^^^^^^^
  File ""query.py"", line 24, in add_counts
    return 25 + query_user(user1) + query_user(user2)
                ^^^^^^^^^^^^^^^^^
  File ""query.py"", line 32, in query_user
    return 1 + query_count(db, response['a']['b']['c']['user'], retry=True)
                               ~~~~~~~~~~~~~~~~~~^^^^^
TypeError: 'NoneType' object is not subscriptable
",3.11
,new-features - pep-657-fine-grained-error-locations-in-tracebacks,"Traceback (most recent call last):
  File ""calculation.py"", line 54, in <module>
    result = (x / y / z) * (a / b / c)
              ~~~~~~^~~
ZeroDivisionError: division by zero
",3.11
"PEP 655: Marking individual TypedDict items as required or not-required

Required and NotRequired provide a
straightforward way to mark whether individual items in a
TypedDict must be present. Previously, this was only possible
using inheritance.
All fields are still required by default,
unless the total parameter is set to False,
in which case all fields are still not-required by default.
For example, the following specifies a TypedDict
with one required and one not-required key:
class Movie(TypedDict):
   title: str
   year: NotRequired[int]

m1: Movie = {""title"": ""Black Panther"", ""year"": 2018}  # OK
m2: Movie = {""title"": ""Star Wars""}  # OK (year is not required)
m3: Movie = {""year"": 2022}  # ERROR (missing required field title)


The following definition is equivalent:
class Movie(TypedDict, total=False):
   title: Required[str]
   year: int


See PEP 655 for more details.
(Contributed by David Foster and Jelle Zijlstra in bpo-47087. PEP
written by David Foster.)",new-features-related-to-type-hints - pep-655-marking-individual-typeddict-items-as-required-or-not-required,"class Movie(TypedDict):
   title: str
   year: NotRequired[int]

m1: Movie = {""title"": ""Black Panther"", ""year"": 2018}  # OK
m2: Movie = {""title"": ""Star Wars""}  # OK (year is not required)
m3: Movie = {""year"": 2022}  # ERROR (missing required field title)
",3.11
,new-features-related-to-type-hints - pep-655-marking-individual-typeddict-items-as-required-or-not-required,"class Movie(TypedDict, total=False):
   title: Required[str]
   year: int
",3.11
"PEP 673: Self type

The new Self annotation provides a simple and intuitive
way to annotate methods that return an instance of their class. This
behaves the same as the TypeVar-based approach
specified in PEP 484,
but is more concise and easier to follow.
Common use cases include alternative constructors provided as
classmethods,
and __enter__() methods that return self:
class MyLock:
    def __enter__(self) -> Self:
        self.lock()
        return self

    ...

class MyInt:
    @classmethod
    def fromhex(cls, s: str) -> Self:
        return cls(int(s, 16))

    ...


Self can also be used to annotate method parameters
or attributes of the same type as their enclosing class.
See PEP 673 for more details.
(Contributed by James Hilton-Balfe in bpo-46534. PEP written by
Pradeep Kumar Srinivasan and James Hilton-Balfe.)",new-features-related-to-type-hints - pep-673-self-type,"class MyLock:
    def __enter__(self) -> Self:
        self.lock()
        return self

    ...

class MyInt:
    @classmethod
    def fromhex(cls, s: str) -> Self:
        return cls(int(s, 16))

    ...
",3.11
"PEP 675: Arbitrary literal string type

The new LiteralString annotation may be used to indicate
that a function parameter can be of any literal string type. This allows
a function to accept arbitrary literal string types, as well as strings
created from other literal strings. Type checkers can then
enforce that sensitive functions, such as those that execute SQL
statements or shell commands, are called only with static arguments,
providing protection against injection attacks.
For example, a SQL query function could be annotated as follows:
def run_query(sql: LiteralString) -> ...
    ...

def caller(
    arbitrary_string: str,
    query_string: LiteralString,
    table_name: LiteralString,
) -> None:
    run_query(""SELECT * FROM students"")       # ok
    run_query(query_string)                   # ok
    run_query(""SELECT * FROM "" + table_name)  # ok
    run_query(arbitrary_string)               # type checker error
    run_query(                                # type checker error
        f""SELECT * FROM students WHERE name = {arbitrary_string}""
    )


See PEP 675 for more details.
(Contributed by Jelle Zijlstra in bpo-47088. PEP written by Pradeep
Kumar Srinivasan and Graham Bleaney.)",new-features-related-to-type-hints - pep-675-arbitrary-literal-string-type,"def run_query(sql: LiteralString) -> ...
    ...

def caller(
    arbitrary_string: str,
    query_string: LiteralString,
    table_name: LiteralString,
) -> None:
    run_query(""SELECT * FROM students"")       # ok
    run_query(query_string)                   # ok
    run_query(""SELECT * FROM "" + table_name)  # ok
    run_query(arbitrary_string)               # type checker error
    run_query(                                # type checker error
        f""SELECT * FROM students WHERE name = {arbitrary_string}""
    )
",3.11
"PEP 681: Data class transforms

dataclass_transform may be used to
decorate a class, metaclass, or a function that is itself a decorator.
The presence of @dataclass_transform() tells a static type checker that the
decorated object performs runtime “magic” that transforms a class,
giving it dataclass-like behaviors.
For example:
# The create_model decorator is defined by a library.
@typing.dataclass_transform()
def create_model(cls: Type[T]) -> Type[T]:
    cls.__init__ = ...
    cls.__eq__ = ...
    cls.__ne__ = ...
    return cls

# The create_model decorator can now be used to create new model classes:
@create_model
class CustomerModel:
    id: int
    name: str

c = CustomerModel(id=327, name=""Eric Idle"")


See PEP 681 for more details.
(Contributed by Jelle Zijlstra in gh-91860. PEP written by
Erik De Bonte and Eric Traut.)",new-features-related-to-type-hints - pep-681-data-class-transforms,"# The create_model decorator is defined by a library.
@typing.dataclass_transform()
def create_model(cls: Type[T]) -> Type[T]:
    cls.__init__ = ...
    cls.__eq__ = ...
    cls.__ne__ = ...
    return cls

# The create_model decorator can now be used to create new model classes:
@create_model
class CustomerModel:
    id: int
    name: str

c = CustomerModel(id=327, name=""Eric Idle"")
",3.11
"functools


functools.singledispatch() now supports types.UnionType
and typing.Union as annotations to the dispatch argument.:
>>> from functools import singledispatch
>>> @singledispatch
... def fun(arg, verbose=False):
...     if verbose:
...         print(""Let me just say,"", end="" "")
...     print(arg)
...
>>> @fun.register
... def _(arg: int | float, verbose=False):
...     if verbose:
...         print(""Strength in numbers, eh?"", end="" "")
...     print(arg)
...
>>> from typing import Union
>>> @fun.register
... def _(arg: Union[list, set], verbose=False):
...     if verbose:
...         print(""Enumerate this:"")
...     for i, elem in enumerate(arg):
...         print(i, elem)
...


(Contributed by Yurii Karabas in bpo-46014.)",improved-modules - functools,">>> from functools import singledispatch
>>> @singledispatch
... def fun(arg, verbose=False):
...     if verbose:
...         print(""Let me just say,"", end="" "")
...     print(arg)
...
>>> @fun.register
... def _(arg: int | float, verbose=False):
...     if verbose:
...         print(""Strength in numbers, eh?"", end="" "")
...     print(arg)
...
>>> from typing import Union
>>> @fun.register
... def _(arg: Union[list, set], verbose=False):
...     if verbose:
...         print(""Enumerate this:"")
...     for i, elem in enumerate(arg):
...         print(i, elem)
...
",3.11
"Faster Startup


Frozen imports / Static code objects

Python caches bytecode in the __pycache__
directory to speed up module loading.
Previously in 3.10, Python module execution looked like this:
Read __pycache__ -> Unmarshal -> Heap allocated code object -> Evaluate


In Python 3.11, the core modules essential for Python startup are “frozen”.
This means that their Code Objects (and bytecode)
are statically allocated by the interpreter.
This reduces the steps in module execution process to:
Statically allocated code object -> Evaluate


Interpreter startup is now 10-15% faster in Python 3.11. This has a big
impact for short-running programs using Python.
(Contributed by Eric Snow, Guido van Rossum and Kumar Aditya in many issues.)",faster-cpython - faster-startup,"Read __pycache__ -> Unmarshal -> Heap allocated code object -> Evaluate
",3.11
,faster-cpython - faster-startup,"Statically allocated code object -> Evaluate
",3.11
"Porting to Python 3.11


Some macros have been converted to static inline functions to avoid
macro pitfalls.
The change should be mostly transparent to users,
as the replacement functions will cast their arguments to the expected types
to avoid compiler warnings due to static type checks.
However, when the limited C API is set to >=3.11,
these casts are not done,
and callers will need to cast arguments to their expected types.
See PEP 670 for more details.
(Contributed by Victor Stinner and Erlend E. Aasland in gh-89653.)
PyErr_SetExcInfo() no longer uses the type and traceback
arguments, the interpreter now derives those values from the exception
instance (the value argument). The function still steals references
of all three arguments.
(Contributed by Irit Katriel in bpo-45711.)
PyErr_GetExcInfo() now derives the type and traceback
fields of the result from the exception instance (the value field).
(Contributed by Irit Katriel in bpo-45711.)
_frozen has a new is_package field to indicate whether
or not the frozen module is a package.  Previously, a negative value
in the size field was the indicator.  Now only non-negative values
be used for size.
(Contributed by Kumar Aditya in bpo-46608.)
_PyFrameEvalFunction() now takes _PyInterpreterFrame*
as its second parameter, instead of PyFrameObject*.
See PEP 523 for more details of how to use this function pointer type.
PyCode_New() and PyCode_NewWithPosOnlyArgs() now take
an additional exception_table argument.
Using these functions should be avoided, if at all possible.
To get a custom code object: create a code object using the compiler,
then get a modified version with the replace method.
PyCodeObject no longer has the co_code, co_varnames,
co_cellvars and co_freevars fields.  Instead, use
PyCode_GetCode(), PyCode_GetVarnames(),
PyCode_GetCellvars() and PyCode_GetFreevars() respectively
to access them via the C API.
(Contributed by Brandt Bucher in bpo-46841 and Ken Jin in gh-92154
and gh-94936.)
The old trashcan macros (Py_TRASHCAN_SAFE_BEGIN/Py_TRASHCAN_SAFE_END)
are now deprecated. They should be replaced by the new macros
Py_TRASHCAN_BEGIN and Py_TRASHCAN_END.
A tp_dealloc function that has the old macros, such as:
static void
mytype_dealloc(mytype *p)
{
    PyObject_GC_UnTrack(p);
    Py_TRASHCAN_SAFE_BEGIN(p);
    ...
    Py_TRASHCAN_SAFE_END
}


should migrate to the new macros as follows:
static void
mytype_dealloc(mytype *p)
{
    PyObject_GC_UnTrack(p);
    Py_TRASHCAN_BEGIN(p, mytype_dealloc)
    ...
    Py_TRASHCAN_END
}


Note that Py_TRASHCAN_BEGIN has a second argument which
should be the deallocation function it is in.
To support older Python versions in the same codebase, you
can define the following macros and use them throughout
the code (credit: these were copied from the mypy codebase):
#if PY_VERSION_HEX >= 0x03080000
#  define CPy_TRASHCAN_BEGIN(op, dealloc) Py_TRASHCAN_BEGIN(op, dealloc)
#  define CPy_TRASHCAN_END(op) Py_TRASHCAN_END
#else
#  define CPy_TRASHCAN_BEGIN(op, dealloc) Py_TRASHCAN_SAFE_BEGIN(op)
#  define CPy_TRASHCAN_END(op) Py_TRASHCAN_SAFE_END(op)
#endif



The PyType_Ready() function now raises an error if a type is defined
with the Py_TPFLAGS_HAVE_GC flag set but has no traverse function
(PyTypeObject.tp_traverse).
(Contributed by Victor Stinner in bpo-44263.)
Heap types with the Py_TPFLAGS_IMMUTABLETYPE flag can now inherit
the PEP 590 vectorcall protocol.  Previously, this was only possible for
static types.
(Contributed by Erlend E. Aasland in bpo-43908)
Since Py_TYPE() is changed to a inline static function,
Py_TYPE(obj) = new_type must be replaced with
Py_SET_TYPE(obj, new_type): see the Py_SET_TYPE() function
(available since Python 3.9). For backward compatibility, this macro can be
used:
#if PY_VERSION_HEX < 0x030900A4 && !defined(Py_SET_TYPE)
static inline void _Py_SET_TYPE(PyObject *ob, PyTypeObject *type)
{ ob->ob_type = type; }
#define Py_SET_TYPE(ob, type) _Py_SET_TYPE((PyObject*)(ob), type)
#endif


(Contributed by Victor Stinner in bpo-39573.)

Since Py_SIZE() is changed to a inline static function,
Py_SIZE(obj) = new_size must be replaced with
Py_SET_SIZE(obj, new_size): see the Py_SET_SIZE() function
(available since Python 3.9). For backward compatibility, this macro can be
used:
#if PY_VERSION_HEX < 0x030900A4 && !defined(Py_SET_SIZE)
static inline void _Py_SET_SIZE(PyVarObject *ob, Py_ssize_t size)
{ ob->ob_size = size; }
#define Py_SET_SIZE(ob, size) _Py_SET_SIZE((PyVarObject*)(ob), size)
#endif


(Contributed by Victor Stinner in bpo-39573.)

<Python.h> no longer includes the header files <stdlib.h>,
<stdio.h>, <errno.h> and <string.h> when the Py_LIMITED_API
macro is set to 0x030b0000 (Python 3.11) or higher. C extensions should
explicitly include the header files after #include <Python.h>.
(Contributed by Victor Stinner in bpo-45434.)
The non-limited API files cellobject.h, classobject.h, code.h, context.h,
funcobject.h, genobject.h and longintrepr.h have been moved to
the Include/cpython directory. Moreover, the eval.h header file was
removed. These files must not be included directly, as they are already
included in Python.h: Include Files. If they have
been included directly, consider including Python.h instead.
(Contributed by Victor Stinner in bpo-35134.)
The PyUnicode_CHECK_INTERNED() macro has been excluded from the
limited C API. It was never usable there, because it used internal structures
which are not available in the limited C API.
(Contributed by Victor Stinner in bpo-46007.)
The following frame functions and type are now directly available with
#include <Python.h>, it’s no longer needed to add
#include <frameobject.h>:

PyFrame_Check()
PyFrame_GetBack()
PyFrame_GetBuiltins()
PyFrame_GetGenerator()
PyFrame_GetGlobals()
PyFrame_GetLasti()
PyFrame_GetLocals()
PyFrame_Type

(Contributed by Victor Stinner in gh-93937.)



The PyFrameObject structure members have been removed from the
public C API.
While the documentation notes that the PyFrameObject fields are
subject to change at any time, they have been stable for a long time and were
used in several popular extensions.
In Python 3.11, the frame struct was reorganized to allow performance
optimizations. Some fields were removed entirely, as they were details of the
old implementation.
PyFrameObject fields:

f_back: use PyFrame_GetBack().
f_blockstack: removed.
f_builtins: use PyFrame_GetBuiltins().
f_code: use PyFrame_GetCode().
f_gen: use PyFrame_GetGenerator().
f_globals: use PyFrame_GetGlobals().
f_iblock: removed.
f_lasti: use PyFrame_GetLasti().
Code using f_lasti with PyCode_Addr2Line() should use
PyFrame_GetLineNumber() instead; it may be faster.
f_lineno: use PyFrame_GetLineNumber()
f_locals: use PyFrame_GetLocals().
f_stackdepth: removed.
f_state: no public API (renamed to f_frame.f_state).
f_trace: no public API.
f_trace_lines: use PyObject_GetAttrString((PyObject*)frame, ""f_trace_lines"").
f_trace_opcodes: use PyObject_GetAttrString((PyObject*)frame, ""f_trace_opcodes"").
f_localsplus: no public API (renamed to f_frame.localsplus).
f_valuestack: removed.

The Python frame object is now created lazily. A side effect is that the
f_back member must not be accessed directly,
since its value is now also
computed lazily. The PyFrame_GetBack() function must be called
instead.
Debuggers that accessed the f_locals directly must call
PyFrame_GetLocals() instead. They no longer need to call
PyFrame_FastToLocalsWithError() or PyFrame_LocalsToFast(),
in fact they should not call those functions. The necessary updating of the
frame is now managed by the virtual machine.
Code defining PyFrame_GetCode() on Python 3.8 and older:
#if PY_VERSION_HEX < 0x030900B1
static inline PyCodeObject* PyFrame_GetCode(PyFrameObject *frame)
{
    Py_INCREF(frame->f_code);
    return frame->f_code;
}
#endif


Code defining PyFrame_GetBack() on Python 3.8 and older:
#if PY_VERSION_HEX < 0x030900B1
static inline PyFrameObject* PyFrame_GetBack(PyFrameObject *frame)
{
    Py_XINCREF(frame->f_back);
    return frame->f_back;
}
#endif


Or use the pythoncapi_compat project to get these two
functions on older Python versions.

Changes of the PyThreadState structure members:

frame: removed, use PyThreadState_GetFrame() (function added
to Python 3.9 by bpo-40429).
Warning: the function returns a strong reference, need to call
Py_XDECREF().
tracing: changed, use PyThreadState_EnterTracing()
and PyThreadState_LeaveTracing()
(functions added to Python 3.11 by bpo-43760).
recursion_depth: removed,
use (tstate->recursion_limit - tstate->recursion_remaining) instead.
stackcheck_counter: removed.

Code defining PyThreadState_GetFrame() on Python 3.8 and older:
#if PY_VERSION_HEX < 0x030900B1
static inline PyFrameObject* PyThreadState_GetFrame(PyThreadState *tstate)
{
    Py_XINCREF(tstate->frame);
    return tstate->frame;
}
#endif


Code defining PyThreadState_EnterTracing() and
PyThreadState_LeaveTracing() on Python 3.10 and older:
#if PY_VERSION_HEX < 0x030B00A2
static inline void PyThreadState_EnterTracing(PyThreadState *tstate)
{
    tstate->tracing++;
#if PY_VERSION_HEX >= 0x030A00A1
    tstate->cframe->use_tracing = 0;
#else
    tstate->use_tracing = 0;
#endif
}

static inline void PyThreadState_LeaveTracing(PyThreadState *tstate)
{
    int use_tracing = (tstate->c_tracefunc != NULL || tstate->c_profilefunc != NULL);
    tstate->tracing--;
#if PY_VERSION_HEX >= 0x030A00A1
    tstate->cframe->use_tracing = use_tracing;
#else
    tstate->use_tracing = use_tracing;
#endif
}
#endif


Or use the pythoncapi-compat project to get these functions
on old Python functions.

Distributors are encouraged to build Python with the optimized Blake2
library libb2.
The PyConfig.module_search_paths_set field must now be set to 1 for
initialization to use PyConfig.module_search_paths to initialize
sys.path. Otherwise, initialization will recalculate the path and replace
any values added to module_search_paths.
PyConfig_Read() no longer calculates the initial search path, and will not
fill any values into PyConfig.module_search_paths. To calculate default
paths and then modify them, finish initialization and use PySys_GetObject()
to retrieve sys.path as a Python list object and modify it directly.",c-api-changes - whatsnew311-c-api-porting,"static void
mytype_dealloc(mytype *p)
{
    PyObject_GC_UnTrack(p);
    Py_TRASHCAN_SAFE_BEGIN(p);
    ...
    Py_TRASHCAN_SAFE_END
}
",3.11
,c-api-changes - whatsnew311-c-api-porting,"static void
mytype_dealloc(mytype *p)
{
    PyObject_GC_UnTrack(p);
    Py_TRASHCAN_BEGIN(p, mytype_dealloc)
    ...
    Py_TRASHCAN_END
}
",3.11
,c-api-changes - whatsnew311-c-api-porting,"#if PY_VERSION_HEX >= 0x03080000
#  define CPy_TRASHCAN_BEGIN(op, dealloc) Py_TRASHCAN_BEGIN(op, dealloc)
#  define CPy_TRASHCAN_END(op) Py_TRASHCAN_END
#else
#  define CPy_TRASHCAN_BEGIN(op, dealloc) Py_TRASHCAN_SAFE_BEGIN(op)
#  define CPy_TRASHCAN_END(op) Py_TRASHCAN_SAFE_END(op)
#endif
",3.11
,c-api-changes - whatsnew311-c-api-porting,"#if PY_VERSION_HEX < 0x030900A4 && !defined(Py_SET_TYPE)
static inline void _Py_SET_TYPE(PyObject *ob, PyTypeObject *type)
{ ob->ob_type = type; }
#define Py_SET_TYPE(ob, type) _Py_SET_TYPE((PyObject*)(ob), type)
#endif
",3.11
,c-api-changes - whatsnew311-c-api-porting,"#if PY_VERSION_HEX < 0x030900A4 && !defined(Py_SET_SIZE)
static inline void _Py_SET_SIZE(PyVarObject *ob, Py_ssize_t size)
{ ob->ob_size = size; }
#define Py_SET_SIZE(ob, size) _Py_SET_SIZE((PyVarObject*)(ob), size)
#endif
",3.11
,c-api-changes - whatsnew311-c-api-porting,"#if PY_VERSION_HEX < 0x030900B1
static inline PyCodeObject* PyFrame_GetCode(PyFrameObject *frame)
{
    Py_INCREF(frame->f_code);
    return frame->f_code;
}
#endif
",3.11
,c-api-changes - whatsnew311-c-api-porting,"#if PY_VERSION_HEX < 0x030900B1
static inline PyFrameObject* PyFrame_GetBack(PyFrameObject *frame)
{
    Py_XINCREF(frame->f_back);
    return frame->f_back;
}
#endif
",3.11
,c-api-changes - whatsnew311-c-api-porting,"#if PY_VERSION_HEX < 0x030900B1
static inline PyFrameObject* PyThreadState_GetFrame(PyThreadState *tstate)
{
    Py_XINCREF(tstate->frame);
    return tstate->frame;
}
#endif
",3.11
,c-api-changes - whatsnew311-c-api-porting,"#if PY_VERSION_HEX < 0x030B00A2
static inline void PyThreadState_EnterTracing(PyThreadState *tstate)
{
    tstate->tracing++;
#if PY_VERSION_HEX >= 0x030A00A1
    tstate->cframe->use_tracing = 0;
#else
    tstate->use_tracing = 0;
#endif
}

static inline void PyThreadState_LeaveTracing(PyThreadState *tstate)
{
    int use_tracing = (tstate->c_tracefunc != NULL || tstate->c_profilefunc != NULL);
    tstate->tracing--;
#if PY_VERSION_HEX >= 0x030A00A1
    tstate->cframe->use_tracing = use_tracing;
#else
    tstate->use_tracing = use_tracing;
#endif
}
#endif
",3.11
"PEP 695: Type Parameter Syntax

Generic classes and functions under PEP 484 were declared using a verbose syntax
that left the scope of type parameters unclear and required explicit declarations of
variance.
PEP 695 introduces a new, more compact and explicit way to create
generic classes and functions:
def max[T](args: Iterable[T]) -> T:
    ...

class list[T]:
    def __getitem__(self, index: int, /) -> T:
        ...

    def append(self, element: T) -> None:
        ...


In addition, the PEP introduces a new way to declare type aliases
using the type statement, which creates an instance of
TypeAliasType:
type Point = tuple[float, float]


Type aliases can also be generic:
type Point[T] = tuple[T, T]


The new syntax allows declaring TypeVarTuple
and ParamSpec parameters, as well as TypeVar
parameters with bounds or constraints:
type IntFunc[**P] = Callable[P, int]  # ParamSpec
type LabeledTuple[*Ts] = tuple[str, *Ts]  # TypeVarTuple
type HashableSequence[T: Hashable] = Sequence[T]  # TypeVar with bound
type IntOrStrSequence[T: (int, str)] = Sequence[T]  # TypeVar with constraints


The value of type aliases and the bound and constraints of type variables
created through this syntax are evaluated only on demand (see
lazy evaluation). This means type aliases are able to
refer to other types defined later in the file.
Type parameters declared through a type parameter list are visible within the
scope of the declaration and any nested scopes, but not in the outer scope. For
example, they can be used in the type annotations for the methods of a generic
class or in the class body. However, they cannot be used in the module scope after
the class is defined. See Type parameter lists for a detailed description of the
runtime semantics of type parameters.
In order to support these scoping semantics, a new kind of scope is introduced,
the annotation scope. Annotation scopes behave for the
most part like function scopes, but interact differently with enclosing class scopes.
In Python 3.13, annotations will also be evaluated in
annotation scopes.
See PEP 695 for more details.
(PEP written by Eric Traut. Implementation by Jelle Zijlstra, Eric Traut,
and others in gh-103764.)",new-features - pep-695-type-parameter-syntax,"def max[T](args: Iterable[T]) -> T:
    ...

class list[T]:
    def __getitem__(self, index: int, /) -> T:
        ...

    def append(self, element: T) -> None:
        ...
",3.12
,new-features - pep-695-type-parameter-syntax,"type Point = tuple[float, float]
",3.12
,new-features - pep-695-type-parameter-syntax,"type Point[T] = tuple[T, T]
",3.12
,new-features - pep-695-type-parameter-syntax,"type IntFunc[**P] = Callable[P, int]  # ParamSpec
type LabeledTuple[*Ts] = tuple[str, *Ts]  # TypeVarTuple
type HashableSequence[T: Hashable] = Sequence[T]  # TypeVar with bound
type IntOrStrSequence[T: (int, str)] = Sequence[T]  # TypeVar with constraints
",3.12
"PEP 701: Syntactic formalization of f-strings

PEP 701 lifts some restrictions on the usage of f-strings.
Expression components inside f-strings can now be any valid Python expression,
including strings reusing the same quote as the containing f-string,
multi-line expressions, comments, backslashes, and unicode escape sequences.
Let’s cover these in detail:

Quote reuse: in Python 3.11, reusing the same quotes as the enclosing f-string
raises a SyntaxError, forcing the user to either use other available
quotes (like using double quotes or triple quotes if the f-string uses single
quotes). In Python 3.12, you can now do things like this:
>>> songs = ['Take me back to Eden', 'Alkaline', 'Ascensionism']
>>> f""This is the playlist: {"", "".join(songs)}""
'This is the playlist: Take me back to Eden, Alkaline, Ascensionism'


Note that before this change there was no explicit limit in how f-strings can
be nested, but the fact that string quotes cannot be reused inside the
expression component of f-strings made it impossible to nest f-strings
arbitrarily. In fact, this is the most nested f-string that could be written:
>>> f""""""{f'''{f'{f""{1+1}""}'}'''}""""""
'2'


As now f-strings can contain any valid Python expression inside expression
components, it is now possible to nest f-strings arbitrarily:
>>> f""{f""{f""{f""{f""{f""{1+1}""}""}""}""}""}""
'2'



Multi-line expressions and comments: In Python 3.11, f-string expressions
must be defined in a single line, even if the expression within the f-string
could normally span multiple lines
(like literal lists being defined over multiple lines),
making them harder to read. In Python 3.12 you can now define f-strings
spanning multiple lines, and add inline comments:
>>> f""This is the playlist: {"", "".join([
...     'Take me back to Eden',  # My, my, those eyes like fire
...     'Alkaline',              # Not acid nor alkaline
...     'Ascensionism'           # Take to the broken skies at last
... ])}""
'This is the playlist: Take me back to Eden, Alkaline, Ascensionism'



Backslashes and unicode characters: before Python 3.12 f-string expressions
couldn’t contain any \ character. This also affected unicode escape
sequences (such as \N{snowman}) as these contain
the \N part that previously could not be part of expression components of
f-strings. Now, you can define expressions like this:
>>> print(f""This is the playlist: {""\n"".join(songs)}"")
This is the playlist: Take me back to Eden
Alkaline
Ascensionism
>>> print(f""This is the playlist: {""\N{BLACK HEART SUIT}"".join(songs)}"")
This is the playlist: Take me back to Eden♥Alkaline♥Ascensionism




See PEP 701 for more details.
As a positive side-effect of how this feature has been implemented (by parsing f-strings
with the PEG parser), now error messages for f-strings are more precise
and include the exact location of the error. For example, in Python 3.11, the following
f-string raises a SyntaxError:
>>> my_string = f""{x z y}"" + f""{1 + 1}""
  File ""<stdin>"", line 1
    (x z y)
     ^^^
SyntaxError: f-string: invalid syntax. Perhaps you forgot a comma?


but the error message doesn’t include the exact location of the error within the line and
also has the expression artificially surrounded by parentheses. In Python 3.12, as f-strings
are parsed with the PEG parser, error messages can be more precise and show the entire line:
>>> my_string = f""{x z y}"" + f""{1 + 1}""
  File ""<stdin>"", line 1
    my_string = f""{x z y}"" + f""{1 + 1}""
                   ^^^
SyntaxError: invalid syntax. Perhaps you forgot a comma?


(Contributed by Pablo Galindo, Batuhan Taskaya, Lysandros Nikolaou, Cristián
Maureira-Fredes and Marta Gómez in gh-102856. PEP written by Pablo Galindo,
Batuhan Taskaya, Lysandros Nikolaou and Marta Gómez).",new-features - pep-701-syntactic-formalization-of-f-strings,">>> songs = ['Take me back to Eden', 'Alkaline', 'Ascensionism']
>>> f""This is the playlist: {"", "".join(songs)}""
'This is the playlist: Take me back to Eden, Alkaline, Ascensionism'
",3.12
,new-features - pep-701-syntactic-formalization-of-f-strings,">>> f""""""{f'''{f'{f""{1+1}""}'}'''}""""""
'2'
",3.12
,new-features - pep-701-syntactic-formalization-of-f-strings,">>> f""{f""{f""{f""{f""{f""{1+1}""}""}""}""}""}""
'2'
",3.12
,new-features - pep-701-syntactic-formalization-of-f-strings,">>> f""This is the playlist: {"", "".join([
...     'Take me back to Eden',  # My, my, those eyes like fire
...     'Alkaline',              # Not acid nor alkaline
...     'Ascensionism'           # Take to the broken skies at last
... ])}""
'This is the playlist: Take me back to Eden, Alkaline, Ascensionism'
",3.12
,new-features - pep-701-syntactic-formalization-of-f-strings,">>> print(f""This is the playlist: {""\n"".join(songs)}"")
This is the playlist: Take me back to Eden
Alkaline
Ascensionism
>>> print(f""This is the playlist: {""\N{BLACK HEART SUIT}"".join(songs)}"")
This is the playlist: Take me back to Eden♥Alkaline♥Ascensionism
",3.12
,new-features - pep-701-syntactic-formalization-of-f-strings,">>> my_string = f""{x z y}"" + f""{1 + 1}""
  File ""<stdin>"", line 1
    (x z y)
     ^^^
SyntaxError: f-string: invalid syntax. Perhaps you forgot a comma?
",3.12
,new-features - pep-701-syntactic-formalization-of-f-strings,">>> my_string = f""{x z y}"" + f""{1 + 1}""
  File ""<stdin>"", line 1
    my_string = f""{x z y}"" + f""{1 + 1}""
                   ^^^
SyntaxError: invalid syntax. Perhaps you forgot a comma?
",3.12
"PEP 684: A Per-Interpreter GIL

PEP 684 introduces a per-interpreter GIL,
so that sub-interpreters may now be created with a unique GIL per interpreter.
This allows Python programs to take full advantage of multiple CPU
cores. This is currently only available through the C-API,
though a Python API is anticipated for 3.13.
Use the new Py_NewInterpreterFromConfig() function to
create an interpreter with its own GIL:
PyInterpreterConfig config = {
    .check_multi_interp_extensions = 1,
    .gil = PyInterpreterConfig_OWN_GIL,
};
PyThreadState *tstate = NULL;
PyStatus status = Py_NewInterpreterFromConfig(&tstate, &config);
if (PyStatus_Exception(status)) {
    return -1;
}
/* The new interpreter is now active in the current thread. */


For further examples how to use the C-API for sub-interpreters with a
per-interpreter GIL, see Modules/_xxsubinterpretersmodule.c.
(Contributed by Eric Snow in gh-104210, etc.)",new-features - pep-684-a-per-interpreter-gil,"PyInterpreterConfig config = {
    .check_multi_interp_extensions = 1,
    .gil = PyInterpreterConfig_OWN_GIL,
};
PyThreadState *tstate = NULL;
PyStatus status = Py_NewInterpreterFromConfig(&tstate, &config);
if (PyStatus_Exception(status)) {
    return -1;
}
/* The new interpreter is now active in the current thread. */
",3.12
"Improved Error Messages


Modules from the standard library are now potentially suggested as part of
the error messages displayed by the interpreter when a NameError is
raised to the top level. (Contributed by Pablo Galindo in gh-98254.)
>>> sys.version_info
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
NameError: name 'sys' is not defined. Did you forget to import 'sys'?



Improve the error suggestion for NameError exceptions for instances.
Now if a NameError is raised in a method and the instance has an
attribute that’s exactly equal to the name in the exception, the suggestion
will include self.<NAME> instead of the closest match in the method
scope. (Contributed by Pablo Galindo in gh-99139.)
>>> class A:
...    def __init__(self):
...        self.blech = 1
...
...    def foo(self):
...        somethin = blech
...
>>> A().foo()
Traceback (most recent call last):
  File ""<stdin>"", line 1
    somethin = blech
               ^^^^^
NameError: name 'blech' is not defined. Did you mean: 'self.blech'?



Improve the SyntaxError error message when the user types import x
from y instead of from y import x. (Contributed by Pablo Galindo in gh-98931.)
>>> import a.y.z from b.y.z
Traceback (most recent call last):
  File ""<stdin>"", line 1
    import a.y.z from b.y.z
    ^^^^^^^^^^^^^^^^^^^^^^^
SyntaxError: Did you mean to use 'from ... import ...' instead?



ImportError exceptions raised from failed from <module> import
<name> statements now include suggestions for the value of <name> based on the
available names in <module>. (Contributed by Pablo Galindo in gh-91058.)
>>> from collections import chainmap
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ImportError: cannot import name 'chainmap' from 'collections'. Did you mean: 'ChainMap'?",new-features - improved-error-messages,">>> sys.version_info
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
NameError: name 'sys' is not defined. Did you forget to import 'sys'?
",3.12
,new-features - improved-error-messages,">>> class A:
...    def __init__(self):
...        self.blech = 1
...
...    def foo(self):
...        somethin = blech
...
>>> A().foo()
Traceback (most recent call last):
  File ""<stdin>"", line 1
    somethin = blech
               ^^^^^
NameError: name 'blech' is not defined. Did you mean: 'self.blech'?
",3.12
,new-features - improved-error-messages,">>> import a.y.z from b.y.z
Traceback (most recent call last):
  File ""<stdin>"", line 1
    import a.y.z from b.y.z
    ^^^^^^^^^^^^^^^^^^^^^^^
SyntaxError: Did you mean to use 'from ... import ...' instead?
",3.12
,new-features - improved-error-messages,">>> from collections import chainmap
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ImportError: cannot import name 'chainmap' from 'collections'. Did you mean: 'ChainMap'?
",3.12
"PEP 692: Using TypedDict for more precise **kwargs typing

Typing **kwargs in a function signature as introduced by PEP 484 allowed
for valid annotations only in cases where all of the **kwargs were of the
same type.
PEP 692 specifies a more precise way of typing **kwargs by relying on
typed dictionaries:
from typing import TypedDict, Unpack

class Movie(TypedDict):
  name: str
  year: int

def foo(**kwargs: Unpack[Movie]): ...


See PEP 692 for more details.
(Contributed by Franek Magiera in gh-103629.)",new-features-related-to-type-hints - pep-692-using-typeddict-for-more-precise-kwargs-typing,"from typing import TypedDict, Unpack

class Movie(TypedDict):
  name: str
  year: int

def foo(**kwargs: Unpack[Movie]): ...
",3.12
"PEP 698: Override Decorator for Static Typing

A new decorator typing.override() has been added to the typing
module. It indicates to type checkers that the method is intended to override
a method in a superclass. This allows type checkers to catch mistakes where
a method that is intended to override something in a base class
does not in fact do so.
Example:
from typing import override

class Base:
  def get_color(self) -> str:
    return ""blue""

class GoodChild(Base):
  @override  # ok: overrides Base.get_color
  def get_color(self) -> str:
    return ""yellow""

class BadChild(Base):
  @override  # type checker error: does not override Base.get_color
  def get_colour(self) -> str:
    return ""red""


See PEP 698 for more details.
(Contributed by Steven Troxler in gh-101561.)",new-features-related-to-type-hints - pep-698-override-decorator-for-static-typing,"from typing import override

class Base:
  def get_color(self) -> str:
    return ""blue""

class GoodChild(Base):
  @override  # ok: overrides Base.get_color
  def get_color(self) -> str:
    return ""yellow""

class BadChild(Base):
  @override  # type checker error: does not override Base.get_color
  def get_colour(self) -> str:
    return ""red""
",3.12
"typing


isinstance() checks against
runtime-checkable protocols now use
inspect.getattr_static() rather than hasattr() to lookup whether
attributes exist. This means that descriptors and __getattr__()
methods are no longer unexpectedly evaluated during isinstance() checks
against runtime-checkable protocols. However, it may also mean that some
objects which used to be considered instances of a runtime-checkable protocol
may no longer be considered instances of that protocol on Python 3.12+, and
vice versa. Most users are unlikely to be affected by this change.
(Contributed by Alex Waygood in gh-102433.)
The members of a runtime-checkable protocol are now considered “frozen” at
runtime as soon as the class has been created. Monkey-patching attributes
onto a runtime-checkable protocol will still work, but will have no impact on
isinstance() checks comparing objects to the protocol. For example:
>>> from typing import Protocol, runtime_checkable
>>> @runtime_checkable
... class HasX(Protocol):
...     x = 1
...
>>> class Foo: ...
...
>>> f = Foo()
>>> isinstance(f, HasX)
False
>>> f.x = 1
>>> isinstance(f, HasX)
True
>>> HasX.y = 2
>>> isinstance(f, HasX)  # unchanged, even though HasX now also has a ""y"" attribute
True


This change was made in order to speed up isinstance() checks against
runtime-checkable protocols.

The performance profile of isinstance() checks against
runtime-checkable protocols has changed
significantly. Most isinstance() checks against protocols with only a few
members should be at least 2x faster than in 3.11, and some may be 20x
faster or more. However, isinstance() checks against protocols with many
members may be slower than in Python 3.11. (Contributed by Alex
Waygood in gh-74690 and gh-103193.)
All typing.TypedDict and typing.NamedTuple classes now have the
__orig_bases__ attribute. (Contributed by Adrian Garcia Badaracco in
gh-103699.)
Add frozen_default parameter to typing.dataclass_transform().
(Contributed by Erik De Bonte in gh-99957.)",improved-modules - typing,">>> from typing import Protocol, runtime_checkable
>>> @runtime_checkable
... class HasX(Protocol):
...     x = 1
...
>>> class Foo: ...
...
>>> f = Foo()
>>> isinstance(f, HasX)
False
>>> f.x = 1
>>> isinstance(f, HasX)
True
>>> HasX.y = 2
>>> isinstance(f, HasX)  # unchanged, even though HasX now also has a ""y"" attribute
True
",3.12
"unittest

Add a --durations command line option, showing the N slowest test cases:
python3 -m unittest --durations=3 lib.tests.test_threading
.....
Slowest test durations
----------------------------------------------------------------------
1.210s     test_timeout (Lib.test.test_threading.BarrierTests)
1.003s     test_default_timeout (Lib.test.test_threading.BarrierTests)
0.518s     test_timeout (Lib.test.test_threading.EventTests)

(0.000 durations hidden.  Use -v to show these durations.)
----------------------------------------------------------------------
Ran 158 tests in 9.869s

OK (skipped=3)


(Contributed by Giampaolo Rodola in gh-48330)",improved-modules - unittest,"python3 -m unittest --durations=3 lib.tests.test_threading
.....
Slowest test durations
----------------------------------------------------------------------
1.210s     test_timeout (Lib.test.test_threading.BarrierTests)
1.003s     test_default_timeout (Lib.test.test_threading.BarrierTests)
0.518s     test_timeout (Lib.test.test_threading.EventTests)

(0.000 durations hidden.  Use -v to show these durations.)
----------------------------------------------------------------------
Ran 158 tests in 9.869s

OK (skipped=3)
",3.12
"imp


The imp module has been removed.  (Contributed by Barry Warsaw in
gh-98040.)
To migrate, consult the following correspondence table:



imp
importlib



imp.NullImporter
Insert None into sys.path_importer_cache

imp.cache_from_source()
importlib.util.cache_from_source()

imp.find_module()
importlib.util.find_spec()

imp.get_magic()
importlib.util.MAGIC_NUMBER

imp.get_suffixes()
importlib.machinery.SOURCE_SUFFIXES, importlib.machinery.EXTENSION_SUFFIXES, and importlib.machinery.BYTECODE_SUFFIXES

imp.get_tag()
sys.implementation.cache_tag

imp.load_module()
importlib.import_module()

imp.new_module(name)
types.ModuleType(name)

imp.reload()
importlib.reload()

imp.source_from_cache()
importlib.util.source_from_cache()

imp.load_source()
See below




Replace imp.load_source() with:
import importlib.util
import importlib.machinery

def load_source(modname, filename):
    loader = importlib.machinery.SourceFileLoader(modname, filename)
    spec = importlib.util.spec_from_file_location(modname, filename, loader=loader)
    module = importlib.util.module_from_spec(spec)
    # The module is always executed and not cached in sys.modules.
    # Uncomment the following line to cache the module.
    # sys.modules[module.__name__] = module
    loader.exec_module(module)
    return module



Remove imp functions and attributes with no replacements:

Undocumented functions:

imp.init_builtin()
imp.load_compiled()
imp.load_dynamic()
imp.load_package()


imp.lock_held(), imp.acquire_lock(), imp.release_lock():
the locking scheme has changed in Python 3.3 to per-module locks.
imp.find_module() constants: SEARCH_ERROR, PY_SOURCE,
PY_COMPILED, C_EXTENSION, PY_RESOURCE, PKG_DIRECTORY,
C_BUILTIN, PY_FROZEN, PY_CODERESOURCE, IMP_HOOK.",removed - imp,"import importlib.util
import importlib.machinery

def load_source(modname, filename):
    loader = importlib.machinery.SourceFileLoader(modname, filename)
    spec = importlib.util.spec_from_file_location(modname, filename, loader=loader)
    module = importlib.util.module_from_spec(spec)
    # The module is always executed and not cached in sys.modules.
    # Uncomment the following line to cache the module.
    # sys.modules[module.__name__] = module
    loader.exec_module(module)
    return module
",3.12
"Changes in the Python API


More strict rules are now applied for numerical group references and
group names in regular expressions.
Only sequence of ASCII digits is now accepted as a numerical reference.
The group name in bytes patterns and replacement strings can now only
contain ASCII letters and digits and underscore.
(Contributed by Serhiy Storchaka in gh-91760.)
Remove randrange() functionality deprecated since Python 3.10.  Formerly,
randrange(10.0) losslessly converted to randrange(10). Now, it raises a
TypeError. Also, the exception raised for non-integer values such as
randrange(10.5) or randrange('10') has been changed from ValueError to
TypeError.  This also prevents bugs where randrange(1e25) would silently
select from a larger range than randrange(10**25).
(Originally suggested by Serhiy Storchaka gh-86388.)
argparse.ArgumentParser changed encoding and error handler
for reading arguments from file (e.g. fromfile_prefix_chars option)
from default text encoding (e.g. locale.getpreferredencoding(False))
to filesystem encoding and error handler.
Argument files should be encoded in UTF-8 instead of ANSI Codepage on Windows.
Remove the asyncore-based smtpd module deprecated in Python 3.4.7
and 3.5.4.  A recommended replacement is the
asyncio-based aiosmtpd PyPI module.
shlex.split(): Passing None for s argument now raises an
exception, rather than reading sys.stdin. The feature was deprecated
in Python 3.9.
(Contributed by Victor Stinner in gh-94352.)
The os module no longer accepts bytes-like paths, like
bytearray and memoryview types: only the exact
bytes type is accepted for bytes strings.
(Contributed by Victor Stinner in gh-98393.)
syslog.openlog() and syslog.closelog() now fail if used in subinterpreters.
syslog.syslog() may still be used in subinterpreters,
but now only if syslog.openlog() has already been called in the main interpreter.
These new restrictions do not apply to the main interpreter,
so only a very small set of users might be affected.
This change helps with interpreter isolation.  Furthermore, syslog is a wrapper
around process-global resources, which are best managed from the main interpreter.
(Contributed by Donghee Na in gh-99127.)
The undocumented locking behavior of cached_property()
is removed, because it locked across all instances of the class, leading to high
lock contention. This means that a cached property getter function could now run
more than once for a single instance, if two threads race. For most simple
cached properties (e.g. those that are idempotent and simply calculate a value
based on other attributes of the instance) this will be fine.  If
synchronization is needed, implement locking within the cached property getter
function or around multi-threaded access points.
sys._current_exceptions() now returns a mapping from thread-id to an
exception instance, rather than to a (typ, exc, tb) tuple.
(Contributed by Irit Katriel in gh-103176.)
When extracting tar files using tarfile or
shutil.unpack_archive(), pass the filter argument to limit features
that may be surprising or dangerous.
See Extraction filters for details.
The output of the tokenize.tokenize() and tokenize.generate_tokens()
functions is now changed due to the changes introduced in PEP 701. This
means that STRING tokens are not emitted any more for f-strings and the
tokens described in PEP 701 are now produced instead: FSTRING_START,
FSTRING_MIDDLE and FSTRING_END are now emitted for f-string “string”
parts in addition to the appropriate tokens for the tokenization in the
expression components. For example for the f-string f""start {1+1} end""
the old version of the tokenizer emitted:
1,0-1,18:           STRING         'f""start {1+1} end""'


while the new version emits:
1,0-1,2:            FSTRING_START  'f""'
1,2-1,8:            FSTRING_MIDDLE 'start '
1,8-1,9:            OP             '{'
1,9-1,10:           NUMBER         '1'
1,10-1,11:          OP             '+'
1,11-1,12:          NUMBER         '1'
1,12-1,13:          OP             '}'
1,13-1,17:          FSTRING_MIDDLE ' end'
1,17-1,18:          FSTRING_END    '""'


Additionally, there may be some minor behavioral changes as a consequence of the
changes required to support PEP 701. Some of these changes include:

The type attribute of the tokens emitted when tokenizing some invalid Python
characters such as ! has changed from ERRORTOKEN to OP.
Incomplete single-line strings now also raise tokenize.TokenError as incomplete
multiline strings do.
Some incomplete or invalid Python code now raises tokenize.TokenError instead of
returning arbitrary ERRORTOKEN tokens when tokenizing it.
Mixing tabs and spaces as indentation in the same file is not supported anymore and will
raise a TabError.


The threading module now expects the _thread module to have
an _is_main_interpreter attribute.  It is a function with no
arguments that returns True if the current interpreter is the
main interpreter.
Any library or application that provides a custom _thread module
should provide _is_main_interpreter().
(See gh-112826.)",porting-to-python-3-12 - changes-in-the-python-api,"1,0-1,18:           STRING         'f""start {1+1} end""'
",3.12
,porting-to-python-3-12 - changes-in-the-python-api,"1,0-1,2:            FSTRING_START  'f""'
1,2-1,8:            FSTRING_MIDDLE 'start '
1,8-1,9:            OP             '{'
1,9-1,10:           NUMBER         '1'
1,10-1,11:          OP             '+'
1,11-1,12:          NUMBER         '1'
1,12-1,13:          OP             '}'
1,13-1,17:          FSTRING_MIDDLE ' end'
1,17-1,18:          FSTRING_END    '""'
",3.12
"Improved error messages


The interpreter now uses color by default when displaying tracebacks in the
terminal. This feature can be controlled
via the new PYTHON_COLORS environment variable as well as
the canonical NO_COLOR and FORCE_COLOR environment variables.
(Contributed by Pablo Galindo Salgado in gh-112730.)


A common mistake is to write a script with the same name as a
standard library module. When this results in errors, we now
display a more helpful error message:
$ python random.py
Traceback (most recent call last):
  File ""/home/me/random.py"", line 1, in <module>
    import random
  File ""/home/me/random.py"", line 3, in <module>
    print(random.randint(5))
          ^^^^^^^^^^^^^^
AttributeError: module 'random' has no attribute 'randint' (consider renaming '/home/me/random.py' since it has the same name as the standard library module named 'random' and the import system gives it precedence)


Similarly, if a script has the same name as a third-party
module that it attempts to import and this results in errors,
we also display a more helpful error message:
$ python numpy.py
Traceback (most recent call last):
  File ""/home/me/numpy.py"", line 1, in <module>
    import numpy as np
  File ""/home/me/numpy.py"", line 3, in <module>
    np.array([1, 2, 3])
    ^^^^^^^^
AttributeError: module 'numpy' has no attribute 'array' (consider renaming '/home/me/numpy.py' if it has the same name as a third-party module you intended to import)


(Contributed by Shantanu Jain in gh-95754.)

The error message now tries to suggest the correct keyword argument
when an incorrect keyword argument is passed to a function.
>>> ""Better error messages!"".split(max_split=1)
Traceback (most recent call last):
  File ""<python-input-0>"", line 1, in <module>
    ""Better error messages!"".split(max_split=1)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
TypeError: split() got an unexpected keyword argument 'max_split'. Did you mean 'maxsplit'?


(Contributed by Pablo Galindo Salgado and Shantanu Jain in gh-107944.)",new-features - improved-error-messages,"$ python random.py
Traceback (most recent call last):
  File ""/home/me/random.py"", line 1, in <module>
    import random
  File ""/home/me/random.py"", line 3, in <module>
    print(random.randint(5))
          ^^^^^^^^^^^^^^
AttributeError: module 'random' has no attribute 'randint' (consider renaming '/home/me/random.py' since it has the same name as the standard library module named 'random' and the import system gives it precedence)
",3.13
,new-features - improved-error-messages,"$ python numpy.py
Traceback (most recent call last):
  File ""/home/me/numpy.py"", line 1, in <module>
    import numpy as np
  File ""/home/me/numpy.py"", line 3, in <module>
    np.array([1, 2, 3])
    ^^^^^^^^
AttributeError: module 'numpy' has no attribute 'array' (consider renaming '/home/me/numpy.py' if it has the same name as a third-party module you intended to import)
",3.13
,new-features - improved-error-messages,">>> ""Better error messages!"".split(max_split=1)
Traceback (most recent call last):
  File ""<python-input-0>"", line 1, in <module>
    ""Better error messages!"".split(max_split=1)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
TypeError: split() got an unexpected keyword argument 'max_split'. Did you mean 'maxsplit'?
",3.13
"Other Language Changes


The compiler now strips common leading whitespace
from every line in a docstring.
This reduces the size of the bytecode cache
(such as .pyc files), with reductions in file size of around 5%,
for example in sqlalchemy.orm.session from SQLAlchemy 2.0.
This change affects tools that use docstrings, such as doctest.
>>> def spam():
...     """"""
...         This is a docstring with
...           leading whitespace.
...
...         It even has multiple paragraphs!
...     """"""
...
>>> spam.__doc__
'\nThis is a docstring with\n  leading whitespace.\n\nIt even has multiple paragraphs!\n'


(Contributed by Inada Naoki in gh-81283.)

Annotation scopes within class scopes
can now contain lambdas and comprehensions.
Comprehensions that are located within class scopes
are not inlined into their parent scope.
class C[T]:
    type Alias = lambda: T


(Contributed by Jelle Zijlstra in gh-109118 and gh-118160.)

Future statements are no longer triggered by
relative imports of the __future__ module,
meaning that statements of the form from .__future__ import ...
are now simply standard relative imports, with no special features activated.
(Contributed by Jeremiah Gabriel Pascual in gh-118216.)
global declarations are now permitted in except blocks
when that global is used in the else block.
Previously this raised an erroneous SyntaxError.
(Contributed by Irit Katriel in gh-111123.)
Add PYTHON_FROZEN_MODULES, a new environment variable that
determines whether frozen modules are ignored by the import machinery,
equivalent to the -X frozen_modules command-line option.
(Contributed by Yilei Yang in gh-111374.)
Add support for the perf profiler working
without frame pointers through
the new environment variable PYTHON_PERF_JIT_SUPPORT
and command-line option -X perf_jit.
(Contributed by Pablo Galindo in gh-118518.)
The location of a .python_history file can be changed via the
new PYTHON_HISTORY environment variable.
(Contributed by Levi Sabah, Zackery Spytz and Hugo van Kemenade
in gh-73965.)
Classes have a new __static_attributes__ attribute.
This is populated by the compiler with a tuple of the class’s attribute names
which are assigned through self.<name> from any function in its body.
(Contributed by Irit Katriel in gh-115775.)
The compiler now creates a __firstlineno__ attribute on classes
with the line number of the first line of the class definition.
(Contributed by Serhiy Storchaka in gh-118465.)
The exec() and eval() builtins now accept
the globals and locals arguments as keywords.
(Contributed by Raphael Gaschignard in gh-105879)
The compile() builtin now accepts a new flag,
ast.PyCF_OPTIMIZED_AST, which is similar to ast.PyCF_ONLY_AST
except that the returned AST is optimized according to
the value of the optimize argument.
(Contributed by Irit Katriel in gh-108113).
Add a __name__ attribute on property objects.
(Contributed by Eugene Toder in gh-101860.)
Add PythonFinalizationError, a new exception derived from
RuntimeError and used to signal when operations are blocked
during finalization.
The following callables now raise PythonFinalizationError,
instead of RuntimeError:

_thread.start_new_thread()
os.fork()
os.forkpty()
subprocess.Popen

(Contributed by Victor Stinner in gh-114570.)

Allow the count argument of str.replace() to be a keyword.
(Contributed by Hugo van Kemenade in gh-106487.)
Many functions now emit a warning if a boolean value is passed as
a file descriptor argument.
This can help catch some errors earlier.
(Contributed by Serhiy Storchaka in gh-82626.)
Added name and mode attributes
for compressed and archived file-like objects in
the bz2, lzma, tarfile, and zipfile modules.
(Contributed by Serhiy Storchaka in gh-115961.)",other-language-changes,">>> def spam():
...     """"""
...         This is a docstring with
...           leading whitespace.
...
...         It even has multiple paragraphs!
...     """"""
...
>>> spam.__doc__
'\nThis is a docstring with\n  leading whitespace.\n\nIt even has multiple paragraphs!\n'
",3.13
,other-language-changes,"class C[T]:
    type Alias = lambda: T
",3.13
"ssl


The create_default_context() API now includes
VERIFY_X509_PARTIAL_CHAIN and VERIFY_X509_STRICT
in its default flags.

Note
VERIFY_X509_STRICT may reject pre-RFC 5280
or malformed certificates that the underlying OpenSSL implementation
might otherwise accept.
Whilst disabling this is not recommended, you can do so using:
import ssl

ctx = ssl.create_default_context()
ctx.verify_flags &= ~ssl.VERIFY_X509_STRICT



(Contributed by William Woodruff in gh-112389.)",improved-modules - ssl,"import ssl

ctx = ssl.create_default_context()
ctx.verify_flags &= ~ssl.VERIFY_X509_STRICT
",3.13
"PEP 594: Remove “dead batteries” from the standard library

PEP 594 proposed removing 19 modules from the standard library,
colloquially referred to as ‘dead batteries’ due to their
historic, obsolete, or insecure status.
All of the following modules were deprecated in Python 3.11,
and are now removed:

aifc
audioop
chunk
cgi and cgitb

cgi.FieldStorage can typically be replaced with
urllib.parse.parse_qsl() for GET and HEAD requests,
and the email.message module or the multipart library
for POST and PUT requests.
cgi.parse() can be replaced by calling
urllib.parse.parse_qs() directly on the desired query string,
unless the input is multipart/form-data,
which should be replaced as described below for cgi.parse_multipart().
cgi.parse_header() can be replaced with the functionality
in the email package, which implements the same MIME RFCs.
For example, with email.message.EmailMessage:
from email.message import EmailMessage

msg = EmailMessage()
msg['content-type'] = 'application/json; charset=""utf8""'
main, params = msg.get_content_type(), msg['content-type'].params



cgi.parse_multipart() can be replaced with the functionality
in the email package, which implements the same MIME RFCs,
or with the multipart library.
For example, the email.message.EmailMessage
and email.message.Message classes.


crypt and the private _crypt extension.
The hashlib module may be an appropriate replacement
when simply hashing a value is required.
Otherwise, various third-party libraries on PyPI are available:

bcrypt:
Modern password hashing for your software and your servers.
passlib:
Comprehensive password hashing framework supporting over 30 schemes.
argon2-cffi:
The secure Argon2 password hashing algorithm.
legacycrypt:
ctypes wrapper to the POSIX crypt library call
and associated functionality.
crypt_r:
Fork of the crypt module,
wrapper to the crypt_r(3) library call
and associated functionality.


imghdr:
The filetype, puremagic, or python-magic libraries
should be used as replacements.
For example, the puremagic.what() function can be used
to replace the imghdr.what() function for all file formats
that were supported by imghdr.
mailcap:
Use the mimetypes module instead.
msilib
nis
nntplib:
Use the nntplib library from PyPI instead.
ossaudiodev:
For audio playback, use the pygame library from PyPI instead.
pipes:
Use the subprocess module instead.
sndhdr:
The filetype, puremagic, or python-magic libraries
should be used as replacements.
spwd:
Use the python-pam library from PyPI instead.
sunau
telnetlib,
Use the telnetlib3 or Exscript libraries from PyPI instead.
uu:
Use the base64 module instead, as a modern alternative.
xdrlib

(Contributed by Victor Stinner and Zachary Ware in gh-104773 and gh-104780.)",removed-modules-and-apis - pep-594-remove-dead-batteries-from-the-standard-library,"from email.message import EmailMessage

msg = EmailMessage()
msg['content-type'] = 'application/json; charset=""utf8""'
main, params = msg.get_content_type(), msg['content-type'].params
",3.13
"Removed C APIs


Remove several functions, macros, variables, etc
with names prefixed by _Py or _PY (which are considered private).
If your project is affected  by one of these removals
and you believe that the removed API should remain available,
please open a new issue to request a public C API
and add cc: @vstinner to the issue to notify Victor Stinner.
(Contributed by Victor Stinner in gh-106320.)
Remove old buffer protocols deprecated in Python 3.0.
Use Buffer Protocol instead.

PyObject_CheckReadBuffer():
Use PyObject_CheckBuffer() to test
whether the object supports the buffer protocol.
Note that PyObject_CheckBuffer() doesn’t guarantee
that PyObject_GetBuffer() will succeed.
To test if the object is actually readable,
see the next example of PyObject_GetBuffer().
PyObject_AsCharBuffer(), PyObject_AsReadBuffer():
Use PyObject_GetBuffer() and PyBuffer_Release() instead:
Py_buffer view;
if (PyObject_GetBuffer(obj, &view, PyBUF_SIMPLE) < 0) {
    return NULL;
}
// Use `view.buf` and `view.len` to read from the buffer.
// You may need to cast buf as `(const char*)view.buf`.
PyBuffer_Release(&view);



PyObject_AsWriteBuffer():
Use PyObject_GetBuffer() and PyBuffer_Release() instead:
Py_buffer view;
if (PyObject_GetBuffer(obj, &view, PyBUF_WRITABLE) < 0) {
    return NULL;
}
// Use `view.buf` and `view.len` to write to the buffer.
PyBuffer_Release(&view);




(Contributed by Inada Naoki in gh-85275.)

Remove various functions deprecated in Python 3.9:

PyEval_CallObject(), PyEval_CallObjectWithKeywords():
Use PyObject_CallNoArgs() or PyObject_Call() instead.

Warning
In PyObject_Call(), positional arguments must be a tuple
and must not be NULL,
and keyword arguments must be a dict or NULL,
whereas the removed functions checked argument types
and accepted NULL positional and keyword arguments.
To replace PyEval_CallObjectWithKeywords(func, NULL, kwargs) with
PyObject_Call(),
pass an empty tuple as positional arguments using
PyTuple_New(0).


PyEval_CallFunction():
Use PyObject_CallFunction() instead.
PyEval_CallMethod():
Use PyObject_CallMethod() instead.
PyCFunction_Call():
Use PyObject_Call() instead.

(Contributed by Victor Stinner in gh-105107.)

Remove the following old functions to configure the Python initialization,
deprecated in Python 3.11:

PySys_AddWarnOptionUnicode():
Use PyConfig.warnoptions instead.
PySys_AddWarnOption():
Use PyConfig.warnoptions instead.
PySys_AddXOption():
Use PyConfig.xoptions instead.
PySys_HasWarnOptions():
Use PyConfig.xoptions instead.
PySys_SetPath():
Set PyConfig.module_search_paths instead.
Py_SetPath():
Set PyConfig.module_search_paths instead.
Py_SetStandardStreamEncoding():
Set PyConfig.stdio_encoding instead,
and set also maybe PyConfig.legacy_windows_stdio (on Windows).
_Py_SetProgramFullPath():
Set PyConfig.executable instead.

Use the new PyConfig API of the Python Initialization
Configuration instead (PEP 587), added to Python 3.8.
(Contributed by Victor Stinner in gh-105145.)

Remove PyEval_AcquireLock() and PyEval_ReleaseLock() functions,
deprecated in Python 3.2.
They didn’t update the current thread state.
They can be replaced with:

PyEval_SaveThread() and PyEval_RestoreThread();
low-level PyEval_AcquireThread() and PyEval_RestoreThread();
or PyGILState_Ensure() and PyGILState_Release().

(Contributed by Victor Stinner in gh-105182.)

Remove the PyEval_ThreadsInitialized() function,
deprecated in Python 3.9.
Since Python 3.7, Py_Initialize() always creates the GIL:
calling PyEval_InitThreads() does nothing and
PyEval_ThreadsInitialized() always returns non-zero.
(Contributed by Victor Stinner in gh-105182.)
Remove the _PyInterpreterState_Get() alias to
PyInterpreterState_Get()
which was kept for backward compatibility with Python 3.8.
The pythoncapi-compat project can be used to get
PyInterpreterState_Get() on Python 3.8 and older.
(Contributed by Victor Stinner in gh-106320.)
Remove the private _PyObject_FastCall() function:
use PyObject_Vectorcall() which is available since Python 3.8
(PEP 590).
(Contributed by Victor Stinner in gh-106023.)
Remove the cpython/pytime.h header file,
which only contained private functions.
(Contributed by Victor Stinner in gh-106316.)
Remove the undocumented PY_TIMEOUT_MAX constant from the limited C API.
(Contributed by Victor Stinner in gh-110014.)
Remove the old trashcan macros Py_TRASHCAN_SAFE_BEGIN
and Py_TRASHCAN_SAFE_END.
Replace both with the new macros Py_TRASHCAN_BEGIN
and Py_TRASHCAN_END.
(Contributed by Irit Katriel in gh-105111.)",c-api-changes - removed-c-apis,"Py_buffer view;
if (PyObject_GetBuffer(obj, &view, PyBUF_SIMPLE) < 0) {
    return NULL;
}
// Use `view.buf` and `view.len` to read from the buffer.
// You may need to cast buf as `(const char*)view.buf`.
PyBuffer_Release(&view);
",3.13
,c-api-changes - removed-c-apis,"Py_buffer view;
if (PyObject_GetBuffer(obj, &view, PyBUF_WRITABLE) < 0) {
    return NULL;
}
// Use `view.buf` and `view.len` to write to the buffer.
PyBuffer_Release(&view);
",3.13
"Changes in the C API


Python.h no longer includes the <ieeefp.h> standard header. It was
included for the finite() function which is now provided by the
<math.h> header. It should now be included explicitly if needed. Remove
also the HAVE_IEEEFP_H macro.
(Contributed by Victor Stinner in gh-108765.)
Python.h no longer includes these standard header files: <time.h>,
<sys/select.h> and <sys/time.h>. If needed, they should now be
included explicitly. For example, <time.h> provides the clock() and
gmtime() functions, <sys/select.h> provides the select()
function, and <sys/time.h> provides the futimes(), gettimeofday()
and setitimer() functions.
(Contributed by Victor Stinner in gh-108765.)
On Windows, Python.h no longer includes the <stddef.h> standard
header file. If needed, it should now be included explicitly. For example, it
provides offsetof() function, and size_t and ptrdiff_t types.
Including <stddef.h> explicitly was already needed by all other
platforms, the HAVE_STDDEF_H macro is only defined on Windows.
(Contributed by Victor Stinner in gh-108765.)
If the Py_LIMITED_API macro is defined, Py_BUILD_CORE,
Py_BUILD_CORE_BUILTIN and Py_BUILD_CORE_MODULE macros
are now undefined by <Python.h>.
(Contributed by Victor Stinner in gh-85283.)
The old trashcan macros Py_TRASHCAN_SAFE_BEGIN and Py_TRASHCAN_SAFE_END
were removed. They should be replaced by the new macros Py_TRASHCAN_BEGIN
and Py_TRASHCAN_END.
A tp_dealloc function that has the old macros, such as:
static void
mytype_dealloc(mytype *p)
{
    PyObject_GC_UnTrack(p);
    Py_TRASHCAN_SAFE_BEGIN(p);
    ...
    Py_TRASHCAN_SAFE_END
}


should migrate to the new macros as follows:
static void
mytype_dealloc(mytype *p)
{
    PyObject_GC_UnTrack(p);
    Py_TRASHCAN_BEGIN(p, mytype_dealloc)
    ...
    Py_TRASHCAN_END
}


Note that Py_TRASHCAN_BEGIN has a second argument which
should be the deallocation function it is in. The new macros were
added in Python 3.8 and the old macros were deprecated in Python 3.11.
(Contributed by Irit Katriel in gh-105111.)



PEP 667 introduces several changes
to frame-related functions:

The effects of mutating the dictionary returned from
PyEval_GetLocals() in an optimized scope have changed.
New dict entries added this way will now only be visible to
subsequent PyEval_GetLocals() calls in that frame,
as PyFrame_GetLocals(), locals(),
and FrameType.f_locals no longer access
the same underlying cached dictionary.
Changes made to entries for actual variable names and names added via
the write-through proxy interfaces will be overwritten on subsequent calls
to PyEval_GetLocals() in that frame.
The recommended code update depends on how the function was being used,
so refer to the deprecation notice on the function for details.
Calling PyFrame_GetLocals() in an optimized scope
now returns a write-through proxy rather than a snapshot
that gets updated at ill-specified times.
If a snapshot is desired, it must be created explicitly
(e.g. with PyDict_Copy()),
or by calling the new PyEval_GetFrameLocals() API.
PyFrame_FastToLocals() and PyFrame_FastToLocalsWithError()
no longer have any effect.
Calling these functions has been redundant since Python 3.11,
when PyFrame_GetLocals() was first introduced.
PyFrame_LocalsToFast() no longer has any effect.
Calling this function is redundant now that PyFrame_GetLocals()
returns a write-through proxy for optimized scopes.",porting-to-python-3-13 - changes-in-the-c-api,"static void
mytype_dealloc(mytype *p)
{
    PyObject_GC_UnTrack(p);
    Py_TRASHCAN_SAFE_BEGIN(p);
    ...
    Py_TRASHCAN_SAFE_END
}
",3.13
,porting-to-python-3-13 - changes-in-the-c-api,"static void
mytype_dealloc(mytype *p)
{
    PyObject_GC_UnTrack(p);
    Py_TRASHCAN_BEGIN(p, mytype_dealloc)
    ...
    Py_TRASHCAN_END
}
",3.13
"PEP 649: Deferred Evaluation of Annotations

The annotations on functions, classes, and modules are no
longer evaluated eagerly. Instead, annotations are stored in special-purpose
annotate functions and evaluated only when
necessary. This is specified in PEP 649 and PEP 749.
This change is designed to make annotations in Python more performant and more
usable in most circumstances. The runtime cost for defining annotations is
minimized, but it remains possible to introspect annotations at runtime.
It is usually no longer necessary to enclose annotations in strings if they
contain forward references.
The new annotationlib module provides tools for inspecting deferred
annotations. Annotations may be evaluated in the VALUE
format (which evaluates annotations to runtime values, similar to the behavior in
earlier Python versions), the FORWARDREF format
(which replaces undefined names with special markers), and the
SOURCE format (which returns annotations as strings).
This example shows how these formats behave:
>>> from annotationlib import get_annotations, Format
>>> def func(arg: Undefined):
...     pass
>>> get_annotations(func, format=Format.VALUE)
Traceback (most recent call last):
  ...
NameError: name 'Undefined' is not defined
>>> get_annotations(func, format=Format.FORWARDREF)
{'arg': ForwardRef('Undefined')}
>>> get_annotations(func, format=Format.SOURCE)
{'arg': 'Undefined'}



Implications for annotated code

If you define annotations in your code (for example, for use with a static type
checker), then this change probably does not affect you: you can keep
writing annotations the same way you did with previous versions of Python.
You will likely be able to remove quoted strings in annotations, which are frequently
used for forward references. Similarly, if you use from __future__ import annotations
to avoid having to write strings in annotations, you may well be able to
remove that import. However, if you rely on third-party libraries that read annotations,
those libraries may need changes to support unquoted annotations before they
work as expected.


Implications for readers of __annotations__

If your code reads the __annotations__ attribute on objects, you may want
to make changes in order to support code that relies on deferred evaluation of
annotations. For example, you may want to use annotationlib.get_annotations()
with the FORWARDREF format, as the dataclasses
module now does.


Related changes

The changes in Python 3.14 are designed to rework how __annotations__
works at runtime while minimizing breakage to code that contains
annotations in source code and to code that reads __annotations__. However,
if you rely on undocumented details of the annotation behavior or on private
functions in the standard library, there are many ways in which your code may
not work in Python 3.14. To safeguard your code against future changes,
use only the documented functionality of the annotationlib module.


from __future__ import annotations

In Python 3.7, PEP 563 introduced the from __future__ import annotations
directive, which turns all annotations into strings. This directive is now
considered deprecated and it is expected to be removed in a future version of Python.
However, this removal will not happen until after Python 3.13, the last version of
Python without deferred evaluation of annotations, reaches its end of life.
In Python 3.14, the behavior of code using from __future__ import annotations
is unchanged.",new-features - pep-649-deferred-evaluation-of-annotations,">>> from annotationlib import get_annotations, Format
>>> def func(arg: Undefined):
...     pass
>>> get_annotations(func, format=Format.VALUE)
Traceback (most recent call last):
  ...
NameError: name 'Undefined' is not defined
>>> get_annotations(func, format=Format.FORWARDREF)
{'arg': ForwardRef('Undefined')}
>>> get_annotations(func, format=Format.SOURCE)
{'arg': 'Undefined'}
",3.14
"Improved Error Messages


When unpacking assignment fails due to incorrect number of variables, the
error message prints the received number of values in more cases than before.
(Contributed by Tushar Sadhwani in gh-122239.)
>>> x, y, z = 1, 2, 3, 4
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
    x, y, z = 1, 2, 3, 4
    ^^^^^^^
ValueError: too many values to unpack (expected 3, got 4)",new-features - improved-error-messages,">>> x, y, z = 1, 2, 3, 4
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
    x, y, z = 1, 2, 3, 4
    ^^^^^^^
ValueError: too many values to unpack (expected 3, got 4)
",3.14
