{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678710179", "pull_request_review_id": 2179143762, "id": 1678710179, "node_id": "PRRC_kwDOA-j9z85kDxWj", "diff_hunk": "@@ -1012,22 +1010,21 @@ struct HelperInterpBase {\n \n     // Rescale float values to int16\n     int16_t * data_i16 = (int16_t *) data_f64;\n-    auto aligned_interp_size = interp_size;\n \n     if (align_i32) {\n       // We should respect int32 alignment as we will load int16 data as int32\n       // See ImagingResampleHorizontalConvolution8u4x, mmk0 = _mm256_set1_epi32(*(int32_t*)&k[x]);\n-      // compute aligned_interp_size = nearest pair value to interp_size\n+      // compute aligned_interp_size = nearest pair value to aligned_interp_size\n       while (aligned_interp_size % sizeof(int32_t) != 0) {\n         aligned_interp_size += 1;\n       }\n       // assert that we wont go out of bounds\n-      TORCH_INTERNAL_ASSERT(aligned_interp_size * sizeof(int16_t) < interp_size * sizeof(double));\n+      TORCH_INTERNAL_ASSERT(aligned_interp_size * sizeof(int16_t) < aligned_interp_size * sizeof(double));", "path": "aten/src/ATen/native/cpu/UpSampleKernel.cpp", "commit_id": "28145dbfa40837551619e86601d31d64669b3e14", "original_commit_id": "dec2183bfb531ebe70979b3e77f1a1848c732944", "user": {"login": "janeyx99", "id": 31798555, "url": "https://api.github.com/users/janeyx99", "html_url": "https://github.com/janeyx99"}, "body": "This assert has now changed--previously, it would compare aligned_interp_size to the result of line 997, but now it is comparing it to itself.", "created_at": "2024-07-16T03:37:32Z", "updated_at": "2024-07-16T03:39:09Z", "html_url": "https://github.com/pytorch/pytorch/pull/130784#discussion_r1678710179", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130784", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678710179"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130784#discussion_r1678710179"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130784"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678710179/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": null, "original_line": 1022, "side": "RIGHT", "original_position": 33, "position": null, "subject_type": "line", "PR": {"title": "[structural binding][10/N] Replace std::tie with structural binding", "number": 130784, "id": 1970406760}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678710829", "pull_request_review_id": 2179143762, "id": 1678710829, "node_id": "PRRC_kwDOA-j9z85kDxgt", "diff_hunk": "@@ -1012,22 +1010,21 @@ struct HelperInterpBase {\n \n     // Rescale float values to int16\n     int16_t * data_i16 = (int16_t *) data_f64;\n-    auto aligned_interp_size = interp_size;\n \n     if (align_i32) {\n       // We should respect int32 alignment as we will load int16 data as int32\n       // See ImagingResampleHorizontalConvolution8u4x, mmk0 = _mm256_set1_epi32(*(int32_t*)&k[x]);\n-      // compute aligned_interp_size = nearest pair value to interp_size\n+      // compute aligned_interp_size = nearest pair value to aligned_interp_size\n       while (aligned_interp_size % sizeof(int32_t) != 0) {\n         aligned_interp_size += 1;\n       }\n       // assert that we wont go out of bounds\n-      TORCH_INTERNAL_ASSERT(aligned_interp_size * sizeof(int16_t) < interp_size * sizeof(double));\n+      TORCH_INTERNAL_ASSERT(aligned_interp_size * sizeof(int16_t) < aligned_interp_size * sizeof(double));\n     }\n \n     for (const auto j : c10::irange(output_size)) {\n-      for (const auto k : c10::irange(interp_size)) {\n-        double v = data_f64[j * interp_size + k] * (1 << weights_precision);\n+      for (const auto k : c10::irange(aligned_interp_size )) {\n+        double v = data_f64[j * aligned_interp_size + k] * (1 << weights_precision);", "path": "aten/src/ATen/native/cpu/UpSampleKernel.cpp", "commit_id": "28145dbfa40837551619e86601d31d64669b3e14", "original_commit_id": "dec2183bfb531ebe70979b3e77f1a1848c732944", "user": {"login": "janeyx99", "id": 31798555, "url": "https://api.github.com/users/janeyx99", "html_url": "https://github.com/janeyx99"}, "body": "Note sure if this is the issue causing CI to fail, but this should probably stay as interp_size, and in line 997 we should keep it as interp_size. And undelete 1015.", "created_at": "2024-07-16T03:38:43Z", "updated_at": "2024-07-16T03:39:09Z", "html_url": "https://github.com/pytorch/pytorch/pull/130784#discussion_r1678710829", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130784", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678710829"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130784#discussion_r1678710829"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130784"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678710829/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": null, "original_line": 1028, "side": "RIGHT", "original_position": 40, "position": null, "subject_type": "line", "PR": {"title": "[structural binding][10/N] Replace std::tie with structural binding", "number": 130784, "id": 1970406760}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678738970", "pull_request_review_id": 2179185843, "id": 1678738970, "node_id": "PRRC_kwDOA-j9z85kD4Ya", "diff_hunk": "@@ -1012,22 +1010,21 @@ struct HelperInterpBase {\n \n     // Rescale float values to int16\n     int16_t * data_i16 = (int16_t *) data_f64;\n-    auto aligned_interp_size = interp_size;\n \n     if (align_i32) {\n       // We should respect int32 alignment as we will load int16 data as int32\n       // See ImagingResampleHorizontalConvolution8u4x, mmk0 = _mm256_set1_epi32(*(int32_t*)&k[x]);\n-      // compute aligned_interp_size = nearest pair value to interp_size\n+      // compute aligned_interp_size = nearest pair value to aligned_interp_size\n       while (aligned_interp_size % sizeof(int32_t) != 0) {\n         aligned_interp_size += 1;\n       }\n       // assert that we wont go out of bounds\n-      TORCH_INTERNAL_ASSERT(aligned_interp_size * sizeof(int16_t) < interp_size * sizeof(double));\n+      TORCH_INTERNAL_ASSERT(aligned_interp_size * sizeof(int16_t) < aligned_interp_size * sizeof(double));", "path": "aten/src/ATen/native/cpu/UpSampleKernel.cpp", "commit_id": "28145dbfa40837551619e86601d31d64669b3e14", "original_commit_id": "dec2183bfb531ebe70979b3e77f1a1848c732944", "user": {"login": "cyyever", "id": 17618148, "url": "https://api.github.com/users/cyyever", "html_url": "https://github.com/cyyever"}, "body": "Previously, interp_size was assigned to aligned_interp_size in line 1015", "created_at": "2024-07-16T04:25:08Z", "updated_at": "2024-07-16T04:25:08Z", "html_url": "https://github.com/pytorch/pytorch/pull/130784#discussion_r1678738970", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130784", "author_association": "COLLABORATOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678738970"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130784#discussion_r1678738970"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130784"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678738970/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": null, "original_line": 1022, "side": "RIGHT", "in_reply_to_id": 1678710179, "original_position": 33, "position": null, "subject_type": "line", "PR": {"title": "[structural binding][10/N] Replace std::tie with structural binding", "number": 130784, "id": 1970406760}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678484971", "pull_request_review_id": 2178820283, "id": 1678484971, "node_id": "PRRC_kwDOA-j9z85kC6Xr", "diff_hunk": "@@ -300,7 +300,13 @@ def non_empty_tensor(x):\n         # runtime assert forcing u0 to be zero.  So if this hasn't happened,\n         # we know that the unbacked SymInt has appropriate size and there are\n         # no problems.\n-        return len(x.shape) != 1 or guard_size_oblivious(x.shape[0] > 0)\n+        if len(x.shape) == 1 and guard_size_oblivious(x.shape[0] == 0):\n+            return False\n+\n+        if dim < len(x.shape) and guard_size_oblivious(x.shape[dim] == 0):", "path": "torch/_inductor/decomposition.py", "commit_id": "f7f5533109e6ba5a8c7535c94667573a0277edab", "original_commit_id": "f7f5533109e6ba5a8c7535c94667573a0277edab", "user": {"login": "peterbell10", "id": 13238737, "url": "https://api.github.com/users/peterbell10", "html_url": "https://github.com/peterbell10"}, "body": "`guard_size_oblivious` implies an unbacked symint could get through this with a runtime value of 0, right? Would we still IMA in that case?", "created_at": "2024-07-15T22:57:39Z", "updated_at": "2024-07-15T22:57:50Z", "html_url": "https://github.com/pytorch/pytorch/pull/130763#discussion_r1678484971", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130763", "author_association": "COLLABORATOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678484971"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130763#discussion_r1678484971"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130763"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678484971/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 306, "original_line": 306, "side": "RIGHT", "original_position": 8, "position": 8, "subject_type": "line", "PR": {"title": "Update torch.cat decomp for 0-dim", "number": 130763, "id": 1970138110}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678542383", "pull_request_review_id": 2178892263, "id": 1678542383, "node_id": "PRRC_kwDOA-j9z85kDIYv", "diff_hunk": "@@ -300,7 +300,13 @@ def non_empty_tensor(x):\n         # runtime assert forcing u0 to be zero.  So if this hasn't happened,\n         # we know that the unbacked SymInt has appropriate size and there are\n         # no problems.\n-        return len(x.shape) != 1 or guard_size_oblivious(x.shape[0] > 0)\n+        if len(x.shape) == 1 and guard_size_oblivious(x.shape[0] == 0):\n+            return False\n+\n+        if dim < len(x.shape) and guard_size_oblivious(x.shape[dim] == 0):", "path": "torch/_inductor/decomposition.py", "commit_id": "f7f5533109e6ba5a8c7535c94667573a0277edab", "original_commit_id": "f7f5533109e6ba5a8c7535c94667573a0277edab", "user": {"login": "eellison", "id": 11477974, "url": "https://api.github.com/users/eellison", "html_url": "https://github.com/eellison"}, "body": "Yea, it's a good point, will do in follow up (we would still want this change anyway)", "created_at": "2024-07-16T00:08:54Z", "updated_at": "2024-07-16T00:08:54Z", "html_url": "https://github.com/pytorch/pytorch/pull/130763#discussion_r1678542383", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130763", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678542383"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130763#discussion_r1678542383"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130763"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678542383/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 306, "original_line": 306, "side": "RIGHT", "in_reply_to_id": 1678484971, "original_position": 8, "position": 8, "subject_type": "line", "PR": {"title": "Update torch.cat decomp for 0-dim", "number": 130763, "id": 1970138110}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678596035", "pull_request_review_id": 2178959524, "id": 1678596035, "node_id": "PRRC_kwDOA-j9z85kDVfD", "diff_hunk": "@@ -300,7 +300,13 @@ def non_empty_tensor(x):\n         # runtime assert forcing u0 to be zero.  So if this hasn't happened,\n         # we know that the unbacked SymInt has appropriate size and there are\n         # no problems.\n-        return len(x.shape) != 1 or guard_size_oblivious(x.shape[0] > 0)\n+        if len(x.shape) == 1 and guard_size_oblivious(x.shape[0] == 0):\n+            return False\n+\n+        if dim < len(x.shape) and guard_size_oblivious(x.shape[dim] == 0):", "path": "torch/_inductor/decomposition.py", "commit_id": "f7f5533109e6ba5a8c7535c94667573a0277edab", "original_commit_id": "f7f5533109e6ba5a8c7535c94667573a0277edab", "user": {"login": "eellison", "id": 11477974, "url": "https://api.github.com/users/eellison", "html_url": "https://github.com/eellison"}, "body": "The issue on master is we are ignoring the masked load and turning into a load. when I set [needs_where=True](https://github.com/pytorch/pytorch/blob/main/torch/_inductor/codegen/triton.py#L1031) the IMA goes away. cc @isuruf, coming from your chagne", "created_at": "2024-07-16T00:39:18Z", "updated_at": "2024-07-16T00:39:18Z", "html_url": "https://github.com/pytorch/pytorch/pull/130763#discussion_r1678596035", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130763", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678596035"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130763#discussion_r1678596035"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130763"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678596035/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 306, "original_line": 306, "side": "RIGHT", "in_reply_to_id": 1678484971, "original_position": 8, "position": 8, "subject_type": "line", "PR": {"title": "Update torch.cat decomp for 0-dim", "number": 130763, "id": 1970138110}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678147081", "pull_request_review_id": 2178248714, "id": 1678147081, "node_id": "PRRC_kwDOA-j9z85kBn4J", "diff_hunk": "@@ -429,15 +429,14 @@ def fx_graph_remote_cache_default():\n \n \n # The multiprocessing start method to use for inductor workers in the codecache.\n-# \"subprocess\", \"fork\", or \"spawn\"\n+# Can be \"subprocess\" or \"fork\".\n def decide_worker_start_method():\n     start_method = os.environ.get(\n         \"TORCHINDUCTOR_WORKER_START\", \"fork\" if is_fbcode() else \"subprocess\"\n     )\n     assert start_method in [", "path": "torch/_inductor/config.py", "commit_id": "8015d02756f6a758642605b21aeb2f46fe6a4428", "original_commit_id": "dbab50adc24cdfa4701e54673f7b885f9eed354c", "user": {"login": "Skylion007", "id": 2053727, "url": "https://api.github.com/users/Skylion007", "html_url": "https://github.com/Skylion007"}, "body": "nit should be a tuple instead of a list (more efficient)", "created_at": "2024-07-15T17:04:09Z", "updated_at": "2024-07-15T17:04:09Z", "html_url": "https://github.com/pytorch/pytorch/pull/130746#discussion_r1678147081", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130746", "author_association": "COLLABORATOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678147081"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130746#discussion_r1678147081"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130746"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678147081/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": null, "original_line": 437, "side": "RIGHT", "original_position": 10, "position": null, "subject_type": "line", "PR": {"title": "[inductor] Remove \"spawn\" as an option for parallel compile method", "number": 130746, "id": 1969847486}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678154699", "pull_request_review_id": 2178261538, "id": 1678154699, "node_id": "PRRC_kwDOA-j9z85kBpvL", "diff_hunk": "@@ -348,7 +348,7 @@ def debug_repr(self):\n     @staticmethod\n     def is_supported_arg(arg):\n         if isinstance(arg, variables.BuiltinVariable):\n-            return arg.fn in [list, tuple, dict]\n+            return arg.fn in [list, tuple, dict, set]", "path": "torch/_dynamo/variables/dicts.py", "commit_id": "1fc24aff0d7eb818d45270be7ed6bfb7feb58d4e", "original_commit_id": "4d1f40688b902a3a850f988deef7d62742f36948", "user": {"login": "XuehaiPan", "id": 16078332, "url": "https://api.github.com/users/XuehaiPan", "html_url": "https://github.com/XuehaiPan"}, "body": "```suggestion\r\n            return arg.fn in {list, tuple, dict, set}\r\n```", "created_at": "2024-07-15T17:11:47Z", "updated_at": "2024-07-15T17:11:48Z", "html_url": "https://github.com/pytorch/pytorch/pull/130745#discussion_r1678154699", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130745", "author_association": "COLLABORATOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678154699"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130745#discussion_r1678154699"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130745"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678154699/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": null, "original_line": 351, "side": "RIGHT", "original_position": 5, "position": null, "subject_type": "line", "PR": {"title": "dynamo add support for `defaultdict(set)`", "number": 130745, "id": 1969837657}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678156756", "pull_request_review_id": 2178264927, "id": 1678156756, "node_id": "PRRC_kwDOA-j9z85kBqPU", "diff_hunk": "@@ -348,7 +348,7 @@ def debug_repr(self):\n     @staticmethod\n     def is_supported_arg(arg):\n         if isinstance(arg, variables.BuiltinVariable):\n-            return arg.fn in [list, tuple, dict]\n+            return arg.fn in [list, tuple, dict, set]", "path": "torch/_dynamo/variables/dicts.py", "commit_id": "1fc24aff0d7eb818d45270be7ed6bfb7feb58d4e", "original_commit_id": "4d1f40688b902a3a850f988deef7d62742f36948", "user": {"login": "Skylion007", "id": 2053727, "url": "https://api.github.com/users/Skylion007", "html_url": "https://github.com/Skylion007"}, "body": "or even just:\r\n```suggestion\r\n            return arg.fn in (list, tuple, dict, set)\r\n```", "created_at": "2024-07-15T17:13:49Z", "updated_at": "2024-07-15T17:13:55Z", "html_url": "https://github.com/pytorch/pytorch/pull/130745#discussion_r1678156756", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130745", "author_association": "COLLABORATOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678156756"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130745#discussion_r1678156756"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130745"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678156756/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": null, "original_line": 351, "side": "RIGHT", "in_reply_to_id": 1678154699, "original_position": 5, "position": null, "subject_type": "line", "PR": {"title": "dynamo add support for `defaultdict(set)`", "number": 130745, "id": 1969837657}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678182419", "pull_request_review_id": 2178306096, "id": 1678182419, "node_id": "PRRC_kwDOA-j9z85kBwgT", "diff_hunk": "@@ -348,7 +348,7 @@ def debug_repr(self):\n     @staticmethod\n     def is_supported_arg(arg):\n         if isinstance(arg, variables.BuiltinVariable):\n-            return arg.fn in [list, tuple, dict]\n+            return arg.fn in [list, tuple, dict, set]", "path": "torch/_dynamo/variables/dicts.py", "commit_id": "1fc24aff0d7eb818d45270be7ed6bfb7feb58d4e", "original_commit_id": "4d1f40688b902a3a850f988deef7d62742f36948", "user": {"login": "alexcdennis", "id": 48365386, "url": "https://api.github.com/users/alexcdennis", "html_url": "https://github.com/alexcdennis"}, "body": "@XuehaiPan would you mind explaining the motivation for this change? the two seem synonymous to me", "created_at": "2024-07-15T17:38:15Z", "updated_at": "2024-07-15T17:38:15Z", "html_url": "https://github.com/pytorch/pytorch/pull/130745#discussion_r1678182419", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130745", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678182419"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130745#discussion_r1678182419"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130745"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678182419/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": null, "original_line": 351, "side": "RIGHT", "in_reply_to_id": 1678154699, "original_position": 5, "position": null, "subject_type": "line", "PR": {"title": "dynamo add support for `defaultdict(set)`", "number": 130745, "id": 1969837657}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678186981", "pull_request_review_id": 2178313064, "id": 1678186981, "node_id": "PRRC_kwDOA-j9z85kBxnl", "diff_hunk": "@@ -348,7 +348,7 @@ def debug_repr(self):\n     @staticmethod\n     def is_supported_arg(arg):\n         if isinstance(arg, variables.BuiltinVariable):\n-            return arg.fn in [list, tuple, dict]\n+            return arg.fn in [list, tuple, dict, set]", "path": "torch/_dynamo/variables/dicts.py", "commit_id": "1fc24aff0d7eb818d45270be7ed6bfb7feb58d4e", "original_commit_id": "4d1f40688b902a3a850f988deef7d62742f36948", "user": {"login": "Skylion007", "id": 2053727, "url": "https://api.github.com/users/Skylion007", "html_url": "https://github.com/Skylion007"}, "body": "Marginally better perf in either case", "created_at": "2024-07-15T17:42:28Z", "updated_at": "2024-07-15T17:42:28Z", "html_url": "https://github.com/pytorch/pytorch/pull/130745#discussion_r1678186981", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130745", "author_association": "COLLABORATOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678186981"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130745#discussion_r1678186981"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130745"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678186981/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": null, "original_line": 351, "side": "RIGHT", "in_reply_to_id": 1678154699, "original_position": 5, "position": null, "subject_type": "line", "PR": {"title": "dynamo add support for `defaultdict(set)`", "number": 130745, "id": 1969837657}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1677390483", "pull_request_review_id": 2176988886, "id": 1677390483, "node_id": "PRRC_kwDOA-j9z85j-vKT", "diff_hunk": "@@ -244,6 +244,12 @@ def run_command_line(cmd_line, cwd=None):\n     return status\n \n \n+def adapte_file_path(orig_path: str) -> str:", "path": "torch/_inductor/cpp_builder.py", "commit_id": "03e2b401a84ff72423a04f117499f32c78db80c3", "original_commit_id": "f356f467251efdf5c944d946b13b19839d81b9fb", "user": {"login": "jgong5", "id": 8359223, "url": "https://api.github.com/users/jgong5", "html_url": "https://github.com/jgong5"}, "body": "`normalize_path_separator` sounds more specific.", "created_at": "2024-07-15T07:05:59Z", "updated_at": "2024-07-15T07:08:30Z", "html_url": "https://github.com/pytorch/pytorch/pull/130713#discussion_r1677390483", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130713", "author_association": "COLLABORATOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1677390483"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130713#discussion_r1677390483"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130713"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1677390483/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": null, "original_line": 247, "side": "RIGHT", "original_position": 4, "position": null, "subject_type": "line", "PR": {"title": "[inductor] adapte windows file path", "number": 130713, "id": 1968748478}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1677390738", "pull_request_review_id": 2176988886, "id": 1677390738, "node_id": "PRRC_kwDOA-j9z85j-vOS", "diff_hunk": "@@ -244,6 +244,12 @@ def run_command_line(cmd_line, cwd=None):\n     return status\n \n \n+def adapte_file_path(orig_path: str) -> str:\n+    if _IS_WINDOWS:\n+        return orig_path.replace(\"\\\\\", \"/\")", "path": "torch/_inductor/cpp_builder.py", "commit_id": "03e2b401a84ff72423a04f117499f32c78db80c3", "original_commit_id": "f356f467251efdf5c944d946b13b19839d81b9fb", "user": {"login": "jgong5", "id": 8359223, "url": "https://api.github.com/users/jgong5", "html_url": "https://github.com/jgong5"}, "body": "```suggestion\r\n        return orig_path.replace(os.sep, \"/\")\r\n```", "created_at": "2024-07-15T07:06:16Z", "updated_at": "2024-07-15T07:08:30Z", "html_url": "https://github.com/pytorch/pytorch/pull/130713#discussion_r1677390738", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130713", "author_association": "COLLABORATOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1677390738"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130713#discussion_r1677390738"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130713"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1677390738/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": null, "original_line": 249, "side": "RIGHT", "original_position": 6, "position": null, "subject_type": "line", "PR": {"title": "[inductor] adapte windows file path", "number": 130713, "id": 1968748478}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1677395674", "pull_request_review_id": 2176998616, "id": 1677395674, "node_id": "PRRC_kwDOA-j9z85j-wba", "diff_hunk": "@@ -244,6 +244,12 @@ def run_command_line(cmd_line, cwd=None):\n     return status\n \n \n+def adapte_file_path(orig_path: str) -> str:", "path": "torch/_inductor/cpp_builder.py", "commit_id": "03e2b401a84ff72423a04f117499f32c78db80c3", "original_commit_id": "f356f467251efdf5c944d946b13b19839d81b9fb", "user": {"login": "xuhancn", "id": 8433590, "url": "https://api.github.com/users/xuhancn", "html_url": "https://github.com/xuhancn"}, "body": "Done.", "created_at": "2024-07-15T07:12:02Z", "updated_at": "2024-07-15T07:12:03Z", "html_url": "https://github.com/pytorch/pytorch/pull/130713#discussion_r1677395674", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130713", "author_association": "COLLABORATOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1677395674"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130713#discussion_r1677395674"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130713"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1677395674/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": null, "original_line": 247, "side": "RIGHT", "in_reply_to_id": 1677390483, "original_position": 4, "position": null, "subject_type": "line", "PR": {"title": "[inductor] adapte windows file path", "number": 130713, "id": 1968748478}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1677545441", "pull_request_review_id": 2177247440, "id": 1677545441, "node_id": "PRRC_kwDOA-j9z85j_U_h", "diff_hunk": "@@ -1645,9 +1645,9 @@ def test_dictliteral_is_typed_from_annotation():\n \n         def test_dictcomprehension_is_typed_from_annotation():\n             metasyntactics = [\"foo\", \"bar\", \"baz\"]\n-            x: Dict[str, Optional[int]] = {\n+            x: Dict[str, Optional[int]] = {  # noqa: C420, RUF025\n                 word: None for word in metasyntactics\n-            }  # noqa: RUF025\n+            }", "path": "test/jit/test_list_dict.py", "commit_id": "ad35c370739a59d933ad59d1a68683a0e5bab70e", "original_commit_id": "ad35c370739a59d933ad59d1a68683a0e5bab70e", "user": {"login": "lezcano", "id": 3291265, "url": "https://api.github.com/users/lezcano", "html_url": "https://github.com/lezcano"}, "body": "why not\r\n```python\r\n  x: Dict[str, Optional[int]] = dict.fronkeys(metasyntactics)\r\n```", "created_at": "2024-07-15T09:27:29Z", "updated_at": "2024-07-15T09:27:30Z", "html_url": "https://github.com/pytorch/pytorch/pull/130699#discussion_r1677545441", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130699", "author_association": "COLLABORATOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1677545441"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130699#discussion_r1677545441"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130699"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1677545441/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": 1648, "original_start_line": 1648, "start_side": "RIGHT", "line": 1650, "original_line": 1650, "side": "RIGHT", "original_position": 8, "position": 8, "subject_type": "line", "PR": {"title": "[BE]: Update flake8-comprehensions and enable C420", "number": 130699, "id": 1968381345}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678084327", "pull_request_review_id": 2178149703, "id": 1678084327, "node_id": "PRRC_kwDOA-j9z85kBYjn", "diff_hunk": "@@ -1645,9 +1645,9 @@ def test_dictliteral_is_typed_from_annotation():\n \n         def test_dictcomprehension_is_typed_from_annotation():\n             metasyntactics = [\"foo\", \"bar\", \"baz\"]\n-            x: Dict[str, Optional[int]] = {\n+            x: Dict[str, Optional[int]] = {  # noqa: C420, RUF025\n                 word: None for word in metasyntactics\n-            }  # noqa: RUF025\n+            }", "path": "test/jit/test_list_dict.py", "commit_id": "ad35c370739a59d933ad59d1a68683a0e5bab70e", "original_commit_id": "ad35c370739a59d933ad59d1a68683a0e5bab70e", "user": {"login": "Skylion007", "id": 2053727, "url": "https://api.github.com/users/Skylion007", "html_url": "https://github.com/Skylion007"}, "body": "@lezcano Because it's testing JIT features and I didn't want to touch it.", "created_at": "2024-07-15T16:13:30Z", "updated_at": "2024-07-15T16:13:31Z", "html_url": "https://github.com/pytorch/pytorch/pull/130699#discussion_r1678084327", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130699", "author_association": "COLLABORATOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678084327"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130699#discussion_r1678084327"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130699"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678084327/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": 1648, "original_start_line": 1648, "start_side": "RIGHT", "line": 1650, "original_line": 1650, "side": "RIGHT", "in_reply_to_id": 1677545441, "original_position": 8, "position": 8, "subject_type": "line", "PR": {"title": "[BE]: Update flake8-comprehensions and enable C420", "number": 130699, "id": 1968381345}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1677182609", "pull_request_review_id": 2176697616, "id": 1677182609, "node_id": "PRRC_kwDOA-j9z85j98aR", "diff_hunk": "@@ -88,25 +88,41 @@ ELSE (\"${SIZE_OF_VOIDP}\" EQUAL 8)\n   SET(iccvers \"ia32\")\n   SET(mkl64s)\n ENDIF (\"${SIZE_OF_VOIDP}\" EQUAL 8)\n-IF(CMAKE_COMPILER_IS_GNUCC)\n-  IF (\"${MKL_THREADING}\" STREQUAL \"TBB\")\n-    SET(mklthreads \"mkl_tbb_thread\")\n-    SET(mklrtls \"tbb\")\n-  ELSE()\n-    SET(mklthreads \"mkl_gnu_thread\" \"mkl_intel_thread\")\n-    SET(mklrtls \"gomp\" \"iomp5\")\n-  ENDIF()\n-  SET(mklifaces  \"intel\" \"gf\")\n-ELSE(CMAKE_COMPILER_IS_GNUCC)\n+\n+IF(WIN32)\n   IF (\"${MKL_THREADING}\" STREQUAL \"TBB\")", "path": "cmake/Modules/FindMKL.cmake", "commit_id": "742fd16db4a0cfdc33c4d851f9cb9d519c933f6f", "original_commit_id": "742fd16db4a0cfdc33c4d851f9cb9d519c933f6f", "user": {"login": "malfet", "id": 2453524, "url": "https://api.github.com/users/malfet", "html_url": "https://github.com/malfet"}, "body": "We got rid of all TBB builds recently, perhaps it's a good opportunity to clean up those as well", "created_at": "2024-07-14T18:24:03Z", "updated_at": "2024-07-14T18:27:43Z", "html_url": "https://github.com/pytorch/pytorch/pull/130697#discussion_r1677182609", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130697", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1677182609"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130697#discussion_r1677182609"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130697"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1677182609/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 93, "original_line": 93, "side": "RIGHT", "original_position": 16, "position": 16, "subject_type": "line", "PR": {"title": "Fix mkl-static issue for Windows.", "number": 130697, "id": 1968312611}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1677183060", "pull_request_review_id": 2176697616, "id": 1677183060, "node_id": "PRRC_kwDOA-j9z85j98hU", "diff_hunk": "@@ -88,25 +88,41 @@ ELSE (\"${SIZE_OF_VOIDP}\" EQUAL 8)\n   SET(iccvers \"ia32\")\n   SET(mkl64s)\n ENDIF (\"${SIZE_OF_VOIDP}\" EQUAL 8)\n-IF(CMAKE_COMPILER_IS_GNUCC)\n-  IF (\"${MKL_THREADING}\" STREQUAL \"TBB\")\n-    SET(mklthreads \"mkl_tbb_thread\")\n-    SET(mklrtls \"tbb\")\n-  ELSE()\n-    SET(mklthreads \"mkl_gnu_thread\" \"mkl_intel_thread\")\n-    SET(mklrtls \"gomp\" \"iomp5\")\n-  ENDIF()\n-  SET(mklifaces  \"intel\" \"gf\")\n-ELSE(CMAKE_COMPILER_IS_GNUCC)\n+\n+IF(WIN32)\n   IF (\"${MKL_THREADING}\" STREQUAL \"TBB\")\n     SET(mklthreads \"mkl_tbb_thread\")\n-    SET(mklrtls \"tbb\")\n+    IF (CMAKE_BUILD_TYPE STREQUAL Debug)\n+      SET(mklrtls \"tbb12_debug\")\n+    ELSE ()\n+      SET(mklrtls \"tbb12\")\n+    ENDIF ()\n   ELSE()\n     SET(mklthreads \"mkl_intel_thread\")\n-    SET(mklrtls \"iomp5\" \"guide\")\n+    SET(mklrtls \"libiomp5md\")\n   ENDIF()\n-  SET(mklifaces  \"intel\")\n-ENDIF (CMAKE_COMPILER_IS_GNUCC)\n+    SET(mklifaces  \"intel\")\n+ELSE(WIN32)\n+  IF(CMAKE_COMPILER_IS_GNUCC)\n+    IF (\"${MKL_THREADING}\" STREQUAL \"TBB\")\n+      SET(mklthreads \"mkl_tbb_thread\")\n+      SET(mklrtls \"tbb\")\n+    ELSE()\n+      SET(mklthreads \"mkl_gnu_thread\" \"mkl_intel_thread\")\n+      SET(mklrtls \"gomp\" \"iomp5\")\n+    ENDIF()\n+    SET(mklifaces  \"intel\" \"gf\")\n+  ELSE(CMAKE_COMPILER_IS_GNUCC)\n+    IF (\"${MKL_THREADING}\" STREQUAL \"TBB\")\n+      SET(mklthreads \"mkl_tbb_thread\")\n+      SET(mklrtls \"tbb\")\n+    ELSE()\n+      SET(mklthreads \"mkl_intel_thread\")\n+      SET(mklrtls \"iomp5\" \"guide\")\n+    ENDIF()\n+    SET(mklifaces  \"intel\")\n+  ENDIF (CMAKE_COMPILER_IS_GNUCC)", "path": "cmake/Modules/FindMKL.cmake", "commit_id": "742fd16db4a0cfdc33c4d851f9cb9d519c933f6f", "original_commit_id": "742fd16db4a0cfdc33c4d851f9cb9d519c933f6f", "user": {"login": "malfet", "id": 2453524, "url": "https://api.github.com/users/malfet", "html_url": "https://github.com/malfet"}, "body": "Can you please explain why one needs section that differentiates between gcc and clang?", "created_at": "2024-07-14T18:26:25Z", "updated_at": "2024-07-14T18:27:43Z", "html_url": "https://github.com/pytorch/pytorch/pull/130697#discussion_r1677183060", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130697", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1677183060"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130697#discussion_r1677183060"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130697"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1677183060/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": 115, "original_start_line": 115, "start_side": "RIGHT", "line": 124, "original_line": 124, "side": "RIGHT", "original_position": 51, "position": 51, "subject_type": "line", "PR": {"title": "Fix mkl-static issue for Windows.", "number": 130697, "id": 1968312611}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1677183302", "pull_request_review_id": 2176697616, "id": 1677183302, "node_id": "PRRC_kwDOA-j9z85j98lG", "diff_hunk": "@@ -88,25 +88,41 @@ ELSE (\"${SIZE_OF_VOIDP}\" EQUAL 8)\n   SET(iccvers \"ia32\")\n   SET(mkl64s)\n ENDIF (\"${SIZE_OF_VOIDP}\" EQUAL 8)\n-IF(CMAKE_COMPILER_IS_GNUCC)\n-  IF (\"${MKL_THREADING}\" STREQUAL \"TBB\")\n-    SET(mklthreads \"mkl_tbb_thread\")\n-    SET(mklrtls \"tbb\")\n-  ELSE()\n-    SET(mklthreads \"mkl_gnu_thread\" \"mkl_intel_thread\")\n-    SET(mklrtls \"gomp\" \"iomp5\")\n-  ENDIF()\n-  SET(mklifaces  \"intel\" \"gf\")\n-ELSE(CMAKE_COMPILER_IS_GNUCC)\n+\n+IF(WIN32)\n   IF (\"${MKL_THREADING}\" STREQUAL \"TBB\")\n     SET(mklthreads \"mkl_tbb_thread\")\n-    SET(mklrtls \"tbb\")\n+    IF (CMAKE_BUILD_TYPE STREQUAL Debug)\n+      SET(mklrtls \"tbb12_debug\")\n+    ELSE ()\n+      SET(mklrtls \"tbb12\")\n+    ENDIF ()\n   ELSE()\n     SET(mklthreads \"mkl_intel_thread\")\n-    SET(mklrtls \"iomp5\" \"guide\")\n+    SET(mklrtls \"libiomp5md\")\n   ENDIF()\n-  SET(mklifaces  \"intel\")\n-ENDIF (CMAKE_COMPILER_IS_GNUCC)\n+    SET(mklifaces  \"intel\")\n+ELSE(WIN32)\n+  IF(CMAKE_COMPILER_IS_GNUCC)\n+    IF (\"${MKL_THREADING}\" STREQUAL \"TBB\")\n+      SET(mklthreads \"mkl_tbb_thread\")\n+      SET(mklrtls \"tbb\")\n+    ELSE()\n+      SET(mklthreads \"mkl_gnu_thread\" \"mkl_intel_thread\")", "path": "cmake/Modules/FindMKL.cmake", "commit_id": "742fd16db4a0cfdc33c4d851f9cb9d519c933f6f", "original_commit_id": "742fd16db4a0cfdc33c4d851f9cb9d519c933f6f", "user": {"login": "malfet", "id": 2453524, "url": "https://api.github.com/users/malfet", "html_url": "https://github.com/malfet"}, "body": "It looks like this PR changes preference from Intel's OpenMP library to GNU one. I'm fine with it, but would prefer to do it in separate change ", "created_at": "2024-07-14T18:27:39Z", "updated_at": "2024-07-14T18:27:43Z", "html_url": "https://github.com/pytorch/pytorch/pull/130697#discussion_r1677183302", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130697", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1677183302"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130697#discussion_r1677183302"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130697"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1677183302/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 111, "original_line": 111, "side": "RIGHT", "original_position": 38, "position": 38, "subject_type": "line", "PR": {"title": "Fix mkl-static issue for Windows.", "number": 130697, "id": 1968312611}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1677183504", "pull_request_review_id": 2176698162, "id": 1677183504, "node_id": "PRRC_kwDOA-j9z85j98oQ", "diff_hunk": "@@ -88,25 +88,41 @@ ELSE (\"${SIZE_OF_VOIDP}\" EQUAL 8)\n   SET(iccvers \"ia32\")\n   SET(mkl64s)\n ENDIF (\"${SIZE_OF_VOIDP}\" EQUAL 8)\n-IF(CMAKE_COMPILER_IS_GNUCC)\n-  IF (\"${MKL_THREADING}\" STREQUAL \"TBB\")\n-    SET(mklthreads \"mkl_tbb_thread\")\n-    SET(mklrtls \"tbb\")\n-  ELSE()\n-    SET(mklthreads \"mkl_gnu_thread\" \"mkl_intel_thread\")\n-    SET(mklrtls \"gomp\" \"iomp5\")\n-  ENDIF()\n-  SET(mklifaces  \"intel\" \"gf\")\n-ELSE(CMAKE_COMPILER_IS_GNUCC)\n+\n+IF(WIN32)\n   IF (\"${MKL_THREADING}\" STREQUAL \"TBB\")\n     SET(mklthreads \"mkl_tbb_thread\")\n-    SET(mklrtls \"tbb\")\n+    IF (CMAKE_BUILD_TYPE STREQUAL Debug)\n+      SET(mklrtls \"tbb12_debug\")\n+    ELSE ()\n+      SET(mklrtls \"tbb12\")\n+    ENDIF ()\n   ELSE()\n     SET(mklthreads \"mkl_intel_thread\")\n-    SET(mklrtls \"iomp5\" \"guide\")\n+    SET(mklrtls \"libiomp5md\")\n   ENDIF()\n-  SET(mklifaces  \"intel\")\n-ENDIF (CMAKE_COMPILER_IS_GNUCC)\n+    SET(mklifaces  \"intel\")\n+ELSE(WIN32)\n+  IF(CMAKE_COMPILER_IS_GNUCC)\n+    IF (\"${MKL_THREADING}\" STREQUAL \"TBB\")\n+      SET(mklthreads \"mkl_tbb_thread\")\n+      SET(mklrtls \"tbb\")\n+    ELSE()\n+      SET(mklthreads \"mkl_gnu_thread\" \"mkl_intel_thread\")\n+      SET(mklrtls \"gomp\" \"iomp5\")\n+    ENDIF()\n+    SET(mklifaces  \"intel\" \"gf\")\n+  ELSE(CMAKE_COMPILER_IS_GNUCC)\n+    IF (\"${MKL_THREADING}\" STREQUAL \"TBB\")\n+      SET(mklthreads \"mkl_tbb_thread\")\n+      SET(mklrtls \"tbb\")\n+    ELSE()\n+      SET(mklthreads \"mkl_intel_thread\")\n+      SET(mklrtls \"iomp5\" \"guide\")\n+    ENDIF()\n+    SET(mklifaces  \"intel\")\n+  ENDIF (CMAKE_COMPILER_IS_GNUCC)", "path": "cmake/Modules/FindMKL.cmake", "commit_id": "742fd16db4a0cfdc33c4d851f9cb9d519c933f6f", "original_commit_id": "742fd16db4a0cfdc33c4d851f9cb9d519c933f6f", "user": {"login": "xuhancn", "id": 8433590, "url": "https://api.github.com/users/xuhancn", "html_url": "https://github.com/xuhancn"}, "body": "It is original code, I don't known why it. I only add a section for `Win32` and move the origianl code to `else` block.", "created_at": "2024-07-14T18:29:32Z", "updated_at": "2024-07-14T18:29:33Z", "html_url": "https://github.com/pytorch/pytorch/pull/130697#discussion_r1677183504", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130697", "author_association": "COLLABORATOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1677183504"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130697#discussion_r1677183504"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130697"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1677183504/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": 115, "original_start_line": 115, "start_side": "RIGHT", "line": 124, "original_line": 124, "side": "RIGHT", "in_reply_to_id": 1677183060, "original_position": 51, "position": 51, "subject_type": "line", "PR": {"title": "Fix mkl-static issue for Windows.", "number": 130697, "id": 1968312611}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1677184236", "pull_request_review_id": 2176698693, "id": 1677184236, "node_id": "PRRC_kwDOA-j9z85j98zs", "diff_hunk": "@@ -88,25 +88,41 @@ ELSE (\"${SIZE_OF_VOIDP}\" EQUAL 8)\n   SET(iccvers \"ia32\")\n   SET(mkl64s)\n ENDIF (\"${SIZE_OF_VOIDP}\" EQUAL 8)\n-IF(CMAKE_COMPILER_IS_GNUCC)\n-  IF (\"${MKL_THREADING}\" STREQUAL \"TBB\")\n-    SET(mklthreads \"mkl_tbb_thread\")\n-    SET(mklrtls \"tbb\")\n-  ELSE()\n-    SET(mklthreads \"mkl_gnu_thread\" \"mkl_intel_thread\")\n-    SET(mklrtls \"gomp\" \"iomp5\")\n-  ENDIF()\n-  SET(mklifaces  \"intel\" \"gf\")\n-ELSE(CMAKE_COMPILER_IS_GNUCC)\n+\n+IF(WIN32)\n   IF (\"${MKL_THREADING}\" STREQUAL \"TBB\")\n     SET(mklthreads \"mkl_tbb_thread\")\n-    SET(mklrtls \"tbb\")\n+    IF (CMAKE_BUILD_TYPE STREQUAL Debug)\n+      SET(mklrtls \"tbb12_debug\")\n+    ELSE ()\n+      SET(mklrtls \"tbb12\")\n+    ENDIF ()\n   ELSE()\n     SET(mklthreads \"mkl_intel_thread\")\n-    SET(mklrtls \"iomp5\" \"guide\")\n+    SET(mklrtls \"libiomp5md\")\n   ENDIF()\n-  SET(mklifaces  \"intel\")\n-ENDIF (CMAKE_COMPILER_IS_GNUCC)\n+    SET(mklifaces  \"intel\")\n+ELSE(WIN32)\n+  IF(CMAKE_COMPILER_IS_GNUCC)\n+    IF (\"${MKL_THREADING}\" STREQUAL \"TBB\")\n+      SET(mklthreads \"mkl_tbb_thread\")\n+      SET(mklrtls \"tbb\")\n+    ELSE()\n+      SET(mklthreads \"mkl_gnu_thread\" \"mkl_intel_thread\")", "path": "cmake/Modules/FindMKL.cmake", "commit_id": "742fd16db4a0cfdc33c4d851f9cb9d519c933f6f", "original_commit_id": "742fd16db4a0cfdc33c4d851f9cb9d519c933f6f", "user": {"login": "xuhancn", "id": 8433590, "url": "https://api.github.com/users/xuhancn", "html_url": "https://github.com/xuhancn"}, "body": "No, I didn't change any thing for original code. \r\n<img width=\"617\" alt=\"image\" src=\"https://github.com/user-attachments/assets/dabb5e2d-e759-40e6-b51c-7d279d7dda46\">\r\nI just move the original code to `else` block of `Win32`, and change nothing except Win32.", "created_at": "2024-07-14T18:34:34Z", "updated_at": "2024-07-14T18:50:59Z", "html_url": "https://github.com/pytorch/pytorch/pull/130697#discussion_r1677184236", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130697", "author_association": "COLLABORATOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1677184236"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130697#discussion_r1677184236"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130697"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1677184236/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 111, "original_line": 111, "side": "RIGHT", "in_reply_to_id": 1677183302, "original_position": 38, "position": 38, "subject_type": "line", "PR": {"title": "Fix mkl-static issue for Windows.", "number": 130697, "id": 1968312611}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1677184882", "pull_request_review_id": 2176699294, "id": 1677184882, "node_id": "PRRC_kwDOA-j9z85j989y", "diff_hunk": "@@ -88,25 +88,41 @@ ELSE (\"${SIZE_OF_VOIDP}\" EQUAL 8)\n   SET(iccvers \"ia32\")\n   SET(mkl64s)\n ENDIF (\"${SIZE_OF_VOIDP}\" EQUAL 8)\n-IF(CMAKE_COMPILER_IS_GNUCC)\n-  IF (\"${MKL_THREADING}\" STREQUAL \"TBB\")\n-    SET(mklthreads \"mkl_tbb_thread\")\n-    SET(mklrtls \"tbb\")\n-  ELSE()\n-    SET(mklthreads \"mkl_gnu_thread\" \"mkl_intel_thread\")\n-    SET(mklrtls \"gomp\" \"iomp5\")\n-  ENDIF()\n-  SET(mklifaces  \"intel\" \"gf\")\n-ELSE(CMAKE_COMPILER_IS_GNUCC)\n+\n+IF(WIN32)\n   IF (\"${MKL_THREADING}\" STREQUAL \"TBB\")", "path": "cmake/Modules/FindMKL.cmake", "commit_id": "742fd16db4a0cfdc33c4d851f9cb9d519c933f6f", "original_commit_id": "742fd16db4a0cfdc33c4d851f9cb9d519c933f6f", "user": {"login": "xuhancn", "id": 8433590, "url": "https://api.github.com/users/xuhancn", "html_url": "https://github.com/xuhancn"}, "body": "I suggest to clean up code in further, this PR is only impact on Windows build.\r\nIf we can fix the performance regression quickly. Maybe we still have chance to cherry pick to `release/2.4`.", "created_at": "2024-07-14T18:40:01Z", "updated_at": "2024-07-14T18:53:17Z", "html_url": "https://github.com/pytorch/pytorch/pull/130697#discussion_r1677184882", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130697", "author_association": "COLLABORATOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1677184882"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130697#discussion_r1677184882"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130697"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1677184882/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 93, "original_line": 93, "side": "RIGHT", "in_reply_to_id": 1677182609, "original_position": 16, "position": 16, "subject_type": "line", "PR": {"title": "Fix mkl-static issue for Windows.", "number": 130697, "id": 1968312611}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1677825960", "pull_request_review_id": 2177710283, "id": 1677825960, "node_id": "PRRC_kwDOA-j9z85kAZeo", "diff_hunk": "@@ -88,25 +88,41 @@ ELSE (\"${SIZE_OF_VOIDP}\" EQUAL 8)\n   SET(iccvers \"ia32\")\n   SET(mkl64s)\n ENDIF (\"${SIZE_OF_VOIDP}\" EQUAL 8)\n-IF(CMAKE_COMPILER_IS_GNUCC)\n-  IF (\"${MKL_THREADING}\" STREQUAL \"TBB\")\n-    SET(mklthreads \"mkl_tbb_thread\")\n-    SET(mklrtls \"tbb\")\n-  ELSE()\n-    SET(mklthreads \"mkl_gnu_thread\" \"mkl_intel_thread\")\n-    SET(mklrtls \"gomp\" \"iomp5\")\n-  ENDIF()\n-  SET(mklifaces  \"intel\" \"gf\")\n-ELSE(CMAKE_COMPILER_IS_GNUCC)\n+\n+IF(WIN32)\n   IF (\"${MKL_THREADING}\" STREQUAL \"TBB\")\n     SET(mklthreads \"mkl_tbb_thread\")\n-    SET(mklrtls \"tbb\")\n+    IF (CMAKE_BUILD_TYPE STREQUAL Debug)\n+      SET(mklrtls \"tbb12_debug\")\n+    ELSE ()\n+      SET(mklrtls \"tbb12\")\n+    ENDIF ()\n   ELSE()\n     SET(mklthreads \"mkl_intel_thread\")\n-    SET(mklrtls \"iomp5\" \"guide\")\n+    SET(mklrtls \"libiomp5md\")\n   ENDIF()\n-  SET(mklifaces  \"intel\")\n-ENDIF (CMAKE_COMPILER_IS_GNUCC)\n+    SET(mklifaces  \"intel\")\n+ELSE(WIN32)\n+  IF(CMAKE_COMPILER_IS_GNUCC)\n+    IF (\"${MKL_THREADING}\" STREQUAL \"TBB\")\n+      SET(mklthreads \"mkl_tbb_thread\")\n+      SET(mklrtls \"tbb\")\n+    ELSE()\n+      SET(mklthreads \"mkl_gnu_thread\" \"mkl_intel_thread\")", "path": "cmake/Modules/FindMKL.cmake", "commit_id": "742fd16db4a0cfdc33c4d851f9cb9d519c933f6f", "original_commit_id": "742fd16db4a0cfdc33c4d851f9cb9d519c933f6f", "user": {"login": "atalman", "id": 7563158, "url": "https://api.github.com/users/atalman", "html_url": "https://github.com/atalman"}, "body": "Looks like Github change highlight is sometimes off. Here is the correct change view :\r\n<img width=\"1034\" alt=\"Screenshot 2024-07-15 at 9 25 25\u202fAM\" src=\"https://github.com/user-attachments/assets/833af29e-5a68-4c98-b1c8-2db7e2fd2a39\">\r\n\r\n", "created_at": "2024-07-15T13:23:41Z", "updated_at": "2024-07-15T13:25:48Z", "html_url": "https://github.com/pytorch/pytorch/pull/130697#discussion_r1677825960", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130697", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1677825960"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130697#discussion_r1677825960"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130697"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1677825960/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 111, "original_line": 111, "side": "RIGHT", "in_reply_to_id": 1677183302, "original_position": 38, "position": 38, "subject_type": "line", "PR": {"title": "Fix mkl-static issue for Windows.", "number": 130697, "id": 1968312611}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676853501", "pull_request_review_id": 2176428856, "id": 1676853501, "node_id": "PRRC_kwDOA-j9z85j8sD9", "diff_hunk": "@@ -63,6 +63,14 @@ def set_isdisjoint(set1, set2):\n     return True\n \n \n+def set_intersection(set1, set2):", "path": "torch/_dynamo/polyfill.py", "commit_id": "3a376f9257ac80bccc84823beedf26d5faa9e471", "original_commit_id": "992dbb3331602ce08d19b261dfe68838c8055c94", "user": {"login": "Skylion007", "id": 2053727, "url": "https://api.github.com/users/Skylion007", "html_url": "https://github.com/Skylion007"}, "body": "While we are here, can we do union and difference ops too?", "created_at": "2024-07-13T16:24:33Z", "updated_at": "2024-07-13T16:24:33Z", "html_url": "https://github.com/pytorch/pytorch/pull/130672#discussion_r1676853501", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130672", "author_association": "COLLABORATOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676853501"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130672#discussion_r1676853501"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130672"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676853501/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 66, "original_line": 66, "side": "RIGHT", "original_position": 4, "position": 4, "subject_type": "line", "PR": {"title": "support intersection by polyfill", "number": 130672, "id": 1967734864}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1677029450", "pull_request_review_id": 2176581986, "id": 1677029450, "node_id": "PRRC_kwDOA-j9z85j9XBK", "diff_hunk": "@@ -63,6 +63,31 @@ def set_isdisjoint(set1, set2):\n     return True\n \n \n+def set_intersection(set1, set2):\n+    intersection_set = set()\n+    for x in set1:\n+        if x in set2:\n+            intersection_set.add(x)\n+    return intersection_set\n+\n+\n+def set_union(set1, set2):\n+    union_set = set()\n+    for s in (set1, set2):\n+        for x in s:\n+            if x not in union_set:\n+                union_set.add(x)\n+    return union_set", "path": "torch/_dynamo/polyfill.py", "commit_id": "3a376f9257ac80bccc84823beedf26d5faa9e471", "original_commit_id": "3589c86a0b3e579fa368fa636fafc6b65942c981", "user": {"login": "XuehaiPan", "id": 16078332, "url": "https://api.github.com/users/XuehaiPan", "html_url": "https://github.com/XuehaiPan"}, "body": "```suggestion\r\ndef set_union(set1, set2):\r\n    union_set = set1.copy()\r\n    for x in set2:\r\n        if x not in union_set:\r\n            union_set.add(x)\r\n    return union_set\r\n```", "created_at": "2024-07-14T05:46:22Z", "updated_at": "2024-07-14T05:46:22Z", "html_url": "https://github.com/pytorch/pytorch/pull/130672#discussion_r1677029450", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130672", "author_association": "COLLABORATOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1677029450"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130672#discussion_r1677029450"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130672"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1677029450/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": 74, "start_side": "RIGHT", "line": null, "original_line": 80, "side": "RIGHT", "original_position": 18, "position": null, "subject_type": "line", "PR": {"title": "support intersection by polyfill", "number": 130672, "id": 1967734864}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1677040769", "pull_request_review_id": 2176586712, "id": 1677040769, "node_id": "PRRC_kwDOA-j9z85j9ZyB", "diff_hunk": "@@ -63,6 +63,31 @@ def set_isdisjoint(set1, set2):\n     return True\n \n \n+def set_intersection(set1, set2):\n+    intersection_set = set()\n+    for x in set1:\n+        if x in set2:\n+            intersection_set.add(x)\n+    return intersection_set\n+\n+\n+def set_union(set1, set2):\n+    union_set = set()\n+    for s in (set1, set2):\n+        for x in s:\n+            if x not in union_set:\n+                union_set.add(x)\n+    return union_set", "path": "torch/_dynamo/polyfill.py", "commit_id": "3a376f9257ac80bccc84823beedf26d5faa9e471", "original_commit_id": "3589c86a0b3e579fa368fa636fafc6b65942c981", "user": {"login": "awayzjj", "id": 38181615, "url": "https://api.github.com/users/awayzjj", "html_url": "https://github.com/awayzjj"}, "body": "@XuehaiPan Thank you very much for your suggestions! I have incorporated your suggestions. Could you please review the PR again? cc @Skylion007 @anijain2305 ", "created_at": "2024-07-14T06:32:39Z", "updated_at": "2024-07-14T06:32:40Z", "html_url": "https://github.com/pytorch/pytorch/pull/130672#discussion_r1677040769", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130672", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1677040769"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130672#discussion_r1677040769"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130672"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1677040769/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": 74, "start_side": "RIGHT", "line": null, "original_line": 80, "side": "RIGHT", "in_reply_to_id": 1677029450, "original_position": 18, "position": null, "subject_type": "line", "PR": {"title": "support intersection by polyfill", "number": 130672, "id": 1967734864}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678138016", "pull_request_review_id": 2178233441, "id": 1678138016, "node_id": "PRRC_kwDOA-j9z85kBlqg", "diff_hunk": "@@ -108,7 +108,7 @@ def _broadcast_object(\n         )\n         dist.broadcast(data_recv_tensor, src=src_rank, group=group, async_op=False)\n         buffer = io.BytesIO(data_recv_tensor.cpu().numpy())\n-        obj = torch.load(buffer, map_location=device)\n+        obj = torch.load(buffer, map_location=device, weights_only=False)", "path": "torch/distributed/optim/zero_redundancy_optimizer.py", "commit_id": "e0275f87e508d596956da5206420e8b982291f29", "original_commit_id": "b43aca5963bedb0aa79ff35a3775008506bb2a66", "user": {"login": "awaelchli", "id": 5495193, "url": "https://api.github.com/users/awaelchli", "html_url": "https://github.com/awaelchli"}, "body": "From the code above, we can see this is broadcasting an object pickled to bytes, so loading here means unpickling. The function doesn't make any assumptions like obj being a tensor, so we need to set `weights_only=False` here.", "created_at": "2024-07-15T16:55:35Z", "updated_at": "2024-07-15T16:59:40Z", "html_url": "https://github.com/pytorch/pytorch/pull/130663#discussion_r1678138016", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130663", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678138016"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130663#discussion_r1678138016"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130663"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678138016/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 111, "original_line": 111, "side": "RIGHT", "original_position": 5, "position": 5, "subject_type": "line", "PR": {"title": "Pass `torch.load(weights_only=)` internally to avoid FutureWarning", "number": 130663, "id": 1967543990}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678140543", "pull_request_review_id": 2178233441, "id": 1678140543, "node_id": "PRRC_kwDOA-j9z85kBmR_", "diff_hunk": "@@ -654,7 +654,11 @@ def read_data(self, plan: LoadPlan, planner: LoadPlanner) -> Future[None]:\n                     else:\n                         tensor = cast(\n                             Tensor,\n-                            torch.load(cast(IO[bytes], file_slice), map_location=\"cpu\"),\n+                            torch.load(\n+                                cast(IO[bytes], file_slice),\n+                                map_location=\"cpu\",\n+                                weights_only=False", "path": "torch/distributed/checkpoint/filesystem.py", "commit_id": "e0275f87e508d596956da5206420e8b982291f29", "original_commit_id": "b43aca5963bedb0aa79ff35a3775008506bb2a66", "user": {"login": "awaelchli", "id": 5495193, "url": "https://api.github.com/users/awaelchli", "html_url": "https://github.com/awaelchli"}, "body": "Here we can set True because we know it has to be a tensor.\r\n```suggestion\r\n                                weights_only=True\r\n```", "created_at": "2024-07-15T16:57:52Z", "updated_at": "2024-07-15T16:59:40Z", "html_url": "https://github.com/pytorch/pytorch/pull/130663#discussion_r1678140543", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130663", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678140543"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130663#discussion_r1678140543"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130663"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678140543/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": null, "original_line": 660, "side": "RIGHT", "original_position": 8, "position": null, "subject_type": "line", "PR": {"title": "Pass `torch.load(weights_only=)` internally to avoid FutureWarning", "number": 130663, "id": 1967543990}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676884939", "pull_request_review_id": 2176496354, "id": 1676884939, "node_id": "PRRC_kwDOA-j9z85j8zvL", "diff_hunk": "@@ -87,52 +89,101 @@ def _vmap_for_bhqkv(\n     )\n     return fn\n \n-\n def _identity(\n-    score: torch.Tensor,\n-    batch: torch.Tensor,\n-    head: torch.Tensor,\n-    token_q: torch.Tensor,\n-    token_kv: torch.Tensor,\n-) -> torch.Tensor:\n+    score: Tensor,\n+    batch: Tensor,\n+    head: Tensor,\n+    token_q: Tensor,\n+    token_kv: Tensor,\n+) -> Tensor:\n     return score\n \n \n def _no_mask(\n-    batch: torch.Tensor,\n-    head: torch.Tensor,\n-    token_q: torch.Tensor,\n-    token_kv: torch.Tensor,\n-) -> torch.Tensor:\n+    batch: Tensor,\n+    head: Tensor,\n+    token_q: Tensor,\n+    token_kv: Tensor,\n+) -> Tensor:\n     return token_q.new_ones(size=(), dtype=torch.bool, device=batch.device)\n \n \n _DEFAULT_SPARSE_BLOCK_SIZE = 128\n \n \n class BlockMask:\n-    kv_num_blocks: torch.Tensor\n-    kv_indices: torch.Tensor\n-    q_num_blocks: torch.Tensor\n-    q_indices: torch.Tensor\n-    full_kv_num_blocks: Optional[torch.Tensor]\n-    full_kv_indices: Optional[torch.Tensor]\n-    full_q_num_blocks: Optional[torch.Tensor]\n-    full_q_indices: Optional[torch.Tensor]\n+    r\"\"\"\n+    BlockMask is our format for representing a block-sparse attention mask.\n+    It is somewhat of a cross in-between BCSR and a non-sparse format.\n+\n+    ## Basics\n+    A block-sparse mask means that instead of representing the sparsity of\n+    individual elements in the mask, we only consider a block sparse if an\n+    entire KV_BLOCK_SIZE x Q_BLOCK_SIZE is sparse. This aligns well with\n+    hardware, which generally expects to perform contiguous loads and\n+    computation.\n+    \n+    This format is primarily optimized for 1. simplicity, and 2. kernel\n+    efficiency. Notably, it is *not* optimized for size, as we believe the mask\n+    is sufficiently small that its size is not a concern.", "path": "torch/nn/attention/_flex_attention.py", "commit_id": "a794c4e4e587e406e81f20fecf5242f72f695102", "original_commit_id": "7c7430552b5f8ac37fcd553c8feb7cb20e59248e", "user": {"login": "drisspg", "id": 32754868, "url": "https://api.github.com/users/drisspg", "html_url": "https://github.com/drisspg"}, "body": "nit: maybe something like and can be controlled via the block size", "created_at": "2024-07-13T19:14:33Z", "updated_at": "2024-07-13T19:14:33Z", "html_url": "https://github.com/pytorch/pytorch/pull/130649#discussion_r1676884939", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130649", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676884939"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130649#discussion_r1676884939"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130649"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676884939/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 126, "original_line": 128, "side": "RIGHT", "original_position": 90, "position": 91, "subject_type": "line", "PR": {"title": "Added some more documentation to block mask creation", "number": 130649, "id": 1967379219}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676553797", "pull_request_review_id": 2175900044, "id": 1676553797, "node_id": "PRRC_kwDOA-j9z85j7i5F", "diff_hunk": "@@ -581,9 +581,9 @@ def f():\n             return bool(val.item() == 2.1)\n \n         def test_f():\n-            make_fx(f, tracing_mode=self.tracing_mode)()\n+            return make_fx(f, tracing_mode=self.tracing_mode)()()\n \n-        self.assertRaisesRegex(RuntimeError, \"data-dependent\", test_f)", "path": "test/test_proxy_tensor.py", "commit_id": "096c44400d7100a6ed88299740371aea8065cd7d", "original_commit_id": "33ec80915d743335a1e927f094e857ea3d7adcd2", "user": {"login": "yf225", "id": 4063635, "url": "https://api.github.com/users/yf225", "html_url": "https://github.com/yf225"}, "body": "This used to raise error:\r\n\r\n`RuntimeError: It appears that you're trying to get value out of a tracing tensor with aten._local_scalar_dense.default - erroring out! It's likely that this is caused by data-dependent control flow or similar.  It may be possible to trace this with dynamic shapes; try setting tracing_mode='symbolic' in your make_fx call.`\r\n\r\nbut now that the `val` input of `val.item()` actually points to the graph input, we no longer hit this error message.\r\n\r\nIs this the right expected behavior?", "created_at": "2024-07-12T23:21:16Z", "updated_at": "2024-07-12T23:21:27Z", "html_url": "https://github.com/pytorch/pytorch/pull/130647#discussion_r1676553797", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130647", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676553797"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130647#discussion_r1676553797"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130647"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676553797/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 586, "original_line": 586, "side": "LEFT", "original_position": 18, "position": 18, "subject_type": "line", "PR": {"title": "Fix tests on top of PR #130577", "number": 130647, "id": 1967372934}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678181773", "pull_request_review_id": 2178305070, "id": 1678181773, "node_id": "PRRC_kwDOA-j9z85kBwWN", "diff_hunk": "@@ -1327,6 +1327,11 @@ def fwd_bwd(optim, w, b, i):\n             optim.zero_grad()\n             loss = (w.mv(i) + b).pow(2).sum()\n             loss.backward()\n+            if optim.__class__.__name__ == \"SparseAdam\":", "path": "test/test_optim.py", "commit_id": "e18d2af53950d91ff10dd85f4bfed41d5788af48", "original_commit_id": "0664f89e459c4de323b72d040adbdbfeccb0c76f", "user": {"login": "janeyx99", "id": 31798555, "url": "https://api.github.com/users/janeyx99", "html_url": "https://github.com/janeyx99"}, "body": "```suggestion\r\n            if optim.only_supports_sparse_grads:\r\n```", "created_at": "2024-07-15T17:37:37Z", "updated_at": "2024-07-16T03:31:05Z", "html_url": "https://github.com/pytorch/pytorch/pull/130645#discussion_r1678181773", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130645", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678181773"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130645#discussion_r1678181773"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130645"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678181773/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": null, "original_line": 1330, "side": "RIGHT", "original_position": 4, "position": null, "subject_type": "line", "PR": {"title": "Add testing regarding SparseAdam state_dicts", "number": 130645, "id": 1967325406}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678707415", "pull_request_review_id": 2179139734, "id": 1678707415, "node_id": "PRRC_kwDOA-j9z85kDwrX", "diff_hunk": "@@ -1327,6 +1327,11 @@ def fwd_bwd(optim, w, b, i):\n             optim.zero_grad()\n             loss = (w.mv(i) + b).pow(2).sum()\n             loss.backward()\n+            if optim.__class__.__name__ == \"SparseAdam\":", "path": "test/test_optim.py", "commit_id": "e18d2af53950d91ff10dd85f4bfed41d5788af48", "original_commit_id": "0664f89e459c4de323b72d040adbdbfeccb0c76f", "user": {"login": "janeyx99", "id": 31798555, "url": "https://api.github.com/users/janeyx99", "html_url": "https://github.com/janeyx99"}, "body": "Please modify this before merging. Thanks for the fix!", "created_at": "2024-07-16T03:31:31Z", "updated_at": "2024-07-16T03:31:31Z", "html_url": "https://github.com/pytorch/pytorch/pull/130645#discussion_r1678707415", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130645", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678707415"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130645#discussion_r1678707415"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130645"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678707415/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": null, "original_line": 1330, "side": "RIGHT", "in_reply_to_id": 1678181773, "original_position": 4, "position": null, "subject_type": "line", "PR": {"title": "Add testing regarding SparseAdam state_dicts", "number": 130645, "id": 1967325406}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676370014", "pull_request_review_id": 2175586172, "id": 1676370014, "node_id": "PRRC_kwDOA-j9z85j62Be", "diff_hunk": "@@ -0,0 +1,192 @@\n+from typing import Callable, Iterable, Optional, Union\n+\n+from .custom_ops import custom_op\n+\n+\n+def triton_op(\n+    name: str,\n+    fn: Optional[Callable] = None,\n+    /,\n+    *,\n+    mutates_args: Union[str, Iterable[str]],\n+    schema: Optional[str] = None,\n+) -> Callable:\n+    \"\"\"Create a custom operator whose implementation is backed by 1+ triton kernels.\n+\n+    Use this instead of :func:`torch.library.custom_op` when the implementation\n+    consists of 1+ triton kernels. :func:`torch.library.custom_op` treats\n+    custom operators as opaque (:func:`torch.compile` and\n+    :func:`torch.export.export` will never trace into them), but ``triton_op``\n+    makes the implementation visible to these subsystems, allowing them\n+    to optimize the triton kernel(s).\n+\n+    Note that ``fn`` must only consist of calls to PyTorch-understood\n+    operators and triton kernels. Any triton kernels called inside ``fn``\n+    must be wrapped in a call to :func:`torch._library.capture_triton``.\n+\n+    Args:\n+        name (str): A name for the custom op that looks like \"{namespace}::{name}\",\n+            e.g. \"mylib::my_linear\". The name is used as the op's stable identifier\n+            in PyTorch subsystems (e.g. torch.export, FX graphs).\n+            To avoid name collisions, please use your project name as the namespace;\n+            e.g. all custom ops in pytorch/fbgemm use \"fbgemm\" as the namespace.\n+        mutates_args (Iterable[str] or \"unknown\"): The names of args that the function mutates.\n+            This MUST be accurate, otherwise, the behavior is undefined. If \"unknown\",\n+            it pessimistically assumes that all inputs to the operator are being mutated.\n+        schema (None | str): A schema string for the operator. If None\n+            (recommended) we'll infer a schema for the operator from its type\n+            annotations. We recommend letting us infer a schema unless you\n+            have a specific reason not to.\n+            Example: \"(Tensor x, int y) -> (Tensor, Tensor)\".\n+\n+    Example::\n+\n+        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA)\n+        >>> import torch\n+        >>> from torch._library import triton_op, capture_triton\n+        >>>\n+        >>> import triton\n+        >>> from triton import language as tl\n+        >>>\n+        >>> @triton.jit\n+        >>> def add_kernel(\n+        >>>     in_ptr0,\n+        >>>     in_ptr1,\n+        >>>     out_ptr,\n+        >>>     n_elements,\n+        >>>     BLOCK_SIZE: \"tl.constexpr\",\n+        >>> ):\n+        >>>     pid = tl.program_id(axis=0)\n+        >>>     block_start = pid * BLOCK_SIZE\n+        >>>     offsets = block_start + tl.arange(0, BLOCK_SIZE)\n+        >>>     mask = offsets < n_elements\n+        >>>     x = tl.load(in_ptr0 + offsets, mask=mask)\n+        >>>     y = tl.load(in_ptr1 + offsets, mask=mask)\n+        >>>     output = x + y\n+        >>>     tl.store(out_ptr + offsets, output, mask=mask)\n+        >>>\n+        >>> @triton_op(\"mylib::add\", mutates_args={})\n+        >>> def add(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n+        >>>     output = torch.empty_like(x)\n+        >>>     n_elements = output.numel()\n+        >>>\n+        >>>     def grid(meta):\n+        >>>         return (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n+        >>>\n+        >>>     # NB: we need to wrap the triton kernel in a call to capture_triton\n+        >>>     capture_triton(add_kernel)[grid](x, y, output, n_elements, 16)\n+        >>>     return output\n+        >>>\n+        >>> @torch.compile\n+        >>> def f(x, y):\n+        >>>     return add(x, y)\n+        >>>\n+        >>> x = torch.randn(3, device=\"cuda\")\n+        >>> y = torch.randn(3, device=\"cuda\")\n+        >>>\n+        >>> z = f(x, y)\n+        >>> assert torch.allclose(z, x + y)\n+\n+    \"\"\"\n+\n+    def dec(fn: Callable) -> Callable:\n+        result = custom_op(name, fn, mutates_args=mutates_args)\n+        from .._subclasses.functional_tensor import FunctionalTensorMode\n+\n+        # We require that the user pass us a function that is make_fx traceable,\n+        # so we can just register it as the Fake/meta kernel.\n+        result.register_fake(fn)\n+\n+        # We decompose the operator when FunctionalTensorMode is active.\n+        # The goal is to decompose the operator in AOTDispatcher.\n+        # - With torch.compile, this means that the backend (usually Inductor)\n+        #   can see a call to the triton kernel(s) and so it can directly optimize\n+        #   them by inlining them into the lowering process.\n+        # - With post-dispatch torch.export, this means that there will\n+        #   be a call(s) to the triton_kernel_wrapper_functional HOP in the\n+        #   graph (that we have yet to figure out how to serialize).\n+        def functional_decomp(  # type: ignore[no-untyped-def]\n+            mode, _, types, args, kwargs\n+        ):\n+            with mode:\n+                return fn(*args, **kwargs)\n+\n+        result.register_torch_dispatch(FunctionalTensorMode, functional_decomp)\n+        return result\n+\n+    if fn is None:\n+        return dec\n+    else:\n+        return dec(fn)\n+\n+\n+def capture_triton(triton_kernel: Callable, /) -> Callable:", "path": "torch/_library/triton.py", "commit_id": "1545b9201079c5dd94d5f40c904c9c6a7509e36d", "original_commit_id": "c76d8681d6f51d425ce9fe975d177b93ea566764", "user": {"login": "zou3519", "id": 5652049, "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519"}, "body": "This was moved from torch/_higher_order_ops/triton_kernel_wrap.py", "created_at": "2024-07-12T19:23:06Z", "updated_at": "2024-07-12T19:23:06Z", "html_url": "https://github.com/pytorch/pytorch/pull/130637#discussion_r1676370014", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130637", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676370014"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130637#discussion_r1676370014"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130637"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676370014/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 123, "original_line": 123, "side": "RIGHT", "original_position": 123, "position": 123, "subject_type": "line", "PR": {"title": "[custom_op] triton_op API V0", "number": 130637, "id": 1967192509}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676370716", "pull_request_review_id": 2175587343, "id": 1676370716, "node_id": "PRRC_kwDOA-j9z85j62Mc", "diff_hunk": "@@ -0,0 +1,192 @@\n+from typing import Callable, Iterable, Optional, Union\n+\n+from .custom_ops import custom_op\n+\n+\n+def triton_op(\n+    name: str,\n+    fn: Optional[Callable] = None,\n+    /,\n+    *,\n+    mutates_args: Union[str, Iterable[str]],\n+    schema: Optional[str] = None,\n+) -> Callable:\n+    \"\"\"Create a custom operator whose implementation is backed by 1+ triton kernels.\n+\n+    Use this instead of :func:`torch.library.custom_op` when the implementation\n+    consists of 1+ triton kernels. :func:`torch.library.custom_op` treats\n+    custom operators as opaque (:func:`torch.compile` and\n+    :func:`torch.export.export` will never trace into them), but ``triton_op``\n+    makes the implementation visible to these subsystems, allowing them\n+    to optimize the triton kernel(s).\n+\n+    Note that ``fn`` must only consist of calls to PyTorch-understood\n+    operators and triton kernels. Any triton kernels called inside ``fn``\n+    must be wrapped in a call to :func:`torch._library.capture_triton``.\n+\n+    Args:\n+        name (str): A name for the custom op that looks like \"{namespace}::{name}\",\n+            e.g. \"mylib::my_linear\". The name is used as the op's stable identifier\n+            in PyTorch subsystems (e.g. torch.export, FX graphs).\n+            To avoid name collisions, please use your project name as the namespace;\n+            e.g. all custom ops in pytorch/fbgemm use \"fbgemm\" as the namespace.\n+        mutates_args (Iterable[str] or \"unknown\"): The names of args that the function mutates.\n+            This MUST be accurate, otherwise, the behavior is undefined. If \"unknown\",\n+            it pessimistically assumes that all inputs to the operator are being mutated.", "path": "torch/_library/triton.py", "commit_id": "1545b9201079c5dd94d5f40c904c9c6a7509e36d", "original_commit_id": "c76d8681d6f51d425ce9fe975d177b93ea566764", "user": {"login": "zou3519", "id": 5652049, "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519"}, "body": "This is required right now, but in theory we can infer it or assume all functional and error out if we detect that isn't the case. I can handle that in a follow-up.", "created_at": "2024-07-12T19:24:02Z", "updated_at": "2024-07-12T19:24:02Z", "html_url": "https://github.com/pytorch/pytorch/pull/130637#discussion_r1676370716", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130637", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676370716"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130637#discussion_r1676370716"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130637"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676370716/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": 33, "original_start_line": 33, "start_side": "RIGHT", "line": 35, "original_line": 35, "side": "RIGHT", "original_position": 35, "position": 35, "subject_type": "line", "PR": {"title": "[custom_op] triton_op API V0", "number": 130637, "id": 1967192509}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676371826", "pull_request_review_id": 2175588953, "id": 1676371826, "node_id": "PRRC_kwDOA-j9z85j62dy", "diff_hunk": "@@ -0,0 +1,192 @@\n+from typing import Callable, Iterable, Optional, Union\n+\n+from .custom_ops import custom_op\n+\n+\n+def triton_op(\n+    name: str,\n+    fn: Optional[Callable] = None,\n+    /,\n+    *,\n+    mutates_args: Union[str, Iterable[str]],\n+    schema: Optional[str] = None,\n+) -> Callable:\n+    \"\"\"Create a custom operator whose implementation is backed by 1+ triton kernels.\n+\n+    Use this instead of :func:`torch.library.custom_op` when the implementation\n+    consists of 1+ triton kernels. :func:`torch.library.custom_op` treats\n+    custom operators as opaque (:func:`torch.compile` and\n+    :func:`torch.export.export` will never trace into them), but ``triton_op``\n+    makes the implementation visible to these subsystems, allowing them\n+    to optimize the triton kernel(s).\n+\n+    Note that ``fn`` must only consist of calls to PyTorch-understood\n+    operators and triton kernels. Any triton kernels called inside ``fn``\n+    must be wrapped in a call to :func:`torch._library.capture_triton``.\n+\n+    Args:\n+        name (str): A name for the custom op that looks like \"{namespace}::{name}\",\n+            e.g. \"mylib::my_linear\". The name is used as the op's stable identifier\n+            in PyTorch subsystems (e.g. torch.export, FX graphs).\n+            To avoid name collisions, please use your project name as the namespace;\n+            e.g. all custom ops in pytorch/fbgemm use \"fbgemm\" as the namespace.\n+        mutates_args (Iterable[str] or \"unknown\"): The names of args that the function mutates.\n+            This MUST be accurate, otherwise, the behavior is undefined. If \"unknown\",\n+            it pessimistically assumes that all inputs to the operator are being mutated.\n+        schema (None | str): A schema string for the operator. If None\n+            (recommended) we'll infer a schema for the operator from its type\n+            annotations. We recommend letting us infer a schema unless you\n+            have a specific reason not to.\n+            Example: \"(Tensor x, int y) -> (Tensor, Tensor)\".\n+\n+    Example::\n+\n+        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA)\n+        >>> import torch\n+        >>> from torch._library import triton_op, capture_triton\n+        >>>\n+        >>> import triton\n+        >>> from triton import language as tl\n+        >>>\n+        >>> @triton.jit\n+        >>> def add_kernel(\n+        >>>     in_ptr0,\n+        >>>     in_ptr1,\n+        >>>     out_ptr,\n+        >>>     n_elements,\n+        >>>     BLOCK_SIZE: \"tl.constexpr\",\n+        >>> ):\n+        >>>     pid = tl.program_id(axis=0)\n+        >>>     block_start = pid * BLOCK_SIZE\n+        >>>     offsets = block_start + tl.arange(0, BLOCK_SIZE)\n+        >>>     mask = offsets < n_elements\n+        >>>     x = tl.load(in_ptr0 + offsets, mask=mask)\n+        >>>     y = tl.load(in_ptr1 + offsets, mask=mask)\n+        >>>     output = x + y\n+        >>>     tl.store(out_ptr + offsets, output, mask=mask)\n+        >>>\n+        >>> @triton_op(\"mylib::add\", mutates_args={})\n+        >>> def add(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n+        >>>     output = torch.empty_like(x)\n+        >>>     n_elements = output.numel()\n+        >>>\n+        >>>     def grid(meta):\n+        >>>         return (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n+        >>>\n+        >>>     # NB: we need to wrap the triton kernel in a call to capture_triton\n+        >>>     capture_triton(add_kernel)[grid](x, y, output, n_elements, 16)\n+        >>>     return output\n+        >>>\n+        >>> @torch.compile\n+        >>> def f(x, y):\n+        >>>     return add(x, y)\n+        >>>\n+        >>> x = torch.randn(3, device=\"cuda\")\n+        >>> y = torch.randn(3, device=\"cuda\")\n+        >>>\n+        >>> z = f(x, y)\n+        >>> assert torch.allclose(z, x + y)\n+\n+    \"\"\"\n+\n+    def dec(fn: Callable) -> Callable:", "path": "torch/_library/triton.py", "commit_id": "1545b9201079c5dd94d5f40c904c9c6a7509e36d", "original_commit_id": "c76d8681d6f51d425ce9fe975d177b93ea566764", "user": {"login": "zou3519", "id": 5652049, "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519"}, "body": "The interesting thing about `@triton_op` is that there is nothing triton-specific about it at the moment. This seems like more of a \"how do we create a custom op that decomposes right before Inductor\" type of deal. But I think custom ops backed with triton kernels are big enough that they warrant their own high-level API.", "created_at": "2024-07-12T19:25:17Z", "updated_at": "2024-07-12T19:25:27Z", "html_url": "https://github.com/pytorch/pytorch/pull/130637#discussion_r1676371826", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130637", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676371826"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130637#discussion_r1676371826"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130637"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676371826/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 92, "original_line": 92, "side": "RIGHT", "original_position": 92, "position": 92, "subject_type": "line", "PR": {"title": "[custom_op] triton_op API V0", "number": 130637, "id": 1967192509}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676429751", "pull_request_review_id": 2175681995, "id": 1676429751, "node_id": "PRRC_kwDOA-j9z85j7Em3", "diff_hunk": "@@ -1071,17 +998,13 @@ def __getitem__(self, *args):\n         return tracing_triton_hopifier_singleton.call_getitem(self, args)\n \n     def run(self, *args, **kwargs):\n-        import torch._dynamo\n-\n-        if not is_fx_tracing() or torch._dynamo.is_compiling():\n+        if False:", "path": "torch/_higher_order_ops/triton_kernel_wrap.py", "commit_id": "1545b9201079c5dd94d5f40c904c9c6a7509e36d", "original_commit_id": "c76d8681d6f51d425ce9fe975d177b93ea566764", "user": {"login": "albanD", "id": 6359743, "url": "https://api.github.com/users/albanD", "html_url": "https://github.com/albanD"}, "body": "I guess these will be fixed?", "created_at": "2024-07-12T20:28:19Z", "updated_at": "2024-07-12T20:33:53Z", "html_url": "https://github.com/pytorch/pytorch/pull/130637#discussion_r1676429751", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130637", "author_association": "COLLABORATOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676429751"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130637#discussion_r1676429751"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130637"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676429751/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": null, "original_line": 1001, "side": "RIGHT", "original_position": 92, "position": null, "subject_type": "line", "PR": {"title": "[custom_op] triton_op API V0", "number": 130637, "id": 1967192509}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676432332", "pull_request_review_id": 2175681995, "id": 1676432332, "node_id": "PRRC_kwDOA-j9z85j7FPM", "diff_hunk": "@@ -0,0 +1,192 @@\n+from typing import Callable, Iterable, Optional, Union\n+\n+from .custom_ops import custom_op\n+\n+\n+def triton_op(\n+    name: str,\n+    fn: Optional[Callable] = None,\n+    /,\n+    *,\n+    mutates_args: Union[str, Iterable[str]],\n+    schema: Optional[str] = None,\n+) -> Callable:\n+    \"\"\"Create a custom operator whose implementation is backed by 1+ triton kernels.\n+\n+    Use this instead of :func:`torch.library.custom_op` when the implementation\n+    consists of 1+ triton kernels. :func:`torch.library.custom_op` treats\n+    custom operators as opaque (:func:`torch.compile` and\n+    :func:`torch.export.export` will never trace into them), but ``triton_op``\n+    makes the implementation visible to these subsystems, allowing them\n+    to optimize the triton kernel(s).\n+\n+    Note that ``fn`` must only consist of calls to PyTorch-understood\n+    operators and triton kernels. Any triton kernels called inside ``fn``\n+    must be wrapped in a call to :func:`torch._library.capture_triton``.\n+\n+    Args:\n+        name (str): A name for the custom op that looks like \"{namespace}::{name}\",\n+            e.g. \"mylib::my_linear\". The name is used as the op's stable identifier\n+            in PyTorch subsystems (e.g. torch.export, FX graphs).\n+            To avoid name collisions, please use your project name as the namespace;\n+            e.g. all custom ops in pytorch/fbgemm use \"fbgemm\" as the namespace.\n+        mutates_args (Iterable[str] or \"unknown\"): The names of args that the function mutates.\n+            This MUST be accurate, otherwise, the behavior is undefined. If \"unknown\",\n+            it pessimistically assumes that all inputs to the operator are being mutated.\n+        schema (None | str): A schema string for the operator. If None\n+            (recommended) we'll infer a schema for the operator from its type\n+            annotations. We recommend letting us infer a schema unless you\n+            have a specific reason not to.\n+            Example: \"(Tensor x, int y) -> (Tensor, Tensor)\".\n+\n+    Example::\n+\n+        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA)\n+        >>> import torch\n+        >>> from torch._library import triton_op, capture_triton\n+        >>>\n+        >>> import triton\n+        >>> from triton import language as tl\n+        >>>\n+        >>> @triton.jit\n+        >>> def add_kernel(\n+        >>>     in_ptr0,\n+        >>>     in_ptr1,\n+        >>>     out_ptr,\n+        >>>     n_elements,\n+        >>>     BLOCK_SIZE: \"tl.constexpr\",\n+        >>> ):\n+        >>>     pid = tl.program_id(axis=0)\n+        >>>     block_start = pid * BLOCK_SIZE\n+        >>>     offsets = block_start + tl.arange(0, BLOCK_SIZE)\n+        >>>     mask = offsets < n_elements\n+        >>>     x = tl.load(in_ptr0 + offsets, mask=mask)\n+        >>>     y = tl.load(in_ptr1 + offsets, mask=mask)\n+        >>>     output = x + y\n+        >>>     tl.store(out_ptr + offsets, output, mask=mask)\n+        >>>\n+        >>> @triton_op(\"mylib::add\", mutates_args={})\n+        >>> def add(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n+        >>>     output = torch.empty_like(x)\n+        >>>     n_elements = output.numel()\n+        >>>\n+        >>>     def grid(meta):\n+        >>>         return (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n+        >>>\n+        >>>     # NB: we need to wrap the triton kernel in a call to capture_triton\n+        >>>     capture_triton(add_kernel)[grid](x, y, output, n_elements, 16)\n+        >>>     return output\n+        >>>\n+        >>> @torch.compile\n+        >>> def f(x, y):\n+        >>>     return add(x, y)\n+        >>>\n+        >>> x = torch.randn(3, device=\"cuda\")\n+        >>> y = torch.randn(3, device=\"cuda\")\n+        >>>\n+        >>> z = f(x, y)\n+        >>> assert torch.allclose(z, x + y)\n+\n+    \"\"\"\n+\n+    def dec(fn: Callable) -> Callable:\n+        result = custom_op(name, fn, mutates_args=mutates_args)\n+        from .._subclasses.functional_tensor import FunctionalTensorMode\n+\n+        # We require that the user pass us a function that is make_fx traceable,\n+        # so we can just register it as the Fake/meta kernel.\n+        result.register_fake(fn)\n+\n+        # We decompose the operator when FunctionalTensorMode is active.\n+        # The goal is to decompose the operator in AOTDispatcher.\n+        # - With torch.compile, this means that the backend (usually Inductor)\n+        #   can see a call to the triton kernel(s) and so it can directly optimize\n+        #   them by inlining them into the lowering process.\n+        # - With post-dispatch torch.export, this means that there will\n+        #   be a call(s) to the triton_kernel_wrapper_functional HOP in the\n+        #   graph (that we have yet to figure out how to serialize).\n+        def functional_decomp(  # type: ignore[no-untyped-def]\n+            mode, _, types, args, kwargs\n+        ):\n+            with mode:\n+                return fn(*args, **kwargs)\n+\n+        result.register_torch_dispatch(FunctionalTensorMode, functional_decomp)", "path": "torch/_library/triton.py", "commit_id": "1545b9201079c5dd94d5f40c904c9c6a7509e36d", "original_commit_id": "c76d8681d6f51d425ce9fe975d177b93ea566764", "user": {"login": "albanD", "id": 6359743, "url": "https://api.github.com/users/albanD", "html_url": "https://github.com/albanD"}, "body": "Very cool!", "created_at": "2024-07-12T20:31:41Z", "updated_at": "2024-07-12T20:33:53Z", "html_url": "https://github.com/pytorch/pytorch/pull/130637#discussion_r1676432332", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130637", "author_association": "COLLABORATOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676432332"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130637#discussion_r1676432332"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130637"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676432332/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 114, "original_line": 114, "side": "RIGHT", "original_position": 114, "position": 114, "subject_type": "line", "PR": {"title": "[custom_op] triton_op API V0", "number": 130637, "id": 1967192509}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1675671238", "pull_request_review_id": 2174437638, "id": 1675671238, "node_id": "PRRC_kwDOA-j9z85j4LbG", "diff_hunk": "@@ -3524,10 +3524,22 @@ std::tuple<Tensor&, Tensor&, Tensor&, Tensor&> linalg_lstsq_out(\n   TORCH_CHECK(\n       0 <= dim_diff && dim_diff <= 1,\n       \"torch.linalg.lstsq: input.dim() must be greater or equal to other.dim() and (input.dim() - other.dim()) <= 1\");\n-  Tensor other_2d = dim_diff ? other.unsqueeze(-1) : other;\n+\n+  // now check whether the provided output tensors can be used directly\n+\n+  // Two types of 'other' tensors are supported:\n+  // - 1-dimensional (1D) tensor or batch of 1D tensors (vector case)\n+  // - 2-dimensional (2D) tensor or batch of 2D tensors (matrix case)\n+  // original torch.lstsq supported only the matrix case, while NumPy works for both cases\n+  // for the batched input we need to be able to distinguish them\n+  // auto expected_batched_rhs_shape = IntArrayRef(input.sizes().data(), input.dim() - 1); // input.shape[:-1]\n+  // bool vector_case = other.dim() == 1 || (input.dim() - 1 == other.dim() && other.sizes().equals(expected_batched_rhs_shape));\n+\n+  bool vector_case = linalg_solve_is_vector_rhs(input, other);\n+  Tensor other_2d = vector_case ? other.unsqueeze(-1) : other;\n   TORCH_CHECK(\n       input.size(-2) == other_2d.size(-2),\n-      dim_diff ? \"torch.linalg.lstsq: input.size(-2) should match other.size(-1)\"\n+      vector_case ? \"torch.linalg.lstsq: input.size(-2) should match other.size(-1)\"\n                : \"torch.linalg.lstsq: input.size(-2) should match other.size(-2)\");", "path": "aten/src/ATen/native/BatchLinearAlgebra.cpp", "commit_id": "06caa7db9940a01c6f06dfda28b3bfb102c9185b", "original_commit_id": "1c4b0486695519b124d0faf940090eb137cfc7b9", "user": {"login": "nikitaved", "id": 1255244, "url": "https://api.github.com/users/nikitaved", "html_url": "https://github.com/nikitaved"}, "body": "Let's add a test!", "created_at": "2024-07-12T10:45:15Z", "updated_at": "2024-07-12T10:45:16Z", "html_url": "https://github.com/pytorch/pytorch/pull/130612#discussion_r1675671238", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130612", "author_association": "COLLABORATOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1675671238"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130612#discussion_r1675671238"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130612"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1675671238/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": 3540, "original_start_line": 3540, "start_side": "RIGHT", "line": 3543, "original_line": 3543, "side": "RIGHT", "original_position": 22, "position": 22, "subject_type": "line", "PR": {"title": "fix torch.linalg.lstsq input check", "number": 130612, "id": 1966294957}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676104952", "pull_request_review_id": 2175188950, "id": 1676104952, "node_id": "PRRC_kwDOA-j9z85j51T4", "diff_hunk": "@@ -3524,10 +3524,22 @@ std::tuple<Tensor&, Tensor&, Tensor&, Tensor&> linalg_lstsq_out(\n   TORCH_CHECK(\n       0 <= dim_diff && dim_diff <= 1,\n       \"torch.linalg.lstsq: input.dim() must be greater or equal to other.dim() and (input.dim() - other.dim()) <= 1\");\n-  Tensor other_2d = dim_diff ? other.unsqueeze(-1) : other;\n+\n+  // now check whether the provided output tensors can be used directly\n+\n+  // Two types of 'other' tensors are supported:\n+  // - 1-dimensional (1D) tensor or batch of 1D tensors (vector case)\n+  // - 2-dimensional (2D) tensor or batch of 2D tensors (matrix case)\n+  // original torch.lstsq supported only the matrix case, while NumPy works for both cases\n+  // for the batched input we need to be able to distinguish them\n+  // auto expected_batched_rhs_shape = IntArrayRef(input.sizes().data(), input.dim() - 1); // input.shape[:-1]\n+  // bool vector_case = other.dim() == 1 || (input.dim() - 1 == other.dim() && other.sizes().equals(expected_batched_rhs_shape));\n+\n+  bool vector_case = linalg_solve_is_vector_rhs(input, other);\n+  Tensor other_2d = vector_case ? other.unsqueeze(-1) : other;\n   TORCH_CHECK(\n       input.size(-2) == other_2d.size(-2),\n-      dim_diff ? \"torch.linalg.lstsq: input.size(-2) should match other.size(-1)\"\n+      vector_case ? \"torch.linalg.lstsq: input.size(-2) should match other.size(-1)\"\n                : \"torch.linalg.lstsq: input.size(-2) should match other.size(-2)\");", "path": "aten/src/ATen/native/BatchLinearAlgebra.cpp", "commit_id": "06caa7db9940a01c6f06dfda28b3bfb102c9185b", "original_commit_id": "1c4b0486695519b124d0faf940090eb137cfc7b9", "user": {"login": "inkcherry", "id": 27563729, "url": "https://api.github.com/users/inkcherry", "html_url": "https://github.com/inkcherry"}, "body": "thanks for the review! added @nikitaved , @lezcano ", "created_at": "2024-07-12T15:27:33Z", "updated_at": "2024-07-12T15:27:33Z", "html_url": "https://github.com/pytorch/pytorch/pull/130612#discussion_r1676104952", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130612", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676104952"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130612#discussion_r1676104952"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130612"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676104952/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": 3540, "original_start_line": 3540, "start_side": "RIGHT", "line": 3543, "original_line": 3543, "side": "RIGHT", "in_reply_to_id": 1675671238, "original_position": 22, "position": 22, "subject_type": "line", "PR": {"title": "fix torch.linalg.lstsq input check", "number": 130612, "id": 1966294957}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1675314554", "pull_request_review_id": 2173871500, "id": 1675314554, "node_id": "PRRC_kwDOA-j9z85j20V6", "diff_hunk": "@@ -706,26 +686,14 @@ def sdpa_dense_backward(\n \n     mask_graph = block_mask[-1]\n     # Gradient of the inline score_mod function, with respect to the scores\n-    in_dim_buffers = (None,) * len(score_mod_other_buffers)\n+    captured_buffers_in_dim = (None,) * len(score_mod_other_buffers)\n     out_dims = [0, None, None, None, None] + [None] * len(score_mod_other_buffers)\n-    joint_score_mod = torch.vmap(\n+    from torch.nn.attention._flex_attention import _vmap_for_bhqkv\n+\n+    joint_score_mod = _vmap_for_bhqkv(\n         joint_graph,\n-        in_dims=(0, None, None, None, 0, 0) + in_dim_buffers,\n-        out_dims=out_dims,\n-    )\n-    joint_score_mod = torch.vmap(\n-        joint_score_mod,\n-        in_dims=(0, None, None, 0, None, 0) + in_dim_buffers,\n-        out_dims=out_dims,\n-    )\n-    joint_score_mod = torch.vmap(\n-        joint_score_mod,\n-        in_dims=(0, None, 0, None, None, 0) + in_dim_buffers,\n-        out_dims=out_dims,\n-    )\n-    joint_score_mod = torch.vmap(\n-        joint_score_mod,\n-        in_dims=(0, 0, None, None, None, 0) + in_dim_buffers,\n+        prefix=(0,),", "path": "torch/_higher_order_ops/flex_attention.py", "commit_id": "b474504101797c8c86b8b65277f8c8d82264b01e", "original_commit_id": "c78d691ea7773e56374721fd8dff079a6e88e262", "user": {"login": "drisspg", "id": 32754868, "url": "https://api.github.com/users/drisspg", "html_url": "https://github.com/drisspg"}, "body": "nit: maybe a comment on why we need this", "created_at": "2024-07-12T04:54:54Z", "updated_at": "2024-07-12T04:54:55Z", "html_url": "https://github.com/pytorch/pytorch/pull/130605#discussion_r1675314554", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130605", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1675314554"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130605#discussion_r1675314554"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130605"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1675314554/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 698, "original_line": 695, "side": "RIGHT", "original_position": 106, "position": 109, "subject_type": "line", "PR": {"title": "some cleanups of flexattention full block support", "number": 130605, "id": 1965897345}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1675319765", "pull_request_review_id": 2173879605, "id": 1675319765, "node_id": "PRRC_kwDOA-j9z85j21nV", "diff_hunk": "@@ -188,100 +192,106 @@ def convert_output_node_to_buffer(output):\n     l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n     acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n \n-    # ~~~~~~~~~~~~~~ fully unmasked blocks ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n-    # FULL_KV_IDX and FULL_KV_NUM_BLKS are always contiguous.\n+    offs_m = q_start * BLOCK_M + tl.arange(0, BLOCK_M)\n+\n+    # FULL_KV_IDX and KV_NUM_BLKS are always contiguous.\n     sparse_hz_offset = sparse_idx_z * SPARSE_H + sparse_idx_h", "path": "torch/_inductor/kernel/flex_attention.py", "commit_id": "b474504101797c8c86b8b65277f8c8d82264b01e", "original_commit_id": "c78d691ea7773e56374721fd8dff079a6e88e262", "user": {"login": "drisspg", "id": 32754868, "url": "https://api.github.com/users/drisspg", "html_url": "https://github.com/drisspg"}, "body": "Noob Q: Lets say mask fn was const over B and H, could we make a  sparse_kv_idx non-contiguous by broadcasting  the B, H dims ", "created_at": "2024-07-12T05:04:15Z", "updated_at": "2024-07-12T05:04:16Z", "html_url": "https://github.com/pytorch/pytorch/pull/130605#discussion_r1675319765", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130605", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1675319765"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130605#discussion_r1675319765"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130605"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1675319765/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 187, "original_line": 198, "side": "RIGHT", "original_position": 60, "position": 86, "subject_type": "line", "PR": {"title": "some cleanups of flexattention full block support", "number": 130605, "id": 1965897345}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1675320072", "pull_request_review_id": 2173880070, "id": 1675320072, "node_id": "PRRC_kwDOA-j9z85j21sI", "diff_hunk": "@@ -188,100 +192,106 @@ def convert_output_node_to_buffer(output):\n     l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n     acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n \n-    # ~~~~~~~~~~~~~~ fully unmasked blocks ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n-    # FULL_KV_IDX and FULL_KV_NUM_BLKS are always contiguous.\n+    offs_m = q_start * BLOCK_M + tl.arange(0, BLOCK_M)\n+\n+    # FULL_KV_IDX and KV_NUM_BLKS are always contiguous.\n     sparse_hz_offset = sparse_idx_z * SPARSE_H + sparse_idx_h", "path": "torch/_inductor/kernel/flex_attention.py", "commit_id": "b474504101797c8c86b8b65277f8c8d82264b01e", "original_commit_id": "c78d691ea7773e56374721fd8dff079a6e88e262", "user": {"login": "drisspg", "id": 32754868, "url": "https://api.github.com/users/drisspg", "html_url": "https://github.com/drisspg"}, "body": "also feels safer to just use the stride info ", "created_at": "2024-07-12T05:04:49Z", "updated_at": "2024-07-12T05:04:49Z", "html_url": "https://github.com/pytorch/pytorch/pull/130605#discussion_r1675320072", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130605", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1675320072"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130605#discussion_r1675320072"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130605"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1675320072/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 187, "original_line": 198, "side": "RIGHT", "in_reply_to_id": 1675319765, "original_position": 60, "position": 86, "subject_type": "line", "PR": {"title": "some cleanups of flexattention full block support", "number": 130605, "id": 1965897345}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676206290", "pull_request_review_id": 2175347367, "id": 1676206290, "node_id": "PRRC_kwDOA-j9z85j6ODS", "diff_hunk": "@@ -65,59 +111,57 @@ def _no_mask(\n \n \n class BlockMask:\n-    full_kv_num_blocks: torch.Tensor\n-    full_kv_indices: torch.Tensor\n-    full_q_num_blocks: torch.Tensor\n-    full_q_indices: torch.Tensor\n-    partial_kv_num_blocks: torch.Tensor\n-    partial_kv_indices: torch.Tensor\n-    partial_q_num_blocks: torch.Tensor\n-    partial_q_indices: torch.Tensor\n+    kv_num_blocks: torch.Tensor\n+    kv_indices: torch.Tensor\n+    q_num_blocks: torch.Tensor\n+    q_indices: torch.Tensor\n+    full_kv_num_blocks: Optional[torch.Tensor]\n+    full_kv_indices: Optional[torch.Tensor]\n+    full_q_num_blocks: Optional[torch.Tensor]\n+    full_q_indices: Optional[torch.Tensor]\n     KV_BLOCK_SIZE: int", "path": "torch/nn/attention/_flex_attention.py", "commit_id": "b474504101797c8c86b8b65277f8c8d82264b01e", "original_commit_id": "141f3e0d34e0ae065a5fbd5385ae614d924d7c87", "user": {"login": "Chillee", "id": 6355099, "url": "https://api.github.com/users/Chillee", "html_url": "https://github.com/Chillee"}, "body": "I renamed the \"full vs partial\" blocks.\r\n\r\n# Before:\r\n **no mask fn**\r\nfull_kv_num_blocks = all the blocks computed\r\npartial_kv_num_blocks = placeholder empty value with all zeros\r\n** with mask fn**\r\nfull_kv_num_blocks = only the blocks that are completely full\r\npartial_kv_num_blocks = blocks that aren't full and aren't empty.\r\n\r\n# After:\r\n **no mask fn**\r\nkv_num_blocks = all the blocks computed\r\nfull_kv_num_blocks = None\r\n** with mask fn**\r\nkv_num_blocks = the blocks that are partially computed\r\nfull_kv_num_blocks = only the blocks that we know are completely full\r\n\r\nThis makes more sense to me. Basically, `kv_num_blocks` are blocks that we don't know anything special about. So we always need to apply both the `score_mod` and the `mask_mod`. If passed a `mask_mod` to BlockMask and given `full_kv_num_blocks`, then we know have some blocks that we know are \"special\" and don't have masking applied. So we only need to apply `score_mod`.", "created_at": "2024-07-12T17:00:28Z", "updated_at": "2024-07-12T17:28:40Z", "html_url": "https://github.com/pytorch/pytorch/pull/130605#discussion_r1676206290", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130605", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676206290"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130605#discussion_r1676206290"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130605"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676206290/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 122, "original_line": 122, "side": "RIGHT", "original_position": 82, "position": 82, "subject_type": "line", "PR": {"title": "some cleanups of flexattention full block support", "number": 130605, "id": 1965897345}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674823794", "pull_request_review_id": 2173292032, "id": 1674823794, "node_id": "PRRC_kwDOA-j9z85j08hy", "diff_hunk": "@@ -1642,7 +1642,7 @@ def compute_mutation_region_ids(graph: torch.fx.GraphModule) -> None:\n class PatternMatcherPass:\n     def __init__(\n         self,\n-        prevent_match_across_mutations: bool = False,\n+        prevent_match_across_mutations: bool = True,", "path": "torch/_inductor/pattern_matcher.py", "commit_id": "e363e1ee0ee21068ecea7f4e854467f22461ed13", "original_commit_id": "339b9782b2aaf10e63f1859cd066ac5fb3913d0a", "user": {"login": "jansel", "id": 533820, "url": "https://api.github.com/users/jansel", "html_url": "https://github.com/jansel"}, "body": "Should we just remove this flag entirely?", "created_at": "2024-07-11T23:39:45Z", "updated_at": "2024-07-11T23:39:47Z", "html_url": "https://github.com/pytorch/pytorch/pull/130584#discussion_r1674823794", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130584", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674823794"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130584#discussion_r1674823794"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130584"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674823794/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": null, "original_line": 1645, "side": "RIGHT", "original_position": 5, "position": null, "subject_type": "line", "PR": {"title": "[Inductor][PatternMatcher] Always prevent match across mutations", "number": 130584, "id": 1965593677}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676379038", "pull_request_review_id": 2175600407, "id": 1676379038, "node_id": "PRRC_kwDOA-j9z85j64Oe", "diff_hunk": "@@ -1642,7 +1642,7 @@ def compute_mutation_region_ids(graph: torch.fx.GraphModule) -> None:\n class PatternMatcherPass:\n     def __init__(\n         self,\n-        prevent_match_across_mutations: bool = False,\n+        prevent_match_across_mutations: bool = True,", "path": "torch/_inductor/pattern_matcher.py", "commit_id": "e363e1ee0ee21068ecea7f4e854467f22461ed13", "original_commit_id": "339b9782b2aaf10e63f1859cd066ac5fb3913d0a", "user": {"login": "yf225", "id": 4063635, "url": "https://api.github.com/users/yf225", "html_url": "https://github.com/yf225"}, "body": "Updated PR to remove this flag entirely", "created_at": "2024-07-12T19:33:57Z", "updated_at": "2024-07-12T19:33:57Z", "html_url": "https://github.com/pytorch/pytorch/pull/130584#discussion_r1676379038", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130584", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676379038"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130584#discussion_r1676379038"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130584"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676379038/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": null, "original_line": 1645, "side": "RIGHT", "in_reply_to_id": 1674823794, "original_position": 5, "position": null, "subject_type": "line", "PR": {"title": "[Inductor][PatternMatcher] Always prevent match across mutations", "number": 130584, "id": 1965593677}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1675946755", "pull_request_review_id": 2174909229, "id": 1675946755, "node_id": "PRRC_kwDOA-j9z85j5OsD", "diff_hunk": "@@ -8,9 +8,12 @@\n class _ParameterMeta(torch._C._TensorMeta):\n     # Make `isinstance(t, Parameter)` return True for custom tensor instances that have the _is_param flag.\n     def __instancecheck__(self, instance):\n-        return super().__instancecheck__(instance) or (\n-            isinstance(instance, torch.Tensor) and getattr(instance, \"_is_param\", False)\n-        )\n+        if self is Parameter:\n+            if isinstance(instance, torch.Tensor) and getattr(", "path": "torch/nn/parameter.py", "commit_id": "1a2015bbdb761ed4e0713e2374def6c935255934", "original_commit_id": "1a2015bbdb761ed4e0713e2374def6c935255934", "user": {"login": "Skylion007", "id": 2053727, "url": "https://api.github.com/users/Skylion007", "html_url": "https://github.com/Skylion007"}, "body": "nit can be one if statement, also can technically be one boolean although this might be marginally more readable.", "created_at": "2024-07-12T13:40:04Z", "updated_at": "2024-07-12T13:40:05Z", "html_url": "https://github.com/pytorch/pytorch/pull/130578#discussion_r1675946755", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130578", "author_association": "COLLABORATOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1675946755"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130578#discussion_r1675946755"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130578"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1675946755/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 12, "original_line": 12, "side": "RIGHT", "original_position": 8, "position": 8, "subject_type": "line", "PR": {"title": "Only test _is_param if doing instance check on Parameter base", "number": 130578, "id": 1965531161}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1675956742", "pull_request_review_id": 2174936431, "id": 1675956742, "node_id": "PRRC_kwDOA-j9z85j5RIG", "diff_hunk": "@@ -8,9 +8,12 @@\n class _ParameterMeta(torch._C._TensorMeta):\n     # Make `isinstance(t, Parameter)` return True for custom tensor instances that have the _is_param flag.\n     def __instancecheck__(self, instance):\n-        return super().__instancecheck__(instance) or (\n-            isinstance(instance, torch.Tensor) and getattr(instance, \"_is_param\", False)\n-        )\n+        if self is Parameter:\n+            if isinstance(instance, torch.Tensor) and getattr(", "path": "torch/nn/parameter.py", "commit_id": "1a2015bbdb761ed4e0713e2374def6c935255934", "original_commit_id": "1a2015bbdb761ed4e0713e2374def6c935255934", "user": {"login": "ezyang", "id": 13564, "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang"}, "body": "yeah, I know this can be golfed but this seemed like good code flow to me", "created_at": "2024-07-12T13:47:34Z", "updated_at": "2024-07-12T13:47:34Z", "html_url": "https://github.com/pytorch/pytorch/pull/130578#discussion_r1675956742", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130578", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1675956742"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130578#discussion_r1675956742"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130578"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1675956742/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 12, "original_line": 12, "side": "RIGHT", "in_reply_to_id": 1675946755, "original_position": 8, "position": 8, "subject_type": "line", "PR": {"title": "Only test _is_param if doing instance check on Parameter base", "number": 130578, "id": 1965531161}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674720576", "pull_request_review_id": 2173091295, "id": 1674720576, "node_id": "PRRC_kwDOA-j9z85j0jVA", "diff_hunk": "@@ -234,26 +233,24 @@ def func(a, *, tag, ranks, group_size):\n             # 2. then, we schedule the ops (g) that ARE NOT required for second all_reduce and DO NOT depend on first all_reduce.\n             # 3. then, we schedule the ops (f) that ARE required for second all_reduce and DO depend on first all_reduce.\n             # and then, we schedule the second all_reduce. And then schedule all ops that depend on second all_reduce.\n-            FileCheck().check(\"dist.all_reduce(\").check(\"triton_poi_fused_relu\").check(\n-                \"extern_kernels.mm(\"\n-            ).check(\"extern_kernels.mm(\").check(\"_wait_tensor(\").check(\n-                \"triton_poi_fused_mul\"\n-            ).check(\n-                \"dist.all_reduce(\"\n-            ).check(\n-                \"_wait_tensor(\"\n-            ).check(\n-                \"triton_poi_fused_add\"\n-            ).check(\n-                \"extern_kernels.mm(\"\n-            ).run(\n-                code\n+            (", "path": "test/distributed/test_compute_comm_reordering.py", "commit_id": "6d93d5f9c4dbfa84a6a71c928c7f04aa03ee566a", "original_commit_id": "6d93d5f9c4dbfa84a6a71c928c7f04aa03ee566a", "user": {"login": "Chillee", "id": 6355099, "url": "https://api.github.com/users/Chillee", "html_url": "https://github.com/Chillee"}, "body": "One thing I think we should add is - I think we should write a (small) simulator that simulates the expected overlap after our reordering/optimization passes. That would make these tests much cleaner to write.", "created_at": "2024-07-11T21:36:31Z", "updated_at": "2024-07-11T21:40:58Z", "html_url": "https://github.com/pytorch/pytorch/pull/130573#discussion_r1674720576", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130573", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674720576"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130573#discussion_r1674720576"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130573"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674720576/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 236, "original_line": 236, "side": "RIGHT", "original_position": 26, "position": 26, "subject_type": "line", "PR": {"title": "Fix and improve reorder_compute_for_overlap", "number": 130573, "id": 1965404300}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674720920", "pull_request_review_id": 2173091295, "id": 1674720920, "node_id": "PRRC_kwDOA-j9z85j0jaY", "diff_hunk": "@@ -196,206 +242,6 @@ def estimate_op_runtime(snode: BaseSchedulerNode) -> float:\n     return runtime\n \n \n-def compute_node_users(", "path": "torch/_inductor/comms.py", "commit_id": "6d93d5f9c4dbfa84a6a71c928c7f04aa03ee566a", "original_commit_id": "6d93d5f9c4dbfa84a6a71c928c7f04aa03ee566a", "user": {"login": "Chillee", "id": 6355099, "url": "https://api.github.com/users/Chillee", "html_url": "https://github.com/Chillee"}, "body": "We can delete all this code since now, `raise_comms/sink_waits` use a very similar logic to this? Deleting code is great :)", "created_at": "2024-07-11T21:36:58Z", "updated_at": "2024-07-11T21:40:58Z", "html_url": "https://github.com/pytorch/pytorch/pull/130573#discussion_r1674720920", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130573", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674720920"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130573#discussion_r1674720920"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130573"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674720920/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 199, "original_line": 199, "side": "LEFT", "original_position": 210, "position": 210, "subject_type": "line", "PR": {"title": "Fix and improve reorder_compute_for_overlap", "number": 130573, "id": 1965404300}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676314485", "pull_request_review_id": 2175499784, "id": 1676314485, "node_id": "PRRC_kwDOA-j9z85j6od1", "diff_hunk": "@@ -196,206 +242,6 @@ def estimate_op_runtime(snode: BaseSchedulerNode) -> float:\n     return runtime\n \n \n-def compute_node_users(", "path": "torch/_inductor/comms.py", "commit_id": "6d93d5f9c4dbfa84a6a71c928c7f04aa03ee566a", "original_commit_id": "6d93d5f9c4dbfa84a6a71c928c7f04aa03ee566a", "user": {"login": "yifuwang", "id": 4156752, "url": "https://api.github.com/users/yifuwang", "html_url": "https://github.com/yifuwang"}, "body": "Yes. To make raise_comms/sink_waits more effective, it used similar logic as reorder_compute_for_overlap (i.e., ready queue based scheduling).", "created_at": "2024-07-12T18:21:31Z", "updated_at": "2024-07-12T18:21:31Z", "html_url": "https://github.com/pytorch/pytorch/pull/130573#discussion_r1676314485", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130573", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676314485"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130573#discussion_r1676314485"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130573"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676314485/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 199, "original_line": 199, "side": "LEFT", "in_reply_to_id": 1674720920, "original_position": 210, "position": 210, "subject_type": "line", "PR": {"title": "Fix and improve reorder_compute_for_overlap", "number": 130573, "id": 1965404300}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674624261", "pull_request_review_id": 2172924804, "id": 1674624261, "node_id": "PRRC_kwDOA-j9z85j0L0F", "diff_hunk": "@@ -5772,10 +5772,6 @@ def scalar_tensor(s, dtype=None, layout=None, device=None, pin_memory=None):\n def topk_meta(self, k, dim=-1, largest=True, sorted=True):\n     # From aten/src/ATen/native/Sorting.cpp\n     dim = maybe_wrap_dim(dim, self.dim(), wrap_scalar=True)\n-    torch._check(\n-        k >= 0 and k <= (self.size(dim) if self.dim() > 0 else 1),\n-        lambda: \"selected index k out of range\",\n-    )\n     sliceSize = 1 if self.dim() == 0 else self.size(dim)\n     torch._check(k >= 0 and k <= sliceSize, lambda: \"k not in range for dimension\")", "path": "torch/_meta_registrations.py", "commit_id": "8def6454378c22130fbf09087b9eaa9a641f277f", "original_commit_id": "1dea1a1d63b8ef28635f0909dc7fd53314431c03", "user": {"login": "ColinPeppler", "id": 23249258, "url": "https://api.github.com/users/ColinPeppler", "html_url": "https://github.com/ColinPeppler"}, "body": "Found out these two `torch._checks` check the same thing. So, I removed the first one :)", "created_at": "2024-07-11T20:02:47Z", "updated_at": "2024-07-11T20:02:47Z", "html_url": "https://github.com/pytorch/pytorch/pull/130562#discussion_r1674624261", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130562", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674624261"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130562#discussion_r1674624261"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130562"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674624261/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 5776, "original_line": 5776, "side": "RIGHT", "original_position": 9, "position": 9, "subject_type": "line", "PR": {"title": "[dynamo] add meta fn for aten.kthvalue.default", "number": 130562, "id": 1965278547}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1675046421", "pull_request_review_id": 2173576784, "id": 1675046421, "node_id": "PRRC_kwDOA-j9z85j1y4V", "diff_hunk": "@@ -1999,7 +1999,6 @@ def f(t):\n     xfail('geqrf', ''),  # aten.geqrf.default - couldn't find symbolic meta function/decomposition\n     xfail('histogram', ''),  # Could not run 'aten::histogram.bin_ct' with arguments from the 'Meta' backend. This c...\n     xfail('histogramdd', ''),  # aten._histogramdd_bin_edges.default - couldn't find symbolic meta function/decomposition\n-    xfail('kthvalue', ''),  # aten.kthvalue.default - couldn't find symbolic meta function/decomposition", "path": "test/test_proxy_tensor.py", "commit_id": "8def6454378c22130fbf09087b9eaa9a641f277f", "original_commit_id": "e57189de8f135d72ac2c14d6b792eff8322ea737", "user": {"login": "zou3519", "id": 5652049, "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519"}, "body": "Add `kthvalue` to out_symbolic_tensor_failures. You added a meta for one of the two kthvalue operators but not the other one. This xfail was blocking both of those from showing up in CI", "created_at": "2024-07-12T02:57:21Z", "updated_at": "2024-07-12T02:57:21Z", "html_url": "https://github.com/pytorch/pytorch/pull/130562#discussion_r1675046421", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130562", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1675046421"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130562#discussion_r1675046421"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130562"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1675046421/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 2002, "original_line": 2002, "side": "LEFT", "original_position": 4, "position": 4, "subject_type": "line", "PR": {"title": "[dynamo] add meta fn for aten.kthvalue.default", "number": 130562, "id": 1965278547}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676383687", "pull_request_review_id": 2175607800, "id": 1676383687, "node_id": "PRRC_kwDOA-j9z85j65XH", "diff_hunk": "@@ -5785,6 +5781,22 @@ def topk_meta(self, k, dim=-1, largest=True, sorted=True):\n     return self.new_empty(topKSize), self.new_empty(topKSize, dtype=torch.int64)\n \n \n+@register_meta([aten.kthvalue.default, aten.kthvalue.values])\n+@out_wrapper(\"values\", \"indices\")", "path": "torch/_meta_registrations.py", "commit_id": "8def6454378c22130fbf09087b9eaa9a641f277f", "original_commit_id": "8def6454378c22130fbf09087b9eaa9a641f277f", "user": {"login": "ColinPeppler", "id": 23249258, "url": "https://api.github.com/users/ColinPeppler", "html_url": "https://github.com/ColinPeppler"}, "body": "Adding this helped", "created_at": "2024-07-12T19:39:51Z", "updated_at": "2024-07-12T19:39:52Z", "html_url": "https://github.com/pytorch/pytorch/pull/130562#discussion_r1676383687", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130562", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676383687"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130562#discussion_r1676383687"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130562"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676383687/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 5785, "original_line": 5785, "side": "RIGHT", "original_position": 16, "position": 16, "subject_type": "line", "PR": {"title": "[dynamo] add meta fn for aten.kthvalue.default", "number": 130562, "id": 1965278547}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676386814", "pull_request_review_id": 2175612765, "id": 1676386814, "node_id": "PRRC_kwDOA-j9z85j66H-", "diff_hunk": "@@ -5785,6 +5781,22 @@ def topk_meta(self, k, dim=-1, largest=True, sorted=True):\n     return self.new_empty(topKSize), self.new_empty(topKSize, dtype=torch.int64)\n \n \n+@register_meta([aten.kthvalue.default, aten.kthvalue.values])\n+@out_wrapper(\"values\", \"indices\")", "path": "torch/_meta_registrations.py", "commit_id": "8def6454378c22130fbf09087b9eaa9a641f277f", "original_commit_id": "8def6454378c22130fbf09087b9eaa9a641f277f", "user": {"login": "zou3519", "id": 5652049, "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519"}, "body": "nice", "created_at": "2024-07-12T19:43:44Z", "updated_at": "2024-07-12T19:43:44Z", "html_url": "https://github.com/pytorch/pytorch/pull/130562#discussion_r1676386814", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130562", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676386814"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130562#discussion_r1676386814"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130562"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676386814/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 5785, "original_line": 5785, "side": "RIGHT", "in_reply_to_id": 1676383687, "original_position": 16, "position": 16, "subject_type": "line", "PR": {"title": "[dynamo] add meta fn for aten.kthvalue.default", "number": 130562, "id": 1965278547}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674629011", "pull_request_review_id": 2172932718, "id": 1674629011, "node_id": "PRRC_kwDOA-j9z85j0M-T", "diff_hunk": "@@ -1625,6 +1625,34 @@ def forward(self, x):\n \n         return changed\n \n+    def _eliminate_dead_code_custom(self, is_impure_node: Callable[[Node], bool]):", "path": "torch/fx/graph.py", "commit_id": "de0d172d73ce73b85874477532ba1e51ecb63734", "original_commit_id": "845bd629a49f605d8aa345f61aea43df0660d3ed", "user": {"login": "pianpwk", "id": 32071673, "url": "https://api.github.com/users/pianpwk", "html_url": "https://github.com/pianpwk"}, "body": "I think this can just be a signature change to `eliminate_dead_code()`?", "created_at": "2024-07-11T20:07:34Z", "updated_at": "2024-07-11T20:07:34Z", "html_url": "https://github.com/pytorch/pytorch/pull/130552#discussion_r1674629011", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130552", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674629011"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130552#discussion_r1674629011"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130552"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674629011/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": null, "original_line": 1628, "side": "RIGHT", "original_position": 4, "position": null, "subject_type": "line", "PR": {"title": "[FX][export] strict DCE pass, check schema for node impurity", "number": 130552, "id": 1964988210}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674656404", "pull_request_review_id": 2172979977, "id": 1674656404, "node_id": "PRRC_kwDOA-j9z85j0TqU", "diff_hunk": "@@ -1625,6 +1625,34 @@ def forward(self, x):\n \n         return changed\n \n+    def _eliminate_dead_code_custom(self, is_impure_node: Callable[[Node], bool]):", "path": "torch/fx/graph.py", "commit_id": "de0d172d73ce73b85874477532ba1e51ecb63734", "original_commit_id": "845bd629a49f605d8aa345f61aea43df0660d3ed", "user": {"login": "yushangdi", "id": 22356083, "url": "https://api.github.com/users/yushangdi", "html_url": "https://github.com/yushangdi"}, "body": "fixed now!", "created_at": "2024-07-11T20:32:59Z", "updated_at": "2024-07-11T20:32:59Z", "html_url": "https://github.com/pytorch/pytorch/pull/130552#discussion_r1674656404", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130552", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674656404"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130552#discussion_r1674656404"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130552"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674656404/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": null, "original_line": 1628, "side": "RIGHT", "in_reply_to_id": 1674629011, "original_position": 4, "position": null, "subject_type": "line", "PR": {"title": "[FX][export] strict DCE pass, check schema for node impurity", "number": 130552, "id": 1964988210}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674662604", "pull_request_review_id": 2172990840, "id": 1674662604, "node_id": "PRRC_kwDOA-j9z85j0VLM", "diff_hunk": "@@ -1625,6 +1625,34 @@ def forward(self, x):\n \n         return changed\n \n+    def _eliminate_dead_code_custom(self, is_impure_node: Callable[[Node], bool]):", "path": "torch/fx/graph.py", "commit_id": "de0d172d73ce73b85874477532ba1e51ecb63734", "original_commit_id": "845bd629a49f605d8aa345f61aea43df0660d3ed", "user": {"login": "pianpwk", "id": 32071673, "url": "https://api.github.com/users/pianpwk", "html_url": "https://github.com/pianpwk"}, "body": "nice! this looks great", "created_at": "2024-07-11T20:36:04Z", "updated_at": "2024-07-11T20:36:04Z", "html_url": "https://github.com/pytorch/pytorch/pull/130552#discussion_r1674662604", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130552", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674662604"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130552#discussion_r1674662604"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130552"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674662604/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": null, "original_line": 1628, "side": "RIGHT", "in_reply_to_id": 1674629011, "original_position": 4, "position": null, "subject_type": "line", "PR": {"title": "[FX][export] strict DCE pass, check schema for node impurity", "number": 130552, "id": 1964988210}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674378529", "pull_request_review_id": 2172513888, "id": 1674378529, "node_id": "PRRC_kwDOA-j9z85jzP0h", "diff_hunk": "@@ -367,7 +367,7 @@ def _nll_loss_and_log_softmax_backward(\n     masked_safe_target = partial_placement._partition_value(safe_target, mesh, mesh_dim)\n     # only update grad_input to -1 if not masked\n     assert partial_placement.mask_buffer.data is not None\n-    grad_update = partial_placement.mask_buffer.data.float() - 1.0\n+    grad_update = partial_placement.mask_buffer.data.to(grad_input.dtype) - 1.0", "path": "torch/distributed/tensor/parallel/loss.py", "commit_id": "f3eb4ef71a420c9f1a1be69544f4b3d2186144e2", "original_commit_id": "f3eb4ef71a420c9f1a1be69544f4b3d2186144e2", "user": {"login": "awgu", "id": 31054793, "url": "https://api.github.com/users/awgu", "html_url": "https://github.com/awgu"}, "body": "@tianyu-l I think we should avoid using `.data` here since its intent can be ambiguous. If you want it to not be part of autograd graph, then you should use `.detach()`.", "created_at": "2024-07-11T17:09:40Z", "updated_at": "2024-07-11T17:09:40Z", "html_url": "https://github.com/pytorch/pytorch/pull/130550#discussion_r1674378529", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130550", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674378529"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130550#discussion_r1674378529"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130550"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674378529/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 370, "original_line": 370, "side": "RIGHT", "original_position": 5, "position": 5, "subject_type": "line", "PR": {"title": "Fix loss_parallel with BF16 logits", "number": 130550, "id": 1964977007}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674602894", "pull_request_review_id": 2172897260, "id": 1674602894, "node_id": "PRRC_kwDOA-j9z85j0GmO", "diff_hunk": "@@ -367,7 +367,7 @@ def _nll_loss_and_log_softmax_backward(\n     masked_safe_target = partial_placement._partition_value(safe_target, mesh, mesh_dim)\n     # only update grad_input to -1 if not masked\n     assert partial_placement.mask_buffer.data is not None\n-    grad_update = partial_placement.mask_buffer.data.float() - 1.0\n+    grad_update = partial_placement.mask_buffer.data.to(grad_input.dtype) - 1.0", "path": "torch/distributed/tensor/parallel/loss.py", "commit_id": "f3eb4ef71a420c9f1a1be69544f4b3d2186144e2", "original_commit_id": "f3eb4ef71a420c9f1a1be69544f4b3d2186144e2", "user": {"login": "mayank31398", "id": 32954280, "url": "https://api.github.com/users/mayank31398", "html_url": "https://github.com/mayank31398"}, "body": "I agree I think `detach` is a safer option.\r\nbut I'll defer to @tianyu-l ", "created_at": "2024-07-11T19:56:37Z", "updated_at": "2024-07-11T19:56:38Z", "html_url": "https://github.com/pytorch/pytorch/pull/130550#discussion_r1674602894", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130550", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674602894"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130550#discussion_r1674602894"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130550"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674602894/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 370, "original_line": 370, "side": "RIGHT", "in_reply_to_id": 1674378529, "original_position": 5, "position": 5, "subject_type": "line", "PR": {"title": "Fix loss_parallel with BF16 logits", "number": 130550, "id": 1964977007}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674832722", "pull_request_review_id": 2173333330, "id": 1674832722, "node_id": "PRRC_kwDOA-j9z85j0-tS", "diff_hunk": "@@ -367,7 +367,7 @@ def _nll_loss_and_log_softmax_backward(\n     masked_safe_target = partial_placement._partition_value(safe_target, mesh, mesh_dim)\n     # only update grad_input to -1 if not masked\n     assert partial_placement.mask_buffer.data is not None\n-    grad_update = partial_placement.mask_buffer.data.float() - 1.0\n+    grad_update = partial_placement.mask_buffer.data.to(grad_input.dtype) - 1.0", "path": "torch/distributed/tensor/parallel/loss.py", "commit_id": "f3eb4ef71a420c9f1a1be69544f4b3d2186144e2", "original_commit_id": "f3eb4ef71a420c9f1a1be69544f4b3d2186144e2", "user": {"login": "tianyu-l", "id": 150487191, "url": "https://api.github.com/users/tianyu-l", "html_url": "https://github.com/tianyu-l"}, "body": "> @tianyu-l I think we should avoid using `.data` here since its intent can be ambiguous. If you want it to not be part of autograd graph, then you should use `.detach()`.\r\n\r\nHmm the `.data` is just a class variable (of type `torch.Tensor`) of the class `MaskBuffer`. So it shouldn't be a concern?\r\n\r\nhttps://github.com/pytorch/pytorch/blob/main/torch/distributed/_tensor/ops/embedding_ops.py#L28", "created_at": "2024-07-11T23:57:28Z", "updated_at": "2024-07-11T23:57:43Z", "html_url": "https://github.com/pytorch/pytorch/pull/130550#discussion_r1674832722", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130550", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674832722"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130550#discussion_r1674832722"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130550"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674832722/reactions", "total_count": 3, "+1": 3, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 370, "original_line": 370, "side": "RIGHT", "in_reply_to_id": 1674378529, "original_position": 5, "position": 5, "subject_type": "line", "PR": {"title": "Fix loss_parallel with BF16 logits", "number": 130550, "id": 1964977007}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674862034", "pull_request_review_id": 2173407320, "id": 1674862034, "node_id": "PRRC_kwDOA-j9z85j1F3S", "diff_hunk": "@@ -367,7 +367,7 @@ def _nll_loss_and_log_softmax_backward(\n     masked_safe_target = partial_placement._partition_value(safe_target, mesh, mesh_dim)\n     # only update grad_input to -1 if not masked\n     assert partial_placement.mask_buffer.data is not None\n-    grad_update = partial_placement.mask_buffer.data.float() - 1.0\n+    grad_update = partial_placement.mask_buffer.data.to(grad_input.dtype) - 1.0", "path": "torch/distributed/tensor/parallel/loss.py", "commit_id": "f3eb4ef71a420c9f1a1be69544f4b3d2186144e2", "original_commit_id": "f3eb4ef71a420c9f1a1be69544f4b3d2186144e2", "user": {"login": "wanchaol", "id": 9443650, "url": "https://api.github.com/users/wanchaol", "html_url": "https://github.com/wanchaol"}, "body": "yeah the `MuskBuffer` is not a torch.Tensor and `data` is just coincidentally a field name of that custom object, we can probably rename that field later to avoid confusing with `tensor.data`", "created_at": "2024-07-12T00:20:11Z", "updated_at": "2024-07-12T00:20:11Z", "html_url": "https://github.com/pytorch/pytorch/pull/130550#discussion_r1674862034", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130550", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674862034"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130550#discussion_r1674862034"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130550"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674862034/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 370, "original_line": 370, "side": "RIGHT", "in_reply_to_id": 1674378529, "original_position": 5, "position": 5, "subject_type": "line", "PR": {"title": "Fix loss_parallel with BF16 logits", "number": 130550, "id": 1964977007}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1675934847", "pull_request_review_id": 2174884102, "id": 1675934847, "node_id": "PRRC_kwDOA-j9z85j5Lx_", "diff_hunk": "@@ -367,7 +367,7 @@ def _nll_loss_and_log_softmax_backward(\n     masked_safe_target = partial_placement._partition_value(safe_target, mesh, mesh_dim)\n     # only update grad_input to -1 if not masked\n     assert partial_placement.mask_buffer.data is not None\n-    grad_update = partial_placement.mask_buffer.data.float() - 1.0\n+    grad_update = partial_placement.mask_buffer.data.to(grad_input.dtype) - 1.0", "path": "torch/distributed/tensor/parallel/loss.py", "commit_id": "f3eb4ef71a420c9f1a1be69544f4b3d2186144e2", "original_commit_id": "f3eb4ef71a420c9f1a1be69544f4b3d2186144e2", "user": {"login": "awgu", "id": 31054793, "url": "https://api.github.com/users/awgu", "html_url": "https://github.com/awgu"}, "body": "My bad!", "created_at": "2024-07-12T13:32:53Z", "updated_at": "2024-07-12T13:32:53Z", "html_url": "https://github.com/pytorch/pytorch/pull/130550#discussion_r1675934847", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130550", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1675934847"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130550#discussion_r1675934847"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130550"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1675934847/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 370, "original_line": 370, "side": "RIGHT", "in_reply_to_id": 1674378529, "original_position": 5, "position": 5, "subject_type": "line", "PR": {"title": "Fix loss_parallel with BF16 logits", "number": 130550, "id": 1964977007}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674387073", "pull_request_review_id": 2172528166, "id": 1674387073, "node_id": "PRRC_kwDOA-j9z85jzR6B", "diff_hunk": "@@ -1206,14 +1206,37 @@ Tensor std_mps(const Tensor& input_t,\n   @autoreleasepool {\n     string key = string(\"any_out_mps:\") + getTensorsStringKey(input_t) + \":\" + std::to_string(dim_);\n     auto cachedGraph = LookUpOrCreateCachedGraph<CachedGraph>(key, [&](auto mpsGraph, auto newCachedGraph) {\n-      MPSGraphTensor* inputTensor = mpsGraphRankedPlaceHolder(mpsGraph, input_t);\n+      auto inputTensor = mpsGraphRankedPlaceHolder(mpsGraph, input_t);\n \n-      MPSGraphTensor* castInputTensor =\n-          castToIHFTypes(mpsGraph, inputTensor, input_t, /*includesInt64=*/macOS13_3_plus);\n-      MPSGraphTensor* castOutputTensor = [mpsGraph reductionOrWithTensor:castInputTensor axis:dim_ name:nil];\n-      MPSGraphTensor* outputTensor = castOutputTensor;\n-      if (MPSDataTypeBool != [castOutputTensor dataType]) {\n-        outputTensor = [mpsGraph castTensor:castOutputTensor toType:MPSDataTypeBool name:@\"outputTensor\"];\n+      auto castInputTensor = castToIHFTypes(mpsGraph, inputTensor, input_t, /*includesInt64=*/macOS13_3_plus);\n+      // reductionOrWithTensor:axis: will throw an internal assert if number of dimentions is more than 4\n+      // See https://github.com/pytorch/pytorch/issues/95538\n+      MPSGraphTensor* outputTensor = nil;\n+      if (input_t.ndimension() > 4) {\n+        auto reduceDimLen = input_t.size(dim_);\n+        if (dim_ == 0) {\n+          castInputTensor = [mpsGraph reshapeTensor:castInputTensor withShape:@[ @(reduceDimLen), @-1 ] name:nil];", "path": "aten/src/ATen/native/mps/operations/ReduceOps.mm", "commit_id": "83bbbe1b7e7b751e2235c2d81968a67edaf1378d", "original_commit_id": "748e1c9087894325e091cc1a59d51ffbe672b64e", "user": {"login": "albanD", "id": 6359743, "url": "https://api.github.com/users/albanD", "html_url": "https://github.com/albanD"}, "body": "I guess you can always do this reshape because the Tensor here is always contiguous?", "created_at": "2024-07-11T17:17:54Z", "updated_at": "2024-07-11T17:17:55Z", "html_url": "https://github.com/pytorch/pytorch/pull/130542#discussion_r1674387073", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130542", "author_association": "COLLABORATOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674387073"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130542#discussion_r1674387073"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130542"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674387073/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 1226, "original_line": 1218, "side": "RIGHT", "original_position": 20, "position": 56, "subject_type": "line", "PR": {"title": "[MPS] Fix `torch.[all|any]` for 5+D tensors", "number": 130542, "id": 1964718778}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674480425", "pull_request_review_id": 2172699783, "id": 1674480425, "node_id": "PRRC_kwDOA-j9z85jzosp", "diff_hunk": "@@ -1206,14 +1206,37 @@ Tensor std_mps(const Tensor& input_t,\n   @autoreleasepool {\n     string key = string(\"any_out_mps:\") + getTensorsStringKey(input_t) + \":\" + std::to_string(dim_);\n     auto cachedGraph = LookUpOrCreateCachedGraph<CachedGraph>(key, [&](auto mpsGraph, auto newCachedGraph) {\n-      MPSGraphTensor* inputTensor = mpsGraphRankedPlaceHolder(mpsGraph, input_t);\n+      auto inputTensor = mpsGraphRankedPlaceHolder(mpsGraph, input_t);\n \n-      MPSGraphTensor* castInputTensor =\n-          castToIHFTypes(mpsGraph, inputTensor, input_t, /*includesInt64=*/macOS13_3_plus);\n-      MPSGraphTensor* castOutputTensor = [mpsGraph reductionOrWithTensor:castInputTensor axis:dim_ name:nil];\n-      MPSGraphTensor* outputTensor = castOutputTensor;\n-      if (MPSDataTypeBool != [castOutputTensor dataType]) {\n-        outputTensor = [mpsGraph castTensor:castOutputTensor toType:MPSDataTypeBool name:@\"outputTensor\"];\n+      auto castInputTensor = castToIHFTypes(mpsGraph, inputTensor, input_t, /*includesInt64=*/macOS13_3_plus);\n+      // reductionOrWithTensor:axis: will throw an internal assert if number of dimentions is more than 4\n+      // See https://github.com/pytorch/pytorch/issues/95538\n+      MPSGraphTensor* outputTensor = nil;\n+      if (input_t.ndimension() > 4) {\n+        auto reduceDimLen = input_t.size(dim_);\n+        if (dim_ == 0) {\n+          castInputTensor = [mpsGraph reshapeTensor:castInputTensor withShape:@[ @(reduceDimLen), @-1 ] name:nil];", "path": "aten/src/ATen/native/mps/operations/ReduceOps.mm", "commit_id": "83bbbe1b7e7b751e2235c2d81968a67edaf1378d", "original_commit_id": "748e1c9087894325e091cc1a59d51ffbe672b64e", "user": {"login": "malfet", "id": 2453524, "url": "https://api.github.com/users/malfet", "html_url": "https://github.com/malfet"}, "body": "To be frank, I don't know how MPS does reshapes under the hood, but currently there is no way to define a non-contiguous MPS tensor.", "created_at": "2024-07-11T18:24:25Z", "updated_at": "2024-07-11T18:24:26Z", "html_url": "https://github.com/pytorch/pytorch/pull/130542#discussion_r1674480425", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130542", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674480425"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130542#discussion_r1674480425"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130542"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674480425/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 1226, "original_line": 1218, "side": "RIGHT", "in_reply_to_id": 1674387073, "original_position": 20, "position": 56, "subject_type": "line", "PR": {"title": "[MPS] Fix `torch.[all|any]` for 5+D tensors", "number": 130542, "id": 1964718778}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1673970433", "pull_request_review_id": 2171820471, "id": 1673970433, "node_id": "PRRC_kwDOA-j9z85jxsMB", "diff_hunk": "@@ -2366,7 +2366,7 @@ inline std::optional<std::reference_wrapper<const std::string>> IValue::\n   if (isNone()) {\n     return std::nullopt;\n   }\n-  AT_ASSERT(isString(), \"Expected optional<string> but got \", tagKind());\n+  AT_ASSERT(isString(), \"Expected std::optional<string> but got \", tagKind());", "path": "aten/src/ATen/core/ivalue_inl.h", "commit_id": "b6f2aacda14ec97a9ee9a05656e2308b9eb020a4", "original_commit_id": "b6f2aacda14ec97a9ee9a05656e2308b9eb020a4", "user": {"login": "janeyx99", "id": 31798555, "url": "https://api.github.com/users/janeyx99", "html_url": "https://github.com/janeyx99"}, "body": "This contains a tiny tiny risk for BC breaking since the assert message has changed. ", "created_at": "2024-07-11T12:56:05Z", "updated_at": "2024-07-11T13:11:03Z", "html_url": "https://github.com/pytorch/pytorch/pull/130510#discussion_r1673970433", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130510", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1673970433"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130510#discussion_r1673970433"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130510"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1673970433/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 2369, "original_line": 2369, "side": "RIGHT", "original_position": 32, "position": 32, "subject_type": "line", "PR": {"title": "[7/N] Replace c10::optional with std::optional ", "number": 130510, "id": 1963512761}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1673256050", "pull_request_review_id": 2170674235, "id": 1673256050, "node_id": "PRRC_kwDOA-j9z85ju9xy", "diff_hunk": "@@ -4297,6 +4298,175 @@ def _get_backend_from_str(backend: Optional[str] = None) -> Backend:\n     return Backend(backend)\n \n \n+@_time_logger\n+def split_group(\n+    parent_pg=None,\n+    split_ranks=None,\n+    timeout=None,\n+    pg_options=None,\n+    group_desc=None,\n+):\n+    \"\"\"\n+    Create a new distributed group splitted from the given parent group.\n+\n+    users of this API must gurantee that all ranks in the parent group enter this API call.\n+    And the split of the group is the same accross the ranks.", "path": "torch/distributed/distributed_c10d.py", "commit_id": "3b376336469eda235814aee75ac079befb5367fc", "original_commit_id": "aff6a99fb26550044a032ceef3663ffb1735ab71", "user": {"login": "wconstab", "id": 4984825, "url": "https://api.github.com/users/wconstab", "html_url": "https://github.com/wconstab"}, "body": "call, and", "created_at": "2024-07-11T00:58:06Z", "updated_at": "2024-07-11T00:58:06Z", "html_url": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1673256050", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130507", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1673256050"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1673256050"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130507"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1673256050/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": null, "original_line": 4313, "side": "RIGHT", "original_position": 24, "position": null, "subject_type": "line", "PR": {"title": "[c10d] a new Pytorch API (split_group) to create a process group", "number": 130507, "id": 1963471316}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1673256221", "pull_request_review_id": 2170674504, "id": 1673256221, "node_id": "PRRC_kwDOA-j9z85ju90d", "diff_hunk": "@@ -4297,6 +4298,175 @@ def _get_backend_from_str(backend: Optional[str] = None) -> Backend:\n     return Backend(backend)\n \n \n+@_time_logger\n+def split_group(\n+    parent_pg=None,\n+    split_ranks=None,\n+    timeout=None,\n+    pg_options=None,\n+    group_desc=None,\n+):\n+    \"\"\"\n+    Create a new distributed group splitted from the given parent group.\n+\n+    users of this API must gurantee that all ranks in the parent group enter this API call.\n+    And the split of the group is the same accross the ranks.\n+\n+    Args:\n+        parent_pg (ProcessGroup, optional): The parent process group. If None,\n+            the default process group will be used. Users need to gurantee that \n+            the parent group is fully initialized (e.g, communicators are initialized)\n+        split_ranks (list[int]): the split ranks, which is a list of list of ranks.\n+            Users need to make sure the validity of the split rannks such that one ", "path": "torch/distributed/distributed_c10d.py", "commit_id": "3b376336469eda235814aee75ac079befb5367fc", "original_commit_id": "aff6a99fb26550044a032ceef3663ffb1735ab71", "user": {"login": "wconstab", "id": 4984825, "url": "https://api.github.com/users/wconstab", "html_url": "https://github.com/wconstab"}, "body": "raanks", "created_at": "2024-07-11T00:58:28Z", "updated_at": "2024-07-11T00:58:28Z", "html_url": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1673256221", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130507", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1673256221"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1673256221"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130507"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1673256221/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": null, "original_line": 4320, "side": "RIGHT", "original_position": 31, "position": null, "subject_type": "line", "PR": {"title": "[c10d] a new Pytorch API (split_group) to create a process group", "number": 130507, "id": 1963471316}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1673256722", "pull_request_review_id": 2170675343, "id": 1673256722, "node_id": "PRRC_kwDOA-j9z85ju98S", "diff_hunk": "@@ -4297,6 +4298,175 @@ def _get_backend_from_str(backend: Optional[str] = None) -> Backend:\n     return Backend(backend)\n \n \n+@_time_logger\n+def split_group(\n+    parent_pg=None,\n+    split_ranks=None,\n+    timeout=None,\n+    pg_options=None,\n+    group_desc=None,\n+):\n+    \"\"\"\n+    Create a new distributed group splitted from the given parent group.\n+\n+    users of this API must gurantee that all ranks in the parent group enter this API call.\n+    And the split of the group is the same accross the ranks.\n+\n+    Args:\n+        parent_pg (ProcessGroup, optional): The parent process group. If None,\n+            the default process group will be used. Users need to gurantee that \n+            the parent group is fully initialized (e.g, communicators are initialized)\n+        split_ranks (list[int]): the split ranks, which is a list of list of ranks.\n+            Users need to make sure the validity of the split rannks such that one \n+            split (represented by one inner list of ints) does not overlap with any other split.\n+            note, the ranks in each split is the group rank in the parent pg.\n+        timeout (timedelta, optional): see `init_process_group` for details and default value.    \n+        group_desc (str, optional): a string to describe the process group.\n+\n+    Returns:\n+        A handle of distributed group that can be given to collective calls or", "path": "torch/distributed/distributed_c10d.py", "commit_id": "3b376336469eda235814aee75ac079befb5367fc", "original_commit_id": "aff6a99fb26550044a032ceef3663ffb1735ab71", "user": {"login": "wconstab", "id": 4984825, "url": "https://api.github.com/users/wconstab", "html_url": "https://github.com/wconstab"}, "body": "s/handle/class`torch.distributed.ProcessGroup`\r\n\r\n(or whatever the right way to refer to a class in docstrings is)", "created_at": "2024-07-11T00:59:34Z", "updated_at": "2024-07-11T00:59:34Z", "html_url": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1673256722", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130507", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1673256722"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1673256722"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130507"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1673256722/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": null, "original_line": 4327, "side": "RIGHT", "original_position": 38, "position": null, "subject_type": "line", "PR": {"title": "[c10d] a new Pytorch API (split_group) to create a process group", "number": 130507, "id": 1963471316}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1673256828", "pull_request_review_id": 2170675517, "id": 1673256828, "node_id": "PRRC_kwDOA-j9z85ju998", "diff_hunk": "@@ -4297,6 +4298,175 @@ def _get_backend_from_str(backend: Optional[str] = None) -> Backend:\n     return Backend(backend)\n \n \n+@_time_logger\n+def split_group(", "path": "torch/distributed/distributed_c10d.py", "commit_id": "3b376336469eda235814aee75ac079befb5367fc", "original_commit_id": "aff6a99fb26550044a032ceef3663ffb1735ab71", "user": {"login": "wconstab", "id": 4984825, "url": "https://api.github.com/users/wconstab", "html_url": "https://github.com/wconstab"}, "body": "please add mypy typing hints", "created_at": "2024-07-11T00:59:49Z", "updated_at": "2024-07-11T00:59:50Z", "html_url": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1673256828", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130507", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1673256828"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1673256828"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130507"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1673256828/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 4312, "original_line": 4302, "side": "RIGHT", "original_position": 13, "position": 23, "subject_type": "line", "PR": {"title": "[c10d] a new Pytorch API (split_group) to create a process group", "number": 130507, "id": 1963471316}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1673257548", "pull_request_review_id": 2170676642, "id": 1673257548, "node_id": "PRRC_kwDOA-j9z85ju-JM", "diff_hunk": "@@ -4297,6 +4298,175 @@ def _get_backend_from_str(backend: Optional[str] = None) -> Backend:\n     return Backend(backend)\n \n \n+@_time_logger\n+def split_group(\n+    parent_pg=None,\n+    split_ranks=None,\n+    timeout=None,\n+    pg_options=None,\n+    group_desc=None,\n+):\n+    \"\"\"\n+    Create a new distributed group splitted from the given parent group.\n+\n+    users of this API must gurantee that all ranks in the parent group enter this API call.\n+    And the split of the group is the same accross the ranks.\n+\n+    Args:\n+        parent_pg (ProcessGroup, optional): The parent process group. If None,\n+            the default process group will be used. Users need to gurantee that \n+            the parent group is fully initialized (e.g, communicators are initialized)\n+        split_ranks (list[int]): the split ranks, which is a list of list of ranks.", "path": "torch/distributed/distributed_c10d.py", "commit_id": "3b376336469eda235814aee75ac079befb5367fc", "original_commit_id": "aff6a99fb26550044a032ceef3663ffb1735ab71", "user": {"login": "wconstab", "id": 4984825, "url": "https://api.github.com/users/wconstab", "html_url": "https://github.com/wconstab"}, "body": "typing looks wrong? you say list[int] and then later you say list of list of int.  \r\n\r\nalso, i'm reading this and not understanding how split_ranks actually works.  need to say what each int represents and if there are nested lists, what each list represents.", "created_at": "2024-07-11T01:01:24Z", "updated_at": "2024-07-11T01:01:24Z", "html_url": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1673257548", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130507", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1673257548"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1673257548"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130507"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1673257548/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": null, "original_line": 4319, "side": "RIGHT", "original_position": 30, "position": null, "subject_type": "line", "PR": {"title": "[c10d] a new Pytorch API (split_group) to create a process group", "number": 130507, "id": 1963471316}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1673258178", "pull_request_review_id": 2170677631, "id": 1673258178, "node_id": "PRRC_kwDOA-j9z85ju-TC", "diff_hunk": "@@ -4297,6 +4298,175 @@ def _get_backend_from_str(backend: Optional[str] = None) -> Backend:\n     return Backend(backend)\n \n \n+@_time_logger\n+def split_group(\n+    parent_pg=None,\n+    split_ranks=None,\n+    timeout=None,\n+    pg_options=None,\n+    group_desc=None,\n+):\n+    \"\"\"", "path": "torch/distributed/distributed_c10d.py", "commit_id": "3b376336469eda235814aee75ac079befb5367fc", "original_commit_id": "aff6a99fb26550044a032ceef3663ffb1735ab71", "user": {"login": "wconstab", "id": 4984825, "url": "https://api.github.com/users/wconstab", "html_url": "https://github.com/wconstab"}, "body": "should document that some backends do not support this API and what the behavior is in unsupported case", "created_at": "2024-07-11T01:02:37Z", "updated_at": "2024-07-11T01:02:37Z", "html_url": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1673258178", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130507", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1673258178"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1673258178"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130507"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1673258178/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 4319, "original_line": 4309, "side": "RIGHT", "original_position": 20, "position": 30, "subject_type": "line", "PR": {"title": "[c10d] a new Pytorch API (split_group) to create a process group", "number": 130507, "id": 1963471316}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674438925", "pull_request_review_id": 2172617184, "id": 1674438925, "node_id": "PRRC_kwDOA-j9z85jzekN", "diff_hunk": "@@ -4297,6 +4298,175 @@ def _get_backend_from_str(backend: Optional[str] = None) -> Backend:\n     return Backend(backend)\n \n \n+@_time_logger\n+def split_group(", "path": "torch/distributed/distributed_c10d.py", "commit_id": "3b376336469eda235814aee75ac079befb5367fc", "original_commit_id": "aff6a99fb26550044a032ceef3663ffb1735ab71", "user": {"login": "shuqiangzhang", "id": 4711970, "url": "https://api.github.com/users/shuqiangzhang", "html_url": "https://github.com/shuqiangzhang"}, "body": "myspy tying hints does not work well with 'None' default value which gives users more flexibility and follows the tradition of other APIs, new_group. We will get lint warning like, e.g.,\r\ndefault has type \"None\", argument has type \"timedelta\", ", "created_at": "2024-07-11T17:58:34Z", "updated_at": "2024-07-11T23:46:46Z", "html_url": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1674438925", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130507", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674438925"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1674438925"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130507"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674438925/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 4312, "original_line": 4302, "side": "RIGHT", "in_reply_to_id": 1673256828, "original_position": 13, "position": 23, "subject_type": "line", "PR": {"title": "[c10d] a new Pytorch API (split_group) to create a process group", "number": 130507, "id": 1963471316}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674829642", "pull_request_review_id": 2173318334, "id": 1674829642, "node_id": "PRRC_kwDOA-j9z85j099K", "diff_hunk": "@@ -4297,6 +4298,175 @@ def _get_backend_from_str(backend: Optional[str] = None) -> Backend:\n     return Backend(backend)\n \n \n+@_time_logger\n+def split_group(", "path": "torch/distributed/distributed_c10d.py", "commit_id": "3b376336469eda235814aee75ac079befb5367fc", "original_commit_id": "aff6a99fb26550044a032ceef3663ffb1735ab71", "user": {"login": "wconstab", "id": 4984825, "url": "https://api.github.com/users/wconstab", "html_url": "https://github.com/wconstab"}, "body": "it takes some work to get right but i think its usually worthwhile.  In general you can use Optional[] or Union[] to make the None case or the multi-type cases work out.  Then it'll force you to assert its not none or assert its the right type before using it.", "created_at": "2024-07-11T23:50:41Z", "updated_at": "2024-07-11T23:50:41Z", "html_url": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1674829642", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130507", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674829642"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1674829642"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130507"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674829642/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 4312, "original_line": 4302, "side": "RIGHT", "in_reply_to_id": 1673256828, "original_position": 13, "position": 23, "subject_type": "line", "PR": {"title": "[c10d] a new Pytorch API (split_group) to create a process group", "number": 130507, "id": 1963471316}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674830376", "pull_request_review_id": 2173321569, "id": 1674830376, "node_id": "PRRC_kwDOA-j9z85j0-Io", "diff_hunk": "@@ -4297,6 +4298,175 @@ def _get_backend_from_str(backend: Optional[str] = None) -> Backend:\n     return Backend(backend)\n \n \n+@_time_logger\n+def split_group(", "path": "torch/distributed/distributed_c10d.py", "commit_id": "3b376336469eda235814aee75ac079befb5367fc", "original_commit_id": "aff6a99fb26550044a032ceef3663ffb1735ab71", "user": {"login": "wconstab", "id": 4984825, "url": "https://api.github.com/users/wconstab", "html_url": "https://github.com/wconstab"}, "body": "just double checked, other APIs in the file are using mypy.  if init_process_group can do it then you can too :) \r\n\r\n```\r\n@_time_logger\r\ndef init_process_group(\r\n    backend: Optional[str] = None,\r\n    init_method: Optional[str] = None,\r\n    timeout: Optional[timedelta] = None,\r\n    world_size: int = -1,\r\n    rank: int = -1,\r\n    store: Optional[Store] = None,\r\n    group_name: str = \"\",\r\n    pg_options: Optional[Any] = None,\r\n    device_id: Optional[torch.device] = None,\r\n) -> None:\r\n```", "created_at": "2024-07-11T23:52:07Z", "updated_at": "2024-07-11T23:52:07Z", "html_url": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1674830376", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130507", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674830376"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1674830376"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130507"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674830376/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 4312, "original_line": 4302, "side": "RIGHT", "in_reply_to_id": 1673256828, "original_position": 13, "position": 23, "subject_type": "line", "PR": {"title": "[c10d] a new Pytorch API (split_group) to create a process group", "number": 130507, "id": 1963471316}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674831041", "pull_request_review_id": 2173325424, "id": 1674831041, "node_id": "PRRC_kwDOA-j9z85j0-TB", "diff_hunk": "@@ -4297,6 +4298,188 @@ def _get_backend_from_str(backend: Optional[str] = None) -> Backend:\n     return Backend(backend)\n \n \n+@_time_logger\n+def split_group(\n+    parent_pg=None,\n+    split_ranks=None,\n+    timeout=None,\n+    pg_options=None,\n+    group_desc=None,\n+):\n+    \"\"\"\n+    Create a new process group splitted from the given parent process group.\n+\n+    warning:: only the ``NCCL`` backend supports this API. Other backends will raise an error.\n+    users of this API must gurantee that all ranks in the parent group enter this API call,\n+    and the split of the sub groups is the same accross all ranks in the parent group.\n+\n+    Args:\n+        parent_pg (ProcessGroup, optional): The parent process group. If None,\n+            the default process group will be used. Users need to gurantee that\n+            the parent group is fully initialized (e.g, communicators are initialized)\n+        split_ranks (list[list[int]]): the split ranks, which is a list of list of ranks.\n+            Users need to make sure the validity of the split ranks such that one\n+            split (represented by one inner list of ints) does not overlap with any other split.\n+            Note that the ranks in each split is the group rank (instead of global rank)\n+            in the parent pg. For example, if the parent group has 4 ranks, and split_ranks can be\n+            [[0, 1], [2, 3]]. Note [[0,1]] is also a valid split, in which case ranks 2, 3 would\n+            return a non-group member.\n+        timeout (timedelta, optional): see `init_process_group` for details and default value.\n+        pg_options (ProcessGroupNCCLOptions, optional): process group options\n+            specifying what additional options need to be passed in during\n+            the construction of specific process groups. i.e.``is_high_priority_stream``\n+            can be specified so that process group can pick up high priority cuda streams.\n+        group_desc (str, optional): a string to describe the process group.\n+\n+    Returns:\n+        ProcessGroup if the current rank is within one split/subgroup given by split_ranks,\n+        or GroupMember.NON_GROUP_MEMBER if the current rank is not part of any split_ranks`.\n+\n+    \"\"\"\n+    # check inputs\n+    if split_ranks is None:\n+        raise ValueError(\"split_ranks cannot be None\")\n+\n+    if pg_options is not None:\n+        assert isinstance(\n+            pg_options, ProcessGroupNCCL.Options\n+        ), \"Expected pg_options argument to be of type ProcessGroupNCCL.Options\"\n+    else:\n+        # default pg_options for NCCL\n+        pg_options = ProcessGroupNCCL.Options()\n+        pg_options.is_high_priority_stream = False\n+\n+    group_desc = \"undefined\" if group_desc is None else group_desc\n+\n+    global _world\n+    default_pg = _get_default_group()\n+    device_id = default_pg.bound_device_id\n+    if not device_id:\n+        raise RuntimeError(\n+            \"No device associated with the default pg, not safe to split any process groups\"\n+        )\n+    default_backend, default_store = _world.pg_map[default_pg]\n+    global_rank = default_pg.rank()\n+    global_world_size = default_pg.size()\n+\n+    if not parent_pg:\n+        parent_pg = default_pg\n+    if parent_pg not in _world.pg_group_ranks:\n+        raise ValueError(f\"Group {parent_pg} is not registered\")\n+\n+    parent_global_to_group_ranks = _world.pg_group_ranks[parent_pg]\n+    parent_group_to_global_ranks = {\n+        group_rank: global_rank\n+        for global_rank, group_rank in parent_global_to_group_ranks.items()\n+    }\n+\n+    if global_rank not in parent_global_to_group_ranks:\n+        raise ValueError(\n+            f\"Global rank {global_rank} is not part of the parent group {parent_pg}\"\n+        )\n+\n+    parent_group_rank = parent_global_to_group_ranks[global_rank]\n+    parent_backend = parent_pg._get_backend(torch.device(\"cuda\"))\n+\n+    # if the parent backend does not support splitting, raise error\n+    # currently this API only support NCCL backend\n+    if (\n+        not parent_backend\n+        or not parent_backend.supports_splitting\n+        or not isinstance(parent_backend, ProcessGroupNCCL)\n+    ):\n+        raise RuntimeError(\n+            \"No backend for the parent process group or its backend does not support splitting\"\n+        )\n+\n+    parent_backend_str, _ = _world.pg_map[parent_pg]\n+    # same type of backend as the parent process group\n+    backend = Backend(parent_backend_str)\n+    backend_config = BackendConfig(backend)\n+\n+    # this timeout defaulting/validation is used for all the new_groups/new_subgroups variants,\n+    # which may just pass their timeout value (or None)\n+    if timeout is None:\n+        timeout = _get_default_timeout(backend)\n+    _check_valid_timeout(timeout)\n+\n+    # find my group of ranks and my group local rank in split_ranks\n+    my_group = None\n+    group_rank = -1\n+\n+    for split_group in split_ranks:\n+        if len(split_group) == 0:\n+            raise ValueError(\"the split group cannot be empty\")\n+        if len(split_group) > global_world_size:\n+            raise ValueError(\n+                \"the split group's size should be less or equal to the world_size set by init_process_group\"\n+            )\n+        if len(split_group) != len(set(split_group)):\n+            raise ValueError(\"the split group cannot have duplicate ranks\")\n+        split_group = sorted(split_group)\n+        if parent_group_rank in split_group:\n+            my_group = split_group\n+            group_rank = split_group.index(parent_group_rank)\n+            break\n+    # if my rank does not belong to any sub group or my rank is the only member in the subgroup,\n+    # no_color split should be called\n+    if my_group is None or group_rank == -1 or len(my_group) == 1:\n+        parent_backend.perform_nocolor_split(device_id)\n+        return GroupMember.NON_GROUP_MEMBER\n+\n+    group_name = _process_group_name(my_group, use_hashed_name=False)\n+    global_ranks_in_my_group = [parent_group_to_global_ranks[rank] for rank in my_group]\n+\n+    prefix_store = PrefixStore(f\"{group_name}/\", default_store)\n+    base_pg_options = ProcessGroup.Options(backend=str(backend))\n+    base_pg_options._timeout = timeout\n+    pg: ProcessGroup = ProcessGroup(\n+        prefix_store, group_rank, len(my_group), base_pg_options\n+    )\n+    pg.bound_device_id = device_id\n+\n+    pg_options._timeout = timeout\n+    pg_options.split_from = parent_backend\n+    pg_options.split_color = _process_group_color(my_group)\n+    pg_options.global_ranks_in_group = global_ranks_in_my_group\n+    pg_options.group_name = group_name\n+    backend_class = ProcessGroupNCCL(\n+        prefix_store, group_rank, len(my_group), pg_options\n+    )\n+    backend_type = ProcessGroup.BackendType.NCCL\n+    backend_class._set_sequence_number_for_group()\n+\n+    pg._register_backend(torch.device(\"cuda\"), backend_type, backend_class)\n+\n+    # set group_name and group_dsec to backend", "path": "torch/distributed/distributed_c10d.py", "commit_id": "3b376336469eda235814aee75ac079befb5367fc", "original_commit_id": "f9b1f21870962f948972cb8c1b28018162c3f1c6", "user": {"login": "wconstab", "id": 4984825, "url": "https://api.github.com/users/wconstab", "html_url": "https://github.com/wconstab"}, "body": "typo desc", "created_at": "2024-07-11T23:53:51Z", "updated_at": "2024-07-11T23:53:51Z", "html_url": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1674831041", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130507", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674831041"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1674831041"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130507"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674831041/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": null, "original_line": 4454, "side": "RIGHT", "original_position": 165, "position": null, "subject_type": "line", "PR": {"title": "[c10d] a new Pytorch API (split_group) to create a process group", "number": 130507, "id": 1963471316}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674831613", "pull_request_review_id": 2173327890, "id": 1674831613, "node_id": "PRRC_kwDOA-j9z85j0-b9", "diff_hunk": "@@ -4297,6 +4298,188 @@ def _get_backend_from_str(backend: Optional[str] = None) -> Backend:\n     return Backend(backend)\n \n \n+@_time_logger\n+def split_group(\n+    parent_pg=None,\n+    split_ranks=None,\n+    timeout=None,\n+    pg_options=None,\n+    group_desc=None,\n+):\n+    \"\"\"\n+    Create a new process group splitted from the given parent process group.\n+\n+    warning:: only the ``NCCL`` backend supports this API. Other backends will raise an error.\n+    users of this API must gurantee that all ranks in the parent group enter this API call,\n+    and the split of the sub groups is the same accross all ranks in the parent group.\n+\n+    Args:\n+        parent_pg (ProcessGroup, optional): The parent process group. If None,\n+            the default process group will be used. Users need to gurantee that\n+            the parent group is fully initialized (e.g, communicators are initialized)\n+        split_ranks (list[list[int]]): the split ranks, which is a list of list of ranks.\n+            Users need to make sure the validity of the split ranks such that one\n+            split (represented by one inner list of ints) does not overlap with any other split.\n+            Note that the ranks in each split is the group rank (instead of global rank)\n+            in the parent pg. For example, if the parent group has 4 ranks, and split_ranks can be\n+            [[0, 1], [2, 3]]. Note [[0,1]] is also a valid split, in which case ranks 2, 3 would\n+            return a non-group member.\n+        timeout (timedelta, optional): see `init_process_group` for details and default value.\n+        pg_options (ProcessGroupNCCLOptions, optional): process group options\n+            specifying what additional options need to be passed in during\n+            the construction of specific process groups. i.e.``is_high_priority_stream``\n+            can be specified so that process group can pick up high priority cuda streams.\n+        group_desc (str, optional): a string to describe the process group.\n+\n+    Returns:\n+        ProcessGroup if the current rank is within one split/subgroup given by split_ranks,\n+        or GroupMember.NON_GROUP_MEMBER if the current rank is not part of any split_ranks`.\n+\n+    \"\"\"\n+    # check inputs\n+    if split_ranks is None:\n+        raise ValueError(\"split_ranks cannot be None\")\n+\n+    if pg_options is not None:\n+        assert isinstance(\n+            pg_options, ProcessGroupNCCL.Options\n+        ), \"Expected pg_options argument to be of type ProcessGroupNCCL.Options\"\n+    else:\n+        # default pg_options for NCCL\n+        pg_options = ProcessGroupNCCL.Options()\n+        pg_options.is_high_priority_stream = False\n+\n+    group_desc = \"undefined\" if group_desc is None else group_desc\n+\n+    global _world\n+    default_pg = _get_default_group()\n+    device_id = default_pg.bound_device_id\n+    if not device_id:", "path": "torch/distributed/distributed_c10d.py", "commit_id": "3b376336469eda235814aee75ac079befb5367fc", "original_commit_id": "f9b1f21870962f948972cb8c1b28018162c3f1c6", "user": {"login": "wconstab", "id": 4984825, "url": "https://api.github.com/users/wconstab", "html_url": "https://github.com/wconstab"}, "body": "we might need a helper fn `_can_split` since i think we check this logic in multiple places", "created_at": "2024-07-11T23:55:00Z", "updated_at": "2024-07-11T23:55:00Z", "html_url": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1674831613", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130507", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674831613"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1674831613"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130507"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674831613/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 4357, "original_line": 4357, "side": "RIGHT", "original_position": 68, "position": 68, "subject_type": "line", "PR": {"title": "[c10d] a new Pytorch API (split_group) to create a process group", "number": 130507, "id": 1963471316}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674833074", "pull_request_review_id": 2173335008, "id": 1674833074, "node_id": "PRRC_kwDOA-j9z85j0-yy", "diff_hunk": "@@ -4297,6 +4298,188 @@ def _get_backend_from_str(backend: Optional[str] = None) -> Backend:\n     return Backend(backend)\n \n \n+@_time_logger\n+def split_group(\n+    parent_pg=None,\n+    split_ranks=None,\n+    timeout=None,\n+    pg_options=None,\n+    group_desc=None,\n+):\n+    \"\"\"\n+    Create a new process group splitted from the given parent process group.\n+\n+    warning:: only the ``NCCL`` backend supports this API. Other backends will raise an error.\n+    users of this API must gurantee that all ranks in the parent group enter this API call,\n+    and the split of the sub groups is the same accross all ranks in the parent group.\n+\n+    Args:\n+        parent_pg (ProcessGroup, optional): The parent process group. If None,\n+            the default process group will be used. Users need to gurantee that\n+            the parent group is fully initialized (e.g, communicators are initialized)\n+        split_ranks (list[list[int]]): the split ranks, which is a list of list of ranks.\n+            Users need to make sure the validity of the split ranks such that one\n+            split (represented by one inner list of ints) does not overlap with any other split.\n+            Note that the ranks in each split is the group rank (instead of global rank)\n+            in the parent pg. For example, if the parent group has 4 ranks, and split_ranks can be\n+            [[0, 1], [2, 3]]. Note [[0,1]] is also a valid split, in which case ranks 2, 3 would\n+            return a non-group member.\n+        timeout (timedelta, optional): see `init_process_group` for details and default value.\n+        pg_options (ProcessGroupNCCLOptions, optional): process group options\n+            specifying what additional options need to be passed in during\n+            the construction of specific process groups. i.e.``is_high_priority_stream``\n+            can be specified so that process group can pick up high priority cuda streams.\n+        group_desc (str, optional): a string to describe the process group.\n+\n+    Returns:\n+        ProcessGroup if the current rank is within one split/subgroup given by split_ranks,\n+        or GroupMember.NON_GROUP_MEMBER if the current rank is not part of any split_ranks`.\n+\n+    \"\"\"\n+    # check inputs\n+    if split_ranks is None:\n+        raise ValueError(\"split_ranks cannot be None\")\n+\n+    if pg_options is not None:\n+        assert isinstance(\n+            pg_options, ProcessGroupNCCL.Options\n+        ), \"Expected pg_options argument to be of type ProcessGroupNCCL.Options\"\n+    else:\n+        # default pg_options for NCCL\n+        pg_options = ProcessGroupNCCL.Options()\n+        pg_options.is_high_priority_stream = False\n+\n+    group_desc = \"undefined\" if group_desc is None else group_desc\n+\n+    global _world\n+    default_pg = _get_default_group()\n+    device_id = default_pg.bound_device_id\n+    if not device_id:\n+        raise RuntimeError(\n+            \"No device associated with the default pg, not safe to split any process groups\"\n+        )\n+    default_backend, default_store = _world.pg_map[default_pg]\n+    global_rank = default_pg.rank()\n+    global_world_size = default_pg.size()\n+\n+    if not parent_pg:\n+        parent_pg = default_pg\n+    if parent_pg not in _world.pg_group_ranks:\n+        raise ValueError(f\"Group {parent_pg} is not registered\")\n+\n+    parent_global_to_group_ranks = _world.pg_group_ranks[parent_pg]\n+    parent_group_to_global_ranks = {", "path": "torch/distributed/distributed_c10d.py", "commit_id": "3b376336469eda235814aee75ac079befb5367fc", "original_commit_id": "f9b1f21870962f948972cb8c1b28018162c3f1c6", "user": {"login": "wconstab", "id": 4984825, "url": "https://api.github.com/users/wconstab", "html_url": "https://github.com/wconstab"}, "body": "dont we have a helper already, like get_group_rank()?", "created_at": "2024-07-11T23:58:15Z", "updated_at": "2024-07-11T23:58:16Z", "html_url": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1674833074", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130507", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674833074"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1674833074"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130507"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674833074/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 4371, "original_line": 4371, "side": "RIGHT", "original_position": 82, "position": 82, "subject_type": "line", "PR": {"title": "[c10d] a new Pytorch API (split_group) to create a process group", "number": 130507, "id": 1963471316}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674833766", "pull_request_review_id": 2173338728, "id": 1674833766, "node_id": "PRRC_kwDOA-j9z85j0-9m", "diff_hunk": "@@ -4297,6 +4298,188 @@ def _get_backend_from_str(backend: Optional[str] = None) -> Backend:\n     return Backend(backend)\n \n \n+@_time_logger\n+def split_group(\n+    parent_pg=None,\n+    split_ranks=None,\n+    timeout=None,\n+    pg_options=None,\n+    group_desc=None,\n+):\n+    \"\"\"\n+    Create a new process group splitted from the given parent process group.\n+\n+    warning:: only the ``NCCL`` backend supports this API. Other backends will raise an error.\n+    users of this API must gurantee that all ranks in the parent group enter this API call,\n+    and the split of the sub groups is the same accross all ranks in the parent group.\n+\n+    Args:\n+        parent_pg (ProcessGroup, optional): The parent process group. If None,\n+            the default process group will be used. Users need to gurantee that\n+            the parent group is fully initialized (e.g, communicators are initialized)\n+        split_ranks (list[list[int]]): the split ranks, which is a list of list of ranks.\n+            Users need to make sure the validity of the split ranks such that one\n+            split (represented by one inner list of ints) does not overlap with any other split.\n+            Note that the ranks in each split is the group rank (instead of global rank)\n+            in the parent pg. For example, if the parent group has 4 ranks, and split_ranks can be\n+            [[0, 1], [2, 3]]. Note [[0,1]] is also a valid split, in which case ranks 2, 3 would\n+            return a non-group member.\n+        timeout (timedelta, optional): see `init_process_group` for details and default value.\n+        pg_options (ProcessGroupNCCLOptions, optional): process group options\n+            specifying what additional options need to be passed in during\n+            the construction of specific process groups. i.e.``is_high_priority_stream``\n+            can be specified so that process group can pick up high priority cuda streams.\n+        group_desc (str, optional): a string to describe the process group.\n+\n+    Returns:\n+        ProcessGroup if the current rank is within one split/subgroup given by split_ranks,\n+        or GroupMember.NON_GROUP_MEMBER if the current rank is not part of any split_ranks`.\n+\n+    \"\"\"\n+    # check inputs\n+    if split_ranks is None:\n+        raise ValueError(\"split_ranks cannot be None\")\n+\n+    if pg_options is not None:\n+        assert isinstance(\n+            pg_options, ProcessGroupNCCL.Options\n+        ), \"Expected pg_options argument to be of type ProcessGroupNCCL.Options\"\n+    else:\n+        # default pg_options for NCCL\n+        pg_options = ProcessGroupNCCL.Options()\n+        pg_options.is_high_priority_stream = False\n+\n+    group_desc = \"undefined\" if group_desc is None else group_desc\n+\n+    global _world\n+    default_pg = _get_default_group()\n+    device_id = default_pg.bound_device_id\n+    if not device_id:\n+        raise RuntimeError(\n+            \"No device associated with the default pg, not safe to split any process groups\"\n+        )\n+    default_backend, default_store = _world.pg_map[default_pg]\n+    global_rank = default_pg.rank()\n+    global_world_size = default_pg.size()\n+\n+    if not parent_pg:\n+        parent_pg = default_pg\n+    if parent_pg not in _world.pg_group_ranks:\n+        raise ValueError(f\"Group {parent_pg} is not registered\")\n+\n+    parent_global_to_group_ranks = _world.pg_group_ranks[parent_pg]\n+    parent_group_to_global_ranks = {\n+        group_rank: global_rank\n+        for global_rank, group_rank in parent_global_to_group_ranks.items()\n+    }\n+\n+    if global_rank not in parent_global_to_group_ranks:\n+        raise ValueError(\n+            f\"Global rank {global_rank} is not part of the parent group {parent_pg}\"\n+        )\n+\n+    parent_group_rank = parent_global_to_group_ranks[global_rank]\n+    parent_backend = parent_pg._get_backend(torch.device(\"cuda\"))\n+\n+    # if the parent backend does not support splitting, raise error\n+    # currently this API only support NCCL backend\n+    if (\n+        not parent_backend\n+        or not parent_backend.supports_splitting\n+        or not isinstance(parent_backend, ProcessGroupNCCL)", "path": "torch/distributed/distributed_c10d.py", "commit_id": "3b376336469eda235814aee75ac079befb5367fc", "original_commit_id": "f9b1f21870962f948972cb8c1b28018162c3f1c6", "user": {"login": "wconstab", "id": 4984825, "url": "https://api.github.com/users/wconstab", "html_url": "https://github.com/wconstab"}, "body": "shouldn't the .supports_splitting check be sufficient? we'll get PRs eventually for widening this from just nccl so if we already have an api maybe just use that?", "created_at": "2024-07-11T23:59:56Z", "updated_at": "2024-07-11T23:59:56Z", "html_url": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1674833766", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130507", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674833766"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1674833766"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130507"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674833766/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 4389, "original_line": 4389, "side": "RIGHT", "original_position": 100, "position": 100, "subject_type": "line", "PR": {"title": "[c10d] a new Pytorch API (split_group) to create a process group", "number": 130507, "id": 1963471316}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674862782", "pull_request_review_id": 2173409098, "id": 1674862782, "node_id": "PRRC_kwDOA-j9z85j1GC-", "diff_hunk": "@@ -4297,6 +4298,188 @@ def _get_backend_from_str(backend: Optional[str] = None) -> Backend:\n     return Backend(backend)\n \n \n+@_time_logger\n+def split_group(\n+    parent_pg=None,\n+    split_ranks=None,\n+    timeout=None,\n+    pg_options=None,\n+    group_desc=None,\n+):\n+    \"\"\"\n+    Create a new process group splitted from the given parent process group.\n+\n+    warning:: only the ``NCCL`` backend supports this API. Other backends will raise an error.\n+    users of this API must gurantee that all ranks in the parent group enter this API call,\n+    and the split of the sub groups is the same accross all ranks in the parent group.\n+\n+    Args:\n+        parent_pg (ProcessGroup, optional): The parent process group. If None,\n+            the default process group will be used. Users need to gurantee that\n+            the parent group is fully initialized (e.g, communicators are initialized)\n+        split_ranks (list[list[int]]): the split ranks, which is a list of list of ranks.\n+            Users need to make sure the validity of the split ranks such that one\n+            split (represented by one inner list of ints) does not overlap with any other split.\n+            Note that the ranks in each split is the group rank (instead of global rank)\n+            in the parent pg. For example, if the parent group has 4 ranks, and split_ranks can be\n+            [[0, 1], [2, 3]]. Note [[0,1]] is also a valid split, in which case ranks 2, 3 would\n+            return a non-group member.\n+        timeout (timedelta, optional): see `init_process_group` for details and default value.\n+        pg_options (ProcessGroupNCCLOptions, optional): process group options\n+            specifying what additional options need to be passed in during\n+            the construction of specific process groups. i.e.``is_high_priority_stream``\n+            can be specified so that process group can pick up high priority cuda streams.\n+        group_desc (str, optional): a string to describe the process group.\n+\n+    Returns:\n+        ProcessGroup if the current rank is within one split/subgroup given by split_ranks,\n+        or GroupMember.NON_GROUP_MEMBER if the current rank is not part of any split_ranks`.\n+\n+    \"\"\"\n+    # check inputs\n+    if split_ranks is None:\n+        raise ValueError(\"split_ranks cannot be None\")\n+\n+    if pg_options is not None:\n+        assert isinstance(\n+            pg_options, ProcessGroupNCCL.Options\n+        ), \"Expected pg_options argument to be of type ProcessGroupNCCL.Options\"\n+    else:\n+        # default pg_options for NCCL\n+        pg_options = ProcessGroupNCCL.Options()\n+        pg_options.is_high_priority_stream = False\n+\n+    group_desc = \"undefined\" if group_desc is None else group_desc\n+\n+    global _world\n+    default_pg = _get_default_group()\n+    device_id = default_pg.bound_device_id\n+    if not device_id:\n+        raise RuntimeError(\n+            \"No device associated with the default pg, not safe to split any process groups\"\n+        )\n+    default_backend, default_store = _world.pg_map[default_pg]\n+    global_rank = default_pg.rank()\n+    global_world_size = default_pg.size()\n+\n+    if not parent_pg:\n+        parent_pg = default_pg\n+    if parent_pg not in _world.pg_group_ranks:\n+        raise ValueError(f\"Group {parent_pg} is not registered\")\n+\n+    parent_global_to_group_ranks = _world.pg_group_ranks[parent_pg]\n+    parent_group_to_global_ranks = {\n+        group_rank: global_rank\n+        for global_rank, group_rank in parent_global_to_group_ranks.items()\n+    }\n+\n+    if global_rank not in parent_global_to_group_ranks:\n+        raise ValueError(\n+            f\"Global rank {global_rank} is not part of the parent group {parent_pg}\"\n+        )\n+\n+    parent_group_rank = parent_global_to_group_ranks[global_rank]\n+    parent_backend = parent_pg._get_backend(torch.device(\"cuda\"))\n+\n+    # if the parent backend does not support splitting, raise error\n+    # currently this API only support NCCL backend\n+    if (\n+        not parent_backend\n+        or not parent_backend.supports_splitting\n+        or not isinstance(parent_backend, ProcessGroupNCCL)\n+    ):\n+        raise RuntimeError(\n+            \"No backend for the parent process group or its backend does not support splitting\"\n+        )\n+\n+    parent_backend_str, _ = _world.pg_map[parent_pg]\n+    # same type of backend as the parent process group\n+    backend = Backend(parent_backend_str)\n+    backend_config = BackendConfig(backend)\n+\n+    # this timeout defaulting/validation is used for all the new_groups/new_subgroups variants,\n+    # which may just pass their timeout value (or None)\n+    if timeout is None:\n+        timeout = _get_default_timeout(backend)\n+    _check_valid_timeout(timeout)\n+\n+    # find my group of ranks and my group local rank in split_ranks\n+    my_group = None\n+    group_rank = -1\n+\n+    for split_group in split_ranks:\n+        if len(split_group) == 0:\n+            raise ValueError(\"the split group cannot be empty\")\n+        if len(split_group) > global_world_size:\n+            raise ValueError(\n+                \"the split group's size should be less or equal to the world_size set by init_process_group\"\n+            )\n+        if len(split_group) != len(set(split_group)):\n+            raise ValueError(\"the split group cannot have duplicate ranks\")\n+        split_group = sorted(split_group)\n+        if parent_group_rank in split_group:\n+            my_group = split_group\n+            group_rank = split_group.index(parent_group_rank)\n+            break\n+    # if my rank does not belong to any sub group or my rank is the only member in the subgroup,\n+    # no_color split should be called\n+    if my_group is None or group_rank == -1 or len(my_group) == 1:\n+        parent_backend.perform_nocolor_split(device_id)\n+        return GroupMember.NON_GROUP_MEMBER\n+\n+    group_name = _process_group_name(my_group, use_hashed_name=False)\n+    global_ranks_in_my_group = [parent_group_to_global_ranks[rank] for rank in my_group]\n+\n+    prefix_store = PrefixStore(f\"{group_name}/\", default_store)\n+    base_pg_options = ProcessGroup.Options(backend=str(backend))\n+    base_pg_options._timeout = timeout", "path": "torch/distributed/distributed_c10d.py", "commit_id": "3b376336469eda235814aee75ac079befb5367fc", "original_commit_id": "f9b1f21870962f948972cb8c1b28018162c3f1c6", "user": {"login": "wconstab", "id": 4984825, "url": "https://api.github.com/users/wconstab", "html_url": "https://github.com/wconstab"}, "body": "are there any other options we need to copy actually? ", "created_at": "2024-07-12T00:20:36Z", "updated_at": "2024-07-12T00:20:36Z", "html_url": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1674862782", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130507", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674862782"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1674862782"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130507"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674862782/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 4450, "original_line": 4435, "side": "RIGHT", "original_position": 146, "position": 161, "subject_type": "line", "PR": {"title": "[c10d] a new Pytorch API (split_group) to create a process group", "number": 130507, "id": 1963471316}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674864039", "pull_request_review_id": 2173412652, "id": 1674864039, "node_id": "PRRC_kwDOA-j9z85j1GWn", "diff_hunk": "@@ -4297,6 +4298,188 @@ def _get_backend_from_str(backend: Optional[str] = None) -> Backend:\n     return Backend(backend)\n \n \n+@_time_logger\n+def split_group(\n+    parent_pg=None,\n+    split_ranks=None,\n+    timeout=None,\n+    pg_options=None,\n+    group_desc=None,\n+):\n+    \"\"\"\n+    Create a new process group splitted from the given parent process group.\n+\n+    warning:: only the ``NCCL`` backend supports this API. Other backends will raise an error.\n+    users of this API must gurantee that all ranks in the parent group enter this API call,\n+    and the split of the sub groups is the same accross all ranks in the parent group.\n+\n+    Args:\n+        parent_pg (ProcessGroup, optional): The parent process group. If None,\n+            the default process group will be used. Users need to gurantee that\n+            the parent group is fully initialized (e.g, communicators are initialized)\n+        split_ranks (list[list[int]]): the split ranks, which is a list of list of ranks.\n+            Users need to make sure the validity of the split ranks such that one\n+            split (represented by one inner list of ints) does not overlap with any other split.\n+            Note that the ranks in each split is the group rank (instead of global rank)\n+            in the parent pg. For example, if the parent group has 4 ranks, and split_ranks can be\n+            [[0, 1], [2, 3]]. Note [[0,1]] is also a valid split, in which case ranks 2, 3 would\n+            return a non-group member.\n+        timeout (timedelta, optional): see `init_process_group` for details and default value.\n+        pg_options (ProcessGroupNCCLOptions, optional): process group options\n+            specifying what additional options need to be passed in during\n+            the construction of specific process groups. i.e.``is_high_priority_stream``\n+            can be specified so that process group can pick up high priority cuda streams.\n+        group_desc (str, optional): a string to describe the process group.\n+\n+    Returns:\n+        ProcessGroup if the current rank is within one split/subgroup given by split_ranks,\n+        or GroupMember.NON_GROUP_MEMBER if the current rank is not part of any split_ranks`.\n+\n+    \"\"\"\n+    # check inputs\n+    if split_ranks is None:\n+        raise ValueError(\"split_ranks cannot be None\")\n+\n+    if pg_options is not None:\n+        assert isinstance(\n+            pg_options, ProcessGroupNCCL.Options\n+        ), \"Expected pg_options argument to be of type ProcessGroupNCCL.Options\"\n+    else:\n+        # default pg_options for NCCL\n+        pg_options = ProcessGroupNCCL.Options()\n+        pg_options.is_high_priority_stream = False\n+\n+    group_desc = \"undefined\" if group_desc is None else group_desc\n+\n+    global _world\n+    default_pg = _get_default_group()\n+    device_id = default_pg.bound_device_id\n+    if not device_id:\n+        raise RuntimeError(\n+            \"No device associated with the default pg, not safe to split any process groups\"\n+        )\n+    default_backend, default_store = _world.pg_map[default_pg]\n+    global_rank = default_pg.rank()\n+    global_world_size = default_pg.size()\n+\n+    if not parent_pg:\n+        parent_pg = default_pg\n+    if parent_pg not in _world.pg_group_ranks:\n+        raise ValueError(f\"Group {parent_pg} is not registered\")\n+\n+    parent_global_to_group_ranks = _world.pg_group_ranks[parent_pg]\n+    parent_group_to_global_ranks = {\n+        group_rank: global_rank\n+        for global_rank, group_rank in parent_global_to_group_ranks.items()\n+    }\n+\n+    if global_rank not in parent_global_to_group_ranks:\n+        raise ValueError(\n+            f\"Global rank {global_rank} is not part of the parent group {parent_pg}\"\n+        )\n+\n+    parent_group_rank = parent_global_to_group_ranks[global_rank]\n+    parent_backend = parent_pg._get_backend(torch.device(\"cuda\"))\n+\n+    # if the parent backend does not support splitting, raise error\n+    # currently this API only support NCCL backend\n+    if (\n+        not parent_backend\n+        or not parent_backend.supports_splitting\n+        or not isinstance(parent_backend, ProcessGroupNCCL)\n+    ):\n+        raise RuntimeError(\n+            \"No backend for the parent process group or its backend does not support splitting\"\n+        )\n+\n+    parent_backend_str, _ = _world.pg_map[parent_pg]\n+    # same type of backend as the parent process group\n+    backend = Backend(parent_backend_str)\n+    backend_config = BackendConfig(backend)\n+\n+    # this timeout defaulting/validation is used for all the new_groups/new_subgroups variants,\n+    # which may just pass their timeout value (or None)\n+    if timeout is None:\n+        timeout = _get_default_timeout(backend)\n+    _check_valid_timeout(timeout)\n+\n+    # find my group of ranks and my group local rank in split_ranks\n+    my_group = None\n+    group_rank = -1\n+\n+    for split_group in split_ranks:\n+        if len(split_group) == 0:\n+            raise ValueError(\"the split group cannot be empty\")\n+        if len(split_group) > global_world_size:\n+            raise ValueError(\n+                \"the split group's size should be less or equal to the world_size set by init_process_group\"\n+            )\n+        if len(split_group) != len(set(split_group)):\n+            raise ValueError(\"the split group cannot have duplicate ranks\")\n+        split_group = sorted(split_group)\n+        if parent_group_rank in split_group:\n+            my_group = split_group\n+            group_rank = split_group.index(parent_group_rank)\n+            break\n+    # if my rank does not belong to any sub group or my rank is the only member in the subgroup,\n+    # no_color split should be called\n+    if my_group is None or group_rank == -1 or len(my_group) == 1:\n+        parent_backend.perform_nocolor_split(device_id)\n+        return GroupMember.NON_GROUP_MEMBER\n+\n+    group_name = _process_group_name(my_group, use_hashed_name=False)\n+    global_ranks_in_my_group = [parent_group_to_global_ranks[rank] for rank in my_group]\n+\n+    prefix_store = PrefixStore(f\"{group_name}/\", default_store)\n+    base_pg_options = ProcessGroup.Options(backend=str(backend))\n+    base_pg_options._timeout = timeout\n+    pg: ProcessGroup = ProcessGroup(\n+        prefix_store, group_rank, len(my_group), base_pg_options\n+    )\n+    pg.bound_device_id = device_id\n+\n+    pg_options._timeout = timeout\n+    pg_options.split_from = parent_backend\n+    pg_options.split_color = _process_group_color(my_group)\n+    pg_options.global_ranks_in_group = global_ranks_in_my_group\n+    pg_options.group_name = group_name\n+    backend_class = ProcessGroupNCCL(", "path": "torch/distributed/distributed_c10d.py", "commit_id": "3b376336469eda235814aee75ac079befb5367fc", "original_commit_id": "f9b1f21870962f948972cb8c1b28018162c3f1c6", "user": {"login": "wconstab", "id": 4984825, "url": "https://api.github.com/users/wconstab", "html_url": "https://github.com/wconstab"}, "body": "shouldn't this be dynamic based on the backend_class we extracted above?", "created_at": "2024-07-12T00:21:26Z", "updated_at": "2024-07-12T00:21:26Z", "html_url": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1674864039", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130507", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674864039"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1674864039"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130507"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674864039/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 4461, "original_line": 4446, "side": "RIGHT", "original_position": 157, "position": 172, "subject_type": "line", "PR": {"title": "[c10d] a new Pytorch API (split_group) to create a process group", "number": 130507, "id": 1963471316}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674866228", "pull_request_review_id": 2173418614, "id": 1674866228, "node_id": "PRRC_kwDOA-j9z85j1G40", "diff_hunk": "@@ -4297,6 +4298,188 @@ def _get_backend_from_str(backend: Optional[str] = None) -> Backend:\n     return Backend(backend)\n \n \n+@_time_logger\n+def split_group(\n+    parent_pg=None,\n+    split_ranks=None,\n+    timeout=None,\n+    pg_options=None,\n+    group_desc=None,\n+):\n+    \"\"\"\n+    Create a new process group splitted from the given parent process group.\n+\n+    warning:: only the ``NCCL`` backend supports this API. Other backends will raise an error.\n+    users of this API must gurantee that all ranks in the parent group enter this API call,\n+    and the split of the sub groups is the same accross all ranks in the parent group.\n+\n+    Args:\n+        parent_pg (ProcessGroup, optional): The parent process group. If None,\n+            the default process group will be used. Users need to gurantee that\n+            the parent group is fully initialized (e.g, communicators are initialized)\n+        split_ranks (list[list[int]]): the split ranks, which is a list of list of ranks.\n+            Users need to make sure the validity of the split ranks such that one\n+            split (represented by one inner list of ints) does not overlap with any other split.\n+            Note that the ranks in each split is the group rank (instead of global rank)\n+            in the parent pg. For example, if the parent group has 4 ranks, and split_ranks can be\n+            [[0, 1], [2, 3]]. Note [[0,1]] is also a valid split, in which case ranks 2, 3 would\n+            return a non-group member.\n+        timeout (timedelta, optional): see `init_process_group` for details and default value.\n+        pg_options (ProcessGroupNCCLOptions, optional): process group options\n+            specifying what additional options need to be passed in during\n+            the construction of specific process groups. i.e.``is_high_priority_stream``\n+            can be specified so that process group can pick up high priority cuda streams.\n+        group_desc (str, optional): a string to describe the process group.\n+\n+    Returns:\n+        ProcessGroup if the current rank is within one split/subgroup given by split_ranks,\n+        or GroupMember.NON_GROUP_MEMBER if the current rank is not part of any split_ranks`.\n+\n+    \"\"\"\n+    # check inputs\n+    if split_ranks is None:\n+        raise ValueError(\"split_ranks cannot be None\")\n+\n+    if pg_options is not None:\n+        assert isinstance(\n+            pg_options, ProcessGroupNCCL.Options\n+        ), \"Expected pg_options argument to be of type ProcessGroupNCCL.Options\"\n+    else:\n+        # default pg_options for NCCL\n+        pg_options = ProcessGroupNCCL.Options()\n+        pg_options.is_high_priority_stream = False\n+\n+    group_desc = \"undefined\" if group_desc is None else group_desc\n+\n+    global _world\n+    default_pg = _get_default_group()\n+    device_id = default_pg.bound_device_id\n+    if not device_id:\n+        raise RuntimeError(\n+            \"No device associated with the default pg, not safe to split any process groups\"\n+        )\n+    default_backend, default_store = _world.pg_map[default_pg]\n+    global_rank = default_pg.rank()\n+    global_world_size = default_pg.size()\n+\n+    if not parent_pg:\n+        parent_pg = default_pg\n+    if parent_pg not in _world.pg_group_ranks:\n+        raise ValueError(f\"Group {parent_pg} is not registered\")\n+\n+    parent_global_to_group_ranks = _world.pg_group_ranks[parent_pg]\n+    parent_group_to_global_ranks = {\n+        group_rank: global_rank\n+        for global_rank, group_rank in parent_global_to_group_ranks.items()\n+    }\n+\n+    if global_rank not in parent_global_to_group_ranks:\n+        raise ValueError(\n+            f\"Global rank {global_rank} is not part of the parent group {parent_pg}\"\n+        )\n+\n+    parent_group_rank = parent_global_to_group_ranks[global_rank]\n+    parent_backend = parent_pg._get_backend(torch.device(\"cuda\"))\n+\n+    # if the parent backend does not support splitting, raise error\n+    # currently this API only support NCCL backend\n+    if (\n+        not parent_backend\n+        or not parent_backend.supports_splitting\n+        or not isinstance(parent_backend, ProcessGroupNCCL)\n+    ):\n+        raise RuntimeError(\n+            \"No backend for the parent process group or its backend does not support splitting\"\n+        )\n+\n+    parent_backend_str, _ = _world.pg_map[parent_pg]\n+    # same type of backend as the parent process group\n+    backend = Backend(parent_backend_str)\n+    backend_config = BackendConfig(backend)\n+\n+    # this timeout defaulting/validation is used for all the new_groups/new_subgroups variants,\n+    # which may just pass their timeout value (or None)\n+    if timeout is None:\n+        timeout = _get_default_timeout(backend)\n+    _check_valid_timeout(timeout)\n+\n+    # find my group of ranks and my group local rank in split_ranks\n+    my_group = None\n+    group_rank = -1\n+\n+    for split_group in split_ranks:\n+        if len(split_group) == 0:\n+            raise ValueError(\"the split group cannot be empty\")\n+        if len(split_group) > global_world_size:\n+            raise ValueError(\n+                \"the split group's size should be less or equal to the world_size set by init_process_group\"\n+            )\n+        if len(split_group) != len(set(split_group)):\n+            raise ValueError(\"the split group cannot have duplicate ranks\")\n+        split_group = sorted(split_group)\n+        if parent_group_rank in split_group:\n+            my_group = split_group\n+            group_rank = split_group.index(parent_group_rank)\n+            break\n+    # if my rank does not belong to any sub group or my rank is the only member in the subgroup,\n+    # no_color split should be called\n+    if my_group is None or group_rank == -1 or len(my_group) == 1:\n+        parent_backend.perform_nocolor_split(device_id)\n+        return GroupMember.NON_GROUP_MEMBER\n+\n+    group_name = _process_group_name(my_group, use_hashed_name=False)\n+    global_ranks_in_my_group = [parent_group_to_global_ranks[rank] for rank in my_group]\n+\n+    prefix_store = PrefixStore(f\"{group_name}/\", default_store)\n+    base_pg_options = ProcessGroup.Options(backend=str(backend))\n+    base_pg_options._timeout = timeout\n+    pg: ProcessGroup = ProcessGroup(\n+        prefix_store, group_rank, len(my_group), base_pg_options\n+    )\n+    pg.bound_device_id = device_id\n+\n+    pg_options._timeout = timeout\n+    pg_options.split_from = parent_backend\n+    pg_options.split_color = _process_group_color(my_group)\n+    pg_options.global_ranks_in_group = global_ranks_in_my_group\n+    pg_options.group_name = group_name\n+    backend_class = ProcessGroupNCCL(\n+        prefix_store, group_rank, len(my_group), pg_options\n+    )\n+    backend_type = ProcessGroup.BackendType.NCCL\n+    backend_class._set_sequence_number_for_group()\n+\n+    pg._register_backend(torch.device(\"cuda\"), backend_type, backend_class)\n+\n+    # set group_name and group_dsec to backend\n+    assert group_name is not None\n+    assert group_desc is not None\n+    pg._set_group_name(group_name)\n+    pg._set_group_desc(group_desc)", "path": "torch/distributed/distributed_c10d.py", "commit_id": "3b376336469eda235814aee75ac079befb5367fc", "original_commit_id": "f9b1f21870962f948972cb8c1b28018162c3f1c6", "user": {"login": "wconstab", "id": 4984825, "url": "https://api.github.com/users/wconstab", "html_url": "https://github.com/wconstab"}, "body": "wonder if we should make a default group_desc if None is provided.  something like parent_desc + \"split\" + num_splits_so_far++?", "created_at": "2024-07-12T00:23:08Z", "updated_at": "2024-07-12T00:24:00Z", "html_url": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1674866228", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130507", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674866228"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1674866228"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130507"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674866228/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 4473, "original_line": 4458, "side": "RIGHT", "original_position": 169, "position": 184, "subject_type": "line", "PR": {"title": "[c10d] a new Pytorch API (split_group) to create a process group", "number": 130507, "id": 1963471316}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674878076", "pull_request_review_id": 2173443210, "id": 1674878076, "node_id": "PRRC_kwDOA-j9z85j1Jx8", "diff_hunk": "@@ -4228,6 +4251,140 @@ def test_nccl_errors_dump(self):\n             sys.exit(1)\n \n \n+# tests that needs to be run with a larger world size\n+class ProcessGroupNCCLLargerScaleTest(MultiProcessTestCase):\n+    def _create_process_group_nccl(self, store, opts, device_id=None):\n+        # create nccl processgroup with opts\n+        c10d.init_process_group(\n+            \"nccl\",\n+            world_size=self.world_size,\n+            rank=self.rank,\n+            store=store,\n+            pg_options=opts,\n+            device_id=device_id,\n+        )\n+        pg = c10d.distributed_c10d._get_default_group()\n+        return pg\n+\n+    def opts(self, high_priority_stream=False):\n+        opts = c10d.ProcessGroupNCCL.Options()\n+        opts.is_high_priority_stream = high_priority_stream\n+        return opts\n+\n+    def setUp(self):\n+        super().setUp()\n+        # TORCH_NCCL_BLOCKING_WAIT overrides TORCH_NCCL_ASYNC_ERROR_HANDLING hence tests\n+        # that use TORCH_NCCL_BLOCKING_WAIT will test it as expected.\n+        os.environ[\"TORCH_NCCL_ASYNC_ERROR_HANDLING\"] = \"1\"\n+        # self.num_gpus = torch.cuda.device_count()\n+        self._spawn_processes()\n+\n+    def tearDown(self):\n+        super().tearDown()\n+        try:\n+            os.remove(self.file_name)\n+        except OSError:\n+            pass\n+\n+    @property\n+    def world_size(self):\n+        return 8\n+\n+    @property\n+    def rank_to_GPU(self):\n+        # return rank to GPU map\n+        return init_multigpu_helper(self.world_size, \"nccl\")\n+\n+    @requires_nccl_version((2, 18), \"Need NCCL 2.18+ for ncclCommSplit\")\n+    @skip_but_pass_in_sandcastle_if(not TEST_MULTIGPU, \"NCCL test requires 2+ GPUs\")\n+    @skip_if_lt_x_gpu(8)\n+    def test_comm_split_group_larger_scale(self):\n+        store = c10d.FileStore(self.file_name, self.world_size)\n+        device = torch.device(f\"cuda:{self.rank}\")\n+        pg = self._create_process_group_nccl(store, self.opts(), device_id=device)\n+        backend = pg._get_backend(torch.device(device))\n+\n+        tensor = torch.full((1,), self.rank).cuda(device)\n+        ng1 = c10d.split_group(pg, [[0, 1], [2, 3, 4, 5, 6, 7]])\n+        backend1 = ng1._get_backend(torch.device(device))\n+\n+        # comm split happens eagerly since device_id is passed to init_process_group.\n+        self.assertEqual(backend.comm_split_count(), 1)\n+        # dist.broadcast take Source rank on global process group\n+        if self.rank < 2:\n+            dist.broadcast(tensor, 0, group=ng1)\n+            self.assertEqual(tensor, torch.full((1,), 0))\n+        else:\n+            dist.broadcast(tensor, 2, group=ng1)\n+            self.assertEqual(tensor, torch.full((1,), 2))\n+\n+        # test split with only one colored group, other ranks should be no color split.\n+        ng2 = c10d.split_group(pg, [[5, 6, 7]], timeout=timedelta(seconds=50))\n+        self.assertEqual(backend.comm_split_count(), 2)\n+\n+        if self.rank >= 5:\n+            tensor2 = torch.full((1,), self.rank).cuda(device)\n+            dist.broadcast(tensor2, 7, group=ng2)\n+            self.assertEqual(tensor2, torch.full((1,), 7))\n+        else:\n+            self.assertEqual(ng2, c10d.GroupMember.NON_GROUP_MEMBER)\n+        # give enough time for the broadcast to finish before destroying all pgs.\n+        time.sleep(2)", "path": "test/distributed/test_c10d_nccl.py", "commit_id": "3b376336469eda235814aee75ac079befb5367fc", "original_commit_id": "f9b1f21870962f948972cb8c1b28018162c3f1c6", "user": {"login": "wconstab", "id": 4984825, "url": "https://api.github.com/users/wconstab", "html_url": "https://github.com/wconstab"}, "body": "can't you just wait the broadcast and synchronize cuda? then avoid sleep?", "created_at": "2024-07-12T00:28:54Z", "updated_at": "2024-07-12T00:28:54Z", "html_url": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1674878076", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130507", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674878076"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1674878076"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130507"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674878076/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": null, "original_line": 4332, "side": "RIGHT", "original_position": 112, "position": null, "subject_type": "line", "PR": {"title": "[c10d] a new Pytorch API (split_group) to create a process group", "number": 130507, "id": 1963471316}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674883205", "pull_request_review_id": 2173454643, "id": 1674883205, "node_id": "PRRC_kwDOA-j9z85j1LCF", "diff_hunk": "@@ -4228,6 +4251,140 @@ def test_nccl_errors_dump(self):\n             sys.exit(1)\n \n \n+# tests that needs to be run with a larger world size\n+class ProcessGroupNCCLLargerScaleTest(MultiProcessTestCase):\n+    def _create_process_group_nccl(self, store, opts, device_id=None):\n+        # create nccl processgroup with opts\n+        c10d.init_process_group(\n+            \"nccl\",\n+            world_size=self.world_size,\n+            rank=self.rank,\n+            store=store,\n+            pg_options=opts,\n+            device_id=device_id,\n+        )\n+        pg = c10d.distributed_c10d._get_default_group()\n+        return pg\n+\n+    def opts(self, high_priority_stream=False):\n+        opts = c10d.ProcessGroupNCCL.Options()\n+        opts.is_high_priority_stream = high_priority_stream\n+        return opts\n+\n+    def setUp(self):\n+        super().setUp()\n+        # TORCH_NCCL_BLOCKING_WAIT overrides TORCH_NCCL_ASYNC_ERROR_HANDLING hence tests\n+        # that use TORCH_NCCL_BLOCKING_WAIT will test it as expected.\n+        os.environ[\"TORCH_NCCL_ASYNC_ERROR_HANDLING\"] = \"1\"\n+        # self.num_gpus = torch.cuda.device_count()\n+        self._spawn_processes()\n+\n+    def tearDown(self):\n+        super().tearDown()\n+        try:\n+            os.remove(self.file_name)\n+        except OSError:\n+            pass\n+\n+    @property\n+    def world_size(self):\n+        return 8\n+\n+    @property\n+    def rank_to_GPU(self):\n+        # return rank to GPU map\n+        return init_multigpu_helper(self.world_size, \"nccl\")\n+\n+    @requires_nccl_version((2, 18), \"Need NCCL 2.18+ for ncclCommSplit\")\n+    @skip_but_pass_in_sandcastle_if(not TEST_MULTIGPU, \"NCCL test requires 2+ GPUs\")\n+    @skip_if_lt_x_gpu(8)\n+    def test_comm_split_group_larger_scale(self):\n+        store = c10d.FileStore(self.file_name, self.world_size)\n+        device = torch.device(f\"cuda:{self.rank}\")\n+        pg = self._create_process_group_nccl(store, self.opts(), device_id=device)\n+        backend = pg._get_backend(torch.device(device))\n+\n+        tensor = torch.full((1,), self.rank).cuda(device)\n+        ng1 = c10d.split_group(pg, [[0, 1], [2, 3, 4, 5, 6, 7]])\n+        backend1 = ng1._get_backend(torch.device(device))\n+\n+        # comm split happens eagerly since device_id is passed to init_process_group.\n+        self.assertEqual(backend.comm_split_count(), 1)\n+        # dist.broadcast take Source rank on global process group\n+        if self.rank < 2:\n+            dist.broadcast(tensor, 0, group=ng1)\n+            self.assertEqual(tensor, torch.full((1,), 0))\n+        else:\n+            dist.broadcast(tensor, 2, group=ng1)\n+            self.assertEqual(tensor, torch.full((1,), 2))\n+\n+        # test split with only one colored group, other ranks should be no color split.\n+        ng2 = c10d.split_group(pg, [[5, 6, 7]], timeout=timedelta(seconds=50))\n+        self.assertEqual(backend.comm_split_count(), 2)\n+\n+        if self.rank >= 5:\n+            tensor2 = torch.full((1,), self.rank).cuda(device)\n+            dist.broadcast(tensor2, 7, group=ng2)\n+            self.assertEqual(tensor2, torch.full((1,), 7))\n+        else:\n+            self.assertEqual(ng2, c10d.GroupMember.NON_GROUP_MEMBER)\n+        # give enough time for the broadcast to finish before destroying all pgs.\n+        time.sleep(2)\n+        dist.destroy_process_group()\n+\n+    @requires_nccl_version((2, 18), \"Need NCCL 2.18+ for ncclCommSplit\")\n+    @skip_but_pass_in_sandcastle_if(not TEST_MULTIGPU, \"NCCL test requires 2+ GPUs\")\n+    @skip_if_lt_x_gpu(8)", "path": "test/distributed/test_c10d_nccl.py", "commit_id": "3b376336469eda235814aee75ac079befb5367fc", "original_commit_id": "f9b1f21870962f948972cb8c1b28018162c3f1c6", "user": {"login": "wconstab", "id": 4984825, "url": "https://api.github.com/users/wconstab", "html_url": "https://github.com/wconstab"}, "body": "could you add tests that make sure options from parent group are applied to the split group (timeout, perhaps also high-prio stream, any others that i forgot about?", "created_at": "2024-07-12T00:31:44Z", "updated_at": "2024-07-12T00:31:45Z", "html_url": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1674883205", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130507", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674883205"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1674883205"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130507"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674883205/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 4346, "original_line": 4337, "side": "RIGHT", "original_position": 117, "position": 126, "subject_type": "line", "PR": {"title": "[c10d] a new Pytorch API (split_group) to create a process group", "number": 130507, "id": 1963471316}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674918406", "pull_request_review_id": 2173504682, "id": 1674918406, "node_id": "PRRC_kwDOA-j9z85j1ToG", "diff_hunk": "@@ -4228,6 +4251,140 @@ def test_nccl_errors_dump(self):\n             sys.exit(1)\n \n \n+# tests that needs to be run with a larger world size\n+class ProcessGroupNCCLLargerScaleTest(MultiProcessTestCase):\n+    def _create_process_group_nccl(self, store, opts, device_id=None):\n+        # create nccl processgroup with opts\n+        c10d.init_process_group(\n+            \"nccl\",\n+            world_size=self.world_size,\n+            rank=self.rank,\n+            store=store,\n+            pg_options=opts,\n+            device_id=device_id,\n+        )\n+        pg = c10d.distributed_c10d._get_default_group()\n+        return pg\n+\n+    def opts(self, high_priority_stream=False):\n+        opts = c10d.ProcessGroupNCCL.Options()\n+        opts.is_high_priority_stream = high_priority_stream\n+        return opts\n+\n+    def setUp(self):\n+        super().setUp()\n+        # TORCH_NCCL_BLOCKING_WAIT overrides TORCH_NCCL_ASYNC_ERROR_HANDLING hence tests\n+        # that use TORCH_NCCL_BLOCKING_WAIT will test it as expected.\n+        os.environ[\"TORCH_NCCL_ASYNC_ERROR_HANDLING\"] = \"1\"\n+        # self.num_gpus = torch.cuda.device_count()\n+        self._spawn_processes()\n+\n+    def tearDown(self):\n+        super().tearDown()\n+        try:\n+            os.remove(self.file_name)\n+        except OSError:\n+            pass\n+\n+    @property\n+    def world_size(self):\n+        return 8\n+\n+    @property\n+    def rank_to_GPU(self):\n+        # return rank to GPU map\n+        return init_multigpu_helper(self.world_size, \"nccl\")\n+\n+    @requires_nccl_version((2, 18), \"Need NCCL 2.18+ for ncclCommSplit\")\n+    @skip_but_pass_in_sandcastle_if(not TEST_MULTIGPU, \"NCCL test requires 2+ GPUs\")\n+    @skip_if_lt_x_gpu(8)\n+    def test_comm_split_group_larger_scale(self):\n+        store = c10d.FileStore(self.file_name, self.world_size)\n+        device = torch.device(f\"cuda:{self.rank}\")\n+        pg = self._create_process_group_nccl(store, self.opts(), device_id=device)\n+        backend = pg._get_backend(torch.device(device))\n+\n+        tensor = torch.full((1,), self.rank).cuda(device)\n+        ng1 = c10d.split_group(pg, [[0, 1], [2, 3, 4, 5, 6, 7]])\n+        backend1 = ng1._get_backend(torch.device(device))\n+\n+        # comm split happens eagerly since device_id is passed to init_process_group.\n+        self.assertEqual(backend.comm_split_count(), 1)\n+        # dist.broadcast take Source rank on global process group\n+        if self.rank < 2:\n+            dist.broadcast(tensor, 0, group=ng1)\n+            self.assertEqual(tensor, torch.full((1,), 0))\n+        else:\n+            dist.broadcast(tensor, 2, group=ng1)\n+            self.assertEqual(tensor, torch.full((1,), 2))\n+\n+        # test split with only one colored group, other ranks should be no color split.\n+        ng2 = c10d.split_group(pg, [[5, 6, 7]], timeout=timedelta(seconds=50))\n+        self.assertEqual(backend.comm_split_count(), 2)\n+\n+        if self.rank >= 5:\n+            tensor2 = torch.full((1,), self.rank).cuda(device)\n+            dist.broadcast(tensor2, 7, group=ng2)\n+            self.assertEqual(tensor2, torch.full((1,), 7))\n+        else:\n+            self.assertEqual(ng2, c10d.GroupMember.NON_GROUP_MEMBER)\n+        # give enough time for the broadcast to finish before destroying all pgs.\n+        time.sleep(2)", "path": "test/distributed/test_c10d_nccl.py", "commit_id": "3b376336469eda235814aee75ac079befb5367fc", "original_commit_id": "f9b1f21870962f948972cb8c1b28018162c3f1c6", "user": {"login": "shuqiangzhang", "id": 4711970, "url": "https://api.github.com/users/shuqiangzhang", "html_url": "https://github.com/shuqiangzhang"}, "body": "weirdly, broadcast() should be sync call by default, which means work.wait is called. But obviously it does not work as expected, this is something we need to follow up", "created_at": "2024-07-12T01:10:17Z", "updated_at": "2024-07-12T01:10:17Z", "html_url": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1674918406", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130507", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674918406"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1674918406"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130507"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674918406/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": null, "original_line": 4332, "side": "RIGHT", "in_reply_to_id": 1674878076, "original_position": 112, "position": null, "subject_type": "line", "PR": {"title": "[c10d] a new Pytorch API (split_group) to create a process group", "number": 130507, "id": 1963471316}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674989823", "pull_request_review_id": 2173532093, "id": 1674989823, "node_id": "PRRC_kwDOA-j9z85j1lD_", "diff_hunk": "@@ -4228,6 +4251,140 @@ def test_nccl_errors_dump(self):\n             sys.exit(1)\n \n \n+# tests that needs to be run with a larger world size\n+class ProcessGroupNCCLLargerScaleTest(MultiProcessTestCase):\n+    def _create_process_group_nccl(self, store, opts, device_id=None):\n+        # create nccl processgroup with opts\n+        c10d.init_process_group(\n+            \"nccl\",\n+            world_size=self.world_size,\n+            rank=self.rank,\n+            store=store,\n+            pg_options=opts,\n+            device_id=device_id,\n+        )\n+        pg = c10d.distributed_c10d._get_default_group()\n+        return pg\n+\n+    def opts(self, high_priority_stream=False):\n+        opts = c10d.ProcessGroupNCCL.Options()\n+        opts.is_high_priority_stream = high_priority_stream\n+        return opts\n+\n+    def setUp(self):\n+        super().setUp()\n+        # TORCH_NCCL_BLOCKING_WAIT overrides TORCH_NCCL_ASYNC_ERROR_HANDLING hence tests\n+        # that use TORCH_NCCL_BLOCKING_WAIT will test it as expected.\n+        os.environ[\"TORCH_NCCL_ASYNC_ERROR_HANDLING\"] = \"1\"\n+        # self.num_gpus = torch.cuda.device_count()\n+        self._spawn_processes()\n+\n+    def tearDown(self):\n+        super().tearDown()\n+        try:\n+            os.remove(self.file_name)\n+        except OSError:\n+            pass\n+\n+    @property\n+    def world_size(self):\n+        return 8\n+\n+    @property\n+    def rank_to_GPU(self):\n+        # return rank to GPU map\n+        return init_multigpu_helper(self.world_size, \"nccl\")\n+\n+    @requires_nccl_version((2, 18), \"Need NCCL 2.18+ for ncclCommSplit\")\n+    @skip_but_pass_in_sandcastle_if(not TEST_MULTIGPU, \"NCCL test requires 2+ GPUs\")\n+    @skip_if_lt_x_gpu(8)\n+    def test_comm_split_group_larger_scale(self):\n+        store = c10d.FileStore(self.file_name, self.world_size)\n+        device = torch.device(f\"cuda:{self.rank}\")\n+        pg = self._create_process_group_nccl(store, self.opts(), device_id=device)\n+        backend = pg._get_backend(torch.device(device))\n+\n+        tensor = torch.full((1,), self.rank).cuda(device)\n+        ng1 = c10d.split_group(pg, [[0, 1], [2, 3, 4, 5, 6, 7]])\n+        backend1 = ng1._get_backend(torch.device(device))\n+\n+        # comm split happens eagerly since device_id is passed to init_process_group.\n+        self.assertEqual(backend.comm_split_count(), 1)\n+        # dist.broadcast take Source rank on global process group\n+        if self.rank < 2:\n+            dist.broadcast(tensor, 0, group=ng1)\n+            self.assertEqual(tensor, torch.full((1,), 0))\n+        else:\n+            dist.broadcast(tensor, 2, group=ng1)\n+            self.assertEqual(tensor, torch.full((1,), 2))\n+\n+        # test split with only one colored group, other ranks should be no color split.\n+        ng2 = c10d.split_group(pg, [[5, 6, 7]], timeout=timedelta(seconds=50))\n+        self.assertEqual(backend.comm_split_count(), 2)\n+\n+        if self.rank >= 5:\n+            tensor2 = torch.full((1,), self.rank).cuda(device)\n+            dist.broadcast(tensor2, 7, group=ng2)\n+            self.assertEqual(tensor2, torch.full((1,), 7))\n+        else:\n+            self.assertEqual(ng2, c10d.GroupMember.NON_GROUP_MEMBER)\n+        # give enough time for the broadcast to finish before destroying all pgs.\n+        time.sleep(2)\n+        dist.destroy_process_group()\n+\n+    @requires_nccl_version((2, 18), \"Need NCCL 2.18+ for ncclCommSplit\")\n+    @skip_but_pass_in_sandcastle_if(not TEST_MULTIGPU, \"NCCL test requires 2+ GPUs\")", "path": "test/distributed/test_c10d_nccl.py", "commit_id": "3b376336469eda235814aee75ac079befb5367fc", "original_commit_id": "f9b1f21870962f948972cb8c1b28018162c3f1c6", "user": {"login": "fduwjj", "id": 6937752, "url": "https://api.github.com/users/fduwjj", "html_url": "https://github.com/fduwjj"}, "body": "Nit: maybe required 8 GPUs?", "created_at": "2024-07-12T01:52:43Z", "updated_at": "2024-07-12T02:04:26Z", "html_url": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1674989823", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130507", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674989823"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1674989823"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130507"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674989823/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": null, "original_line": 4336, "side": "RIGHT", "original_position": 116, "position": null, "subject_type": "line", "PR": {"title": "[c10d] a new Pytorch API (split_group) to create a process group", "number": 130507, "id": 1963471316}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1675002554", "pull_request_review_id": 2173532093, "id": 1675002554, "node_id": "PRRC_kwDOA-j9z85j1oK6", "diff_hunk": "@@ -4297,6 +4298,188 @@ def _get_backend_from_str(backend: Optional[str] = None) -> Backend:\n     return Backend(backend)\n \n \n+@_time_logger\n+def split_group(\n+    parent_pg=None,\n+    split_ranks=None,\n+    timeout=None,\n+    pg_options=None,\n+    group_desc=None,\n+):\n+    \"\"\"\n+    Create a new process group splitted from the given parent process group.\n+\n+    warning:: only the ``NCCL`` backend supports this API. Other backends will raise an error.\n+    users of this API must gurantee that all ranks in the parent group enter this API call,", "path": "torch/distributed/distributed_c10d.py", "commit_id": "3b376336469eda235814aee75ac079befb5367fc", "original_commit_id": "f9b1f21870962f948972cb8c1b28018162c3f1c6", "user": {"login": "fduwjj", "id": 6937752, "url": "https://api.github.com/users/fduwjj", "html_url": "https://github.com/fduwjj"}, "body": "all ranks in the parent group meaning \"all global ranks from default group\"?", "created_at": "2024-07-12T01:58:39Z", "updated_at": "2024-07-12T02:04:26Z", "html_url": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1675002554", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130507", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1675002554"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1675002554"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130507"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1675002554/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": null, "original_line": 4313, "side": "RIGHT", "original_position": 24, "position": null, "subject_type": "line", "PR": {"title": "[c10d] a new Pytorch API (split_group) to create a process group", "number": 130507, "id": 1963471316}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1675007943", "pull_request_review_id": 2173532093, "id": 1675007943, "node_id": "PRRC_kwDOA-j9z85j1pfH", "diff_hunk": "@@ -4297,6 +4298,188 @@ def _get_backend_from_str(backend: Optional[str] = None) -> Backend:\n     return Backend(backend)\n \n \n+@_time_logger\n+def split_group(\n+    parent_pg=None,\n+    split_ranks=None,\n+    timeout=None,\n+    pg_options=None,\n+    group_desc=None,\n+):\n+    \"\"\"\n+    Create a new process group splitted from the given parent process group.\n+\n+    warning:: only the ``NCCL`` backend supports this API. Other backends will raise an error.\n+    users of this API must gurantee that all ranks in the parent group enter this API call,\n+    and the split of the sub groups is the same accross all ranks in the parent group.\n+\n+    Args:\n+        parent_pg (ProcessGroup, optional): The parent process group. If None,\n+            the default process group will be used. Users need to gurantee that\n+            the parent group is fully initialized (e.g, communicators are initialized)\n+        split_ranks (list[list[int]]): the split ranks, which is a list of list of ranks.\n+            Users need to make sure the validity of the split ranks such that one\n+            split (represented by one inner list of ints) does not overlap with any other split.\n+            Note that the ranks in each split is the group rank (instead of global rank)\n+            in the parent pg. For example, if the parent group has 4 ranks, and split_ranks can be\n+            [[0, 1], [2, 3]]. Note [[0,1]] is also a valid split, in which case ranks 2, 3 would\n+            return a non-group member.\n+        timeout (timedelta, optional): see `init_process_group` for details and default value.\n+        pg_options (ProcessGroupNCCLOptions, optional): process group options\n+            specifying what additional options need to be passed in during\n+            the construction of specific process groups. i.e.``is_high_priority_stream``\n+            can be specified so that process group can pick up high priority cuda streams.\n+        group_desc (str, optional): a string to describe the process group.\n+\n+    Returns:\n+        ProcessGroup if the current rank is within one split/subgroup given by split_ranks,\n+        or GroupMember.NON_GROUP_MEMBER if the current rank is not part of any split_ranks`.\n+\n+    \"\"\"\n+    # check inputs\n+    if split_ranks is None:\n+        raise ValueError(\"split_ranks cannot be None\")\n+\n+    if pg_options is not None:\n+        assert isinstance(\n+            pg_options, ProcessGroupNCCL.Options\n+        ), \"Expected pg_options argument to be of type ProcessGroupNCCL.Options\"\n+    else:\n+        # default pg_options for NCCL\n+        pg_options = ProcessGroupNCCL.Options()\n+        pg_options.is_high_priority_stream = False\n+\n+    group_desc = \"undefined\" if group_desc is None else group_desc\n+\n+    global _world\n+    default_pg = _get_default_group()\n+    device_id = default_pg.bound_device_id\n+    if not device_id:\n+        raise RuntimeError(\n+            \"No device associated with the default pg, not safe to split any process groups\"\n+        )\n+    default_backend, default_store = _world.pg_map[default_pg]\n+    global_rank = default_pg.rank()\n+    global_world_size = default_pg.size()\n+\n+    if not parent_pg:\n+        parent_pg = default_pg\n+    if parent_pg not in _world.pg_group_ranks:\n+        raise ValueError(f\"Group {parent_pg} is not registered\")\n+\n+    parent_global_to_group_ranks = _world.pg_group_ranks[parent_pg]\n+    parent_group_to_global_ranks = {\n+        group_rank: global_rank\n+        for global_rank, group_rank in parent_global_to_group_ranks.items()\n+    }\n+\n+    if global_rank not in parent_global_to_group_ranks:\n+        raise ValueError(\n+            f\"Global rank {global_rank} is not part of the parent group {parent_pg}\"\n+        )\n+\n+    parent_group_rank = parent_global_to_group_ranks[global_rank]\n+    parent_backend = parent_pg._get_backend(torch.device(\"cuda\"))\n+\n+    # if the parent backend does not support splitting, raise error\n+    # currently this API only support NCCL backend\n+    if (\n+        not parent_backend\n+        or not parent_backend.supports_splitting\n+        or not isinstance(parent_backend, ProcessGroupNCCL)\n+    ):\n+        raise RuntimeError(\n+            \"No backend for the parent process group or its backend does not support splitting\"\n+        )\n+\n+    parent_backend_str, _ = _world.pg_map[parent_pg]\n+    # same type of backend as the parent process group\n+    backend = Backend(parent_backend_str)\n+    backend_config = BackendConfig(backend)\n+\n+    # this timeout defaulting/validation is used for all the new_groups/new_subgroups variants,\n+    # which may just pass their timeout value (or None)\n+    if timeout is None:\n+        timeout = _get_default_timeout(backend)\n+    _check_valid_timeout(timeout)\n+\n+    # find my group of ranks and my group local rank in split_ranks\n+    my_group = None\n+    group_rank = -1\n+\n+    for split_group in split_ranks:\n+        if len(split_group) == 0:\n+            raise ValueError(\"the split group cannot be empty\")\n+        if len(split_group) > global_world_size:\n+            raise ValueError(\n+                \"the split group's size should be less or equal to the world_size set by init_process_group\"\n+            )\n+        if len(split_group) != len(set(split_group)):\n+            raise ValueError(\"the split group cannot have duplicate ranks\")\n+        split_group = sorted(split_group)\n+        if parent_group_rank in split_group:\n+            my_group = split_group\n+            group_rank = split_group.index(parent_group_rank)\n+            break\n+    # if my rank does not belong to any sub group or my rank is the only member in the subgroup,\n+    # no_color split should be called\n+    if my_group is None or group_rank == -1 or len(my_group) == 1:\n+        parent_backend.perform_nocolor_split(device_id)\n+        return GroupMember.NON_GROUP_MEMBER\n+\n+    group_name = _process_group_name(my_group, use_hashed_name=False)\n+    global_ranks_in_my_group = [parent_group_to_global_ranks[rank] for rank in my_group]\n+\n+    prefix_store = PrefixStore(f\"{group_name}/\", default_store)\n+    base_pg_options = ProcessGroup.Options(backend=str(backend))\n+    base_pg_options._timeout = timeout\n+    pg: ProcessGroup = ProcessGroup(\n+        prefix_store, group_rank, len(my_group), base_pg_options\n+    )\n+    pg.bound_device_id = device_id\n+\n+    pg_options._timeout = timeout\n+    pg_options.split_from = parent_backend\n+    pg_options.split_color = _process_group_color(my_group)\n+    pg_options.global_ranks_in_group = global_ranks_in_my_group\n+    pg_options.group_name = group_name\n+    backend_class = ProcessGroupNCCL(\n+        prefix_store, group_rank, len(my_group), pg_options\n+    )\n+    backend_type = ProcessGroup.BackendType.NCCL\n+    backend_class._set_sequence_number_for_group()\n+\n+    pg._register_backend(torch.device(\"cuda\"), backend_type, backend_class)\n+\n+    # set group_name and group_dsec to backend\n+    assert group_name is not None\n+    assert group_desc is not None\n+    pg._set_group_name(group_name)\n+    pg._set_group_desc(group_desc)\n+\n+    # always eagerly initialize the backend in split_group\n+    eager_backend = pg._get_backend(device_id)\n+    eager_backend.eager_connect_single_device(device_id)\n+\n+    # update global state\n+    _world.pg_map[pg] = (backend, prefix_store)\n+    _world.pg_names[pg] = group_name\n+    _register_process_group(group_name, pg)\n+    _world.pg_backend_config[pg] = str(backend_config)\n+    pg_tag = f\"ptd:{group_name}\"\n+    _world.tags_to_pg.setdefault(\"\", []).append(pg)\n+    _world.tags_to_pg.setdefault(pg_tag, []).append(pg)", "path": "torch/distributed/distributed_c10d.py", "commit_id": "3b376336469eda235814aee75ac079befb5367fc", "original_commit_id": "f9b1f21870962f948972cb8c1b28018162c3f1c6", "user": {"login": "fduwjj", "id": 6937752, "url": "https://api.github.com/users/fduwjj", "html_url": "https://github.com/fduwjj"}, "body": "Why do we need to do this twice?", "created_at": "2024-07-12T02:04:00Z", "updated_at": "2024-07-12T02:04:26Z", "html_url": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1675007943", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130507", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1675007943"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1675007943"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130507"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1675007943/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": 4470, "start_side": "RIGHT", "line": null, "original_line": 4471, "side": "RIGHT", "original_position": 182, "position": null, "subject_type": "line", "PR": {"title": "[c10d] a new Pytorch API (split_group) to create a process group", "number": 130507, "id": 1963471316}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676195375", "pull_request_review_id": 2175330168, "id": 1676195375, "node_id": "PRRC_kwDOA-j9z85j6LYv", "diff_hunk": "@@ -4297,6 +4298,188 @@ def _get_backend_from_str(backend: Optional[str] = None) -> Backend:\n     return Backend(backend)\n \n \n+@_time_logger\n+def split_group(\n+    parent_pg=None,\n+    split_ranks=None,\n+    timeout=None,\n+    pg_options=None,\n+    group_desc=None,\n+):\n+    \"\"\"\n+    Create a new process group splitted from the given parent process group.\n+\n+    warning:: only the ``NCCL`` backend supports this API. Other backends will raise an error.\n+    users of this API must gurantee that all ranks in the parent group enter this API call,", "path": "torch/distributed/distributed_c10d.py", "commit_id": "3b376336469eda235814aee75ac079befb5367fc", "original_commit_id": "f9b1f21870962f948972cb8c1b28018162c3f1c6", "user": {"login": "shuqiangzhang", "id": 4711970, "url": "https://api.github.com/users/shuqiangzhang", "html_url": "https://github.com/shuqiangzhang"}, "body": "No, it's only the ranks in the parent group who need to call the API", "created_at": "2024-07-12T16:49:47Z", "updated_at": "2024-07-12T16:50:12Z", "html_url": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1676195375", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130507", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676195375"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1676195375"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130507"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676195375/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": null, "original_line": 4313, "side": "RIGHT", "in_reply_to_id": 1675002554, "original_position": 24, "position": null, "subject_type": "line", "PR": {"title": "[c10d] a new Pytorch API (split_group) to create a process group", "number": 130507, "id": 1963471316}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676205080", "pull_request_review_id": 2175345647, "id": 1676205080, "node_id": "PRRC_kwDOA-j9z85j6NwY", "diff_hunk": "@@ -4297,6 +4298,188 @@ def _get_backend_from_str(backend: Optional[str] = None) -> Backend:\n     return Backend(backend)\n \n \n+@_time_logger\n+def split_group(\n+    parent_pg=None,\n+    split_ranks=None,\n+    timeout=None,\n+    pg_options=None,\n+    group_desc=None,\n+):\n+    \"\"\"\n+    Create a new process group splitted from the given parent process group.\n+\n+    warning:: only the ``NCCL`` backend supports this API. Other backends will raise an error.\n+    users of this API must gurantee that all ranks in the parent group enter this API call,\n+    and the split of the sub groups is the same accross all ranks in the parent group.\n+\n+    Args:\n+        parent_pg (ProcessGroup, optional): The parent process group. If None,\n+            the default process group will be used. Users need to gurantee that\n+            the parent group is fully initialized (e.g, communicators are initialized)\n+        split_ranks (list[list[int]]): the split ranks, which is a list of list of ranks.\n+            Users need to make sure the validity of the split ranks such that one\n+            split (represented by one inner list of ints) does not overlap with any other split.\n+            Note that the ranks in each split is the group rank (instead of global rank)\n+            in the parent pg. For example, if the parent group has 4 ranks, and split_ranks can be\n+            [[0, 1], [2, 3]]. Note [[0,1]] is also a valid split, in which case ranks 2, 3 would\n+            return a non-group member.\n+        timeout (timedelta, optional): see `init_process_group` for details and default value.\n+        pg_options (ProcessGroupNCCLOptions, optional): process group options\n+            specifying what additional options need to be passed in during\n+            the construction of specific process groups. i.e.``is_high_priority_stream``\n+            can be specified so that process group can pick up high priority cuda streams.\n+        group_desc (str, optional): a string to describe the process group.\n+\n+    Returns:\n+        ProcessGroup if the current rank is within one split/subgroup given by split_ranks,\n+        or GroupMember.NON_GROUP_MEMBER if the current rank is not part of any split_ranks`.\n+\n+    \"\"\"\n+    # check inputs\n+    if split_ranks is None:\n+        raise ValueError(\"split_ranks cannot be None\")\n+\n+    if pg_options is not None:\n+        assert isinstance(\n+            pg_options, ProcessGroupNCCL.Options\n+        ), \"Expected pg_options argument to be of type ProcessGroupNCCL.Options\"\n+    else:\n+        # default pg_options for NCCL\n+        pg_options = ProcessGroupNCCL.Options()\n+        pg_options.is_high_priority_stream = False\n+\n+    group_desc = \"undefined\" if group_desc is None else group_desc\n+\n+    global _world\n+    default_pg = _get_default_group()\n+    device_id = default_pg.bound_device_id\n+    if not device_id:\n+        raise RuntimeError(\n+            \"No device associated with the default pg, not safe to split any process groups\"\n+        )\n+    default_backend, default_store = _world.pg_map[default_pg]\n+    global_rank = default_pg.rank()\n+    global_world_size = default_pg.size()\n+\n+    if not parent_pg:\n+        parent_pg = default_pg\n+    if parent_pg not in _world.pg_group_ranks:\n+        raise ValueError(f\"Group {parent_pg} is not registered\")\n+\n+    parent_global_to_group_ranks = _world.pg_group_ranks[parent_pg]\n+    parent_group_to_global_ranks = {", "path": "torch/distributed/distributed_c10d.py", "commit_id": "3b376336469eda235814aee75ac079befb5367fc", "original_commit_id": "f9b1f21870962f948972cb8c1b28018162c3f1c6", "user": {"login": "shuqiangzhang", "id": 4711970, "url": "https://api.github.com/users/shuqiangzhang", "html_url": "https://github.com/shuqiangzhang"}, "body": "We need convert a sub list of input split_ranks to a group of global ranks, so we need a group rank to global rank dict, instead of just group ranks. With parent_global_to_group_ranks  as a start, we just need to reverse the mapping, there is no need to call the helper function", "created_at": "2024-07-12T16:59:45Z", "updated_at": "2024-07-12T17:02:04Z", "html_url": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1676205080", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130507", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676205080"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1676205080"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130507"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676205080/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 4371, "original_line": 4371, "side": "RIGHT", "in_reply_to_id": 1674833074, "original_position": 82, "position": 82, "subject_type": "line", "PR": {"title": "[c10d] a new Pytorch API (split_group) to create a process group", "number": 130507, "id": 1963471316}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676211680", "pull_request_review_id": 2175355990, "id": 1676211680, "node_id": "PRRC_kwDOA-j9z85j6PXg", "diff_hunk": "@@ -4297,6 +4298,188 @@ def _get_backend_from_str(backend: Optional[str] = None) -> Backend:\n     return Backend(backend)\n \n \n+@_time_logger\n+def split_group(\n+    parent_pg=None,\n+    split_ranks=None,\n+    timeout=None,\n+    pg_options=None,\n+    group_desc=None,\n+):\n+    \"\"\"\n+    Create a new process group splitted from the given parent process group.\n+\n+    warning:: only the ``NCCL`` backend supports this API. Other backends will raise an error.\n+    users of this API must gurantee that all ranks in the parent group enter this API call,\n+    and the split of the sub groups is the same accross all ranks in the parent group.\n+\n+    Args:\n+        parent_pg (ProcessGroup, optional): The parent process group. If None,\n+            the default process group will be used. Users need to gurantee that\n+            the parent group is fully initialized (e.g, communicators are initialized)\n+        split_ranks (list[list[int]]): the split ranks, which is a list of list of ranks.\n+            Users need to make sure the validity of the split ranks such that one\n+            split (represented by one inner list of ints) does not overlap with any other split.\n+            Note that the ranks in each split is the group rank (instead of global rank)\n+            in the parent pg. For example, if the parent group has 4 ranks, and split_ranks can be\n+            [[0, 1], [2, 3]]. Note [[0,1]] is also a valid split, in which case ranks 2, 3 would\n+            return a non-group member.\n+        timeout (timedelta, optional): see `init_process_group` for details and default value.\n+        pg_options (ProcessGroupNCCLOptions, optional): process group options\n+            specifying what additional options need to be passed in during\n+            the construction of specific process groups. i.e.``is_high_priority_stream``\n+            can be specified so that process group can pick up high priority cuda streams.\n+        group_desc (str, optional): a string to describe the process group.\n+\n+    Returns:\n+        ProcessGroup if the current rank is within one split/subgroup given by split_ranks,\n+        or GroupMember.NON_GROUP_MEMBER if the current rank is not part of any split_ranks`.\n+\n+    \"\"\"\n+    # check inputs\n+    if split_ranks is None:\n+        raise ValueError(\"split_ranks cannot be None\")\n+\n+    if pg_options is not None:\n+        assert isinstance(\n+            pg_options, ProcessGroupNCCL.Options\n+        ), \"Expected pg_options argument to be of type ProcessGroupNCCL.Options\"\n+    else:\n+        # default pg_options for NCCL\n+        pg_options = ProcessGroupNCCL.Options()\n+        pg_options.is_high_priority_stream = False\n+\n+    group_desc = \"undefined\" if group_desc is None else group_desc\n+\n+    global _world\n+    default_pg = _get_default_group()\n+    device_id = default_pg.bound_device_id\n+    if not device_id:\n+        raise RuntimeError(\n+            \"No device associated with the default pg, not safe to split any process groups\"\n+        )\n+    default_backend, default_store = _world.pg_map[default_pg]\n+    global_rank = default_pg.rank()\n+    global_world_size = default_pg.size()\n+\n+    if not parent_pg:\n+        parent_pg = default_pg\n+    if parent_pg not in _world.pg_group_ranks:\n+        raise ValueError(f\"Group {parent_pg} is not registered\")\n+\n+    parent_global_to_group_ranks = _world.pg_group_ranks[parent_pg]\n+    parent_group_to_global_ranks = {\n+        group_rank: global_rank\n+        for global_rank, group_rank in parent_global_to_group_ranks.items()\n+    }\n+\n+    if global_rank not in parent_global_to_group_ranks:\n+        raise ValueError(\n+            f\"Global rank {global_rank} is not part of the parent group {parent_pg}\"\n+        )\n+\n+    parent_group_rank = parent_global_to_group_ranks[global_rank]\n+    parent_backend = parent_pg._get_backend(torch.device(\"cuda\"))\n+\n+    # if the parent backend does not support splitting, raise error\n+    # currently this API only support NCCL backend\n+    if (\n+        not parent_backend\n+        or not parent_backend.supports_splitting\n+        or not isinstance(parent_backend, ProcessGroupNCCL)", "path": "torch/distributed/distributed_c10d.py", "commit_id": "3b376336469eda235814aee75ac079befb5367fc", "original_commit_id": "f9b1f21870962f948972cb8c1b28018162c3f1c6", "user": {"login": "shuqiangzhang", "id": 4711970, "url": "https://api.github.com/users/shuqiangzhang", "html_url": "https://github.com/shuqiangzhang"}, "body": "this API is only for NCCL. If we need to widen this API, the function needs to be overhauled anyway. And we also need this check: isinstance(parent_backend, ProcessGroupNCCL) to avoid lint warnings on other direct access of NCCL backend members", "created_at": "2024-07-12T17:06:00Z", "updated_at": "2024-07-12T18:37:57Z", "html_url": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1676211680", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130507", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676211680"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1676211680"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130507"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676211680/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 4389, "original_line": 4389, "side": "RIGHT", "in_reply_to_id": 1674833766, "original_position": 100, "position": 100, "subject_type": "line", "PR": {"title": "[c10d] a new Pytorch API (split_group) to create a process group", "number": 130507, "id": 1963471316}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676218116", "pull_request_review_id": 2175365692, "id": 1676218116, "node_id": "PRRC_kwDOA-j9z85j6Q8E", "diff_hunk": "@@ -4297,6 +4298,188 @@ def _get_backend_from_str(backend: Optional[str] = None) -> Backend:\n     return Backend(backend)\n \n \n+@_time_logger\n+def split_group(\n+    parent_pg=None,\n+    split_ranks=None,\n+    timeout=None,\n+    pg_options=None,\n+    group_desc=None,\n+):\n+    \"\"\"\n+    Create a new process group splitted from the given parent process group.\n+\n+    warning:: only the ``NCCL`` backend supports this API. Other backends will raise an error.\n+    users of this API must gurantee that all ranks in the parent group enter this API call,\n+    and the split of the sub groups is the same accross all ranks in the parent group.\n+\n+    Args:\n+        parent_pg (ProcessGroup, optional): The parent process group. If None,\n+            the default process group will be used. Users need to gurantee that\n+            the parent group is fully initialized (e.g, communicators are initialized)\n+        split_ranks (list[list[int]]): the split ranks, which is a list of list of ranks.\n+            Users need to make sure the validity of the split ranks such that one\n+            split (represented by one inner list of ints) does not overlap with any other split.\n+            Note that the ranks in each split is the group rank (instead of global rank)\n+            in the parent pg. For example, if the parent group has 4 ranks, and split_ranks can be\n+            [[0, 1], [2, 3]]. Note [[0,1]] is also a valid split, in which case ranks 2, 3 would\n+            return a non-group member.\n+        timeout (timedelta, optional): see `init_process_group` for details and default value.\n+        pg_options (ProcessGroupNCCLOptions, optional): process group options\n+            specifying what additional options need to be passed in during\n+            the construction of specific process groups. i.e.``is_high_priority_stream``\n+            can be specified so that process group can pick up high priority cuda streams.\n+        group_desc (str, optional): a string to describe the process group.\n+\n+    Returns:\n+        ProcessGroup if the current rank is within one split/subgroup given by split_ranks,\n+        or GroupMember.NON_GROUP_MEMBER if the current rank is not part of any split_ranks`.\n+\n+    \"\"\"\n+    # check inputs\n+    if split_ranks is None:\n+        raise ValueError(\"split_ranks cannot be None\")\n+\n+    if pg_options is not None:\n+        assert isinstance(\n+            pg_options, ProcessGroupNCCL.Options\n+        ), \"Expected pg_options argument to be of type ProcessGroupNCCL.Options\"\n+    else:\n+        # default pg_options for NCCL\n+        pg_options = ProcessGroupNCCL.Options()\n+        pg_options.is_high_priority_stream = False\n+\n+    group_desc = \"undefined\" if group_desc is None else group_desc\n+\n+    global _world\n+    default_pg = _get_default_group()\n+    device_id = default_pg.bound_device_id\n+    if not device_id:\n+        raise RuntimeError(\n+            \"No device associated with the default pg, not safe to split any process groups\"\n+        )\n+    default_backend, default_store = _world.pg_map[default_pg]\n+    global_rank = default_pg.rank()\n+    global_world_size = default_pg.size()\n+\n+    if not parent_pg:\n+        parent_pg = default_pg\n+    if parent_pg not in _world.pg_group_ranks:\n+        raise ValueError(f\"Group {parent_pg} is not registered\")\n+\n+    parent_global_to_group_ranks = _world.pg_group_ranks[parent_pg]\n+    parent_group_to_global_ranks = {\n+        group_rank: global_rank\n+        for global_rank, group_rank in parent_global_to_group_ranks.items()\n+    }\n+\n+    if global_rank not in parent_global_to_group_ranks:\n+        raise ValueError(\n+            f\"Global rank {global_rank} is not part of the parent group {parent_pg}\"\n+        )\n+\n+    parent_group_rank = parent_global_to_group_ranks[global_rank]\n+    parent_backend = parent_pg._get_backend(torch.device(\"cuda\"))\n+\n+    # if the parent backend does not support splitting, raise error\n+    # currently this API only support NCCL backend\n+    if (\n+        not parent_backend\n+        or not parent_backend.supports_splitting\n+        or not isinstance(parent_backend, ProcessGroupNCCL)\n+    ):\n+        raise RuntimeError(\n+            \"No backend for the parent process group or its backend does not support splitting\"\n+        )\n+\n+    parent_backend_str, _ = _world.pg_map[parent_pg]\n+    # same type of backend as the parent process group\n+    backend = Backend(parent_backend_str)\n+    backend_config = BackendConfig(backend)\n+\n+    # this timeout defaulting/validation is used for all the new_groups/new_subgroups variants,\n+    # which may just pass their timeout value (or None)\n+    if timeout is None:\n+        timeout = _get_default_timeout(backend)\n+    _check_valid_timeout(timeout)\n+\n+    # find my group of ranks and my group local rank in split_ranks\n+    my_group = None\n+    group_rank = -1\n+\n+    for split_group in split_ranks:\n+        if len(split_group) == 0:\n+            raise ValueError(\"the split group cannot be empty\")\n+        if len(split_group) > global_world_size:\n+            raise ValueError(\n+                \"the split group's size should be less or equal to the world_size set by init_process_group\"\n+            )\n+        if len(split_group) != len(set(split_group)):\n+            raise ValueError(\"the split group cannot have duplicate ranks\")\n+        split_group = sorted(split_group)\n+        if parent_group_rank in split_group:\n+            my_group = split_group\n+            group_rank = split_group.index(parent_group_rank)\n+            break\n+    # if my rank does not belong to any sub group or my rank is the only member in the subgroup,\n+    # no_color split should be called\n+    if my_group is None or group_rank == -1 or len(my_group) == 1:\n+        parent_backend.perform_nocolor_split(device_id)\n+        return GroupMember.NON_GROUP_MEMBER\n+\n+    group_name = _process_group_name(my_group, use_hashed_name=False)\n+    global_ranks_in_my_group = [parent_group_to_global_ranks[rank] for rank in my_group]\n+\n+    prefix_store = PrefixStore(f\"{group_name}/\", default_store)\n+    base_pg_options = ProcessGroup.Options(backend=str(backend))\n+    base_pg_options._timeout = timeout\n+    pg: ProcessGroup = ProcessGroup(\n+        prefix_store, group_rank, len(my_group), base_pg_options\n+    )\n+    pg.bound_device_id = device_id\n+\n+    pg_options._timeout = timeout\n+    pg_options.split_from = parent_backend\n+    pg_options.split_color = _process_group_color(my_group)\n+    pg_options.global_ranks_in_group = global_ranks_in_my_group\n+    pg_options.group_name = group_name\n+    backend_class = ProcessGroupNCCL(", "path": "torch/distributed/distributed_c10d.py", "commit_id": "3b376336469eda235814aee75ac079befb5367fc", "original_commit_id": "f9b1f21870962f948972cb8c1b28018162c3f1c6", "user": {"login": "shuqiangzhang", "id": 4711970, "url": "https://api.github.com/users/shuqiangzhang", "html_url": "https://github.com/shuqiangzhang"}, "body": "I would prefer the code to be short and succinct and this API is only for nccl (not gloo/MPI) and we check the input rigorously. That's why we can keep the code here much more succinct than the nested calls of new_group  ", "created_at": "2024-07-12T17:10:36Z", "updated_at": "2024-07-12T17:10:37Z", "html_url": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1676218116", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130507", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676218116"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1676218116"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130507"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676218116/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 4461, "original_line": 4446, "side": "RIGHT", "in_reply_to_id": 1674864039, "original_position": 157, "position": 172, "subject_type": "line", "PR": {"title": "[c10d] a new Pytorch API (split_group) to create a process group", "number": 130507, "id": 1963471316}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676222007", "pull_request_review_id": 2175372082, "id": 1676222007, "node_id": "PRRC_kwDOA-j9z85j6R43", "diff_hunk": "@@ -4297,6 +4298,188 @@ def _get_backend_from_str(backend: Optional[str] = None) -> Backend:\n     return Backend(backend)\n \n \n+@_time_logger\n+def split_group(\n+    parent_pg=None,\n+    split_ranks=None,\n+    timeout=None,\n+    pg_options=None,\n+    group_desc=None,\n+):\n+    \"\"\"\n+    Create a new process group splitted from the given parent process group.\n+\n+    warning:: only the ``NCCL`` backend supports this API. Other backends will raise an error.\n+    users of this API must gurantee that all ranks in the parent group enter this API call,\n+    and the split of the sub groups is the same accross all ranks in the parent group.\n+\n+    Args:\n+        parent_pg (ProcessGroup, optional): The parent process group. If None,\n+            the default process group will be used. Users need to gurantee that\n+            the parent group is fully initialized (e.g, communicators are initialized)\n+        split_ranks (list[list[int]]): the split ranks, which is a list of list of ranks.\n+            Users need to make sure the validity of the split ranks such that one\n+            split (represented by one inner list of ints) does not overlap with any other split.\n+            Note that the ranks in each split is the group rank (instead of global rank)\n+            in the parent pg. For example, if the parent group has 4 ranks, and split_ranks can be\n+            [[0, 1], [2, 3]]. Note [[0,1]] is also a valid split, in which case ranks 2, 3 would\n+            return a non-group member.\n+        timeout (timedelta, optional): see `init_process_group` for details and default value.\n+        pg_options (ProcessGroupNCCLOptions, optional): process group options\n+            specifying what additional options need to be passed in during\n+            the construction of specific process groups. i.e.``is_high_priority_stream``\n+            can be specified so that process group can pick up high priority cuda streams.\n+        group_desc (str, optional): a string to describe the process group.\n+\n+    Returns:\n+        ProcessGroup if the current rank is within one split/subgroup given by split_ranks,\n+        or GroupMember.NON_GROUP_MEMBER if the current rank is not part of any split_ranks`.\n+\n+    \"\"\"\n+    # check inputs\n+    if split_ranks is None:\n+        raise ValueError(\"split_ranks cannot be None\")\n+\n+    if pg_options is not None:\n+        assert isinstance(\n+            pg_options, ProcessGroupNCCL.Options\n+        ), \"Expected pg_options argument to be of type ProcessGroupNCCL.Options\"\n+    else:\n+        # default pg_options for NCCL\n+        pg_options = ProcessGroupNCCL.Options()\n+        pg_options.is_high_priority_stream = False\n+\n+    group_desc = \"undefined\" if group_desc is None else group_desc\n+\n+    global _world\n+    default_pg = _get_default_group()\n+    device_id = default_pg.bound_device_id\n+    if not device_id:\n+        raise RuntimeError(\n+            \"No device associated with the default pg, not safe to split any process groups\"\n+        )\n+    default_backend, default_store = _world.pg_map[default_pg]\n+    global_rank = default_pg.rank()\n+    global_world_size = default_pg.size()\n+\n+    if not parent_pg:\n+        parent_pg = default_pg\n+    if parent_pg not in _world.pg_group_ranks:\n+        raise ValueError(f\"Group {parent_pg} is not registered\")\n+\n+    parent_global_to_group_ranks = _world.pg_group_ranks[parent_pg]\n+    parent_group_to_global_ranks = {\n+        group_rank: global_rank\n+        for global_rank, group_rank in parent_global_to_group_ranks.items()\n+    }\n+\n+    if global_rank not in parent_global_to_group_ranks:\n+        raise ValueError(\n+            f\"Global rank {global_rank} is not part of the parent group {parent_pg}\"\n+        )\n+\n+    parent_group_rank = parent_global_to_group_ranks[global_rank]\n+    parent_backend = parent_pg._get_backend(torch.device(\"cuda\"))\n+\n+    # if the parent backend does not support splitting, raise error\n+    # currently this API only support NCCL backend\n+    if (\n+        not parent_backend\n+        or not parent_backend.supports_splitting\n+        or not isinstance(parent_backend, ProcessGroupNCCL)\n+    ):\n+        raise RuntimeError(\n+            \"No backend for the parent process group or its backend does not support splitting\"\n+        )\n+\n+    parent_backend_str, _ = _world.pg_map[parent_pg]\n+    # same type of backend as the parent process group\n+    backend = Backend(parent_backend_str)\n+    backend_config = BackendConfig(backend)\n+\n+    # this timeout defaulting/validation is used for all the new_groups/new_subgroups variants,\n+    # which may just pass their timeout value (or None)\n+    if timeout is None:\n+        timeout = _get_default_timeout(backend)\n+    _check_valid_timeout(timeout)\n+\n+    # find my group of ranks and my group local rank in split_ranks\n+    my_group = None\n+    group_rank = -1\n+\n+    for split_group in split_ranks:\n+        if len(split_group) == 0:\n+            raise ValueError(\"the split group cannot be empty\")\n+        if len(split_group) > global_world_size:\n+            raise ValueError(\n+                \"the split group's size should be less or equal to the world_size set by init_process_group\"\n+            )\n+        if len(split_group) != len(set(split_group)):\n+            raise ValueError(\"the split group cannot have duplicate ranks\")\n+        split_group = sorted(split_group)\n+        if parent_group_rank in split_group:\n+            my_group = split_group\n+            group_rank = split_group.index(parent_group_rank)\n+            break\n+    # if my rank does not belong to any sub group or my rank is the only member in the subgroup,\n+    # no_color split should be called\n+    if my_group is None or group_rank == -1 or len(my_group) == 1:\n+        parent_backend.perform_nocolor_split(device_id)\n+        return GroupMember.NON_GROUP_MEMBER\n+\n+    group_name = _process_group_name(my_group, use_hashed_name=False)\n+    global_ranks_in_my_group = [parent_group_to_global_ranks[rank] for rank in my_group]\n+\n+    prefix_store = PrefixStore(f\"{group_name}/\", default_store)\n+    base_pg_options = ProcessGroup.Options(backend=str(backend))\n+    base_pg_options._timeout = timeout\n+    pg: ProcessGroup = ProcessGroup(\n+        prefix_store, group_rank, len(my_group), base_pg_options\n+    )\n+    pg.bound_device_id = device_id\n+\n+    pg_options._timeout = timeout\n+    pg_options.split_from = parent_backend\n+    pg_options.split_color = _process_group_color(my_group)\n+    pg_options.global_ranks_in_group = global_ranks_in_my_group\n+    pg_options.group_name = group_name\n+    backend_class = ProcessGroupNCCL(\n+        prefix_store, group_rank, len(my_group), pg_options\n+    )\n+    backend_type = ProcessGroup.BackendType.NCCL\n+    backend_class._set_sequence_number_for_group()\n+\n+    pg._register_backend(torch.device(\"cuda\"), backend_type, backend_class)\n+\n+    # set group_name and group_dsec to backend\n+    assert group_name is not None\n+    assert group_desc is not None\n+    pg._set_group_name(group_name)\n+    pg._set_group_desc(group_desc)\n+\n+    # always eagerly initialize the backend in split_group\n+    eager_backend = pg._get_backend(device_id)\n+    eager_backend.eager_connect_single_device(device_id)\n+\n+    # update global state\n+    _world.pg_map[pg] = (backend, prefix_store)\n+    _world.pg_names[pg] = group_name\n+    _register_process_group(group_name, pg)\n+    _world.pg_backend_config[pg] = str(backend_config)\n+    pg_tag = f\"ptd:{group_name}\"\n+    _world.tags_to_pg.setdefault(\"\", []).append(pg)\n+    _world.tags_to_pg.setdefault(pg_tag, []).append(pg)", "path": "torch/distributed/distributed_c10d.py", "commit_id": "3b376336469eda235814aee75ac079befb5367fc", "original_commit_id": "f9b1f21870962f948972cb8c1b28018162c3f1c6", "user": {"login": "shuqiangzhang", "id": 4711970, "url": "https://api.github.com/users/shuqiangzhang", "html_url": "https://github.com/shuqiangzhang"}, "body": "This is basically copying the same logic of _new_process_group_helper when no tag is provided by users. But I guess, we just need to keep the later", "created_at": "2024-07-12T17:14:52Z", "updated_at": "2024-07-12T17:14:52Z", "html_url": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1676222007", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130507", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676222007"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1676222007"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130507"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676222007/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": 4470, "start_side": "RIGHT", "line": null, "original_line": 4471, "side": "RIGHT", "in_reply_to_id": 1675007943, "original_position": 182, "position": null, "subject_type": "line", "PR": {"title": "[c10d] a new Pytorch API (split_group) to create a process group", "number": 130507, "id": 1963471316}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676227687", "pull_request_review_id": 2175380964, "id": 1676227687, "node_id": "PRRC_kwDOA-j9z85j6TRn", "diff_hunk": "@@ -4297,6 +4298,188 @@ def _get_backend_from_str(backend: Optional[str] = None) -> Backend:\n     return Backend(backend)\n \n \n+@_time_logger\n+def split_group(\n+    parent_pg=None,\n+    split_ranks=None,\n+    timeout=None,\n+    pg_options=None,\n+    group_desc=None,\n+):\n+    \"\"\"\n+    Create a new process group splitted from the given parent process group.\n+\n+    warning:: only the ``NCCL`` backend supports this API. Other backends will raise an error.\n+    users of this API must gurantee that all ranks in the parent group enter this API call,\n+    and the split of the sub groups is the same accross all ranks in the parent group.\n+\n+    Args:\n+        parent_pg (ProcessGroup, optional): The parent process group. If None,\n+            the default process group will be used. Users need to gurantee that\n+            the parent group is fully initialized (e.g, communicators are initialized)\n+        split_ranks (list[list[int]]): the split ranks, which is a list of list of ranks.\n+            Users need to make sure the validity of the split ranks such that one\n+            split (represented by one inner list of ints) does not overlap with any other split.\n+            Note that the ranks in each split is the group rank (instead of global rank)\n+            in the parent pg. For example, if the parent group has 4 ranks, and split_ranks can be\n+            [[0, 1], [2, 3]]. Note [[0,1]] is also a valid split, in which case ranks 2, 3 would\n+            return a non-group member.\n+        timeout (timedelta, optional): see `init_process_group` for details and default value.\n+        pg_options (ProcessGroupNCCLOptions, optional): process group options\n+            specifying what additional options need to be passed in during\n+            the construction of specific process groups. i.e.``is_high_priority_stream``\n+            can be specified so that process group can pick up high priority cuda streams.\n+        group_desc (str, optional): a string to describe the process group.\n+\n+    Returns:\n+        ProcessGroup if the current rank is within one split/subgroup given by split_ranks,\n+        or GroupMember.NON_GROUP_MEMBER if the current rank is not part of any split_ranks`.\n+\n+    \"\"\"\n+    # check inputs\n+    if split_ranks is None:\n+        raise ValueError(\"split_ranks cannot be None\")\n+\n+    if pg_options is not None:\n+        assert isinstance(\n+            pg_options, ProcessGroupNCCL.Options\n+        ), \"Expected pg_options argument to be of type ProcessGroupNCCL.Options\"\n+    else:\n+        # default pg_options for NCCL\n+        pg_options = ProcessGroupNCCL.Options()\n+        pg_options.is_high_priority_stream = False\n+\n+    group_desc = \"undefined\" if group_desc is None else group_desc\n+\n+    global _world\n+    default_pg = _get_default_group()\n+    device_id = default_pg.bound_device_id\n+    if not device_id:\n+        raise RuntimeError(\n+            \"No device associated with the default pg, not safe to split any process groups\"\n+        )\n+    default_backend, default_store = _world.pg_map[default_pg]\n+    global_rank = default_pg.rank()\n+    global_world_size = default_pg.size()\n+\n+    if not parent_pg:\n+        parent_pg = default_pg\n+    if parent_pg not in _world.pg_group_ranks:\n+        raise ValueError(f\"Group {parent_pg} is not registered\")\n+\n+    parent_global_to_group_ranks = _world.pg_group_ranks[parent_pg]\n+    parent_group_to_global_ranks = {\n+        group_rank: global_rank\n+        for global_rank, group_rank in parent_global_to_group_ranks.items()\n+    }\n+\n+    if global_rank not in parent_global_to_group_ranks:\n+        raise ValueError(\n+            f\"Global rank {global_rank} is not part of the parent group {parent_pg}\"\n+        )\n+\n+    parent_group_rank = parent_global_to_group_ranks[global_rank]\n+    parent_backend = parent_pg._get_backend(torch.device(\"cuda\"))\n+\n+    # if the parent backend does not support splitting, raise error\n+    # currently this API only support NCCL backend\n+    if (\n+        not parent_backend\n+        or not parent_backend.supports_splitting\n+        or not isinstance(parent_backend, ProcessGroupNCCL)\n+    ):\n+        raise RuntimeError(\n+            \"No backend for the parent process group or its backend does not support splitting\"\n+        )\n+\n+    parent_backend_str, _ = _world.pg_map[parent_pg]\n+    # same type of backend as the parent process group\n+    backend = Backend(parent_backend_str)\n+    backend_config = BackendConfig(backend)\n+\n+    # this timeout defaulting/validation is used for all the new_groups/new_subgroups variants,\n+    # which may just pass their timeout value (or None)\n+    if timeout is None:\n+        timeout = _get_default_timeout(backend)\n+    _check_valid_timeout(timeout)\n+\n+    # find my group of ranks and my group local rank in split_ranks\n+    my_group = None\n+    group_rank = -1\n+\n+    for split_group in split_ranks:\n+        if len(split_group) == 0:\n+            raise ValueError(\"the split group cannot be empty\")\n+        if len(split_group) > global_world_size:\n+            raise ValueError(\n+                \"the split group's size should be less or equal to the world_size set by init_process_group\"\n+            )\n+        if len(split_group) != len(set(split_group)):\n+            raise ValueError(\"the split group cannot have duplicate ranks\")\n+        split_group = sorted(split_group)\n+        if parent_group_rank in split_group:\n+            my_group = split_group\n+            group_rank = split_group.index(parent_group_rank)\n+            break\n+    # if my rank does not belong to any sub group or my rank is the only member in the subgroup,\n+    # no_color split should be called\n+    if my_group is None or group_rank == -1 or len(my_group) == 1:\n+        parent_backend.perform_nocolor_split(device_id)\n+        return GroupMember.NON_GROUP_MEMBER\n+\n+    group_name = _process_group_name(my_group, use_hashed_name=False)\n+    global_ranks_in_my_group = [parent_group_to_global_ranks[rank] for rank in my_group]\n+\n+    prefix_store = PrefixStore(f\"{group_name}/\", default_store)\n+    base_pg_options = ProcessGroup.Options(backend=str(backend))\n+    base_pg_options._timeout = timeout", "path": "torch/distributed/distributed_c10d.py", "commit_id": "3b376336469eda235814aee75ac079befb5367fc", "original_commit_id": "f9b1f21870962f948972cb8c1b28018162c3f1c6", "user": {"login": "shuqiangzhang", "id": 4711970, "url": "https://api.github.com/users/shuqiangzhang", "html_url": "https://github.com/shuqiangzhang"}, "body": "This options is just a placeholder to create the base pg I think, the real option is pg_options", "created_at": "2024-07-12T17:20:45Z", "updated_at": "2024-07-12T17:20:45Z", "html_url": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1676227687", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130507", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676227687"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1676227687"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130507"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676227687/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 4450, "original_line": 4435, "side": "RIGHT", "in_reply_to_id": 1674862782, "original_position": 146, "position": 161, "subject_type": "line", "PR": {"title": "[c10d] a new Pytorch API (split_group) to create a process group", "number": 130507, "id": 1963471316}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676261941", "pull_request_review_id": 2175429144, "id": 1676261941, "node_id": "PRRC_kwDOA-j9z85j6bo1", "diff_hunk": "@@ -4297,6 +4298,188 @@ def _get_backend_from_str(backend: Optional[str] = None) -> Backend:\n     return Backend(backend)\n \n \n+@_time_logger\n+def split_group(\n+    parent_pg=None,\n+    split_ranks=None,\n+    timeout=None,\n+    pg_options=None,\n+    group_desc=None,\n+):\n+    \"\"\"\n+    Create a new process group splitted from the given parent process group.\n+\n+    warning:: only the ``NCCL`` backend supports this API. Other backends will raise an error.\n+    users of this API must gurantee that all ranks in the parent group enter this API call,\n+    and the split of the sub groups is the same accross all ranks in the parent group.\n+\n+    Args:\n+        parent_pg (ProcessGroup, optional): The parent process group. If None,\n+            the default process group will be used. Users need to gurantee that\n+            the parent group is fully initialized (e.g, communicators are initialized)\n+        split_ranks (list[list[int]]): the split ranks, which is a list of list of ranks.\n+            Users need to make sure the validity of the split ranks such that one\n+            split (represented by one inner list of ints) does not overlap with any other split.\n+            Note that the ranks in each split is the group rank (instead of global rank)\n+            in the parent pg. For example, if the parent group has 4 ranks, and split_ranks can be\n+            [[0, 1], [2, 3]]. Note [[0,1]] is also a valid split, in which case ranks 2, 3 would\n+            return a non-group member.\n+        timeout (timedelta, optional): see `init_process_group` for details and default value.\n+        pg_options (ProcessGroupNCCLOptions, optional): process group options\n+            specifying what additional options need to be passed in during\n+            the construction of specific process groups. i.e.``is_high_priority_stream``\n+            can be specified so that process group can pick up high priority cuda streams.\n+        group_desc (str, optional): a string to describe the process group.\n+\n+    Returns:\n+        ProcessGroup if the current rank is within one split/subgroup given by split_ranks,\n+        or GroupMember.NON_GROUP_MEMBER if the current rank is not part of any split_ranks`.\n+\n+    \"\"\"\n+    # check inputs\n+    if split_ranks is None:\n+        raise ValueError(\"split_ranks cannot be None\")\n+\n+    if pg_options is not None:\n+        assert isinstance(\n+            pg_options, ProcessGroupNCCL.Options\n+        ), \"Expected pg_options argument to be of type ProcessGroupNCCL.Options\"\n+    else:\n+        # default pg_options for NCCL\n+        pg_options = ProcessGroupNCCL.Options()\n+        pg_options.is_high_priority_stream = False\n+\n+    group_desc = \"undefined\" if group_desc is None else group_desc\n+\n+    global _world\n+    default_pg = _get_default_group()\n+    device_id = default_pg.bound_device_id\n+    if not device_id:", "path": "torch/distributed/distributed_c10d.py", "commit_id": "3b376336469eda235814aee75ac079befb5367fc", "original_commit_id": "f9b1f21870962f948972cb8c1b28018162c3f1c6", "user": {"login": "shuqiangzhang", "id": 4711970, "url": "https://api.github.com/users/shuqiangzhang", "html_url": "https://github.com/shuqiangzhang"}, "body": "Will create a helper func, but I think here just checking device_id is more efficient, as we need to assert use device_id in many other cases, such as get_backend and perform_nocolor_split", "created_at": "2024-07-12T17:45:55Z", "updated_at": "2024-07-12T17:45:55Z", "html_url": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1676261941", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130507", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676261941"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1676261941"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130507"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676261941/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 4357, "original_line": 4357, "side": "RIGHT", "in_reply_to_id": 1674831613, "original_position": 68, "position": 68, "subject_type": "line", "PR": {"title": "[c10d] a new Pytorch API (split_group) to create a process group", "number": 130507, "id": 1963471316}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676283927", "pull_request_review_id": 2175453868, "id": 1676283927, "node_id": "PRRC_kwDOA-j9z85j6hAX", "diff_hunk": "@@ -4228,6 +4251,140 @@ def test_nccl_errors_dump(self):\n             sys.exit(1)\n \n \n+# tests that needs to be run with a larger world size\n+class ProcessGroupNCCLLargerScaleTest(MultiProcessTestCase):\n+    def _create_process_group_nccl(self, store, opts, device_id=None):\n+        # create nccl processgroup with opts\n+        c10d.init_process_group(\n+            \"nccl\",\n+            world_size=self.world_size,\n+            rank=self.rank,\n+            store=store,\n+            pg_options=opts,\n+            device_id=device_id,\n+        )\n+        pg = c10d.distributed_c10d._get_default_group()\n+        return pg\n+\n+    def opts(self, high_priority_stream=False):\n+        opts = c10d.ProcessGroupNCCL.Options()\n+        opts.is_high_priority_stream = high_priority_stream\n+        return opts\n+\n+    def setUp(self):\n+        super().setUp()\n+        # TORCH_NCCL_BLOCKING_WAIT overrides TORCH_NCCL_ASYNC_ERROR_HANDLING hence tests\n+        # that use TORCH_NCCL_BLOCKING_WAIT will test it as expected.\n+        os.environ[\"TORCH_NCCL_ASYNC_ERROR_HANDLING\"] = \"1\"\n+        # self.num_gpus = torch.cuda.device_count()\n+        self._spawn_processes()\n+\n+    def tearDown(self):\n+        super().tearDown()\n+        try:\n+            os.remove(self.file_name)\n+        except OSError:\n+            pass\n+\n+    @property\n+    def world_size(self):\n+        return 8\n+\n+    @property\n+    def rank_to_GPU(self):\n+        # return rank to GPU map\n+        return init_multigpu_helper(self.world_size, \"nccl\")\n+\n+    @requires_nccl_version((2, 18), \"Need NCCL 2.18+ for ncclCommSplit\")\n+    @skip_but_pass_in_sandcastle_if(not TEST_MULTIGPU, \"NCCL test requires 2+ GPUs\")\n+    @skip_if_lt_x_gpu(8)\n+    def test_comm_split_group_larger_scale(self):\n+        store = c10d.FileStore(self.file_name, self.world_size)\n+        device = torch.device(f\"cuda:{self.rank}\")\n+        pg = self._create_process_group_nccl(store, self.opts(), device_id=device)\n+        backend = pg._get_backend(torch.device(device))\n+\n+        tensor = torch.full((1,), self.rank).cuda(device)\n+        ng1 = c10d.split_group(pg, [[0, 1], [2, 3, 4, 5, 6, 7]])\n+        backend1 = ng1._get_backend(torch.device(device))\n+\n+        # comm split happens eagerly since device_id is passed to init_process_group.\n+        self.assertEqual(backend.comm_split_count(), 1)\n+        # dist.broadcast take Source rank on global process group\n+        if self.rank < 2:\n+            dist.broadcast(tensor, 0, group=ng1)\n+            self.assertEqual(tensor, torch.full((1,), 0))\n+        else:\n+            dist.broadcast(tensor, 2, group=ng1)\n+            self.assertEqual(tensor, torch.full((1,), 2))\n+\n+        # test split with only one colored group, other ranks should be no color split.\n+        ng2 = c10d.split_group(pg, [[5, 6, 7]], timeout=timedelta(seconds=50))\n+        self.assertEqual(backend.comm_split_count(), 2)\n+\n+        if self.rank >= 5:\n+            tensor2 = torch.full((1,), self.rank).cuda(device)\n+            dist.broadcast(tensor2, 7, group=ng2)\n+            self.assertEqual(tensor2, torch.full((1,), 7))\n+        else:\n+            self.assertEqual(ng2, c10d.GroupMember.NON_GROUP_MEMBER)\n+        # give enough time for the broadcast to finish before destroying all pgs.\n+        time.sleep(2)", "path": "test/distributed/test_c10d_nccl.py", "commit_id": "3b376336469eda235814aee75ac079befb5367fc", "original_commit_id": "f9b1f21870962f948972cb8c1b28018162c3f1c6", "user": {"login": "wconstab", "id": 4984825, "url": "https://api.github.com/users/wconstab", "html_url": "https://github.com/wconstab"}, "body": "No, it works as expected but you forgot the cuda synchronize().\n\nWork.wait() syncs one cuda stream used for comms back to the default cuda stream, it does Not sync to the CPU.", "created_at": "2024-07-12T17:53:21Z", "updated_at": "2024-07-12T17:53:21Z", "html_url": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1676283927", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130507", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676283927"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130507#discussion_r1676283927"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130507"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676283927/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": null, "original_line": 4332, "side": "RIGHT", "in_reply_to_id": 1674878076, "original_position": 112, "position": null, "subject_type": "line", "PR": {"title": "[c10d] a new Pytorch API (split_group) to create a process group", "number": 130507, "id": 1963471316}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676323055", "pull_request_review_id": 2175513036, "id": 1676323055, "node_id": "PRRC_kwDOA-j9z85j6qjv", "diff_hunk": "@@ -1538,6 +1538,15 @@ def num_fw_fixed_arguments(dynamo_gm_num_inputs: int, aot_fw_gm_num_inputs: int)\n     num_rng_seed_offset_inputs = (\n         2 if torch._functorch.config.functionalize_rng_ops else 0\n     )\n+    # AOT won't lift any parameters if we're inlining NN Modules\n+    # however desugaring subclasses will still add arguments\n+    # resulted in extra fixed inputs https://github.com/pytorch/pytorch/issues/130502\n+    if (\n+        torch._dynamo.config.inline_inbuilt_nn_modules\n+        and not torch._dynamo.utils.is_parameter_freezing()\n+    ):\n+        return 0\n+\n     return aot_fw_gm_num_inputs - dynamo_gm_num_inputs - num_rng_seed_offset_inputs", "path": "torch/_inductor/utils.py", "commit_id": "df963020f11831f47748ece404019074ddbb8a9b", "original_commit_id": "df963020f11831f47748ece404019074ddbb8a9b", "user": {"login": "bdhirsh", "id": 16311747, "url": "https://api.github.com/users/bdhirsh", "html_url": "https://github.com/bdhirsh"}, "body": "Not directly related to this PR but @angelayi effect tokens will cause AOTDispatcher to add extra arguments in the AOT graph that were not in the dynamo graph. We probably need to account for them here? (otherwise inductor will think that there are extra graphargs that have static addresses)", "created_at": "2024-07-12T18:30:16Z", "updated_at": "2024-07-12T18:30:17Z", "html_url": "https://github.com/pytorch/pytorch/pull/130503#discussion_r1676323055", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130503", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676323055"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130503#discussion_r1676323055"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130503"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676323055/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 1550, "original_line": 1550, "side": "RIGHT", "original_position": 13, "position": 13, "subject_type": "line", "PR": {"title": "Remove static param counting if inlining NN modules", "number": 130503, "id": 1963385573}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676324871", "pull_request_review_id": 2175515721, "id": 1676324871, "node_id": "PRRC_kwDOA-j9z85j6rAH", "diff_hunk": "@@ -1538,6 +1538,15 @@ def num_fw_fixed_arguments(dynamo_gm_num_inputs: int, aot_fw_gm_num_inputs: int)\n     num_rng_seed_offset_inputs = (\n         2 if torch._functorch.config.functionalize_rng_ops else 0\n     )\n+    # AOT won't lift any parameters if we're inlining NN Modules\n+    # however desugaring subclasses will still add arguments\n+    # resulted in extra fixed inputs https://github.com/pytorch/pytorch/issues/130502\n+    if (\n+        torch._dynamo.config.inline_inbuilt_nn_modules\n+        and not torch._dynamo.utils.is_parameter_freezing()", "path": "torch/_inductor/utils.py", "commit_id": "df963020f11831f47748ece404019074ddbb8a9b", "original_commit_id": "df963020f11831f47748ece404019074ddbb8a9b", "user": {"login": "bdhirsh", "id": 16311747, "url": "https://api.github.com/users/bdhirsh", "html_url": "https://github.com/bdhirsh"}, "body": "hmm maybe i'm missing something - but with your PR above this one (plumbing `fw_metadata. static_input_indices` through to inductor), why do we still need this function?", "created_at": "2024-07-12T18:32:12Z", "updated_at": "2024-07-12T18:32:12Z", "html_url": "https://github.com/pytorch/pytorch/pull/130503#discussion_r1676324871", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130503", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676324871"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130503#discussion_r1676324871"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130503"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676324871/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 1546, "original_line": 1546, "side": "RIGHT", "original_position": 9, "position": 9, "subject_type": "line", "PR": {"title": "Remove static param counting if inlining NN modules", "number": 130503, "id": 1963385573}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676330051", "pull_request_review_id": 2175523695, "id": 1676330051, "node_id": "PRRC_kwDOA-j9z85j6sRD", "diff_hunk": "@@ -1538,6 +1538,15 @@ def num_fw_fixed_arguments(dynamo_gm_num_inputs: int, aot_fw_gm_num_inputs: int)\n     num_rng_seed_offset_inputs = (\n         2 if torch._functorch.config.functionalize_rng_ops else 0\n     )\n+    # AOT won't lift any parameters if we're inlining NN Modules\n+    # however desugaring subclasses will still add arguments\n+    # resulted in extra fixed inputs https://github.com/pytorch/pytorch/issues/130502\n+    if (\n+        torch._dynamo.config.inline_inbuilt_nn_modules\n+        and not torch._dynamo.utils.is_parameter_freezing()", "path": "torch/_inductor/utils.py", "commit_id": "df963020f11831f47748ece404019074ddbb8a9b", "original_commit_id": "df963020f11831f47748ece404019074ddbb8a9b", "user": {"login": "mlazos", "id": 4105940, "url": "https://api.github.com/users/mlazos", "html_url": "https://github.com/mlazos"}, "body": "if we're parameter freezing we still lift some params as constants :(, and we don't inline for export either", "created_at": "2024-07-12T18:37:58Z", "updated_at": "2024-07-12T18:37:58Z", "html_url": "https://github.com/pytorch/pytorch/pull/130503#discussion_r1676330051", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130503", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676330051"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130503#discussion_r1676330051"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130503"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676330051/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 1546, "original_line": 1546, "side": "RIGHT", "in_reply_to_id": 1676324871, "original_position": 9, "position": 9, "subject_type": "line", "PR": {"title": "Remove static param counting if inlining NN modules", "number": 130503, "id": 1963385573}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676338003", "pull_request_review_id": 2175536255, "id": 1676338003, "node_id": "PRRC_kwDOA-j9z85j6uNT", "diff_hunk": "@@ -1538,6 +1538,15 @@ def num_fw_fixed_arguments(dynamo_gm_num_inputs: int, aot_fw_gm_num_inputs: int)\n     num_rng_seed_offset_inputs = (\n         2 if torch._functorch.config.functionalize_rng_ops else 0\n     )\n+    # AOT won't lift any parameters if we're inlining NN Modules\n+    # however desugaring subclasses will still add arguments\n+    # resulted in extra fixed inputs https://github.com/pytorch/pytorch/issues/130502\n+    if (\n+        torch._dynamo.config.inline_inbuilt_nn_modules\n+        and not torch._dynamo.utils.is_parameter_freezing()\n+    ):\n+        return 0\n+\n     return aot_fw_gm_num_inputs - dynamo_gm_num_inputs - num_rng_seed_offset_inputs", "path": "torch/_inductor/utils.py", "commit_id": "df963020f11831f47748ece404019074ddbb8a9b", "original_commit_id": "df963020f11831f47748ece404019074ddbb8a9b", "user": {"login": "angelayi", "id": 10901756, "url": "https://api.github.com/users/angelayi", "html_url": "https://github.com/angelayi"}, "body": "If the backend is inductor, AOTDispatcher will not add extra arguments to the AOT graph. Instead it will insert a `_make_dep_token` and `_sink_tokens` call in the graph (more specific graph pastes in https://github.com/pytorch/pytorch/pull/122347).", "created_at": "2024-07-12T18:46:38Z", "updated_at": "2024-07-12T18:46:46Z", "html_url": "https://github.com/pytorch/pytorch/pull/130503#discussion_r1676338003", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130503", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676338003"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130503#discussion_r1676338003"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130503"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1676338003/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 1550, "original_line": 1550, "side": "RIGHT", "in_reply_to_id": 1676323055, "original_position": 13, "position": 13, "subject_type": "line", "PR": {"title": "Remove static param counting if inlining NN modules", "number": 130503, "id": 1963385573}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678089077", "pull_request_review_id": 2178157307, "id": 1678089077, "node_id": "PRRC_kwDOA-j9z85kBZt1", "diff_hunk": "@@ -1538,6 +1538,15 @@ def num_fw_fixed_arguments(dynamo_gm_num_inputs: int, aot_fw_gm_num_inputs: int)\n     num_rng_seed_offset_inputs = (\n         2 if torch._functorch.config.functionalize_rng_ops else 0\n     )\n+    # AOT won't lift any parameters if we're inlining NN Modules\n+    # however desugaring subclasses will still add arguments\n+    # resulted in extra fixed inputs https://github.com/pytorch/pytorch/issues/130502\n+    if (\n+        torch._dynamo.config.inline_inbuilt_nn_modules\n+        and not torch._dynamo.utils.is_parameter_freezing()\n+    ):\n+        return 0\n+\n     return aot_fw_gm_num_inputs - dynamo_gm_num_inputs - num_rng_seed_offset_inputs", "path": "torch/_inductor/utils.py", "commit_id": "df963020f11831f47748ece404019074ddbb8a9b", "original_commit_id": "df963020f11831f47748ece404019074ddbb8a9b", "user": {"login": "bdhirsh", "id": 16311747, "url": "https://api.github.com/users/bdhirsh", "html_url": "https://github.com/bdhirsh"}, "body": "ahhh right thank you!", "created_at": "2024-07-15T16:16:18Z", "updated_at": "2024-07-15T16:16:18Z", "html_url": "https://github.com/pytorch/pytorch/pull/130503#discussion_r1678089077", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130503", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678089077"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130503#discussion_r1678089077"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130503"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678089077/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 1550, "original_line": 1550, "side": "RIGHT", "in_reply_to_id": 1676323055, "original_position": 13, "position": 13, "subject_type": "line", "PR": {"title": "Remove static param counting if inlining NN modules", "number": 130503, "id": 1963385573}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678297833", "pull_request_review_id": 2178493664, "id": 1678297833, "node_id": "PRRC_kwDOA-j9z85kCMrp", "diff_hunk": "@@ -1538,6 +1538,15 @@ def num_fw_fixed_arguments(dynamo_gm_num_inputs: int, aot_fw_gm_num_inputs: int)\n     num_rng_seed_offset_inputs = (\n         2 if torch._functorch.config.functionalize_rng_ops else 0\n     )\n+    # AOT won't lift any parameters if we're inlining NN Modules\n+    # however desugaring subclasses will still add arguments\n+    # resulted in extra fixed inputs https://github.com/pytorch/pytorch/issues/130502\n+    if (\n+        torch._dynamo.config.inline_inbuilt_nn_modules\n+        and not torch._dynamo.utils.is_parameter_freezing()", "path": "torch/_inductor/utils.py", "commit_id": "df963020f11831f47748ece404019074ddbb8a9b", "original_commit_id": "df963020f11831f47748ece404019074ddbb8a9b", "user": {"login": "mlazos", "id": 4105940, "url": "https://api.github.com/users/mlazos", "html_url": "https://github.com/mlazos"}, "body": "After some offline discussion, I think we will be able to remove this function once inlining is on by default.", "created_at": "2024-07-15T19:31:02Z", "updated_at": "2024-07-15T19:31:02Z", "html_url": "https://github.com/pytorch/pytorch/pull/130503#discussion_r1678297833", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130503", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678297833"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130503#discussion_r1678297833"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130503"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678297833/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 1546, "original_line": 1546, "side": "RIGHT", "in_reply_to_id": 1676324871, "original_position": 9, "position": 9, "subject_type": "line", "PR": {"title": "Remove static param counting if inlining NN modules", "number": 130503, "id": 1963385573}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1673261347", "pull_request_review_id": 2170682601, "id": 1673261347, "node_id": "PRRC_kwDOA-j9z85ju_Ej", "diff_hunk": "@@ -254,6 +255,9 @@ def __init__(\n                     self._get_or_create_default_group()\n                     self._init_process_groups()\n \n+                if get_backend() == \"threaded\":", "path": "torch/distributed/device_mesh.py", "commit_id": "0c3a929af92dd992dea02b76950dd0bdb029adbb", "original_commit_id": "0c3a929af92dd992dea02b76950dd0bdb029adbb", "user": {"login": "wz337", "id": 31293777, "url": "https://api.github.com/users/wz337", "html_url": "https://github.com/wz337"}, "body": "Moving this check out of `if _init_backend:` check since, even if _init_backend()==False, default backend would be initialized at this point. \r\n\r\nWithout this, the DeviceMesh from group tests (MTPG) would fail, because _init_backend==False. ", "created_at": "2024-07-11T01:08:25Z", "updated_at": "2024-07-11T01:08:25Z", "html_url": "https://github.com/pytorch/pytorch/pull/130495#discussion_r1673261347", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130495", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1673261347"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130495#discussion_r1673261347"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130495"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1673261347/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": null, "start_side": null, "line": 258, "original_line": 258, "side": "RIGHT", "original_position": 21, "position": 21, "subject_type": "line", "PR": {"title": "[DeviceMesh] Only include the real thread_id in DeviceMesh hash under threaded backend", "number": 130495, "id": 1963340645}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674039994", "pull_request_review_id": 2171941475, "id": 1674039994, "node_id": "PRRC_kwDOA-j9z85jx9K6", "diff_hunk": "@@ -4793,6 +4794,20 @@ def f(x, gy):\n \n         self.vmap_outplace_test(f, (x, gy), {}, in_dims=(None, 0))\n \n+    @onlyCUDA\n+    @parametrize(\"inplace\", [True, False])\n+    def test_0d_tensor_index_put(self, device, inplace):\n+        def f(t, idx, v):\n+            fn = torch.index_put_ if inplace else torch.index_put\n+            return fn(t, idx, v)\n+\n+        t = torch.zeros((1, 5), device=\"cuda\")\n+        idx = torch.tensor([1, 3])", "path": "test/functorch/test_vmap.py", "commit_id": "668990b001efe53d4ff4319960113df25424987e", "original_commit_id": "053c92bb92dcde2bb67d5401ae3522933237cfd5", "user": {"login": "zou3519", "id": 5652049, "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519"}, "body": "t should have vmap dimension > 1 so we test more", "created_at": "2024-07-11T13:41:26Z", "updated_at": "2024-07-11T13:41:34Z", "html_url": "https://github.com/pytorch/pytorch/pull/130479#discussion_r1674039994", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130479", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674039994"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130479#discussion_r1674039994"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130479"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674039994/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": 4804, "start_side": "RIGHT", "line": null, "original_line": 4805, "side": "RIGHT", "original_position": 20, "position": null, "subject_type": "line", "PR": {"title": "Ensure tensors devices match on `torch.index_put` batch rule impl", "number": 130479, "id": 1963131139}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674075233", "pull_request_review_id": 2172002053, "id": 1674075233, "node_id": "PRRC_kwDOA-j9z85jyFxh", "diff_hunk": "@@ -4793,6 +4794,20 @@ def f(x, gy):\n \n         self.vmap_outplace_test(f, (x, gy), {}, in_dims=(None, 0))\n \n+    @onlyCUDA\n+    @parametrize(\"inplace\", [True, False])\n+    def test_0d_tensor_index_put(self, device, inplace):\n+        def f(t, idx, v):\n+            fn = torch.index_put_ if inplace else torch.index_put\n+            return fn(t, idx, v)\n+\n+        t = torch.zeros((1, 5), device=\"cuda\")\n+        idx = torch.tensor([1, 3])\n+        v = torch.tensor(1, dtype=t.dtype, device=\"cpu\")\n+\n+        expected = torch.tensor([[0, 1, 0, 1, 0]], dtype=t.dtype)\n+        self.assertEqual(expected, vmap(f, in_dims=(0, None, None))(t, (idx,), v))", "path": "test/functorch/test_vmap.py", "commit_id": "668990b001efe53d4ff4319960113df25424987e", "original_commit_id": "053c92bb92dcde2bb67d5401ae3522933237cfd5", "user": {"login": "zou3519", "id": 5652049, "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519"}, "body": "Can you also test the \"batched\" v case? \r\n```\r\nt = torch.zeros((N, 5), device=\"cuda\")\r\nidx = torch.tensor([1, 3])\r\nv = torch.tensor([3.15, 2.71])\r\n\r\n# if t[0].index_put_(idx, v[0]) works, then vmap(Tensor.index_put_, (0, None, 0))(t, idx, v) should also work.\r\n```", "created_at": "2024-07-11T14:03:34Z", "updated_at": "2024-07-11T14:03:34Z", "html_url": "https://github.com/pytorch/pytorch/pull/130479#discussion_r1674075233", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130479", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674075233"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130479#discussion_r1674075233"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130479"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674075233/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": 4806, "start_side": "RIGHT", "line": null, "original_line": 4809, "side": "RIGHT", "original_position": 24, "position": null, "subject_type": "line", "PR": {"title": "Ensure tensors devices match on `torch.index_put` batch rule impl", "number": 130479, "id": 1963131139}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674400997", "pull_request_review_id": 2172551968, "id": 1674400997, "node_id": "PRRC_kwDOA-j9z85jzVTl", "diff_hunk": "@@ -4793,6 +4794,20 @@ def f(x, gy):\n \n         self.vmap_outplace_test(f, (x, gy), {}, in_dims=(None, 0))\n \n+    @onlyCUDA\n+    @parametrize(\"inplace\", [True, False])\n+    def test_0d_tensor_index_put(self, device, inplace):\n+        def f(t, idx, v):\n+            fn = torch.index_put_ if inplace else torch.index_put\n+            return fn(t, idx, v)\n+\n+        t = torch.zeros((1, 5), device=\"cuda\")\n+        idx = torch.tensor([1, 3])\n+        v = torch.tensor(1, dtype=t.dtype, device=\"cpu\")\n+\n+        expected = torch.tensor([[0, 1, 0, 1, 0]], dtype=t.dtype)\n+        self.assertEqual(expected, vmap(f, in_dims=(0, None, None))(t, (idx,), v))", "path": "test/functorch/test_vmap.py", "commit_id": "668990b001efe53d4ff4319960113df25424987e", "original_commit_id": "053c92bb92dcde2bb67d5401ae3522933237cfd5", "user": {"login": "guilhermeleobas", "id": 2712115, "url": "https://api.github.com/users/guilhermeleobas", "html_url": "https://github.com/guilhermeleobas"}, "body": "`v` stops being a 0d tensor if we batch it.", "created_at": "2024-07-11T17:28:55Z", "updated_at": "2024-07-11T17:28:55Z", "html_url": "https://github.com/pytorch/pytorch/pull/130479#discussion_r1674400997", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130479", "author_association": "COLLABORATOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674400997"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130479#discussion_r1674400997"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130479"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674400997/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": 4806, "start_side": "RIGHT", "line": null, "original_line": 4809, "side": "RIGHT", "in_reply_to_id": 1674075233, "original_position": 24, "position": null, "subject_type": "line", "PR": {"title": "Ensure tensors devices match on `torch.index_put` batch rule impl", "number": 130479, "id": 1963131139}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674401948", "pull_request_review_id": 2172553663, "id": 1674401948, "node_id": "PRRC_kwDOA-j9z85jzVic", "diff_hunk": "@@ -4793,6 +4794,20 @@ def f(x, gy):\n \n         self.vmap_outplace_test(f, (x, gy), {}, in_dims=(None, 0))\n \n+    @onlyCUDA\n+    @parametrize(\"inplace\", [True, False])\n+    def test_0d_tensor_index_put(self, device, inplace):\n+        def f(t, idx, v):\n+            fn = torch.index_put_ if inplace else torch.index_put\n+            return fn(t, idx, v)\n+\n+        t = torch.zeros((1, 5), device=\"cuda\")\n+        idx = torch.tensor([1, 3])\n+        v = torch.tensor(1, dtype=t.dtype, device=\"cpu\")\n+\n+        expected = torch.tensor([[0, 1, 0, 1, 0]], dtype=t.dtype)\n+        self.assertEqual(expected, vmap(f, in_dims=(0, None, None))(t, (idx,), v))", "path": "test/functorch/test_vmap.py", "commit_id": "668990b001efe53d4ff4319960113df25424987e", "original_commit_id": "053c92bb92dcde2bb67d5401ae3522933237cfd5", "user": {"login": "guilhermeleobas", "id": 2712115, "url": "https://api.github.com/users/guilhermeleobas", "html_url": "https://github.com/guilhermeleobas"}, "body": "```python\r\n  File \"/home/guilhermeleobas/git/pytorch/test/functorch/test_vmap.py\", line 4814, in test_0d_tensor_index_put\r\n    self.assertEqual(expected, vmap(f, in_dims=(0, None, 0))(t, (idx,), v))\r\n  File \"/home/guilhermeleobas/git/pytorch/torch/_functorch/apis.py\", line 201, in wrapped\r\n    return vmap_impl(\r\n  File \"/home/guilhermeleobas/git/pytorch/torch/_functorch/vmap.py\", line 331, in vmap_impl\r\n    return _flat_vmap(\r\n  File \"/home/guilhermeleobas/git/pytorch/torch/_functorch/vmap.py\", line 48, in fn\r\n    return f(*args, **kwargs)\r\n  File \"/home/guilhermeleobas/git/pytorch/torch/_functorch/vmap.py\", line 480, in _flat_vmap\r\n    batched_outputs = func(*batched_inputs, **kwargs)\r\n  File \"/home/guilhermeleobas/git/pytorch/test/functorch/test_vmap.py\", line 4802, in f\r\n    return fn(t, idx, v)\r\nRuntimeError: batched == nullptr INTERNAL ASSERT FAILED at \"/home/guilhermeleobas/git/pytorch/aten/src/ATen/functorch/Interpreter.cpp\":99, please report a bug to PyTorch.\r\n```", "created_at": "2024-07-11T17:29:54Z", "updated_at": "2024-07-11T17:29:54Z", "html_url": "https://github.com/pytorch/pytorch/pull/130479#discussion_r1674401948", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130479", "author_association": "COLLABORATOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674401948"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130479#discussion_r1674401948"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130479"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674401948/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": 4806, "start_side": "RIGHT", "line": null, "original_line": 4809, "side": "RIGHT", "in_reply_to_id": 1674075233, "original_position": 24, "position": null, "subject_type": "line", "PR": {"title": "Ensure tensors devices match on `torch.index_put` batch rule impl", "number": 130479, "id": 1963131139}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674411729", "pull_request_review_id": 2172569718, "id": 1674411729, "node_id": "PRRC_kwDOA-j9z85jzX7R", "diff_hunk": "@@ -4793,6 +4794,20 @@ def f(x, gy):\n \n         self.vmap_outplace_test(f, (x, gy), {}, in_dims=(None, 0))\n \n+    @onlyCUDA\n+    @parametrize(\"inplace\", [True, False])\n+    def test_0d_tensor_index_put(self, device, inplace):\n+        def f(t, idx, v):\n+            fn = torch.index_put_ if inplace else torch.index_put\n+            return fn(t, idx, v)\n+\n+        t = torch.zeros((1, 5), device=\"cuda\")\n+        idx = torch.tensor([1, 3])\n+        v = torch.tensor(1, dtype=t.dtype, device=\"cpu\")\n+\n+        expected = torch.tensor([[0, 1, 0, 1, 0]], dtype=t.dtype)\n+        self.assertEqual(expected, vmap(f, in_dims=(0, None, None))(t, (idx,), v))", "path": "test/functorch/test_vmap.py", "commit_id": "668990b001efe53d4ff4319960113df25424987e", "original_commit_id": "053c92bb92dcde2bb67d5401ae3522933237cfd5", "user": {"login": "zou3519", "id": 5652049, "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519"}, "body": "```py\r\nt = torch.zeros((2, 5), device=\"cuda\")\r\nidx = torch.tensor([1, 3])\r\nv = torch.tensor([3.15, 2.71])\r\nvmap(Tensor.index_put_, (0, None, 0))(t, [idx], v)\r\n```\r\nThe above should pass. The argument is that because `t[0].index_put_([idx], v[0])` (which it does), then the above should pass.\r\n\r\nNow, v does stop being a 0D tensor, but from the perspective of vmap **it is** a 0D tensor.", "created_at": "2024-07-11T17:39:06Z", "updated_at": "2024-07-11T17:39:07Z", "html_url": "https://github.com/pytorch/pytorch/pull/130479#discussion_r1674411729", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130479", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674411729"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130479#discussion_r1674411729"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130479"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1674411729/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": 4806, "start_side": "RIGHT", "line": null, "original_line": 4809, "side": "RIGHT", "in_reply_to_id": 1674075233, "original_position": 24, "position": null, "subject_type": "line", "PR": {"title": "Ensure tensors devices match on `torch.index_put` batch rule impl", "number": 130479, "id": 1963131139}}
{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678037429", "pull_request_review_id": 2178074309, "id": 1678037429, "node_id": "PRRC_kwDOA-j9z85kBNG1", "diff_hunk": "@@ -4793,6 +4794,20 @@ def f(x, gy):\n \n         self.vmap_outplace_test(f, (x, gy), {}, in_dims=(None, 0))\n \n+    @onlyCUDA\n+    @parametrize(\"inplace\", [True, False])\n+    def test_0d_tensor_index_put(self, device, inplace):\n+        def f(t, idx, v):\n+            fn = torch.index_put_ if inplace else torch.index_put\n+            return fn(t, idx, v)\n+\n+        t = torch.zeros((1, 5), device=\"cuda\")\n+        idx = torch.tensor([1, 3])\n+        v = torch.tensor(1, dtype=t.dtype, device=\"cpu\")\n+\n+        expected = torch.tensor([[0, 1, 0, 1, 0]], dtype=t.dtype)\n+        self.assertEqual(expected, vmap(f, in_dims=(0, None, None))(t, (idx,), v))", "path": "test/functorch/test_vmap.py", "commit_id": "668990b001efe53d4ff4319960113df25424987e", "original_commit_id": "053c92bb92dcde2bb67d5401ae3522933237cfd5", "user": {"login": "zou3519", "id": 5652049, "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519"}, "body": "(you can merge this first and fix this in a follow-up)", "created_at": "2024-07-15T15:42:36Z", "updated_at": "2024-07-15T15:42:36Z", "html_url": "https://github.com/pytorch/pytorch/pull/130479#discussion_r1678037429", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/130479", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678037429"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/130479#discussion_r1678037429"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/130479"}}, "reactions": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/1678037429/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "start_line": null, "original_start_line": 4806, "start_side": "RIGHT", "line": null, "original_line": 4809, "side": "RIGHT", "in_reply_to_id": 1674075233, "original_position": 24, "position": null, "subject_type": "line", "PR": {"title": "Ensure tensors devices match on `torch.index_put` batch rule impl", "number": 130479, "id": 1963131139}}
