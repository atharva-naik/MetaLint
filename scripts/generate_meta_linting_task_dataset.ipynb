{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arnaik/anaconda3/envs/py3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import pathlib\n",
    "from collections import Counter\n",
    "\n",
    "sys.path.append(\"/home/arnaik/OracleProject\")\n",
    "random.seed(42) # seed for deterministic behavior\n",
    "\n",
    "from src.datautils import MetaLinterDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb 14 16:05:18 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  |   00000000:01:00.0 Off |                    0 |\n",
      "| N/A   33C    P0             54W /  270W |       1MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          On  |   00000000:25:00.0 Off |                    0 |\n",
      "| N/A   32C    P0             54W /  270W |       1MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arnaik/OracleProject\n"
     ]
    }
   ],
   "source": [
    "cd \"/home/arnaik/OracleProject\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112it [02:03,  1.11s/it]\n",
      "5000it [00:01, 3292.03it/s]    | 0/129 [00:00<?, ?it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 3681.62it/s]\n",
      "2it [00:00, 61.06it/s]         | 1/129 [00:02<06:21,  2.98s/it]\n",
      "100%|██████████| 2/2 [00:00<00:00, 8774.69it/s]\n",
      "5000it [00:03, 1639.37it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4230.40it/s]\n",
      "5000it [00:02, 2054.46it/s]    | 3/129 [00:07<05:02,  2.40s/it]\n",
      "100%|██████████| 5000/5000 [00:02<00:00, 1958.47it/s]\n",
      "5000it [00:02, 2437.64it/s]    | 4/129 [00:12<06:59,  3.36s/it]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 2796.28it/s]\n",
      "5000it [00:03, 1594.96it/s]    | 5/129 [00:16<07:22,  3.57s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 9251.37it/s]\n",
      "5000it [00:03, 1458.99it/s]    | 6/129 [00:20<07:29,  3.66s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8851.05it/s]\n",
      "5000it [00:03, 1456.26it/s]    | 7/129 [00:24<07:44,  3.81s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8743.86it/s]\n",
      "5000it [00:04, 1242.54it/s]    | 8/129 [00:28<07:54,  3.92s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 9071.89it/s]\n",
      "2500it [00:02, 1215.67it/s]    | 9/129 [00:33<08:20,  4.17s/it]\n",
      "100%|██████████| 2500/2500 [00:03<00:00, 774.37it/s]\n",
      "5000it [00:02, 2320.29it/s]    | 10/129 [00:38<09:01,  4.55s/it]\n",
      "100%|██████████| 5000/5000 [00:03<00:00, 1305.86it/s]\n",
      "5000it [00:02, 2181.98it/s]    | 11/129 [00:44<09:54,  5.03s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 7859.74it/s]\n",
      "5000it [00:01, 3088.40it/s]    | 12/129 [00:48<08:40,  4.45s/it]\n",
      "100%|██████████| 5000/5000 [00:04<00:00, 1133.11it/s]\n",
      "2500it [00:02, 1103.56it/s]    | 13/129 [00:54<09:37,  4.98s/it]\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 4240.14it/s]\n",
      "5000it [00:01, 2752.69it/s]    | 14/129 [00:57<08:25,  4.39s/it]\n",
      "100%|██████████| 5000/5000 [00:05<00:00, 983.04it/s] \n",
      "5000it [00:01, 4781.73it/s]    | 15/129 [01:04<09:58,  5.25s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8859.50it/s]\n",
      "5000it [00:01, 3721.99it/s]    | 16/129 [01:06<07:56,  4.21s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 9042.01it/s]\n",
      "5000it [00:06, 770.49it/s]     | 17/129 [01:08<06:39,  3.57s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 7508.63it/s]\n",
      "5000it [00:01, 4164.95it/s]    | 18/129 [01:15<08:41,  4.70s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8987.11it/s]\n",
      "5000it [00:01, 4447.65it/s]    | 19/129 [01:17<07:05,  3.87s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8605.41it/s]\n",
      "5000it [00:06, 722.19it/s]     | 20/129 [01:19<05:56,  3.27s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8885.77it/s]\n",
      "5000it [00:01, 4766.25it/s]    | 21/129 [01:27<08:15,  4.59s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8799.89it/s]\n",
      "2500it [00:00, 2808.87it/s]    | 22/129 [01:29<06:41,  3.75s/it]\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 8835.34it/s]\n",
      "5000it [00:01, 3810.82it/s]    | 23/129 [01:30<05:18,  3.01s/it]\n",
      "100%|██████████| 5000/5000 [00:07<00:00, 656.74it/s]\n",
      "5000it [00:01, 4945.22it/s]    | 24/129 [01:39<08:26,  4.82s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 9264.70it/s]\n",
      "2500it [00:00, 3651.82it/s]    | 25/129 [01:41<06:44,  3.89s/it]\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 8171.41it/s]\n",
      "5000it [00:01, 4180.14it/s]    | 26/129 [01:42<05:14,  3.05s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 9586.18it/s] \n",
      "5000it [00:01, 4524.58it/s]    | 27/129 [01:44<04:34,  2.69s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8884.18it/s]\n",
      "5000it [00:09, 518.54it/s]     | 28/129 [01:45<04:05,  2.43s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 9067.77it/s]\n",
      "5000it [00:01, 4708.44it/s]    | 29/129 [01:56<08:01,  4.81s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8698.53it/s]\n",
      "5000it [00:01, 4306.07it/s]    | 30/129 [01:58<06:28,  3.92s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 9228.12it/s] \n",
      "2500it [00:00, 2785.41it/s]    | 31/129 [01:59<05:23,  3.30s/it]\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 7508.91it/s]\n",
      "5000it [00:01, 3952.52it/s]    | 32/129 [02:01<04:23,  2.72s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8998.10it/s]\n",
      "5000it [00:10, 465.35it/s]     | 33/129 [02:03<03:59,  2.49s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 9893.31it/s] \n",
      "5000it [00:00, 5372.18it/s]    | 34/129 [02:14<08:10,  5.17s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 9392.61it/s]\n",
      "5000it [00:01, 4367.74it/s]    | 35/129 [02:16<06:25,  4.11s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 7813.95it/s]\n",
      "5000it [00:01, 4573.60it/s]    | 36/129 [02:18<05:21,  3.46s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 7932.65it/s]\n",
      "5000it [00:01, 3265.62it/s]    | 37/129 [02:20<04:35,  2.99s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8890.67it/s]\n",
      "5000it [00:01, 2740.59it/s]    | 38/129 [02:22<04:12,  2.78s/it]\n",
      "100%|██████████| 5000/5000 [00:12<00:00, 412.50it/s]\n",
      "5000it [00:01, 4159.97it/s]    | 39/129 [02:36<09:16,  6.18s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8596.48it/s]\n",
      "loading shards:  30%|███       | 39/129 [02:38<06:05,  4.06s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset = MetaLinterDataset(\"ruff\", \"./data/ruff_results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_neutral_and_flagged_files(\n",
    "        data: list,\n",
    "        neutral_file_to_flagged_file_ratio: float=1.0,\n",
    "    ):\n",
    "    # iterate over data and create a list of neutral files and flagged files.\n",
    "    neutral_files = []\n",
    "    flagged_files = []\n",
    "    for rec in data:\n",
    "        response = rec['messages'][1]['content']\n",
    "        if response.strip() == \"NO VIOLATIONS FOUND\": neutral_files.append(rec)\n",
    "        else: flagged_files.append(rec)\n",
    "    \n",
    "    # balance the amount of neutral and modified files.\n",
    "    num_neutral_files = min(int(neutral_file_to_flagged_file_ratio*len(flagged_files)), len(neutral_files))\n",
    "    neutral_files = random.sample(neutral_files, k=num_neutral_files)\n",
    "    data = neutral_files + flagged_files \n",
    "    data = random.sample(data, k=len(data)) # shuffle the data around.\n",
    "\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idiom_mix = [\n",
    "    [\"F405\", \"F501\", \"F502\", \"F601\", \"F621\"],\n",
    "    [\"E402\", \"E701\", \"E721\", \"E741\", \"E743\"],\n",
    "    [\"N801\", \"N802\", \"N803\", \"N804\", \"N805\"],\n",
    "    [\"N806\", \"N807\", \"N811\", \"N812\", \"N813\"],\n",
    "    [\"UP001\", \"UP003\", \"UP004\", \"UP005\", \"UP006\"],\n",
    "    [\"UP007\", \"UP008\", \"UP009\", \"UP010\", \"UP011\"],\n",
    "    [\"UP044\", \"UP045\", \"UP046\", \"UP047\", \"UP040\"],\n",
    "    [\"ERA001\", \"C901\", \"I001\", \"I002\", \"BLE001\"],\n",
    "    [\"B002\", \"B003\", \"B004\", \"B005\", \"B006\"],\n",
    "    [\"B007\", \"B008\", \"B009\", \"B010\", \"B012\"],\n",
    "]\n",
    "test_idiom_mix = [\n",
    "    [\"F406\", \"F403\", \"F503\", \"F602\", \"F622\"],\n",
    "    [\"E401\", \"E702\", \"E722\", \"E731\", \"E742\"],\n",
    "    [\"ERA001\", \"C901\", \"I001\", \"I002\", \"BLE001\"],\n",
    "    [\"ANN001\", \"ANN002\", \"ANN003\", \"ANN201\", \"ANN202\"],\n",
    "    [\"ASYNC100\", \"ASYNC105\", \"ASYNC109\", \"ASYNC110\", \"ASYNC115\"],\n",
    "    [\"ASYNC116\", \"ASYNC210\", \"ASYNC220\", \"ASYNC221\", \"ASYNC222\"],\n",
    "    [\"ASYNC230\", \"ASYNC251\", \"ANN204\", \"ANN205\", \"ANN206\"],\n",
    "    [\"S102\", \"S103\", \"S104\", \"S105\", \"S106\"],\n",
    "    [\"S107\", \"S108\", \"S110\", \"S112\", \"S113\"],\n",
    "    [\"S201\", \"S202\", \"S301\", \"S302\", \"S303\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F405', 'F501', 'F502', 'F601', 'F621'] 6664\n",
      "['E402', 'E701', 'E721', 'E741', 'E743'] 10151\n",
      "['N801', 'N802', 'N803', 'N804', 'N805'] 21798\n",
      "['N806', 'N807', 'N811', 'N812', 'N813'] 13654\n",
      "['UP001', 'UP003', 'UP004', 'UP005', 'UP006'] 8280\n",
      "['UP007', 'UP008', 'UP009', 'UP010', 'UP011'] 25805\n",
      "['UP044', 'UP045', 'UP046', 'UP047', 'UP040'] 4\n",
      "['ERA001', 'C901', 'I001', 'I002', 'BLE001'] 93209\n",
      "['B002', 'B003', 'B004', 'B005', 'B006'] 1070\n",
      "['B007', 'B008', 'B009', 'B010', 'B012'] 6773\n",
      "['F406', 'F403', 'F503', 'F602', 'F622'] 7743\n",
      "['E401', 'E702', 'E722', 'E731', 'E742'] 7544\n",
      "['ERA001', 'C901', 'I001', 'I002', 'BLE001'] 93209\n",
      "['ANN001', 'ANN002', 'ANN003', 'ANN201', 'ANN202'] 13753\n",
      "['ASYNC100', 'ASYNC105', 'ASYNC109', 'ASYNC110', 'ASYNC115'] 14\n",
      "['ASYNC116', 'ASYNC210', 'ASYNC220', 'ASYNC221', 'ASYNC222'] 34\n",
      "['ASYNC230', 'ASYNC251', 'ANN204', 'ANN205', 'ANN206'] 24476\n",
      "['S102', 'S103', 'S104', 'S105', 'S106'] 4598\n",
      "['S107', 'S108', 'S110', 'S112', 'S113'] 3952\n",
      "['S201', 'S202', 'S301', 'S302', 'S303'] 1684\n",
      "1486370\n",
      "1486370\n"
     ]
    }
   ],
   "source": [
    "all_train_data = []\n",
    "all_test_data = []\n",
    "random.seed(42)\n",
    "\n",
    "for idiom_mix in train_idiom_mix:\n",
    "    mix_data = dataset.generate_data_mix(idiom_mix, max_code_lines=200)\n",
    "    print(idiom_mix, len([rec for rec in mix_data if rec['messages'][1]['content'] != 'NO VIOLATIONS FOUND']))\n",
    "    all_train_data.extend(mix_data)\n",
    "    \n",
    "for idiom_mix in test_idiom_mix:\n",
    "    mix_data = dataset.generate_data_mix(idiom_mix, max_code_lines=200)\n",
    "    print(idiom_mix, len([rec for rec in mix_data if rec['messages'][1]['content'] != 'NO VIOLATIONS FOUND']))\n",
    "    all_test_data.extend(mix_data)\n",
    "    \n",
    "print(len(all_train_data))\n",
    "print(len(all_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\t    experiments\t\t\t plots\t    vllm_env.yaml\n",
      "access_tokens.json  filter_codereviewer_data.py  ruff.toml\n",
      "alignment-handbook  handbook.yml\t\t scripts\n",
      "data\t\t    peft_requirements.txt\t src\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mix_data = dataset.generate_data_mix(['ERA001'])\n",
    "# print(len(mix_data))\n",
    "# mix_data[2]['messages'][1]['content']\n",
    "# print(len([rec for rec in mix_data if rec['messages'][1]['content'] != 'NO VIOLATIONS FOUND']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "1000\n",
      "81074\n",
      "8048\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "from collections import defaultdict\n",
    "\n",
    "def balance_neutral_and_flagged_files(\n",
    "        data: list,\n",
    "        neutral_file_to_flagged_file_ratio: float=1.0,\n",
    "    ):\n",
    "    # iterate over data and create a list of neutral files and flagged files.\n",
    "    neutral_files = []\n",
    "    flagged_files = []\n",
    "    for rec in data:\n",
    "        response = rec['messages'][1]['content']\n",
    "        if response.strip() == \"NO VIOLATIONS FOUND\": neutral_files.append(rec)\n",
    "        else: flagged_files.append(rec)\n",
    "    \n",
    "    # balance the amount of neutral and modified files.\n",
    "    num_neutral_files = min(int(neutral_file_to_flagged_file_ratio*len(flagged_files)), len(neutral_files))\n",
    "    neutral_files = random.sample(neutral_files, k=num_neutral_files)\n",
    "    data = neutral_files + flagged_files \n",
    "    data = random.sample(data, k=len(data)) # shuffle the data around.\n",
    "\n",
    "    return data\n",
    "\n",
    "def impose_idiom_mix_ceilings(data, ceiling: int=5000):\n",
    "    \"\"\"reduce size of data stratified by the idiom mix and violation or no violation category\"\"\"\n",
    "    category_to_data = defaultdict(lambda: [])\n",
    "    for rec in data:\n",
    "        violation_present = \"yes\" if rec['messages'][1]['content'] != \"NO VIOLATIONS FOUND\" else \"no\"\n",
    "        category_to_data[rec['source']+\"-\"+violation_present].append(rec)\n",
    "    category_to_data = dict(category_to_data)\n",
    "    final_data = []\n",
    "    for category, data_subset in category_to_data.items():\n",
    "        selected_data = random.sample(data_subset, k=min(len(data_subset), ceiling))\n",
    "        final_data.extend(selected_data)\n",
    "        # print(category, len(selected_data))\n",
    "    # print(len(final_data))\n",
    "    return final_data\n",
    "\n",
    "def split_train_and_test_data(train_data, test_data):\n",
    "    train_ids = set()\n",
    "    test_ids = set()\n",
    "    id_to_data = {}\n",
    "\n",
    "    for rec in train_data:\n",
    "        train_ids.add(rec['id'])\n",
    "        id_to_data[rec['id']] = rec\n",
    "    for rec in test_data:\n",
    "        test_ids.add(rec['id'])\n",
    "        id_to_data[rec['id']] = rec\n",
    "    \n",
    "    common_ids = train_ids.intersection(test_ids)\n",
    "    train_only_ids = train_ids.difference(test_ids)\n",
    "    test_only_ids = test_ids.difference(train_ids)\n",
    "\n",
    "    train_only_data = impose_idiom_mix_ceilings([id_to_data[ID] for ID in train_only_ids], ceiling=5000)\n",
    "    test_only_data = impose_idiom_mix_ceilings([id_to_data[ID] for ID in test_only_ids], ceiling=500)\n",
    "    # common_data = [id_to_data[ID] for ID in test_only_ids]\n",
    "    common_data_split_1_IDs = set(random.sample(list(common_ids), k=len(common_ids)//2)) \n",
    "    common_data_split_2_IDs = common_ids.difference(common_data_split_1_IDs)\n",
    "    common_data_split_1 = [id_to_data[ID] for ID in common_data_split_1_IDs]\n",
    "    common_data_split_2 = [id_to_data[ID] for ID in common_data_split_2_IDs]\n",
    "    \n",
    "    train_from_common_data = impose_idiom_mix_ceilings(common_data_split_1, ceiling=5000)\n",
    "    test_from_common_data = impose_idiom_mix_ceilings(common_data_split_2, ceiling=500)\n",
    "\n",
    "    print(len(train_from_common_data))\n",
    "    print(len(test_from_common_data))\n",
    "    print(len(train_only_data))\n",
    "    print(len(test_only_data))\n",
    "\n",
    "    return train_only_data+train_from_common_data, test_only_data+test_from_common_data\n",
    "\n",
    "filt_train_data, filt_test_data = split_train_and_test_data(all_train_data, all_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(data: list[dict]):\n",
    "    return random.sample(data, k=len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data len: 91074\n",
      "{'rull_linter/N801-N802-N803-N804-N805': 10000, 'rull_linter/UP001-UP003-UP004-UP005-UP006': 10000, 'rull_linter/F405-F501-F502-F601-F621': 10000, 'rull_linter/E402-E701-E721-E741-E743': 10000, 'rull_linter/UP007-UP008-UP009-UP010-UP011': 10000, 'rull_linter/B007-B008-B009-B010-B012': 10000, 'rull_linter/N806-N807-N811-N812-N813': 10000, 'rull_linter/ERA001-C901-I001-I002-BLE001': 10000, 'rull_linter/B002-B003-B004-B005-B006': 6070, 'rull_linter/UP044-UP045-UP046-UP047-UP040': 5004}\n",
      "test data len: 9048\n",
      "{'rull_linter/S107-S108-S110-S112-S113': 1000, 'rull_linter/S201-S202-S301-S302-S303': 1000, 'rull_linter/F406-F403-F503-F602-F622': 1000, 'rull_linter/ASYNC230-ASYNC251-ANN204-ANN205-ANN206': 1000, 'rull_linter/ANN001-ANN002-ANN003-ANN201-ANN202': 1000, 'rull_linter/E401-E702-E722-E731-E742': 1000, 'rull_linter/S102-S103-S104-S105-S106': 1000, 'rull_linter/ERA001-C901-I001-I002-BLE001': 1000, 'rull_linter/ASYNC116-ASYNC210-ASYNC220-ASYNC221-ASYNC222': 534, 'rull_linter/ASYNC100-ASYNC105-ASYNC109-ASYNC110-ASYNC115': 514}\n"
     ]
    }
   ],
   "source": [
    "# neutral_file_to_flagged_file_ratio = 1.0\n",
    "# train_data = balance_neutral_and_flagged_files(all_train_data, neutral_file_to_flagged_file_ratio)\n",
    "# test_data = balance_neutral_and_flagged_files(all_test_data, neutral_file_to_flagged_file_ratio)\n",
    "\n",
    "with open(\"./data/ruff_meta_linting/train_v3.json\", \"w\") as f:\n",
    "    print(f\"train data len: {len(filt_train_data)}\")\n",
    "    # print(dict(Counter([rec['source'] for rec in filt_train_data if rec['messages'][1]['content'] != 'NO VIOLATIONS FOUND']).most_common()))\n",
    "    # print(dict(Counter([rec['source'] for rec in filt_train_data if rec['messages'][1]['content'] == 'NO VIOLATIONS FOUND']).most_common()))\n",
    "    print(dict(Counter([rec['source'] for rec in filt_train_data]).most_common()))\n",
    "    json.dump(shuffle_data(filt_train_data), f, indent=4)\n",
    "with open(\"./data/ruff_meta_linting/test_v3.json\", \"w\") as f:\n",
    "    print(f\"test data len: {len(filt_test_data)}\")\n",
    "    # print(dict(Counter([rec['source'] for rec in filt_test_data if rec['messages'][1]['content'] != 'NO VIOLATIONS FOUND']).most_common()))\n",
    "    # print(dict(Counter([rec['source'] for rec in filt_test_data if rec['messages'][1]['content'] == 'NO VIOLATIONS FOUND']).most_common()))\n",
    "    print(dict(Counter([rec['source'] for rec in filt_test_data]).most_common()))\n",
    "    json.dump(shuffle_data(filt_test_data), f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
