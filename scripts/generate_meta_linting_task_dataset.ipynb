{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arnaik/anaconda3/envs/py3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import pathlib\n",
    "from collections import Counter\n",
    "\n",
    "sys.path.append(\"/home/arnaik/OracleProject\")\n",
    "random.seed(42) # seed for deterministic behavior\n",
    "\n",
    "from src.datautils import MetaLinterDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar 11 18:09:07 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  |   00000000:41:00.0 Off |                    0 |\n",
      "| N/A   40C    P0             55W /  270W |       1MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          On  |   00000000:61:00.0 Off |                    0 |\n",
      "| N/A   41C    P0             57W /  270W |       1MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arnaik/OracleProject\n"
     ]
    }
   ],
   "source": [
    "cd \"/home/arnaik/OracleProject\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112it [00:39,  2.86it/s]\n",
      "5000it [00:01, 3773.88it/s]    | 0/129 [00:00<?, ?it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 3729.25it/s]\n",
      "2it [00:00, 2130.71it/s]       | 1/129 [00:02<05:54,  2.77s/it]\n",
      "100%|██████████| 2/2 [00:00<00:00, 11650.84it/s]\n",
      "5000it [00:01, 2796.81it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 2506.22it/s]\n",
      "5000it [00:00, 5719.63it/s]    | 3/129 [00:06<04:33,  2.17s/it]\n",
      "100%|██████████| 5000/5000 [00:02<00:00, 1957.99it/s]\n",
      "5000it [00:00, 5483.99it/s]    | 4/129 [00:10<05:31,  2.65s/it]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 2741.22it/s]\n",
      "5000it [00:02, 2027.06it/s]    | 5/129 [00:13<05:38,  2.73s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8887.94it/s]\n",
      "5000it [00:02, 1798.49it/s]    | 6/129 [00:16<05:53,  2.87s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8535.66it/s]\n",
      "5000it [00:03, 1619.89it/s]    | 7/129 [00:19<06:15,  3.08s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8306.57it/s]\n",
      "5000it [00:03, 1572.44it/s]    | 8/129 [00:23<06:41,  3.32s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8957.34it/s]\n",
      "2500it [00:01, 1982.38it/s]    | 9/129 [00:27<06:59,  3.50s/it]\n",
      "100%|██████████| 2500/2500 [00:03<00:00, 783.69it/s]\n",
      "5000it [00:01, 4655.10it/s]    | 10/129 [00:32<07:35,  3.83s/it]\n",
      "100%|██████████| 5000/5000 [00:03<00:00, 1322.29it/s]\n",
      "5000it [00:01, 4469.94it/s]    | 11/129 [00:37<08:14,  4.19s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 7795.53it/s]\n",
      "5000it [00:01, 4555.49it/s]    | 12/129 [00:39<06:50,  3.50s/it]\n",
      "100%|██████████| 5000/5000 [00:04<00:00, 1157.38it/s]\n",
      "2500it [00:01, 2030.69it/s]    | 13/129 [00:44<07:59,  4.13s/it]\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 4110.52it/s]\n",
      "5000it [00:01, 4844.44it/s]    | 14/129 [00:46<06:41,  3.49s/it]\n",
      "100%|██████████| 5000/5000 [00:05<00:00, 990.52it/s] \n",
      "5000it [00:00, 5310.54it/s]    | 15/129 [00:53<08:18,  4.37s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8683.43it/s]\n",
      "5000it [00:01, 4042.85it/s]    | 16/129 [00:54<06:43,  3.57s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8735.39it/s]\n",
      "5000it [00:06, 807.47it/s]     | 17/129 [00:56<05:46,  3.09s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 7498.88it/s]\n",
      "5000it [00:00, 5149.18it/s]    | 18/129 [01:03<07:55,  4.28s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 9046.82it/s]\n",
      "5000it [00:01, 4528.68it/s]    | 19/129 [01:05<06:25,  3.50s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8713.81it/s]\n",
      "5000it [00:06, 724.21it/s]     | 20/129 [01:07<05:27,  3.01s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8630.06it/s]\n",
      "5000it [00:01, 4508.33it/s]    | 21/129 [01:15<07:55,  4.40s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8730.28it/s]\n",
      "2500it [00:00, 4027.17it/s]    | 22/129 [01:16<06:29,  3.64s/it]\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 8597.48it/s]\n",
      "5000it [00:01, 4567.47it/s]    | 23/129 [01:17<05:02,  2.85s/it]\n",
      "100%|██████████| 5000/5000 [00:07<00:00, 669.22it/s]\n",
      "5000it [00:00, 5602.86it/s]    | 24/129 [01:26<08:04,  4.61s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 9264.07it/s]\n",
      "2500it [00:00, 5217.76it/s]    | 25/129 [01:28<06:25,  3.71s/it]\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 7545.67it/s]\n",
      "5000it [00:01, 4088.61it/s]    | 26/129 [01:29<04:55,  2.87s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 9656.35it/s] \n",
      "5000it [00:00, 5325.14it/s]    | 27/129 [01:31<04:22,  2.57s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8337.14it/s]\n",
      "5000it [00:09, 540.26it/s]     | 28/129 [01:32<03:53,  2.31s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8811.30it/s]\n",
      "5000it [00:01, 4860.01it/s]    | 29/129 [01:42<07:41,  4.61s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8503.67it/s]\n",
      "5000it [00:01, 4613.64it/s]    | 30/129 [01:44<06:12,  3.77s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8564.21it/s]\n",
      "2500it [00:00, 2983.09it/s]    | 31/129 [01:46<05:12,  3.19s/it]\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 7437.00it/s]\n",
      "5000it [00:01, 4061.47it/s]    | 32/129 [01:47<04:13,  2.62s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8720.42it/s]\n",
      "5000it [00:10, 463.22it/s]     | 33/129 [01:49<03:52,  2.42s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8799.96it/s]\n",
      "5000it [00:00, 5781.07it/s]    | 34/129 [02:01<08:09,  5.15s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 9022.92it/s]\n",
      "5000it [00:01, 4152.71it/s]    | 35/129 [02:02<06:23,  4.08s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 7565.81it/s]\n",
      "5000it [00:01, 4716.59it/s]    | 36/129 [02:04<05:22,  3.47s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 7279.37it/s]\n",
      "5000it [00:01, 4234.37it/s]    | 37/129 [02:06<04:39,  3.04s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 6820.29it/s]\n",
      "5000it [00:01, 3455.42it/s]    | 38/129 [02:08<04:10,  2.76s/it]\n",
      "100%|██████████| 5000/5000 [00:13<00:00, 383.69it/s]\n",
      "5000it [00:01, 4399.04it/s]    | 39/129 [02:23<09:30,  6.34s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8056.64it/s]\n",
      "5000it [00:01, 4821.63it/s]    | 40/129 [02:25<07:26,  5.02s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8352.95it/s]\n",
      "2500it [00:00, 2682.14it/s]    | 41/129 [02:27<05:57,  4.06s/it]\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 7450.37it/s]\n",
      "2500it [00:00, 3556.37it/s]    | 42/129 [02:28<04:43,  3.26s/it]\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 8378.02it/s]\n",
      "5000it [00:01, 4729.36it/s]    | 43/129 [02:29<03:44,  2.61s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 9453.96it/s]\n",
      "5000it [00:01, 4913.41it/s]    | 44/129 [02:31<03:19,  2.35s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8690.56it/s]\n",
      "5000it [00:00, 5596.97it/s]    | 45/129 [02:33<03:02,  2.17s/it]\n",
      "100%|██████████| 5000/5000 [00:14<00:00, 338.17it/s]\n",
      "5000it [00:00, 5120.76it/s]    | 46/129 [02:49<08:42,  6.30s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8814.43it/s]\n",
      "5000it [00:01, 4864.73it/s]    | 47/129 [02:50<06:43,  4.92s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8834.67it/s]\n",
      "5000it [00:01, 4347.82it/s]    | 48/129 [02:52<05:21,  3.97s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8552.41it/s]\n",
      "5000it [00:00, 5590.70it/s]    | 49/129 [02:54<04:28,  3.35s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 9116.28it/s]\n",
      "5000it [00:01, 4367.35it/s]    | 50/129 [02:56<03:46,  2.87s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 7991.05it/s]\n",
      "2500it [00:01, 1888.77it/s]    | 51/129 [02:58<03:22,  2.60s/it]\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 3857.96it/s]\n",
      "5000it [00:01, 3982.08it/s]    | 52/129 [03:00<03:09,  2.46s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 7890.79it/s]\n",
      "5000it [00:17, 292.12it/s]     | 53/129 [03:02<02:58,  2.35s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8222.53it/s]\n",
      "5000it [00:01, 4572.44it/s]    | 54/129 [03:20<08:46,  7.02s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 6833.90it/s]\n",
      "5000it [00:01, 4010.91it/s]    | 55/129 [03:22<06:49,  5.54s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 7173.53it/s]\n",
      "5000it [00:01, 2999.48it/s]    | 56/129 [03:24<05:29,  4.52s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8127.79it/s]\n",
      "5000it [00:01, 4737.96it/s]    | 57/129 [03:27<04:41,  3.91s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 7269.81it/s]\n",
      "5000it [00:01, 3457.50it/s]    | 58/129 [03:29<03:55,  3.31s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 7924.46it/s]\n",
      "5000it [00:01, 3867.00it/s]    | 59/129 [03:31<03:29,  3.00s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 7127.37it/s]\n",
      "5000it [00:01, 4034.31it/s]    | 60/129 [03:33<03:10,  2.75s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 7864.33it/s]\n",
      "5000it [00:22, 226.69it/s]     | 61/129 [03:35<02:54,  2.56s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8624.74it/s]\n",
      "2500it [00:00, 3275.95it/s]    | 62/129 [03:58<09:40,  8.66s/it]\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 7703.20it/s]\n",
      "2500it [00:01, 1809.33it/s]    | 63/129 [03:59<07:03,  6.42s/it]\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 4309.23it/s]\n",
      "5000it [00:00, 5065.80it/s]    | 64/129 [04:01<05:34,  5.15s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8647.78it/s]\n",
      "2500it [00:00, 5010.03it/s]    | 65/129 [04:03<04:24,  4.14s/it]\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 7709.89it/s]\n",
      "5000it [00:01, 4040.29it/s]    | 66/129 [04:04<03:20,  3.19s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8483.97it/s]\n",
      "5000it [00:00, 5205.23it/s]    | 67/129 [04:06<02:55,  2.83s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 9518.59it/s]\n",
      "5000it [00:01, 4618.88it/s]    | 68/129 [04:08<02:31,  2.48s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 9121.15it/s]\n",
      "5000it [00:00, 5296.95it/s]    | 69/129 [04:10<02:16,  2.28s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8014.21it/s]\n",
      "5000it [00:01, 3980.64it/s]    | 70/129 [04:12<02:06,  2.15s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 7027.17it/s]\n",
      "5000it [00:00, 5103.00it/s]    | 71/129 [04:14<02:04,  2.15s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 9028.84it/s]\n",
      "5000it [00:23, 215.28it/s]▌    | 72/129 [04:15<01:54,  2.01s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8231.92it/s]\n",
      "2500it [00:00, 4138.58it/s]    | 73/129 [04:39<08:02,  8.62s/it]\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 6310.41it/s]\n",
      "5000it [00:01, 4837.77it/s]    | 74/129 [04:41<05:50,  6.37s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8347.33it/s]\n",
      "2500it [00:00, 3802.03it/s]    | 75/129 [04:42<04:30,  5.00s/it]\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 6198.19it/s]\n",
      "5000it [00:01, 4608.64it/s]    | 76/129 [04:44<03:24,  3.86s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 7746.68it/s]\n",
      "2500it [00:00, 3749.65it/s]    | 77/129 [04:45<02:50,  3.28s/it]\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 6330.18it/s]\n",
      "5000it [00:00, 5414.69it/s]    | 78/129 [04:47<02:15,  2.65s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8505.02it/s]\n",
      "5000it [00:00, 5312.99it/s]    | 79/129 [04:48<01:58,  2.36s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8087.97it/s]\n",
      "5000it [00:00, 6161.54it/s]▏   | 80/129 [04:50<01:46,  2.18s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8936.84it/s]\n",
      "5000it [00:00, 6304.59it/s]▎   | 81/129 [04:52<01:35,  1.99s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 9338.61it/s]\n",
      "5000it [00:00, 6184.46it/s]▎   | 82/129 [04:53<01:26,  1.84s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 9223.98it/s]\n",
      "5000it [00:00, 6249.76it/s]▍   | 83/129 [04:55<01:20,  1.74s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 9620.17it/s] \n",
      "5000it [00:00, 6169.12it/s]▌   | 84/129 [04:56<01:14,  1.67s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 9251.45it/s]\n",
      "5000it [00:27, 181.86it/s] ▌   | 85/129 [04:58<01:11,  1.62s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8845.98it/s]\n",
      "2500it [00:01, 1693.58it/s]▋   | 86/129 [05:26<06:52,  9.60s/it]\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 5822.52it/s]\n",
      "5000it [00:01, 4327.62it/s]▋   | 87/129 [05:28<05:08,  7.33s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8521.67it/s]\n",
      "5000it [00:00, 5640.53it/s]▊   | 88/129 [05:30<03:54,  5.71s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8907.27it/s]\n",
      "5000it [00:01, 4916.74it/s]▉   | 89/129 [05:31<02:59,  4.48s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8270.12it/s]\n",
      "5000it [00:00, 6046.72it/s]▉   | 90/129 [05:33<02:23,  3.68s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8760.29it/s]\n",
      "5000it [00:01, 4467.88it/s]█   | 91/129 [05:35<01:55,  3.05s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8492.64it/s]\n",
      "5000it [00:01, 4092.65it/s]█▏  | 92/129 [05:37<01:39,  2.70s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8819.57it/s]\n",
      "5000it [00:01, 4980.74it/s]█▏  | 93/129 [05:39<01:29,  2.48s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 9623.39it/s] \n",
      "5000it [00:01, 4431.02it/s]█▎  | 94/129 [05:40<01:18,  2.25s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8278.00it/s]\n",
      "5000it [00:01, 3847.45it/s]█▎  | 95/129 [05:42<01:12,  2.15s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8090.29it/s]\n",
      "2500it [00:00, 4084.75it/s]█▍  | 96/129 [05:44<01:10,  2.14s/it]\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 7748.99it/s]\n",
      "5000it [00:00, 5318.24it/s]█▌  | 97/129 [05:45<00:57,  1.81s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 9091.43it/s]\n",
      "2500it [00:00, 3374.20it/s]█▌  | 98/129 [05:47<00:54,  1.77s/it]\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 7370.74it/s]\n",
      "2500it [00:00, 4641.06it/s]█▋  | 99/129 [05:48<00:47,  1.60s/it]\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 7784.94it/s]\n",
      "5000it [00:31, 156.72it/s]██▊  | 100/129 [05:49<00:40,  1.41s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8434.58it/s]\n",
      "5000it [00:01, 4603.26it/s]█▊  | 101/129 [06:22<05:01, 10.78s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8048.54it/s]\n",
      "5000it [00:00, 5530.59it/s]█▉  | 102/129 [06:24<03:39,  8.11s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8628.39it/s]\n",
      "2500it [00:00, 4833.99it/s]█▉  | 103/129 [06:25<02:40,  6.18s/it]\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 8638.47it/s]\n",
      "2500it [00:00, 2757.57it/s]██  | 104/129 [06:26<01:54,  4.60s/it]\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 5942.71it/s]\n",
      "5000it [00:01, 4933.68it/s]██▏ | 105/129 [06:28<01:27,  3.65s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8696.49it/s]\n",
      "5000it [00:01, 4422.15it/s]██▏ | 106/129 [06:30<01:10,  3.08s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 7857.12it/s]\n",
      "5000it [00:01, 4932.02it/s]██▎ | 107/129 [06:32<01:00,  2.74s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8618.47it/s]\n",
      "5000it [00:00, 5408.67it/s]██▎ | 108/129 [06:33<00:51,  2.45s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8640.40it/s]\n",
      "2500it [00:00, 2791.22it/s]██▍ | 109/129 [06:35<00:44,  2.22s/it]\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 5267.29it/s]\n",
      "5000it [00:01, 4344.82it/s]██▌ | 110/129 [06:37<00:38,  2.01s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8192.04it/s]\n",
      "5000it [00:01, 3821.57it/s]██▌ | 111/129 [06:38<00:35,  1.99s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 7753.16it/s]\n",
      "5000it [00:01, 4747.84it/s]██▋ | 112/129 [06:41<00:34,  2.04s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8531.63it/s]\n",
      "5000it [00:01, 3583.86it/s]██▊ | 113/129 [06:42<00:31,  1.97s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8713.43it/s]\n",
      "5000it [00:01, 4397.66it/s]██▊ | 114/129 [06:45<00:30,  2.03s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8875.62it/s]\n",
      "2500it [00:01, 2225.47it/s]██▉ | 115/129 [06:46<00:27,  1.98s/it]\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 5413.72it/s]\n",
      "5000it [00:00, 5241.87it/s]██▉ | 116/129 [06:48<00:24,  1.90s/it]\n",
      "100%|██████████| 5000/5000 [00:37<00:00, 133.23it/s]\n",
      "2500it [00:00, 3369.35it/s]███ | 117/129 [07:27<02:35, 12.93s/it]\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 8048.88it/s]\n",
      "5000it [00:01, 4242.19it/s]███▏| 118/129 [07:28<01:43,  9.39s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8752.45it/s]\n",
      "5000it [00:00, 5287.95it/s]███▏| 119/129 [07:30<01:11,  7.15s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8397.33it/s]\n",
      "5000it [00:01, 2753.22it/s]███▎| 120/129 [07:32<00:49,  5.52s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8334.09it/s]\n",
      "2500it [00:00, 4498.37it/s]███▍| 121/129 [07:34<00:37,  4.64s/it]\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 8462.33it/s]\n",
      "5000it [00:01, 4398.56it/s]███▍| 122/129 [07:35<00:24,  3.54s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 10148.36it/s]\n",
      "5000it [00:01, 3640.44it/s]███▌| 123/129 [07:37<00:18,  3.01s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 7953.15it/s]\n",
      "5000it [00:01, 4336.20it/s]███▌| 124/129 [07:39<00:14,  2.81s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8466.97it/s]\n",
      "5000it [00:01, 4340.85it/s]███▋| 125/129 [07:41<00:10,  2.55s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 7899.30it/s]\n",
      "5000it [00:01, 4083.04it/s]███▊| 126/129 [07:43<00:07,  2.38s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8808.53it/s]\n",
      "5000it [00:01, 4067.68it/s]███▊| 127/129 [07:45<00:04,  2.26s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8978.84it/s]\n",
      "5000it [00:00, 5065.19it/s]███▉| 128/129 [07:47<00:02,  2.17s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 9467.25it/s]\n",
      "loading shards: 100%|██████████| 129/129 [07:49<00:00,  3.64s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset = MetaLinterDataset(\"ruff\", \"./data/ruff_results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_neutral_and_flagged_files(\n",
    "        data: list,\n",
    "        neutral_file_to_flagged_file_ratio: float=1.0,\n",
    "    ):\n",
    "    # iterate over data and create a list of neutral files and flagged files.\n",
    "    neutral_files = []\n",
    "    flagged_files = []\n",
    "    for rec in data:\n",
    "        response = rec['messages'][1]['content']\n",
    "        if response.strip() == \"NO VIOLATIONS FOUND\": neutral_files.append(rec)\n",
    "        else: flagged_files.append(rec)\n",
    "    \n",
    "    # balance the amount of neutral and modified files.\n",
    "    num_neutral_files = min(int(neutral_file_to_flagged_file_ratio*len(flagged_files)), len(neutral_files))\n",
    "    neutral_files = random.sample(neutral_files, k=num_neutral_files)\n",
    "    data = neutral_files + flagged_files \n",
    "    data = random.sample(data, k=len(data)) # shuffle the data around.\n",
    "\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idiom_mix = [\n",
    "    [\"F405\", \"F501\", \"F502\", \"F601\", \"F621\"],\n",
    "    [\"E402\", \"E701\", \"E721\", \"E741\", \"E743\"],\n",
    "    [\"N801\", \"N802\", \"N803\", \"N804\", \"N805\"],\n",
    "    [\"N806\", \"N807\", \"N811\", \"N812\", \"N813\"],\n",
    "    [\"UP001\", \"UP003\", \"UP004\", \"UP005\", \"UP006\"],\n",
    "    [\"UP007\", \"UP008\", \"UP009\", \"UP010\", \"UP011\"],\n",
    "    [\"UP044\", \"UP045\", \"UP046\", \"UP047\", \"UP040\"],\n",
    "    [\"ERA001\", \"C901\", \"I001\", \"I002\", \"BLE001\"],\n",
    "    [\"B002\", \"B003\", \"B004\", \"B005\", \"B006\"],\n",
    "    [\"B007\", \"B008\", \"B009\", \"B010\", \"B012\"],\n",
    "]\n",
    "test_idiom_mix = [\n",
    "    [\"F406\", \"F403\", \"F503\", \"F602\", \"F622\"],\n",
    "    [\"E401\", \"E702\", \"E722\", \"E731\", \"E742\"],\n",
    "    [\"ERA001\", \"C901\", \"I001\", \"I002\", \"BLE001\"],\n",
    "    [\"ANN001\", \"ANN002\", \"ANN003\", \"ANN201\", \"ANN202\"],\n",
    "    [\"ASYNC100\", \"ASYNC105\", \"ASYNC109\", \"ASYNC110\", \"ASYNC115\"],\n",
    "    [\"ASYNC116\", \"ASYNC210\", \"ASYNC220\", \"ASYNC221\", \"ASYNC222\"],\n",
    "    [\"ASYNC230\", \"ASYNC251\", \"ANN204\", \"ANN205\", \"ANN206\"],\n",
    "    [\"S102\", \"S103\", \"S104\", \"S105\", \"S106\"],\n",
    "    [\"S107\", \"S108\", \"S110\", \"S112\", \"S113\"],\n",
    "    [\"S201\", \"S202\", \"S301\", \"S302\", \"S303\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F405', 'F501', 'F502', 'F601', 'F621'] 21155\n",
      "['E402', 'E701', 'E721', 'E741', 'E743'] 32186\n",
      "['N801', 'N802', 'N803', 'N804', 'N805'] 68822\n",
      "['N806', 'N807', 'N811', 'N812', 'N813'] 43329\n",
      "['UP001', 'UP003', 'UP004', 'UP005', 'UP006'] 26420\n",
      "['UP007', 'UP008', 'UP009', 'UP010', 'UP011'] 82555\n",
      "['UP044', 'UP045', 'UP046', 'UP047', 'UP040'] 14\n",
      "['ERA001', 'C901', 'I001', 'I002', 'BLE001'] 296494\n",
      "['B002', 'B003', 'B004', 'B005', 'B006'] 3377\n",
      "['B007', 'B008', 'B009', 'B010', 'B012'] 21277\n",
      "['F406', 'F403', 'F503', 'F602', 'F622'] 24595\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "all_train_data = []\n",
    "all_test_data = []\n",
    "random.seed(42)\n",
    "\n",
    "for idiom_mix in train_idiom_mix:\n",
    "    mix_data = dataset.generate_data_mix(idiom_mix, max_code_lines=200)\n",
    "    print(idiom_mix, len([rec for rec in mix_data if rec['messages'][1]['content'] != 'NO VIOLATIONS FOUND']))\n",
    "    all_train_data.extend(mix_data)\n",
    "    \n",
    "for idiom_mix in test_idiom_mix:\n",
    "    mix_data = dataset.generate_data_mix(idiom_mix, max_code_lines=200)\n",
    "    print(idiom_mix, len([rec for rec in mix_data if rec['messages'][1]['content'] != 'NO VIOLATIONS FOUND']))\n",
    "    all_test_data.extend(mix_data)\n",
    "    \n",
    "print(len(all_train_data))\n",
    "print(len(all_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\t    experiments\t\t\t plots\t    vllm_env.yaml\n",
      "access_tokens.json  filter_codereviewer_data.py  ruff.toml\n",
      "alignment-handbook  handbook.yml\t\t scripts\n",
      "data\t\t    peft_requirements.txt\t src\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mix_data = dataset.generate_data_mix(['ERA001'])\n",
    "# print(len(mix_data))\n",
    "# mix_data[2]['messages'][1]['content']\n",
    "# print(len([rec for rec in mix_data if rec['messages'][1]['content'] != 'NO VIOLATIONS FOUND']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "1000\n",
      "81074\n",
      "8048\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "from collections import defaultdict\n",
    "\n",
    "def balance_neutral_and_flagged_files(\n",
    "        data: list,\n",
    "        neutral_file_to_flagged_file_ratio: float=1.0,\n",
    "    ):\n",
    "    # iterate over data and create a list of neutral files and flagged files.\n",
    "    neutral_files = []\n",
    "    flagged_files = []\n",
    "    for rec in data:\n",
    "        response = rec['messages'][1]['content']\n",
    "        if response.strip() == \"NO VIOLATIONS FOUND\": neutral_files.append(rec)\n",
    "        else: flagged_files.append(rec)\n",
    "    \n",
    "    # balance the amount of neutral and modified files.\n",
    "    num_neutral_files = min(int(neutral_file_to_flagged_file_ratio*len(flagged_files)), len(neutral_files))\n",
    "    neutral_files = random.sample(neutral_files, k=num_neutral_files)\n",
    "    data = neutral_files + flagged_files \n",
    "    data = random.sample(data, k=len(data)) # shuffle the data around.\n",
    "\n",
    "    return data\n",
    "\n",
    "def impose_idiom_mix_ceilings(data, ceiling: int=5000):\n",
    "    \"\"\"reduce size of data stratified by the idiom mix and violation or no violation category\"\"\"\n",
    "    category_to_data = defaultdict(lambda: [])\n",
    "    for rec in data:\n",
    "        violation_present = \"yes\" if rec['messages'][1]['content'] != \"NO VIOLATIONS FOUND\" else \"no\"\n",
    "        category_to_data[rec['source']+\"-\"+violation_present].append(rec)\n",
    "    category_to_data = dict(category_to_data)\n",
    "    final_data = []\n",
    "    for category, data_subset in category_to_data.items():\n",
    "        selected_data = random.sample(data_subset, k=min(len(data_subset), ceiling))\n",
    "        final_data.extend(selected_data)\n",
    "        # print(category, len(selected_data))\n",
    "    # print(len(final_data))\n",
    "    return final_data\n",
    "\n",
    "def split_train_and_test_data(train_data, test_data):\n",
    "    train_ids = set()\n",
    "    test_ids = set()\n",
    "    id_to_data = {}\n",
    "\n",
    "    for rec in train_data:\n",
    "        train_ids.add(rec['id'])\n",
    "        id_to_data[rec['id']] = rec\n",
    "    for rec in test_data:\n",
    "        test_ids.add(rec['id'])\n",
    "        id_to_data[rec['id']] = rec\n",
    "    \n",
    "    common_ids = train_ids.intersection(test_ids)\n",
    "    train_only_ids = train_ids.difference(test_ids)\n",
    "    test_only_ids = test_ids.difference(train_ids)\n",
    "\n",
    "    train_only_data = impose_idiom_mix_ceilings([id_to_data[ID] for ID in train_only_ids], ceiling=5000)\n",
    "    test_only_data = impose_idiom_mix_ceilings([id_to_data[ID] for ID in test_only_ids], ceiling=500)\n",
    "    # common_data = [id_to_data[ID] for ID in test_only_ids]\n",
    "    common_data_split_1_IDs = set(random.sample(list(common_ids), k=len(common_ids)//2)) \n",
    "    common_data_split_2_IDs = common_ids.difference(common_data_split_1_IDs)\n",
    "    common_data_split_1 = [id_to_data[ID] for ID in common_data_split_1_IDs]\n",
    "    common_data_split_2 = [id_to_data[ID] for ID in common_data_split_2_IDs]\n",
    "    \n",
    "    train_from_common_data = impose_idiom_mix_ceilings(common_data_split_1, ceiling=5000)\n",
    "    test_from_common_data = impose_idiom_mix_ceilings(common_data_split_2, ceiling=500)\n",
    "\n",
    "    print(len(train_from_common_data))\n",
    "    print(len(test_from_common_data))\n",
    "    print(len(train_only_data))\n",
    "    print(len(test_only_data))\n",
    "\n",
    "    return train_only_data+train_from_common_data, test_only_data+test_from_common_data\n",
    "\n",
    "filt_train_data, filt_test_data = split_train_and_test_data(all_train_data, all_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(data: list[dict]):\n",
    "    return random.sample(data, k=len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"code\": \"UP009\", \"code_spans_and_lines\": [{\"line\": \"# -*- coding: utf-8 -*-\", \"span\": \"# -*- coding: utf-8 -*-\"}], \"fix\": {\"edits\": [{\"content\": \"\", \"code_spans_and_lines\": [{\"line\": \"# -*- coding: utf-8 -*-\", \"span\": \"\"}, {\"line\": \"\\\"\\\"\\\"DiabtetesPredictions.ipynb\", \"span\": \"\"}]}]}}\n"
     ]
    }
   ],
   "source": [
    "# print(filt_train_data[0]['messages'][0]['content'])\n",
    "print(shuffle_data(filt_train_data)[0]['messages'][1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./data/ruff_meta_linting/train_v3.json\", \"w\") as f:\n",
    "#     print(f\"train data len: {len(filt_train_data)}\")\n",
    "#     # print(dict(Counter([rec['source'] for rec in filt_train_data if rec['messages'][1]['content'] != 'NO VIOLATIONS FOUND']).most_common()))\n",
    "#     # print(dict(Counter([rec['source'] for rec in filt_train_data if rec['messages'][1]['content'] == 'NO VIOLATIONS FOUND']).most_common()))\n",
    "#     print(dict(Counter([rec['source'] for rec in filt_train_data]).most_common()))\n",
    "#     json.dump(shuffle_data(filt_train_data), f, indent=4)\n",
    "# with open(\"./data/ruff_meta_linting/test_v3.json\", \"w\") as f:\n",
    "#     print(f\"test data len: {len(filt_test_data)}\")\n",
    "#     # print(dict(Counter([rec['source'] for rec in filt_test_data if rec['messages'][1]['content'] != 'NO VIOLATIONS FOUND']).most_common()))\n",
    "#     # print(dict(Counter([rec['source'] for rec in filt_test_data if rec['messages'][1]['content'] == 'NO VIOLATIONS FOUND']).most_common()))\n",
    "#     print(dict(Counter([rec['source'] for rec in filt_test_data]).most_common()))\n",
    "#     json.dump(shuffle_data(filt_test_data), f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idiom Learnability/Hardness Experiment Data Creation\n",
    "Dataset with the same idioms in the train and test split (and same idiom mix as the original test set) to evaluate 3 things:\n",
    "1. is the task learnable/doable\n",
    "2. is the training framework working properly\n",
    "3. are some idioms harder to learn than others? and which one (also is it related to training frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F406', 'F403', 'F503', 'F602', 'F622'] 24595\n",
      "['E401', 'E702', 'E722', 'E731', 'E742'] 23983\n",
      "['ERA001', 'C901', 'I001', 'I002', 'BLE001'] 296494\n",
      "['ANN001', 'ANN002', 'ANN003', 'ANN201', 'ANN202'] 43702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ASYNC100', 'ASYNC105', 'ASYNC109', 'ASYNC110', 'ASYNC115'] 46\n",
      "['ASYNC116', 'ASYNC210', 'ASYNC220', 'ASYNC221', 'ASYNC222'] 89\n",
      "['ASYNC230', 'ASYNC251', 'ANN204', 'ANN205', 'ANN206'] 77455\n",
      "['S102', 'S103', 'S104', 'S105', 'S106'] 14756\n",
      "['S107', 'S108', 'S110', 'S112', 'S113'] 12301\n",
      "['S201', 'S202', 'S301', 'S302', 'S303'] 5357\n",
      "4727890\n"
     ]
    }
   ],
   "source": [
    "idiom_mixes = [\n",
    "    [\"F406\", \"F403\", \"F503\", \"F602\", \"F622\"],\n",
    "    [\"E401\", \"E702\", \"E722\", \"E731\", \"E742\"],\n",
    "    [\"ERA001\", \"C901\", \"I001\", \"I002\", \"BLE001\"],\n",
    "    [\"ANN001\", \"ANN002\", \"ANN003\", \"ANN201\", \"ANN202\"],\n",
    "    [\"ASYNC100\", \"ASYNC105\", \"ASYNC109\", \"ASYNC110\", \"ASYNC115\"],\n",
    "    [\"ASYNC116\", \"ASYNC210\", \"ASYNC220\", \"ASYNC221\", \"ASYNC222\"],\n",
    "    [\"ASYNC230\", \"ASYNC251\", \"ANN204\", \"ANN205\", \"ANN206\"],\n",
    "    [\"S102\", \"S103\", \"S104\", \"S105\", \"S106\"],\n",
    "    [\"S107\", \"S108\", \"S110\", \"S112\", \"S113\"],\n",
    "    [\"S201\", \"S202\", \"S301\", \"S302\", \"S303\"],\n",
    "]\n",
    "all_data = []\n",
    "random.seed(42)\n",
    "\n",
    "for idiom_mix in idiom_mixes:\n",
    "    mix_data = dataset.generate_data_mix(idiom_mix, max_code_lines=200)\n",
    "    print(idiom_mix, len([rec for rec in mix_data if rec['messages'][1]['content'] != 'NO VIOLATIONS FOUND']))\n",
    "    all_data.extend(mix_data)\n",
    "    \n",
    "print(len(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "from collections import defaultdict\n",
    "\n",
    "def create_random_split_of_given_size(data, split_1_size: int):\n",
    "    split_1, split_2 = [], []\n",
    "    split_1_indices = random.sample(range(len(data)), k=split_1_size)\n",
    "    put_in_split_1 = [False for _ in range(len(data))]\n",
    "    for index in split_1_indices:\n",
    "        put_in_split_1[index] = True\n",
    "    for index in range(len(data)):\n",
    "        if put_in_split_1[index]:\n",
    "            split_1.append(data[index])\n",
    "        else: split_2.append(data[index])\n",
    "\n",
    "    return split_1, split_2\n",
    "\n",
    "def impose_idiom_mix_ceilings_and_split_data(data, train_ceiling: int=5000, test_ceiling: int=500):\n",
    "    \"\"\"reduce size of data stratified by the idiom mix and violation or no violation category\"\"\"\n",
    "    category_to_data = defaultdict(lambda: [])\n",
    "    violation_present_counts = defaultdict(lambda: 0) # the counts of data with at least one violation present for the meta-task idioms.\n",
    "    for rec in data:\n",
    "        violation_present = \"yes\" if rec['messages'][1]['content'] != \"NO VIOLATIONS FOUND\" else \"no\"\n",
    "        category_to_data[rec['source']+\"-\"+violation_present].append(rec)\n",
    "    for category, data_subset in category_to_data.items():\n",
    "        if category.endswith(\"-yes\"):\n",
    "            violation_present_counts[category.replace(\"-yes\",\"\")] = len(data_subset)\n",
    "\n",
    "    category_to_data = dict(category_to_data)\n",
    "    final_train_data = []\n",
    "    final_test_data = []\n",
    "    for category, data_subset in category_to_data.items():\n",
    "        subset_max_possible_size = violation_present_counts[category.replace(\"-yes\",\"\").replace(\"-no\",\"\")]\n",
    "        # selected_data = random.sample(data_subset, k=min(len(data_subset), train_ceiling+test_ceiling))\n",
    "        selected_data = random.sample(data_subset, k=min(subset_max_possible_size, train_ceiling+test_ceiling))\n",
    "        data_selected_for_train, data_selected_for_test = create_random_split_of_given_size(\n",
    "            selected_data, split_1_size=min(5000, \n",
    "            int(train_ceiling*len(selected_data)/(train_ceiling+test_ceiling)))\n",
    "        )\n",
    "        final_train_data.extend(data_selected_for_train)\n",
    "        final_test_data.extend(data_selected_for_test)\n",
    "        # print(category, len(selected_data))\n",
    "    # print(len(final_data))\n",
    "    return final_train_data, final_test_data\n",
    "\n",
    "def impose_idiom_mix_ceilings(data, ceiling: int=5000):\n",
    "    \"\"\"reduce size of data stratified by the idiom mix and violation or no violation category\"\"\"\n",
    "    category_to_data = defaultdict(lambda: [])\n",
    "    violation_present_counts = defaultdict(lambda: 0) # the counts of data with at least one violation present for the meta-task idioms.\n",
    "    for rec in data:\n",
    "        violation_present = \"yes\" if rec['messages'][1]['content'] != \"NO VIOLATIONS FOUND\" else \"no\"\n",
    "        category_to_data[rec['source']+\"-\"+violation_present].append(rec)\n",
    "    for category, data_subset in category_to_data.items():\n",
    "        if category.endswith(\"-yes\"):\n",
    "            violation_present_counts[category.replace(\"-yes\",\"\")] = len(data_subset)\n",
    "\n",
    "    category_to_data = dict(category_to_data)\n",
    "    final_data = []\n",
    "    for category, data_subset in category_to_data.items():\n",
    "        subset_max_possible_size = violation_present_counts[category.replace(\"-yes\",\"\").replace(\"-no\",\"\")]\n",
    "        # selected_data = random.sample(data_subset, k=min(len(data_subset), train_ceiling+test_ceiling))\n",
    "        selected_data = random.sample(data_subset, k=min(subset_max_possible_size, ceiling))\n",
    "        final_data.extend(selected_data)\n",
    "        # print(category, len(selected_data))\n",
    "    # print(len(final_data))\n",
    "    return final_data\n",
    "\n",
    "# filt_train_data, filt_test_data = impose_idiom_mix_ceilings_and_split_data(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data len: 79982\n",
      "{'rull_linter/F406-F403-F503-F602-F622': 5000, 'rull_linter/E401-E702-E722-E731-E742': 5000, 'rull_linter/ERA001-C901-I001-I002-BLE001': 5000, 'rull_linter/ANN001-ANN002-ANN003-ANN201-ANN202': 5000, 'rull_linter/ASYNC230-ASYNC251-ANN204-ANN205-ANN206': 5000, 'rull_linter/S102-S103-S104-S105-S106': 5000, 'rull_linter/S107-S108-S110-S112-S113': 5000, 'rull_linter/S201-S202-S301-S302-S303': 4870, 'rull_linter/ASYNC116-ASYNC210-ASYNC220-ASYNC221-ASYNC222': 80, 'rull_linter/ASYNC100-ASYNC105-ASYNC109-ASYNC110-ASYNC115': 41}\n",
      "{'rull_linter/F406-F403-F503-F602-F622': 5000, 'rull_linter/E401-E702-E722-E731-E742': 5000, 'rull_linter/ERA001-C901-I001-I002-BLE001': 5000, 'rull_linter/ANN001-ANN002-ANN003-ANN201-ANN202': 5000, 'rull_linter/ASYNC230-ASYNC251-ANN204-ANN205-ANN206': 5000, 'rull_linter/S102-S103-S104-S105-S106': 5000, 'rull_linter/S107-S108-S110-S112-S113': 5000, 'rull_linter/S201-S202-S301-S302-S303': 4870, 'rull_linter/ASYNC116-ASYNC210-ASYNC220-ASYNC221-ASYNC222': 80, 'rull_linter/ASYNC100-ASYNC105-ASYNC109-ASYNC110-ASYNC115': 41}\n",
      "{'rull_linter/F406-F403-F503-F602-F622': 10000, 'rull_linter/E401-E702-E722-E731-E742': 10000, 'rull_linter/ERA001-C901-I001-I002-BLE001': 10000, 'rull_linter/ANN001-ANN002-ANN003-ANN201-ANN202': 10000, 'rull_linter/ASYNC230-ASYNC251-ANN204-ANN205-ANN206': 10000, 'rull_linter/S102-S103-S104-S105-S106': 10000, 'rull_linter/S107-S108-S110-S112-S113': 10000, 'rull_linter/S201-S202-S301-S302-S303': 9740, 'rull_linter/ASYNC116-ASYNC210-ASYNC220-ASYNC221-ASYNC222': 160, 'rull_linter/ASYNC100-ASYNC105-ASYNC109-ASYNC110-ASYNC115': 82}\n",
      "test data len: 8002\n",
      "{'rull_linter/F406-F403-F503-F602-F622': 500, 'rull_linter/E401-E702-E722-E731-E742': 500, 'rull_linter/ERA001-C901-I001-I002-BLE001': 500, 'rull_linter/ANN001-ANN002-ANN003-ANN201-ANN202': 500, 'rull_linter/ASYNC230-ASYNC251-ANN204-ANN205-ANN206': 500, 'rull_linter/S102-S103-S104-S105-S106': 500, 'rull_linter/S107-S108-S110-S112-S113': 500, 'rull_linter/S201-S202-S301-S302-S303': 487, 'rull_linter/ASYNC116-ASYNC210-ASYNC220-ASYNC221-ASYNC222': 9, 'rull_linter/ASYNC100-ASYNC105-ASYNC109-ASYNC110-ASYNC115': 5}\n",
      "{'rull_linter/F406-F403-F503-F602-F622': 500, 'rull_linter/E401-E702-E722-E731-E742': 500, 'rull_linter/ERA001-C901-I001-I002-BLE001': 500, 'rull_linter/ANN001-ANN002-ANN003-ANN201-ANN202': 500, 'rull_linter/ASYNC230-ASYNC251-ANN204-ANN205-ANN206': 500, 'rull_linter/S102-S103-S104-S105-S106': 500, 'rull_linter/S107-S108-S110-S112-S113': 500, 'rull_linter/S201-S202-S301-S302-S303': 487, 'rull_linter/ASYNC116-ASYNC210-ASYNC220-ASYNC221-ASYNC222': 9, 'rull_linter/ASYNC100-ASYNC105-ASYNC109-ASYNC110-ASYNC115': 5}\n",
      "{'rull_linter/F406-F403-F503-F602-F622': 1000, 'rull_linter/E401-E702-E722-E731-E742': 1000, 'rull_linter/ERA001-C901-I001-I002-BLE001': 1000, 'rull_linter/ANN001-ANN002-ANN003-ANN201-ANN202': 1000, 'rull_linter/ASYNC230-ASYNC251-ANN204-ANN205-ANN206': 1000, 'rull_linter/S102-S103-S104-S105-S106': 1000, 'rull_linter/S107-S108-S110-S112-S113': 1000, 'rull_linter/S201-S202-S301-S302-S303': 974, 'rull_linter/ASYNC116-ASYNC210-ASYNC220-ASYNC221-ASYNC222': 18, 'rull_linter/ASYNC100-ASYNC105-ASYNC109-ASYNC110-ASYNC115': 10}\n"
     ]
    }
   ],
   "source": [
    "filt_train_data, filt_test_data = impose_idiom_mix_ceilings_and_split_data(all_data)\n",
    "with open(\"./data/ruff_meta_linting/hardness_experiment/train.json\", \"w\") as f:\n",
    "    print(f\"train data len: {len(filt_train_data)}\")\n",
    "    print(dict(Counter([rec['source'] for rec in filt_train_data if rec['messages'][1]['content'] != 'NO VIOLATIONS FOUND']).most_common()))\n",
    "    print(dict(Counter([rec['source'] for rec in filt_train_data if rec['messages'][1]['content'] == 'NO VIOLATIONS FOUND']).most_common()))\n",
    "    print(dict(Counter([rec['source'] for rec in filt_train_data]).most_common()))\n",
    "    json.dump(shuffle_data(filt_train_data), f, indent=4)\n",
    "with open(\"./data/ruff_meta_linting/hardness_experiment/test.json\", \"w\") as f:\n",
    "    print(f\"test data len: {len(filt_test_data)}\")\n",
    "    print(dict(Counter([rec['source'] for rec in filt_test_data if rec['messages'][1]['content'] != 'NO VIOLATIONS FOUND']).most_common()))\n",
    "    print(dict(Counter([rec['source'] for rec in filt_test_data if rec['messages'][1]['content'] == 'NO VIOLATIONS FOUND']).most_common()))\n",
    "    print(dict(Counter([rec['source'] for rec in filt_test_data]).most_common()))\n",
    "    json.dump(shuffle_data(filt_test_data), f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idiom Transfer Learning\n",
    "Dataset with the some of the same idioms, some paired near transfer idioms and some unseen test only idioms/meta-tasks to evaluate tendency for memorization vs instruction following/reasoning in LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ERA001', 'C901', 'I001', 'I002', 'BLE001'] 296494\n",
      "10000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "train_only_idiom_mix = [\n",
    "    [\"F405\", \"F501\", \"F502\", \"F601\", \"F621\"],\n",
    "    [\"E402\", \"E701\", \"E721\", \"E741\", \"E743\"],\n",
    "    [\"N801\", \"N802\", \"N803\", \"N804\", \"N805\"],\n",
    "    [\"N806\", \"N807\", \"N811\", \"N812\", \"N813\"],\n",
    "    [\"UP001\", \"UP003\", \"UP004\", \"UP005\", \"UP006\"],\n",
    "    [\"UP007\", \"UP008\", \"UP009\", \"UP010\", \"UP011\"],\n",
    "    [\"UP044\", \"UP045\", \"UP046\", \"UP047\", \"UP040\"],\n",
    "    [\"B002\", \"B003\", \"B004\", \"B005\", \"B006\"],\n",
    "    [\"B007\", \"B008\", \"B009\", \"B010\", \"B012\"],\n",
    "]\n",
    "test_only_idiom_mix = [\n",
    "    [\"F406\", \"F403\", \"F503\", \"F602\", \"F622\"],\n",
    "    [\"E401\", \"E702\", \"E722\", \"E731\", \"E742\"],\n",
    "    [\"ANN001\", \"ANN002\", \"ANN003\", \"ANN201\", \"ANN202\"],\n",
    "    [\"ASYNC100\", \"ASYNC105\", \"ASYNC109\", \"ASYNC110\", \"ASYNC115\"],\n",
    "    [\"ASYNC116\", \"ASYNC210\", \"ASYNC220\", \"ASYNC221\", \"ASYNC222\"],\n",
    "    [\"ASYNC230\", \"ASYNC251\", \"ANN204\", \"ANN205\", \"ANN206\"],\n",
    "    [\"S102\", \"S103\", \"S104\", \"S105\", \"S106\"],\n",
    "    [\"S107\", \"S108\", \"S110\", \"S112\", \"S113\"],\n",
    "    [\"S201\", \"S202\", \"S301\", \"S302\", \"S303\"],\n",
    "]\n",
    "\n",
    "shared_data = [] # basically no-transfer setting/\n",
    "random.seed(42)\n",
    "\n",
    "shared_idiom_mix = [\"ERA001\", \"C901\", \"I001\", \"I002\", \"BLE001\"]\n",
    "shared_data = dataset.generate_data_mix(shared_idiom_mix, max_code_lines=200)\n",
    "print(shared_idiom_mix, len([rec for rec in shared_data if rec['messages'][1]['content'] != 'NO VIOLATIONS FOUND']))\n",
    "shared_train_data, shared_test_data = impose_idiom_mix_ceilings_and_split_data(shared_data)\n",
    "print(len(shared_train_data))\n",
    "print(len(shared_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del shared_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F405', 'F501', 'F502', 'F601', 'F621'] 5000\n",
      "['E402', 'E701', 'E721', 'E741', 'E743'] 5000\n",
      "['N801', 'N802', 'N803', 'N804', 'N805'] 5000\n",
      "['N806', 'N807', 'N811', 'N812', 'N813'] 5000\n",
      "['UP001', 'UP003', 'UP004', 'UP005', 'UP006'] 5000\n",
      "['UP007', 'UP008', 'UP009', 'UP010', 'UP011'] 5000\n",
      "['UP044', 'UP045', 'UP046', 'UP047', 'UP040'] 14\n",
      "['B002', 'B003', 'B004', 'B005', 'B006'] 3377\n",
      "['B007', 'B008', 'B009', 'B010', 'B012'] 5000\n",
      "['F406', 'F403', 'F503', 'F602', 'F622'] 500\n",
      "['E401', 'E702', 'E722', 'E731', 'E742'] 500\n",
      "['ANN001', 'ANN002', 'ANN003', 'ANN201', 'ANN202'] 500\n",
      "['ASYNC100', 'ASYNC105', 'ASYNC109', 'ASYNC110', 'ASYNC115'] 46\n",
      "['ASYNC116', 'ASYNC210', 'ASYNC220', 'ASYNC221', 'ASYNC222'] 89\n",
      "['ASYNC230', 'ASYNC251', 'ANN204', 'ANN205', 'ANN206'] 500\n",
      "['S102', 'S103', 'S104', 'S105', 'S106'] 500\n",
      "['S107', 'S108', 'S110', 'S112', 'S113'] 500\n",
      "['S201', 'S202', 'S301', 'S302', 'S303'] 500\n",
      "76782\n",
      "7270\n"
     ]
    }
   ],
   "source": [
    "train_only_data = []\n",
    "test_only_data = []\n",
    "\n",
    "for idiom_mix in train_only_idiom_mix:\n",
    "    mix_data = impose_idiom_mix_ceilings(dataset.generate_data_mix(idiom_mix, max_code_lines=200), ceiling=5000)\n",
    "    print(idiom_mix, len([rec for rec in mix_data if rec['messages'][1]['content'] != 'NO VIOLATIONS FOUND']))\n",
    "    train_only_data.extend(mix_data)\n",
    "    del mix_data\n",
    "for idiom_mix in test_only_idiom_mix:\n",
    "    mix_data = impose_idiom_mix_ceilings(dataset.generate_data_mix(idiom_mix, max_code_lines=200), ceiling=500)\n",
    "    print(idiom_mix, len([rec for rec in mix_data if rec['messages'][1]['content'] != 'NO VIOLATIONS FOUND']))\n",
    "    test_only_data.extend(mix_data)\n",
    "    del mix_data\n",
    "\n",
    "# train_only_data = impose_idiom_mix_ceilings(all_train_data, ceiling=5000)\n",
    "# del all_train_data\n",
    "# test_only_data = impose_idiom_mix_ceilings(all_test_data, ceiling=500)\n",
    "# del all_test_data\n",
    "\n",
    "print(len(train_only_data))\n",
    "print(len(test_only_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_train_data = train_only_data + shared_train_data\n",
    "filt_test_data = test_only_data + shared_test_data\n",
    "\n",
    "# # shuffle data\n",
    "# filt_train_data = random.sample(filt_train_data, k=len(filt_train_data))\n",
    "# filt_test_data = random.sample(filt_test_data, k=len(filt_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data len: 86782\n",
      "{'rull_linter/F405-F501-F502-F601-F621': 5000, 'rull_linter/E402-E701-E721-E741-E743': 5000, 'rull_linter/N801-N802-N803-N804-N805': 5000, 'rull_linter/N806-N807-N811-N812-N813': 5000, 'rull_linter/UP001-UP003-UP004-UP005-UP006': 5000, 'rull_linter/UP007-UP008-UP009-UP010-UP011': 5000, 'rull_linter/B007-B008-B009-B010-B012': 5000, 'rull_linter/ERA001-C901-I001-I002-BLE001': 5000, 'rull_linter/B002-B003-B004-B005-B006': 3377, 'rull_linter/UP044-UP045-UP046-UP047-UP040': 14}\n",
      "{'rull_linter/F405-F501-F502-F601-F621': 5000, 'rull_linter/E402-E701-E721-E741-E743': 5000, 'rull_linter/N801-N802-N803-N804-N805': 5000, 'rull_linter/N806-N807-N811-N812-N813': 5000, 'rull_linter/UP001-UP003-UP004-UP005-UP006': 5000, 'rull_linter/UP007-UP008-UP009-UP010-UP011': 5000, 'rull_linter/B007-B008-B009-B010-B012': 5000, 'rull_linter/ERA001-C901-I001-I002-BLE001': 5000, 'rull_linter/B002-B003-B004-B005-B006': 3377, 'rull_linter/UP044-UP045-UP046-UP047-UP040': 14}\n",
      "{'rull_linter/F405-F501-F502-F601-F621': 10000, 'rull_linter/E402-E701-E721-E741-E743': 10000, 'rull_linter/N801-N802-N803-N804-N805': 10000, 'rull_linter/N806-N807-N811-N812-N813': 10000, 'rull_linter/UP001-UP003-UP004-UP005-UP006': 10000, 'rull_linter/UP007-UP008-UP009-UP010-UP011': 10000, 'rull_linter/B007-B008-B009-B010-B012': 10000, 'rull_linter/ERA001-C901-I001-I002-BLE001': 10000, 'rull_linter/B002-B003-B004-B005-B006': 6754, 'rull_linter/UP044-UP045-UP046-UP047-UP040': 28}\n",
      "test data len: 8270\n",
      "{'rull_linter/F406-F403-F503-F602-F622': 500, 'rull_linter/E401-E702-E722-E731-E742': 500, 'rull_linter/ANN001-ANN002-ANN003-ANN201-ANN202': 500, 'rull_linter/ASYNC230-ASYNC251-ANN204-ANN205-ANN206': 500, 'rull_linter/S102-S103-S104-S105-S106': 500, 'rull_linter/S107-S108-S110-S112-S113': 500, 'rull_linter/S201-S202-S301-S302-S303': 500, 'rull_linter/ERA001-C901-I001-I002-BLE001': 500, 'rull_linter/ASYNC116-ASYNC210-ASYNC220-ASYNC221-ASYNC222': 89, 'rull_linter/ASYNC100-ASYNC105-ASYNC109-ASYNC110-ASYNC115': 46}\n",
      "{'rull_linter/F406-F403-F503-F602-F622': 500, 'rull_linter/E401-E702-E722-E731-E742': 500, 'rull_linter/ANN001-ANN002-ANN003-ANN201-ANN202': 500, 'rull_linter/ASYNC230-ASYNC251-ANN204-ANN205-ANN206': 500, 'rull_linter/S102-S103-S104-S105-S106': 500, 'rull_linter/S107-S108-S110-S112-S113': 500, 'rull_linter/S201-S202-S301-S302-S303': 500, 'rull_linter/ERA001-C901-I001-I002-BLE001': 500, 'rull_linter/ASYNC116-ASYNC210-ASYNC220-ASYNC221-ASYNC222': 89, 'rull_linter/ASYNC100-ASYNC105-ASYNC109-ASYNC110-ASYNC115': 46}\n",
      "{'rull_linter/F406-F403-F503-F602-F622': 1000, 'rull_linter/E401-E702-E722-E731-E742': 1000, 'rull_linter/ANN001-ANN002-ANN003-ANN201-ANN202': 1000, 'rull_linter/ASYNC230-ASYNC251-ANN204-ANN205-ANN206': 1000, 'rull_linter/S102-S103-S104-S105-S106': 1000, 'rull_linter/S107-S108-S110-S112-S113': 1000, 'rull_linter/S201-S202-S301-S302-S303': 1000, 'rull_linter/ERA001-C901-I001-I002-BLE001': 1000, 'rull_linter/ASYNC116-ASYNC210-ASYNC220-ASYNC221-ASYNC222': 178, 'rull_linter/ASYNC100-ASYNC105-ASYNC109-ASYNC110-ASYNC115': 92}\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/ruff_meta_linting/train_v4.json\", \"w\") as f:\n",
    "    print(f\"train data len: {len(filt_train_data)}\")\n",
    "    print(dict(Counter([rec['source'] for rec in filt_train_data if rec['messages'][1]['content'] != 'NO VIOLATIONS FOUND']).most_common()))\n",
    "    print(dict(Counter([rec['source'] for rec in filt_train_data if rec['messages'][1]['content'] == 'NO VIOLATIONS FOUND']).most_common()))\n",
    "    print(dict(Counter([rec['source'] for rec in filt_train_data]).most_common()))\n",
    "    json.dump(shuffle_data(filt_train_data), f, indent=4)\n",
    "with open(\"./data/ruff_meta_linting/test_v4.json\", \"w\") as f:\n",
    "    print(f\"test data len: {len(filt_test_data)}\")\n",
    "    print(dict(Counter([rec['source'] for rec in filt_test_data if rec['messages'][1]['content'] != 'NO VIOLATIONS FOUND']).most_common()))\n",
    "    print(dict(Counter([rec['source'] for rec in filt_test_data if rec['messages'][1]['content'] == 'NO VIOLATIONS FOUND']).most_common()))\n",
    "    print(dict(Counter([rec['source'] for rec in filt_test_data]).most_common()))\n",
    "    json.dump(shuffle_data(filt_test_data), f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
