{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# module_path = str(pathlib.Path(os.path.abspath(__file__)).parent.parent)\n",
    "# sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arnaik/OracleProject\n"
     ]
    }
   ],
   "source": [
    "cd /home/arnaik/OracleProject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/arnaik/OracleProject\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datautils import load_stack_dump, load_ruff_results, load_ruff_idiom_specs, idiom_spec_extractor_for_ruff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112it [00:14,  7.65it/s]\n"
     ]
    }
   ],
   "source": [
    "stack_data = load_stack_dump(\"data/STACK-V2\", as_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "94it [00:12,  7.79it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatautils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_stack_dump, load_ruff_results, load_ruff_idiom_specs, idiom_spec_extractor_for_ruff\n\u001b[0;32m----> 3\u001b[0m stack_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_stack_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/STACK-V2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m code_idiom_specs \u001b[38;5;241m=\u001b[39m load_ruff_idiom_specs(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/ruff_pages\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m ruff_results \u001b[38;5;241m=\u001b[39m load_ruff_results(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/ruff_results\u001b[39m\u001b[38;5;124m\"\u001b[39m, as_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/OracleProject/src/datautils.py:88\u001b[0m, in \u001b[0;36mload_stack_dump\u001b[0;34m(folder, folds, as_dict)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,file \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(folder))):\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m folds \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m folds: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mread_jsonl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m     90\u001b[0m         unique_data[row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblob_id\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m row\n",
      "File \u001b[0;32m~/OracleProject/src/datautils.py:138\u001b[0m, in \u001b[0;36mread_jsonl\u001b[0;34m(path, disable)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m tqdm(f, disable\u001b[38;5;241m=\u001b[39mdisable):\n\u001b[0;32m--> 138\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend(\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/py3.13/lib/python3.13/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/anaconda3/envs/py3.13/lib/python3.13/json/decoder.py:345\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    341\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 345\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/anaconda3/envs/py3.13/lib/python3.13/json/decoder.py:361\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    358\u001b[0m \n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 361\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "code_idiom_specs = load_ruff_idiom_specs(\"./data/ruff_pages\")\n",
    "ruff_results = load_ruff_results(\"data/ruff_results\", as_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "META_LINTING_PROMPT_V2 = \"\"\"Look at the following list of code idiom specifications with definitions and examples:\n",
    "{LIST_OF_IDIOM_SPECS}\n",
    "\n",
    "Given these idioms, your task is to look at a code file and detect violations of the above idioms, and flag them like a linter. You should also suggest a fix if possible. Report the results per idiom specification mentioned above and just say 'NO VIOLATIONS FOUND' if no violations are found for a given idiom. Do not detect any idioms not specified above.\n",
    "\n",
    "Code file:\n",
    "{CODE_FILE}\n",
    "\n",
    "Violations per idiom:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_from_violations(violations, stack_file_lines: list[str], meta_task_idiom_codes, include_message: bool=False, add_line_numbers: bool=False):\n",
    "    filt_violations = [violation for violation in violations if violation['code'] in meta_task_idiom_codes]\n",
    "    grouped_violations = {code: [] for code in meta_task_idiom_codes}\n",
    "    # group violations by each idiom in the meta-task.\n",
    "    for violation in filt_violations:\n",
    "        grouped_violations[violation['code']].append(violation)\n",
    "    # sort violations by start position.\n",
    "    response = \"\"\n",
    "    for code, violations in grouped_violations.items():\n",
    "        grouped_violations[code] = sorted(violations, key=lambda x: (x['location']['row'], x['location']['column'])) \n",
    "        if len(violations) == 0:\n",
    "            response += f\"**Idiom {code} Violations:**\\n\\nNO VIOLATIONS FOUND\\n\\n\"\n",
    "        else: \n",
    "            response += f\"**Idiom {code} Violations:**\\n\"\n",
    "            for num, violation in enumerate(violations):\n",
    "                if include_message:\n",
    "                    det_dict = {\"line\": \"\", \"span\": \"\", \"message\": violation[\"message\"], \"fix\": None}\n",
    "                else: det_dict = {\"line\": \"\", \"span\": \"\", \"fix\": None}\n",
    "                det_line = []\n",
    "                det_span = []\n",
    "                edits = []\n",
    "\n",
    "                for lineno in range(violation['location']['row'], violation['end_location']['row']+1):\n",
    "                    # print(stack_file_lines[lineno-1])\n",
    "                    line = stack_file_lines[lineno-1]\n",
    "\n",
    "                    if add_line_numbers:\n",
    "                        det_line.append(f\"{str(lineno).rjust(3)} {line}\")\n",
    "                    else: det_line.append(f\"{line}\")\n",
    "                    # populate span.\n",
    "                    span_line = line\n",
    "                    if lineno == violation['location']['row'] and lineno == violation['end_location']['row']:\n",
    "                        span_line = line[violation[\"location\"][\"column\"]-1:violation[\"end_location\"][\"column\"]-1]\n",
    "                    elif lineno == violation['location']['row']: # start line for multi-line span.\n",
    "                        span_line = line[violation[\"location\"][\"column\"]-1:]\n",
    "                    elif lineno == violation['end_location']['row']: # end line for multi-line span.\n",
    "                        span_line = line[:violation[\"end_location\"][\"column\"]-1]\n",
    "                    else: # intermediate line for multi-line span.\n",
    "                        span_line = line\n",
    "                    det_span.append(span_line)\n",
    "                det_dict[\"line\"] = \"\\n\".join(det_line)\n",
    "                det_dict[\"span\"] = \"\\n\".join(det_span)\n",
    "                if violation[\"fix\"] is not None and violation[\"fix\"][\"applicability\"] == \"safe\":\n",
    "                    for edit in violation[\"fix\"][\"edits\"]:\n",
    "                        before_span = []\n",
    "                        after_span = edit[\"content\"]\n",
    "                        for lineno in range(edit[\"location\"][\"row\"], edit[\"end_location\"][\"row\"]+1):\n",
    "                            # print(violation[\"fix\"])\n",
    "                            line = stack_file_lines[lineno-1]\n",
    "                            # populate span.\n",
    "                            span_line = line\n",
    "                            if lineno == edit['location']['row'] and lineno == edit['end_location']['row']:\n",
    "                                span_line = line[edit[\"location\"][\"column\"]-1:edit[\"end_location\"][\"column\"]-1]\n",
    "                            elif lineno == edit['location']['row']: # start line for multi-line span.\n",
    "                                span_line = line[edit[\"location\"][\"column\"]-1:]\n",
    "                            elif lineno == edit['end_location']['row']: # end line for multi-line span.\n",
    "                                span_line = line[:edit[\"end_location\"][\"column\"]-1]\n",
    "                            else: # intermediate line for multi-line span.\n",
    "                                span_line = line\n",
    "                            before_span.append(span_line)\n",
    "\n",
    "                        before_span = \"\\n\".join(before_span)\n",
    "                        edits.append({\"before\": before_span, \"after\": after_span})\n",
    "                    det_dict[\"fix\"] = edits\n",
    "\n",
    "                response += f\"\\n{json.dumps(det_dict)}\"\n",
    "            response += \"\\n\\n\"\n",
    "\n",
    "    return response\n",
    "\n",
    "def reprocess_data(train_data, code_idiom_specs: dict, ruff_results: dict, stack_data: dict, add_line_numbers: bool=True):\n",
    "    for rec in tqdm(train_data):\n",
    "        blob_id = rec[\"id\"].split(\"_\")[-1].strip()\n",
    "        meta_task_idiom_codes = rec[\"id\"].split(\"_\")[0].strip().split(\"-\")\n",
    "        stack_file = stack_data[blob_id]['content']\n",
    "        stack_file_lines = stack_data[blob_id]['content'].split(\"\\n\")\n",
    "        violations = ruff_results[blob_id]['violations']\n",
    "\n",
    "        response = generate_response_from_violations(\n",
    "            violations=violations, \n",
    "            stack_file_lines=stack_file_lines, \n",
    "            meta_task_idiom_codes=meta_task_idiom_codes,\n",
    "            add_line_numbers=add_line_numbers\n",
    "        )\n",
    "\n",
    "        stack_file_with_lineno = []\n",
    "        if add_line_numbers:\n",
    "            for lineno, line in enumerate(stack_file.split(\"\\n\")):\n",
    "                stack_file_with_lineno.append(f\"{str(lineno+1).rjust(3)} {line}\")\n",
    "        \n",
    "        if add_line_numbers:\n",
    "            CODE_FILE = \"\\n\".join(stack_file_with_lineno)\n",
    "        else: CODE_FILE = stack_file\n",
    "        LIST_OF_IDIOM_SPECS = \"\\n\\n\".join([idiom_spec_extractor_for_ruff(code_idiom_specs[idiom_code]) for idiom_code in meta_task_idiom_codes])\n",
    "\n",
    "        rec[\"messages\"][0]['content'] = META_LINTING_PROMPT_V2.format(LIST_OF_IDIOM_SPECS=LIST_OF_IDIOM_SPECS, CODE_FILE=CODE_FILE)\n",
    "        rec[\"messages\"][1]['content'] = response\n",
    "\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 86782/86782 [00:04<00:00, 17852.61it/s]\n"
     ]
    }
   ],
   "source": [
    "split = \"train\"\n",
    "data = json.load(open(f\"./data/ruff_meta_linting/{split}_v4.json\"))\n",
    "\n",
    "proc_data = reprocess_data(train_data=data, code_idiom_specs=code_idiom_specs, ruff_results=ruff_results, stack_data=stack_data, add_line_numbers=True)\n",
    "with open(f\"./data/ruff_meta_linting/{split}_v4_new_format_with_lineno.json\", \"w\") as f:\n",
    "    json.dump(proc_data, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
