{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# module_path = str(pathlib.Path(os.path.abspath(__file__)).parent.parent)\n",
    "# sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arnaik/OracleProject\n"
     ]
    }
   ],
   "source": [
    "cd /home/arnaik/OracleProject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/arnaik/OracleProject\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arnaik/anaconda3/envs/py3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.datautils import load_stack_dump, load_ruff_results, load_ruff_idiom_specs, idiom_spec_extractor_for_ruff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112it [00:14,  7.58it/s]\n"
     ]
    }
   ],
   "source": [
    "stack_data = load_stack_dump(\"data/STACK-V2\", as_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 129/129 [03:36<00:00,  1.68s/it]\n"
     ]
    }
   ],
   "source": [
    "code_idiom_specs = load_ruff_idiom_specs(\"./data/ruff_pages\")\n",
    "ruff_results = load_ruff_results(\"data/ruff_results\", as_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# META_LINTING_PROMPT_V2 = \"\"\"Look at the following list of code idiom specifications with definitions and examples:\n",
    "# {LIST_OF_IDIOM_SPECS}\n",
    "\n",
    "# Given these idioms, your task is to look at a code file and detect violations of the above idioms, and flag them like a linter. You should also suggest a fix if possible. Report the results per idiom specification mentioned above and just say 'NO VIOLATIONS FOUND' if no violations are found for a given idiom. Do not detect any idioms not specified above.\n",
    "\n",
    "# Code file:\n",
    "# {CODE_FILE}\n",
    "\n",
    "# Violations per idiom:\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_response_from_violations(violations, stack_file_lines: list[str], meta_task_idiom_codes, include_message: bool=False, add_line_numbers: bool=False):\n",
    "#     filt_violations = [violation for violation in violations if violation['code'] in meta_task_idiom_codes]\n",
    "#     grouped_violations = {code: [] for code in meta_task_idiom_codes}\n",
    "#     # group violations by each idiom in the meta-task.\n",
    "#     for violation in filt_violations:\n",
    "#         grouped_violations[violation['code']].append(violation)\n",
    "#     # sort violations by start position.\n",
    "#     response = \"\"\n",
    "#     for code, violations in grouped_violations.items():\n",
    "#         grouped_violations[code] = sorted(violations, key=lambda x: (x['location']['row'], x['location']['column'])) \n",
    "#         if len(violations) == 0:\n",
    "#             response += f\"**Idiom {code} Violations:**\\n\\nNO VIOLATIONS FOUND\\n\\n\"\n",
    "#         else: \n",
    "#             response += f\"**Idiom {code} Violations:**\\n\"\n",
    "#             for num, violation in enumerate(violations):\n",
    "#                 if include_message:\n",
    "#                     det_dict = {\"line\": \"\", \"span\": \"\", \"message\": violation[\"message\"], \"fix\": None}\n",
    "#                 else: det_dict = {\"line\": \"\", \"span\": \"\", \"fix\": None}\n",
    "#                 det_line = []\n",
    "#                 det_span = []\n",
    "#                 edits = []\n",
    "\n",
    "#                 for lineno in range(violation['location']['row'], violation['end_location']['row']+1):\n",
    "#                     # print(stack_file_lines[lineno-1])\n",
    "#                     line = stack_file_lines[lineno-1]\n",
    "\n",
    "#                     if add_line_numbers:\n",
    "#                         det_line.append(f\"{str(lineno).rjust(3)} {line}\")\n",
    "#                     else: det_line.append(f\"{line}\")\n",
    "#                     # populate span.\n",
    "#                     span_line = line\n",
    "#                     if lineno == violation['location']['row'] and lineno == violation['end_location']['row']:\n",
    "#                         span_line = line[violation[\"location\"][\"column\"]-1:violation[\"end_location\"][\"column\"]-1]\n",
    "#                     elif lineno == violation['location']['row']: # start line for multi-line span.\n",
    "#                         span_line = line[violation[\"location\"][\"column\"]-1:]\n",
    "#                     elif lineno == violation['end_location']['row']: # end line for multi-line span.\n",
    "#                         span_line = line[:violation[\"end_location\"][\"column\"]-1]\n",
    "#                     else: # intermediate line for multi-line span.\n",
    "#                         span_line = line\n",
    "#                     det_span.append(span_line)\n",
    "#                 det_dict[\"line\"] = \"\\n\".join(det_line)\n",
    "#                 det_dict[\"span\"] = \"\\n\".join(det_span)\n",
    "#                 if violation[\"fix\"] is not None and violation[\"fix\"][\"applicability\"] == \"safe\":\n",
    "#                     for edit in violation[\"fix\"][\"edits\"]:\n",
    "#                         before_span = []\n",
    "#                         after_span = edit[\"content\"]\n",
    "#                         for lineno in range(edit[\"location\"][\"row\"], edit[\"end_location\"][\"row\"]+1):\n",
    "#                             # print(violation[\"fix\"])\n",
    "#                             line = stack_file_lines[lineno-1]\n",
    "#                             # populate span.\n",
    "#                             span_line = line\n",
    "#                             if lineno == edit['location']['row'] and lineno == edit['end_location']['row']:\n",
    "#                                 span_line = line[edit[\"location\"][\"column\"]-1:edit[\"end_location\"][\"column\"]-1]\n",
    "#                             elif lineno == edit['location']['row']: # start line for multi-line span.\n",
    "#                                 span_line = line[edit[\"location\"][\"column\"]-1:]\n",
    "#                             elif lineno == edit['end_location']['row']: # end line for multi-line span.\n",
    "#                                 span_line = line[:edit[\"end_location\"][\"column\"]-1]\n",
    "#                             else: # intermediate line for multi-line span.\n",
    "#                                 span_line = line\n",
    "#                             before_span.append(span_line)\n",
    "\n",
    "#                         before_span = \"\\n\".join(before_span)\n",
    "#                         edits.append({\"before\": before_span, \"after\": after_span})\n",
    "#                     det_dict[\"fix\"] = edits\n",
    "\n",
    "#                 response += f\"\\n{json.dumps(det_dict)}\"\n",
    "#             response += \"\\n\\n\"\n",
    "\n",
    "#     return response\n",
    "\n",
    "# def reprocess_data(train_data, code_idiom_specs: dict, ruff_results: dict, stack_data: dict, add_line_numbers: bool=True):\n",
    "#     for rec in tqdm(train_data):\n",
    "#         blob_id = rec[\"id\"].split(\"_\")[-1].strip()\n",
    "#         meta_task_idiom_codes = rec[\"id\"].split(\"_\")[0].strip().split(\"-\")\n",
    "#         stack_file = stack_data[blob_id]['content']\n",
    "#         stack_file_lines = stack_data[blob_id]['content'].split(\"\\n\")\n",
    "#         violations = ruff_results[blob_id]['violations']\n",
    "\n",
    "#         response = generate_response_from_violations(\n",
    "#             violations=violations, \n",
    "#             stack_file_lines=stack_file_lines, \n",
    "#             meta_task_idiom_codes=meta_task_idiom_codes,\n",
    "#             add_line_numbers=add_line_numbers\n",
    "#         )\n",
    "\n",
    "#         stack_file_with_lineno = []\n",
    "#         if add_line_numbers:\n",
    "#             for lineno, line in enumerate(stack_file.split(\"\\n\")):\n",
    "#                 stack_file_with_lineno.append(f\"{str(lineno+1).rjust(3)} {line}\")\n",
    "        \n",
    "#         if add_line_numbers:\n",
    "#             CODE_FILE = \"\\n\".join(stack_file_with_lineno)\n",
    "#         else: CODE_FILE = stack_file\n",
    "#         LIST_OF_IDIOM_SPECS = \"\\n\\n\".join([idiom_spec_extractor_for_ruff(code_idiom_specs[idiom_code]) for idiom_code in meta_task_idiom_codes])\n",
    "\n",
    "#         rec[\"messages\"][0]['content'] = META_LINTING_PROMPT_V2.format(LIST_OF_IDIOM_SPECS=LIST_OF_IDIOM_SPECS, CODE_FILE=CODE_FILE)\n",
    "#         rec[\"messages\"][1]['content'] = response\n",
    "\n",
    "#     return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datautils import reprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/8270 [00:00<00:01, 5974.79it/s]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'code' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m split \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/ruff_meta_linting/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_v4.json\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m----> 4\u001b[0m proc_data \u001b[38;5;241m=\u001b[39m \u001b[43mreprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_idiom_specs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_idiom_specs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mruff_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mruff_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstack_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstack_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_line_numbers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/ruff_meta_linting/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_v4_new_format_with_lineno.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      6\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(proc_data, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[0;32m~/OracleProject/src/datautils.py:411\u001b[0m, in \u001b[0;36mreprocess_data\u001b[0;34m(train_data, code_idiom_specs, ruff_results, stack_data, add_line_numbers)\u001b[0m\n\u001b[1;32m    408\u001b[0m stack_file_lines \u001b[38;5;241m=\u001b[39m stack_data[blob_id][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    409\u001b[0m violations \u001b[38;5;241m=\u001b[39m ruff_results[blob_id][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mviolations\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 411\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_response_from_violations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mviolations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mviolations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstack_file_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstack_file_lines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeta_task_idiom_codes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta_task_idiom_codes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_line_numbers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_line_numbers\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m stack_file_with_lineno \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m add_line_numbers:\n",
      "File \u001b[0;32m~/OracleProject/src/datautils.py:337\u001b[0m, in \u001b[0;36mgenerate_response_from_violations\u001b[0;34m(violations, stack_file_lines, meta_task_idiom_codes, include_message, add_line_numbers)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_response_from_violations\u001b[39m(violations, stack_file_lines: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m], meta_task_idiom_codes, include_message: \u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, add_line_numbers: \u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 337\u001b[0m     filt_violations \u001b[38;5;241m=\u001b[39m [violation \u001b[38;5;28;01mfor\u001b[39;00m violation \u001b[38;5;129;01min\u001b[39;00m violations \u001b[38;5;28;01mif\u001b[39;00m violation[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m meta_task_idiom_codes \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcode\u001b[49m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mANN001\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mANN201\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m    338\u001b[0m     grouped_violations \u001b[38;5;241m=\u001b[39m {code: [] \u001b[38;5;28;01mfor\u001b[39;00m code \u001b[38;5;129;01min\u001b[39;00m meta_task_idiom_codes \u001b[38;5;28;01mif\u001b[39;00m code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mANN001\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mANN201\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;66;03m# group violations by each idiom in the meta-task.\u001b[39;00m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'code' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "split = \"test\"\n",
    "data = json.load(open(f\"./data/ruff_meta_linting/{split}_v4.json\"))\n",
    "\n",
    "proc_data = reprocess_data(train_data=data, code_idiom_specs=code_idiom_specs, ruff_results=ruff_results, stack_data=stack_data, add_line_numbers=True)\n",
    "with open(f\"./data/ruff_meta_linting/{split}_v4_new_format_with_lineno.json\", \"w\") as f:\n",
    "    json.dump(proc_data, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
