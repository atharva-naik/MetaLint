{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# module_path = str(pathlib.Path(os.path.abspath(__file__)).parent.parent)\n",
    "# sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arnaik/OracleProject\n"
     ]
    }
   ],
   "source": [
    "cd /home/arnaik/OracleProject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/arnaik/OracleProject\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arnaik/anaconda3/envs/py3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.datautils import load_stack_dump, load_ruff_results, load_ruff_idiom_specs, idiom_spec_extractor_for_ruff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112it [00:49,  2.28it/s]\n"
     ]
    }
   ],
   "source": [
    "stack_data = load_stack_dump(\"data/STACK-V2\", as_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 12/129 [00:21<04:18,  2.21s/it]"
     ]
    }
   ],
   "source": [
    "code_idiom_specs = load_ruff_idiom_specs(\"./data/ruff_pages\")\n",
    "ruff_results = load_ruff_results(\"data/ruff_results\", as_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# META_LINTING_PROMPT_V2 = \"\"\"Look at the following list of code idiom specifications with definitions and examples:\n",
    "# {LIST_OF_IDIOM_SPECS}\n",
    "\n",
    "# Given these idioms, your task is to look at a code file and detect violations of the above idioms, and flag them like a linter. You should also suggest a fix if possible. Report the results per idiom specification mentioned above and just say 'NO VIOLATIONS FOUND' if no violations are found for a given idiom. Do not detect any idioms not specified above.\n",
    "\n",
    "# Code file:\n",
    "# {CODE_FILE}\n",
    "\n",
    "# Violations per idiom:\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_response_from_violations(violations, stack_file_lines: list[str], meta_task_idiom_codes, include_message: bool=False, add_line_numbers: bool=False):\n",
    "#     filt_violations = [violation for violation in violations if violation['code'] in meta_task_idiom_codes]\n",
    "#     grouped_violations = {code: [] for code in meta_task_idiom_codes}\n",
    "#     # group violations by each idiom in the meta-task.\n",
    "#     for violation in filt_violations:\n",
    "#         grouped_violations[violation['code']].append(violation)\n",
    "#     # sort violations by start position.\n",
    "#     response = \"\"\n",
    "#     for code, violations in grouped_violations.items():\n",
    "#         grouped_violations[code] = sorted(violations, key=lambda x: (x['location']['row'], x['location']['column'])) \n",
    "#         if len(violations) == 0:\n",
    "#             response += f\"**Idiom {code} Violations:**\\n\\nNO VIOLATIONS FOUND\\n\\n\"\n",
    "#         else: \n",
    "#             response += f\"**Idiom {code} Violations:**\\n\"\n",
    "#             for num, violation in enumerate(violations):\n",
    "#                 if include_message:\n",
    "#                     det_dict = {\"line\": \"\", \"span\": \"\", \"message\": violation[\"message\"], \"fix\": None}\n",
    "#                 else: det_dict = {\"line\": \"\", \"span\": \"\", \"fix\": None}\n",
    "#                 det_line = []\n",
    "#                 det_span = []\n",
    "#                 edits = []\n",
    "\n",
    "#                 for lineno in range(violation['location']['row'], violation['end_location']['row']+1):\n",
    "#                     # print(stack_file_lines[lineno-1])\n",
    "#                     line = stack_file_lines[lineno-1]\n",
    "\n",
    "#                     if add_line_numbers:\n",
    "#                         det_line.append(f\"{str(lineno).rjust(3)} {line}\")\n",
    "#                     else: det_line.append(f\"{line}\")\n",
    "#                     # populate span.\n",
    "#                     span_line = line\n",
    "#                     if lineno == violation['location']['row'] and lineno == violation['end_location']['row']:\n",
    "#                         span_line = line[violation[\"location\"][\"column\"]-1:violation[\"end_location\"][\"column\"]-1]\n",
    "#                     elif lineno == violation['location']['row']: # start line for multi-line span.\n",
    "#                         span_line = line[violation[\"location\"][\"column\"]-1:]\n",
    "#                     elif lineno == violation['end_location']['row']: # end line for multi-line span.\n",
    "#                         span_line = line[:violation[\"end_location\"][\"column\"]-1]\n",
    "#                     else: # intermediate line for multi-line span.\n",
    "#                         span_line = line\n",
    "#                     det_span.append(span_line)\n",
    "#                 det_dict[\"line\"] = \"\\n\".join(det_line)\n",
    "#                 det_dict[\"span\"] = \"\\n\".join(det_span)\n",
    "#                 if violation[\"fix\"] is not None and violation[\"fix\"][\"applicability\"] == \"safe\":\n",
    "#                     for edit in violation[\"fix\"][\"edits\"]:\n",
    "#                         before_span = []\n",
    "#                         after_span = edit[\"content\"]\n",
    "#                         for lineno in range(edit[\"location\"][\"row\"], edit[\"end_location\"][\"row\"]+1):\n",
    "#                             # print(violation[\"fix\"])\n",
    "#                             line = stack_file_lines[lineno-1]\n",
    "#                             # populate span.\n",
    "#                             span_line = line\n",
    "#                             if lineno == edit['location']['row'] and lineno == edit['end_location']['row']:\n",
    "#                                 span_line = line[edit[\"location\"][\"column\"]-1:edit[\"end_location\"][\"column\"]-1]\n",
    "#                             elif lineno == edit['location']['row']: # start line for multi-line span.\n",
    "#                                 span_line = line[edit[\"location\"][\"column\"]-1:]\n",
    "#                             elif lineno == edit['end_location']['row']: # end line for multi-line span.\n",
    "#                                 span_line = line[:edit[\"end_location\"][\"column\"]-1]\n",
    "#                             else: # intermediate line for multi-line span.\n",
    "#                                 span_line = line\n",
    "#                             before_span.append(span_line)\n",
    "\n",
    "#                         before_span = \"\\n\".join(before_span)\n",
    "#                         edits.append({\"before\": before_span, \"after\": after_span})\n",
    "#                     det_dict[\"fix\"] = edits\n",
    "\n",
    "#                 response += f\"\\n{json.dumps(det_dict)}\"\n",
    "#             response += \"\\n\\n\"\n",
    "\n",
    "#     return response\n",
    "\n",
    "# def reprocess_data(train_data, code_idiom_specs: dict, ruff_results: dict, stack_data: dict, add_line_numbers: bool=True):\n",
    "#     for rec in tqdm(train_data):\n",
    "#         blob_id = rec[\"id\"].split(\"_\")[-1].strip()\n",
    "#         meta_task_idiom_codes = rec[\"id\"].split(\"_\")[0].strip().split(\"-\")\n",
    "#         stack_file = stack_data[blob_id]['content']\n",
    "#         stack_file_lines = stack_data[blob_id]['content'].split(\"\\n\")\n",
    "#         violations = ruff_results[blob_id]['violations']\n",
    "\n",
    "#         response = generate_response_from_violations(\n",
    "#             violations=violations, \n",
    "#             stack_file_lines=stack_file_lines, \n",
    "#             meta_task_idiom_codes=meta_task_idiom_codes,\n",
    "#             add_line_numbers=add_line_numbers\n",
    "#         )\n",
    "\n",
    "#         stack_file_with_lineno = []\n",
    "#         if add_line_numbers:\n",
    "#             for lineno, line in enumerate(stack_file.split(\"\\n\")):\n",
    "#                 stack_file_with_lineno.append(f\"{str(lineno+1).rjust(3)} {line}\")\n",
    "        \n",
    "#         if add_line_numbers:\n",
    "#             CODE_FILE = \"\\n\".join(stack_file_with_lineno)\n",
    "#         else: CODE_FILE = stack_file\n",
    "#         LIST_OF_IDIOM_SPECS = \"\\n\\n\".join([idiom_spec_extractor_for_ruff(code_idiom_specs[idiom_code]) for idiom_code in meta_task_idiom_codes])\n",
    "\n",
    "#         rec[\"messages\"][0]['content'] = META_LINTING_PROMPT_V2.format(LIST_OF_IDIOM_SPECS=LIST_OF_IDIOM_SPECS, CODE_FILE=CODE_FILE)\n",
    "#         rec[\"messages\"][1]['content'] = response\n",
    "\n",
    "#     return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datautils import reprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 86782/86782 [00:04<00:00, 17852.61it/s]\n"
     ]
    }
   ],
   "source": [
    "split = \"test\"\n",
    "data = json.load(open(f\"./data/ruff_meta_linting/{split}_v4.json\"))\n",
    "\n",
    "proc_data = reprocess_data(train_data=data, code_idiom_specs=code_idiom_specs, ruff_results=ruff_results, stack_data=stack_data, add_line_numbers=True)\n",
    "with open(f\"./data/ruff_meta_linting/{split}_v4_new_format_with_lineno.json\", \"w\") as f:\n",
    "    json.dump(proc_data, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
